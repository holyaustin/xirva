[{"id": "1906.00003", "submitter": "Kyungchul Song", "authors": "Nathan Canen and Kyungchul Song", "title": "Counterfactual Analysis under Partial Identification Using Locally\n  Robust Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural models that admit multiple reduced forms, such as game-theoretic\nmodels with multiple equilibria, pose challenges in practice, especially when\nparameters are set-identified and the identified set is large. In such cases,\nresearchers often choose to focus on a particular subset of equilibria for\ncounterfactual analysis, but this choice can be hard to justify. This paper\nshows that some parameter values can be more \"desirable\" than others for\ncounterfactual analysis, even if they are empirically equivalent given the\ndata. In particular, within the identified set, some counterfactual predictions\ncan exhibit more robustness than others, against local perturbations of the\nreduced forms (e.g. the equilibrium selection rule). We provide a\nrepresentation of this subset which can be used to simplify the implementation.\nWe illustrate our message using moment inequality models, and provide an\nempirical application based on a model with top-coded data.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 16:50:57 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 01:49:59 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 03:50:47 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Canen", "Nathan", ""], ["Song", "Kyungchul", ""]]}, {"id": "1906.00031", "submitter": "Daniele Bigoni", "authors": "Michael C. Brennan and Daniele Bigoni and Olivier Zahm and Alessio\n  Spantini and Youssef Marzouk", "title": "Greedy inference with structure-exploiting lazy maps", "comments": "21 pages, 37 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for solving high-dimensional Bayesian inference\nproblems using \\emph{structure-exploiting} low-dimensional transport maps or\nflows. These maps are confined to a low-dimensional subspace (hence, lazy), and\nthe subspace is identified by minimizing an upper bound on the\nKullback--Leibler divergence (hence, structured). Our framework provides a\nprincipled way of identifying and exploiting low-dimensional structure in an\ninference problem. It focuses the expressiveness of a transport map along the\ndirections of most significant discrepancy from the posterior, and can be used\nto build deep compositions of lazy maps, where low-dimensional projections of\nthe parameters are iteratively transformed to match the posterior. We prove\nweak convergence of the generated sequence of distributions to the posterior,\nand we demonstrate the benefits of the framework on challenging inference\nproblems in machine learning and differential equations, using inverse\nautoregressive flows and polynomial maps as examples of the underlying density\nestimators.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 18:52:21 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 06:20:34 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 21:37:44 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Brennan", "Michael C.", ""], ["Bigoni", "Daniele", ""], ["Zahm", "Olivier", ""], ["Spantini", "Alessio", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1906.00042", "submitter": "Yajuan Si", "authors": "Yajuan Si, Mari Palta and Maureen Smith", "title": "Bayesian Profiling Multiple Imputation for Missing Electronic Health\n  Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHRs) are increasingly used for clinical and\ncomparative effectiveness research, but suffer from missing data. Motivated by\nhealth services research on diabetes care, we seek to increase the quality of\nEHRs by focusing on missing values of longitudinal glycosylated hemoglobin\n(A1c), a key risk factor for diabetes complications and adverse events. Under\nthe framework of multiple imputation (MI), we propose an individualized\nBayesian latent profiling approach to capture A1c measurement trajectories\nsubject to missingness. The proposed method is applied to EHRs of adult\npatients with diabetes in a large academic Midwestern health system between\n2003 and 2013 and had Medicare A and B coverage. We combine MI inferences to\nevaluate the association of A1c levels with the incidence of acute adverse\nhealth events and examine patient heterogeneity across identified patient\nprofiles. We investigate different missingness mechanisms and perform\nimputation diagnostics. Our approach is computationally efficient and fits\nflexible models that provide useful clinical insights.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 19:24:32 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 21:25:42 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Si", "Yajuan", ""], ["Palta", "Mari", ""], ["Smith", "Maureen", ""]]}, {"id": "1906.00051", "submitter": "Fan Yang", "authors": "Zheng Tracy Ke, Lingzhou Xue and Fan Yang", "title": "Diagonally-Dominant Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of decomposing a large covariance matrix into the sum\nof a low-rank matrix and a diagonally dominant matrix, and we call this problem\nthe \"Diagonally-Dominant Principal Component Analysis (DD-PCA)\". DD-PCA is an\neffective tool for designing statistical methods for strongly correlated data.\nWe showcase the use of DD-PCA in two statistical problems: covariance matrix\nestimation, and global detection in multiple testing. Using the output of\nDD-PCA, we propose a new estimator for estimating a large covariance matrix\nwith factor structure. Thanks to a nice property of diagonally dominant\nmatrices, this estimator enjoys the advantage of simultaneous good estimation\nof the covariance matrix and the precision matrix (by a plain inversion). A\nplug-in of this estimator to linear discriminant analysis and portfolio\noptimization yields appealing performance in real data. We also propose two new\ntests for testing the global null hypothesis in multiple testing when the\n$z$-scores have a factor covariance structure. Both tests first use DD-PCA to\nadjust the individual $p$-values and then plug in the adjusted $p$-values to\nthe Higher Criticism (HC) test. These new tests significantly improve over the\nHC test and compare favorably with other existing tests. For computation of\nDD-PCA, we propose an iterative projection algorithm and an ADMM algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 20:02:03 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ke", "Zheng Tracy", ""], ["Xue", "Lingzhou", ""], ["Yang", "Fan", ""]]}, {"id": "1906.00116", "submitter": "Raif Rustamov", "authors": "Raif M. Rustamov and James T. Klosowski", "title": "Kernel Mean Embedding Based Hypothesis Tests for Comparing Spatial Point\n  Patterns", "comments": "Accepted to Spatial Statistics; updated source code provided", "journal-ref": null, "doi": null, "report-no": "TD:102504/2019-04-04", "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an approach for detecting differences in the\nfirst-order structures of spatial point patterns. The proposed approach\nleverages the kernel mean embedding in a novel way by introducing its\napproximate version tailored to spatial point processes. While the original\nembedding is infinite-dimensional and implicit, our approximate embedding is\nfinite-dimensional and comes with explicit closed-form formulas. With its help\nwe reduce the pattern comparison problem to the comparison of means in the\nEuclidean space. Hypothesis testing is based on conducting t-tests on each\ndimension of the embedding and combining the resulting p-values using one of\nthe recently introduced p-value combination techniques. If desired,\ncorresponding Bayes factors can be computed and averaged over all tests to\nquantify the evidence against the null. The main advantages of the proposed\napproach are that it can be applied to both single and replicated pattern\ncomparisons and that neither bootstrap nor permutation procedures are needed to\nobtain or calibrate the p-values. Our experiments show that the resulting tests\nare powerful and the p-values are well-calibrated; two applications to real\nworld data are presented.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 23:03:49 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 16:16:52 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 18:44:41 GMT"}, {"version": "v4", "created": "Fri, 12 Jun 2020 00:27:55 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Rustamov", "Raif M.", ""], ["Klosowski", "James T.", ""]]}, {"id": "1906.00198", "submitter": "Matias Cattaneo", "authors": "Sebastian Calonico, Matias D. Cattaneo, Max H. Farrell", "title": "nprobust: Nonparametric Kernel-Based Estimation and Robust\n  Bias-Corrected Inference", "comments": null, "journal-ref": "Journal of Statistical Software, 91(8): 1-33, 2019", "doi": null, "report-no": null, "categories": "stat.CO econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric kernel density and local polynomial regression estimators are\nvery popular in Statistics, Economics, and many other disciplines. They are\nroutinely employed in applied work, either as part of the main empirical\nanalysis or as a preliminary ingredient entering some other estimation or\ninference procedure. This article describes the main methodological and\nnumerical features of the software package nprobust, which offers an array of\nestimation and inference procedures for nonparametric kernel-based density and\nlocal polynomial regression methods, implemented in both the R and Stata\nstatistical platforms. The package includes not only classical bandwidth\nselection, estimation, and inference methods (Wand and Jones, 1995; Fan and\nGijbels, 1996), but also other recent developments in the statistics and\neconometrics literatures such as robust bias-corrected inference and coverage\nerror optimal bandwidth selection (Calonico, Cattaneo and Farrell, 2018, 2019).\nFurthermore, this article also proposes a simple way of estimating optimal\nbandwidths in practice that always delivers the optimal mean square error\nconvergence rate regardless of the specific evaluation point, that is, no\nmatter whether it is implemented at a boundary or interior point. Numerical\nperformance is illustrated using an empirical application and simulated data,\nwhere a detailed numerical comparison with other R packages is given.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 10:33:04 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Calonico", "Sebastian", ""], ["Cattaneo", "Matias D.", ""], ["Farrell", "Max H.", ""]]}, {"id": "1906.00202", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Max H. Farrell, Yingjie Feng", "title": "lspartition: Partitioning-Based Least Squares Regression", "comments": null, "journal-ref": "R Journal 12(1): 172-187, 2020", "doi": null, "report-no": null, "categories": "stat.CO econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric partitioning-based least squares regression is an important\ntool in empirical work. Common examples include regressions based on splines,\nwavelets, and piecewise polynomials. This article discusses the main\nmethodological and numerical features of the R software package lspartition,\nwhich implements modern estimation and inference results for partitioning-based\nleast squares (series) regression estimation. This article discusses the main\nmethodological and numerical features of the R software package lspartition,\nwhich implements results for partitioning-based least squares (series)\nregression estimation and inference from Cattaneo and Farrell (2013) and\nCattaneo, Farrell, and Feng (2019). These results cover the multivariate\nregression function as well as its derivatives. First, the package provides\ndata-driven methods to choose the number of partition knots optimally,\naccording to integrated mean squared error, yielding optimal point estimation.\nSecond, robust bias correction is implemented to combine this point estimator\nwith valid inference. Third, the package provides estimates and inference for\nthe unknown function both pointwise and uniformly in the conditioning\nvariables. In particular, valid confidence bands are provided. Finally, an\nextension to two-sample analysis is developed, which can be used in\ntreatment-control comparisons and related problems\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 10:56:29 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 10:53:29 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Farrell", "Max H.", ""], ["Feng", "Yingjie", ""]]}, {"id": "1906.00281", "submitter": "Shuhao Jiao", "authors": "Shuhao Jiao, Alexander Aue, Hernando Ombao", "title": "Functional time series prediction under partial observation of the\n  future curve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles one of the most fundamental goals in functional time\nseries analysis which is to provide reliable predictions for functions.\nExisting functional time series methods seek to predict a complete future\nfunctional observation based on a set of observed complete trajectories. The\nproblem of interest discussed here is how to advance prediction methodology to\ncases where partial information on the next trajectory is available, with the\naim of improving prediction accuracy. To solve this problem, we propose a new\nmethod \"partial functional prediction (PFP)\". The proposed method combines\n\"next-interval\" prediction and fully functional regression prediction, so that\nthe partially observed part of the trajectory can aid in producing a better\nprediction for the unobserved part of the future curve. In PFP, we include\nautomatic selection criterion for tuning parameters based on minimizing the\nprediction error. Simulations indicate that the proposed method can outperform\nexisting methods with respect to mean-square prediction error and its practical\nutility is illustrated in an analysis of environmental and traffic flow data.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 20:05:47 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 14:24:59 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Jiao", "Shuhao", ""], ["Aue", "Alexander", ""], ["Ombao", "Hernando", ""]]}, {"id": "1906.00286", "submitter": "Anders Hildeman", "authors": "Anders Hildeman, David Bolin, Igor Rychlik", "title": "Joint spatial modeling of significant wave height and wave period using\n  the SPDE approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ocean wave distribution in a specific region of space and time is\ndescribed by its sea state. Knowledge about the sea states a ship encounters on\na journey can be used to assess various parameters of risk and wear associated\nwith the journey.\n  Two important characteristics of the sea state are the significant wave\nheight and mean wave period. We propose a joint spatial model of these two\nquantities on the north Atlantic ocean. The model describes the distribution of\nthe logarithm of the two quantities as a bivariate Gaussian random field. This\nrandom field is modeled as a solution to a system of coupled stochastic partial\ndifferential equations. The bivariate random field can model a wide variety of\nnon-stationary anisotropy and allows for arbitrary, and different,\ndifferentiability for the two marginal fields.\n  The parameters of the model are estimated on data of the north Atlantic using\na stepwise maximum likelihood method. The fitted model is used to derive the\ndistribution of accumulated fatigue damage for a ship sailing a transatlantic\nroute. Also, a method for estimating the risk of capsizing due to broaching-to,\nbased on the joint distribution of the two sea state characteristics, is\ninvestigated. The risks are calculated for a transatlantic route between\nAmerica and Europe using both data and the fitted model.\n  The results show that the model compares well with observed data. Also, it\nshows that the bivariate model is needed and cannot simply be approximated by a\nmodel of significant wave height alone.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 20:37:07 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Hildeman", "Anders", ""], ["Bolin", "David", ""], ["Rychlik", "Igor", ""]]}, {"id": "1906.00348", "submitter": "Panagiotis Papastamoulis", "authors": "Panagiotis Papastamoulis", "title": "Clustering Multivariate Data using Factor Analytic Bayesian Mixtures\n  with an Unknown Number of Components", "comments": "Revised version (22 pages)", "journal-ref": "Statistics and Computing, 2019", "doi": "10.1007/s11222-019-09891-z", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on overfitting Bayesian mixtures of distributions offers a\npowerful framework for clustering multivariate data using a latent Gaussian\nmodel which resembles the factor analysis model. The flexibility provided by\noverfitting mixture models yields a simple and efficient way in order to\nestimate the unknown number of clusters and model parameters by Markov chain\nMonte Carlo (MCMC) sampling. The present study extends this approach by\nconsidering a set of eight parameterizations, giving rise to parsimonious\nrepresentations of the covariance matrix per cluster. A Gibbs sampler combined\nwith a prior parallel tempering scheme is implemented in order to approximately\nsample from the posterior distribution of the overfitting mixture. The\nparameterization and number of factors is selected according to the Bayesian\nInformation Criterion. Identifiability issues related to label switching are\ndealt by post-processing the simulated output with the Equivalence Classes\nRepresentatives algorithm. The contributed method and software are demonstrated\nand compared to similar models estimated using the Expectation-Maximization\nalgorithm on simulated and real datasets. The software is available online at\nhttps://CRAN.R-project.org/package=fabMix.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 05:26:50 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Papastamoulis", "Panagiotis", ""]]}, {"id": "1906.00349", "submitter": "Anri Mutoh", "authors": "Anri Mutoh, Masamichi Wada, Kou Amano", "title": "Comprehensive cluster validity Index based on structural simplicity", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nonhierarchical clustering depending on unsupervised algorithms may not\nretrieve the optimal partition of datasets. Determining if clusters fit\n``natural partitions`` can be achieved using cluster validity indices (CVIs).\nMost existing CVIs consider criteria such as cohesion, separation, and their\nequivalents. However, these binary relations may provide neither the optimal\nmeasure of partition suitability nor reference values corresponding to the\nworst partition. Moreover, previous CVI studies have been mostly focused on\nfitting correct partitions according to researchers' a priori assumptions. In\ncontrast, we investigated desirable properties of CVIs, namely, scale shift\ntransform invariance, optimal clustering, and unbiased clustering with\nrepresenting the worst partition. Then, we conducted experiments to evaluate\nwhether existing CVIs fulfill these properties. As none of these CVIs fulfilled\nthe desired properties, we propose the simplicity index, which measures the\nsimplicity of tree structures in clusters. The simplicity index is the unique\nindex invariant to the ``correct rate`` and provides both a reference\nindicating the most complex partition and the best value indicating the\nsimplest one.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 05:35:17 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Mutoh", "Anri", ""], ["Wada", "Masamichi", ""], ["Amano", "Kou", ""]]}, {"id": "1906.00364", "submitter": "Craig Wang", "authors": "Craig Wang, Reinhard Furrer (for the SNC Study Group)", "title": "Combining Heterogeneous Spatial Datasets with Process-based Spatial\n  Fusion Models: A Unifying Framework", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern spatial statistics, the structure of data that is collected has\nbecome more heterogeneous. Depending on the type of spatial data, different\nmodeling strategies for spatial data are used. For example, a kriging approach\nfor geostatistical data; a Gaussian Markov random field model for lattice data;\nor a log Gaussian Cox process for point-pattern data. Despite these different\nmodeling choices, the nature of underlying scientific data-generating (latent)\nprocesses is often the same, which can be represented by some continuous\nspatial surfaces. In this paper, we introduce a unifying framework for\nprocess-based multivariate spatial fusion models. The framework can jointly\nanalyze all three aforementioned types of spatial data (or any combinations\nthereof). Moreover, the framework accommodates different conditional\ndistributions for geostatistical and lattice data. We show that some\nestablished approaches, such as linear models of coregionalization, can be\nviewed as special cases of our proposed framework. We offer flexible and\nscalable implementations in R using Stan and INLA. Simulation studies confirm\nthat the predictive performance of latent processes improves as we move from\nunivariate spatial models to multivariate spatial fusion models. The introduced\nframework is illustrated using a cross-sectional study linked with a national\ncohort dataset in Switzerland, we examine differences in underlying spatial\nrisk patterns between respiratory disease and lung cancer.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 08:10:38 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Wang", "Craig", "", "for the SNC Study Group"], ["Furrer", "Reinhard", "", "for the SNC Study Group"]]}, {"id": "1906.00455", "submitter": "Harrison Quick", "authors": "Harrison Quick", "title": "Generating Poisson-Distributed Differentially Private Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dissemination of synthetic data can be an effective means of making\ninformation from sensitive data publicly available while reducing the risk of\ndisclosure associated with releasing the sensitive data directly. While\nmechanisms exist for synthesizing data that satisfy formal privacy guarantees,\nthe utility of the synthetic data is often an afterthought. More recently, the\nuse of methods from the disease mapping literature has been proposed to\ngenerate spatially-referenced synthetic data with high utility, albeit without\nformal privacy guarantees. The objective for this paper is to help bridge the\ngap between the disease mapping and the formal privacy literatures. In\nparticular, we extend an existing approach for generating formally private\nsynthetic data to the case of Poisson-distributed count data in a way that\nallows for the infusion of prior information. To evaluate the utility of the\nsynthetic data, we conducted a simulation study inspired by publicly available,\ncounty-level heart disease-related death counts. The results of this study\ndemonstrate that the proposed approach for generating differentially private\nsynthetic data outperforms a popular technique when the counts correspond to\nevents arising from subgroups with unequal population sizes or unequal event\nrates.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 17:26:23 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Quick", "Harrison", ""]]}, {"id": "1906.00494", "submitter": "Soumendu Sundar Mukherjee", "authors": "Soumendu Sundar Mukherjee and Sayak Chakrabarti", "title": "Graphon Estimation from Partially Observed Network Data", "comments": "12 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating the edge-probability matrix of a network generated\nfrom a graphon model when the full network is not observed---only some\noverlapping subgraphs are. We extend the neighbourhood smoothing (NBS)\nalgorithm of Zhang et al. (2017) to this missing-data set-up and show\nexperimentally that, for a wide range of graphons, the extended NBS algorithm\nachieves significantly smaller error rates than standard graphon estimation\nalgorithms such as vanilla neighbourhood smoothing (NBS), universal singular\nvalue thresholding (USVT), blockmodel approximation, matrix completion, etc. We\nalso show that the extended NBS algorithm is much more robust to missing data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 22:18:42 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 17:34:21 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Mukherjee", "Soumendu Sundar", ""], ["Chakrabarti", "Sayak", ""]]}, {"id": "1906.00505", "submitter": "Yotam Hechtlinger", "authors": "Yoav Benjamini, Yotam Hechtlinger and Philip B. Stark", "title": "Confidence Intervals for Selected Parameters", "comments": "36 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Practical or scientific considerations often lead to selecting a subset of\nparameters as ``important.'' Inferences about those parameters often are based\non the same data used to select them in the first place. That can make the\nreported uncertainties deceptively optimistic: confidence intervals that ignore\nselection generally have less than their nominal coverage probability.\nControlling the probability that one or more intervals for selected parameters\ndo not cover---the ``simultaneous over the selected'' (SoS) error rate---is\ncrucial in many scientific problems. Intervals that control the SoS error rate\ncan be constructed in ways that take advantage of knowledge of the selection\nrule. We construct SoS-controlling confidence intervals for parameters deemed\nthe most ``important'' $k$ of $m$ shift parameters because they are estimated\n(by independent estimators) to be the largest. The new intervals improve\nsubstantially over \\v{S}id\\'{a}k intervals when $k$ is small compared to $m$,\nand approach the standard Bonferroni-corrected intervals when $k \\approx m$.\nStandard, unadjusted confidence intervals for location parameters have the\ncorrect coverage probability for $k=1$, $m=2$ if, when the true parameters are\nzero, the estimators are exchangeable and symmetric.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 23:58:17 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Benjamini", "Yoav", ""], ["Hechtlinger", "Yotam", ""], ["Stark", "Philip B.", ""]]}, {"id": "1906.00528", "submitter": "Tianchen Qian", "authors": "Tianchen Qian, Hyesun Yoo, Predrag Klasnja, Daniel Almirall, Susan A.\n  Murphy", "title": "Estimating Time-Varying Causal Excursion Effect in Mobile Health with\n  Binary Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in wearables and digital technology now make it possible to deliver\nbehavioral mobile health interventions to individuals in their everyday life.\nThe micro-randomized trial (MRT) is increasingly used to provide data to inform\nthe construction of these interventions. In an MRT, each individual is\nrepeatedly randomized among multiple intervention options, often hundreds or\neven thousands of times, over the course of the trial. This work is motivated\nby multiple MRTs that have been conducted, or are currently in the field, in\nwhich the primary outcome is a longitudinal binary outcome. The primary aim of\nsuch MRTs is to examine whether a particular time-varying intervention has an\neffect on the longitudinal binary outcome, often marginally over all but a\nsmall subset of the individual's data. We propose the definition of causal\nexcursion effect that can be used in such primary aim analysis for MRTs with\nbinary outcomes. Under rather restrictive assumptions one can, based on\nexisting literature, derive a semiparametric, locally efficient estimator of\nthe causal effect. We, starting from this estimator, develop an estimator that\ncan be used as the basis of a primary aim analysis under more plausible\nassumptions. Simulation studies are conducted to compare the estimators. We\nillustrate the developed methods using data from the MRT, BariFit. In BariFit,\nthe goal is to support weight maintenance for individuals who received\nbariatric surgery.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 02:17:17 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 20:09:52 GMT"}, {"version": "v3", "created": "Sun, 19 Jan 2020 18:02:09 GMT"}, {"version": "v4", "created": "Tue, 16 Jun 2020 01:32:45 GMT"}, {"version": "v5", "created": "Wed, 29 Jul 2020 16:02:59 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Qian", "Tianchen", ""], ["Yoo", "Hyesun", ""], ["Klasnja", "Predrag", ""], ["Almirall", "Daniel", ""], ["Murphy", "Susan A.", ""]]}, {"id": "1906.00538", "submitter": "Wentian Huang", "authors": "Wentian Huang, David Ruppert", "title": "Copula-based functional Bayes classification with principal components\n  and partial least squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new functional Bayes classifier that uses principal component\n(PC) or partial least squares (PLS) scores from the common covariance function,\nthat is, the covariance function marginalized over groups. When the groups have\ndifferent covariance functions, the PC or PLS scores need not be independent or\neven uncorrelated. We use copulas to model the dependence. Our method is\nsemiparametric; the marginal densities are estimated nonparametrically by\nkernel smoothing and the copula is modeled parametrically. We focus on Gaussian\nand t-copulas, but other copulas could be used. The strong performance of our\nmethodology is demonstrated through simulation, real data examples, and\nasymptotic properties.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 02:51:56 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 00:37:11 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Huang", "Wentian", ""], ["Ruppert", "David", ""]]}, {"id": "1906.00558", "submitter": "Jiaqi Yin", "authors": "Jiaqi Yin, Thomas S. Richardson and Linbo Wang", "title": "Multiplicative Effect Modeling: The General Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models, such as logistic regression, are widely used to\nmodel the association between a treatment and a binary outcome as a function of\nbaseline covariates. However, the coefficients of a logistic regression model\ncorrespond to log odds ratios, while subject-matter scientists are often\ninterested in relative risks. Although odds ratios are sometimes used to\napproximate relative risks, this approximation is appropriate only when the\noutcome of interest is rare for all levels of the covariates. Poisson\nregressions do measure multiplicative treatment effects including relative\nrisks, but with a binary outcome not all combinations of parameters lead to\nfitted means that are between zero and one. Enforcing this constraint makes the\nparameters variation dependent, which is undesirable for modeling, estimation\nand computation. Focusing on the special case where the treatment is also\nbinary, Richardson et al. (2017) propose a novel binomial regression model,\nthat allows direct modeling of the relative risk. The model uses a log\nodds-product nuisance model leading to variation independent parameter spaces.\nBuilding on this we present general approaches to modeling the multiplicative\neffect of a continuous or categorical treatment on a binary outcome. Monte\nCarlo simulations demonstrate the superior performance of our proposed methods.\nA data analysis further exemplifies our methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 04:02:08 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 06:52:03 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 04:13:44 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Yin", "Jiaqi", ""], ["Richardson", "Thomas S.", ""], ["Wang", "Linbo", ""]]}, {"id": "1906.00587", "submitter": "Antonio Punzo", "authors": "Luca Bagnato, Antonio Punzo", "title": "Unconstrained representation of orthogonal matrices with application to\n  common principle components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical problems involve the estimation of a $\\left(d\\times\nd\\right)$ orthogonal matrix $\\textbf{Q}$. Such an estimation is often\nchallenging due to the orthonormality constraints on $\\textbf{Q}$. To cope with\nthis problem, we propose a very simple decomposition for orthogonal matrices\nwhich we abbreviate as PLR decomposition. It produces a one-to-one\ncorrespondence between $\\textbf{Q}$ and a $\\left(d\\times d\\right)$ unit lower\ntriangular matrix $\\textbf{L}$ whose $d\\left(d-1\\right)/2$ entries below the\ndiagonal are unconstrained real values. Once the decomposition is applied,\nregardless of the objective function under consideration, we can use any\nclassical unconstrained optimization method to find the minimum (or maximum) of\nthe objective function with respect to $\\textbf{L}$. For illustrative purposes,\nwe apply the PLR decomposition in common principle components analysis (CPCA)\nfor the maximum likelihood estimation of the common orthogonal matrix when a\nmultivariate leptokurtic-normal distribution is assumed in each group. Compared\nto the commonly used normal distribution, the leptokurtic-normal has an\nadditional parameter governing the excess kurtosis; this makes the estimation\nof $\\textbf{Q}$ in CPCA more robust against mild outliers. The usefulness of\nthe PLR decomposition in leptokurtic-normal CPCA is illustrated by two\nbiometric data analyses.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 05:58:54 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Bagnato", "Luca", ""], ["Punzo", "Antonio", ""]]}, {"id": "1906.00694", "submitter": "Eliana Christou", "authors": "Eliana Christou", "title": "Central Quantile Subspace", "comments": "arXiv admin note: text overlap with arXiv:1906.00696", "journal-ref": null, "doi": "10.1007/s11222-019-09915-8", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression (QR) is becoming increasingly popular due to its\nrelevance in many scientific investigations. There is a great amount of work\nabout linear and nonlinear QR models. Specifically, nonparametric estimation of\nthe conditional quantiles received particular attention, due to its model\nflexibility. However, nonparametric QR techniques are limited in the number of\ncovariates. Dimension reduction offers a solution to this problem by\nconsidering low-dimensional smoothing without specifying any parametric or\nnonparametric regression relation. Existing dimension reduction techniques\nfocus on the entire conditional distribution. We, on the other hand, turn our\nattention to dimension reduction techniques for conditional quantiles and\nintroduce a new method for reducing the dimension of the predictor X. The\nnovelty of this paper is threefold. We start by considering a single index\nquantile regression model, which assumes that the conditional quantile depends\non X through a single linear combination of the predictors, then extend to a\nmulti index quantile regression model, and finally, generalize the proposed\nmethodology to any statistical functional of the conditional distribution. The\nperformance of the methodology is demonstrated through simulation examples and\na real data application. Our results suggest that this method has a good finite\nsample performance and often outperforms existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 10:43:52 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Christou", "Eliana", ""]]}, {"id": "1906.00696", "submitter": "Eliana Christou", "authors": "Eliana Christou", "title": "Transformed Central Quantile Subspace", "comments": "arXiv admin note: text overlap with arXiv:1906.00694", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression (QR) is becoming increasingly popular due to its\nrelevance in many scientific investigations. However, application of QR can\nbecome very challenging when dealing with high-dimensional data, making it\nnecessary to use dimension reduction techniques. Existing dimension reduction\ntechniques focus on the entire conditional distribution. We turn our attention\nto dimension reduction techniques for conditional quantiles and introduce a\nmethod that serves as an intermediate step between linear and nonlinear\ndimension reduction. The idea is to apply existing linear dimension reduction\ntechniques on the transformed predictors. The proposed estimator, which is\nshown to be root-n consistent, is demonstrated through simulation examples and\nreal data applications. Our results suggest that this method outperforms linear\ndimension reduction for conditional quantiles.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 10:48:35 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 15:09:13 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Christou", "Eliana", ""]]}, {"id": "1906.00818", "submitter": "Molin Wang", "authors": "Abigail Sloan and Molin Wang", "title": "Statistical methods for biomarker data pooled from multiple nested\n  case-control studies", "comments": "2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooling biomarker data across multiple studies allows for examination of a\nwider exposure range than generally possible in individual studies, evaluation\nof population subgroups and disease subtypes with more statistical power, and\nmore precise estimation of biomarker-disease associations. However, biomarker\nmeasurements often require calibration to a reference assay prior to pooling\ndue to assay and laboratory variability across studies. We propose several\nmethods for calibrating and combining biomarker data from nested case-control\nstudies when reference assay data are obtained from a subset of controls in\neach contributing study. Specifically, we describe a two-stage method and two\naggregated methods, named the internalized and full calibration methods, to\nevaluate the main effect of the biomarker exposure on disease risk and whether\nthat association is modified by a potential covariate. The internalized method\nuses the reference laboratory measurement in the analysis when available and\notherwise uses the calibrated measurement. The full calibration method uses\ncalibrated biomarker measurements for all subjects, even those with reference\nlaboratory measurements. Our results demonstrate that the full calibration\nmethod is the preferred aggregated approach to minimize bias in point estimates\nregardless of the inclusion of an interaction term in the model. We also\nobserve that the two-stage and full calibration methods provide similar effect\nand variance estimates, but that the variance estimates for these two methods\nare slightly larger than those from the internalized approach. As an\nillustrative example, we apply the methods in a pooling project of nested\ncase-control studies to evaluate (i) the association between circulating\nvitamin D levels and risk of stroke, and (ii) how BMI modifies the association\nbetween circulating vitamin D levels and cardiovascular disease.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 13:57:13 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Sloan", "Abigail", ""], ["Wang", "Molin", ""]]}, {"id": "1906.00928", "submitter": "Basil Saeed", "authors": "Basil Saeed, Anastasiya Belyaeva, Yuhao Wang, Caroline Uhler", "title": "Anchored Causal Inference in the Presence of Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a causal graph in the presence of\nmeasurement error. This setting is for example common in genomics, where gene\nexpression is corrupted through the measurement process. We develop a provably\nconsistent procedure for estimating the causal structure in a linear Gaussian\nstructural equation model from corrupted observations on its nodes, under a\nvariety of measurement error models. We provide an estimator based on the\nmethod-of-moments, which can be used in conjunction with constraint-based\ncausal structure discovery algorithms. We prove asymptotic consistency of the\nprocedure and also discuss finite-sample considerations. We demonstrate our\nmethod's performance through simulations and on real data, where we recover the\nunderlying gene regulatory network from zero-inflated single-cell RNA-seq data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 17:01:35 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Saeed", "Basil", ""], ["Belyaeva", "Anastasiya", ""], ["Wang", "Yuhao", ""], ["Uhler", "Caroline", ""]]}, {"id": "1906.01006", "submitter": "Ben Derrick Mr", "authors": "Ben Derrick, Deirdre Toher, Paul White", "title": "The performance of the partially overlapping samples t-tests at the\n  limits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note includes an assessment of the partially overlapping samples t-tests\nin scenarios where elements of the test are at their extremes, including where\nonly one sample contains independent observations\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 18:14:56 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Derrick", "Ben", ""], ["Toher", "Deirdre", ""], ["White", "Paul", ""]]}, {"id": "1906.01057", "submitter": "Jie Ren", "authors": "Jie Ren, Fei Zhou, Xiaoxi Li, Qi Chen, Hongmei Zhang, Shuangge Ma, Yu\n  Jiang and Cen Wu", "title": "Semi-parametric Bayesian variable selection for gene-environment\n  interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex diseases are known to be affected by the interactions between\ngenetic variants and environmental exposures beyond the main genetic and\nenvironmental effects. Study of gene-environment (G$\\times$E) interactions is\nimportant for elucidating the disease etiology. Existing Bayesian methods for\nG$\\times$E interaction studies are challenged by the high-dimensional nature of\nthe study and the complexity of environmental influences. Many studies have\nshown the advantages of penalization methods in detecting G$\\times$E\ninteractions in \"large p, small n\" settings. However, Bayesian variable\nselection, which can provide fresh insight into G$\\times$E study, has not been\nwidely examined. We propose a novel and powerful semi-parametric Bayesian\nvariable selection model that can investigate linear and nonlinear G$\\times$E\ninteractions simultaneously. Furthermore, the proposed method can conduct\nstructural identification by distinguishing nonlinear interactions from\nmain-effects-only case within the Bayesian framework. Spike and slab priors are\nincorporated on both individual and group levels to identify the sparse main\nand interaction effects. The proposed method conducts Bayesian variable\nselection more efficiently than existing methods. Simulation shows that the\nproposed model outperforms competing alternatives in terms of both\nidentification and prediction. The proposed Bayesian method leads to the\nidentification of main and interaction effects with important implications in a\nhigh-throughput profiling study with high-dimensional SNP data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 20:07:25 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 20:41:01 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 21:52:15 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Ren", "Jie", ""], ["Zhou", "Fei", ""], ["Li", "Xiaoxi", ""], ["Chen", "Qi", ""], ["Zhang", "Hongmei", ""], ["Ma", "Shuangge", ""], ["Jiang", "Yu", ""], ["Wu", "Cen", ""]]}, {"id": "1906.01204", "submitter": "Paulo Orenstein", "authors": "Paulo Orenstein", "title": "Robust Mean Estimation with the Bayesian Median of Means", "comments": "54 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sample mean is often used to aggregate different unbiased estimates of a\nparameter, producing a final estimate that is unbiased but possibly\nhigh-variance. This paper introduces the Bayesian median of means, an\naggregation rule that roughly interpolates between the sample mean and median,\nresulting in estimates with much smaller variance at the expense of bias. While\nthe procedure is non-parametric, its squared bias is asymptotically negligible\nrelative to the variance, similar to maximum likelihood estimators. The\nBayesian median of means is consistent, and concentration bounds for the\nestimator's bias and $L_1$ error are derived, as well as a fast non-randomized\napproximating algorithm. The performances of both the exact and the approximate\nprocedures match that of the sample mean in low-variance settings, and exhibit\nmuch better results in high-variance scenarios. The empirical performances are\nexamined in real and simulated data, and in applications such as importance\nsampling, cross-validation and bagging.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 05:41:52 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Orenstein", "Paulo", ""]]}, {"id": "1906.01341", "submitter": "Toru Imai", "authors": "Toru Imai", "title": "Estimating Real Log Canonical Thresholds", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of the marginal likelihood plays an important role in model\nselection problems. The widely applicable Bayesian information criterion (WBIC)\nand singular Bayesian information criterion (sBIC) give approximations to the\nlog marginal likelihood, which can be applied to both regular and singular\nmodels. When the real log canonical thresholds are known, the performance of\nsBIC is considered to be better than that of WBIC, but only few real log\ncanonical thresholds are known. In this paper, we propose a new estimator of\nthe real log canonical thresholds based on the variance of thermodynamic\nintegration with an inverse temperature. In addition, we propose an application\nto make sBIC widely applicable. Finally, we investigate the performance of the\nestimator and model selection by simulation studies and application to real\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 10:59:33 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 07:14:18 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Imai", "Toru", ""]]}, {"id": "1906.01461", "submitter": "Kellyn Arnold", "authors": "Kellyn F. Arnold, Vinny Davies, Marc de Kamps, Peter W. G. Tennant,\n  John Mbotwa, Mark S. Gilthorpe", "title": "Generalised linear models for prognosis and intervention: Theory,\n  practice, and implications for machine learning", "comments": "15 pages, 1 figure; minor changes made following external feedback\n  [v2]", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction and causal explanation are fundamentally distinct tasks of data\nanalysis. In health applications, this difference can be understood in terms of\nthe difference between prognosis (prediction) and prevention/treatment (causal\nexplanation). Nevertheless, these two concepts are often conflated in practice.\nWe use the framework of generalised linear models (GLMs) to illustrate that\npredictive and causal queries require distinct processes for their application\nand subsequent interpretation of results. In particular, we identify five\nprimary ways in which GLMs for prediction differ from GLMs for causal\ninference: (1) The covariates that should be considered for inclusion in (and\npossibly exclusion from) the model; (2) How a suitable set of covariates to\ninclude in the model is determined; (3) Which covariates are ultimately\nselected, and what functional form (i.e. parameterisation) they take; (4) How\nthe model is evaluated; and (5) How the model is interpreted. We outline some\nof the potential consequences of failing to acknowledge and respect these\ndifferences, and additionally consider the implications for machine learning\n(ML) methods. We then conclude with three recommendations which we hope will\nhelp ensure that both prediction and causal modelling are used appropriately\nand to greatest effect in health research.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 14:12:36 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 12:09:40 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Arnold", "Kellyn F.", ""], ["Davies", "Vinny", ""], ["de Kamps", "Marc", ""], ["Tennant", "Peter W. G.", ""], ["Mbotwa", "John", ""], ["Gilthorpe", "Mark S.", ""]]}, {"id": "1906.01465", "submitter": "Jeffrey Uhlmann", "authors": "Truc Le and Jeffrey Uhlmann", "title": "Gap-Measure Tests with Applications to Data Integrity Verification", "comments": null, "journal-ref": "Statistics Research Letters}, Statistics Research Letters, vol. 4,\n  pp. 11-17, 2015", "doi": "10.14355/srl.2015.04.003", "report-no": null, "categories": "stat.ME cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose and examine gap statistics for assessing uniform\ndistribution hypotheses. We provide examples relevant to data integrity testing\nfor which max-gap statistics provide greater sensitivity than chi-square\n($\\chi^2$), thus allowing the new test to be used in place of or as a\ncomplement to $\\chi^2$ testing for purposes of distinguishing a larger class of\ndeviations from uniformity. We establish that the proposed max-gap test has the\nsame sequential and parallel computational complexity as $\\chi^2$ and thus is\napplicable for Big Data analytics and integrity verification.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 16:20:14 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Le", "Truc", ""], ["Uhlmann", "Jeffrey", ""]]}, {"id": "1906.01484", "submitter": "Matthias Eckardt", "authors": "Matthias Eckardt and Jorge Mateu", "title": "Partial and semi-partial measures of spatial associations for\n  multivariate lattice data", "comments": "submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the development of partial and semi-partial measures of\nspatial associations in the context of multivariate spatial lattice data which\ndescribe global or local associations among spatially aggregated measurements\nfor pairs of different components conditional on all remaining components. The\nnew measures are illustrated using aggregated data on crime counts at ward\nlevel.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 14:47:29 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Eckardt", "Matthias", ""], ["Mateu", "Jorge", ""]]}, {"id": "1906.01547", "submitter": "Fabien Navarro", "authors": "Marie du Roy de Chaumaray, Matthieu Marbac, Fabien Navarro", "title": "Mixture of hidden Markov models for accelerometer data", "comments": null, "journal-ref": "Ann. Appl. Stat. 14 (2020), no. 4, 1834--1855", "doi": "10.1214/20-AOAS1375", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the analysis of accelerometer data, we introduce a specific\nfinite mixture of hidden Markov models with particular characteristics that\nadapt well to the specific nature of this type of data. Our model allows for\nthe computation of statistics that characterize the physical activity of a\nsubject (\\emph{e.g.}, the mean time spent at different activity levels and the\nprobability of the transition between two activity levels) without specifying\nthe activity levels in advance but by estimating them from the data. In\naddition, this approach allows the heterogeneity of the population to be taken\ninto account and subpopulations with homogeneous physical activity behavior to\nbe defined. We prove that, under mild assumptions, this model implies that the\nprobability of misclassifying a subject decreases at an exponential decay with\nthe length of its measurement sequence. Model identifiability is also\ninvestigated. We also report a comprehensive suite of numerical simulations to\nsupport our theoretical findings. Method is motivated by and applied to the PAT\nstudy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 16:06:22 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 14:20:03 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["de Chaumaray", "Marie du Roy", ""], ["Marbac", "Matthieu", ""], ["Navarro", "Fabien", ""]]}, {"id": "1906.01611", "submitter": "Nikolaos Ignatiadis", "authors": "Nikolaos Ignatiadis, Stefan Wager", "title": "Covariate-Powered Empirical Bayes Estimation", "comments": "Advances in Neural Information Processing Systems 32 (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study methods for simultaneous analysis of many noisy experiments in the\npresence of rich covariate information. The goal of the analyst is to optimally\nestimate the true effect underlying each experiment. Both the noisy\nexperimental results and the auxiliary covariates are useful for this purpose,\nbut neither data source on its own captures all the information available to\nthe analyst. In this paper, we propose a flexible plug-in empirical Bayes\nestimator that synthesizes both sources of information and may leverage any\nblack-box predictive model. We show that our approach is within a constant\nfactor of minimax for a simple data-generating model. Furthermore, we establish\nrobust convergence guarantees for our method that hold under considerable\ngenerality, and exhibit promising empirical performance on both real and\nsimulated data.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 17:44:25 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 02:26:53 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Ignatiadis", "Nikolaos", ""], ["Wager", "Stefan", ""]]}, {"id": "1906.01701", "submitter": "Xiongzhi Chen", "authors": "Xiongzhi Chen and Sanat K. Sarkar", "title": "On Benjamini-Hochberg procedure applied to mid p-values", "comments": "23 pages; 4 figures; accepted by Journal of Statistical Planning and\n  Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiple testing with discrete p-values routinely arises in various\nscientific endeavors. However, procedures, including the false discovery rate\n(FDR) controlling Benjamini-Hochberg (BH) procedure, often used in such\nsettings, being developed originally for p-values with continuous\ndistributions, are too conservative, and so may not be as powerful as one would\nhope for. Therefore, improving the BH procedure by suitably adapting it to\ndiscrete p-values without losing its FDR control is currently an important path\nof research. This paper studies the FDR control of the BH procedure when it is\napplied to mid p-values and derive conditions under which it is conservative.\nOur simulation study reveals that the BH procedure applied to mid p-values may\nbe conservative under much more general settings than characterized in this\nwork, and that an adaptive version of the BH procedure applied to mid p-values\nis as powerful as an existing adaptive procedure based on randomized p-values.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 19:57:31 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Chen", "Xiongzhi", ""], ["Sarkar", "Sanat K.", ""]]}, {"id": "1906.01741", "submitter": "Louis Capitaine", "authors": "Louis Capitaine, J\\'er\\'emie Bigot, Rodolphe Thi\\'ebaut and Robin\n  Genuer", "title": "Fr\\'echet random forests for metric space valued regression with non\n  euclidean predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests are a statistical learning method widely used in many areas of\nscientific research because of its ability to learn complex relationships\nbetween input and output variables and also their capacity to handle\nhigh-dimensional data. However, current random forest approaches are not\nflexible enough to handle heterogeneous data such as curves, images and shapes.\nIn this paper, we introduce Fr\\'echet trees and Fr\\'echet random forests, which\nallow to handle data for which input and output variables take values in\ngeneral metric spaces (which can be unordered). To this end, a new way of\nsplitting the nodes of trees is introduced and the prediction procedures of\ntrees and forests are generalized. Then, random forests out-of-bag error and\nvariable importance score are naturally adapted. A consistency theorem for\nFr\\'echet regressogram predictor using data-driven partitions is given and\napplied to Fr\\'echet purely uniformly random trees. The method is studied\nthrough several simulation scenarios on heterogeneous data combining\nlongitudinal, image and scalar data. Finally, two real datasets from HIV\nvaccine trials are analyzed with the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 22:07:24 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 13:10:50 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Capitaine", "Louis", ""], ["Bigot", "J\u00e9r\u00e9mie", ""], ["Thi\u00e9baut", "Rodolphe", ""], ["Genuer", "Robin", ""]]}, {"id": "1906.01850", "submitter": "F. Richard Guo", "authors": "F. Richard Guo and Thomas S. Richardson", "title": "On Testing Marginal versus Conditional Independence", "comments": "Revisions and updated references", "journal-ref": null, "doi": "10.1093/biomet/asaa040", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider testing marginal independence versus conditional independence in\na trivariate Gaussian setting. The two models are non-nested and their\nintersection is a union of two marginal independences. We consider two\nsequences of such models, one from each type of independence, that are closest\nto each other in the Kullback-Leibler sense as they approach the intersection.\nThey become indistinguishable if the signal strength, as measured by the\nproduct of two correlation parameters, decreases faster than the standard\nparametric rate. Under local alternatives at such rate, we show that the\nasymptotic distribution of the likelihood ratio depends on where and how the\nlocal alternatives approach the intersection. To deal with this non-uniformity,\nwe study a class of \"envelope\" distributions by taking pointwise suprema over\nasymptotic cumulative distribution functions. We show that these envelope\ndistributions are well-behaved and lead to model selection procedures with\nrate-free uniform error guarantees and near-optimal power. To control the error\neven when the two models are indistinguishable, rather than insist on a\ndichotomous choice, the proposed procedure will choose either or both models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 06:45:12 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 06:19:24 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Guo", "F. Richard", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1906.01853", "submitter": "Xin Wang", "authors": "Xin Wang, Zhengyuan Zhu, Hao Helen Zhang", "title": "Spatial Heterogeneity Automatic Detection and Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial regression is widely used for modeling the relationship between a\ndependent variable and explanatory covariates. Oftentimes, the linear\nrelationships vary across space, when some covariates have location-specific\neffects on the response. One fundamental question is how to detect the\nsystematic variation in the model and identify which locations share common\nregression coefficients and which do not. Only a correct model structure can\nassure unbiased estimation of coefficients and valid inferences. In this work,\nwe propose a new procedure, called Spatial Heterogeneity Automatic Detection\nand Estimation (SHADE), for automatically and simultaneously subgrouping and\nestimating covariate effects for spatial regression models. The SHADE employs a\nclass of spatially-weighted fusion type penalty on all pairs of observations,\nwith location-specific weight adaptively constructed using spatial information,\nto cluster coefficients into subgroups. Under certain regularity conditions,\nthe SHADE is shown to be able to identify the true model structure with\nprobability approaching one and estimate regression coefficients consistently.\nWe develop an alternating direction method of multiplier algorithm (ADMM) to\ncompute the SHAD efficiently. In numerical studies, we demonstrate empirical\nperformance of the SHADE by using different choices of weights and compare\ntheir accuracy. The results suggest that spatial information can enhance\nsubgroup structure analysis in challenging situations when the spatial\nvariation among regression coefficients is small or the number of repeated\nmeasures is small. Finally, the SHADE is applied to find the relationship\nbetween a natural resource survey and a land cover data layer to identify\nspatially interpretable groups.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 06:49:22 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 04:13:44 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Wang", "Xin", ""], ["Zhu", "Zhengyuan", ""], ["Zhang", "Hao Helen", ""]]}, {"id": "1906.01882", "submitter": "Fabien Navarro", "authors": "Basile de Loynes, Fabien Navarro, Baptiste Olivier", "title": "Data-driven Thresholding in Denoising with Spectral Graph Wavelet\n  Transform", "comments": null, "journal-ref": "Journal of Computational and Applied Mathematics, Volume 389, 2021", "doi": "10.1016/j.cam.2020.113319", "report-no": null, "categories": "eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to adaptive signal denoising in the context of Graph\nSignal Processing (GSP) using Spectral Graph Wavelet Transform (SGWT). This\nissue is addressed \\emph{via} a data-driven thresholding process in the\ntransformed domain by optimizing the parameters in the sense of the Mean Square\nError (MSE) using the Stein's Unbiased Risk Estimator (SURE). The SGWT\nconsidered is built upon a partition of unity making the transform\nsemi-orthogonal so that the optimization can be performed in the transformed\ndomain. However, since the SGWT is over-complete, the divergence term in the\nSURE needs to be computed in the context of correlated noise. Two thresholding\nstrategies called coordinatewise and block thresholding process are\ninvestigated. For each of them, the SURE is derived for a whole family of\nelementary thresholding functions among which the soft threshold and the\nJames-Stein threshold. This multi-scales analysis shows better performance than\nthe most recent methods from the literature. That is illustrated numerically\nfor a series of signals on different graphs.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 08:47:40 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 23:04:11 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 10:24:19 GMT"}, {"version": "v4", "created": "Wed, 18 Nov 2020 12:58:54 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["de Loynes", "Basile", ""], ["Navarro", "Fabien", ""], ["Olivier", "Baptiste", ""]]}, {"id": "1906.01939", "submitter": "Hyeyoung Maeng", "authors": "Hyeyoung Maeng and Piotr Fryzlewicz", "title": "Detecting linear trend changes and point anomalies in data sequences", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose TrendSegment, a methodology for detecting multiple change-points\ncorresponding to linear trend changes or point anomalies in one dimensional\ndata. A core ingredient of TrendSegment is a new Tail-Greedy Unbalanced Wavelet\ntransform: a conditionally orthonormal, bottom-up transformation of the data\nthrough an adaptively constructed unbalanced wavelet basis, which results in a\nsparse representation of the data. The bottom-up nature of this multiscale\ndecomposition enables the detection of point anomalies and linear trend changes\nat once as the decomposition focuses on local features in its early stages and\non global features next. To reduce the computational complexity, the proposed\nmethod merges multiple regions in a single pass over the data. We show the\nconsistency of the estimated number and locations of change-points. The\npracticality of our approach is demonstrated through simulations and two real\ndata examples, involving Iceland temperature data and sea ice extent of the\nArctic and the Antarctic. Our methodology is implemented in the R package\ntrendsegmentR, available from CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 11:00:54 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Maeng", "Hyeyoung", ""], ["Fryzlewicz", "Piotr", ""]]}, {"id": "1906.01990", "submitter": "Patrick Laurie Davies Mr", "authors": "Laurie Davies and Lutz D\\\"umbgen", "title": "Covariate Selection Based on a Model-free Approach to Linear Regression\n  with Exact Probabilities", "comments": "40 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a completely new approach to the problem of\ncovariate selection in linear regression. It is intuitive, very simple, fast\nand powerful, non-frequentist and non-Bayesian. It does not overfit, there is\nno shrinkage of the least squares coefficients, and it is model-free. A\ncovariate or a set of covariates is included only if they are better in the\nsense of least squares than the same number of Gaussian covariates consisting\nof i.i.d. $N(0,1)$ random variables. The degree to which they are better is\nmeasured by the P-value which is the probability that the Gaussian covariates\nare better. This probability is given in terms of the Beta distribution, it is\nexact and it holds for the data at hand whatever this may be. The idea extends\nto a stepwise procedure, the main contribution of the paper, where the best of\nthe remaining covariates is only accepted if it is better than the best of the\nsame number of random Gaussian covariates. Again this probability is given in\nterms of the Beta distribution, it is exact and it holds for the data at hand\nwhatever this may be. We use a version with default parameters which works for\na large collection of known data sets with up to a few hundred thousand\ncovariates. The computing time for the largest data sets was about four\nseconds, and it outperforms all other selection procedures of which we are\naware. The paper gives the results of simulations, applications to real data\nsets and theorems on the asymptotic behaviour under the standard linear model.\nAn R-package {\\it gausscov} is available. \\\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 12:42:04 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 17:01:25 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Davies", "Laurie", ""], ["D\u00fcmbgen", "Lutz", ""]]}, {"id": "1906.02014", "submitter": "Christopher Drovandi Dr", "authors": "Christopher Drovandi, Richard G Everitt, Andrew Golightly, Dennis\n  Prangle", "title": "Ensemble MCMC: Accelerating Pseudo-Marginal MCMC for State Space Models\n  using the Ensemble Kalman Filter", "comments": "minor edits, more extensive results, added web link to supporting\n  computer code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Markov chain Monte Carlo (pMCMC) is now a popular method for\nperforming Bayesian statistical inference on challenging state space models\n(SSMs) with unknown static parameters. It uses a particle filter (PF) at each\niteration of an MCMC algorithm to unbiasedly estimate the likelihood for a\ngiven static parameter value. However, pMCMC can be computationally intensive\nwhen a large number of particles in the PF is required, such as when the data\nis highly informative, the model is misspecified and/or the time series is\nlong. In this paper we exploit the ensemble Kalman filter (EnKF) developed in\nthe data assimilation literature to speed up pMCMC. We replace the unbiased PF\nlikelihood with the biased EnKF likelihood estimate within MCMC to sample over\nthe space of the static parameter. On a wide class of different non-linear SSM\nmodels, we demonstrate that our new ensemble MCMC (eMCMC) method can\nsignificantly reduce the computational cost whilst maintaining reasonable\naccuracy. We also propose several extensions of the vanilla eMCMC algorithm to\nfurther improve computational efficiency. Computer code to implement our\nmethods on all the examples can be downloaded from\nhttps://github.com/cdrovandi/Ensemble-MCMC.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 13:09:10 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 14:46:05 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Drovandi", "Christopher", ""], ["Everitt", "Richard G", ""], ["Golightly", "Andrew", ""], ["Prangle", "Dennis", ""]]}, {"id": "1906.02030", "submitter": "Zhichao Jiang", "authors": "Zhichao Jiang and Peng Ding", "title": "Measurement errors in the binary instrumental variable model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable methods can identify causal effects even when the\ntreatment and outcome are confounded. We study the problem of imperfect\nmeasurements of the binary instrumental variable, treatment or outcome. We\nfirst consider non-differential measurement errors, that is, the mis-measured\nvariable does not depend on other variables given its true value. We show that\nthe measurement error of the instrumental variable does not bias the estimate,\nthe measurement error of the treatment biases the estimate away from zero, and\nthe measurement error of the outcome biases the estimate toward zero. Moreover,\nwe derive sharp bounds on the causal effects without additional assumptions.\nThese bounds are informative because they exclude zero. We then consider\ndifferential measurement errors, and focus on sensitivity analyses in those\nsettings.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 13:45:31 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Jiang", "Zhichao", ""], ["Ding", "Peng", ""]]}, {"id": "1906.02120", "submitter": "Claudia Shi", "authors": "Claudia Shi, David M. Blei, Victor Veitch", "title": "Adapting Neural Networks for the Estimation of Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the use of neural networks for the estimation of\ntreatment effects from observational data. Generally, estimation proceeds in\ntwo stages. First, we fit models for the expected outcome and the probability\nof treatment (propensity score) for each unit. Second, we plug these fitted\nmodels into a downstream estimator of the effect. Neural networks are a natural\nchoice for the models in the first step. The question we address is: how can we\nadapt the design and training of the neural networks used in the first step in\norder to improve the quality of the final estimate of the treatment effect? We\npropose two adaptations based on insights from the statistical literature on\nthe estimation of treatment effects. The first is a new architecture, the\nDragonnet, that exploits the sufficiency of the propensity score for estimation\nadjustment. The second is a regularization procedure, targeted regularization,\nthat induces a bias towards models that have non-parametrically optimal\nasymptotic properties `out-of-the-box`. Studies on benchmark datasets for\ncausal inference show these adaptations outperform existing methods. Code is\navailable at github.com/claudiashi57/dragonnet.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 16:47:13 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 18:05:31 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Shi", "Claudia", ""], ["Blei", "David M.", ""], ["Veitch", "Victor", ""]]}, {"id": "1906.02140", "submitter": "Matteo Iacopini", "authors": "Matteo Iacopini and Luca Rossini", "title": "Bayesian nonparametric graphical models for time-varying parameters VAR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, big data have poured into econometrics, demanding new\nstatistical methods for analysing high-dimensional data and complex non-linear\nrelationships. A common approach for addressing dimensionality issues relies on\nthe use of static graphical structures for extracting the most significant\ndependence interrelationships between the variables of interest. Recently,\nBayesian nonparametric techniques have become popular for modelling complex\nphenomena in a flexible and efficient manner, but only few attempts have been\nmade in econometrics. In this paper, we provide an innovative Bayesian\nnonparametric (BNP) time-varying graphical framework for making inference in\nhigh-dimensional time series. We include a Bayesian nonparametric dependent\nprior specification on the matrix of coefficients and the covariance matrix by\nmean of a Time-Series DPP as in Nieto-Barajas et al. (2012). Following Billio\net al. (2019), our hierarchical prior overcomes over-parametrization and\nover-fitting issues by clustering the vector autoregressive (VAR) coefficients\ninto groups and by shrinking the coefficients of each group toward a common\nlocation. Our BNP timevarying VAR model is based on a spike-and-slab\nconstruction coupled with dependent Dirichlet Process prior (DPP) and allows\nto: (i) infer time-varying Granger causality networks from time series; (ii)\nflexibly model and cluster non-zero time-varying coefficients; (iii)\naccommodate for potential non-linearities. In order to assess the performance\nof the model, we study the merits of our approach by considering a well-known\nmacroeconomic dataset. Moreover, we check the robustness of the method by\ncomparing two alternative specifications, with Dirac and diffuse spike prior\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 09:21:21 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Iacopini", "Matteo", ""], ["Rossini", "Luca", ""]]}, {"id": "1906.02183", "submitter": "Andrius Buteikis", "authors": "Andrius Buteikis, Remigijus Leipus", "title": "A copula-based bivariate integer-valued autoregressive process with\n  application", "comments": "Published at https://doi.org/10.15559/19-VMSTA130 in the Modern\n  Stochastics: Theory and Applications (https://vmsta.org/) by VTeX\n  (http://www.vtex.lt/)", "journal-ref": "Modern Stochastics: Theory and Applications 2019, Vol. 6, No. 2,\n  227-249", "doi": "10.15559/19-VMSTA130", "report-no": "VTeX-VMSTA-VMSTA130", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bivariate integer-valued autoregressive process of order 1 (BINAR(1)) with\ncopula-joint innovations is studied. Different parameter estimation methods are\nanalyzed and compared via Monte Carlo simulations with emphasis on estimation\nof the copula dependence parameter. An empirical application on defaulted and\nnon-defaulted loan data is carried out using different combinations of copula\nfunctions and marginal distribution functions covering the cases where both\nmarginal distributions are from the same family, as well as the case where they\nare from different distribution families.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 10:05:47 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Buteikis", "Andrius", ""], ["Leipus", "Remigijus", ""]]}, {"id": "1906.02196", "submitter": "Jos\\'e Gonz\\'alez-Barrios Dr.", "authors": "Jos\\'e M. Gonz\\'alez-Barrios, Eduardo Guti\\'errez-Pe\\~na, Juan D.\n  Nieves and Ra\\'ul Rueda", "title": "A novel characterization and new simple tests of multivariate\n  independence using copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is twofold. First, we provide a novel\ncharacterization of independence of random vectors based on the checkerboard\napproximation to a multivariate copula. Using this result, we then propose a\nnew family of tests of multivariate independence for continuous random vectors.\nThe tests rely on estimating the checkerboard approximation by means of the\nsample copula recently introduced in [9] and improved in [11]. Such estimators\nhave nice properties, including a Glivenko-Cantelli-type theorem that\nguarantees almost-sure uniform convergence to the checkerboard approximation.\nEach of our test statistics is defined in terms of one of a number of different\nmetrics, including the supremum, total variation and Hellinger distances, as\nwell as the Kullback-Leibler divergence. All of these tests can be easily\nimplemented since the corresponding test statistics can be efficiently\nsimulated under any alternative hypothesis, even for moderate and large sample\nsizes in relatively large dimensions. Finally, we assess the performance of our\ntests by means of a simulation study and provide one real data example.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 17:54:47 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Gonz\u00e1lez-Barrios", "Jos\u00e9 M.", ""], ["Guti\u00e9rrez-Pe\u00f1a", "Eduardo", ""], ["Nieves", "Juan D.", ""], ["Rueda", "Ra\u00fal", ""]]}, {"id": "1906.02265", "submitter": "Ruizhi Zhang", "authors": "Ruizhi Zhang, Yajun Mei, Jianjun Shi", "title": "Robust real-time monitoring of high-dimensional data streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust real-time monitoring of high-dimensional data streams has many\nimportant real-world applications such as industrial quality control, signal\ndetection, biosurveillance, but unfortunately it is highly non-trivial to\ndevelop efficient schemes due to two challenges: (1) the unknown sparse number\nor subset of affected data streams and (2) the uncertainty of model\nspecification for high-dimensional data. In this article, motivated by the\ndetection of smaller persistent changes in the presence of larger transient\noutliers, we develop a family of efficient real-time robust detection schemes\nfor high-dimensional data streams through monitoring feature spaces such as PCA\nor wavelet coefficients when the feature coefficients are from Tukey-Huber's\ngross error models with outliers. We propose to construct a new local detection\nstatistic for each feature called $L_{\\alpha}$-CUSUM statistic that can reduce\nthe effect of outliers by using the Box-Cox transformation of the likelihood\nfunction, and then raise a global alarm based upon the sum of the\nsoft-thresholding transformation of these local $L_{\\alpha}$-CUSUM statistics\nso that to filter out unaffected features. In addition, we propose a new\nconcept called false alarm breakdown point to measure the robustness of online\nmonitoring schemes, and also characterize the breakdown point of our proposed\nschemes. Asymptotic analysis, extensive numerical simulations and case study of\nnonlinear profile monitoring are conducted to illustrate the robustness and\nusefulness of our proposed schemes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 19:24:11 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Zhang", "Ruizhi", ""], ["Mei", "Yajun", ""], ["Shi", "Jianjun", ""]]}, {"id": "1906.02269", "submitter": "Mark Meyer", "authors": "Mark J. Meyer, Elizabeth J. Malloy, and Brent A. Coull", "title": "Bayesian Wavelet-packet Historical Functional Linear Models", "comments": "Submitted for publication in JCGS", "journal-ref": null, "doi": "10.1007/s11222-020-09981-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical Functional Linear Models (HFLM) quantify associations between a\nfunctional predictor and functional outcome where the predictor is an exposure\nvariable that occurs before, or at least concurrently with, the outcome.\nCurrent work on the HFLM is largely limited to frequentist estimation\ntechniques that employ spline-based basis representations. In this work, we\npropose a novel use of the discrete wavelet-packet transformation, which has\nnot previously been used in functional models, to estimate historical\nrelationships in a fully Bayesian model. Since inference has not been an\nemphasis of the existing work on HFLMs, we also employ two established Bayesian\ninference procedures in this historical functional setting. We investigate the\noperating characteristics of our wavelet-packet HFLM, as well as the two\ninference procedures, in simulation and use the model to analyze data on the\nimpact of lagged exposure to particulate matter finer than 2.5$\\mu$g on heart\nrate variability in a cohort of journeyman boilermakers over the course of a\nday's shift.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 19:33:00 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Meyer", "Mark J.", ""], ["Malloy", "Elizabeth J.", ""], ["Coull", "Brent A.", ""]]}, {"id": "1906.02840", "submitter": "Andrew Zammit-Mangion", "authors": "Andrew Zammit-Mangion, Tin Lok James Ng, Quan Vu and Maurizio\n  Filippone", "title": "Deep Compositional Spatial Models", "comments": "46 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial processes with nonstationary and anisotropic covariance structure are\noften used when modelling, analysing and predicting complex environmental\nphenomena. Such processes may often be expressed as ones that have stationary\nand isotropic covariance structure on a warped spatial domain. However, the\nwarping function is generally difficult to fit and not constrained to be\ninjective, often resulting in `space-folding.' Here, we propose modelling an\ninjective warping function through a composition of multiple elemental\ninjective functions in a deep-learning framework. We consider two cases; first,\nwhen these functions are known up to some weights that need to be estimated,\nand, second, when the weights in each layer are random. Inspired by recent\nmethodological and technological advances in deep learning and deep Gaussian\nprocesses, we employ approximate Bayesian methods to make inference with these\nmodels using graphics processing units. Through simulation studies in one and\ntwo dimensions we show that the deep compositional spatial models are quick to\nfit, and are able to provide better predictions and uncertainty quantification\nthan other deep stochastic models of similar complexity. We also show their\nremarkable capacity to model nonstationary, anisotropic spatial data using\nradiances from the MODIS instrument aboard the Aqua satellite.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 23:31:18 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 16:05:52 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Zammit-Mangion", "Andrew", ""], ["Ng", "Tin Lok James", ""], ["Vu", "Quan", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1906.02881", "submitter": "Hayden Helm", "authors": "Hayden Helm, Joshua Vogelstein, Carey Priebe", "title": "Vertex Classification on Weighted Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a discrimination technique for vertices in a weighted\nnetwork. We assume that the edge weights and adjacencies in the network are\nconditionally independent and that both sources of information encode class\nmembership information. In particular, we introduce a edge weight distribution\nmatrix to the standard K-Block Stochastic Block Model to model weighted\nnetworks. This allows us to develop simple yet powerful extensions of\nclassification techniques using the spectral embedding of the unweighted\nadjacency matrix. We consider two assumptions on the edge weight distributions\nand propose classification procedures in both settings. We show the\neffectiveness of the proposed classifiers by comparing them to quadratic\ndiscriminant analysis following the spectral embedding of a transformed\nweighted network. Moreover, we discuss and show how the methods perform when\nthe edge weights do not encode class membership information.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 03:16:08 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Helm", "Hayden", ""], ["Vogelstein", "Joshua", ""], ["Priebe", "Carey", ""]]}, {"id": "1906.02884", "submitter": "Nghia Nguyen", "authors": "Nghia Nguyen, Minh-Ngoc Tran, David Gunawan and R. Kohn", "title": "A long short-term memory stochastic volatility model", "comments": "34 pages, 14 figure, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Volatility (SV) models are widely used in the financial sector\nwhile Long Short-Term Memory (LSTM) models are successfully used in many\nlarge-scale industrial applications of Deep Learning. Our article combines\nthese two methods in a non-trivial way and proposes a model, which we call the\nLSTM-SV model, to capture the dynamics of stochastic volatility. The proposed\nmodel overcomes the short-term memory problem in conventional SV models, is\nable to capture non-linear dependence in the latent volatility process, and\noften has a better out-of-sample forecast performance than SV models. These\nproperties are illustrated through simulation study and applications to three\nfinancial time series datasets: The US stock market weekly index SP500, the\nAustralian stock weekly index ASX200 and the Australian-US dollar daily\nexchange rates. Based on our analysis, we argue that there are significant\ndifferences in the underlying dynamics between the volatility process of the\nSP500 and ASX200 datasets and that of the exchange rate dataset. For the stock\nindex data, there is strong evidence of long-term memory and non-linear\ndependence in the volatility process, while this is not the case for the\nexchange rates. An user-friendly software package together with the examples\nreported in the paper are available at https://github.com/vbayeslab.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 03:29:46 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 11:45:48 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Nguyen", "Nghia", ""], ["Tran", "Minh-Ngoc", ""], ["Gunawan", "David", ""], ["Kohn", "R.", ""]]}, {"id": "1906.02903", "submitter": "Hongji Wei", "authors": "T. Tony Cai, Hongji Wei", "title": "Transfer Learning for Nonparametric Classification: Minimax Rate and\n  Adaptive Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human learners have the natural ability to use knowledge gained in one\nsetting for learning in a different but related setting. This ability to\ntransfer knowledge from one task to another is essential for effective\nlearning. In this paper, we study transfer learning in the context of\nnonparametric classification based on observations from different distributions\nunder the posterior drift model, which is a general framework and arises in\nmany practical problems.\n  We first establish the minimax rate of convergence and construct a\nrate-optimal two-sample weighted $K$-NN classifier. The results characterize\nprecisely the contribution of the observations from the source distribution to\nthe classification task under the target distribution. A data-driven adaptive\nclassifier is then proposed and is shown to simultaneously attain within a\nlogarithmic factor of the optimal rate over a large collection of parameter\nspaces. Simulation studies and real data applications are carried out where the\nnumerical results further illustrate the theoretical analysis. Extensions to\nthe case of multiple source distributions are also considered.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 05:29:48 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Cai", "T. Tony", ""], ["Wei", "Hongji", ""]]}, {"id": "1906.03001", "submitter": "Yang-Wen Sun", "authors": "Yang-Wen Sun, Katerina Papagiannouli, Vladmir Spokoiny", "title": "Online Graph-Based Change-Point Detection for High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online change-point detection (OCPD) is important for application in various\nareas such as finance, biology, and the Internet of Things (IoT). However, OCPD\nfaces major challenges due to high-dimensionality, and it is still rarely\nstudied in literature. In this paper, we propose a novel, online, graph-based,\nchange-point detection algorithm to detect change of distribution in low- to\nhigh-dimensional data. We introduce a similarity measure, which is derived from\nthe graph-spanning ratio, to test statistically if a change occurs. Through\nnumerical study using artificial online datasets, our data-driven approach\ndemonstrates high detection power for high-dimensional data, while the false\nalarm rate (type I error) is controlled at a nominal significant level. In\nparticular, our graph-spanning approach has desirable power with small and\nmultiple scanning window, which allows timely detection of change-point in the\nonline setting.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 10:40:45 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Sun", "Yang-Wen", ""], ["Papagiannouli", "Katerina", ""], ["Spokoiny", "Vladmir", ""]]}, {"id": "1906.03044", "submitter": "Hannes Ullrich", "authors": "Michael Allan Ribers and Hannes Ullrich", "title": "Battling Antibiotic Resistance: Can Machine Learning Improve\n  Prescribing?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.CY cs.LG q-fin.EC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Antibiotic resistance constitutes a major health threat. Predicting bacterial\ncauses of infections is key to reducing antibiotic misuse, a leading driver of\nantibiotic resistance. We train a machine learning algorithm on administrative\nand microbiological laboratory data from Denmark to predict diagnostic test\noutcomes for urinary tract infections. Based on predictions, we develop\npolicies to improve prescribing in primary care, highlighting the relevance of\nphysician expertise and policy implementation when patient distributions vary\nover time. The proposed policies delay antibiotic prescriptions for some\npatients until test results are known and give them instantly to others. We\nfind that machine learning can reduce antibiotic use by 7.42 percent without\nreducing the number of treated bacterial infections. As Denmark is one of the\nmost conservative countries in terms of antibiotic use, this result is likely\nto be a lower bound of what can be achieved elsewhere.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 14:38:13 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Ribers", "Michael Allan", ""], ["Ullrich", "Hannes", ""]]}, {"id": "1906.03151", "submitter": "Nadja Klein Prof. Dr.", "authors": "Nadja Klein, Torsten Hothorn, Luisa Barbanti and Thomas Kneib", "title": "Multivariate Conditional Transformation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models describing the joint distribution of multivariate response\nvariables conditional on covariate information have become an important aspect\nof contemporary regression analysis. However, a limitation of such models is\nthat they often rely on rather simplistic assumptions, e.g. a constant\ndependency structure that is not allowed to vary with the covariates or the\nrestriction to linear dependence between the responses only. We propose a\ngeneral framework for multivariate conditional transformation models that\novercomes these limitations and describes the entire distribution in a\ntractable and interpretable yet flexible way conditional on nonlinear effects\nof covariates. The framework can be embedded into likelihood-based inference,\nincluding results on asymptotic normality, and allows the dependence structure\nto vary with covariates. In addition, the framework scales well beyond\nbivariate response situations, which were the main focus of most earlier\ninvestigations. We illustrate the application of multivariate conditional\ntransformation models in a trivariate analysis of childhood undernutrition and\ndemonstrate empirically that our approach can be beneficial compared to\nexisting benchmarks such that complex truly multivariate data-generating\nprocesses can be inferred from observations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 15:11:52 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 13:29:23 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 18:48:59 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Klein", "Nadja", ""], ["Hothorn", "Torsten", ""], ["Barbanti", "Luisa", ""], ["Kneib", "Thomas", ""]]}, {"id": "1906.03182", "submitter": "Yonggen Zhang", "authors": "Yonggen Zhang, Marcel G. Schaap, and Zhongwang Wei", "title": "Hierarchical Multimodel Ensemble Estimates of Soil Water Retention with\n  Global Coverage", "comments": null, "journal-ref": null, "doi": "10.1029/2020GL088819", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A correct quantification of mass and energy exchange processes among land\nsurface and atmosphere requires an accurate description of unsaturated soil\nhydraulic properties. Soil pedotransfer functions (PTFs) have been widely used\nto predict soil hydraulic parameters. Here, 13 PTFs were grouped according to\ninput data requirements and evaluated against a well-documented soil database\nwith global coverage. Weighted ensembles (calibrated by four groups and the\nfull 13-member set of PTFs) were shown to have improved performance over\nindividual PTFs in terms of root mean square error and other model selection\ncriteria. Global maps of soil water retention data from the ensemble models as\nwell as their uncertainty were provided. These maps demonstrate that five PTF\nensembles tend to have different estimates, especially in middle and high\nlatitudes in the Northern Hemisphere. Our full 13-member ensemble model\nprovides more accurate estimates than PTFs that are currently being used in\nearth system models.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 08:42:04 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Zhang", "Yonggen", ""], ["Schaap", "Marcel G.", ""], ["Wei", "Zhongwang", ""]]}, {"id": "1906.03187", "submitter": "Malka Gorfine", "authors": "Malka Gorfine, Nir Keret, Asaf Ben Arie, David Zucker, Li Hsu", "title": "Marginalized Frailty-Based Illness-Death Model: Application to the\n  UK-Biobank Survival Data", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2020.1831922", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The UK Biobank is a large-scale health resource comprising genetic,\nenvironmental and medical information on approximately 500,000 volunteer\nparticipants in the UK, recruited at ages 40--69 during the years 2006--2010.\nThe project monitors the health and well-being of its participants. This work\ndemonstrates how these data can be used to estimate in a semi-parametric\nfashion the effects of genetic and environmental risk factors on the hazard\nfunctions of various diseases, such as colorectal cancer. An illness-death\nmodel is adopted, which inherently is a semi-competing risks model, since death\ncan censor the disease, but not vice versa. Using a shared-frailty approach to\naccount for the dependence between time to disease diagnosis and time to death,\nwe provide a new illness-death model that assumes Cox models for the marginal\nhazard functions. The recruitment procedure used in this study introduces\ndelayed entry to the data. An additional challenge arising from the recruitment\nprocedure is that information coming from both prevalent and incident cases\nmust be aggregated. Lastly, we do not observe any deaths prior to the minimal\nrecruitment age, 40. In this work we provide an estimation procedure for our\nnew illness-death model that overcomes all the above challenges.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 22:55:58 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Gorfine", "Malka", ""], ["Keret", "Nir", ""], ["Arie", "Asaf Ben", ""], ["Zucker", "David", ""], ["Hsu", "Li", ""]]}, {"id": "1906.03222", "submitter": "Gabriel Hassler", "authors": "Gabriel Hassler, Max R. Tolkoff, William L. Allen, Lam Si Tung Ho,\n  Philippe Lemey, and Marc A. Suchard", "title": "Inferring phenotypic trait evolution on large trees with many incomplete\n  measurements", "comments": "29 pages, 7 figures, 2 tables, 3 supplementary sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparative biologists are often interested in inferring covariation between\nmultiple biological traits sampled across numerous related taxa. To properly\nstudy these relationships, we must control for the shared evolutionary history\nof the taxa to avoid spurious inference. Existing control techniques almost\nuniversally scale poorly as the number of taxa increases. An additional\nchallenge arises as obtaining a full suite of measurements becomes increasingly\ndifficult with increasing taxa. This typically necessitates data imputation or\nintegration that further exacerbates scalability. We propose an inference\ntechnique that integrates out missing measurements analytically and scales\nlinearly with the number of taxa by using a post-order traversal algorithm\nunder a multivariate Brownian diffusion (MBD) model to characterize trait\nevolution. We further exploit this technique to extend the MBD model to account\nfor sampling error or non-heritable residual variance. We test these methods to\nexamine mammalian life history traits, prokaryotic genomic and phenotypic\ntraits, and HIV infection traits. We find computational efficiency increases\nthat top two orders-of-magnitude over current best practices. While we focus on\nthe utility of this algorithm in phylogenetic comparative methods, our approach\ngeneralizes to solve long-standing challenges in computing the likelihood for\nmatrix-normal and multivariate normal distributions with missing data at scale.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 16:41:55 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Hassler", "Gabriel", ""], ["Tolkoff", "Max R.", ""], ["Allen", "William L.", ""], ["Ho", "Lam Si Tung", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1906.03224", "submitter": "Ishfaq Shah Syed", "authors": "Ishfaq Shah Ahmad, Anwar Hassan, Peer Bilal Ahmad", "title": "Negative binomial-reciprocal inverse Gaussian distribution: Statistical\n  properties with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a new three parameter distribution by compounding\nnegative binomial with reciprocal inverse Gaussian model called negative\nbinomial-reciprocal inverse Gaussian distribution. This model is tractable with\nsome important properties not only in actuarial science but in other fields as\nwell where overdispersion pattern is seen. Some basic properties including\nrecurrence relation of probabilities for computation of successive\nprobabilities have been discussed. In its compound version, when the claims are\nabsolutely continuous, an integral equation for the probability density\nfunction is discussed. Brief discussion about extension of univariate version\nhave also been done to its respective multivariate version. parameters involved\nin the model have been estimated by Maximum Likelihood Estimation technique.\nApplications of the proposed distribution are carried out by taking two real\ncount data sets. The result shown that the negative binomial-reciprocal inverse\nGaussian distribution gives better fit when compared to the Poisson and\nnegative binomial distributions.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 04:44:22 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Ahmad", "Ishfaq Shah", ""], ["Hassan", "Anwar", ""], ["Ahmad", "Peer Bilal", ""]]}, {"id": "1906.03225", "submitter": "Tobias Kley", "authors": "Josua G\\\"osmann, Tobias Kley, Holger Dette", "title": "A new approach for open-end sequential change point monitoring", "comments": "31 pages, 5 figures; online appendix (20 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new sequential monitoring scheme for changes in the parameters\nof a multivariate time series. In contrast to procedures proposed in the\nliterature which compare an estimator from the training sample with an\nestimator calculated from the remaining data, we suggest to divide the sample\nat each time point after the training sample. Estimators from the sample before\nand after all separation points are then continuously compared calculating a\nmaximum of norms of their differences. For open-end scenarios our approach\nyields an asymptotic level $\\alpha$ procedure, which is consistent under the\nalternative of a change in the parameter. By means of a simulation study it is\ndemonstrated that the new method outperforms the commonly used procedures with\nrespect to power and the feasibility of our approach is illustrated by\nanalyzing two data examples.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 13:56:13 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 17:32:58 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 14:59:30 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2020 13:46:20 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["G\u00f6smann", "Josua", ""], ["Kley", "Tobias", ""], ["Dette", "Holger", ""]]}, {"id": "1906.03320", "submitter": "Stephanie Tienshaw T", "authors": "Stephanie T. Chen, Luo Xiao, Ana-Maria Staicu", "title": "An Approximate Restricted Likelihood Ratio Test for Variance Components\n  in Generalized Linear Mixed Models", "comments": "19 pages, 1 figure, Supplementary Materials (additional simulation\n  results) excluded for brevity", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear mixed models (GLMMs) are used to model responses from\nexponential families with a combination of fixed and random effects. For\nvariance components in GLMMs, we propose an approximate restricted likelihood\nratio test that conducts testing on the working responses used in penalized\nquasi-likelihood estimation. This presents the hypothesis test in terms of\nnormalized responses, allowing for application of existing testing methods for\nlinear mixed models. Our test is flexible, computationally efficient, and\noutperforms several competitors. We illustrate the utility of the proposed\nmethod with an extensive simulation study and two data applications. An R\npackage is provided.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 20:23:59 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Chen", "Stephanie T.", ""], ["Xiao", "Luo", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "1906.03336", "submitter": "Kayla Frisoli", "authors": "Xiao Hui Tai, Kayla Frisoli", "title": "Benchmarking Minimax Linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimax linkage was first introduced by Ao et al. [3] in 2004, as an\nalternative to standard linkage methods used in hierarchical clustering.\nMinimax linkage relies on distances to a prototype for each cluster; this\nprototype can be thought of as a representative object in the cluster, hence\nimproving the interpretability of clustering results. Bien and Tibshirani\nanalyzed properties of this method in 2011 [2], popularizing the method within\nthe statistics community. Additionally, they performed comparisons of minimax\nlinkage to standard linkage methods, making use of five data sets and two\ndifferent evaluation metrics (distance to prototype and misclassification\nrate). In an effort to expand upon their work and evaluate minimax linkage more\ncomprehensively, our benchmark study focuses on thorough method evaluation via\nmultiple performance metrics on several well-described data sets. We also make\nall code and data publicly available through an R package, for full\nreproducibility. Similarly to [2], we find that minimax linkage often produces\nthe smallest maximum minimax radius of all linkage methods, meaning that\nminimax linkage produces clusters where objects in a cluster are tightly\nclustered around their prototype. This is true across a range of values for the\ntotal number of clusters (k). However, this is not always the case, and special\nattention should be paid to the case when k is the true known value. For true\nk, minimax linkage does not always perform the best in terms of all the\nevaluation metrics studied, including maximum minimax radius. This paper was\nmotivated by the IFCS Cluster Benchmarking Task Force's call for clustering\nbenchmark studies and the white paper [5], which put forth guidelines and\nprinciples for comprehensive benchmarking in clustering. Our work is designed\nto be a neutral benchmark study of minimax linkage.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 21:25:12 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Tai", "Xiao Hui", ""], ["Frisoli", "Kayla", ""]]}, {"id": "1906.03401", "submitter": "Preetam Nandy", "authors": "Kinjal Basu and Preetam Nandy", "title": "Optimal Convergence for Stochastic Optimization with Multiple\n  Expectation Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the problem of stochastic optimization where the\nobjective function can be written as an expectation function over a closed\nconvex set. We also consider multiple expectation constraints which restrict\nthe domain of the problem. We extend the cooperative stochastic approximation\nalgorithm from Lan and Zhou [2016] to solve the particular problem. We close\nthe gaps in the previous analysis and provide a novel proof technique to show\nthat our algorithm attains the optimal rate of convergence for both optimality\ngap and constraint violation when the functions are generally convex. We also\ncompare our algorithm empirically to the state-of-the-art and show improved\nconvergence in many situations.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 06:56:39 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2019 17:08:45 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Basu", "Kinjal", ""], ["Nandy", "Preetam", ""]]}, {"id": "1906.03564", "submitter": "Roberto Rivera", "authors": "Roberto Rivera", "title": "A Low Rank Gaussian Process Prediction Model for Very Large Datasets", "comments": null, "journal-ref": "In 2015 IEEE First International Conference on Big Data Computing\n  Service and Applications (pp. 308-313). IEEE", "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial prediction requires expensive computation to invert the spatial\ncovariance matrix it depends on and also has considerable storage needs. This\nwork concentrates on computationally efficient algorithms for prediction using\nvery large datasets. A recent prediction model for spatial data known as Fixed\nRank Kriging is much faster than the kriging and can be easily implemented with\nless assumptions about the process. However, Fixed Rank Kriging requires the\nestimation of a matrix which must be positive definite and the original\nestimation procedure cannot guarantee this property. We present a result that\nshows when a matrix subtraction of a given form will give a positive definite\nmatrix. Motivated by this result, we present an iterative Fixed Rank Kriging\nalgorithm that ensures positive definiteness of the matrix required for\nprediction and show that under mild conditions the algorithm numerically\nconverges. The modified Fixed Rank Kriging procedure is implemented to predict\nmissing chlorophyll observations for very large regions of ocean color.\nPredictions are compared to those made by other well known methods of spatial\nprediction.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 04:37:53 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Rivera", "Roberto", ""]]}, {"id": "1906.03661", "submitter": "Jes\\'us Daniel Arroyo Reli\\'on", "authors": "Junhao Xiong, Cencheng Shen, Jes\\\"us Arroyo, Joshua T. Vogelstein", "title": "Graph Independence Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying statistically significant dependency between variables is a key\nstep in scientific discoveries. Many recent methods, such as distance and\nkernel tests, have been proposed for valid and consistent independence testing\nand can be applied to data in Euclidean and non-Euclidean spaces. However, in\nthose works, $n$ pairs of points in $\\mathcal{X} \\times \\mathcal{Y}$ are\nobserved. Here, we consider the setting where a pair of $n \\times n$ graphs are\nobserved, and the corresponding adjacency matrices are treated as kernel\nmatrices. Under a $\\rho$-correlated stochastic block model, we demonstrate that\na na\\\"ive test (permutation and Pearson's) for a conditional dependency graph\nmodel is invalid. Instead, we propose a block-permutation procedure. We prove\nthat our procedure is valid and consistent -- even when the two graphs have\ndifferent marginal distributions, are weighted or unweighted, and the latent\nvertex assignments are unknown -- and provide sufficient conditions for the\ntests to estimate $\\rho$. Simulations corroborate these results on both binary\nand weighted graphs. Applying these tests to the whole-organism,\nsingle-cell-resolution structural connectomes of C. elegans, we identify strong\nstatistical dependency between the chemical synapse connectome and the gap\njunction connectome.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 15:40:23 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 14:51:44 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Xiong", "Junhao", ""], ["Shen", "Cencheng", ""], ["Arroyo", "Jes\u00fcs", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1906.03694", "submitter": "Arjun Sondhi", "authors": "Arjun Sondhi, David Arbour, Drew Dimmery", "title": "Balanced off-policy evaluation in general action spaces", "comments": "Accepted to AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of importance sampling weights for off-policy evaluation of\ncontextual bandits often results in imbalance - a mismatch between the desired\nand the actual distribution of state-action pairs after weighting. In this work\nwe present balanced off-policy evaluation (B-OPE), a generic method for\nestimating weights which minimize this imbalance. Estimation of these weights\nreduces to a binary classification problem regardless of action type. We show\nthat minimizing the risk of the classifier implies minimization of imbalance to\nthe desired counterfactual distribution of state-action pairs. The classifier\nloss is tied to the error of the off-policy estimate, allowing for easy tuning\nof hyperparameters. We provide experimental evidence that B-OPE improves\nweighting-based approaches for offline policy evaluation in both discrete and\ncontinuous action spaces.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 19:25:17 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 15:51:01 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 16:28:49 GMT"}, {"version": "v4", "created": "Thu, 5 Mar 2020 04:33:49 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Sondhi", "Arjun", ""], ["Arbour", "David", ""], ["Dimmery", "Drew", ""]]}, {"id": "1906.03714", "submitter": "Roberto Rivera", "authors": "Roberto Rivera and Wolfgang Rolke", "title": "Modeling Excess Deaths After a Natural Disaster with Application to\n  Hurricane Maria", "comments": "(accepted)", "journal-ref": "Statistics in Medicine 2019", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of excess deaths due to a natural disaster is an important public\nhealth problem. The CDC provides guidelines to fill death certificates to help\ndetermine the death toll of such events. But, even when followed by medical\nexaminers, the guidelines can not guarantee a precise calculation of excess\ndeaths.%particularly due to the ambiguity of indirect deaths. We propose two\nmodels to estimate excess deaths due to an emergency. The first model is\nsimple, permitting excess death estimation with little data through a profile\nlikelihood method. The second model is more flexible, incorporating: temporal\nvariation, covariates, and possible population displacement; while allowing\ninference on how the emergency's effect changes with time. The models are\nimplemented to build confidence intervals estimating Hurricane Maria's death\ntoll.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 21:28:01 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Rivera", "Roberto", ""], ["Rolke", "Wolfgang", ""]]}, {"id": "1906.03722", "submitter": "Eric Lock", "authors": "Jun Young Park and Eric F. Lock", "title": "Integrative Factorization of Bidimensionally Linked Matrices", "comments": "27 pages, 4 figures", "journal-ref": "Biometrics, 2019", "doi": "10.1111/biom.13141", "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in molecular \"omics'\" technologies have motivated new methodology\nfor the integration of multiple sources of high-content biomedical data.\nHowever, most statistical methods for integrating multiple data matrices only\nconsider data shared vertically (one cohort on multiple platforms) or\nhorizontally (different cohorts on a single platform). This is limiting for\ndata that take the form of bidimensionally linked matrices (e.g., multiple\ncohorts measured on multiple platforms), which are increasingly common in\nlarge-scale biomedical studies. In this paper, we propose BIDIFAC\n(Bidimensional Integrative Factorization) for integrative dimension reduction\nand signal approximation of bidimensionally linked data matrices. Our method\nfactorizes the data into (i) globally shared, (ii) row-shared, (iii)\ncolumn-shared, and (iv) single-matrix structural components, facilitating the\ninvestigation of shared and unique patterns of variability. For estimation we\nuse a penalized objective function that extends the nuclear norm penalization\nfor a single matrix. As an alternative to the complicated rank selection\nproblem, we use results from random matrix theory to choose tuning parameters.\nWe apply our method to integrate two genomics platforms (mRNA and miRNA\nexpression) across two sample cohorts (tumor samples and normal tissue samples)\nusing the breast cancer data from TCGA. We provide R code for fitting BIDIFAC,\nimputing missing values, and generating simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 21:50:19 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Park", "Jun Young", ""], ["Lock", "Eric F.", ""]]}, {"id": "1906.03772", "submitter": "Pengfei Zhang", "authors": "Pengfei Zhang and Gareth W. Peters and Ido Nevat and Keng Boon Teo and\n  Yixin Wang", "title": "Multimodal Data Fusion of Non-Gaussian Spatial Fields in Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a robust data fusion algorithm for field reconstruction of\nmultiple physical phenomena. The contribution of this paper is twofold: First,\nwe demonstrate how multi-spatial fields which can have any marginal\ndistributions and exhibit complex dependence structures can be constructed. To\nthis end we develop a model where a latent process of these physical phenomena\nis modelled as Multiple Gaussian Process (MGP), and the dependence structure\nbetween these phenomena is captured through a Copula process. This model has\nthe advantage of allowing one to choose any marginal distributions for the\nphysical phenomenon. Second, we develop an efficient and robust linear\nestimation algorithm to predict the mean behaviour of the physical phenomena\nusing rank correlation instead of the conventional linear Pearson correlation.\nOur approach has the advantage of avoiding the need to derive intractable\npredictive posterior distribution and also has a tractable solution for the\nrank correlation values. We show that our model outperforms the model which\nuses the conventional linear Pearson correlation metric in terms of the\nprediction mean-squared-errors (MSE). This provides the motivation for using\nour models for multimodal data fusion.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 02:39:56 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Zhang", "Pengfei", ""], ["Peters", "Gareth W.", ""], ["Nevat", "Ido", ""], ["Teo", "Keng Boon", ""], ["Wang", "Yixin", ""]]}, {"id": "1906.03807", "submitter": "Miaoyan Wang", "authors": "Miaoyan Wang and Yuchen Zeng", "title": "Multiway clustering via tensor block models", "comments": "add the supplements", "journal-ref": "Advances in Neural Information Processing Systems 32 (NeurIPS\n  2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of identifying multiway block structure from a large\nnoisy tensor. Such problems arise frequently in applications such as genomics,\nrecommendation system, topic modeling, and sensor network localization. We\npropose a tensor block model, develop a unified least-square estimation, and\nobtain the theoretical accuracy guarantees for multiway clustering. The\nstatistical convergence of the estimator is established, and we show that the\nassociated clustering procedure achieves partition consistency. A sparse\nregularization is further developed for identifying important blocks with\nelevated means. The proposal handles a broad range of data types, including\nbinary, continuous, and hybrid observations. Through simulation and application\nto two real datasets, we demonstrate the outperformance of our approach over\nprevious methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 06:07:41 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 22:10:42 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2019 06:14:01 GMT"}, {"version": "v4", "created": "Sat, 2 Jan 2021 23:25:41 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Miaoyan", ""], ["Zeng", "Yuchen", ""]]}, {"id": "1906.03851", "submitter": "Gerhard Tutz", "authors": "Gerhard Tutz", "title": "On the Structure of Ordered Latent Trait Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordered item response models that are in common use can be divided into three\ngroups, cumulative, sequential and adjacent categories model. The derivation\nand motivation of the models is typically based on the assumed presence of\nlatent traits or underlying process models. In the construction frequently\nbinary models play an important role. The objective of this paper is to give\nmotivations for the models and to clarify the role of the binary models for the\nvarious types of ordinal models. It is investigated which binary models are\nincluded in an ordinal model but also how the models can be constructed from a\nsequence of binary models. In all the models one finds a Guttman space\nstructure, which has previously been investigated in particular for the partial\ncredit model. The consideration of the binary models adds to the interpretation\nof model parameters, which is helpful, in particular, in the case of the\npartial credit model, for which interpretation is less straightforward than for\nthe other models. A specific topic that is addressed is the ordering of\nthresholds in the partial credit model because for some researchers reversed\nordering is an anomaly, others disagree. It is argued that the ordering of\nthresholds is not a constitutive element of the partial credit model.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 09:09:13 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Tutz", "Gerhard", ""]]}, {"id": "1906.04072", "submitter": "Wesley Tansey", "authors": "Wesley Tansey, Christopher Tosh, David M. Blei", "title": "A Bayesian Model of Dose-Response for Cancer Drug Studies", "comments": "Extended to handle covariates; additional benchmarks comparing to\n  related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploratory cancer drug studies test multiple tumor cell lines against\nmultiple candidate drugs. The goal in each paired (cell line, drug) experiment\nis to map out the dose-response curve of the cell line as the dose level of the\ndrug increases. We propose Bayesian Tensor Filtering (BTF), a hierarchical\nBayesian model for dose-response modeling in multi-sample, multi-treatment\ncancer drug studies. BTF uses low-dimensional embeddings to share statistical\nstrength between similar drugs and similar cell lines. Structured shrinkage\npriors in BTF encourage smoothness in the dose-response curves while remaining\nadaptive to sharp jumps when the data call for it. We focus on a pair of cancer\ndrug studies exhibiting a particular pathology in their experimental design,\nleading us to a non-conjugate monotone mixture-of-Gammas likelihood. To perform\nposterior inference, we develop a variant of the elliptical slice sampling\nalgorithm for sampling from linearly-constrained multivariate normal priors\nwith non-conjugate likelihoods. In benchmarks, BTF outperforms state-of-the-art\nmethods for covariance regression and dynamic Poisson matrix factorization. On\nthe two cancer drug studies, BTF outperforms the current standard approach in\nbiology and reveals potential new biomarkers of drug sensitivity in cancer.\nCode is available at https://github.com/tansey/functionalmf.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 15:26:39 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 15:34:45 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 16:34:11 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tansey", "Wesley", ""], ["Tosh", "Christopher", ""], ["Blei", "David M.", ""]]}, {"id": "1906.04175", "submitter": "Mariusz Kubkowski", "authors": "Mariusz Kubkowski, Jan Mielniczuk", "title": "Selection consistency of Lasso-based procedures for misspecified\n  high-dimensional binary model and random regressors", "comments": null, "journal-ref": null, "doi": "10.3390/e22020153", "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider selection of random predictors for high-dimensional regression\nproblem with binary response for a general loss function. Important special\ncase is when the binary model is semiparametric and the response function is\nmisspecified under parametric model fit. Selection for such a scenario aims at\nrecovering the support of the minimizer of the associated risk with large\nprobability. We propose a two-step selection procedure which consists of\nscreening and ordering predictors by Lasso method and then selecting a subset\nof predictors which minimizes Generalized Information Criterion on the\ncorresponding nested family of models. We prove consistency of the selection\nmethod under conditions which allow for much larger number of predictors than\nnumber of observations. For the semiparametric case when distribution of random\npredictors satisfies linear regression conditions the true and the estimated\nparameters are collinear and their common support can be consistently\nidentified.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 15:25:01 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Kubkowski", "Mariusz", ""], ["Mielniczuk", "Jan", ""]]}, {"id": "1906.04222", "submitter": "Alejandra Estefan\\'ia Pati\\~no Hoyos", "authors": "Alejandra Estefan\\'ia Pati\\~no Hoyos and Victor Fossaluza", "title": "Adaptative significance levels in linear regression models with known\n  variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Full Bayesian Significance Test (FBST) for precise hypotheses was\npresented by Pereira and Stern [Entropy 1(4) (1999) 99-110] as a Bayesian\nalternative instead of the traditional significance test using p-value. The\nFBST is based on the evidence in favor of the null hypothesis (H). An important\npractical issue for the implementation of the FBST is the determination of how\nlarge the evidence must be in order to decide for its rejection. In the\nClassical significance tests, it is known that p-value decreases as sample size\nincreases, so by setting a single significance level, it usually leads H\nrejection. In the FBST procedure, the evidence in favor of H exhibits the same\nbehavior as the p-value when the sample size increases. This suggests that the\ncut-off point to define the rejection of H in the FBST should be a sample size\nfunction. In this work, the scenario of Linear Regression Models with known\nvariance under the Bayesian approach is considered, and a method to find a\ncut-off value for the evidence in the FBST is presented by minimizing the\nlinear combination of the averaged type I and type II error probabilities for a\ngiven sample size and also for a given dimension of the parametric space.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 18:38:39 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Hoyos", "Alejandra Estefan\u00eda Pati\u00f1o", ""], ["Fossaluza", "Victor", ""]]}, {"id": "1906.04242", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Rocio Titiunik, Gonzalo Vazquez-Bare", "title": "The Regression Discontinuity Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This handbook chapter gives an introduction to the sharp regression\ndiscontinuity design, covering identification, estimation, inference, and\nfalsification methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 19:20:25 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 17:22:12 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Titiunik", "Rocio", ""], ["Vazquez-Bare", "Gonzalo", ""]]}, {"id": "1906.04322", "submitter": "Jean-Fran\\c{c}ois B\\'egin", "authors": "Jean-Fran\\c{c}ois B\\'egin and Mathieu Boudreault", "title": "Likelihood Evaluation of Jump-Diffusion Models Using Deterministic\n  Nonlinear Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we develop a deterministic nonlinear filtering algorithm based\non a high-dimensional version of Kitagawa (1987) to evaluate the likelihood\nfunction of models that allow for stochastic volatility and jumps whose arrival\nintensity is also stochastic. We show numerically that the deterministic\nfiltering method is precise and much faster than the particle filter, in\naddition to yielding a smooth function over the parameter space. We then find\nthe maximum likelihood estimates of various models that include stochastic\nvolatility, jumps in the returns and variance, and also stochastic jump arrival\nintensity with the S&P 500 daily returns. During the Great Recession, the jump\narrival intensity increases significantly and contributes to the clustering of\nvolatility and negative returns.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 23:31:49 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 19:48:46 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["B\u00e9gin", "Jean-Fran\u00e7ois", ""], ["Boudreault", "Mathieu", ""]]}, {"id": "1906.04347", "submitter": "Scott Sisson", "authors": "G. S. Rodrigues and D. J. Nott and S. A. Sisson", "title": "Likelihood-free approximate Gibbs sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-free methods such as approximate Bayesian computation (ABC) have\nextended the reach of statistical inference to problems with computationally\nintractable likelihoods. Such approaches perform well for small-to-moderate\ndimensional problems, but suffer a curse of dimensionality in the number of\nmodel parameters. We introduce a likelihood-free approximate Gibbs sampler that\nnaturally circumvents the dimensionality issue by focusing on lower-dimensional\nconditional distributions. These distributions are estimated by flexible\nregression models either before the sampler is run, or adaptively during\nsampler implementation. As a result, and in comparison to Metropolis-Hastings\nbased approaches, we are able to fit substantially more challenging statistical\nmodels than would otherwise be possible. We demonstrate the sampler's\nperformance via two simulated examples, and a real analysis of Airbnb rental\nprices using a intractable high-dimensional multivariate non-linear state space\nmodel containing 13,140 parameters, which presents a real challenge to standard\nABC techniques.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 01:56:27 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Rodrigues", "G. S.", ""], ["Nott", "D. J.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1906.04396", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul, Venkata K Jandhyala, Stergios B Fotopoulos", "title": "Detection and estimation of parameters in high dimensional multiple\n  change point regression models via $\\ell_1/\\ell_0$ regularization and\n  discrete optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary segmentation, which is sequential in nature is thus far the most\nwidely used method for identifying multiple change points in statistical\nmodels. Here we propose a top down methodology called arbitrary segmentation\nthat proceeds in a conceptually reverse manner. We begin with an arbitrary\nsuperset of the parametric space of the change points, and locate unknown\nchange points by suitably filtering this space down. Critically, we reframe the\nproblem as that of variable selection in the change point parameters, this\nenables the filtering down process to be achieved in a single step with the aid\nof an $\\ell_0$ regularization, thus avoiding the sequentiality of binary\nsegmentation. We study this method under a high dimensional multiple change\npoint linear regression model and show that rates convergence of the error in\nthe regression and change point estimates are near optimal. We propose a\nsimulated annealing (SA) approach to implement a key finite state space\ndiscrete optimization that arises in our method. Theoretical results are\nnumerically supported via simulations. The proposed method is shown to possess\nthe ability to agnostically detect the `no change' scenario. Furthermore, its\ncomputational complexity is of order $O(Np^2)$+SA, where SA is the cost of a SA\noptimization on a $N$(no. of change points) dimensional grid. Thus, the\nproposed methodology is significantly more computationally efficient than\nexisting approaches. Finally, our theoretical results are obtained under weaker\nmodel conditions than those assumed in the current literature.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 05:15:24 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Kaul", "Abhishek", ""], ["Jandhyala", "Venkata K", ""], ["Fotopoulos", "Stergios B", ""]]}, {"id": "1906.04398", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Jae Kwang Kim", "title": "An Approximate Bayesian Approach to Model-assisted Survey Estimation\n  with Many Auxiliary Variables", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-assisted estimation with complex survey data is an important practical\nproblem in survey sampling. When there are many auxiliary variables, selecting\nsignificant variables associated with the study variable would be necessary to\nachieve efficient estimation of population parameters of interest. In this\npaper, we formulate a regularized regression estimator in the framework of\nBayesian inference using the penalty function as the shrinkage prior for model\nselection. The proposed Bayesian approach enables us to get not only efficient\npoint estimates but also reasonable credible intervals. Results from two\nlimited simulation studies are presented to facilitate comparison with existing\nfrequentist methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 05:35:56 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 07:12:42 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1906.04538", "submitter": "M{\\aa}ns Karlsson", "authors": "M{\\aa}ns Karlsson and Ola H\\\"ossjer", "title": "Identification of taxon through classification with partial reject\n  options", "comments": "About half are appendices, which contains mathematical details", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of taxa can significantly be assisted by statistical\nclassification based on trait measurements in two major ways; either\nindividually or by phylogenetic (clustering) methods. In this paper we present\na general Bayesian approach for classifying species individually based on\nmeasurements of a mixture of continuous and ordinal traits as well as any type\nof covariates. It is assumed that the trait vector is derived from a latent\nvariable with a multivariate Gaussian distribution. Decision rules based on\nsupervised learning are presented that estimate model parameters through\nblockwise Gibbs sampling. These decision regions allow for uncertainty (partial\nrejection), so that not necessarily one specific category (taxon) is output\nwhen new subjects are classified, but rather a set of categories including the\nmost probable taxa. This type of discriminant analysis employs reward functions\nwith a set-valued input argument, so that an optimal Bayes classifier can be\ndefined. We also present a way of safeguarding against outlying new\nobservations, using an analogue of a $p$-value within our Bayesian setting. Our\nmethod is illustrated on an original ornithological data set of birds. We also\nincorporate model selection through cross-validation, examplified on another\noriginal data set of birds.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 12:52:20 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 14:58:16 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 16:25:42 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Karlsson", "M\u00e5ns", ""], ["H\u00f6ssjer", "Ola", ""]]}, {"id": "1906.04563", "submitter": "Clifford Anderson-Bergman", "authors": "Clifford Anderson-Bergman, Phan Nguyen and Jose Cadena Pico", "title": "Latent Channel Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Euclidean embedding models a given network by representing each node\nin a Euclidean space, where the probability of two nodes sharing an edge is a\nfunction of the distances between the nodes. This implies that for two nodes to\nshare an edge with high probability, they must be relatively close in all\ndimensions. This constraint may be overly restrictive for describing modern\nnetworks, in which having similarities in at least one area may be sufficient\nfor having a high edge probability. We introduce a new model, which we call\nLatent Channel Networks, which allows for such features of a network. We\npresent an EM algorithm for fitting the model, for which the computational\ncomplexity is linear in the number of edges and number of channels and apply\nthe algorithm to both synthetic and classic network datasets.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 16:50:06 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 22:47:51 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Anderson-Bergman", "Clifford", ""], ["Nguyen", "Phan", ""], ["Pico", "Jose Cadena", ""]]}, {"id": "1906.04566", "submitter": "Marjan Cugmas", "authors": "Marjan Cugmas and Dawn DeLay and Ale\\v{s} \\v{Z}iberna and Anu\\v{s}ka\n  Ferligoj", "title": "Symmetric core-cohesive blockmodel in preschool children's interaction\n  networks", "comments": "17 pages, 6 figures, 2 tables", "journal-ref": "PLOS ONE 15(1): e0226801 (2020)", "doi": "10.1371/journal.pone.0226801", "report-no": null, "categories": "cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have extensively studied the social mechanisms that drive the\nformation of networks observed among preschool children. However, less\nattention has been given to global network structures in terms of blockmodels.\nA blockmodel is a network where the nodes are groups of equivalent units\n(according to links to others) from a studied network. Cugmas et al. (2019)\nshowed that mutuality, popularity, assortativity, and different types of\ntransitivity mechanisms can lead the global network structure to the proposed\nasymmetric core-cohesive blockmodel. Yet, they did not provide any evidence\nthat such a global network structure actually appears in any empirical data. In\nthis paper, the symmetric version of the core-cohesive blockmodel type is\nproposed. This blockmodel type consists of three or more groups of units. The\nunits from each group are internally well linked to each other while those from\ndifferent groups are not linked to each other. This is true for all groups,\nexcept one in which the units have mutual links to all other units in the\nnetwork. In this study, it is shown that the proposed blockmodel type appears\nin empirical interactional networks collected among preschool children. Monte\nCarlo simulations confirm that the most often studied social network mechanisms\ncan lead the global network structure to the proposed symmetric blockmodel\ntype. The units' attributes are not considered in this study.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 13:42:04 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Cugmas", "Marjan", ""], ["DeLay", "Dawn", ""], ["\u017diberna", "Ale\u0161", ""], ["Ferligoj", "Anu\u0161ka", ""]]}, {"id": "1906.04631", "submitter": "Christoph Rothe", "authors": "Claudia Noack, Christoph Rothe", "title": "Bias-Aware Inference in Fuzzy Regression Discontinuity Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new confidence sets (CSs) for the regression discontinuity\nparameter in fuzzy designs. Our CSs are based on nonparametric local linear\nregression, and are bias-aware, in the sense that they take possible smoothing\nbias explicitly into account. Their construction shares similarities with that\nof Anderson-Rubin CSs in exactly identified instrumental variable models, and\nthereby avoids issues with \"delta method\" approximations that underlie most\ncommonly used existing inference methods for fuzzy regression discontinuity\nanalysis. Our CSs compare favorably in terms of both theoretical and practical\nperformance to existing procedures in canonical settings with strong\nidentification and a continuous running variable. However, due to their\nparticular construction they are also valid under a wide range of empirically\nrelevant conditions in which existing methods generally fail, such as setups\nwith discrete running variables, donut designs, and weak identification.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 14:49:29 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 11:29:29 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 10:37:35 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Noack", "Claudia", ""], ["Rothe", "Christoph", ""]]}, {"id": "1906.04668", "submitter": "Fernando Alarid-Escudero", "authors": "Fernando Alarid-Escudero, Amy B. Knudsen, Jonathan Ozik, Nicholson\n  Collier, Karen M. Kuntz", "title": "Characterization and valuation of uncertainty of calibrated parameters\n  in stochastic decision models", "comments": "17 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluated the implications of different approaches to characterize\nuncertainty of calibrated parameters of stochastic decision models (DMs) in the\nquantified value of such uncertainty in decision making. We used a\nmicrosimulation DM of colorectal cancer (CRC) screening to conduct a\ncost-effectiveness analysis (CEA) of a 10-year colonoscopy screening. We\ncalibrated the natural history model of CRC to epidemiological data with\ndifferent degrees of uncertainty and obtained the joint posterior distribution\nof the parameters using a Bayesian approach. We conducted a probabilistic\nsensitivity analysis (PSA) on all the model parameters with different\ncharacterizations of uncertainty of the calibrated parameters and estimated the\nvalue of uncertainty of the different characterizations with a value of\ninformation analysis. All analyses were conducted using high performance\ncomputing resources running the Extreme-scale Model Exploration with Swift\n(EMEWS) framework. The posterior distribution had high correlation among some\nparameters. The parameters of the Weibull hazard function for the age of onset\nof adenomas had the highest posterior correlation of -0.958. Considering full\nposterior distributions and the maximum-a-posteriori estimate of the calibrated\nparameters, there is little difference on the spread of the distribution of the\nCEA outcomes with a similar expected value of perfect information (EVPI) of\n\\$653 and \\$685, respectively, at a WTP of \\$66,000/QALY. Ignoring correlation\non the posterior distribution of the calibrated parameters, produced the widest\ndistribution of CEA outcomes and the highest EVPI of \\$809 at the same WTP.\nDifferent characterizations of uncertainty of calibrated parameters have\nimplications on the expect value of reducing uncertainty on the CEA. Ignoring\ninherent correlation among calibrated parameters on a PSA overestimates the\nvalue of uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 15:47:32 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Alarid-Escudero", "Fernando", ""], ["Knudsen", "Amy B.", ""], ["Ozik", "Jonathan", ""], ["Collier", "Nicholson", ""], ["Kuntz", "Karen M.", ""]]}, {"id": "1906.04763", "submitter": "Daniel Manrique-Vallier", "authors": "Daniel Manrique-Vallier, Patrick Ball, David Sulmont", "title": "Estimating the Number of Fatal Victims of the Peruvian Internal Armed\n  Conflict, 1980-2000: an application of modern multi-list Capture-Recapture\n  techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We estimate the number of fatal victims of the Peruvian internal armed\nconflict between 1980-2000 using stratified seven-list Capture-Recapture\nmethods based on Dirichlet process mixtures, which we extend to accommodate\nincomplete stratification information. We use matched data from six sources,\noriginally analyzed by the Peruvian Truth and Reconciliation Commission in\n2003, together with a new large dataset, originally published in 2006 by the\nPeruvian government. We deal with missing stratification labels by developing a\ngeneral framework and estimation methods based on MCMC sampling for jointly\nfitting generic Bayesian Capture-Recapture models and the missing labels.\nThrough a detailed exploration driven by domain-knowledge, modeling and\nrefining, with special precautions to avoid cherry-picking of results, we\narrive to a conservative posterior estimate of 58,234 (CI95% = [56,741,\n61,289]), and a more liberal estimate of 65,958 (CI95% = [61,462, 75,387])\nfatal victims. We also determine that the Shining Path guerrillas killed more\npeople than the Peruvian armed forces. We additionally explore and discuss\nestimates based on log-linear modeling and multiple-imputation. We finish by\ndiscussing several lessons learned about the use of Capture-Recapture methods\nfor estimating casualties in conflicts.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 18:27:47 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 21:27:46 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Manrique-Vallier", "Daniel", ""], ["Ball", "Patrick", ""], ["Sulmont", "David", ""]]}, {"id": "1906.04776", "submitter": "Somabha Mukherjee", "authors": "Divyansh Agarwal, Somabha Mukherjee, Bhaswar Bikram Bhattacharya,\n  Nancy Ruonan Zhang", "title": "Distribution-Free Multisample Test Based on Optimal Matching with\n  Applications to Single Cell Genomics", "comments": "40 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a nonparametric graphical test based on optimal\nmatching, for assessing the equality of multiple unknown multivariate\nprobability distributions. Our procedure pools the data from the different\nclasses to create a graph based on the minimum non-bipartite matching, and then\nutilizes the number of edges connecting data points from different classes to\nexamine the closeness between the distributions. The proposed test is exactly\ndistribution-free (the null distribution does not depend on the distribution of\nthe data) and can be efficiently applied to multivariate as well as\nnon-Euclidean data, whenever the inter-point distances are well-defined. We\nshow that the test is universally consistent, and prove a distributional limit\ntheorem for the test statistic under general alternatives. Through simulation\nstudies, we demonstrate its superior performance against other common and\nwell-known multisample tests. In scenarios where our test suggests\ndistributional differences across classes, we also propose an approach for\nidentifying which class or group contributes to this overall difference. The\nmethod is applied to single cell transcriptomics data obtained from the\nperipheral blood, cancer tissue, and tumor-adjacent normal tissue of human\nsubjects with hepatocellular carcinoma and non-small-cell lung cancer. Our\nmethod unveils patterns in how biochemical metabolic pathways are altered\nacross immune cells in a cancer setting, depending on the tissue location. All\nof the methods described herein are implemented in the R package multicross.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 19:12:39 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Agarwal", "Divyansh", ""], ["Mukherjee", "Somabha", ""], ["Bhattacharya", "Bhaswar Bikram", ""], ["Zhang", "Nancy Ruonan", ""]]}, {"id": "1906.04812", "submitter": "Jonathan P Williams", "authors": "Jonathan P Williams and Yuying Xie and Jan Hannig", "title": "The EAS approach for graphical selection consistency in vector\n  autoregression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As evidenced by various recent and significant papers within the frequentist\nliterature, along with numerous applications in macroeconomics, genomics, and\nneuroscience, there continues to be substantial interest to understand the\ntheoretical estimation properties of high-dimensional vector autoregression\n(VAR) models. To date, however, while Bayesian VAR (BVAR) models have been\ndeveloped and studied empirically (primarily in the econometrics literature)\nthere exist very few theoretical investigations of the repeated sampling\nproperties for BVAR models in the literature. In this direction, we construct\nmethodology via the $\\varepsilon$-$admissible$ subsets (EAS) approach for\nposterior-like inference based on a generalized fiducial distribution of\nrelative model probabilities over all sets of active/inactive components\n(graphs) of the VAR transition matrix. We provide a mathematical proof of\n$pairwise$ and $strong$ graphical selection consistency for the EAS approach\nfor stable VAR(1) models which is robust to model misspecification, and\ndemonstrate numerically that it is an effective strategy in high-dimensional\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 20:47:04 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Williams", "Jonathan P", ""], ["Xie", "Yuying", ""], ["Hannig", "Jan", ""]]}, {"id": "1906.04834", "submitter": "Alexander Fisher", "authors": "Alexander A. Fisher, Xiang Ji, Philippe Lemey, Marc A. Suchard", "title": "Relaxed random walks at scale", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relaxed random walk (RRW) models of trait evolution introduce branch-specific\nrate multipliers to modulate the variance of a standard Brownian diffusion\nprocess along a phylogeny and more accurately model overdispersed biological\ndata. Increased taxonomic sampling challenges inference under RRWs as the\nnumber of unknown parameters grows with the number of taxa. To solve this\nproblem, we present a scalable method to efficiently fit RRWs and infer this\nbranch-specific variation in a Bayesian framework. We develop a Hamiltonian\nMonte Carlo (HMC) sampler to approximate the high-dimensional, correlated\nposterior that exploits a closed-form evaluation of the gradient of the trait\ndata log-likelihood with respect to all branch-rate multipliers simultaneously.\nOur gradient calculation achieves computational complexity that scales only\nlinearly with the number of taxa under study. We compare the efficiency of our\nHMC sampler to the previously standard univariable Metropolis-Hastings approach\nwhile studying the spatial emergence of the West Nile virus in North America in\nthe early 2000s. Our method achieves an over 300-fold speed-increase over the\nunivariable approach. Additionally, we demonstrate the scalability of our\nmethod by applying the RRW to study the correlation between five mammalian life\nhistory traits in a phylogenetic tree with 3650 tips.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 21:35:30 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 05:12:53 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Fisher", "Alexander A.", ""], ["Ji", "Xiang", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1906.04844", "submitter": "Yongqiang Tang", "authors": "Yongqiang Tang", "title": "A monotone data augmentation algorithm for longitudinal data analysis\n  via multivariate skew-t, skew-normal or t distributions", "comments": "2019", "journal-ref": "Statistical Methods in Medical Research 2019", "doi": "10.1177/0962280219865579", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixed effects model for repeated measures (MMRM) has been widely used for\nthe analysis of longitudinal clinical data collected at a number of fixed time\npoints. We propose a robust extension of the MMRM for skewed and heavy-tailed\ndata on basis of the multivariate skew-t distribution, and it includes the\nmultivariate normal, t, and skew-normal distributions as special cases. An\nefficient Markov chain Monte Carlo algorithm is developed using the monotone\ndata augmentation and parameter expansion techniques. We employ the algorithm\nto perform controlled pattern imputations for sensitivity analyses of\nlongitudinal clinical trials with nonignorable dropouts. The proposed methods\nare illustrated by real data analyses. Sample SAS programs for the analyses are\nprovided in the online supplementary material.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 22:15:06 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 03:07:41 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Tang", "Yongqiang", ""]]}, {"id": "1906.04889", "submitter": "Stephanie Tienshaw Chen", "authors": "Stephanie T. Chen, Luo Xiao, Ana-Maria Staicu", "title": "Model Testing for Generalized Scalar-on-Function Linear Models", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalar-on-function linear models are commonly used to regress functional\npredictors on a scalar response. However, functional models are more difficult\nto estimate and interpret than traditional linear models, and may be\nunnecessarily complex for a data application. Hypothesis testing can be used to\nguide model selection by determining if a functional predictor is necessary.\nUsing a mixed effects representation with penalized splines and variance\ncomponent tests, we propose a framework for testing functional linear models\nwith responses from exponential family distributions. The proposed method can\naccommodate dense and sparse functional data, and be used to test functional\npredictors for no effect and form of the effect. We show via simulation study\nthat the proposed method achieves the nominal level and has high power, and we\ndemonstrate its utility with two data applications.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 02:07:58 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Chen", "Stephanie T.", ""], ["Xiao", "Luo", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "1906.04992", "submitter": "Ghislaine Gayraud", "authors": "Witold Wiecek, Frederic Y. Bois, Ghislaine Gayraud", "title": "Structure learning of Bayesian networks involving cyclic structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many biological networks include cyclic structures. In such cases, Bayesian\nnetworks (BNs), which must be acyclic, are not sound models for structure\nlearning. Dynamic BNs can be used but require relatively large time series\ndata. We discuss an alternative model that embeds cyclic structures within\nacyclic BNs, allowing us to still use the factorization property and\ninformative priors on network structure. We present an implementation in the\nlinear Gaussian case, where cyclic structures are treated as multivariate\nnodes. We use a Markov Chain Monte Carlo algorithm for inference, allowing us\nto work with posterior distribution on the space of graphs.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 08:10:38 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 09:44:08 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2020 11:37:46 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Wiecek", "Witold", ""], ["Bois", "Frederic Y.", ""], ["Gayraud", "Ghislaine", ""]]}, {"id": "1906.05098", "submitter": "Xiaowei Zhang", "authors": "Liang Ding, L. Jeff Hong, Haihui Shen, Xiaowei Zhang", "title": "Knowledge Gradient for Selection with Covariates: Consistency and\n  Computation", "comments": "40 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge gradient is a design principle for developing Bayesian sequential\nsampling policies to solve optimization problems. In this paper we consider the\nranking and selection problem in the presence of covariates, where the best\nalternative is not universal but depends on the covariates. In this context, we\nprove that under minimal assumptions, the sampling policy based on knowledge\ngradient is consistent, in the sense that following the policy the best\nalternative as a function of the covariates will be identified almost surely as\nthe number of samples grows. We also propose a stochastic gradient ascent\nalgorithm for computing the sampling policy and demonstrate its performance via\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 12:53:55 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 12:44:56 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 14:13:43 GMT"}, {"version": "v4", "created": "Mon, 8 Mar 2021 06:50:40 GMT"}, {"version": "v5", "created": "Wed, 23 Jun 2021 14:35:36 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ding", "Liang", ""], ["Hong", "L. Jeff", ""], ["Shen", "Haihui", ""], ["Zhang", "Xiaowei", ""]]}, {"id": "1906.05136", "submitter": "Mandev Gill", "authors": "Guy Baele, Mandev S. Gill, Philippe Lemey, Marc A. Suchard", "title": "Markov-modulated continuous-time Markov chains to identify site- and\n  branch-specific evolutionary variation", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov models of character substitution on phylogenies form the foundation of\nphylogenetic inference frameworks. Early models made the simplifying assumption\nthat the substitution process is homogeneous over time and across sites in the\nmolecular sequence alignment. While standard practice adopts extensions that\naccommodate heterogeneity of substitution rates across sites, heterogeneity in\nthe process over time in a site-specific manner remains frequently overlooked.\nThis is problematic, as evolutionary processes that act at the molecular level\nare highly variable, subjecting different sites to different selective\nconstraints over time, impacting their substitution behaviour. We propose\nincorporating time variability through Markov-modulated models (MMMs) that\nallow the substitution process (including relative character exchange rates as\nwell as the overall substitution rate) that models the evolution at an\nindividual site to vary across lineages. We implement a general MMM framework\nin BEAST, a popular Bayesian phylogenetic inference software package, allowing\nresearchers to compose a wide range of MMMs through flexible XML specification.\nUsing examples from bacterial, viral and plastid genome evolution, we show that\nMMMs impact phylogenetic tree estimation and can substantially improve model\nfit compared to standard substitution models. Through simulations, we show that\nmarginal likelihood estimation accurately identifies the generative model and\ndoes not systematically prefer the more parameter-rich MMMs. In order to\nmitigate the increased computational demands associated with MMMs, our\nimplementation exploits recently developed updates to BEAGLE, a\nhigh-performance computational library for phylogenetic inference.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 13:46:38 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Baele", "Guy", ""], ["Gill", "Mandev S.", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1906.05150", "submitter": "Dimitris Vavoulis", "authors": "Dimitrios V Vavoulis", "title": "Exploring Bayesian approaches to eQTL mapping through probabilistic\n  programming", "comments": "25 pages, 3 figures; to appear as a book chapter in \"eQTL Analysis:\n  Methods and Protocols\", a volume for the series \"Methods in Molecular\n  Biology\" published by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The discovery of genomic polymorphisms influencing gene expression (also\nknown as expression quantitative trait loci or eQTLs) can be formulated as a\nsparse Bayesian multivariate/multiple regression problem. An important aspect\nin the development of such models is the implementation of bespoke inference\nmethodologies, a process which can become quite laborious, when multiple\ncandidate models are being considered. We describe automatic, black-box\ninference in such models using Stan, a popular probabilistic programming\nlanguage. The utilisation of systems like Stan can facilitate model prototyping\nand testing, thus accelerating the data modelling process. The code described\nin this chapter can be found at https://github.com/dvav/eQTLBookChapter.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 14:07:58 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Vavoulis", "Dimitrios V", ""]]}, {"id": "1906.05159", "submitter": "Yuhao Wang", "authors": "Yuhao Wang, Uma Roy, Caroline Uhler", "title": "Learning High-dimensional Gaussian Graphical Models under Total\n  Positivity without Adjustment of Tuning Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating an undirected Gaussian graphical model\nwhen the underlying distribution is multivariate totally positive of order 2\n(MTP2), a strong form of positive dependence. Such distributions are relevant\nfor example for portfolio selection, since assets are usually positively\ndependent. A large body of methods have been proposed for learning undirected\ngraphical models without the MTP2 constraint. A major limitation of these\nmethods is that their structure recovery guarantees in the high-dimensional\nsetting usually require a particular choice of a tuning parameter, which is\nunknown a priori in real world applications. We here propose a new method to\nestimate the underlying undirected graphical model under MTP2 and show that it\nis provably consistent in structure recovery without adjusting the tuning\nparameters. This is achieved by a constraint-based estimator that infers the\nstructure of the underlying graphical model by testing the signs of the\nempirical partial correlation coefficients. We evaluate the performance of our\nestimator in simulations and on financial data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 14:17:21 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 21:44:02 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 14:01:05 GMT"}, {"version": "v4", "created": "Fri, 20 Mar 2020 00:46:41 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Wang", "Yuhao", ""], ["Roy", "Uma", ""], ["Uhler", "Caroline", ""]]}, {"id": "1906.05232", "submitter": "Mehdi Maadooliat", "authors": "Hossein Haghbin, Seyed Morteza Najibi, Rahim Mahmoudvand, Jordan\n  Trinka, and Mehdi Maadooliat", "title": "Functional Singular Spectrum Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new extension of the Singular Spectrum Analysis\n(SSA) called functional SSA to analyze functional time series. The new\nmethodology is developed by integrating ideas from functional data analysis and\nunivariate SSA. We explore the advantages of the functional SSA in terms of\nsimulation results and two real data applications. We compare the proposed\napproach with Multivariate SSA (MSSA) and dynamic Functional Principal\nComponent Analysis (dFPCA). The results suggest that further improvement to\nMSSA is possible, and the new method provides an attractive alternative to the\ndFPCA approach that is used for analyzing correlated functions. We implement\nthe proposed technique to an application of remote sensing data and a call\ncenter dataset. We have also developed an efficient and user-friendly R package\nand a shiny web application to allow interactive exploration of the results.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 16:07:54 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 12:38:11 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Haghbin", "Hossein", ""], ["Najibi", "Seyed Morteza", ""], ["Mahmoudvand", "Rahim", ""], ["Trinka", "Jordan", ""], ["Maadooliat", "Mehdi", ""]]}, {"id": "1906.05244", "submitter": "Neil A. Spencer", "authors": "Neil A. Spencer and Jared S. Murray", "title": "A Bayesian Hierarchical Model for Evaluating Forensic Footwear Evidence", "comments": "Accepted for publication at the Annals of Applied Statistics. Code\n  available at http://github.com/neilspencer/cindRella", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a latent shoeprint is discovered at a crime scene, forensic analysts\ninspect it for distinctive patterns of wear such as scratches and holes (known\nas accidentals) on the source shoe's sole. If its accidentals correspond to\nthose of a suspect's shoe, the print can be used as forensic evidence to place\nthe suspect at the crime scene. The strength of this evidence depends on the\nrandom match probability---the chance that a shoe chosen at random would match\nthe crime scene print's accidentals. Evaluating random match probabilities\nrequires an accurate model for the spatial distribution of accidentals on shoe\nsoles. A recent report by the President's Council of Advisors in Science and\nTechnology criticized existing models in the literature, calling for new\nempirically validated techniques. We respond to this request with a new spatial\npoint process model for accidental locations, developed within a hierarchical\nBayesian framework. We treat the tread pattern of each shoe as a covariate,\nallowing us to pool information across large heterogeneous databases of shoes.\nExisting models ignore this information; our results show that including it\nleads to significantly better model fit. We demonstrate this by fitting our\nmodel to one such database.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 16:57:13 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 19:27:08 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Spencer", "Neil A.", ""], ["Murray", "Jared S.", ""]]}, {"id": "1906.05349", "submitter": "Ryan Martin", "authors": "Vaidehi Dixit and Ryan Martin", "title": "Permutation-based uncertainty quantification about a mixing distribution", "comments": "10 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric estimation of a mixing distribution based on data coming from a\nmixture model is a challenging problem. Beyond estimation, there is interest in\nuncertainty quantification, e.g., confidence intervals for features of the\nmixing distribution. This paper focuses on estimation via the predictive\nrecursion algorithm, and here we take advantage of this estimator's seemingly\nundesirable dependence on the data ordering to obtain a permutation-based\napproximation of the sampling distribution which can be used to quantify\nuncertainty. Theoretical and numerical results confirm that the proposed method\nleads to valid confidence intervals, at least approximately.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 19:42:17 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Dixit", "Vaidehi", ""], ["Martin", "Ryan", ""]]}, {"id": "1906.05358", "submitter": "You-Lin Chen", "authors": "You-Lin Chen and Mladen Kolar and Ruey S. Tsay", "title": "Tensor Canonical Correlation Analysis with Convergence and Statistical\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, such as classification of images or videos, it is of\ninterest to develop a framework for tensor data instead of an ad-hoc way of\ntransforming data to vectors due to the computational and under-sampling\nissues. In this paper, we study convergence and statistical properties of\ntwo-dimensional canonical correlation analysis \\citep{Lee2007Two} under an\nassumption that data come from a probabilistic model. We show that carefully\ninitialized the power method converges to the optimum and provide a finite\nsample bound. Then we extend this framework to tensor-valued data and propose\nthe higher-order power method, which is commonly used in tensor decomposition,\nto extract the canonical directions. Our method can be used effectively in a\nlarge-scale data setting by solving the inner least squares problem with a\nstochastic gradient descent, and we justify convergence via the theory of\nLojasiewicz's inequalities without any assumption on data generating process\nand initialization. For practical applications, we further develop (a) an\ninexact updating scheme which allows us to use the state-of-the-art stochastic\ngradient descent algorithm, (b) an effective initialization scheme which\nalleviates the problem of local optimum in non-convex optimization, and (c) a\ndeflation procedure for extracting several canonical components. Empirical\nanalyses on challenging data including gene expression and air pollution\nindexes in Taiwan, show the effectiveness and efficiency of the proposed\nmethodology. Our results fill a missing, but crucial, part in the literature on\ntensor data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 19:54:34 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 20:20:57 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 23:02:56 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2020 23:00:39 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Chen", "You-Lin", ""], ["Kolar", "Mladen", ""], ["Tsay", "Ruey S.", ""]]}, {"id": "1906.05533", "submitter": "Chencheng Cai", "authors": "Chencheng Cai, Rong Chen, Min-ge Xie", "title": "Individualized Group Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many massive data are assembled through collections of information of a large\nnumber of individuals in a population. The analysis of such data, especially in\nthe aspect of individualized inferences and solutions, has the potential to\ncreate significant value for practical applications. Traditionally, inference\nfor an individual in the data set is either solely relying on the information\nof the individual or from summarizing the information about the whole\npopulation. However, with the availability of big data, we have the\nopportunity, as well as a unique challenge, to make a more effective\nindividualized inference that takes into consideration of both the population\ninformation and the individual discrepancy. To deal with the possible\nheterogeneity within the population while providing effective and credible\ninferences for individuals in a data set, this article develops a new approach\ncalled the individualized group learning (iGroup). The iGroup approach uses\nlocal nonparametric techniques to generate an individualized group by pooling\nother entities in the population which share similar characteristics with the\ntarget individual, even when individual estimates are biased due to limited\nnumber of observations. Three general cases of iGroup are discussed, and their\nasymptotic performances are investigated. Both theoretical results and\nempirical simulations reveal that, by applying iGroup, the performance of\nstatistical inference on the individual level are ensured and can be\nsubstantially improved from inference based on either solely individual\ninformation or entire population information. The method has a broad range of\napplications. Two examples in financial statistics and maritime anomaly\ndetection are presented.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 08:00:23 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 03:25:22 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Cai", "Chencheng", ""], ["Chen", "Rong", ""], ["Xie", "Min-ge", ""]]}, {"id": "1906.05545", "submitter": "Maurizio Daniele", "authors": "Maurizio Daniele, Winfried Pohlmeier, Aygul Zagidullina", "title": "Sparse Approximate Factor Estimation for High-Dimensional Covariance\n  Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST q-fin.PM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel estimation approach for the covariance matrix based on the\n$l_1$-regularized approximate factor model. Our sparse approximate factor (SAF)\ncovariance estimator allows for the existence of weak factors and hence relaxes\nthe pervasiveness assumption generally adopted for the standard approximate\nfactor model. We prove consistency of the covariance matrix estimator under the\nFrobenius norm as well as the consistency of the factor loadings and the\nfactors.\n  Our Monte Carlo simulations reveal that the SAF covariance estimator has\nsuperior properties in finite samples for low and high dimensions and different\ndesigns of the covariance matrix. Moreover, in an out-of-sample portfolio\nforecasting application the estimator uniformly outperforms alternative\nportfolio strategies based on alternative covariance estimation approaches and\nmodeling strategies including the $1/N$-strategy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 08:29:32 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Daniele", "Maurizio", ""], ["Pohlmeier", "Winfried", ""], ["Zagidullina", "Aygul", ""]]}, {"id": "1906.05575", "submitter": "Gentry White", "authors": "Gentry White and Dongchu Sun and Paul Speckman", "title": "Direct Sampling of Bayesian Thin-Plate Splines for Spatial Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radial basis functions are a common mathematical tool used to construct a\nsmooth interpolating function from a set of data points. A spatial prior based\non thin-plate spline radial basis functions can be easily implemented resulting\nin a posterior that can be sampled directly using Monte Carlo integration,\navoiding the computational burden and potential inefficiency of an Monte Carlo\nMarkov Chain (MCMC) sampling scheme. The derivation of the prior and sampling\nscheme are demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 09:47:56 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["White", "Gentry", ""], ["Sun", "Dongchu", ""], ["Speckman", "Paul", ""]]}, {"id": "1906.05647", "submitter": "Cai Wu", "authors": "Cai Wu, Liang Li, Ruosha Li", "title": "Dynamic Prediction of Competing Risk Events using Landmark\n  Sub-distribution Hazard Model with Multiple Longitudinal Biomarker", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cause-specific cumulative incidence function (CIF) quantifies the\nsubject-specific disease risk with competing risk outcome. With longitudinally\ncollected biomarker data, it is of interest to dynamically update the predicted\nCIF by incorporating the most recent biomarker as well as the cumulating\nlongitudinal history. Motivated by a longitudinal cohort study of chronic\nkidney disease, we propose a framework for dynamic prediction of end stage\nrenal disease using multivariate longitudinal biomarkers, accounting for the\ncompeting risk of death. The proposed framework extends the landmark survival\nmodeling to competing risks data, and implies that a distinct sub-distribution\nhazard regression model is defined at each landmark time. The model parameters,\nprediction horizon, longitudinal history and at-risk population are allowed to\nvary over the landmark time. When the measurement times of biomarkers are\nirregularly spaced, the predictor variable may not be observed at the time of\nprediction. Local polynomial is used to estimate the model parameters without\nexplicitly imputing the predictor or modeling its longitudinal trajectory. The\nproposed model leads to simple interpretation of the regression coefficients\nand closed-form calculation of the predicted CIF. The estimation and prediction\ncan be implemented through standard statistical software with tractable\ncomputation. We conducted simulations to evaluate the performance of the\nestimation procedure and predictive accuracy. The methodology is illustrated\nwith data from the African American Study of Kidney Disease and Hypertension.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 03:34:59 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Wu", "Cai", ""], ["Li", "Liang", ""], ["Li", "Ruosha", ""]]}, {"id": "1906.05741", "submitter": "Zhuoyi Yang", "authors": "Xi Chen, Weidong Liu, Xiaojun Mao and Zhuoyi Yang", "title": "Distributed High-dimensional Regression Under a Quantile Loss Function", "comments": "42 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies distributed estimation and support recovery for\nhigh-dimensional linear regression model with heavy-tailed noise. To deal with\nheavy-tailed noise whose variance can be infinite, we adopt the quantile\nregression loss function instead of the commonly used squared loss. However,\nthe non-smooth quantile loss poses new challenges to high-dimensional\ndistributed estimation in both computation and theoretical development. To\naddress the challenge, we transform the response variable and establish a new\nconnection between quantile regression and ordinary linear regression. Then, we\nprovide a distributed estimator that is both computationally and\ncommunicationally efficient, where only the gradient information is\ncommunicated at each iteration. Theoretically, we show that, after a constant\nnumber of iterations, the proposed estimator achieves a near-oracle convergence\nrate without any restriction on the number of machines. Moreover, we establish\nthe theoretical guarantee for the support recovery. The simulation analysis is\nprovided to demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 15:00:31 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 16:35:12 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Chen", "Xi", ""], ["Liu", "Weidong", ""], ["Mao", "Xiaojun", ""], ["Yang", "Zhuoyi", ""]]}, {"id": "1906.05758", "submitter": "James Salter", "authors": "James M Salter and Daniel B Williamson", "title": "Efficient calibration for high-dimensional computer model output using\n  basis methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration of expensive computer models with high-dimensional output fields\ncan be approached via history matching. If the entire output field is matched,\nwith patterns or correlations between locations or time points represented,\ncalculating the distance metric between observational data and model output for\na single input setting requires a time intensive inversion of a\nhigh-dimensional matrix. By using a low-dimensional basis representation rather\nthan emulating each output individually, we define a metric in the reduced\nspace that allows the implausibility for the field to be calculated\nefficiently, with only small matrix inversions required, using projection that\nis consistent with the variance specifications in the implausibility. We show\nthat projection using the $L_2$ norm can result in different conclusions, with\nthe ordering of points not maintained on the basis, with implications for both\nhistory matching and probabilistic methods. We demonstrate the scalability of\nour method through history matching of the Canadian atmosphere model, CanAM4,\ncomparing basis methods to emulation of each output individually, showing that\nthe basis approach can be more accurate, whilst also being more efficient.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 15:39:43 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Salter", "James M", ""], ["Williamson", "Daniel B", ""]]}, {"id": "1906.05850", "submitter": "Adji Bousso Dieng", "authors": "Adji B. Dieng and John Paisley", "title": "Reweighted Expectation Maximization", "comments": "Code can be found at https://github.com/adjidieng/REM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep generative models with maximum likelihood remains a challenge.\nThe typical workaround is to use variational inference (VI) and maximize a\nlower bound to the log marginal likelihood of the data. Variational\nauto-encoders (VAEs) adopt this approach. They further amortize the cost of\ninference by using a recognition network to parameterize the variational\nfamily. Amortized VI scales approximate posterior inference in deep generative\nmodels to large datasets. However it introduces an amortization gap and leads\nto approximate posteriors of reduced expressivity due to the problem known as\nposterior collapse. In this paper, we consider expectation maximization (EM) as\na paradigm for fitting deep generative models. Unlike VI, EM directly maximizes\nthe log marginal likelihood of the data. We rediscover the importance weighted\nauto-encoder (IWAE) as an instance of EM and propose a new EM-based algorithm\nfor fitting deep generative models called reweighted expectation maximization\n(REM). REM learns better generative models than the IWAE by decoupling the\nlearning dynamics of the generative model and the recognition network using a\nseparate expressive proposal found by moment matching. We compared REM to the\nVAE and the IWAE on several density estimation benchmarks and found it leads to\nsignificantly better performance as measured by log-likelihood.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 17:49:54 GMT"}, {"version": "v2", "created": "Sat, 10 Aug 2019 19:46:56 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Dieng", "Adji B.", ""], ["Paisley", "John", ""]]}, {"id": "1906.05944", "submitter": "Francois-Xavier Briol", "authors": "Francois-Xavier Briol, Alessandro Barp, Andrew B. Duncan, Mark\n  Girolami", "title": "Statistical Inference for Generative Models with Maximum Mean\n  Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While likelihood-based inference and its variants provide a statistically\nefficient and widely applicable approach to parametric inference, their\napplication to models involving intractable likelihoods poses challenges. In\nthis work, we study a class of minimum distance estimators for intractable\ngenerative models, that is, statistical models for which the likelihood is\nintractable, but simulation is cheap. The distance considered, maximum mean\ndiscrepancy (MMD), is defined through the embedding of probability measures\ninto a reproducing kernel Hilbert space. We study the theoretical properties of\nthese estimators, showing that they are consistent, asymptotically normal and\nrobust to model misspecification. A main advantage of these estimators is the\nflexibility offered by the choice of kernel, which can be used to trade-off\nstatistical efficiency and robustness. On the algorithmic side, we study the\ngeometry induced by MMD on the parameter space and use this to introduce a\nnovel natural gradient descent-like algorithm for efficient implementation of\nthese estimators. We illustrate the relevance of our theoretical results on\nseveral classes of models including a discrete-time latent Markov process and\ntwo multivariate stochastic differential equation models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 21:53:55 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Briol", "Francois-Xavier", ""], ["Barp", "Alessandro", ""], ["Duncan", "Andrew B.", ""], ["Girolami", "Mark", ""]]}, {"id": "1906.06080", "submitter": "Jiuyong Li", "authors": "Jiuyong Li, Saisai Ma, Lin Liu, Thuc Duy Le, Jixue Liu, and Yizhao Han", "title": "Identify treatment effect patterns for personalised decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In personalised decision making, evidence is required to determine suitable\nactions for individuals. Such evidence can be obtained by identifying treatment\neffect heterogeneity in different subgroups of the population. In this paper,\nwe design a new type of pattern, treatment effect pattern to represent and\ndiscover treatment effect heterogeneity from data for determining whether a\ntreatment will work for an individual or not. Our purpose is to use the\ncomputational power to find the most specific and relevant conditions for\nindividuals with respect to a treatment or an action to assist with\npersonalised decision making. Most existing work on identifying treatment\neffect heterogeneity takes a top-down or partitioning based approach to search\nfor subgroups with heterogeneous treatment effects. We propose a bottom-up\ngeneralisation algorithm to obtain the most specific patterns that fit\nindividual circumstances the best for personalised decision making. For the\ngeneralisation, we follow a consistency driven strategy to maintain inner-group\nhomogeneity and inter-group heterogeneity of treatment effects. We also employ\ngraphical causal modelling technique to identify adjustment variables for\nreliable treatment effect pattern discovery. Our method can find the treatment\neffect patterns reliably as validated by the experiments. The method is faster\nthan the two existing machine learning methods for heterogeneous treatment\neffect identification and it produces subgroups with higher inner-group\ntreatment effect homogeneity.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 08:42:21 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Li", "Jiuyong", ""], ["Ma", "Saisai", ""], ["Liu", "Lin", ""], ["Le", "Thuc Duy", ""], ["Liu", "Jixue", ""], ["Han", "Yizhao", ""]]}, {"id": "1906.06095", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen and Siliang Zhang", "title": "A Latent Gaussian Process Model for Analyzing Intensive Longitudinal\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intensive longitudinal studies are becoming progressively more prevalent\nacross many social science areas, especially in psychology. New technologies\nlike smart-phones, fitness trackers, and the Internet of Things make it much\neasier than in the past for data collection in intensive longitudinal studies,\nproviding an opportunity to look deep into the underlying characteristics of\nindividuals under a high temporal resolution. In this paper, we introduce a new\nmodeling framework for latent curve analysis that is more suitable for the\nanalysis of intensive longitudinal data than existing latent curve models.\nSpecifically, through the modeling of an individual-specific continuous-time\nlatent process, some unique features of intensive longitudinal data are better\ncaptured, including intensive measurements in time and unequally spaced time\npoints of observations. Technically, the continuous-time latent process is\nmodeled by a Gaussian process model. This model can be regarded as a\nsemi-parametric extension of the classical latent curve models and falls under\nthe framework of structural equation modeling. Procedures for parameter\nestimation and statistical inference are provided under an empirical Bayes\nframework and evaluated by simulation studies. We illustrate the use of the\nproposed model through the analysis of an ecological momentary assessment\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 09:36:43 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Chen", "Yunxiao", ""], ["Zhang", "Siliang", ""]]}, {"id": "1906.06360", "submitter": "Martin Weidner", "authors": "St\\'ephane Bonhomme, Martin Weidner", "title": "Posterior Average Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economists are often interested in estimating averages with respect to\ndistributions of unobservables, such as moments of individual fixed-effects, or\naverage partial effects in discrete choice models. For such quantities, we\npropose and study posterior average effects (PAE), where the average is\ncomputed conditional on the sample, in the spirit of empirical Bayes and\nshrinkage methods. While the usefulness of shrinkage for prediction is\nwell-understood, a justification of posterior conditioning to estimate\npopulation averages is currently lacking. We show that PAE have minimum\nworst-case specification error under various forms of misspecification of the\nparametric distribution of unobservables. In addition, we introduce a measure\nof informativeness of the posterior conditioning, which quantifies the\nworst-case specification error of PAE relative to parametric model-based\nestimators. As illustrations, we report PAE estimates of distributions of\nneighborhood effects in the US, and of permanent and transitory components in a\nmodel of income dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 18:22:47 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 16:43:00 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 19:29:26 GMT"}, {"version": "v4", "created": "Wed, 7 Oct 2020 10:55:42 GMT"}, {"version": "v5", "created": "Fri, 5 Mar 2021 00:07:25 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Bonhomme", "St\u00e9phane", ""], ["Weidner", "Martin", ""]]}, {"id": "1906.06390", "submitter": "Meisam Hejazi Nia", "authors": "Meisam Hejazinia, Majid Hosseini, Bryant Sih", "title": "A/B Testing Measurement Framework for Recommendation Models Based on\n  Expected Revenue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a method to determine whether a new recommendation system improves\nthe revenue per visit (RPV) compared to the status quo. We achieve our goal by\nsplitting RPV into conversion rate and average order value (AOV). We use the\ntwo-part test suggested by Lachenbruch to determine if the data generating\nprocess in the new system is different. In cases that this test does not give\nus a definitive answer about the change in RPV, we propose two alternative\ntests to determine if RPV has changed. Both of these tests rely on the\nassumption that non-zero purchase values follow a log-normal distribution. We\nempirically validate this assumption using data collected at different points\nin time from Staples.com. On average, our method needs a smaller sample size\nthan other methods. Furthermore, it does not require any subjective outlier\nremoval. Finally, it characterizes the uncertainty around RPV by providing a\nconfidence interval.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 20:24:47 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Hejazinia", "Meisam", ""], ["Hosseini", "Majid", ""], ["Sih", "Bryant", ""]]}, {"id": "1906.06459", "submitter": "Zhou Lan", "authors": "Zhou Lan, Brian J Reich", "title": "Probabilistic Diffusion MRI Fiber Tracking Using a Directed Acyclic\n  Graph Auto-Regressive Model of Positive Definite Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion MRI is a neuroimaging technique measuring the anatomical structure\nof tissues. Using diffusion MRI to construct the connections of tissues, known\nas fiber tracking, is one of the most important uses of diffusion MRI. Many\ntechniques are available recently but few properly quantify statistical\nuncertainties. In this paper, we propose a directed acyclic graph\nauto-regressive model of positive definite matrices and apply a probabilistic\nfiber tracking algorithm. We use both real data analysis and numerical studies\nto demonstrate our proposal.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 03:20:14 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Lan", "Zhou", ""], ["Reich", "Brian J", ""]]}, {"id": "1906.06463", "submitter": "Jasjeet Sekhon", "authors": "S\\\"oren R. K\\\"unzel, Theo F. Saarinen, Edward W. Liu, Jasjeet S.\n  Sekhon", "title": "Linear Aggregation in Tree-based Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression trees and their ensemble methods are popular methods for\nnonparametric regression: they combine strong predictive performance with\ninterpretable estimators. To improve their utility for locally smooth response\nsurfaces, we study regression trees and random forests with linear aggregation\nfunctions. We introduce a new algorithm that finds the best axis-aligned split\nto fit linear aggregation functions on the corresponding nodes, and we offer a\nquasilinear time implementation. We demonstrate the algorithm's favorable\nperformance on real-world benchmarks and in an extensive simulation study, and\nwe demonstrate its improved interpretability using a large get-out-the-vote\nexperiment. We provide an open-source software package that implements several\ntree-based estimators with linear aggregation functions.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 04:25:55 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 20:00:42 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 23:17:20 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2021 20:45:03 GMT"}, {"version": "v5", "created": "Sat, 23 Jan 2021 02:18:06 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["K\u00fcnzel", "S\u00f6ren R.", ""], ["Saarinen", "Theo F.", ""], ["Liu", "Edward W.", ""], ["Sekhon", "Jasjeet S.", ""]]}, {"id": "1906.06484", "submitter": "Amadou Diadie Ba", "authors": "Amadou Diadie Ba, Gane Samb Lo, Cheikh Tidiane Seck", "title": "Non parametric estimation of joint, Renyi-Stallis entropies and mutual\n  information and asymptotic limits", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for estimating the joint probability mass\nfunction of a pair of discrete random variables. This estimator is used to\nconstruct joint Shannon R\\'enyi-Tsallis entropies, and the mutual information\nestimates of a pair of discrete random variables. Almost sure consistency and\ncentral limit Theorems are established. Our theorical results are validated by\nsimulations.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 07:20:56 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 03:33:07 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Ba", "Amadou Diadie", ""], ["Lo", "Gane Samb", ""], ["Seck", "Cheikh Tidiane", ""]]}, {"id": "1906.06513", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Bayesian Network Models for Incomplete and Dynamic Data", "comments": "24 pages, 4 figures", "journal-ref": "Statistica Neerlandica (2020), 74(3), 397-419", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks are a versatile and powerful tool to model complex\nphenomena and the interplay of their components in a probabilistically\nprincipled way. Moving beyond the comparatively simple case of completely\nobserved, static data, which has received the most attention in the literature,\nin this paper we will review how Bayesian networks can model dynamic data and\ndata with incomplete observations. Such data are the norm at the forefront of\nresearch and in practical applications, and Bayesian networks are uniquely\npositioned to model them due to their explainability and interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 09:59:04 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 12:17:33 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1906.06580", "submitter": "Mike West", "authors": "Isaac Lavine, Michael Lindon, and Mike West", "title": "Adaptive Variable Selection for Sequential Prediction in Multivariate\n  Dynamic Models", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss Bayesian model uncertainty analysis and forecasting in sequential\ndynamic modeling of multivariate time series. The perspective is that of a\ndecision-maker with a specific forecasting objective that guides thinking about\nrelevant models. Based on formal Bayesian decision-theoretic reasoning, we\ndevelop a time-adaptive approach to exploring, weighting, combining and\nselecting models that differ in terms of predictive variables included. The\nadaptivity allows for changes in the sets of favored models over time, and is\nguided by the specific forecasting goals. A synthetic example illustrates how\ndecision-guided variable selection differs from traditional Bayesian model\nuncertainty analysis and standard model averaging. An applied study in one\nmotivating application of long-term macroeconomic forecasting highlights the\nutility of the new approach in terms of improving predictions as well as its\nability to identify and interpret different sets of relevant models over time\nwith respect to specific, defined forecasting goals.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 15:53:50 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 14:03:58 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Lavine", "Isaac", ""], ["Lindon", "Michael", ""], ["West", "Mike", ""]]}, {"id": "1906.06583", "submitter": "Emmanuel Caron", "authors": "Emmanuel Caron and J\\'er\\^ome Dedecker and Bertrand Michel", "title": "Linear regression with stationary errors : the R package slm", "comments": "31 pages, 11 figures, 5 tables. The associated R package 'slm' is\n  available on the CRAN website (https://cran.r-project.org/index.html) or on\n  the GitHub website (https://github.com/E-Caron/slm)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the R package slm which stands for Stationary Linear\nModels. The package contains a set of statistical procedures for linear\nregression in the general context where the error process is strictly\nstationary with short memory. We work in the setting of Hannan (1973), who\nproved the asymptotic normality of the (normalized) least squares estimators\n(LSE) under very mild conditions on the error process. We propose different\nways to estimate the asymptotic covariance matrix of the LSE, and then to\ncorrect the type I error rates of the usual tests on the parameters (as well as\nconfidence intervals). The procedures are evaluated through different sets of\nsimulations, and two examples of real datasets are studied.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 16:09:48 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 14:51:36 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 18:45:33 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Caron", "Emmanuel", ""], ["Dedecker", "J\u00e9r\u00f4me", ""], ["Michel", "Bertrand", ""]]}, {"id": "1906.06615", "submitter": "Sara Algeri", "authors": "Sara Algeri", "title": "Detecting new signals under background mismodelling", "comments": null, "journal-ref": "Phys. Rev. D 101, 015003 (2020)", "doi": "10.1103/PhysRevD.101.015003", "report-no": null, "categories": "physics.data-an astro-ph.HE astro-ph.IM hep-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searches for new astrophysical phenomena often involve several sources of\nnon-random uncertainties which can lead to highly misleading results. Among\nthese, model-uncertainty arising from background mismodelling can dramatically\ncompromise the sensitivity of the experiment under study. Specifically,\noverestimating the background distribution in the signal region increases the\nchances of missing new physics. Conversely, underestimating the background\noutside the signal region leads to an artificially enhanced sensitivity and a\nhigher likelihood of claiming false discoveries. The aim of this work is to\nprovide a unified statistical strategy to perform modelling, estimation,\ninference, and signal characterization under background mismodelling. The\nmethod proposed allows to incorporate the (partial) scientific knowledge\navailable on the background distribution and provides a data-updated version of\nit in a purely nonparametric fashion without requiring the specification of\nprior distributions on the parameters. Applications in the context of dark\nmatter searches and radio surveys show how the tools presented in this article\ncan be used to incorporate non-stochastic uncertainty due to instrumental noise\nand to overcome violations of classical distributional assumptions in stacking\nexperiments.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 21:10:54 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 16:29:21 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 03:11:25 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Algeri", "Sara", ""]]}, {"id": "1906.06646", "submitter": "Eric Rose", "authors": "Eric J. Rose, Eric B. Laber, Marie Davidian, Anastasios A. Tsiatis,\n  Ying-Qi Zhao, Michael R. Kosorok", "title": "Sample Size Calculations for SMARTs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Multiple Assignment Randomized Trials (SMARTs) are considered the\ngold standard for estimation and evaluation of treatment regimes. SMARTs are\ntypically sized to ensure sufficient power for a simple comparison, e.g., the\ncomparison of two fixed treatment sequences. Estimation of an optimal treatment\nregime is conducted as part of a secondary and hypothesis-generating analysis\nwith formal evaluation of the estimated optimal regime deferred to a follow-up\ntrial. However, running a follow-up trial to evaluate an estimated optimal\ntreatment regime is costly and time-consuming; furthermore, the estimated\noptimal regime that is to be evaluated in such a follow-up trial may be far\nfrom optimal if the original trial was underpowered for estimation of an\noptimal regime. We derive sample size procedures for a SMART that ensure: (i)\nsufficient power for comparing the optimal treatment regime with standard of\ncare; and (ii) the estimated optimal regime is within a given tolerance of the\ntrue optimal regime with high-probability. We establish asymptotic validity of\nthe proposed procedures and demonstrate their finite sample performance in a\nseries of simulation experiments.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 04:40:17 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Rose", "Eric J.", ""], ["Laber", "Eric B.", ""], ["Davidian", "Marie", ""], ["Tsiatis", "Anastasios A.", ""], ["Zhao", "Ying-Qi", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1906.06713", "submitter": "Zhigang Yao", "authors": "Yan Liu, Zhiqiang Hou, Zhigang Yao, Zhidong Bai, Jiang Hu, Shurong\n  Zheng", "title": "Community Detection Based on the $L_\\infty$ convergence of eigenvectors\n  in DCBM", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is one of the most popular algorithms for community\ndetection in network analysis. Based on this rationale, in this paper we give\nthe convergence rate of eigenvectors for the adjacency matrix in the $l_\\infty$\nnorm, under the stochastic block model (BM) and degree corrected stochastic\nblock model (DCBM), adding some mild and rational conditions. We also extend\nthis result to a more general model, presented based on the DCBM such that the\nvalue of random variables in the adjacency matrix is not 0 or 1, but an\narbitrary real number. During the process of proving the above conclusion, we\nobtain the relationship of the eigenvalues in the adjacency matrix and the\ncorresponding `population' matrix, which vary in dimension from the\ncommunity-wise edge probability matrix. Using that result, we can give an\nestimate of the number of the communities in a known set of network data.\nMeanwhile we proved the consistency of the estimator. Furthermore, according to\nthe derivation of proof for the convergence of eigenvectors, we propose a new\napproach to community detection -- Spectral Clustering based on Difference of\nRatios of Eigenvectors (SCDRE). Our simulation experiments demonstrate the\nsuperiority of our method in community detection.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 15:01:19 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Liu", "Yan", ""], ["Hou", "Zhiqiang", ""], ["Yao", "Zhigang", ""], ["Bai", "Zhidong", ""], ["Hu", "Jiang", ""], ["Zheng", "Shurong", ""]]}, {"id": "1906.06729", "submitter": "Zhiqiang Tan", "authors": "Ting Yang, Zhiqiang Tan", "title": "Hierarchical Total Variations and Doubly Penalized ANOVA Modeling for\n  Multivariate Nonparametric Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For multivariate nonparametric regression, functional analysis-of-variance\n(ANOVA) modeling aims to capture the relationship between a response and\ncovariates by decomposing the unknown function into various components,\nrepresenting main effects, two-way interactions, etc. Such an approach has been\npursued explicitly in smoothing spline ANOVA modeling and implicitly in various\ngreedy methods such as MARS. We develop a new method for functional ANOVA\nmodeling, based on doubly penalized estimation using total-variation and\nempirical-norm penalties, to achieve sparse selection of component functions\nand their knots. For this purpose, we formulate a new class of hierarchical\ntotal variations, which measures total variations at different levels including\nmain effects and multi-way interactions, possibly after some order of\ndifferentiation. Furthermore, we derive suitable basis functions for\nmultivariate splines such that the hierarchical total variation can be\nrepresented as a regular Lasso penalty, and hence we extend a previous\nbackfitting algorithm to handle doubly penalized estimation for ANOVA modeling.\nWe present extensive numerical experiments on simulations and real data to\ncompare our method with existing methods including MARS, tree boosting, and\nrandom forest. The results are very encouraging and demonstrate considerable\ngains from our method in both prediction or classification accuracy and\nsimplicity of the fitted functions.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 17:09:30 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Yang", "Ting", ""], ["Tan", "Zhiqiang", ""]]}, {"id": "1906.06742", "submitter": "Yongli Sang", "authors": "Yongli Sang, Xin Dang and Yichuan Zhao", "title": "Depth-based Weighted Jackknife Empirical Likelihood for Non-smooth\n  U-structure Equations", "comments": "25 pages, 4 tables and one figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, parameters of interest are estimated by solving some\nnon-smooth estimating equations with $U$-statistic structure. Jackknife\nempirical likelihood (JEL) approach can solve this problem efficiently by\nreducing the computation complexity of the empirical likelihood (EL) method.\nHowever, as EL, JEL suffers the sensitivity problem to outliers. In this paper,\nwe propose a weighted jackknife empirical likelihood (WJEL) to tackle the above\nlimitation of JEL. The proposed WJEL tilts the JEL function by assigning\nsmaller weights to outliers. The asymptotic of the WJEL ratio statistic is\nderived. It converges in distribution to a multiple of a chi-square random\nvariable. The multiplying constant depends on the weighting scheme. The\nself-normalized version of WJEL ratio does not require to know the constant and\nhence yields the standard chi-square distribution in the limit. Robustness of\nthe proposed method is illustrated by simulation studies and one real data\napplication.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 18:06:29 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Sang", "Yongli", ""], ["Dang", "Xin", ""], ["Zhao", "Yichuan", ""]]}, {"id": "1906.06749", "submitter": "David Jones", "authors": "David E. Jones and Xiao-Li Meng", "title": "Designing Test Information and Test Information in Design", "comments": "32 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DeGroot (1962) developed a general framework for constructing Bayesian\nmeasures of the expected information that an experiment will provide for\nestimation. We propose an analogous framework for measures of information for\nhypothesis testing. In contrast to estimation information measures that are\ntypically used for surface estimation, test information measures are more\nuseful in experimental design for hypothesis testing and model selection. In\nparticular, we obtain a probability based measure, which has more appealing\nproperties than variance based measures in design contexts where decision\nproblems are of interest. The underlying intuition of our design proposals is\nstraightforward: to distinguish between models we should collect data from\nregions of the covariate space for which the models differ most. Nicolae et al.\n(2008) gave an asymptotic equivalence between their test information measures\nand Fisher information. We extend this result to all test information measures\nunder our framework. Simulation studies and an application in astronomy\ndemonstrate the utility of our approach, and provide comparison to other\nmethods including that of Box and Hill (1967).\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 18:49:17 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Jones", "David E.", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "1906.06828", "submitter": "Abhijit Mandal", "authors": "Abhijit Mandal", "title": "An Optimal Test for the Additive Model with Discrete or Categorical\n  Predictors", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multivariate nonparametric regression the additive models are very useful\nwhen a suitable parametric model is difficult to find. The backfitting\nalgorithm is a powerful tool to estimate the additive components. However, due\nto complexity of the estimators, the asymptotic $p$-value of the associated\ntest is difficult to calculate without a Monte Carlo simulation. Moreover, the\nconventional tests assume that the predictor variables are strictly continuous.\nIn this paper, a new test is introduced for the additive components with\ndiscrete or categorical predictors, where the model may contain continuous\ncovariates. This method is also applied to the semiparametric regression to\ntest the goodness-of-fit of the model. These tests are asymptotically optimal\nin terms of the rate of convergence, as they can detect a specific class of\ncontiguous alternatives at a rate of $n^{-1/2}$. An extensive simulation study\nis presented to support the theoretical results derived in this paper. Finally,\nthe method is applied to a real data to model the diamond price based on its\nquality attributes and physical measurements.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 03:33:28 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Mandal", "Abhijit", ""]]}, {"id": "1906.07177", "submitter": "Taylor Pospisil", "authors": "Taylor Pospisil and Ann B. Lee", "title": "(f)RFCDE: Random Forests for Conditional Density Estimation and\n  Functional Data", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.05753", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests is a common non-parametric regression technique which performs\nwell for mixed-type unordered data and irrelevant features, while being robust\nto monotonic variable transformations. Standard random forests, however, do not\nefficiently handle functional data and runs into a curse-of dimensionality when\npresented with high-resolution curves and surfaces. Furthermore, in settings\nwith heteroskedasticity or multimodality, a regression point estimate with\nstandard errors do not fully capture the uncertainty in our predictions. A more\ninformative quantity is the conditional density p(y | x) which describes the\nfull extent of the uncertainty in the response y given covariates x. In this\npaper we show how random forests can be efficiently leveraged for conditional\ndensity estimation, functional covariates, and multiple responses without\nincreasing computational complexity. We provide open-source software for all\nprocedures with R and Python versions that call a common C++ library.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 03:57:41 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Pospisil", "Taylor", ""], ["Lee", "Ann B.", ""]]}, {"id": "1906.07265", "submitter": "Keith Levin", "authors": "Keith Levin and Asad Lodhia and Elizaveta Levina", "title": "Recovering shared structure from multiple networks with unknown edge\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG eess.SP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In increasingly many settings, data sets consist of multiple samples from a\npopulation of networks, with vertices aligned across these networks. For\nexample, brain connectivity networks in neuroscience consist of measures of\ninteraction between brain regions that have been aligned to a common template.\nWe consider the setting where the observed networks have a shared expectation,\nbut may differ in the noise structure on their edges. Our approach exploits the\nshared mean structure to denoise edge-level measurements of the observed\nnetworks and estimate the underlying population-level parameters. We also\nexplore the extent to which edge-level errors influence estimation and\ndownstream inference. We establish a finite-sample concentration inequality for\nthe low-rank eigenvalue truncation of a random weighted adjacency matrix that\nmay be of independent interest. The proposed approach is illustrated on\nsynthetic networks and on data from an fMRI study of schizophrenia.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 01:30:27 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 20:46:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Levin", "Keith", ""], ["Lodhia", "Asad", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1906.07313", "submitter": "Yinan Li", "authors": "Yinan Li and Fang Liu", "title": "Continuous-time Markov-switching GARCH Process with Robust and Efficient\n  State Path and Volatility Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a continuous-time Markov-switching generalized autoregressive\nconditional heteroskedasticity (COMS-GARCH) process for handling irregularly\nspaced time series (TS) with multiple volatilities states. We employ a Gibbs\nsampler in the Bayesian framework to estimate the COMS-GARCH model parameters,\nthe latent state path and volatilities. To improve the inferential robustness\nand computational efficiency for obtaining the maximum a posteriori estimates\nfor the state path and volatilities, we suggest a multi-path sampling scheme\nand incorporate the Bernoulli noise injection in the computational algorithm.\nWe provide theoretical justifications for the improved stability and robustness\nwith the Bernoulli noise injection through the concept of ensemble learning and\nthe low sensitivity of the objective function to external perturbation in the\nTS. We apply the proposed COMS-GARCH process and the computational procedure to\nsimulated TS, a real currency exchange rate TS, and a real blood volume\namplitude TS. The empirical results demonstrate that the COMS-GARCH process and\nthe computational procedure are able to predict volatility regimes and\nvolatilities in a TS with satisfactory accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 00:15:07 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 22:16:29 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Yinan", ""], ["Liu", "Fang", ""]]}, {"id": "1906.07500", "submitter": "Steven Gilmour Prof", "authors": "Heloisa M. de Oliveira, Cesar B. A. de Oliveira, Steven G. Gilmour,\n  Luzia A. Trinca", "title": "Prediction properties of optimum response surface designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction capability is considered an important issue in response surface\nmethodology. Following the line of argument that a design should have several\ndesirable properties we have extended an existing compound design criterion to\ninclude prediction properties. Prediction of responses and of differences in\nresponse are considered. Point and interval predictions are allowed for.\nExtensions of existing graphical tools for inspecting prediction performances\nof the designs in the whole region of experimentation are also introduced. The\nmethods are illustrated with two examples.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 11:21:19 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["de Oliveira", "Heloisa M.", ""], ["de Oliveira", "Cesar B. A.", ""], ["Gilmour", "Steven G.", ""], ["Trinca", "Luzia A.", ""]]}, {"id": "1906.07505", "submitter": "Benjamin Taylor", "authors": "Erick A. Chac\\'on-Montalv\\'an and Luke Parry and Gemma Davies and\n  Benjamin M. Taylor", "title": "A Model-Based General Alternative to the Standardised Precipitation\n  Index", "comments": "19 pages, 4 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce two new model-based versions of the widely-used\nstandardized precipitation index (SPI) for detecting and quantifying the\nmagnitude of extreme hydro-climatic events. Our analytical approach is based on\ngeneralized additive models for location, scale and shape (GAMLSS), which helps\nas to overcome some limitations of the SPI. We compare our model-based\nstandardised indices (MBSIs) with the SPI using precipitation data collected\nbetween January 2004 - December 2013 (522 weeks) in Caapiranga, a road-less\nmunicipality of Amazonas State. As a result, it is shown that the MBSI-1 is an\nindex with similar properties to the SPI, but with improved methodology. In\ncomparison to the SPI, our MBSI-1 index allows for the use of different\nzero-augmented distributions, it works with more flexible time-scales, can be\napplied to shorter records of data and also takes into account temporal\ndependencies in known seasonal behaviours. Our approach is implemented in an R\npackage, mbsi, available from Github.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 11:45:42 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Chac\u00f3n-Montalv\u00e1n", "Erick A.", ""], ["Parry", "Luke", ""], ["Davies", "Gemma", ""], ["Taylor", "Benjamin M.", ""]]}, {"id": "1906.07524", "submitter": "Riko Kelter", "authors": "Riko Kelter", "title": "A new Bayesian two-sample t-test for effect size estimation under\n  uncertainty based on a two-component Gaussian mixture with known allocations\n  and the region of practical equivalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing differences between a treatment and control group is common practice\nin biomedical research like randomized controlled trials (RCT). The standard\ntwo-sample t-test relies on null hypothesis significance testing (NHST) via\np-values, which has several drawbacks. Bayesian alternatives were recently\nintroduced using the Bayes factor, which has its own limitations. This paper\nintroduces an alternative to current Bayesian two-sample t-tests by\ninterpreting the underlying model as a two-component Gaussian mixture in which\nthe effect size is the quantity of interest, which is most relevant in clinical\nresearch. Unlike p-values or the Bayes factor, the proposed method focusses on\nestimation under uncertainty instead of explicit hypothesis testing. Therefore,\nvia a Gibbs sampler the posterior of the effect size is produced, which is used\nsubsequently for either estimation under uncertainty or explicit hypothesis\ntesting based on the region of practical equivalence (ROPE). An illustrative\nexample, theoretical results and a simulation study show the usefulness of the\nproposed method, and the test is made available in the R package bayest.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 12:25:20 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 06:11:57 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Kelter", "Riko", ""]]}, {"id": "1906.07695", "submitter": "Fabien Navarro", "authors": "Christophe Chesneau, Salima El Kolei, Junke Kou, Fabien Navarro", "title": "Nonparametric estimation in a regression model with additive and\n  multiplicative noise", "comments": null, "journal-ref": "Journal of Computational and Applied Mathematics, Volume 380, 2020", "doi": "10.1016/j.cam.2020.112971", "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an unknown functional estimation problem in a\ngeneral nonparametric regression model with the feature of having both\nmultiplicative and additive noise.We propose two new wavelet estimators in this\ngeneral context. We prove that they achieve fast convergence rates under the\nmean integrated square error over Besov spaces. The obtained rates have the\nparticularity of being established under weak conditions on the model. A\nnumerical study in a context comparable to stochastic frontier estimation (with\nthe difference that the boundary is not necessarily a production function)\nsupports the theory.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 17:16:32 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 08:16:53 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Chesneau", "Christophe", ""], ["Kolei", "Salima El", ""], ["Kou", "Junke", ""], ["Navarro", "Fabien", ""]]}, {"id": "1906.07757", "submitter": "Jichun Xie", "authors": "John Pura and Xuechan Li and Cliburn Chan and Jichun Xie", "title": "TEAM: A Multiple Testing Algorithm on the Aggregation Tree for Flow\n  Cytometry Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In immunology studies, flow cytometry is a commonly used multivariate\nsingle-cell assay. One key goal in flow cytometry analysis is to pinpoint the\nimmune cells responsive to certain stimuli. Statistically, this problem can be\ntranslated into comparing two protein expression probability density functions\n(PDFs) before and after the stimulus; the goal is to pinpoint the regions where\nthese two pdfs differ. In this paper, we model this comparison as a multiple\ntesting problem. First, we partition the sample space into small bins. In each\nbin we form a hypothesis to test the existence of differential pdfs. Second, we\ndevelop a novel multiple testing method, called TEAM (Testing on the\nAggregation tree Method), to identify those bins that harbor differential pdfs\nwhile controlling the false discovery rate (FDR) under the desired level. TEAM\nembeds the testing procedure into an aggregation tree to test from fine- to\ncoarse-resolution. The procedure achieves the statistical goal of pinpointing\ndifferential pdfs to the smallest possible regions. TEAM is computationally\nefficient, capable of analyzing large flow cytometry data sets in much shorter\ntime compared with competing methods. We applied TEAM and competing methods on\na flow cytometry data set to identify T cells responsive to the cytomeglovirus\n(CMV)-pp65 antigen stimulation. TEAM successfully identified the\nmonofunctional, bifunctional, and polyfunctional T cells while the competing\nmethods either did not finish in a reasonable time frame or provided less\ninterpretable results. Numerical simulations and theoretical justifications\ndemonstrate that TEAM has asymptotically valid, powerful, and robust\nperformance. Overall, TEAM is a computationally efficient and statistically\npowerful algorithm that can yield meaningful biological insights in flow\ncytometry studies.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 18:30:36 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 14:32:16 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Pura", "John", ""], ["Li", "Xuechan", ""], ["Chan", "Cliburn", ""], ["Xie", "Jichun", ""]]}, {"id": "1906.07798", "submitter": "Matthias Eckardt", "authors": "Matthias Eckardt, Jorge Mateu", "title": "A spatial dependence graph model for multivariate spatial hybrid\n  processes", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the joint analysis of multivariate mixed-type\nspatial data, where some components are point processes and some are of\nlattice-type by nature. After a survey of statistical methods for marked\nspatial point and lattice processes, the class of multivariate spatial hybrid\nprocesses is defined and embedded within the framework of spatial dependence\ngraph models. In this model, the point and lattice sub-processes are identified\nwith nodes of a graph whereas missing edges represent conditional independence\namong the components. This finally leads to a general framework for any type of\nspatial data in a multivariate setting. We demonstrate the application of our\nmethod in the analysis of a multivariate point-lattice pattern on crime and\nambulance service call-out incidents recorded in London, where the points are\nthe locations of different pre-classified crime events and the lattice\ncomponents report different aggregated incident rates at ward level.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 20:37:02 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Eckardt", "Matthias", ""], ["Mateu", "Jorge", ""]]}, {"id": "1906.07801", "submitter": "Rianne de Heide", "authors": "Peter Gr\\\"unwald, Rianne de Heide, and Wouter Koolen", "title": "Safe Testing", "comments": "Preliminary version, not yet submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the theory of hypothesis testing based on the e-value, a notion of\nevidence that, unlike the p-value, allows for effortlessly combining results\nfrom several tests. Even in the common scenario of optional continuation, where\nthe decision to perform a new test depends on previous test outcomes, 'safe'\ntests based on e-values generally preserve Type-I error guarantees. Our main\nresult shows that e-values exist for completely general testing problems with\ncomposite null and alternatives. Their prime interpretation is in terms of\ngambling or investing, each e-value corresponding to a particular investment.\nSurprisingly, optimal 'GROW' e-values, which lead to fastest capital growth,\nare fully characterized by the joint information projection (JIPr) between the\nset of all Bayes marginal distributions on H0 and H1. Thus, optimal e-values\nalso have an interpretation as Bayes factors, with priors given by the JIPr. We\nillustrate the theory using several 'classic' examples including a one-sample\nsafe t-test and the 2 x 2 contingency table. Sharing Fisherian, Neymanian and\nJeffreys-Bayesian interpretations, e-values and safe tests may provide a\nmethodology acceptable to adherents of all three schools.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 20:39:27 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 08:38:35 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Gr\u00fcnwald", "Peter", ""], ["de Heide", "Rianne", ""], ["Koolen", "Wouter", ""]]}, {"id": "1906.07828", "submitter": "Daniel Zilber", "authors": "Daniel Zilber, Matthias Katzfuss", "title": "Vecchia-Laplace approximations of generalized Gaussian processes for big\n  non-Gaussian spatial data", "comments": "26 pages, 10 figures, code available at\n  https://github.com/katzfuss-group/GPvecchia-Laplace", "journal-ref": "Computational Statistics & Data Analysis (2021), 153, 107081", "doi": "10.1016/j.csda.2020.107081", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Gaussian processes (GGPs) are highly flexible models that combine\nlatent GPs with potentially non-Gaussian likelihoods from the exponential\nfamily. GGPs can be used in a variety of settings, including GP classification,\nnonparametric count regression, modeling non-Gaussian spatial data, and\nanalyzing point patterns. However, inference for GGPs can be analytically\nintractable, and large datasets pose computational challenges due to the\ninversion of the GP covariance matrix. We propose a Vecchia-Laplace\napproximation for GGPs, which combines a Laplace approximation to the\nnon-Gaussian likelihood with a computationally efficient Vecchia approximation\nto the GP, resulting in a simple, general, scalable, and accurate methodology.\nWe provide numerical studies and comparisons on simulated and real spatial\ndata. Our methods are implemented in a freely available R package.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 21:58:30 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 17:49:32 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 20:39:42 GMT"}, {"version": "v4", "created": "Thu, 4 Jun 2020 20:18:45 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zilber", "Daniel", ""], ["Katzfuss", "Matthias", ""]]}, {"id": "1906.07869", "submitter": "Yuqi Gu", "authors": "Yuqi Gu and Gongjun Xu", "title": "Identifiability of Hierarchical Latent Attribute Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Latent Attribute Models (HLAMs) are a family of discrete latent\nvariable models that are attracting increasing attention in educational,\npsychological, and behavioral sciences. The key ingredients of an HLAM include\na binary structural matrix and a directed acyclic graph specifying hierarchical\nconstraints on the configurations of latent attributes. These components encode\npractitioners' design information and carry important scientific meanings.\nDespite the popularity of HLAMs, the fundamental identifiability issue remains\nunaddressed. The existence of the attribute hierarchy graph leads to degenerate\nparameter space, and the potentially unknown structural matrix further\ncomplicates the identifiability problem. This paper addresses this issue of\nidentifying the latent structure and model parameters underlying an HLAM. We\ndevelop sufficient and necessary identifiability conditions. These results\ndirectly and sharply characterize the different impacts on identifiability cast\nby different attribute types in the graph. The proposed conditions not only\nprovide insights into diagnostic test designs under the attribute hierarchy,\nbut also serve as tools to assess the validity of an estimated HLAM.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 01:27:10 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 22:50:23 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 21:46:24 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gu", "Yuqi", ""], ["Xu", "Gongjun", ""]]}, {"id": "1906.07933", "submitter": "Paul Kabaila", "authors": "Paul Kabaila, Alan H. Welsh and Christeen Wijethunga", "title": "Finite sample properties of the Buckland-Burnham-Augustin confidence\n  interval centered on a model averaged estimator", "comments": "Journal of Statistical Planning and Inference (2019)", "journal-ref": null, "doi": "10.1016/j.jspi.2019.10.004", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the confidence interval centered on a frequentist model averaged\nestimator that was proposed by Buckland, Burnham & Augustin (1997). In the\ncontext of a simple testbed situation involving two linear regression models,\nwe derive exact expressions for the confidence interval and then for the\ncoverage and scaled expected length of the confidence interval. We use these\nmeasures to explore the exact finite sample performance of the\nBuckland-Burnham-Augustin confidence interval. We also explore the limiting\nasymptotic case (as the residual degrees of freedom increases) and compare our\nresults for this case to those obtained for the asymptotic coverage of the\nconfidence interval by Hjort & Claeskens (2003).\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 06:22:21 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Kabaila", "Paul", ""], ["Welsh", "Alan H.", ""], ["Wijethunga", "Christeen", ""]]}, {"id": "1906.07957", "submitter": "Angus Lewis", "authors": "Nigel Bean, Angus Lewis, Giang Nguyen", "title": "Estimation of Markovian-regime-switching models with independent regimes", "comments": "36 pages, 5 figures, edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markovian-regime-switching (MRS) models are commonly used for modelling\neconomic time series, including electricity prices where independent regime\nmodels are used, since they can more accurately and succinctly capture\nelectricity price dynamics than dependent regime MRS models can. We can think\nof these independent regime MRS models for electricity prices as a collection\nof independent AR(1) processes, of which only one process is observed at each\ntime; which is observed is determined by a (hidden) Markov chain. Here we\ndevelop novel, computationally feasible methods for MRS models with independent\nregimes including forward, backward and EM algorithms. The key idea is to\naugment the hidden process with a counter which records the time since the\nhidden Markov chain last visited each state that corresponding to an AR(1)\nprocess.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 08:10:49 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 06:25:35 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Bean", "Nigel", ""], ["Lewis", "Angus", ""], ["Nguyen", "Giang", ""]]}, {"id": "1906.08038", "submitter": "Inez Maria Zwetsloot", "authors": "Jimoh Olawale Ajadi and Inez Maria Zwetsloot", "title": "Should Observations be Grouped for Effective Monitoring of Multivariate\n  Process Variability?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multivariate dispersion control chart monitors changes in the process\nvariability of multiple correlated quality characteristics. In this article, we\ninvestigate and compare the performance of charts designed to monitor\nvariability based on individual and grouped multivariate observations. We\ncompare one of the most well-known methods for monitoring individual\nobservations -- a multivariate EWMA chart proposed by Huwang et al -- to\nvarious charts based on grouped observations. In addition, we compare charts\nbased on monitoring with overlapping and nonoverlapping subgroups. We recommend\nusing charts based on overlapping subgroups when monitoring with subgroup data.\nThe effect of subgroup size is also investigated. Steady-state average time to\nsignal is used as performance measure. We show that monitoring methods based on\nindividual observations are the quickest in detecting sustained shifts in the\nprocess variability. We use a simulation study to obtain our results and\nillustrated these with a case study.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 12:14:33 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Ajadi", "Jimoh Olawale", ""], ["Zwetsloot", "Inez Maria", ""]]}, {"id": "1906.08096", "submitter": "Cyrus Samii", "authors": "Rajeev Dehejia, Cristian Pop-Eleches, Cyrus Samii", "title": "From Local to Global: External Validity in a Fertility Natural\n  Experiment", "comments": "forthcoming at Journal of Business and Economic Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study issues related to external validity for treatment effects using over\n100 replications of the Angrist and Evans (1998) natural experiment on the\neffects of sibling sex composition on fertility and labor supply. The\nreplications are based on census data from around the world going back to 1960.\nWe decompose sources of error in predicting treatment effects in external\ncontexts in terms of macro and micro sources of variation. In our empirical\nsetting, we find that macro covariates dominate over micro covariates for\nreducing errors in predicting treatments, an issue that past studies of\nexternal validity have been unable to evaluate. We develop methods for two\napplications to evidence-based decision-making, including determining where to\nlocate an experiment and whether policy-makers should commission new\nexperiments or rely on an existing evidence base for making a policy decision.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 13:43:37 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Dehejia", "Rajeev", ""], ["Pop-Eleches", "Cristian", ""], ["Samii", "Cyrus", ""]]}, {"id": "1906.08110", "submitter": "Adolphus Wagala", "authors": "Adolphus Wagala, Graciela Gonzalez-Far{\\i}as, Rogelio Ramos, Oscar\n  Dalmau", "title": "PLS Generalized Linear Regression and Kernel Multilogit Algorithm (KMA)\n  for Microarray Data Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We implement extensions of the partial least squares generalized linear\nregression (PLSGLR) due to Bastien et al. (2005) through its combination with\nlogistic regression and linear discriminant analysis, to get a partial least\nsquares generalized linear regression-logistic regression model (PLSGLR-log),\nand a partial least squares generalized linear regression-linear discriminant\nanalysis model (PLSGLRDA). These two classification methods are then compared\nwith classical methodologies like the k-nearest neighbours (KNN), linear\ndiscriminant analysis (LDA), partial least squares discriminant analysis\n(PLSDA), ridge partial least squares (RPLS), and support vector machines(SVM).\nFurthermore, we implement the kernel multilogit algorithm (KMA) by Dalmau et\nal. (2015)and compare its performance with that of the other classifiers. The\nresults indicate that for both un-preprocessed and preprocessed data, the KMA\nhas the lowest classification error rates.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 14:17:39 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Wagala", "Adolphus", ""], ["Gonzalez-Far\u0131as", "Graciela", ""], ["Ramos", "Rogelio", ""], ["Dalmau", "Oscar", ""]]}, {"id": "1906.08283", "submitter": "Francois-Xavier Briol", "authors": "Alessandro Barp, Francois-Xavier Briol, Andrew B. Duncan, Mark\n  Girolami, Lester Mackey", "title": "Minimum Stein Discrepancy Estimators", "comments": "Accepted for publication at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When maximum likelihood estimation is infeasible, one often turns to score\nmatching, contrastive divergence, or minimum probability flow to obtain\ntractable parameter estimates. We provide a unifying perspective of these\ntechniques as minimum Stein discrepancy estimators, and use this lens to design\nnew diffusion kernel Stein discrepancy (DKSD) and diffusion score matching\n(DSM) estimators with complementary strengths. We establish the consistency,\nasymptotic normality, and robustness of DKSD and DSM estimators, then derive\nstochastic Riemannian gradient descent algorithms for their efficient\noptimisation. The main strength of our methodology is its flexibility, which\nallows us to design estimators with desirable properties for specific models at\nhand by carefully selecting a Stein discrepancy. We illustrate this advantage\nfor several challenging problems for score matching, such as non-smooth,\nheavy-tailed or light-tailed densities.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 18:04:28 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 10:11:43 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Barp", "Alessandro", ""], ["Briol", "Francois-Xavier", ""], ["Duncan", "Andrew B.", ""], ["Girolami", "Mark", ""], ["Mackey", "Lester", ""]]}, {"id": "1906.08296", "submitter": "Ryan Martin", "authors": "Zhe Wang and Ryan Martin", "title": "Model-free posterior inference on the area under the receiver operating\n  characteristic curve", "comments": "19 pages, 3 figures, 5 tables", "journal-ref": "Journal of Statistical Planning and Inference, 2020, volume 209,\n  pages 174--186", "doi": "10.1016/j.jspi.2020.03.008", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area under the receiver operating characteristic curve (AUC) serves as a\nsummary of a binary classifier's performance. Methods for estimating the AUC\nhave been developed under a binormality assumption which restricts the\ndistribution of the score produced by the classifier. However, this assumption\nintroduces an infinite-dimensional nuisance parameter and can be inappropriate,\nespecially in the context of machine learning. This motivates us to adopt a\nmodel-free Gibbs posterior distribution for the AUC. We present the asymptotic\nGibbs posterior concentration rate, and a strategy for tuning the learning rate\nso that the corresponding credible intervals achieve the nominal frequentist\ncoverage probability. Simulation experiments and a real data analysis\ndemonstrate the Gibbs posterior's strong performance compared to existing\nmethods based on a rank likelihood.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 18:36:52 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 20:33:53 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wang", "Zhe", ""], ["Martin", "Ryan", ""]]}, {"id": "1906.08357", "submitter": "Liying Luo", "authors": "Liying Luo and James Hodges", "title": "The Age-Period-Cohort-Interaction Model for Describing and Investigating\n  Inter-Cohort Deviations and Intra-Cohort Life-Course Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social scientists have frequently sought to understand the distinct effects\nof age, period, and cohort, but disaggregation of the three dimensions is\ndifficult because cohort = period - age. We argue that this technical\ndifficulty reflects a disconnection between how cohort effect is conceptualized\nand how it is modeled in the traditional age-period-cohort framework. We\npropose a new method, called the age-period-cohort-interaction (APC-I) model,\nthat is qualitatively different from previous methods in that it represents\nRyder's (1965) theoretical account about the conditions under which cohort\ndifferentiation may arise. This APC-I model does not require problematic\nstatistical assumptions and the interpretation is straightforward. It\nquantifies inter-cohort deviations from the age and period main effects and\nalso permits hypothesis testing about intra-cohort life-course dynamics. We\ndemonstrate how this new model can be used to examine age, period, and cohort\npatterns in women's labor force participation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 01:33:35 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Luo", "Liying", ""], ["Hodges", "James", ""]]}, {"id": "1906.08359", "submitter": "Andrew L. Johnson", "authors": "Daisuke Yagi, Yining Chen, Andrew L. Johnson, and Hiroshi Morita", "title": "An axiomatic nonparametric production function estimator: Modeling\n  production in Japan's cardboard industry", "comments": "81 pages, 40 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach to estimate a production function based on the\neconomic axioms of the Regular Ultra Passum law and convex non-homothetic input\nisoquants. Central to the development of our estimator is stating the axioms as\nshape constraints and using shape constrained nonparametric regression methods.\nWe implement this approach using data from the Japanese corrugated cardboard\nindustry from 1997-2007. Using this new approach, we find most productive scale\nsize is a function of the capital-to-labor ratio and the largest firms operate\nclose to the largest most productive scale size associated with a high\ncapital-to-labor ratio. We measure the productivity growth across the panel\nperiods based on the residuals from our axiomatic model. We also decompose\nproductivity into scale, input mix, and unexplained effects to clarify the\nsources the productivity differences and provide managers guidance to make\nfirms more productive.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 21:19:09 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Yagi", "Daisuke", ""], ["Chen", "Yining", ""], ["Johnson", "Andrew L.", ""], ["Morita", "Hiroshi", ""]]}, {"id": "1906.08420", "submitter": "Tirthankar Dasgupta", "authors": "Rahul Mukerjee and Tirthankar Dasgupta", "title": "Causal Inference from Possibly Unbalanced Split-Plot Designs: A\n  Randomization-based Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Split-plot designs find wide applicability in multifactor experiments with\nrandomization restrictions. Practical considerations often warrant the use of\nunbalanced designs. This paper investigates randomization based causal\ninference in split-plot designs that are possibly unbalanced. Extension of\nideas from the recently studied balanced case yields an expression for the\nsampling variance of a treatment contrast estimator as well as a conservative\nestimator of the sampling variance. However, the bias of this variance\nestimator does not vanish even when the treatment effects are strictly\nadditive. A careful and involved matrix analysis is employed to overcome this\ndifficulty, resulting in a new variance estimator, which becomes unbiased under\nmilder conditions. A construction procedure that generates such an estimator\nwith minimax bias is proposed.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 02:35:23 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Mukerjee", "Rahul", ""], ["Dasgupta", "Tirthankar", ""]]}, {"id": "1906.08428", "submitter": "Shonosuke Sugasawa", "authors": "Tsubasa Ito and Shonosuke Sugasawa", "title": "Improved Confidence Regions in Meta-analysis of Diagnostic Test Accuracy", "comments": "15 pages (main text) + 10 pages (supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analyses of diagnostic test accuracy (DTA) studies have been gathering\nattention in research in clinical epidemiology and health technology\ndevelopment, and bivariate random-effects model is becoming a standard tool.\nHowever, standard inference methods usually underestimate statistical errors\nand possibly provide highly overconfident results under realistic situations\nsince they ignore the variability in the estimation of variance parameters. To\novercome the difficulty, a new improved inference method, namely, an accurate\nconfidence region for the meta-analysis of DTA, by asymptotically expanding the\ncoverage probability of the standard confidence region. The advantage of the\nproposed confidence region is that it holds a relatively simple expression and\ndoes not require any repeated calculations such as Bootstrap or Monte Carlo\nmethods to compute the region, thereby the proposed method can be easily\ncarried out in practical applications. The effectiveness of the proposed method\nis demonstrated through simulation studies and an application to meta-analysis\nof screening test accuracy for alcohol problems.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 03:23:59 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 08:13:35 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Ito", "Tsubasa", ""], ["Sugasawa", "Shonosuke", ""]]}, {"id": "1906.08436", "submitter": "Zhenke Wu", "authors": "Zhenke Wu, Irena Chen", "title": "Regression Analysis of Dependent Binary Data for Estimating Disease\n  Etiology from Case-Control Studies", "comments": "26 pages of main text, 3 figures and 1 table; 18 pages of\n  supplementary materials, 6 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In large-scale disease etiology studies, epidemiologists often need to use\nmultiple binary measures of unobserved causes of disease that are not perfectly\nsensitive or specific to estimate cause-specific case fractions, referred to as\n\"population etiologic fractions\" (PEFs). Despite recent methodological\nadvances, the scientific need of incorporating control data to estimate the\neffect of explanatory variables upon the PEFs, however, remains unmet. In this\npaper, we build on and extend nested partially-latent class model (npLCMs, Wu\net al., 2017) to a general framework for etiology regression analysis in\ncase-control studies. Data from controls provide requisite information about\nmeasurement specificities and covariations, which is used to correctly assign\ncause-specific probabilities for each case given her measurements. We estimate\nthe distribution of the controls' diagnostic measures given the covariates via\na separate regression model and a priori encourage simpler conditional\ndependence structures. We use Markov chain Monte Carlo for posterior inference\nof the PEF functions, cases' latent classes and the overall PEFs of policy\ninterest. We illustrate the regression analysis with simulations and show less\nbiased estimation and more valid inference of the overall PEFs than an npLCM\nanalysis omitting covariates. A regression analysis of data from a childhood\npneumonia study site reveals the dependence of pneumonia etiology upon season,\nage, disease severity and HIV status.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 04:06:50 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Wu", "Zhenke", ""], ["Chen", "Irena", ""]]}, {"id": "1906.08444", "submitter": "Vladislav Beresovsky", "authors": "Vladislav Beresovsky", "title": "On application of a response propensity model to estimation from web\n  samples", "comments": "arXiv admin note: This submission has been removed by arXiv\n  administrators as the submitter did not have the right to agree to the\n  license at the time of submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing nonresponse rates and the cost of data collection are two pressing\nproblems encountered in traditional probability surveys. The proliferation of\ninexpensive data from web surveys stimulates interest in statistical techniques\nfor valid inferences from web samples. We consider estimation of population and\ndomain means in the two-sample setup, where the web sample contains variables\nof interest and covariates that are shared with an auxiliary probability survey\nsample. First, we propose an estimator of population mean, based on the\nestimated propensity of response to a web survey. This makes inferences from\nweb samples that are similar to well-established techniques used for\nobservational studies and missing data problems. Second, we propose an\n'implicit' logistic regression for estimating parameters of the web response\nmodel in the two-sample setup. Implicit logistic regression uses selection\nprobabilities, nominally defined for web sample units, and the size of the\nhypothetic population of responders to a web survey. A simulation study\nconfirms the validity of implicit logistic regression and its higher efficiency\ncomparing to alternative estimators of web response propensity.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 04:53:03 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 22:53:34 GMT"}], "update_date": "2019-12-31", "authors_parsed": [["Beresovsky", "Vladislav", ""]]}, {"id": "1906.08451", "submitter": "Behtash Babadi", "authors": "Proloy Das and Behtash Babadi", "title": "Multitaper Spectral Analysis of Neuronal Spiking Activity Driven by\n  Latent Stationary Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigating the spectral properties of the neural covariates that underlie\nspiking activity is an important problem in systems neuroscience, as it allows\nto study the role of brain rhythms in cognitive functions. While the spectral\nestimation of continuous time-series is a well-established domain, computing\nthe spectral representation of these neural covariates from spiking data sets\nforth various challenges due to the intrinsic non-linearities involved. In this\npaper, we address this problem by proposing a variant of the multitaper method\nspecifically tailored for point process data. To this end, we construct\nauxiliary spiking statistics from which the eigen-spectra of the underlying\nlatent process can be directly inferred using maximum likelihood estimation,\nand thereby the multitaper estimate can be efficiently computed. Comparison of\nour proposed technique to existing methods using simulated data reveals\nsignificant gains in terms of the bias-variance trade-off.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 05:35:41 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Das", "Proloy", ""], ["Babadi", "Behtash", ""]]}, {"id": "1906.08522", "submitter": "Christian Rohrbeck", "authors": "Christian Rohrbeck and Jonathan A Tawn", "title": "Bayesian spatial clustering of extremal behaviour for hydrological\n  variables", "comments": "41", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the need for efficient inference for a range of hydrological\nextreme value problems, spatial pooling of information is the standard approach\nfor marginal tail estimation. We propose the first extreme value spatial\nclustering methods which account for both the similarity of the marginal tails\nand the spatial dependence structure of the data to determine the appropriate\nlevel of pooling. Spatial dependence is incorporated in two ways: to determine\nthe cluster selection and to account for dependence of the data over sites\nwithin a cluster when making the marginal inference. We introduce a statistical\nmodel for the pairwise extremal dependence which incorporates distance between\nsites, and accommodates our belief that sites within the same cluster tend to\nexhibit a higher degree of dependence than sites in different clusters. We use\na Bayesian framework which learns about both the number of clusters and their\nspatial structure, and that enables the inference of site-specific marginal\ndistributions of extremes to incorporate uncertainty in the clustering\nallocation. The approach is illustrated using simulations, the analysis of\ndaily precipitation levels in Norway and daily river flow levels in the UK.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 09:41:28 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Rohrbeck", "Christian", ""], ["Tawn", "Jonathan A", ""]]}, {"id": "1906.08577", "submitter": "Ioannis Kalogridis Mr", "authors": "Ioannis Kalogridis and Stefan Van Aelst", "title": "M-type penalized splines with auxiliary scale estimation", "comments": null, "journal-ref": "Journal of Statistical Planning and Inference, 2021", "doi": "10.1016/j.jspi.2020.09.004", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized spline smoothing is a popular and flexible method of obtaining\nestimates in nonparametric regression but the classical least-squares criterion\nis highly susceptible to model deviations and atypical observations. Penalized\nspline estimation with a resistant loss function is a natural remedy, yet to\nthis day the asymptotic properties of M-type penalized spline estimators have\nnot been studied. We show in this paper that M-type penalized spline estimators\nachieve the same rates of convergence as their least-squares counterparts, even\nwith auxiliary scale estimation. We further find theoretical justification for\nthe use of a small number of knots relative to the sample size. We illustrate\nthe benefits of M-type penalized splines in a Monte-Carlo study and two\nreal-data examples, which contain atypical observations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 12:33:47 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 14:59:06 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 09:33:48 GMT"}, {"version": "v4", "created": "Fri, 6 Mar 2020 11:12:39 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Kalogridis", "Ioannis", ""], ["Van Aelst", "Stefan", ""]]}, {"id": "1906.08599", "submitter": "Huafeng Xu", "authors": "Huafeng Xu", "title": "Optimal measurement network of pairwise differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When both the difference between two quantities and their individual values\ncan be measured or computational predicted, multiple quantities can be\ndetermined from the measurements or predictions of select individual quantities\nand select pairwise differences. These measurements and predictions form a\nnetwork connecting the quantities through their differences. Here, I analyze\nthe optimization of such networks, where the trace ($A$-optimal), the largest\neigenvalue ($E$-optimal), or the determinant ($D$-optimal) of the covariance\nmatrix associated with the estimated quantities are minimized with respect to\nthe allocation of the measurement (or computational) cost to different\nmeasurements (or predictions). My statistical analysis of the performance of\nsuch optimal measurement networks -- based on large sets of simulated data --\nsuggests that they substantially accelerate the determination of the\nquantities, and that they may be useful in applications such as the\ncomputational prediction of binding free energies of candidate drug molecules.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 01:40:05 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 04:39:19 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 23:30:08 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Xu", "Huafeng", ""]]}, {"id": "1906.08726", "submitter": "Tenglong Li", "authors": "Tenglong Li, Kenneth A. Frank", "title": "On the probability of a causal inference is robust for internal validity", "comments": "33 pages , 4 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internal validity of observational study is often subject to debate. In\nthis study, we define the counterfactuals as the unobserved sample and intend\nto quantify its relationship with the null hypothesis statistical testing\n(NHST). We propose the probability of a causal inference is robust for internal\nvalidity, i.e., the PIV, as a robustness index of causal inference. Formally,\nthe PIV is the probability of rejecting the null hypothesis again based on both\nthe observed sample and the counterfactuals, provided the same null hypothesis\nhas already been rejected based on the observed sample. Under either\nfrequentist or Bayesian framework, one can bound the PIV of an inference based\non his bounded belief about the counterfactuals, which is often needed when the\nunconfoundedness assumption is dubious. The PIV is equivalent to statistical\npower when the NHST is thought to be based on both the observed sample and the\ncounterfactuals. We summarize the process of evaluating internal validity with\nthe PIV into an eight-step procedure and illustrate it with an empirical\nexample (i.e., Hong and Raudenbush (2005)).\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 16:13:03 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Li", "Tenglong", ""], ["Frank", "Kenneth A.", ""]]}, {"id": "1906.08735", "submitter": "Khanh To Duc", "authors": "Duc-Khanh To, Gianfranco Adimari and Monica Chiogna", "title": "Improving estimation of the volume under the ROC surface when data are\n  missing not at random", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a mean score equation-based approach to estimate\nthe the volume under the receiving operating characteristic (ROC) surface (VUS)\nof a diagnostic test, under nonignorable (NI) verification bias. The proposed\napproach involves a parametric regression model for the verification process,\nwhich accommodates for possible NI missingness in the disease status of sample\nsubjects, and may use instrumental variables, which help avoid possible\nidentifiability problems. In order to solve the mean score equation derived by\nthe chosen verification model, we preliminarily need to estimate the parameters\nof a model for the disease process, but its specification is required only for\nverified subjects under study. Then, by using the estimated verification and\ndisease probabilities, we obtain four verification bias-corrected VUS\nestimators, which are alternative to those recently proposed by To Duc et al.\n(2019), based on a full likelihood approach. Consistency and asymptotic\nnormality of the new estimators are established. Simulation experiments are\nconducted to evaluate their finite sample performances, and an application to a\ndataset from a research on epithelial ovarian cancer is presented.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 16:32:06 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["To", "Duc-Khanh", ""], ["Adimari", "Gianfranco", ""], ["Chiogna", "Monica", ""]]}, {"id": "1906.08755", "submitter": "Corli van Zyl Dr", "authors": "C van Zyl and F Lombard", "title": "Signed Sequential Rank Shiryaev-Roberts Schemes", "comments": "Submitted to Quality and Reliability Engineering International", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Shiryaev-Roberts schemes based on signed sequential ranks to\ndetect a persistent change in location of a continuous symmetric distribution\nwith known median. The in-control properties of these schemes are distribution\nfree, hence they do not require a parametric specification of an underlying\ndensity function or the existence of any moments. Tables of control limits are\nprovided. The out-of-control average run length properties of the schemes are\ngauged via theory-based calculations and Monte Carlo simulation. Comparisons\nare made with two existing distribution-free schemes. We conclude that the\nnewly proposed scheme has much to recommend its use in practice. Implementation\nof the methodology is illustrated in an application to a data set from an\nindustrial environment.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 17:16:50 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 13:22:57 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["van Zyl", "C", ""], ["Lombard", "F", ""]]}, {"id": "1906.08843", "submitter": "Arnab Chakraborty", "authors": "Arnab Chakraborty and Soumendra N. Lahiri", "title": "On Statistical Properties of A Veracity Scoring Method for Spatial Data", "comments": "37 pages, 4 figures, 6 tables, submitted to JRSS-B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring veracity or reliability of noisy data is of utmost importance,\nespecially in the scenarios where the information are gathered through\nautomated systems. In a recent paper, Chakraborty et. al. (2019) have\nintroduced a veracity scoring technique for geostatistical data. The authors\nhave used a high-quality `reference' data to measure the veracity of the\nvarying-quality observations and incorporated the veracity scores in their\nanalysis of mobile-sensor generated noisy weather data to generate efficient\npredictions of the ambient temperature process. In this paper, we consider the\nscenario when no reference data is available and hence, the veracity scores\n(referred as VS) are defined based on `local' summaries of the observations. We\ndevelop a VS-based estimation method for parameters of a spatial regression\nmodel. Under a non-stationary noise structure and fairly general assumptions on\nthe underlying spatial process, we show that the VS-based estimators of the\nregression parameters are consistent. Moreover, we establish the advantage of\nthe VS-based estimators as compared to the ordinary least squares (OLS)\nestimator by analyzing their asymptotic mean squared errors. We illustrate the\nmerits of the VS-based technique through simulations and apply the methodology\nto a real data set on mass percentages of ash in coal seams in Pennsylvania.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 20:47:38 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Chakraborty", "Arnab", ""], ["Lahiri", "Soumendra N.", ""]]}, {"id": "1906.08850", "submitter": "Topi Paananen", "authors": "Topi Paananen, Juho Piironen, Paul-Christian B\\\"urkner, Aki Vehtari", "title": "Implicitly Adaptive Importance Sampling", "comments": "Major revision: More comparisons to adaptive importance sampling with\n  parametric distributions", "journal-ref": "Stat Comput 31, 16 (2021)", "doi": "10.1007/s11222-020-09982-2", "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive importance sampling is a class of techniques for finding good\nproposal distributions for importance sampling. Often the proposal\ndistributions are standard probability distributions whose parameters are\nadapted based on the mismatch between the current proposal and a target\ndistribution. In this work, we present an implicit adaptive importance sampling\nmethod that applies to complicated distributions which are not available in\nclosed form. The method iteratively matches the moments of a set of Monte Carlo\ndraws to weighted moments based on importance weights. We apply the method to\nBayesian leave-one-out cross-validation and show that it performs better than\nmany existing parametric adaptive importance sampling methods while being\ncomputationally inexpensive.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 21:16:35 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 12:16:20 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Paananen", "Topi", ""], ["Piironen", "Juho", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Vehtari", "Aki", ""]]}, {"id": "1906.08882", "submitter": "Zhong Guan", "authors": "Zhong Guan", "title": "Maximum Approximate Bernstein Likelihood Estimation in Proportional\n  Hazard Model for Interval-Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum approximate Bernstein likelihood estimates of the baseline density\nfunction and the regression coefficients in the proportional hazard regression\nmodels based on interval-censored event time data are proposed. This results in\nnot only a smooth estimate of the survival function which enjoys faster\nconvergence rate but also improved estimates of the regression coefficients.\nSimulation shows that the finite sample performance of the proposed method is\nbetter than the existing ones. The proposed method is illustrated by real data\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 22:29:42 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 06:27:21 GMT"}, {"version": "v3", "created": "Fri, 23 Aug 2019 20:07:37 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Guan", "Zhong", ""]]}, {"id": "1906.08884", "submitter": "Yuchao Liu", "authors": "Yuchao Liu, Ery Arias-Castro", "title": "A Multiscale Scan Statistic for Adaptive Submatrix Localization", "comments": "The original version was accepted by KDD2019 Research Track. Detail\n  of the proof is available at https://escholarship.org/uc/item/9wt627dg", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of localizing a submatrix with larger-than-usual\nentry values inside a data matrix, without the prior knowledge of the submatrix\nsize. We establish an optimization framework based on a multiscale scan\nstatistic, and develop algorithms in order to approach the optimizer. We also\nshow that our estimator only requires a signal strength of the same order as\nthe minimax estimator with oracle knowledge of the submatrix size, to exactly\nrecover the anomaly with high probability. We perform some simulations that\nshow that our estimator has superior performance compared to other estimators\nwhich do not require prior submatrix knowledge, while being comparatively\nfaster to compute.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 22:36:12 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Liu", "Yuchao", ""], ["Arias-Castro", "Ery", ""]]}, {"id": "1906.09004", "submitter": "Tom\\'a\\v{s} Mrkvi\\v{c}ka", "authors": "Tomas Mrkvicka, Mari Myllymaki, Mikko Kuronen, Naveen Naidu Narisetty", "title": "New methods for multiple testing in permutation inference for the\n  general linear model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation methods are commonly used to test significance of regressors of\ninterest in general linear models (GLMs) for functional (image) data sets, in\nparticular for neuroimaging applications as they rely on mild assumptions.\nPermutation inference for GLMs typically consists of three parts: choosing a\nrelevant test statistic, computing pointwise permutation tests and applying a\nmultiple testing correction. We propose new multiple testing methods as an\nalternative to the commonly used maximum value of test statistics across the\nimage. The new methods improve power and robustness against inhomogeneity of\nthe test statistic across its domain. The methods rely on sorting the permuted\nfunctional test statistics based on pointwise rank measures; still they can be\nimplemented even for large brain data. The performance of the methods is\ndemonstrated through a designed simulation experiment, and an example of brain\nimaging data. We developed the R package GET which can be used for computation\nof the proposed procedures.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 08:27:02 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 05:44:48 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 12:19:31 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Mrkvicka", "Tomas", ""], ["Myllymaki", "Mari", ""], ["Kuronen", "Mikko", ""], ["Narisetty", "Naveen Naidu", ""]]}, {"id": "1906.09175", "submitter": "Zhigang Li", "authors": "Quran Wu, A. James O'Malley, Janaka S.S. Liyanage, Susmita Datta, Raad\n  Z. Gharaibeh, Christian Jobin, Modupe O. Coker, Anne G. Hoen, Brock C.\n  Christensen, Juliette C. Madan, Margaret R. Karagas, Zhigang Li", "title": "MedZIM: Mediation analysis for Zero-Inflated Mediators with applications\n  to microbiome data", "comments": "Corresponding: Zhigang Li", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human microbiome can contribute to pathogeneses of many complex diseases\nby mediating disease-leading causal pathways. However, standard mediation\nanalysis methods are not adequate to analyze the microbiome as a mediator due\nto the excessive number of zero-valued sequencing reads in the data. The two\nmain challenges raised by the zero-inflated data structure are: (a)\ndisentangling the mediation effect induced by the point mass at zero; and (b)\nidentifying the observed zero-valued data points that are actually not zero\n(i.e., false zeros). We develop a novel mediation analysis method under the\npotential-outcomes framework to fill this gap. We show that the mediation\neffect of the microbiome can be decomposed into two components that are\ninherent to the two-part nature of zero-inflated distributions. With\nprobabilistic models to account for observing zeros, we also address the\nchallenge with false zeros. A comprehensive simulation study and the\napplications in two real microbiome studies demonstrate that our approach\noutperforms existing mediation analysis approaches.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 14:53:13 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 16:45:28 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 21:04:35 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Wu", "Quran", ""], ["O'Malley", "A. James", ""], ["Liyanage", "Janaka S. S.", ""], ["Datta", "Susmita", ""], ["Gharaibeh", "Raad Z.", ""], ["Jobin", "Christian", ""], ["Coker", "Modupe O.", ""], ["Hoen", "Anne G.", ""], ["Christensen", "Brock C.", ""], ["Madan", "Juliette C.", ""], ["Karagas", "Margaret R.", ""], ["Li", "Zhigang", ""]]}, {"id": "1906.09222", "submitter": "Sergio G\\'omez", "authors": "Alberto Fern\\'andez, Sergio G\\'omez", "title": "Versatile linkage: a family of space-conserving strategies for\n  agglomerative hierarchical clustering", "comments": "To appear in Journal of Classification. Software for Versatile\n  linkage available at http://deim.urv.cat/~sergio.gomez/multidendrograms.php", "journal-ref": "Journal of Classification 37 (2020) 584-597", "doi": "10.1007/s00357-019-09339-z", "report-no": null, "categories": "stat.ME cs.IR physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agglomerative hierarchical clustering can be implemented with several\nstrategies that differ in the way elements of a collection are grouped together\nto build a hierarchy of clusters. Here we introduce versatile linkage, a new\ninfinite system of agglomerative hierarchical clustering strategies based on\ngeneralized means, which go from single linkage to complete linkage, passing\nthrough arithmetic average linkage and other clustering methods yet unexplored\nsuch as geometric linkage and harmonic linkage. We compare the different\nclustering strategies in terms of cophenetic correlation, mean absolute error,\nand also tree balance and space distortion, two new measures proposed to\ndescribe hierarchical trees. Unlike the $\\beta$-flexible clustering system, we\nshow that the versatile linkage family is space-conserving.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 16:10:24 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Fern\u00e1ndez", "Alberto", ""], ["G\u00f3mez", "Sergio", ""]]}, {"id": "1906.09303", "submitter": "Joseph Antonelli", "authors": "Joseph Antonelli and Matthew Cefalu", "title": "Averaging causal estimators in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been increasing interest in recent years in the development of\napproaches to estimate causal effects when the number of potential confounders\nis prohibitively large. This growth in interest has led to a number of\npotential estimators one could use in this setting. Each of these estimators\nhas different operating characteristics, and it is unlikely that one estimator\nwill outperform all others across all possible scenarios. Coupling this with\nthe fact that an analyst can never know which approach is best for their\nparticular data, we propose a synthetic estimator that averages over a set of\ncandidate estimators. Averaging is widely used in statistics for problems such\nas prediction, where there are many possible models, and averaging can improve\nperformance and increase robustness to using incorrect models. We show that\nthese ideas carry over into the estimation of causal effects in\nhigh-dimensional scenarios. We show theoretically that averaging provides\nrobustness against choosing a bad model, and show empirically via simulation\nthat the averaging estimator performs quite well, and in most cases nearly as\nwell as the best among all possible candidate estimators. Finally, we\nillustrate these ideas in an environmental wide association study and see that\naveraging provides the largest benefit in the more difficult scenarios that\nhave large numbers of confounders.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 19:45:17 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 03:15:37 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Antonelli", "Joseph", ""], ["Cefalu", "Matthew", ""]]}, {"id": "1906.09359", "submitter": "Behtash Babadi", "authors": "Anuththara Rupasinghe and Behtash Babadi", "title": "Multitaper Analysis of Evolutionary Spectra from Multivariate Spiking\n  Observations", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.3010197", "report-no": null, "categories": "cs.IT cs.SY eess.SY math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting the spectral representations of the neural processes that underlie\nspiking activity is key to understanding how the brain rhythms mediate\ncognitive functions. While spectral estimation of continuous time-series is\nwell studied, inferring the spectral representation of latent non-stationary\nprocesses based on spiking observations is a challenging problem. In this\npaper, we address this issue by developing a multitaper spectral estimation\nmethodology that can be directly applied to multivariate spiking observations\nin order to extract the evolutionary spectral density of the latent\nnon-stationary processes that drive spiking activity, based on point process\ntheory. We establish theoretical bounds on the bias-variance trade-off of the\nproposed estimator. Finally, we compare the performance of our proposed\ntechnique with existing methods using simulation studies and application to\nreal data, which reveal significant gains in terms of the bias-variance\ntrade-off.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 00:42:47 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Rupasinghe", "Anuththara", ""], ["Babadi", "Behtash", ""]]}, {"id": "1906.09388", "submitter": "Leming Qu", "authors": "Leming Qu and Yang Lu", "title": "Copula Density Estimation by Finite Mixture of Parametric Copula\n  Densities", "comments": null, "journal-ref": null, "doi": "10.1080/03610918.2019.1622720", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Copula density estimation method that is based on a finite mixture of\nheterogeneous parametric copula densities is proposed here. More specifically,\nthe mixture components are Clayton, Frank, Gumbel, T, and normal copula\ndensities, which are capable of capturing lower tail,strong central, upper\ntail, heavy tail, and symmetrical elliptical dependence, respectively. The\nmodel parameters are estimated by an interior-point algorithm for the\nconstrained maximum likelihood problem. The interior-point algorithm is\ncompared with the commonly used EM algorithm. Simulation and real data\napplication show that the proposed approach is effective to model complex\ndependencies for data in dimensions beyond two or three.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 05:11:35 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Qu", "Leming", ""], ["Lu", "Yang", ""]]}, {"id": "1906.09473", "submitter": "Yang Liu", "authors": "Yang Liu and David Ruppert", "title": "Density Estimation on a Network", "comments": "38 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a novel approach to density estimation on a network. We\nformulate nonparametric density estimation on a network as a nonparametric\nregression problem by binning. Nonparametric regression using local polynomial\nkernel-weighted least squares have been studied rigorously, and its asymptotic\nproperties make it superior to kernel estimators such as the Nadaraya-Watson\nestimator. When applied to a network, the best estimator near a vertex depends\non the amount of smoothness at the vertex. Often, there are no compelling\nreasons to assume that a density will be continuous or discontinuous at a\nvertex, hence a data driven approach is proposed. To estimate the density in a\nneighborhood of a vertex, we propose a two-step procedure. The first step of\nthis pretest estimator fits a separate local polynomial regression on each edge\nusing data only on that edge, and then tests for equality of the estimates at\nthe vertex. If the null hypothesis is not rejected, then the second step\nre-estimates the regression function in a small neighborhood of the vertex,\nsubject to a joint equality constraint. Since the derivative of the density may\nbe discontinuous at the vertex, we propose a piecewise polynomial local\nregression estimate to model the change in slope. We study in detail the\nspecial case of local piecewise linear regression and derive the leading bias\nand variance terms using weighted least squares theory. We show that the\nproposed approach will remove the bias near a vertex that has been noted for\nexisting methods, which typically do not allow for discontinuity at vertices.\nFor a fixed network, the proposed method scales sub-linearly with sample size\nand it can be extended to regression and varying coefficient models on a\nnetwork. We demonstrate the workings of the proposed model by simulation\nstudies and apply it to a dendrite network data set.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 17:22:56 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 18:04:30 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Liu", "Yang", ""], ["Ruppert", "David", ""]]}, {"id": "1906.09581", "submitter": "Qiang Sun", "authors": "Chenyu Liu, Qiang Sun, Kean Ming Tan", "title": "Robust convex clustering: How does fusion penalty enhance robustness?", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex clustering has gained popularity recently due to its desirable\nperformance in empirical studies. It involves solving a convex optimization\nproblem with the cost function being a squared error loss plus a fusion penalty\nthat encourages the estimated centroids for observations in the same cluster to\nbe identical. However, when data are contaminated, convex clustering with a\nsquared error loss will fail to identify correct cluster memberships even when\nthere is only one arbitrary outlier. To address this challenge, we propose a\nrobust convex clustering method. Theoretically, we show that the new estimator\nis resistant to arbitrary outliers: it does not break down until more than half\nof the observations are arbitrary outliers. In particular, we observe a new\nphenomenon that the fusion penalty can help enhance robustness. Numerical\nstudies are performed to demonstrate the competitive performance of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 10:52:07 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 08:44:57 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Liu", "Chenyu", ""], ["Sun", "Qiang", ""], ["Tan", "Kean Ming", ""]]}, {"id": "1906.09712", "submitter": "Steven Howard", "authors": "Steven R. Howard, Aaditya Ramdas", "title": "Sequential estimation of quantiles with applications to A/B-testing and\n  best-arm identification", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose confidence sequences -- sequences of confidence intervals which\nare valid uniformly over time -- for quantiles of any distribution over a\ncomplete, fully-ordered set, based on a stream of i.i.d. observations. We give\nmethods both for tracking a fixed quantile and for tracking all quantiles\nsimultaneously. Specifically, we provide explicit expressions with small\nconstants for intervals whose widths shrink at the fastest possible\n$\\sqrt{t^{-1} \\log\\log t}$ rate, along with a non-asymptotic concentration\ninequality for the empirical distribution function which holds uniformly over\ntime with the same rate. The latter strengthens Smirnov's empirical process law\nof the iterated logarithm and extends the Dvoretzky-Kiefer-Wolfowitz inequality\nto hold uniformly over time. We give a new algorithm and sample complexity\nbound for selecting an arm with an approximately best quantile in a multi-armed\nbandit framework. In simulations, our method requires fewer samples than\nexisting methods by a factor of five to fifty.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 03:47:58 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 18:51:58 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 03:58:30 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Howard", "Steven R.", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1906.09735", "submitter": "Victor Coscrato", "authors": "Victor Coscrato, Marco Henrique de Almeida In\\'acio, Rafael Izbicki", "title": "The NN-Stacking: Feature weighted linear stacking through neural\n  networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2020.02.073", "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacking methods improve the prediction performance of regression models. A\nsimple way to stack base regressions estimators is by combining them linearly,\nas done by \\citet{breiman1996stacked}. Even though this approach is useful from\nan interpretative perspective, it often does not lead to high predictive power.\nWe propose the NN-Stacking method (NNS), which generalizes Breiman's method by\nallowing the linear parameters to vary with input features. This improvement\nenables NNS to take advantage of the fact that distinct base models often\nperform better at different regions of the feature space. Our method uses\nneural networks to estimate the stacking coefficients. We show that while our\napproach keeps the interpretative features of Breiman's method at a local\nlevel, it leads to better predictive power, especially in datasets with large\nsample sizes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 05:46:07 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Coscrato", "Victor", ""], ["In\u00e1cio", "Marco Henrique de Almeida", ""], ["Izbicki", "Rafael", ""]]}, {"id": "1906.09757", "submitter": "Xuan Yin", "authors": "Xuan Yin and Liangjie Hong", "title": "The Identification and Estimation of Direct and Indirect Effects in A/B\n  Tests through Causal Mediation Analysis", "comments": "Accepted by The 25th ACM SIGKDD Conference on Knowledge Discovery and\n  DataMining (KDD '19), August 4-8, 2019, Anchorage, AK, USA", "journal-ref": "The 25th ACM SIGKDD Conference on Knowledge Discovery and Data\n  Mining (KDD '19), August 4-8, 2019, Anchorage, AK, USA. ACM, New York, NY,\n  USA", "doi": "10.1145/3292500.3330769", "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-commerce companies have a number of online products, such as organic\nsearch, sponsored search, and recommendation modules, to fulfill customer\nneeds. Although each of these products provides a unique opportunity for users\nto interact with a portion of the overall inventory, they are all similar\nchannels for users and compete for limited time and monetary budgets of users.\nTo optimize users' overall experiences on an E-commerce platform, instead of\nunderstanding and improving different products separately, it is important to\ngain insights into the evidence that a change in one product would induce users\nto change their behaviors in others, which may be due to the fact that these\nproducts are functionally similar. In this paper, we introduce causal mediation\nanalysis as a formal statistical tool to reveal the underlying causal\nmechanisms. Existing literature provides little guidance on cases where\nmultiple unmeasured causally-dependent mediators exist, which are common in A/B\ntests. We seek a novel approach to identify in those scenarios direct and\nindirect effects of the treatment. In the end, we demonstrate the effectiveness\nof the proposed method in data from Etsy's real A/B tests and shed lights on\ncomplex relationships between different products.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 07:22:30 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Yin", "Xuan", ""], ["Hong", "Liangjie", ""]]}, {"id": "1906.09982", "submitter": "Alberto Ferrari", "authors": "Alberto Ferrari", "title": "A note on sum and difference of correlated chi-squared variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate distributions for sum and difference of linearly correlated\n$\\chi^{2}$ distributed random variables are derived. It is shown that they can\nbe reduced to conveniently parametrized gamma and Variance-Gamma distributions,\nrespectively. The proposed distributions are very flexible, and the one for sum\nin particular has straight-forward generalizations to cases where multiple\n$\\chi^{2}$ variables with different parameters are involved. The results\npromptly extend to every sum of gamma variables with common scale and to every\ndifference between gamma variables with common shape and scale. The fit of the\ndistributions is tested on simulated data with remarkable results.The\napproximations presented are expected to be especially useful to researchers\nworking on gamma-distributed variables.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 14:12:27 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Ferrari", "Alberto", ""]]}, {"id": "1906.10026", "submitter": "Jes\\'us Daniel Arroyo Reli\\'on", "authors": "Jes\\'us Arroyo, Avanti Athreya, Joshua Cape, Guodong Chen, Carey E.\n  Priebe, and Joshua T. Vogelstein", "title": "Inference for multiple heterogeneous networks with a common invariant\n  subspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of models for multiple heterogeneous network data is of\ncritical importance both in statistical network theory and across multiple\napplication domains. Although single-graph inference is well-studied, multiple\ngraph inference is largely unexplored, in part because of the challenges\ninherent in appropriately modeling graph differences and yet retaining\nsufficient model simplicity to render estimation feasible. This paper addresses\nexactly this gap, by introducing a new model, the common subspace\nindependent-edge (COSIE) multiple random graph model, which describes a\nheterogeneous collection of networks with a shared latent structure on the\nvertices but potentially different connectivity patterns for each graph. The\nCOSIE model encompasses many popular network representations, including the\nstochastic blockmodel. The model is both flexible enough to meaningfully\naccount for important graph differences and tractable enough to allow for\naccurate inference in multiple networks. In particular, a joint spectral\nembedding of adjacency matrices - the multiple adjacency spectral embedding\n(MASE) - leads, in a COSIE model, to simultaneous consistent estimation of\nunderlying parameters for each graph. Under mild additional assumptions, MASE\nestimates satisfy asymptotic normality and yield improvements for graph\neigenvalue estimation and hypothesis testing. In both simulated and real data,\nthe COSIE model and the MASE embedding can be deployed for a number of\nsubsequent network inference tasks, including dimensionality reduction,\nclassification, hypothesis testing and community detection. Specifically, when\nMASE is applied to a dataset of connectomes constructed through diffusion\nmagnetic resonance imaging, the result is an accurate classification of brain\nscans by patient and a meaningful determination of heterogeneity across scans\nof different subjects.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 15:29:25 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 21:24:58 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 06:21:07 GMT"}, {"version": "v4", "created": "Sat, 22 Aug 2020 05:41:17 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Arroyo", "Jes\u00fas", ""], ["Athreya", "Avanti", ""], ["Cape", "Joshua", ""], ["Chen", "Guodong", ""], ["Priebe", "Carey E.", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1906.10125", "submitter": "Osama Idais", "authors": "Osama Idais", "title": "A note on locally optimal designs for generalized linear models with\n  restricted support", "comments": "12", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal designs for generalized linear models require a prior knowledge of\nthe regression parameters. At certain values of the parameters we propose\nparticular assumptions which allow to derive a locally optimal design for a\nmodel without intercept from a locally optimal design for the corresponding\nmodel with intercept and vice versa. Applications to Poisson and logistic\nmodels and Extensions to nonlinear models are provided.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 14:14:52 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Idais", "Osama", ""]]}, {"id": "1906.10159", "submitter": "Matthew Tudball", "authors": "Matthew Tudball, Qingyuan Zhao, Rachael Hughes, Kate Tilling and Jack\n  Bowden", "title": "An Interval Estimation Approach to Sample Selection Bias", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A widespread and largely unaddressed challenge in statistics is that\nnon-random participation in study samples can bias the estimation of parameters\nof interest. To address this problem, we propose a computationally efficient\ninterval estimator for a class of population parameters encompassing population\nmeans, ordinary least squares and instrumental variables estimands which makes\nminimal assumptions about the selection mechanism. Using results from\nstochastic programming, we derive valid confidence intervals and hypothesis\ntests based on this estimator. In addition, we demonstrate how to tighten the\nintervals by incorporating additional constraints based on population-level\ninformation commonly available to researchers, such as survey response rates\nand covariate means. We conduct a comprehensive simulation study to evaluate\nthe finite sample performance of our estimator and conclude with a real data\nstudy on the causal effect of education on income in the highly-selected UK\nBiobank cohort. We are able to demonstrate that our method can produce\ninformative bounds under relatively few population-level auxiliary constraints.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 18:14:05 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 23:55:27 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 05:48:53 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Tudball", "Matthew", ""], ["Zhao", "Qingyuan", ""], ["Hughes", "Rachael", ""], ["Tilling", "Kate", ""], ["Bowden", "Jack", ""]]}, {"id": "1906.10173", "submitter": "Peter Jacko", "authors": "Peter Jacko", "title": "The Finite-Horizon Two-Armed Bandit Problem with Binary Responses: A\n  Multidisciplinary Survey of the History, State of the Art, and Myths", "comments": "Submitted for publication in an academic journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the two-armed bandit problem, which often naturally\nappears per se or as a subproblem in some multi-armed generalizations, and\nserves as a starting point for introducing additional problem features. The\nconsideration of binary responses is motivated by its widespread applicability\nand by being one of the most studied settings. We focus on the undiscounted\nfinite-horizon objective, which is the most relevant in many applications. We\nmake an attempt to unify the terminology as this is different across\ndisciplines that have considered this problem, and present a unified model cast\nin the Markov decision process framework, with subject responses modelled using\nthe Bernoulli distribution, and the corresponding Beta distribution for\nBayesian updating. We give an extensive account of the history and state of the\nart of approaches from several disciplines, including design of experiments,\nBayesian decision theory, naive designs, reinforcement learning, biostatistics,\nand combination designs. We evaluate these designs, together with a few newly\nproposed, accurately computationally (using a newly written package in Julia\nprogramming language by the author) in order to compare their performance. We\nshow that conclusions are different for moderate horizons (typical in practice)\nthan for small horizons (typical in academic literature reporting computational\nresults). We further list and clarify a number of myths about this problem,\ne.g., we show that, computationally, much larger problems can be designed to\nBayes-optimality than what is commonly believed.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 16:02:31 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Jacko", "Peter", ""]]}, {"id": "1906.10179", "submitter": "Lisa Schlosser", "authors": "Lisa Schlosser and Torsten Hothorn and Achim Zeileis", "title": "The Power of Unbiased Recursive Partitioning: A Unifying View of CTree,\n  MOB, and GUIDE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core step of every algorithm for learning regression trees is the selection\nof the best splitting variable from the available covariates and the\ncorresponding split point. Early tree algorithms (e.g., AID, CART) employed\ngreedy search strategies, directly comparing all possible split points in all\navailable covariates. However, subsequent research showed that this is biased\ntowards selecting covariates with more potential split points. Therefore,\nunbiased recursive partitioning algorithms have been suggested (e.g., QUEST,\nGUIDE, CTree, MOB) that first select the covariate based on statistical\ninference using p-values that are adjusted for the possible split points. In a\nsecond step a split point optimizing some objective function is selected in the\nchosen split variable. However, different unbiased tree algorithms obtain these\np-values from different inference frameworks and their relative advantages or\ndisadvantages are not well understood, yet. Therefore, three different popular\napproaches are considered here: classical categorical association tests (as in\nGUIDE), conditional inference (as in CTree), and parameter instability tests\n(as in MOB). First, these are embedded into a common inference framework\nencompassing parametric model trees, in particular linear model trees. Second,\nit is assessed how different building blocks from this common framework affect\nthe power of the algorithms to select the appropriate covariates for splitting:\nobservation-wise goodness-of-fit measure (residuals vs. model scores),\ndichotomization of residuals/scores at zero, and binning of possible split\nvariables. This shows that specifically the goodness-of-fit measure is crucial\nfor the power of the procedures, with model scores without dichotomization\nperforming much better in many scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 19:04:42 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Schlosser", "Lisa", ""], ["Hothorn", "Torsten", ""], ["Zeileis", "Achim", ""]]}, {"id": "1906.10221", "submitter": "Hamdy Mahmoud Dr.", "authors": "Hamdy F. F. Mahmoud", "title": "Parametric versus Semi and Nonparametric Regression Models", "comments": "24 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Three types of regression models researchers need to be familiar with and\nknow the requirements of each: parametric, semiparametric and nonparametric\nregression models. The type of modeling used is based on how much information\nare available about the form of the relationship between response variable and\nexplanatory variables, and the random error distribution. In this article,\ndifferences between models, common methods of estimation, robust estimation,\nand applications are introduced. The R code for all the graphs and analyses\npresented here, in this article, is available in the Appendix.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 20:29:07 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Mahmoud", "Hamdy F. F.", ""]]}, {"id": "1906.10252", "submitter": "Yu Luo", "authors": "Yu Luo, David A. Stephens, David L. Buckeridge", "title": "Bayesian Clustering for Continuous-Time Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop clustering procedures for longitudinal trajectories based on a\ncontinuous-time hidden Markov model (CTHMM) and a generalized linear\nobservation model. Specifically in this paper, we carry out finite and infinite\nmixture model-based clustering for a CTHMM and achieve inference using Markov\nchain Monte Carlo (MCMC). For a finite mixture model with prior on the number\nof components, we implement reversible-jump MCMC to facilitate the\ntrans-dimensional move between different number of clusters. For a Dirichlet\nprocess mixture model, we utilize restricted Gibbs sampling split-merge\nproposals to expedite the MCMC algorithm. We employ proposed algorithms to the\nsimulated data as well as a real data example, and the results demonstrate the\ndesired performance of the new sampler.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 22:17:53 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 14:29:33 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 08:12:04 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Luo", "Yu", ""], ["Stephens", "David A.", ""], ["Buckeridge", "David L.", ""]]}, {"id": "1906.10258", "submitter": "Davide Viviano Mr.", "authors": "Davide Viviano", "title": "Policy Targeting under Network Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.SI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the problem of estimating treatment allocation rules\nunder network interference. I propose a method with several attractive features\nfor applications: (i) it does not rely on the correct specification of a\nparticular structural model; (ii) it exploits heterogeneity in treatment\neffects for targeting individuals; (iii) it accommodates arbitrary constraints\non the policy function; (iv) it does not necessitate network information of the\ntarget units. I introduce estimation procedures that leverage experimental or\nobservational data and derive strong guarantees on the utilitarian regret. I\nprovide a mixed-integer linear program formulation, which can be solved using\noff-the-shelf algorithms. I illustrate the advantages of the method for\ntargeting information on social networks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 22:42:32 GMT"}, {"version": "v10", "created": "Tue, 30 Mar 2021 14:53:08 GMT"}, {"version": "v11", "created": "Tue, 11 May 2021 03:43:47 GMT"}, {"version": "v12", "created": "Tue, 1 Jun 2021 16:09:04 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 04:21:07 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 16:39:58 GMT"}, {"version": "v4", "created": "Mon, 28 Oct 2019 05:37:55 GMT"}, {"version": "v5", "created": "Sun, 10 Nov 2019 17:46:00 GMT"}, {"version": "v6", "created": "Tue, 7 Apr 2020 14:46:03 GMT"}, {"version": "v7", "created": "Fri, 12 Jun 2020 02:23:28 GMT"}, {"version": "v8", "created": "Thu, 17 Dec 2020 02:23:59 GMT"}, {"version": "v9", "created": "Thu, 25 Mar 2021 17:35:18 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Viviano", "Davide", ""]]}, {"id": "1906.10305", "submitter": "Zheng Fang", "authors": "Zheng Fang", "title": "Refinements of the Kiefer-Wolfowitz Theorem and a Test of Concavity", "comments": "Forthcoming in Electronic Journal of Statistics. Compared to the\n  journal version, the difference is that this version contains additional\n  simulation results, collected in Appendix C", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies estimation of and inference on a distribution function $F$\nthat is concave on the nonnegative half line and admits a density function $f$\nwith potentially unbounded support. When $F$ is strictly concave, we show that\nthe supremum distance between the Grenander distribution estimator and the\nempirical distribution may still be of order $O(n^{-2/3}(\\log n)^{2/3})$ almost\nsurely, which reduces to an existing result of Kiefer and Wolfowitz when $f$\nhas bounded support. We further refine this result by allowing $F$ to be not\nstrictly concave or even non-concave and instead requiring it be\n\"asymptotically\" strictly concave. Building on these results, we then develop a\ntest of concavity of $F$ or equivalently monotonicity of $f$, which is shown to\nhave asymptotically pointwise level control under the entire null as well as\nconsistency under any fixed alternative. In fact, we show that our test has\nlocal size control and nontrivial local power against any local alternatives\nthat do not approach the null too fast, which may be of interest given the\nirregularity of the problem. Extensions to settings involving testing\nconcavity/convexity/monotonicity are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 03:17:27 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2019 22:02:50 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Fang", "Zheng", ""]]}, {"id": "1906.10372", "submitter": "Nick Whiteley Prof.", "authors": "Nick Whiteley", "title": "Dynamic time series clustering via volatility change-points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note outlines a method for clustering time series based on a statistical\nmodel in which volatility shifts at unobserved change-points. The model\naccommodates some classical stylized features of returns and its relation to\nGARCH is discussed. Clustering is performed using a probability metric\nevaluated between posterior distributions of the most recent change-point\nassociated with each series. This implies series are grouped together at a\ngiven time if there is evidence the most recent shifts in their respective\nvolatilities were coincident or closely timed. The clustering method is\ndynamic, in that groupings may be updated in an online manner as data arrive.\nNumerical results are given analyzing daily returns of constituents of the S&P\n500.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 08:18:58 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Whiteley", "Nick", ""]]}, {"id": "1906.10445", "submitter": "Hisashi Noma", "authors": "Yuki Matsushima, Hisashi Noma, Tomohide Yamada and Toshi A. Furukawa", "title": "Bayesian influence diagnostics and outlier detection for meta-analysis\n  of diagnostic test accuracy", "comments": "18 pages, 3 tables, 4 figures", "journal-ref": "Res Synth Methods 2020;11(2):237-247", "doi": "10.1002/jrsm.1387", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analyses of diagnostic test accuracy (DTA) studies have been gaining\nprominence in research in clinical epidemiology and health technology\ndevelopment. In these DTA meta-analyses, some studies may have markedly\ndifferent characteristics from the others, and potentially be inappropriate to\ninclude. The inclusion of these \"outlying\" studies might lead to biases,\nyielding misleading results. In addition, there might be influential studies\nthat have notable impacts on the results. In this article, we propose Bayesian\nmethods for detecting outlying studies and their influence diagnostics in DTA\nmeta-analyses. Synthetic influence measures based on the bivariate hierarchical\nBayesian random effects models are developed because the overall influences of\nindividual studies should be simultaneously assessed by the two outcome\nvariables and their correlation information. We propose four synthetic measures\nfor influence analyses: (1) relative distance, (2) standardized residual, (3)\nBayesian p-value, and (4) influence statistic on the area under the summary\nreceiver operating characteristic curve. We also show conventional univariate\nBayesian influential measures can be applied to the bivariate random effects\nmodels, which can be used as marginal influential measures. We illustrate the\neffectiveness of the proposed methods by applying them to a DTA meta-analysis\nof ultrasound in screening for vesicoureteral reflux among children with\nurinary tract infections.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 10:49:29 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 10:06:25 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Matsushima", "Yuki", ""], ["Noma", "Hisashi", ""], ["Yamada", "Tomohide", ""], ["Furukawa", "Toshi A.", ""]]}, {"id": "1906.10564", "submitter": "Chris Oates", "authors": "Junyang Wang, Jon Cockayne, Chris J. Oates", "title": "A Role for Symmetry in the Bayesian Solution of Differential Equations", "comments": "A summary version of this manuscript appeared in the proceedings of\n  MaxEnt 2018 in London, UK; see arXiv:1805.07109", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of numerical methods, such as finite difference methods\nfor differential equations, as point estimators suggests that formal\nuncertainty quantification can also be performed in this context. Competing\nstatistical paradigms can be considered and Bayesian probabilistic numerical\nmethods (PNMs) are obtained when Bayesian statistical principles are deployed.\nBayesian PNM have the appealing property of being closed under composition,\nsuch that uncertainty due to different sources of discretisation in a numerical\nmethod can be jointly modelled and rigorously propagated. Despite recent\nattention, no exact Bayesian PNM for the numerical solution of ordinary\ndifferential equations (ODEs) has been proposed. This raises the fundamental\nquestion of whether exact Bayesian methods for (in general nonlinear) ODEs even\nexist. The purpose of this paper is to provide a positive answer for a limited\nclass of ODE. To this end, we work at a foundational level, where a novel\nBayesian PNM is proposed as a proof-of-concept. Our proposal is a synthesis of\nclassical Lie group methods, to exploit underlying symmetries in the gradient\nfield, and non-parametric regression in a transformed solution space for the\nODE. The procedure is presented in detail for first and second order ODEs and\nrelies on a certain strong technical condition -- existence of a solvable Lie\nalgebra -- being satisfied. Numerical illustrations are provided.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 15:59:27 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 13:05:14 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 11:11:05 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Wang", "Junyang", ""], ["Cockayne", "Jon", ""], ["Oates", "Chris J.", ""]]}, {"id": "1906.10591", "submitter": "Per Sid\\'en", "authors": "Per Sid\\'en, Finn Lindgren, David Bolin, Anders Eklund and Mattias\n  Villani", "title": "Spatial 3D Mat\\'ern priors for fast whole-brain fMRI analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian whole-brain functional magnetic resonance imaging (fMRI) analysis\nwith three-dimensional spatial smoothing priors has been shown to produce\nstate-of-the-art activity maps without pre-smoothing the data. The proposed\ninference algorithms are computationally demanding however, and the proposed\nspatial priors have several less appealing properties, such as being improper\nand having infinite spatial range. We propose a statistical inference framework\nfor whole-brain fMRI analysis based on the class of Mat\\'ern covariance\nfunctions. The framework uses the Gaussian Markov random field (GMRF)\nrepresentation of possibly anisotropic spatial Mat\\'ern fields via the\nstochastic partial differential equation (SPDE) approach of Lindgren et al.\n(2011). This allows for more flexible and interpretable spatial priors, while\nmaintaining the sparsity required for fast inference in the high-dimensional\nwhole-brain setting. We develop an accelerated stochastic gradient descent\n(SGD) optimization algorithm for empirical Bayes (EB) inference of the spatial\nhyperparameters. Conditionally on the inferred hyperparameters, we make a fully\nBayesian treatment of the brain activity. The Mat\\'ern prior is applied to both\nsimulated and experimental task-fMRI data and clearly demonstrates that it is a\nmore reasonable choice than the previously used priors, using comparisons of\nactivity maps, prior simulation and cross-validation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 15:13:39 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 09:26:53 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Sid\u00e9n", "Per", ""], ["Lindgren", "Finn", ""], ["Bolin", "David", ""], ["Eklund", "Anders", ""], ["Villani", "Mattias", ""]]}, {"id": "1906.10737", "submitter": "Chris Hans", "authors": "Casey B. Davis, Christopher M. Hans, Thomas J. Santner", "title": "Prediction Using a Bayesian Heteroscedastic Composite Gaussian Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research proposes a flexible Bayesian extension of the composite\nGaussian process (CGP) model of Ba and Joseph (2012) for predicting (stationary\nor) non-stationary $y(\\mathbf{x})$. The CGP generalizes the regression plus\nstationary Gaussian process (GP) model by replacing the regression term with a\nGP. The new model, $Y(\\mathbf{x})$, can accommodate large-scale trends\nestimated by a global GP, local trends estimated by an independent local GP,\nand a third process to describe heteroscedastic data in which\n$Var(Y(\\mathbf{x}))$ can depend on the inputs. This paper proposes a prior\nwhich ensures that the fitted global mean is smoother than the local\ndeviations, and extends the covariance structure of the CGP to allow for\ndifferentially-weighted global and local components. A Markov chain Monte Carlo\nalgorithm is proposed to provide posterior estimates of the parameters,\nincluding the values of the heteroscedastic variance at the training and test\ndata locations. The posterior distribution is used to make predictions and to\nquantify the uncertainty of the predictions using prediction intervals. The\nmethod is illustrated using both stationary and non-stationary $y(\\mathbf{x})$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 19:38:31 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Davis", "Casey B.", ""], ["Hans", "Christopher M.", ""], ["Santner", "Thomas J.", ""]]}, {"id": "1906.10781", "submitter": "Matthew Heiner", "authors": "Matthew Heiner and Athanasios Kottas", "title": "Estimation and selection for high-order Markov chains with Bayesian\n  mixture transition distribution models", "comments": null, "journal-ref": null, "doi": null, "report-no": "UCSC-SOE-18-07", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop two models for Bayesian estimation and selection in high-order,\ndiscrete-state Markov chains. Both are based on the mixture transition\ndistribution, which constructs a transition probability tensor with additive\nmixing of probabilities from first-order transition matrices. We demonstrate\ntwo uses for the proposed models: parsimonious approximation of high-order\ndynamics by mixing lower-order transition models, and order/lag selection\nthrough over-specification and shrinkage via priors for sparse probability\nvectors. The priors further shrink all models to an identifiable and\ninterpretable parameterization, useful for data analysis. We discuss properties\nof the models and demonstrate their utility with simulation studies. We further\napply the methodology to a data analysis from the high-order Markov chain\nliterature and to a time series of pink salmon abundance in a creek in Alaska,\nU.S.A.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 23:06:04 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Heiner", "Matthew", ""], ["Kottas", "Athanasios", ""]]}, {"id": "1906.10792", "submitter": "Issa Dahabreh", "authors": "Issa J. Dahabreh and James M. Robins and Sebastien J-P.A. Haneuse and\n  Miguel A. Hern\\'an", "title": "Generalizing causal inferences from randomized trials: counterfactual\n  and graphical identification", "comments": "first upload", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When engagement with a randomized trial is driven by factors that affect the\noutcome or when trial engagement directly affects the outcome independent of\ntreatment, the average treatment effect among trial participants is unlikely to\ngeneralize to a target population. In this paper, we use counterfactual and\ngraphical causal models to examine under what conditions we can generalize\ncausal inferences from a randomized trial to the target population of\ntrial-eligible individuals. We offer an interpretation of generalizability\nanalyses using the notion of a hypothetical intervention to \"scale-up\" trial\nengagement to the target population. We consider the interpretation of\ngeneralizability analyses when trial engagement does or does not directly\naffect the outcome, highlight connections with censoring in longitudinal\nstudies, and discuss identification of the distribution of counterfactual\noutcomes via g-formula computation and inverse probability weighting. Last, we\nshow how the methods can be extended to address time-varying treatments,\nnon-adherence, and censoring.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 00:29:15 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Dahabreh", "Issa J.", ""], ["Robins", "James M.", ""], ["Haneuse", "Sebastien J-P. A.", ""], ["Hern\u00e1n", "Miguel A.", ""]]}, {"id": "1906.10843", "submitter": "Ercan Yildiz", "authors": "Ercan Yildiz, Joshua Safyan, Marc Harper", "title": "User Sentiment as a Success Metric: Persistent Biases Under Full\n  Randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study user sentiment (reported via optional surveys) as a metric for fully\nrandomized A/B tests. Both user-level covariates and treatment assignment can\nimpact response propensity. We propose a set of consistent estimators for the\naverage and local treatment effects on treated and respondent users. We show\nthat our problem can be mapped onto the intersection of the missing data\nproblem and observational causal inference, and we identify conditions under\nwhich consistent estimators exist. We evaluate the performance of estimators\nvia simulation studies and find that more complicated models do not necessarily\nprovide superior performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 04:40:50 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Yildiz", "Ercan", ""], ["Safyan", "Joshua", ""], ["Harper", "Marc", ""]]}, {"id": "1906.10951", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti and Irene Crimaldi", "title": "The Rescaled Polya Urn: local reinforcement and chi-squared goodness of\n  fit test", "comments": "28 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent studies of big samples, this work aims at constructing a\nparametric model which is characterized by the following features: (i) a\n\"local\" reinforcement, i.e. a reinforcement mechanism mainly based on the last\nobservations, (ii) a random persistent fluctuation of the predictive mean, and\n(iii) a long-term convergence of the empirical mean to a deterministic limit,\ntogether with a chi-squared goodness of fit result. This triple purpose has\nbeen achieved by the introduction of a new variant of the Eggenberger-Polya\nurn, that we call the \"Rescaled\" Polya urn. We provide a complete asymptotic\ncharacterization of this model, pointing out that, for a certain choice of the\nparameters, it has properties different from the ones typically exhibited from\nthe other urn models in the literature. Therefore, beyond the possible\nstatistical application, this work could be interesting for those who are\nconcerned with stochastic processes with reinforcement.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 10:16:41 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 13:47:22 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Aletti", "Giacomo", ""], ["Crimaldi", "Irene", ""]]}, {"id": "1906.11020", "submitter": "Bardia Panahbehagh Ph.D.", "authors": "Bardia Panahbehagh, Rainer Bruggemann and Mohammad M. Salehi", "title": "Sampling of multiple variables based on partial order set theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is going to introduce a new method for ranked set sampling with\nmultiple criteria. The method is based on a version of ranked set sampling,\nintroduced by Panahbehagh et al. (2017), which relaxes the restriction of\nselecting just one individual variable from each ranked set. Under the new\nmethod for ranking, elements are ranked in sets based on linear extensions in\npartial order sets theory, where based on all the variables simultaneously.\nResults will be evaluated by some simulations and two real case study on\neconomical, medicinal use of flowers and the pollution of herb-layer by Lead,\nCadmium, Zinc and Sulfur in regions in the southwest of Germany.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 12:19:48 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 14:05:44 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Panahbehagh", "Bardia", ""], ["Bruggemann", "Rainer", ""], ["Salehi", "Mohammad M.", ""]]}, {"id": "1906.11043", "submitter": "Anne Sabourin", "authors": "Holger Drees, Anne Sabourin (LTCI)", "title": "Principal Component Analysis for Multivariate Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first order behavior of multivariate heavy-tailed random vectors above\nlarge radial thresholds is ruled by a limit measure in a regular variation\nframework. For a high dimensional vector, a reasonable assumption is that the\nsupport of this measure is concentrated on a lower dimensional subspace,\nmeaning that certain linear combinations of the components are much likelier to\nbe large than others. Identifying this subspace and thus reducing the dimension\nwill facilitate a refined statistical analysis. In this work we apply Principal\nComponent Analysis (PCA) to a re-scaled version of radially thresholded\nobservations. Within the statistical learning framework of empirical risk\nminimization, our main focus is to analyze the squared reconstruction error for\nthe exceedances over large radial thresholds. We prove that the empirical risk\nconverges to the true risk, uniformly over all projection subspaces. As a\nconsequence, the best projection subspace is shown to converge in probability\nto the optimal one, in terms of the Hausdorff distance between their\nintersections with the unit sphere. In addition, if the exceedances are\nre-scaled to the unit ball, we obtain finite sample uniform guarantees to the\nreconstruction error pertaining to the estimated projection sub-space.\nNumerical experiments illustrate the relevance of the proposed framework for\npractical purposes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 12:44:54 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Drees", "Holger", "", "LTCI"], ["Sabourin", "Anne", "", "LTCI"]]}, {"id": "1906.11113", "submitter": "Filip Elvander", "authors": "Filip Elvander, Johan Sw\\\"ard, Andreas Jakobsson", "title": "Mismatched Estimation of Polynomially Damped Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the problem of estimating the parameters of\npolynomially damped sinusoidal signals, commonly encountered in, for instance,\nspectroscopy. Generally, finding the parameter values of such signals\nconstitutes a high-dimensional problem, often further complicated by not\nknowing the number of signal components or their specific signal structures. In\norder to alleviate the computational burden, we herein propose a mismatched\nestimation procedure using simplified, approximate signal models. Despite the\napproximation, we show that such a procedure is expected to yield predictable\nresults, allowing for statistically and computationally efficient estimates of\nthe signal parameters.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 14:10:30 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Elvander", "Filip", ""], ["Sw\u00e4rd", "Johan", ""], ["Jakobsson", "Andreas", ""]]}, {"id": "1906.11208", "submitter": "Li-Chun Zhang", "authors": "Li-Chun Zhang", "title": "Proxy expenditure weights for Consumer Price Index: Audit sampling\n  inference for big data statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purchase data from retail chains provide proxy measures of private household\nexpenditure on items that are the most troublesome to collect in the\ntraditional expenditure survey. Due to the sheer amount of proxy data, the bias\ndue to coverage and selection errors completely dominates the variance. We\ndevelop tests for bias based on audit sampling, which makes use of available\nsurvey data that cannot be linked to the proxy data source at the individual\nlevel. However, audit sampling fails to yield a meaningful mean squared error\nestimate, because the sampling variance is too large compared to the bias of\nthe big data estimate. We propose a novel accuracy measure that is applicable\nin such situations. This can provide a necessary part of the statistical\nargument for the uptake of big data source, in replacement of traditional\nsurvey sampling. An application to disaggregated food price index is used to\ndemonstrate the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 10:19:13 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Zhang", "Li-Chun", ""]]}, {"id": "1906.11291", "submitter": "Xinran Li", "authors": "Xinran Li and Peng Ding", "title": "Rerandomization and Regression Adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomization is a basis for the statistical inference of treatment effects\nwithout strong assumptions on the outcome-generating process. Appropriately\nusing covariates further yields more precise estimators in randomized\nexperiments. R. A. Fisher suggested blocking on discrete covariates in the\ndesign stage or conducting analysis of covariance (ANCOVA) in the analysis\nstage. We can embed blocking into a wider class of experimental design called\nrerandomization, and extend the classical ANCOVA to more general regression\nadjustment. Rerandomization trumps complete randomization in the design stage,\nand regression adjustment trumps the simple difference-in-means estimator in\nthe analysis stage. It is then intuitive to use both rerandomization and\nregression adjustment. Under the randomization-inference framework, we\nestablish a unified theory allowing the designer and analyzer to have access to\ndifferent sets of covariates. We find that asymptotically (a) for any given\nestimator with or without regression adjustment, rerandomization never hurts\neither the sampling precision or the estimated precision, and (b) for any given\ndesign with or without rerandomization, our regression-adjusted estimator never\nhurts the estimated precision. Therefore, combining rerandomization and\nregression adjustment yields better coverage properties and thus improves\nstatistical inference. To theoretically quantify these statements, we discuss\noptimal regression-adjusted estimators in terms of the sampling precision and\nthe estimated precision, and then measure the additional gains of the designer\nand the analyzer. We finally suggest using rerandomization in the design and\nregression adjustment in the analysis followed by the Huber--White robust\nstandard error.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 18:33:00 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 01:28:49 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Li", "Xinran", ""], ["Ding", "Peng", ""]]}, {"id": "1906.11324", "submitter": "Thomas Jaki", "authors": "John Whitehead, Yasin Desai, Thomas Jaki", "title": "Estimation of treatment effects following a sequential trial of multiple\n  treatments", "comments": "23 pages, 2 Figures, 8 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a clinical trial is subject to a series of interim analyses as a result\nof which the study may be terminated or modified, final frequentist analyses\nneed to take account of the design used. Failure to do so may result in\noverstated levels of significance, biased effect estimates and confidence\nintervals with inadequate coverage probabilities. A wide variety of valid\nmethods of frequentist analysis have been devised for sequential designs\ncomparing a single experimental treatment with a single control treatment. It\nis less clear how to perform the final analysis of a sequential or adaptive\ndesign applied in a more complex setting, for example to determine which\ntreatment or set of treatments amongst several candidates should be\nrecommended.\n  This paper has been motivated by consideration of a trial in which four\ntreatments for sepsis are to be compared, with interim analyses allowing the\ndropping of treatments or termination of the trial to declare a single winner\nor to conclude that there is little difference between the treatments that\nremain. The approach taken is based on the method of Rao-Blackwellisation which\nenhances the accuracy of unbiased estimates available from the first interim\nanalysis by taking their conditional expectations given final sufficient\nstatistics. Analytic approaches to determine such expectations are difficult\nand specific to the details of the design, and instead \"reverse simulations\"\nare conducted to construct replicate realisations of the first interim analysis\nfrom the final test statistics. The method also provides approximate confidence\nintervals for the differences between treatments.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 20:09:40 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Whitehead", "John", ""], ["Desai", "Yasin", ""], ["Jaki", "Thomas", ""]]}, {"id": "1906.11382", "submitter": "Yuta Umezu", "authors": "Yuta Umezu and Ichiro Takeuchi", "title": "Selective Inference via Marginal Screening for High Dimensional\n  Classification", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-selection inference is a statistical technique for determining salient\nvariables after model or variable selection. Recently, selective inference, a\nkind of post-selection inference framework, has garnered the attention in the\nstatistics and machine learning communities. By conditioning on a specific\nvariable selection procedure, selective inference can properly control for\nso-called selective type I error, which is a type I error conditional on a\nvariable selection procedure, without imposing excessive additional\ncomputational costs. While selective inference can provide a valid hypothesis\ntesting procedure, the main focus has hitherto been on Gaussian linear\nregression models. In this paper, we develop a selective inference framework\nfor binary classification problem. We consider a logistic regression model\nafter variable selection based on marginal screening, and derive the high\ndimensional statistical behavior of the post-selection estimator. This enables\nus to asymptotically control for selective type I error for the purposes of\nhypothesis testing after variable selection. We conduct several simulation\nstudies to confirm the statistical power of the test, and compare our proposed\nmethod with data splitting and other methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 23:20:24 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Umezu", "Yuta", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1906.11592", "submitter": "Jonathan Rougier", "authors": "Jonathan Rougier and Carey Priebe", "title": "The exact form of the 'Ockham factor' in model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the arguments for maximizing the `evidence' as an algorithm for\nmodel selection. We show, using a new definition of model complexity which we\nterm `flexibility', that maximizing the evidence should appeal to both Bayesian\nand Frequentist statisticians. This is due to flexibility's unique position in\nthe exact decomposition of log-evidence into log-fit minus flexibility. In the\nGaussian linear model, flexibility is asymptotically equal to the Bayesian\nInformation Criterion (BIC) penalty, but we caution against using BIC in place\nof flexibility for model selection.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 12:42:17 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 13:09:09 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Rougier", "Jonathan", ""], ["Priebe", "Carey", ""]]}, {"id": "1906.11641", "submitter": "Daniela De Canditiis", "authors": "Daniela De Canditiis", "title": "A global approach for learning sparse Ising models", "comments": "15 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1902.04728 by other authors", "journal-ref": "Mathematics and Computers in Simulation (2020)", "doi": "10.1016/j.matcom.2020.02.012", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the link parameters as well as the\nstructure of a binary-valued pairwise Markov model. Under sparsity assumption,\nwe propose a method based on $l_1$- regularized logistic regression, which\nestimate globally the whole set of edges and link parameters. Unlike the more\nrecent methods discussed in literature that learn the edges and the\ncorresponding link parameters one node at a time, in this work we propose a\nmethod that learns all the edges and corresponding link parameters\nsimultaneously for all nodes. The idea behind this proposal is to exploit the\nreciprocal information of the nodes between each other during the estimation\nprocess. Numerical experiments highlight the advantage of this technique and\nconfirm the intuition behind it.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 14:24:54 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 07:28:31 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["De Canditiis", "Daniela", ""]]}, {"id": "1906.11653", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal and Antonio Canale", "title": "Simultaneous Transformation and Rounding (STAR) Models for\n  Integer-Valued Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet powerful framework for modeling integer-valued data,\nsuch as counts, scores, and rounded data. The data-generating process is\ndefined by Simultaneously Transforming and Rounding (STAR) a continuous-valued\nprocess, which produces a flexible family of integer-valued distributions\ncapable of modeling zero-inflation, bounded or censored data, and over- or\nunderdispersion. The transformation is modeled as unknown for greater\ndistributional flexibility, while the rounding operation ensures a coherent\ninteger-valued data-generating process. An efficient MCMC algorithm is\ndeveloped for posterior inference and provides a mechanism for adaptation of\nsuccessful Bayesian models and algorithms for continuous data to the\ninteger-valued data setting. Using the STAR framework, we design a new Bayesian\nAdditive Regression Tree (BART) model for integer-valued data, which\ndemonstrates impressive predictive distribution accuracy for both synthetic\ndata and a large healthcare utilization dataset. For interpretable\nregression-based inference, we develop a STAR additive model, which offers\ngreater flexibility and scalability than existing integer-valued models. The\nSTAR additive model is applied to study the recent decline in Amazon river\ndolphins.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 13:56:39 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 15:27:26 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kowal", "Daniel R.", ""], ["Canale", "Antonio", ""]]}, {"id": "1906.11658", "submitter": "Marco Morucci", "authors": "M. Usaid Awan, Yameng Liu, Marco Morucci, Sudeepa Roy, Cynthia Rudin,\n  Alexander Volfovsky", "title": "Interpretable Almost-Matching-Exactly With Instrumental Variables", "comments": null, "journal-ref": "Proceedings of the Thirty-fifth Conference on Uncertainty in\n  Artificial Intelligence (UAI 2019)", "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Uncertainty in the estimation of the causal effect in observational studies\nis often due to unmeasured confounding, i.e., the presence of unobserved\ncovariates linking treatments and outcomes. Instrumental Variables (IV) are\ncommonly used to reduce the effects of unmeasured confounding. Existing methods\nfor IV estimation either require strong parametric assumptions, use arbitrary\ndistance metrics, or do not scale well to large datasets. We propose a matching\nframework for IV in the presence of observed categorical confounders that\naddresses these weaknesses. Our method first matches units exactly, and then\nconsecutively drops variables to approximately match the remaining units on as\nmany variables as possible. We show that our algorithm constructs better\nmatches than other existing methods on simulated datasets, and we produce\ninteresting results in an application to political canvassing.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 14:00:39 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 14:59:30 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Awan", "M. Usaid", ""], ["Liu", "Yameng", ""], ["Morucci", "Marco", ""], ["Roy", "Sudeepa", ""], ["Rudin", "Cynthia", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "1906.11748", "submitter": "Nicholas Hutzler", "authors": "Nicholas R. Hutzler", "title": "Chi-squared Test for Binned, Gaussian Samples", "comments": "To appear in Metrologia", "journal-ref": "Metrologia 56, 055007 (2019)", "doi": "10.1088/1681-7575/ab2d53", "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the $\\chi^2$ test for binned, Gaussian samples, including effects\ndue to the fact that the experimentally available sample standard deviation and\nthe unavailable true standard deviation have different statistical properties.\nFor data formed by binning Gaussian samples with bin size $n$, we find that the\nexpected value and standard deviation of the reduced $\\chi^2$ statistic is\n\\begin{equation} \\frac{n-1}{n-3}\\pm\n\\frac{n-1}{n-3}\\sqrt{\\frac{n-2}{n-5}}\\sqrt{\\frac{2}{N-1}}, \\end{equation} where\n$N$ is the total number of binned values. This is strictly larger in both mean\nand standard deviation than the value of $1\\pm (2/(N-1))^{1/2}$ reported in\nstandard treatments, which ignore the distinction between true and sample\nstandard deviation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 15:51:19 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hutzler", "Nicholas R.", ""]]}, {"id": "1906.11920", "submitter": "Xiaojing Wang", "authors": "Xiaojing Wang, Jingang Miao, Yunting Sun", "title": "A Python Library For Empirical Calibration", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with biased data samples is a common task across many statistical\nfields. In survey sampling, bias often occurs due to unrepresentative samples.\nIn causal studies with observational data, the treated versus untreated group\nassignment is often correlated with covariates, i.e., not random. Empirical\ncalibration is a generic weighting method that presents a unified view on\ncorrecting or reducing the data biases for the tasks mentioned above. We\nprovide a Python library EC to compute the empirical calibration weights. The\nproblem is formulated as convex optimization and solved efficiently in the dual\nform. Compared to existing software, EC is both more efficient and robust. EC\nalso accommodates different optimization objectives, supports weight clipping,\nand allows inexact calibration, which improves usability. We demonstrate its\nusage across various experiments with both simulated and real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 19:18:29 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 18:58:10 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Wang", "Xiaojing", ""], ["Miao", "Jingang", ""], ["Sun", "Yunting", ""]]}, {"id": "1906.11982", "submitter": "Amrit Dhar", "authors": "Amrit Dhar, Duncan K. Ralph, Vladimir N. Minin and Frederick A. Matsen\n  IV", "title": "A Bayesian Phylogenetic Hidden Markov Model for B Cell Receptor Sequence\n  Analysis", "comments": "26 pages", "journal-ref": null, "doi": "10.1371/journal.pcbi.1008030", "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The human body is able to generate a diverse set of high affinity antibodies,\nthe soluble form of B cell receptors (BCRs), that bind to and neutralize\ninvading pathogens. The natural development of BCRs must be understood in order\nto design vaccines for highly mutable pathogens such as influenza and HIV. BCR\ndiversity is induced by naturally occurring combinatorial \"V(D)J\"\nrearrangement, mutation, and selection processes. Most current methods for BCR\nsequence analysis focus on separately modeling the above processes. Statistical\nphylogenetic methods are often used to model the mutational dynamics of BCR\nsequence data, but these techniques do not consider all the complexities\nassociated with B cell diversification such as the V(D)J rearrangement process.\nIn particular, standard phylogenetic approaches assume the DNA bases of the\nprogenitor (or \"naive\") sequence arise independently and according to the same\ndistribution, ignoring the complexities of V(D)J rearrangement. In this paper,\nwe introduce a novel approach to Bayesian phylogenetic inference for BCR\nsequences that is based on a phylogenetic hidden Markov model (phylo-HMM). This\ntechnique not only integrates a naive rearrangement model with a phylogenetic\nmodel for BCR sequence evolution but also naturally accounts for uncertainty in\nall unobserved variables, including the phylogenetic tree, via posterior\ndistribution sampling.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 22:10:32 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Dhar", "Amrit", ""], ["Ralph", "Duncan K.", ""], ["Minin", "Vladimir N.", ""], ["Matsen", "Frederick A.", "IV"]]}, {"id": "1906.12024", "submitter": "Kevin Bello", "authors": "Asish Ghoshal and Kevin Bello and Jean Honorio", "title": "Direct Learning with Guarantees of the Difference DAG Between Structural\n  Equation Models", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering cause-effect relationships between variables from observational\ndata is a fundamental challenge in many scientific disciplines. However, in\nmany situations it is desirable to directly estimate the change in causal\nrelationships across two different conditions, e.g., estimating the change in\ngenetic expression across healthy and diseased subjects can help isolate\ngenetic factors behind the disease. This paper focuses on the problem of\ndirectly estimating the structural difference between two structural equation\nmodels (SEMs), having the same topological ordering, given two sets of samples\ndrawn from the individual SEMs. We present an principled algorithm that can\nrecover the difference SEM in $\\mathcal{O}(d^2 \\log p)$ samples, where $d$ is\nrelated to the number of edges in the difference SEM of $p$ nodes. We also\nstudy the fundamental limits and show that any method requires at least\n$\\Omega(d' \\log \\frac{p}{d'})$ samples to learn difference SEMs with at most\n$d'$ parents per node. Finally, we validate our theoretical results with\nsynthetic experiments and show that our method outperforms the\nstate-of-the-art. Moreover, we show the usefulness of our method by using data\nfrom the medical domain.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 03:04:02 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 05:36:34 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ghoshal", "Asish", ""], ["Bello", "Kevin", ""], ["Honorio", "Jean", ""]]}, {"id": "1906.12072", "submitter": "Yohann de Castro", "authors": "J.-M. Aza\\\"is and Y. De Castro", "title": "Multiple Testing and Variable Selection along Least Angle Regression's\n  path", "comments": "62 pages; new: FDR control and power comparison between Knockoff,\n  FCD, Slope and our proposed method; new: the introduction has been revised\n  and now present a synthetic presentation of the main results. We believe that\n  this introduction brings new insists compared to previous versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate multiple testing and variable selection using\nLeast Angle Regression (LARS) algorithm in high dimensions under the Gaussian\nnoise assumption. LARS is known to produce a piecewise affine solutions path\nwith change points referred to as knots of the LARS path. The cornerstone of\nthe present work is the expression in closed form of the exact joint law of\nK-uplets of knots conditional on the variables selected by LARS, namely the\nso-called post-selection joint law of the LARS knots. Numerical experiments\ndemonstrate the perfect fit of our finding.\n  Our main contributions are three fold. First, we build testing procedures on\nvariables entering the model along the LARS path in the general design case\nwhen the noise level can be unknown. This testing procedures are referred to as\nthe Generalized t-Spacing tests (GtSt) and we prove that they have exact\nnon-asymptotic level (i.e., Type I error is exactly controlled). In that way,\nwe extend a work from (Taylor et al., 2014) where the Spacing test works for\nconsecutive knots and known variance. Second, we introduce a new exact multiple\nfalse negatives test after model selection in the general design case when the\nnoise level can be unknown. We prove that this testing procedure has exact\nnon-asymptotic level for general design and unknown noise level. Last, we give\nan exact control of the false discovery rate (FDR) under orthogonal design\nassumption. Monte-Carlo simulations and a real data experiment are provided to\nillustrate our results in this case. Of independent interest, we introduce an\nequivalent formulation of LARS algorithm based on a recursive function.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 07:43:43 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 22:50:10 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 12:16:42 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Aza\u00efs", "J. -M.", ""], ["De Castro", "Y.", ""]]}, {"id": "1906.12100", "submitter": "Els Goetghebeur", "authors": "Els Goetghebeur, Saskia le Cessie, Bianca De Stavola, Erica Moodie and\n  Ingeborg Waernbaum (on behalf of the topic group Causal Inference (TG7) of\n  the STRATOS initiative)", "title": "Formulating causal questions and principled statistical answers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although review papers on causal inference methods are now available, there\nis a lack of introductory overviews on what they can render and on the guiding\ncriteria for choosing one particular method. This tutorial gives an overview in\nsituations where an exposure of interest is set at a chosen baseline (`point\nexposure') and the target outcome arises at a later time point. We first phrase\nrelevant causal questions and make a case for being specific about the possible\nexposure levels involved and the populations for which the question is\nrelevant. Using the potential outcomes framework, we describe principled\ndefinitions of causal effects and of estimation approaches classified according\nto whether they invoke the no unmeasured confounding assumption (including\noutcome regression and propensity score-based methods) or an instrumental\nvariable with added assumptions. We discuss challenges and potential pitfalls\nand illustrate application using a `simulation learner', that mimics the effect\nof various breastfeeding interventions on a child's later development. This\ninvolves a typical simulation component with generated exposure, covariate, and\noutcome data that mimic those from an observational or randomised intervention\nstudy. The simulation learner further generates various (linked) exposure types\nwith a set of possible values per observation unit, from which observed as well\nas potential outcome data are generated. It thus provides true values of\nseveral causal effects. R code for data generation and analysis is available on\nwww.ofcaus.org, where SAS and Stata code for analysis is also provided.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 09:00:14 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Goetghebeur", "Els", "", "on behalf of the topic group Causal Inference"], ["Cessie", "Saskia le", "", "on behalf of the topic group Causal Inference"], ["De Stavola", "Bianca", "", "on behalf of the topic group Causal Inference"], ["Moodie", "Erica", "", "on behalf of the topic group Causal Inference"], ["Waernbaum", "Ingeborg", "", "on behalf of the topic group Causal Inference"]]}, {"id": "1906.12106", "submitter": "Jaco Visagie", "authors": "I.J.H. Visagie and F. Lombard", "title": "On the conditional distribution of the mean of the two closest among a\n  set of three observations", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical analyses of raw materials are often repeated in duplicate or\ntriplicate. The assay values obtained are then combined using a predetermined\nformula to obtain an estimate of the true value of the material of interest.\nWhen duplicate observations are obtained, their average typically serves as an\nestimate of the true value. On the other hand, the \"best of three\" method\ninvolves taking three measurements and using the average of the two closest\nones as estimate of the true value.\n  In this paper, we consider another method which potentially involves three\nmeasurements. Initially two measurements are obtained and if their difference\nis sufficiently small, their average is taken as estimate of the true value.\nHowever, if the difference is too large then a third independent measurement is\nobtained. The estimator is then defined as the average between the third\nobservation and the one among the first two which is closest to it.\n  Our focus in the paper is the conditional distribution of the estimate in\ncases where the initial difference is too large. We find that the conditional\ndistributions are markedly different under the assumption of a normal\ndistribution and a Laplace distribution.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 09:11:25 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Visagie", "I. J. H.", ""], ["Lombard", "F.", ""]]}, {"id": "1906.12125", "submitter": "Richard Samworth", "authors": "Ziwei Zhu, Tengyao Wang and Richard J. Samworth", "title": "High-dimensional principal component analysis with heterogeneous\n  missingness", "comments": "42 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of high-dimensional Principal Component Analysis (PCA)\nwith missing observations. In simple, homogeneous missingness settings with a\nnoise level of constant order, we show that an existing inverse-probability\nweighted (IPW) estimator of the leading principal components can (nearly)\nattain the minimax optimal rate of convergence. However, deeper investigation\nreveals both that, particularly in more realistic settings where the\nmissingness mechanism is heterogeneous, the empirical performance of the IPW\nestimator can be unsatisfactory, and moreover that, in the noiseless case, it\nfails to provide exact recovery of the principal components. Our main\ncontribution, then, is to introduce a new method for high-dimensional PCA,\ncalled `primePCA', that is designed to cope with situations where observations\nmay be missing in a heterogeneous manner. Starting from the IPW estimator,\nprimePCA iteratively projects the observed entries of the data matrix onto the\ncolumn space of our current estimate to impute the missing entries, and then\nupdates our estimate by computing the leading right singular space of the\nimputed data matrix. It turns out that the interaction between the\nheterogeneity of missingness and the low-dimensional structure is crucial in\ndetermining the feasibility of the problem. We therefore introduce an\nincoherence condition on the principal components and prove that in the\nnoiseless case, the error of primePCA converges to zero at a geometric rate\nwhen the signal strength is not too small. An important feature of our\ntheoretical guarantees is that they depend on average, as opposed to\nworst-case, properties of the missingness mechanism. Our numerical studies on\nboth simulated and real data reveal that primePCA exhibits very encouraging\nperformance across a wide range of scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 10:37:16 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Zhu", "Ziwei", ""], ["Wang", "Tengyao", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1906.12208", "submitter": "Junmo Song", "authors": "Junmo Song", "title": "Robust test for dispersion parameter change in discretely observed\n  diffusion processes", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of testing for dispersion parameter change\nin discretely observed diffusion processes when the observations are\ncontaminated by outliers. To lessen the impact of outliers, we first calculate\nresiduals using a robust estimate and then propose a trimmed-residual based\nCUSUM test. The proposed test is shown to converge weakly to a function of the\nBrownian bridge under the null hypothesis of no parameter change. We conduct\nsimulations to evaluate performances of the proposed test in the presence of\noutliers. Numerical results confirm that the proposed test posses a strong\nrobust property against outliers. In real data analysis, we fit the\nOrnstein-Uhlenbeck process to KOSPI200 volatility index data and locate some\nchange points that are not detected by a naive CUSUM test.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:28:07 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Song", "Junmo", ""]]}]