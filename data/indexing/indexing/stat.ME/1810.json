[{"id": "1810.00042", "submitter": "Shu Yang", "authors": "Shu Yang", "title": "Semiparametric efficient estimation of structural nested mean models\n  with irregularly spaced observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural Nested Mean Models (SNMMs) are useful for causal inference of\ntreatment effects in longitudinal observational studies. Most existing works\nassume that the data are collected at pre-fixed time points for all subjects,\nwhich, however, is restrictive in practice. To deal with irregularly spaced\nobservations, we assume a class of continuous-time SNMMs and a martingale\ncondition of no unmeasured confounding (NUC) to identify the causal parameters.\nWe develop the first semiparametric efficiency theory and locally efficient\nestimators for continuous-time SNMMs. This task is non-trivial due to the\nrestrictions from the NUC assumption imposed on the SNMM parameter. In the\npresence of dependent censoring, we propose an inverse probability of censoring\nweighting estimator, which achieves a multiple robustness feature in that it is\nunbiased if either the model for the treatment process or the potential outcome\nmean function is correctly specified, regardless whether the censoring model is\ncorrectly specified. The new framework allows us to conduct causal analysis\nrespecting the underlying continuous-time nature of the data processes. We\nestimate the effect of time to initiate highly active antiretroviral therapy on\nthe CD4 count at year 2 from the observational Acute Infection and Early\nDisease Research Program database.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 18:51:24 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 10:35:54 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 21:55:26 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Yang", "Shu", ""]]}, {"id": "1810.00121", "submitter": "Garritt Page", "authors": "Garritt L. Page, Fernando A. Quintana, Gary L. Rosner", "title": "Discovering Interactions Using Covariate Informed Random Partition\n  Models", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combination chemotherapy treatment regimens created for patients diagnosed\nwith childhood acute lymphoblastic leukemia have had great success in improving\ncure rates. Unfortunately, patients prescribed these types of treatment\nregimens have displayed susceptibility to the onset of osteonecrosis. Some have\nsuggested that this is due to pharmacokinetic interaction between two agents in\nthe treatment regimen (asparaginase and dexamethasone) and other physiological\nvariables. Determining which physiological variables to consider when searching\nfor interactions in scenarios like these, minus a priori guidance, has proved\nto be a challenging problem, particularly if interactions influence the\nresponse distribution in ways beyond shifts in expectation or dispersion only.\nIn this paper we propose an exploratory technique that is able to discover\nassociations between covariates and responses in a very general way. The\nprocedure connects covariates to responses very flexibly through dependent\nrandom partition prior distributions, and then employs machine learning\ntechniques to highlight potential associations found in each cluster. We\nprovide a simulation study to show utility and apply the method to data\nproduced from a study dedicated to learning which physiological predictors\ninfluence severity of osteonecrosis multiplicatively.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 00:49:53 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 01:43:20 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Page", "Garritt L.", ""], ["Quintana", "Fernando A.", ""], ["Rosner", "Gary L.", ""]]}, {"id": "1810.00141", "submitter": "Minsuk Shin", "authors": "Minsuk Shin and Jun S Liu", "title": "Neuronized Priors for Bayesian Sparse Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Bayesian variable selection methods have been intensively studied,\ntheir routine use in practice has not caught up with their non-Bayesian\ncounterparts such as Lasso, likely due to difficulties in both computations and\nflexibilities of prior choices. To ease these challenges, we propose the\nneuronized priors to unify and extend some popular shrinkage priors, such as\nLaplace, Cauchy, horseshoe, and spike-and-slab priors. A neuronized prior can\nbe written as the product of a Gaussian weight variable and a scale variable\ntransformed from Gaussian via an activation function. Compared with classic\nspike-and-slab priors, the neuronized priors achieve the same explicit variable\nselection without employing any latent indicator variables, which results in\nboth more efficient and flexible posterior sampling and more effective\nposterior modal estimation. Theoretically, we provide specific conditions on\nthe neuronized formulation to achieve the optimal posterior contraction rate,\nand show that a broadly applicable MCMC algorithm achieves an exponentially\nfast convergence rate under the neuronized formulation. We also examine various\nsimulated and real data examples and demonstrate that using the neuronization\nrepresentation is computationally more or comparably efficient than its\nstandard counterpart in all well-known cases. An R package NPrior is provided\nin the CRAN for using neuronized priors in Bayesian linear regression.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 03:33:38 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 14:30:43 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 02:55:42 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Shin", "Minsuk", ""], ["Liu", "Jun S", ""]]}, {"id": "1810.00274", "submitter": "Jian Kang", "authors": "Qingpo Cai, Jian Kang, Tianwei Yu", "title": "Bayesian network marker selection via the thresholded graph Laplacian\n  Gaussian prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting informative nodes over large-scale networks becomes increasingly\nimportant in many research areas. Most existing methods focus on the local\nnetwork structure and incur heavy computational costs for the large-scale\nproblem. In this work, we propose a novel prior model for Bayesian network\nmarker selection in the generalized linear model (GLM) framework: the\nThresholded Graph Laplacian Gaussian (TGLG) prior, which adopts the graph\nLaplacian matrix to characterize the conditional dependence between neighboring\nmarkers accounting for the global network structure. Under mild conditions, we\nshow the proposed model enjoys the posterior consistency with a diverging\nnumber of edges and nodes in the network. We also develop a Metropolis-adjusted\nLangevin algorithm (MALA) for efficient posterior computation, which is\nscalable to large-scale networks. We illustrate the superiorities of the\nproposed method compared with existing alternatives via extensive simulation\nstudies and an analysis of the breast cancer gene expression dataset in the\nCancer Genome Atlas (TCGA).\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 22:53:42 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Cai", "Qingpo", ""], ["Kang", "Jian", ""], ["Yu", "Tianwei", ""]]}, {"id": "1810.00412", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban, Yue Sheng", "title": "Distributed linear regression by averaging", "comments": "V2 adds a new section on iterative averaging methods, adds\n  applications of the calculus of deterministic equivalents, and reorganizes\n  the paper", "journal-ref": "Annals of Statistics, 2020+", "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed statistical learning problems arise commonly when dealing with\nlarge datasets. In this setup, datasets are partitioned over machines, which\ncompute locally, and communicate short messages. Communication is often the\nbottleneck. In this paper, we study one-step and iterative weighted parameter\naveraging in statistical linear models under data parallelism. We do linear\nregression on each machine, send the results to a central server, and take a\nweighted average of the parameters. Optionally, we iterate, sending back the\nweighted average and doing local ridge regressions centered at it. How does\nthis work compared to doing linear regression on the full data? Here we study\nthe performance loss in estimation, test error, and confidence interval length\nin high dimensions, where the number of parameters is comparable to the\ntraining data size. We find the performance loss in one-step weighted\naveraging, and also give results for iterative averaging. We also find that\ndifferent problems are affected differently by the distributed framework.\nEstimation error and confidence interval length increase a lot, while\nprediction error increases much less. We rely on recent results from random\nmatrix theory, where we develop a new calculus of deterministic equivalents as\na tool of broader interest.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 15:59:03 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 04:33:28 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Dobriban", "Edgar", ""], ["Sheng", "Yue", ""]]}, {"id": "1810.00678", "submitter": "Hildete Pinheiro", "authors": "Hildete P. Pinheiro, Pranab K. Sen, Alu\\'isio Pinheiro and Samara F.\n  Kiihl", "title": "A nonparametric approach to assess undergraduate performance", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric methodologies are proposed to assess college students'\nperformance. Emphasis is given to gender and sector of High School. The\napplication concerns the University of Campinas, a research university in\nSoutheast Brazil. In Brazil college is based on a somewhat rigid set of\nsubjects for each major. Thence a student's relative performance can not be\naccurately measured by the Grade Point Average or by any other single measure.\nWe then define individual vectors of course grades. These vectors are used in\npairwise comparisons of common subject grades for individuals that entered\ncollege in the same year. The relative college performances of any two students\nis compared to their relative performances on the Entrance Exam Score. A test\nbased on generalized U-statistics is developed for homogeneity of some\npredefined groups. Asymptotic normality of the test statistic is true for both\nnull and alternative hypotheses. Maximum power is attained by employing the\nunion intersection principle.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 12:51:08 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Pinheiro", "Hildete P.", ""], ["Sen", "Pranab K.", ""], ["Pinheiro", "Alu\u00edsio", ""], ["Kiihl", "Samara F.", ""]]}, {"id": "1810.00680", "submitter": "Boris Beranger", "authors": "Boris Beranger, Simone A. Padoan, Yangfan Xu, Scott A. Sisson", "title": "Extremal properties of the multivariate extended skew-normal\n  distribution", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.03316", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The skew-normal and related families are flexible and asymmetric parametric\nmodels suitable for modelling a diverse range of systems. We show that the\nmultivariate maximum of a high-dimensional extended skew-normal random sample\nhas asymptotically independent components and derive the speed of convergence\nof the joint tail. To describe the possible dependence among the components of\nthe multivariate maximum, we show that under appropriate conditions an\napproximate multivariate extreme-value distribution that leads to a rich\ndependence structure can be derived.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 03:21:18 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Beranger", "Boris", ""], ["Padoan", "Simone A.", ""], ["Xu", "Yangfan", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1810.00709", "submitter": "Ruitao Lin", "authors": "Ruitao Lin, Robert L Coleman, Ying Yuan", "title": "TOP: Time-to-Event Bayesian Optimal Phase II Trial Design for Cancer\n  Immunotherapy", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immunotherapies have revolutionized cancer treatment. Unlike chemotherapies,\nimmune agents often take longer time to show benefit, and the complex and\nunique mechanism of action of these agents renders the use of multiple\nendpoints more appropriate in some trials. These new features of immunotherapy\nmake conventional phase II trial designs, which assume a single binary endpoint\nthat is quickly ascertainable, inefficient and dysfunctional. We propose a\nflexible and efficient time-to-event Bayesian optimal phase II (TOP) design.\nThe TOP design is efficient in that it allows real-time \"go/no-go\" interim\ndecision making in the presence of late-onset responses by using all available\ndata, and maximizes the statistical power for detecting effective treatments.\nTOP is flexible in the number of interim looks and capable of handling simple\nand complicated endpoints under a unified framework. We conduct simulation\nstudies to evaluate the operating characteristics of the TOP design.Compared to\nsome existing designs, the TOP design shortens the trial duration and has\nhigher power to detect effective treatment with well controlled type I errors.\nThe TOP design allows for making real-time \"go/no-go\" interim decisions in the\npresence of late-onset responses, and is capable of handling various types of\nendpoints under a unified framework. It is transparent and easy to implement as\nits decision rules can be tabulated and included in the protocol prior to the\nconduct of the trial. The TOP design provides a flexible, efficient and\neasy-to-implement method to accelerate and improve the development of\nimmunotherapies.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 14:06:19 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Lin", "Ruitao", ""], ["Coleman", "Robert L", ""], ["Yuan", "Ying", ""]]}, {"id": "1810.00739", "submitter": "Ryan Martin", "authors": "Chang Liu, Yue Yang, Howard Bondell, Ryan Martin", "title": "Bayesian inference in high-dimensional linear models using an empirical\n  correlation-adaptive prior", "comments": "25 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of a high-dimensional linear regression model, we propose the\nuse of an empirical correlation-adaptive prior that makes use of information in\nthe observed predictor variable matrix to adaptively address high collinearity,\ndetermining if parameters associated with correlated predictors should be\nshrunk together or kept apart. Under suitable conditions, we prove that this\nempirical Bayes posterior concentrates around the true sparse parameter at the\noptimal rate asymptotically. A simplified version of a shotgun stochastic\nsearch algorithm is employed to implement the variable selection procedure, and\nwe show, via simulation experiments across different settings and a real-data\napplication, the favorable performance of the proposed method compared to\nexisting methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 14:51:26 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Liu", "Chang", ""], ["Yang", "Yue", ""], ["Bondell", "Howard", ""], ["Martin", "Ryan", ""]]}, {"id": "1810.00767", "submitter": "Maria Cuellar", "authors": "Maria Cuellar and Edward H. Kennedy", "title": "A nonparametric projection-based estimator for the probability of\n  causation, with application to water sanitation in Kenya", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current estimation methods for the probability of causation (PC) make strong\nparametric assumptions or are inefficient. We derive a nonparametric\ninfluence-function-based estimator for a projection of PC, which allows for\nsimple interpretation and valid inference by making weak structural\nassumptions. We apply our estimator to real data from an experiment in Kenya,\nwhich found, by estimating the average treatment effect, that protecting water\nsprings reduces childhood disease. However, before scaling up this\nintervention, it is important to determine whether it was the exposure, and not\nsomething else, that caused the outcome. Indeed, we find that some children,\nwho were exposed to a high concentration of bacteria in drinking water and had\na diarrheal disease, would likely have contracted the disease absent the\nexposure since the estimated PC for an average child in this study is 0.12 with\na 95% confidence interval of (0.11, 0.13). Our nonparametric method offers\nresearchers a way to estimate PC, which is essential if one wishes to determine\nnot only the average treatment effect, but also whether an exposure likely\ncaused the observed outcome.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 15:45:39 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 15:50:44 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 16:02:07 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Cuellar", "Maria", ""], ["Kennedy", "Edward H.", ""]]}, {"id": "1810.00786", "submitter": "Deirel Paz-Linares", "authors": "Eduardo Gonzalez-Moreira, Deirel Paz-Linares, Ariosky Areces-Gonzalez,\n  Rigel Wang, Jorge Bosch-Bayard, Maria Luisa Bringas-Vega and Pedro A.\n  Valdes-Sosa", "title": "Caulking the Leakage Effect in MEEG Source Connectivity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simplistic estimation of neural connectivity in MEEG sensor space is\nimpossible due to volume conduction. The only viable alternative is to carry\nout connectivity estimation in source space. Among the neuroscience community\nthis is claimed to be impossible or misleading due to Leakage: linear mixing of\nthe reconstructed sources. To address this problematic we propose a novel\nsolution method that caulks the Leakage in MEEG source activity and\nconnectivity estimates: BC-VARETA. It is based on a joint estimation of source\nactivity and connectivity in the frequency domain representation of MEEG time\nseries. To achieve this, we go beyond current methods that assume a fixed\ngaussian graphical model for source connectivity. In contrast we estimate this\ngraphical model in a Bayesian framework by placing priors on it, which allows\nfor highly optimized computations of the connectivity, via a new procedure\nbased on the local quadratic approximation under quite general prior models. A\nfurther contribution of this paper is the rigorous definition of leakage via\nthe Spatial Dispersion Measure and Earth Movers Distance based on the geodesic\ndistances over the cortical manifold. Both measures are extended for the first\ntime to quantify Connectivity Leakage by defining them on the cartesian product\nof cortical manifolds. Using these measures, we show that BC-VARETA outperforms\nmost state of the art inverse solvers by several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 12:19:05 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 00:19:07 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Gonzalez-Moreira", "Eduardo", ""], ["Paz-Linares", "Deirel", ""], ["Areces-Gonzalez", "Ariosky", ""], ["Wang", "Rigel", ""], ["Bosch-Bayard", "Jorge", ""], ["Bringas-Vega", "Maria Luisa", ""], ["Valdes-Sosa", "Pedro A.", ""]]}, {"id": "1810.00832", "submitter": "Tiffany Tang", "authors": "Tiffany M. Tang, Genevera I. Allen", "title": "Integrated Principal Components Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration, or the strategic analysis of multiple sources of data\nsimultaneously, can often lead to discoveries that may be hidden in\nindividualistic analyses of a single data source. We develop a new unsupervised\ndata integration method named Integrated Principal Components Analysis (iPCA),\nwhich is a model-based generalization of PCA and serves as a practical tool to\nfind and visualize common patterns that occur in multiple data sets. The key\nidea driving iPCA is the matrix-variate normal model, whose Kronecker product\ncovariance structure captures both individual patterns within each data set and\njoint patterns shared by multiple data sets. Building upon this model, we\ndevelop several penalized (sparse and non-sparse) covariance estimators for\niPCA, and using geodesic convexity, we prove that our non-sparse iPCA estimator\nconverges to the global solution of a non-convex problem. We also demonstrate\nthe practical advantages of iPCA through extensive simulations and a case study\napplication to integrative genomics for Alzheimer's disease. In particular, we\nshow that the joint patterns extracted via iPCA are highly predictive of a\npatient's cognition and Alzheimer's diagnosis.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 17:21:50 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 06:53:21 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 21:27:49 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tang", "Tiffany M.", ""], ["Allen", "Genevera I.", ""]]}, {"id": "1810.00908", "submitter": "Rahul Ghosal", "authors": "Indrabati Bhattacharya, Rahul Ghosal, Sujit Ghosh", "title": "A Statistical Exploration of Duckworth-Lewis Method Using Bayesian\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Duckworth-Lewis (D/L) method is the incumbent rain rule used to decide the\nresult of a limited overs cricket match should it not be able to reach its\nnatural conclusion. Duckworth and Lewis (1998) devised a two factor\nrelationship between the numbers of overs a team had remaining and the number\nof wickets they had lost in order to quantify the percentage resources a team\nhas at any stage of the match. As number of remaining overs decrease and lost\nwickets increase the resources are expected to decrease. The resource table\nwhich is still being used by ICC (International Cricket Council) for 50 overs\ncricket match suffers from lack of monotonicity both in numbers of overs left\nand number of wickets lost. We apply Bayesian inference to build a resource\ntable which overcomes the non monotonicity problem of the current D/L resource\ntable and show that it gives better prediction for teams in first innings score\nand hence it is more suitable for using in rain affected matches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 18:24:01 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Bhattacharya", "Indrabati", ""], ["Ghosal", "Rahul", ""], ["Ghosh", "Sujit", ""]]}, {"id": "1810.00919", "submitter": "Irene Epifanio", "authors": "Jes\\'us Moliner, Irene Epifanio", "title": "Robust multivariate and functional archetypal analysis with application\n  to financial time series analysis", "comments": "Physica A: Statistical Mechanics and its Applications, 2019", "journal-ref": null, "doi": "10.1016/j.physa.2018.12.036", "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archetypal analysis approximates data by means of mixtures of actual extreme\ncases (archetypoids) or archetypes, which are a convex combination of cases in\nthe data set. Archetypes lie on the boundary of the convex hull. This makes the\nanalysis very sensitive to outliers. A robust methodology by means of\nM-estimators for classical multivariate and functional data is proposed. This\nunsupervised methodology allows complex data to be understood even by\nnon-experts. The performance of the new procedure is assessed in a simulation\nstudy, where a comparison with a previous methodology for the multivariate case\nis also carried out, and our proposal obtains favorable results. Finally,\nrobust bivariate functional archetypoid analysis is applied to a set of\ncompanies in the S\\&P 500 described by two time series of stock quotes. A new\ngraphic representation is also proposed to visualize the results. The analysis\nshows how the information can be easily interpreted and how even non-experts\ncan gain a qualitative understanding of the data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 18:48:26 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 17:18:57 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Moliner", "Jes\u00fas", ""], ["Epifanio", "Irene", ""]]}, {"id": "1810.01072", "submitter": "Kenyon Ng", "authors": "Kenyon Ng, Berwin A. Turlach, Kevin Murray", "title": "A flexible sequential Monte Carlo algorithm for parametric constrained\n  regression", "comments": "Typo corrections. Code available on\n  https://github.com/weiyaw/blackbox", "journal-ref": "Computational Statistics & Data Analysis 138 (2019) 13-26", "doi": "10.1016/j.csda.2019.03.011", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm is proposed that enables the imposition of shape constraints on\nregression curves, without requiring the constraints to be written as\nclosed-form expressions, nor assuming the functional form of the loss function.\nThis algorithm is based on Sequential Monte Carlo-Simulated Annealing and only\nrelies on an indicator function that assesses whether or not the constraints\nare fulfilled, thus allowing the enforcement of various complex constraints by\nspecifying an appropriate indicator function without altering other parts of\nthe algorithm. The algorithm is illustrated by fitting rational function and\nB-spline regression models subject to a monotonicity constraint. An\nimplementation of the algorithm using R is freely available on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 05:18:44 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 12:26:58 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 05:26:21 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Ng", "Kenyon", ""], ["Turlach", "Berwin A.", ""], ["Murray", "Kevin", ""]]}, {"id": "1810.01174", "submitter": "Deirel Paz-Linares", "authors": "Deirel Paz-Linares, Eduardo Gonzalez-Moreira, Jorge Bosch-Bayard,\n  Ariosky Areces-Gonzalez, Maria L. Bringas-Vega and Pedro A. Valdes-Sosa", "title": "Neural Connectivity with Hidden Gaussian Graphical State-Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The noninvasive procedures for neural connectivity are under questioning.\nTheoretical models sustain that the electromagnetic field registered at\nexternal sensors is elicited by currents at neural space. Nevertheless, what we\nobserve at the sensor space is a superposition of projected fields, from the\nwhole gray-matter. This is the reason for a major pitfall of noninvasive\nElectrophysiology methods: distorted reconstruction of neural activity and its\nconnectivity or leakage. It has been proven that current methods produce\nincorrect connectomes. Somewhat related to the incorrect connectivity\nmodelling, they disregard either Systems Theory and Bayesian Information\nTheory. We introduce a new formalism that attains for it, Hidden Gaussian\nGraphical State-Model (HIGGS). A neural Gaussian Graphical Model (GGM) hidden\nby the observation equation of Magneto-encephalographic (MEEG) signals. HIGGS\nis equivalent to a frequency domain Linear State Space Model (LSSM) but with\nsparse connectivity prior. The mathematical contribution here is the theory for\nhigh-dimensional and frequency-domain HIGGS solvers. We demonstrate that HIGGS\ncan attenuate the leakage effect in the most critical case: the distortion EEG\nsignal due to head volume conduction heterogeneities. Its application in EEG is\nillustrated with retrieved connectivity patterns from human Steady State Visual\nEvoked Potentials (SSVEP). We provide for the first time confirmatory evidence\nfor noninvasive procedures of neural connectivity: concurrent EEG and\nElectrocorticography (ECoG) recordings on monkey. Open source packages are\nfreely available online, to reproduce the results presented in this paper and\nto analyze external MEEG databases.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 11:26:54 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 12:02:11 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 11:15:30 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Paz-Linares", "Deirel", ""], ["Gonzalez-Moreira", "Eduardo", ""], ["Bosch-Bayard", "Jorge", ""], ["Areces-Gonzalez", "Ariosky", ""], ["Bringas-Vega", "Maria L.", ""], ["Valdes-Sosa", "Pedro A.", ""]]}, {"id": "1810.01195", "submitter": "Han Lin Shang", "authors": "Yuan Gao, Han Lin Shang, Yanrong Yang", "title": "High-dimensional functional time series forecasting: An application to\n  age-specific mortality rates", "comments": "17 pages, 2 figures, To appear in Journal of Multivariate Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of forecasting high-dimensional functional time series\nthrough a two-fold dimension reduction procedure. The difficulty of forecasting\nhigh-dimensional functional time series lies in the curse of dimensionality. In\nthis paper, we propose a novel method to solve this problem. Dynamic functional\nprincipal component analysis is first applied to reduce each functional time\nseries to a vector. We then use the factor model as a further dimension\nreduction technique so that only a small number of latent factors are\npreserved. Classic time series models can be used to forecast the factors and\nconditional forecasts of the functions can be constructed. Asymptotic\nproperties of the approximated functions are established, including both\nestimation error and forecast error. The proposed method is easy to implement\nespecially when the dimension of the functional time series is large. We show\nthe superiority of our approach by both simulation studies and an application\nto Japanese age-specific mortality rates.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 12:11:23 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Gao", "Yuan", ""], ["Shang", "Han Lin", ""], ["Yang", "Yanrong", ""]]}, {"id": "1810.01294", "submitter": "Lola Etievant", "authors": "Lola Etievant and Vivian Viallon", "title": "Causal inference under over-simplified longitudinal causal models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many causal models of interest in epidemiology involve longitudinal\nexposures, confounders and mediators. However, in practice, repeated\nmeasurements are not always available. Then, practitioners tend to overlook the\ntime-varying nature of exposures and work under over-simplified causal models.\nOur objective here was to assess whether - and how - the causal effect\nidentified under such misspecified causal models relates to true causal effects\nof interest. We focus on situations regarding the type of available data for\nexposures: when they correspond to (i) ``instantaneous'' levels measured at\ninclusion in the study or (ii) summary measures of their levels up to inclusion\nin the study. In each of these two situations, we derive sufficient conditions\nensuring that the quantities estimated in practice under over-simplified causal\nmodels can be expressed as true longitudinal causal effects of interest, or\nsome weighted averages thereof. Unsurprisingly, these sufficient conditions are\nvery restrictive, and our results state that inference based on either\n``instantaneous'' levels or summary measures usually returns quantities that do\nnot directly relate to any causal effect of interest and should be interpreted\nwith caution. They raise the need for the availability of repeated measurements\nand/or the development of sensitivity analyses when such data is not available.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 14:29:55 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 09:35:05 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 15:23:51 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Etievant", "Lola", ""], ["Viallon", "Vivian", ""]]}, {"id": "1810.01296", "submitter": "Gaonyalelwe Maribe Mr", "authors": "Jan Beirlant, Gaonyalelwe Maribe, Philippe Naveau and Andrehette\n  Verster", "title": "Bias Reduced Peaks over Threshold Tail Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years several attempts have been made to extend tail modelling\ntowards the modal part of the data. Frigessi et al. (2002) introduced dynamic\nmixtures of two components with a weight function {\\pi} = {\\pi}(x) smoothly\nconnecting the bulk and the tail of the distribution. Recently, Naveau et al.\n(2016) reviewed this topic, and, continuing on the work by Papastathopoulos and\nTawn (2013), proposed a statistical model which is in compliance with extreme\nvalue theory and allows for a smooth transition between the modal and tail\npart. Incorporating second order rates of convergence for distributions of\npeaks over thresholds (POT), Beirlant et al. (2002, 2009) constructed models\nthat can be viewed as special cases from both approaches discussed above. When\nfitting such second order models it turns out that the bias of the resulting\nextreme value estimators is significantly reduced compared to the classical\ntail fits using only the first order tail component based on the Pareto or\ngeneralized Pareto fits to peaks over threshold distributions. In this paper we\nprovide novel bias reduced tail fitting techniques, improving upon the\nclassical generalized Pareto (GP) approximation for POTs using the flexible\nsemiparametric GP modelling introduced in Tencaliec et al. (2018). We also\nrevisit and extend the secondorder refined POT approach started in Beirlant et\nal. (2009) to all max-domains of attraction using flexible semiparametric\nmodelling of the second order component. In this way we relax the classical\nsecond order regular variation assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 14:31:53 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Beirlant", "Jan", ""], ["Maribe", "Gaonyalelwe", ""], ["Naveau", "Philippe", ""], ["Verster", "Andrehette", ""]]}, {"id": "1810.01300", "submitter": "Shankar Bhamidi", "authors": "Nelson Antunes and Shankar Bhamidi and Tianjian Guo and Vladas Pipiras\n  and Bang Wang", "title": "Sampling-based Estimation of In-degree Distribution with Applications to\n  Directed Complex Networks", "comments": "30 pages , 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this work is on estimation of the in-degree distribution in\ndirected networks from sampling network nodes or edges. A number of sampling\nschemes are considered, including random sampling with and without replacement,\nand several approaches based on random walks with possible jumps. When sampling\nnodes, it is assumed that only the out-edges of that node are visible, that is,\nthe in-degree of that node is not observed. The suggested estimation of the\nin-degree distribution is based on two approaches. The inversion approach\nexploits the relation between the original and sample in-degree distributions,\nand can estimate the bulk of the in-degree distribution, but not the tail of\nthe distribution. The tail of the in-degree distribution is estimated through\nan asymptotic approach, which itself has two versions: one assuming a power-law\ntail and the other for a tail of general form. The two estimation approaches\nare examined on synthetic and real networks, with good performance results,\nespecially striking for the asymptotic approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 14:34:53 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Antunes", "Nelson", ""], ["Bhamidi", "Shankar", ""], ["Guo", "Tianjian", ""], ["Pipiras", "Vladas", ""], ["Wang", "Bang", ""]]}, {"id": "1810.01370", "submitter": "Pedro H. C. Sant'Anna", "authors": "Pedro H. C. Sant'Anna, Xiaojun Song, Qi Xu", "title": "Covariate Distribution Balance via Propensity Scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes new estimators for the propensity score that aim to\nmaximize the covariate distribution balance among different treatment groups.\nHeuristically, our proposed procedure attempts to estimate a propensity score\nmodel by making the underlying covariate distribution of different treatment\ngroups as close to each other as possible. Our estimators are data-driven, do\nnot rely on tuning parameters such as bandwidths, admit an asymptotic linear\nrepresentation, and can be used to estimate different treatment effect\nparameters under different identifying assumptions, including unconfoundedness\nand local treatment effects. We derive the asymptotic properties of inverse\nprobability weighted estimators for the average, distributional, and quantile\ntreatment effects based on the proposed propensity score estimator and\nillustrate their finite sample performance via Monte Carlo simulations and two\nempirical applications.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 17:00:13 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 20:34:02 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 01:59:46 GMT"}, {"version": "v4", "created": "Sat, 4 Apr 2020 02:25:35 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sant'Anna", "Pedro H. C.", ""], ["Song", "Xiaojun", ""], ["Xu", "Qi", ""]]}, {"id": "1810.01382", "submitter": "Maxime Rischard", "authors": "Maxime Rischard, Pierre E. Jacob, Natesh Pillai", "title": "Unbiased estimation of log normalizing constants with applications to\n  Bayesian cross-validation", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Posterior distributions often feature intractable normalizing constants,\ncalled marginal likelihoods or evidence, that are useful for model comparison\nvia Bayes factors. This has motivated a number of methods for estimating ratios\nof normalizing constants in statistics. In computational physics the logarithm\nof these ratios correspond to free energy differences. Combining unbiased\nMarkov chain Monte Carlo estimators with path sampling, also called\nthermodynamic integration, we propose new unbiased estimators of the logarithm\nof ratios of normalizing constants. As a by-product, we propose unbiased\nestimators of the Bayesian cross-validation criterion. The proposed estimators\nare consistent, asymptotically Normal and can easily benefit from parallel\nprocessing devices. Various examples are considered for illustration.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 17:26:18 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Rischard", "Maxime", ""], ["Jacob", "Pierre E.", ""], ["Pillai", "Natesh", ""]]}, {"id": "1810.01509", "submitter": "Tianxi Li", "authors": "Tianxi Li, Lihua Lei, Sharmodeep Bhattacharyya, Koen Van den Berge,\n  Purnamrita Sarkar, Peter J. Bickel, Elizaveta Levina", "title": "Hierarchical community detection by recursive partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of community detection in networks is usually formulated as\nfinding a single partition of the network into some \"correct\" number of\ncommunities. We argue that it is more interpretable and in some regimes more\naccurate to construct a hierarchical tree of communities instead. This can be\ndone with a simple top-down recursive partitioning algorithm, starting with a\nsingle community and separating the nodes into two communities by spectral\nclustering repeatedly, until a stopping rule suggests there are no further\ncommunities. This class of algorithms is model-free, computationally efficient,\nand requires no tuning other than selecting a stopping rule. We show that there\nare regimes where this approach outperforms K-way spectral clustering, and\npropose a natural framework for analyzing the algorithm's theoretical\nperformance, the binary tree stochastic block model. Under this model, we prove\nthat the algorithm correctly recovers the entire community tree under\nrelatively mild assumptions. We apply the algorithm to a gene network based on\ngene co-occurrence in 1580 research papers on anemia, and identify six clusters\nof genes in a meaningful hierarchy. We also illustrate the algorithm on a\ndataset of statistics papers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 20:58:20 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 06:41:52 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 22:43:37 GMT"}, {"version": "v4", "created": "Thu, 7 Mar 2019 19:46:12 GMT"}, {"version": "v5", "created": "Fri, 13 Sep 2019 04:37:41 GMT"}, {"version": "v6", "created": "Thu, 14 May 2020 05:53:24 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Li", "Tianxi", ""], ["Lei", "Lihua", ""], ["Bhattacharyya", "Sharmodeep", ""], ["Berge", "Koen Van den", ""], ["Sarkar", "Purnamrita", ""], ["Bickel", "Peter J.", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1810.01538", "submitter": "Andee Kaplan", "authors": "Andee Kaplan, Brenda Betancourt, and Rebecca C. Steorts", "title": "Posterior Prototyping: Bridging the Gap between Bayesian Record Linkage\n  and Regression", "comments": "23 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage (entity resolution or de-deduplication) is the process of\nmerging noisy databases to remove duplicate entities. While record linkage\nremoves duplicate entities from the data, many researchers are interested in\nperforming inference, prediction or post-linkage analysis on the linked data,\nwhich we call the downstream task. Depending on the downstream task, one may\nwish to find the most representative record before performing the post-linkage\nanalysis. Motivated by the downstream task, we propose first performing record\nlinkage using a Bayesian model and then choosing representative records through\nprototyping. Given the information about the representative records, we then\nexplore two downstream tasks - linear regression and binary classification via\nlogistic regression. In addition, we explore how error propagation occurs in\nboth of these settings. We provide thorough empirical studies for our proposed\nmethodology, and conclude with a discussion of practical insights into our\nwork.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 22:55:58 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Kaplan", "Andee", ""], ["Betancourt", "Brenda", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1810.01576", "submitter": "Tymon Sloczynski", "authors": "Tymon S{\\l}oczy\\'nski", "title": "Interpreting OLS Estimands When Treatment Effects Are Heterogeneous:\n  Smaller Groups Get Larger Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applied work often studies the effect of a binary variable (\"treatment\")\nusing linear models with additive effects. I study the interpretation of the\nOLS estimands in such models when treatment effects are heterogeneous. I show\nthat the treatment coefficient is a convex combination of two parameters, which\nunder certain conditions can be interpreted as the average treatment effects on\nthe treated and untreated. The weights on these parameters are inversely\nrelated to the proportion of observations in each group. Reliance on these\nimplicit weights can have serious consequences for applied work, as I\nillustrate with two well-known applications. I develop simple diagnostic tools\nthat empirical researchers can use to avoid potential biases. Software for\nimplementing these methods is available in R and Stata. In an important special\ncase, my diagnostics only require the knowledge of the proportion of treated\nunits.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 04:01:27 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 17:39:19 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 22:04:18 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["S\u0142oczy\u0144ski", "Tymon", ""]]}, {"id": "1810.01625", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo, Modou Ngom, Tchilabola Abozou Kpanzou, Mouminou Diallo", "title": "Weak Convergence (IIA) - Functional and Random Aspects of the Univariate\n  Extreme Value Theory", "comments": "175 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The univariate extreme value theory deals with the convergence in type of\npowers of elements of sequences of cumulative distribution functions on the\nreal line when the power index gets infinite. In terms of convergence of random\nvariables, this amounts to the the weak convergence, in the sense of\nprobability measures weak convergence, of the partial maximas of a sequence of\nindependent and identically distributed random variables. In this monograph,\nthis theory is comprehensively studied in the broad frame of weak convergence\nof random vectors as exposed in Lo et al.(2016). It has two main parts. The\nfirst is devoted to its nice mathematical foundation. Most of the materials of\nthis part is taken from the most essential Lo\\`eve(1936,177) and Haan (1970),\nbased on the stunning theory of regular, pi or gamma variation. To prepare the\nstatistical applications, a number contributions I made in my PhD and my\nDoctorate of Sciences are added in the last chapter of the last chapter of that\npart. Our real concern is to put these materials together with others, among\nthem those of the authors from his PhD dissertations and Science doctorate\nthesis, in a way to have an almost full coverage of the theory on the real line\nthat may serve as a master course of one semester in our universities. As well,\nit will help the second part of the monograph. This second part will deal with\nstatistical estimations problems related to extreme values. It addresses\nvarious estimation questions and should be considered as the beginning of a\nsurvey study to be updated progressively. Research questions are tackled\ntherein. Many results of the author, either unpublished or not sufficiently\nknown, are stated and/or updated therein.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 08:19:31 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Lo", "Gane Samb", ""], ["Ngom", "Modou", ""], ["Kpanzou", "Tchilabola Abozou", ""], ["Diallo", "Mouminou", ""]]}, {"id": "1810.01670", "submitter": "Fr\\'ed\\'eric Bertrand", "authors": "Isma\\\"il Aouadi, Nicolas Jung, Raphael Carapito, Laurent Vallat,\n  Seiamak Bahram, Myriam Maumy-Bertrand and Fr\\'ed\\'eric Bertrand", "title": "selectBoost: a general algorithm to enhance the performance of variable\n  selection methods in correlated datasets", "comments": "This article supersedes arXiv:1512.03307", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: With the growth of big data, variable selection has become one of\nthe major challenges in statistics. Although many methods have been proposed in\nthe literature their performance in terms of recall and precision are limited\nin a context where the number of variables by far exceeds the number of\nobservations or in a high correlated setting.\n  Results: In this article, we propose a general algorithm which improves the\nprecision of any existing variable selection method. This algorithm is based on\nhighly intensive simulations and takes into account the correlation structure\nof the data. Our algorithm can either produce a confidence index for variable\nselection or it can be used in an experimental design planning perspective. We\ndemonstrate the performance of our algorithm on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 10:05:24 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Aouadi", "Isma\u00efl", ""], ["Jung", "Nicolas", ""], ["Carapito", "Raphael", ""], ["Vallat", "Laurent", ""], ["Bahram", "Seiamak", ""], ["Maumy-Bertrand", "Myriam", ""], ["Bertrand", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1810.01675", "submitter": "Sanjay Chaudhuri", "authors": "Sanjay Chaudhuri, Subhro Ghosh, David J. Nott, Kim Cuc Pham", "title": "An easy-to-use empirical likelihood ABC method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientifically well-motivated statistical models in natural, engineering\nand environmental sciences are specified through a generative process, but in\nsome cases it may not be possible to write down a likelihood for these models\nanalytically. Approximate Bayesian computation (ABC) methods, which allow\nBayesian inference in these situations, are typically computationally\nintensive. Recently, computationally attractive empirical likelihood based ABC\nmethods have been suggested in the literature. These methods heavily rely on\nthe availability of a set of suitable analytically tractable estimating\nequations. We propose an easy-to-use empirical likelihood ABC method, where the\nonly inputs required are a choice of summary statistic, it's observed value,\nand the ability to simulate summary statistics for any parameter value under\nthe model. It is shown that the posterior obtained using the proposed method is\nconsistent, and its performance is explored using various examples.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 10:37:31 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 11:20:19 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Chaudhuri", "Sanjay", ""], ["Ghosh", "Subhro", ""], ["Nott", "David J.", ""], ["Pham", "Kim Cuc", ""]]}, {"id": "1810.01720", "submitter": "Tomohiro Nishiyama", "authors": "Tomohiro Nishiyama", "title": "Sum decomposition of divergence into three divergences", "comments": "9pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divergence functions play a key role as to measure the discrepancy between\ntwo points in the field of machine learning, statistics and signal processing.\nWell-known divergences are the Bregman divergences, the Jensen divergences and\nthe f-divergences. In this paper, we show that the symmetric Bregman divergence\ncan be decomposed into the sum of two types of Jensen divergences and the\nBregman divergence. Furthermore, applying this result, we show another sum\ndecomposition of divergence is possible which includes f-divergences\nexplicitly.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 13:02:19 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 08:17:30 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Nishiyama", "Tomohiro", ""]]}, {"id": "1810.01724", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep (DEEP) Mukhopadhyay and Kaijun Wang", "title": "A Nonparametric Approach to High-dimensional k-sample Comparison\n  Problems", "comments": "Biometrika (in press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional k-sample comparison is a common applied problem. We\nconstruct a class of easy-to-implement nonparametric distribution-free tests\nbased on new tools and unexplored connections with spectral graph theory. The\ntest is shown to possess various desirable properties along with a\ncharacteristic exploratory flavor that has practical consequences. The\nnumerical examples show that our method works surprisingly well under a broad\nrange of realistic situations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 13:20:28 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 18:40:50 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Subhadeep", "", "", "DEEP"], ["Mukhopadhyay", "", ""], ["Wang", "Kaijun", ""]]}, {"id": "1810.01903", "submitter": "Jiangeng Huang", "authors": "Jiangeng Huang, Robert B. Gramacy, Mickael Binois, Mirko Libraschi", "title": "On-site surrogates for large-scale calibration", "comments": "31 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a computer model calibration problem from the oil and gas\nindustry, involving the design of a honeycomb seal, we develop a new Bayesian\nmethodology to cope with limitations in the canonical apparatus stemming from\nseveral factors. We propose a new strategy of on-site design and surrogate\nmodeling for a computer simulator acting on a high-dimensional input space\nthat, although relatively speedy, is prone to numerical instabilities, missing\ndata, and nonstationary dynamics. Our aim is to strike a balance between\ndata-faithful modeling and computational tractability in a calibration\nframework--tailoring the computer model to a limited field experiment.\nSituating our on-site surrogates within the canonical calibration apparatus\nrequires updates to that framework. We describe a novel yet intuitive Bayesian\nsetup that carefully decomposes otherwise prohibitively large matrices by\nexploiting the sparse blockwise structure. Empirical illustrations demonstrate\nthat this approach performs well on toy data and our motivating honeycomb\nexample.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 18:25:08 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 15:55:05 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Huang", "Jiangeng", ""], ["Gramacy", "Robert B.", ""], ["Binois", "Mickael", ""], ["Libraschi", "Mirko", ""]]}, {"id": "1810.02030", "submitter": "Chao Gao", "authors": "Chao Gao, Jiyi Liu, Yuan Yao, Weizhi Zhu", "title": "Robust Estimation and Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation under Huber's $\\epsilon$-contamination model has become an\nimportant topic in statistics and theoretical computer science. Statistically\noptimal procedures such as Tukey's median and other estimators based on depth\nfunctions are impractical because of their computational intractability. In\nthis paper, we establish an intriguing connection between $f$-GANs and various\ndepth functions through the lens of $f$-Learning. Similar to the derivation of\n$f$-GANs, we show that these depth functions that lead to statistically optimal\nrobust estimators can all be viewed as variational lower bounds of the total\nvariation distance in the framework of $f$-Learning. This connection opens the\ndoor of computing robust estimators using tools developed for training GANs. In\nparticular, we show in both theory and experiments that some appropriate\nstructures of discriminator networks with hidden layers in GANs lead to\nstatistically optimal robust location estimators for both Gaussian distribution\nand general elliptical distributions where first moment may not exist.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 02:37:16 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 01:47:46 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 20:09:43 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Gao", "Chao", ""], ["Liu", "Jiyi", ""], ["Yao", "Yuan", ""], ["Zhu", "Weizhi", ""]]}, {"id": "1810.02037", "submitter": "Yan Zhou", "authors": "Yan Zhou, Jiadi Zhu, Tiejun Tong, Junhui Wang, Bingqing Lin and Jun\n  Zhang", "title": "A statistical normalization method and differential expression analysis\n  for RNA-seq data between different species", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: High-throughput techniques bring novel tools but also statistical\nchallenges to genomic research. Identifying genes with differential expression\nbetween different species is an effective way to discover evolutionarily\nconserved transcriptional responses. To remove systematic variation between\ndifferent species for a fair comparison, the normalization procedure serves as\na crucial pre-processing step that adjusts for the varying sample sequencing\ndepths and other confounding technical effects.\n  Results: In this paper, we propose a scale based normalization (SCBN) method\nby taking into account the available knowledge of conserved orthologous genes\nand hypothesis testing framework. Considering the different gene lengths and\nunmapped genes between different species, we formulate the problem from the\nperspective of hypothesis testing and search for the optimal scaling factor\nthat minimizes the deviation between the empirical and nominal type I errors.\nConclusions: Simulation studies show that the proposed method performs\nsignificantly better than the existing competitor in a wide range of settings.\nAn RNA-seq dataset of different species is also analyzed and it coincides with\nthe conclusion that the proposed method outperforms the existing method. For\npractical applications, we have also developed an R package named \"SCBN\" and\nthe software is available at\nhttp://www.bioconductor.org/packages/devel/bioc/html/SCBN.html.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 03:19:44 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Zhou", "Yan", ""], ["Zhu", "Jiadi", ""], ["Tong", "Tiejun", ""], ["Wang", "Junhui", ""], ["Lin", "Bingqing", ""], ["Zhang", "Jun", ""]]}, {"id": "1810.02043", "submitter": "Haoran Li", "authors": "Haoran Li and Alexander Aue and Debashis Paul", "title": "High-dimensional general linear hypothesis tests via non-linear spectral\n  shrinkage", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in testing general linear hypotheses in a high-dimensional\nmultivariate linear regression model. The framework includes many well-studied\nproblems such as two-sample tests for equality of population means, MANOVA and\nothers as special cases. A family of rotation-invariant tests is proposed that\ninvolves a flexible spectral shrinkage scheme applied to the sample error\ncovariance matrix. The asymptotic normality of the test statistic under the\nnull hypothesis is derived in the setting where dimensionality is comparable to\nsample sizes, assuming the existence of certain moments for the observations.\nThe asymptotic power of the proposed test is studied under various local\nalternatives. The power characteristics are then utilized to propose a\ndata-driven selection of the spectral shrinkage function. As an illustration of\nthe general theory, we construct a family of tests involving ridge-type\nregularization and suggest possible extensions to more complex regularizers. A\nsimulation study is carried out to examine the numerical performance of the\nproposed tests.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 03:51:29 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Li", "Haoran", ""], ["Aue", "Alexander", ""], ["Paul", "Debashis", ""]]}, {"id": "1810.02457", "submitter": "Grzegorz A Rempala", "authors": "Daniel F. Linder and Grzegorz A. Rempala", "title": "Synthetic likelihood method for reaction network inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Markov chain Monte-Carlo (MCMC) method for reverse\nengineering the topological structure of stochastic reaction networks, a\nnotoriously challenging problem that is relevant in many modern areas of\nresearch, like discovering gene regulatory networks or analyzing epidemic\nspread. The method relies on projecting the original time series trajectories\nonto information rich summary statistics and constructing the appropriate\nsynthetic likelihood function to estimate reaction rates. The resulting\nestimates are consistent in the large volume limit and are obtained without\nemploying complicated tuning strategies and expensive resampling as typically\nused by likelihood-free MCMC and approximate Bayesian methods. To illustrate\nrun time improvements that can be achieved with our approach, we present a\nsimulation study on inferring rates in a stochastic dynamical system arising\nfrom a density dependent Markov jump process. We then apply the method to two\nreal data examples: the RNA-seq data from zebrafish experiment and the\nincidence data from 1665 plague outbreak at Eyam, England.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 23:33:16 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Linder", "Daniel F.", ""], ["Rempala", "Grzegorz A.", ""]]}, {"id": "1810.02665", "submitter": "Ulrike Schneider", "authors": "Nicolai Amann, Ulrike Schneider", "title": "Uniform Asymptotics and Confidence Regions Based on the Adaptive Lasso\n  with Partially Consistent Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the adaptive Lasso estimator with componentwise tuning in the\nframework of a low-dimensional linear regression model. In our setting, at\nleast one of the components is penalized at the rate of consistent model\nselection and certain components may not be penalized at all. We perform a\ndetailed study of the consistency properties and the asymptotic distribution\nwhich includes the effects of componentwise tuning within a so-called\nmoving-parameter framework. These results enable us to explicitly provide a set\n$\\mathcal{M}$ such that every open superset acts as a confidence set with\nuniform asymptotic coverage equal to 1, whereas removing an arbitrarily small\nopen set along the boundary yields a confidence set with uniform asymptotic\ncoverage equal to 0. The shape of the set $\\mathcal{M}$ depends on the\nregressor matrix as well as the deviations within the componentwise tuning\nparameters. Our findings can be viewed as a broad generalization of P\\\"otscher\n& Schneider (2009, 2010) who considered distributional properties and\nconfidence intervals based on components of the adaptive Lasso estimator for\nthe case of orthogonal regressors.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 13:18:37 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 18:00:22 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Amann", "Nicolai", ""], ["Schneider", "Ulrike", ""]]}, {"id": "1810.02761", "submitter": "Zach Branson", "authors": "Zach Branson and Fabrizia Mealli", "title": "The Local Randomization Framework for Regression Discontinuity Designs:\n  A Review and Some Extensions", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression discontinuity designs (RDDs) are a common quasi-experiment in\neconomics and statistics. The most popular methodologies for analyzing RDDs\nutilize continuity-based assumptions and local polynomial regression, but\nrecent works have developed alternative assumptions based on local\nrandomization. The local randomization framework avoids modeling assumptions by\ninstead placing assumptions on the assignment mechanism near the cutoff.\nHowever, most works have focused on completely randomized assignment\nmechanisms, which posit that propensity scores are equal for all units near the\ncutoff. In our review of the local randomization framework, we extend the\nframework to allow for any assignment mechanism, such that propensity scores\nmay differ. We outline randomization tests that can be used to select a window\naround the cutoff where a particular assignment mechanism is most plausible, as\nwell as methodologies for estimating causal effects after a window and\nassignment mechanism are chosen. We apply our methodology to a fuzzy RDD\nassessing the effects of financial aid on college dropout rates in Italy. We\nfind that positing different assignment mechanisms within a single RDD can\nprovide more nuanced sensitivity analyses as well as more precise inferences\nfor causal effects.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 15:48:52 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 00:08:34 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 15:42:11 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Branson", "Zach", ""], ["Mealli", "Fabrizia", ""]]}, {"id": "1810.02782", "submitter": "Markus Matilainen", "authors": "Markus Matilainen, Christophe Croux, Klaus Nordhausen, Hannu Oja", "title": "Sliced Average Variance Estimation for Multivariate Time Series", "comments": null, "journal-ref": "Statistics, 2019, Vol. 53, 630-655", "doi": "10.1080/02331888.2019.1605515", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised dimension reduction for time series is challenging as there may be\ntemporal dependence between the response $y$ and the predictors $\\boldsymbol\nx$. Recently a time series version of sliced inverse regression, TSIR, was\nsuggested, which applies approximate joint diagonalization of several\nsupervised lagged covariance matrices to consider the temporal nature of the\ndata. In this paper we develop this concept further and propose a time series\nversion of sliced average variance estimation, TSAVE. As both TSIR and TSAVE\nhave their own advantages and disadvantages, we consider furthermore a hybrid\nversion of TSIR and TSAVE. Based on examples and simulations we demonstrate and\nevaluate the differences between the three methods and show also that they are\nsuperior to apply their iid counterparts to when also using lagged values of\nthe explaining variables as predictors.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 16:32:12 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Matilainen", "Markus", ""], ["Croux", "Christophe", ""], ["Nordhausen", "Klaus", ""], ["Oja", "Hannu", ""]]}, {"id": "1810.02954", "submitter": "Feng Ruan", "authors": "Andrea Montanari, Feng Ruan and Jun Yan", "title": "Adapting to Unknown Noise Distribution in Matrix Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating an unknown matrix $\\boldsymbol{X}\\in\n{\\mathbb R}^{m\\times n}$, from observations $\\boldsymbol{Y} =\n\\boldsymbol{X}+\\boldsymbol{W}$ where $\\boldsymbol{W}$ is a noise matrix with\nindependent and identically distributed entries, as to minimize estimation\nerror measured in operator norm. Assuming that the underlying signal\n$\\boldsymbol{X}$ is low-rank and incoherent with respect to the canonical\nbasis, we prove that minimax risk is equivalent to\n$(\\sqrt{m}\\vee\\sqrt{n})/\\sqrt{I_W}$ in the high-dimensional limit\n$m,n\\to\\infty$, where $I_W$ is the Fisher information of the noise. Crucially,\nwe develop an efficient procedure that achieves this risk, adaptively over the\nnoise distribution (under certain regularity assumptions).\n  Letting $\\boldsymbol{X} =\n\\boldsymbol{U}{\\boldsymbol{\\Sigma}}\\boldsymbol{V}^{{\\sf T}}$ --where\n$\\boldsymbol{U}\\in {\\mathbb R}^{m\\times r}$, $\\boldsymbol{V}\\in{\\mathbb\nR}^{n\\times r}$ are orthogonal, and $r$ is kept fixed as $m,n\\to\\infty$-- we\nuse our method to estimate $\\boldsymbol{U}$, $\\boldsymbol{V}$. Standard\nspectral methods provide non-trivial estimates of the factors\n$\\boldsymbol{U},\\boldsymbol{V}$ (weak recovery) only if the singular values of\n$\\boldsymbol{X}$ are larger than $(mn)^{1/4}{\\rm Var}(W_{11})^{1/2}$. We prove\nthat the new approach achieves weak recovery down to the the\ninformation-theoretically optimal threshold $(mn)^{1/4}I_W^{1/2}$.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 07:59:44 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 09:20:39 GMT"}, {"version": "v3", "created": "Mon, 5 Nov 2018 01:39:35 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Montanari", "Andrea", ""], ["Ruan", "Feng", ""], ["Yan", "Jun", ""]]}, {"id": "1810.02956", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami, Hajime Seya, Daniel A. Griffith", "title": "Low rank spatial econometric models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a re-structuring of spatial econometric models in a\nlinear mixed model framework. To that end, it proposes low rank spatial\neconometric models that are robust to the existence of noise (i.e., measurement\nerror), and can enjoy fast parameter estimation and inference by Type II\nrestricted likelihood maximization (empirical Bayes) techniques. The small\nsample properties of the proposed low rank spatial econometric models are\nexamined using Monte Carlo simulation experiments, the results of these\nexperiments confirm that direct effects and indirect effects a la LeSage and\nPace (2009) can be estimated with a high degree of accuracy. Also, when data\nare noisy, estimators for coefficients in the proposed models have lower root\nmean squared errors compared to conventional specifications, despite them being\nlow rank approximations. The proposed approach is implemented in an R package\n\"spmoran\".\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 08:09:31 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Murakami", "Daisuke", ""], ["Seya", "Hajime", ""], ["Griffith", "Daniel A.", ""]]}, {"id": "1810.02962", "submitter": "Fr\\'ed\\'eric Bertrand", "authors": "Fr\\'ed\\'eric Bertrand and Philippe Bastien and Myriam Maumy-Bertrand", "title": "Cross validating extensions of kernel, sparse or regular partial least\n  squares regression models to censored data", "comments": "42 pages, SI 16 pages, 127 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When cross-validating standard or extended Cox models, the commonly used\ncriterion is the cross-validated partial loglikelihood using a naive or a van\nHouwelingen scheme -to make efficient use of the death times of the left out\ndata in relation to the death times of all the data-. Quite astonishingly, we\nwill show, using a strong simulation study involving three different data\nsimulation algorithms, that these two cross-validation methods fail with the\nextensions, either straightforward or more involved ones, of partial least\nsquares regression to the Cox model. This is quite an interesting result for at\nleast two reasons. Firstly, several nice features of PLS based models,\nincluding regularization, interpretability of the components, missing data\nsupport, data visualization thanks to biplots of individuals and variables -and\neven parsimony for SPLS based models-, account for a common use of these\nextensions by statisticians who usually select their hyperparameters using\ncross-validation. Secondly, they are almost always featured in benchmarking\nstudies to assess the performance of a new estimation technique used in a high\ndimensional context and often show poor statistical properties. We carried out\na vast simulation study to evaluate more than a dozen of potential\ncross-validation criteria, either AUC or prediction error based. Several of\nthem lead to the selection of a reasonable number of components. Using these\nnewly found cross-validation criteria to fit extensions of partial least\nsquares regression to the Cox model, we performed a benchmark reanalysis that\nshowed enhanced performances of these techniques. In addition, we defined a new\nrobust measure based on the Schmid score and the R coefficient of determination\nfor least absolute deviation: the integrated R Schmid Score weighted.\nSimulations were carried out using the R-package plsRcox.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 08:45:13 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Bertrand", "Fr\u00e9d\u00e9ric", ""], ["Bastien", "Philippe", ""], ["Maumy-Bertrand", "Myriam", ""]]}, {"id": "1810.02991", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh and Ayanendranath Basu", "title": "Robust and Efficient Estimation in the Parametric Cox Regression Model\n  under Random Censoring", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cox proportional hazard regression model is a popular tool to analyze the\nrelationship between a censored lifetime variable with other relevant factors.\nThe semi-parametric Cox model is widely used to study different types of data\narising from applied disciplines like medical science, biology, reliability\nstudies and many more. A fully parametric version of the Cox regression model,\nif properly specified, can yield more efficient parameter estimates leading to\nbetter insight generation. However, the existing maximum likelihood approach of\ngenerating inference under the fully parametric Cox regression model is highly\nnon-robust against data-contamination which restricts its practical usage. In\nthis paper we develop a robust estimation procedure for the parametric Cox\nregression model based on the minimum density power divergence approach. The\nproposed minimum density power divergence estimator is seen to produce highly\nrobust estimates under data contamination with only a slight loss in efficiency\nunder pure data. Further, they are always seen to generate more precise\ninference than the likelihood based estimates under the semi-parametric Cox\nmodels or their existing robust versions. We also sketch the derivation of the\nasymptotic properties of the proposed estimator using the martingale approach\nand justify their robustness theoretically through the influence function\nanalysis. The practical applicability and usefulness of the proposal are\nillustrated through simulations and a real data example.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 12:08:23 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1810.03030", "submitter": "Linh Tran", "authors": "Linh Tran, Maya Petersen, Joshua Schwab, Mark J van der Laan", "title": "Robust variance estimation and inference for causal effect estimation", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a longitudinal data structure consisting of baseline covariates,\ntime-varying treatment variables, intermediate time-dependent covariates, and a\npossibly time dependent outcome. Previous studies have shown that estimating\nthe variance of asymptotically linear estimators using empirical influence\nfunctions in this setting result in anti-conservative estimates with increasing\nmagnitudes of positivity violations, leading to poor coverage and uncontrolled\nType I errors. In this paper, we present two alternative approaches of\nestimating the variance of these estimators: (i) a robust approach which\ndirectly targets the variance of the influence function as a counterfactual\nmean outcome, and (ii) a non-parametric bootstrap based approach that is\ntheoretically valid and lowers the computational cost, thereby increasing the\nfeasibility in non-parametric settings using complex machine learning\nalgorithms. The performance of these approaches are compared to that of the\nempirical influence function in simulations across different levels of\npositivity violations and treatment effect sizes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 17:40:30 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Tran", "Linh", ""], ["Petersen", "Maya", ""], ["Schwab", "Joshua", ""], ["van der Laan", "Mark J", ""]]}, {"id": "1810.03136", "submitter": "Tom Reynkens", "authors": "Sander Devriendt, Katrien Antonio, Tom Reynkens, Roel Verbelen", "title": "Sparse Regression with Multi-type Regularized Feature Modeling", "comments": null, "journal-ref": "Insurance: Mathematics and Economics (2020)", "doi": "10.1016/j.insmatheco.2020.11.010", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the statistical and machine learning literature, regularization\ntechniques are often used to construct sparse (predictive) models. Most\nregularization strategies only work for data where all predictors are treated\nidentically, such as Lasso regression for (continuous) predictors treated as\nlinear effects. However, many predictive problems involve different types of\npredictors and require a tailored regularization term. We propose a multi-type\nLasso penalty that acts on the objective function as a sum of subpenalties, one\nfor each type of predictor. As such, we allow for predictor selection and level\nfusion within a predictor in a data-driven way, simultaneous with the parameter\nestimation process. We develop a new estimation strategy for convex predictive\nmodels with this multi-type penalty. Using the theory of proximal operators,\nour estimation procedure is computationally efficient, partitioning the overall\noptimization problem into easier to solve subproblems, specific for each\npredictor type and its associated penalty. Earlier research applies\napproximations to non-differentiable penalties to solve the optimization\nproblem. The proposed SMuRF algorithm removes the need for approximations and\nachieves a higher accuracy and computational efficiency. This is demonstrated\nwith an extensive simulation study and the analysis of a case-study on\ninsurance pricing analytics.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 12:42:23 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 17:10:06 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Devriendt", "Sander", ""], ["Antonio", "Katrien", ""], ["Reynkens", "Tom", ""], ["Verbelen", "Roel", ""]]}, {"id": "1810.03192", "submitter": "Emma Jingfei Zhang", "authors": "Jingfei Zhang, Will Wei Sun and Lexin Li", "title": "Generalized Connectivity Matrix Response Regression with Applications in\n  Brain Connectivity Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple-subject network data are fast emerging in recent years, where a\nseparate connectivity matrix is measured over a common set of nodes for each\nindividual subject, along with subject covariates information. In this article,\nwe propose a new generalized matrix response regression model, where the\nobserved networks are treated as matrix-valued responses and the subject\ncovariates as predictors. The new model characterizes the population-level\nconnectivity pattern through a low-rank intercept matrix, and the effect of\nsubject covariates through a sparse slope tensor. We develop an efficient\nalternating gradient descent algorithm for parameter estimation, and establish\nthe non-asymptotic error bound for the actual estimator from the algorithm,\nwhich quantifies the interplay between the computational and statistical\nerrors. We further show the strong consistency for graph community recovery, as\nwell as the edge selection consistency. We demonstrate the efficacy of our\nmethod through simulations and two brain connectivity studies.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 18:07:47 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 22:33:46 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 22:15:17 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Jingfei", ""], ["Sun", "Will Wei", ""], ["Li", "Lexin", ""]]}, {"id": "1810.03260", "submitter": "Aaron Fisher", "authors": "Aaron Fisher, Edward H. Kennedy", "title": "Visually Communicating and Teaching Intuition for Influence Functions", "comments": "This manuscript version includes 2 additional supplemental figures to\n  further aid intuition. In total: 4 figures, 36 pages (double spaced)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimators based on influence functions (IFs) have been shown to be effective\nin many settings, especially when combined with machine learning techniques. By\nfocusing on estimating a specific target of interest (e.g., the average effect\nof a treatment), rather than on estimating the full underlying data generating\ndistribution, IF-based estimators are often able to achieve asymptotically\noptimal mean-squared error. Still, many researchers find IF-based estimators to\nbe opaque or overly technical, which makes their use less prevalent and their\nbenefits less available. To help foster understanding and trust in IF-based\nestimators, we present tangible, visual illustrations of when and how IF-based\nestimators can outperform standard ``plug-in'' estimators. The figures we show\nare based on connections between IFs, gradients, linear approximations, and\nNewton-Raphson.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 03:46:42 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 00:08:47 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 20:48:00 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Fisher", "Aaron", ""], ["Kennedy", "Edward H.", ""]]}, {"id": "1810.03269", "submitter": "Ted Westling", "authors": "Ted Westling, Peter Gilbert, and Marco Carone", "title": "Causal isotonic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, potential confounders may distort the causal\nrelationship between an exposure and an outcome. However, under some\nconditions, a causal dose-response curve can be recovered using the\nG-computation formula. Most classical methods for estimating such curves when\nthe exposure is continuous rely on restrictive parametric assumptions, which\ncarry significant risk of model misspecification. Nonparametric estimation in\nthis context is challenging because in a nonparametric model these curves\ncannot be estimated at regular rates. Many available nonparametric estimators\nare sensitive to the selection of certain tuning parameters, and performing\nvalid inference with such estimators can be difficult. In this work, we propose\na nonparametric estimator of a causal dose-response curve known to be monotone.\nWe show that our proposed estimation procedure generalizes the classical\nleast-squares isotonic regression estimator of a monotone regression function.\nSpecifically, it does not involve tuning parameters, and is invariant to\nstrictly monotone transformations of the exposure variable. We describe\ntheoretical properties of our proposed estimator, including its irregular limit\ndistribution and the potential for doubly-robust inference. Furthermore, we\nillustrate its performance via numerical studies, and use it to assess the\nrelationship between BMI and immune response in HIV vaccine trials.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 04:51:00 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 20:10:35 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Westling", "Ted", ""], ["Gilbert", "Peter", ""], ["Carone", "Marco", ""]]}, {"id": "1810.03296", "submitter": "Yi Yu", "authors": "Tony Sit, Zhiliang Ying and Yi Yu", "title": "Event History Analysis of Dynamic Communication Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis on networks has received growing attention due to demand\nfrom various emerging applications. In dynamic networks, one of the key\ninterests is to model the event history of time-stamped interactions amongst\nnodes. We propose to model dynamic directed communication networks via\nmultivariate counting processes. A pseudo partial likelihood approach is\nexploited to capture the network dependence structure. Asymptotic results of\nthe resulting estimation are established. Numerical results are performed to\ndemonstrate effectiveness of our proposal.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 07:51:46 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Sit", "Tony", ""], ["Ying", "Zhiliang", ""], ["Yu", "Yi", ""]]}, {"id": "1810.03353", "submitter": "BaoLuo Sun", "authors": "BaoLuo Sun and Wang Miao", "title": "On Semiparametric Instrumental Variable Estimation of Average Treatment\n  Effects through Data Fusion", "comments": "34 pages", "journal-ref": null, "doi": "10.5705/ss.202020.0081", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose one is interested in estimating causal effects in the presence of\npotentially unmeasured confounding with the aid of a valid instrumental\nvariable. This paper investigates the problem of making inferences about the\naverage treatment effect when data are fused from two separate sources, one of\nwhich contains information on the treatment and the other contains information\non the outcome, while values for the instrument and a vector of baseline\ncovariates are recorded in both. We provide a general set of sufficient\nconditions under which the average treatment effect is nonparametrically\nidentified from the observed data law induced by data fusion, even when the\ndata are from two heterogeneous populations, and derive the efficiency bound\nfor estimating this causal parameter. For inference, we develop both parametric\nand semiparametric methods, including a multiply robust and locally efficient\nestimator that is consistent even under partial misspecification of the\nobserved data model. We illustrate the methods through simulations and an\napplication on public housing projects.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 10:00:08 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 13:58:42 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 06:38:33 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Sun", "BaoLuo", ""], ["Miao", "Wang", ""]]}, {"id": "1810.03440", "submitter": "Filip Tronarp", "authors": "Filip Tronarp, Hans Kersting, Simo S\\\"arkk\\\"a, Philipp Hennig", "title": "Probabilistic Solutions To Ordinary Differential Equations As Non-Linear\n  Bayesian Filtering: A New Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate probabilistic numerical approximations to solutions of ordinary\ndifferential equations (ODEs) as problems in Gaussian process (GP) regression\nwith non-linear measurement functions. This is achieved by defining the\nmeasurement sequence to consist of the observations of the difference between\nthe derivative of the GP and the vector field evaluated at the GP---which are\nall identically zero at the solution of the ODE. When the GP has a state-space\nrepresentation, the problem can be reduced to a non-linear Bayesian filtering\nproblem and all widely-used approximations to the Bayesian filtering and\nsmoothing problems become applicable. Furthermore, all previous GP-based ODE\nsolvers that are formulated in terms of generating synthetic measurements of\nthe gradient field come out as specific approximations. Based on the non-linear\nBayesian filtering problem posed in this paper, we develop novel Gaussian\nsolvers for which we establish favourable stability properties. Additionally,\nnon-Gaussian approximations to the filtering problem are derived by the\nparticle filter approach. The resulting solvers are compared with other\nprobabilistic solvers in illustrative experiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 13:36:24 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 16:30:22 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 12:20:07 GMT"}, {"version": "v4", "created": "Wed, 24 Apr 2019 09:13:11 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Tronarp", "Filip", ""], ["Kersting", "Hans", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Hennig", "Philipp", ""]]}, {"id": "1810.03496", "submitter": "Hojin Yang", "authors": "Hojin Yang, Veerabhadran Baladandayuthapani, Arvind U.K. Rao, and\n  Jeffrey S. Morris", "title": "Regression Analyses of Distributions using Quantile Functional\n  Regression", "comments": "83 pages, 32 figures. arXiv admin note: substantial text overlap with\n  arXiv:1711.00031", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiomics involves the study of tumor images to identify quantitative markers\nexplaining cancer heterogeneity. The predominant approach is to extract\nhundreds to thousands of image features, including histogram features comprised\nof summaries of the marginal distribution of pixel intensities, which leads to\nmultiple testing problems and can miss out on insights not contained in the\nselected features. In this paper, we present methods to model the entire\nmarginal distribution of pixel intensities via the quantile function as\nfunctional data, regressed on a set of demographic, clinical, and genetic\npredictors. We call this approach quantile functional regression, regressing\nsubject-specific marginal distributions across repeated measurements on a set\nof covariates, allowing us to assess which covariates are associated with the\ndistribution in a global sense, as well as to identify distributional features\ncharacterizing these differences, including mean, variance, skewness, and\nvarious upper and lower quantiles. To account for smoothness in the quantile\nfunctions, we introduce custom basis functions we call quantlets that are\nsparse, regularized, near-lossless, and empirically defined, adapting to the\nfeatures of a given data set. We fit this model using a Bayesian framework that\nuses nonlinear shrinkage of quantlet coefficients to regularize the functional\nregression coefficients and provides fully Bayesian inference after fitting a\nMarkov chain Monte Carlo. We demonstrate the benefit of the basis space\nmodeling through simulation studies, and apply the method to Magnetic resonance\nimaging (MRI) based radiomic dataset from Glioblastoma Multiforme to relate\nimaging-based quantile functions to demographic, clinical, and genetic\npredictors, finding specific differences in tumor pixel intensity distribution\nbetween males and females and between tumors with and without DDIT3 mutations.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 20:58:41 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Yang", "Hojin", ""], ["Baladandayuthapani", "Veerabhadran", ""], ["Rao", "Arvind U. K.", ""], ["Morris", "Jeffrey S.", ""]]}, {"id": "1810.03591", "submitter": "Samuel Tickle", "authors": "S. O. Tickle, I. A. Eckley, P. Fearnhead, K. Haynes", "title": "Parallelisation of a Common Changepoint Detection Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, various means of efficiently detecting changepoints in the\nunivariate setting have been proposed, with one popular approach involving\nminimising a penalised cost function using dynamic programming. In some\nsituations, these algorithms can have an expected computational cost that is\nlinear in the number of data points; however, the worst case cost remains\nquadratic. We introduce two means of improving the computational performance of\nthese methods, both based on parallelising the dynamic programming approach. We\nestablish that parallelisation can give substantial computational improvements:\nin some situations the computational cost decreases roughly quadratically in\nthe number of cores used. These parallel implementations are no longer\nguaranteed to find the true minimum of the penalised cost; however, we show\nthat they retain the same asymptotic guarantees in terms of their accuracy in\nestimating the number and location of the changes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 17:33:53 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Tickle", "S. O.", ""], ["Eckley", "I. A.", ""], ["Fearnhead", "P.", ""], ["Haynes", "K.", ""]]}, {"id": "1810.03671", "submitter": "Abhijoy Saha", "authors": "Abhijoy Saha, Sebastian Kurtek", "title": "Geometric Sensitivity Measures for Bayesian Nonparametric Density\n  Estimation Models", "comments": "Accepted for publication in Sankhya A", "journal-ref": null, "doi": "10.1007/s13171-018-0145-7", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a geometric framework to assess global sensitivity in Bayesian\nnonparametric models for density estimation. We study sensitivity of\nnonparametric Bayesian models for density estimation, based on Dirichlet-type\npriors, to perturbations of either the precision parameter or the base\nprobability measure. To quantify the different effects of the perturbations of\nthe parameters and hyperparameters in these models on the posterior, we define\nthree geometrically-motivated global sensitivity measures based on geodesic\npaths and distances computed under the nonparametric Fisher-Rao Riemannian\nmetric on the space of densities, applied to posterior samples of densities:\n(1) the Fisher-Rao distance between density averages of posterior samples, (2)\nthe log-ratio of Karcher variances of posterior samples, and (3) the norm of\nthe difference of scaled cumulative eigenvalues of empirical covariance\noperators obtained from posterior samples. We validate our approach using\nmultiple simulation studies, and consider the problem of sensitivity analysis\nfor Bayesian density estimation models in the context of three real datasets\nthat have previously been studied.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 19:31:54 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Saha", "Abhijoy", ""], ["Kurtek", "Sebastian", ""]]}, {"id": "1810.03751", "submitter": "Ick Hoon Jin", "authors": "Haiyan Liu, Ick Hoon Jin, Zhiyong Zhang, Ying Yuan", "title": "Social Network Mediation Analysis: a Latent Space Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks contain data on both actor attributes and social connections\namong them. Such connections reflect the dependence among social actors, which\nis important for individual's mental health and social development. To\ninvestigate the potential mediation role of a social network, we propose a\nmediation model with a social network as a mediator. In the model, dependence\namong actors is accounted by a few mutually orthogonal latent dimensions. The\nscores on these dimensions are directly involved in the intervention process\nbetween an independent variable and a dependent variable. Because all the\nlatent dimensions are equivalent in terms of their relationship to social\nnetworks, it is hardly to name them. The intervening effect through an\nindividual dimension is thus of little practical interest. Therefore, we would\nrather focus on the mediation effect of a network. Although the scores are not\nunique, we rigorously articulate that the proposed network mediation effect is\nstill well-defined. To estimate the model, we adopt a Bayesian estimation\nmethod. This modeling framework and the Bayesian estimation method is evaluated\nthrough a simulation study under representative conditions. Its usefulness is\ndemonstrated through an empirical application to a college friendship network.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 23:57:52 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 13:18:48 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Liu", "Haiyan", ""], ["Jin", "Ick Hoon", ""], ["Zhang", "Zhiyong", ""], ["Yuan", "Ying", ""]]}, {"id": "1810.03814", "submitter": "Yueyong Shi", "authors": "Jian Huang, Yuling Jiao, Xiliang Lu, Yueyong Shi, Qinglong Yang", "title": "SNAP: A semismooth Newton algorithm for pathwise optimization with\n  optimal local convergence rate and oracle properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semismooth Newton algorithm for pathwise optimization (SNAP) for\nthe LASSO and Enet in sparse, high-dimensional linear regression. SNAP is\nderived from a suitable formulation of the KKT conditions based on Newton\nderivatives. It solves the semismooth KKT equations efficiently by actively and\ncontinuously seeking the support of the regression coefficients along the\nsolution path with warm start. At each knot in the path, SNAP converges locally\nsuperlinearly for the Enet criterion and achieves an optimal local convergence\nrate for the LASSO criterion, i.e., SNAP converges in one step at the cost of\ntwo matrix-vector multiplication per iteration. Under certain regularity\nconditions on the design matrix and the minimum magnitude of the nonzero\nelements of the target regression coefficients, we show that SNAP hits a\nsolution with the same signs as the regression coefficients and achieves a\nsharp estimation error bound in finite steps with high probability. The\ncomputational complexity of SNAP is shown to be the same as that of LARS and\ncoordinate descent algorithms per iteration. Simulation studies and real data\nanalysis support our theoretical results and demonstrate that SNAP is faster\nand accurate than LARS and coordinate descent algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 04:44:42 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Lu", "Xiliang", ""], ["Shi", "Yueyong", ""], ["Yang", "Qinglong", ""]]}, {"id": "1810.03866", "submitter": "Quentin Clairon", "authors": "Quentin Clairon", "title": "A regularization method for the parameter estimation problem in ordinary\n  differential equations via discrete optimal control theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parameter estimation method in Ordinary Differential Equation\n(ODE) models. Due to complex relationships between parameters and states the\nuse of standard techniques such as nonlinear least squares can lead to the\npresence of poorly identifiable parameters. Moreover, ODEs are generally\napproximations of the true process and the influence of misspecification on\ninference is often neglected. Methods based on control theory have emerged to\nregularize the ill posed problem of parameter estimation in this context.\nHowever, they are computationally intensive and rely on a nonparametric state\nestimator known to be biased in the sparse sample case. In this paper, we\nconstruct criteria based on discrete control theory which are computationally\nefficient and bypass the presmoothing step of signal estimation while retaining\nthe benefits of control theory for estimation. We describe how the estimation\nproblem can be turned into a control one and present the numerical methods used\nto solve it. We show convergence of our estimator in the parametric and\nwell-specified case. For small sample sizes, numerical experiments with models\ncontaining poorly identifiable parameters and with various sources of model\nmisspecification demonstrate the acurracy of our method. We finally test our\napproach on a real data example.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 09:03:24 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 07:40:43 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Clairon", "Quentin", ""]]}, {"id": "1810.03893", "submitter": "Rainer Schwabe", "authors": "Ulrike Gra{\\ss}hoff, Heinz Holling, Rainer Schwabe", "title": "D-Optimal Design for the Rasch Counts Model with Multiple Binary\n  Predictors", "comments": null, "journal-ref": "British Journal of Mathematical and Statistical Psychology Volume\n  73, 2020, Issue 3, p. 541-555", "doi": "10.1111/bmsp.12204", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive optimal designs for the Rasch Poisson counts model\nand the Rasch Poisson-Gamma counts model incorporating several binary\npredictors for the difficulty parameter. To efficiently estimate the regression\ncoefficients of the predictors, locally D-optimal designs are developed. After\nan introduction to the Rasch Poisson counts model and the Rasch Poisson-Gamma\ncounts model we will specify these models as a particular generalized linear\nmixed model. Based on this embedding optimal designs for both models including\nseveral binary explanatory variables will be presented. Therefore, we will\nderive conditions on the effect sizes of certain designs to be locally\nD-optimal. Finally, it is pointed out that the results derived for the Rasch\nPoisson models can be applied for more general Poisson regression models which\nshould receive more attention in future psychological research.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 10:22:35 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Gra\u00dfhoff", "Ulrike", ""], ["Holling", "Heinz", ""], ["Schwabe", "Rainer", ""]]}, {"id": "1810.04115", "submitter": "Nick Whiteley Prof.", "authors": "Nick Whiteley, Matt W. Jones and Aleks P.F. Domanski", "title": "The Viterbi process, decay-convexity and parallelized maximum\n  a-posteriori estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Viterbi process is the limiting maximum a-posteriori estimate of the\nunobserved path in a hidden Markov model as the length of the time horizon\ngrows. The existence of such a process suggests that approximate estimation\nusing optimization algorithms which process data segments in parallel may be\naccurate. For models on state-space $\\mathbb{R}^{d}$ satisfying a new\n\"decay-convexity\" condition, we develop an approach to existence of the Viterbi\nprocess via fixed points of ordinary differential equations in a certain\ninfinite dimensional Hilbert space. Bounds on the distance to the Viterbi\nprocess show that approximate estimation via parallelization can indeed be\naccurate and scaleable to high-dimensional problems because the rate of\nconvergence to the Viterbi process does not necessarily depend on $d$. The\nresults are applied to a factor model with stochastic volatility and a model of\nneural population activity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 16:17:21 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 12:34:27 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 09:35:41 GMT"}, {"version": "v4", "created": "Wed, 18 Dec 2019 18:24:59 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Whiteley", "Nick", ""], ["Jones", "Matt W.", ""], ["Domanski", "Aleks P. F.", ""]]}, {"id": "1810.04140", "submitter": "Katherine Wilson", "authors": "Katie Wilson, Jon Wakefield", "title": "Child Mortality Estimation Incorporating Summary Birth History Data", "comments": "50 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United Nations' Sustainable Development Goal 3.2 aims to reduce under-5\nchild mortality to 25 deaths per 1,000 live births by 2030. Child mortality\ntends to be concentrated in developing regions where much of the information\nneeded to assess achievement of this goal comes from surveys and censuses. In\nboth, women are asked about their birth histories, but with varying degrees of\ndetail. Full birth history (FBH) data contain the reported dates of births and\ndeaths of every surveyed mother's children. In contrast, summary birth history\n(SBH) data contain only the total number of children born and total number of\nchildren who died for each mother. Specialized methods are needed to\naccommodate this type of data into analyses of child mortality trends. We\ndevelop a data augmentation scheme within a Bayesian framework where for SBH\ndata, birth and death dates are introduced as auxiliary variables. Since we\nspecify a full probability model for the data, many of the well-known biases\nthat exist in this data can be accommodated, along with space-time smoothing on\nthe underlying mortality rates. We illustrate our approach in a simulation,\nshowing that uncertainty is reduced when incorporating SBH data over simply\nanalyzing all available FBH data. We also apply our approach to data in the\nCentral region of Malawi. We compare with the well-known Brass method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 17:19:05 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Wilson", "Katie", ""], ["Wakefield", "Jon", ""]]}, {"id": "1810.04200", "submitter": "Marcin Jurek", "authors": "Marcin Jurek and Matthias Katzfuss", "title": "Multi-resolution filters for massive spatio-temporal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal data sets are rapidly growing in size. For example,\nenvironmental variables are measured with ever-higher resolution by increasing\nnumbers of automated sensors mounted on satellites and aircraft. Using such\ndata, which are typically noisy and incomplete, the goal is to obtain complete\nmaps of the spatio-temporal process, together with proper uncertainty\nquantification. We focus here on real-time filtering inference in linear\nGaussian state-space models. At each time point, the state is a spatial field\nevaluated on a very large spatial grid, making exact inference using the Kalman\nfilter computationally infeasible. Instead, we propose a multi-resolution\nfilter (MRF), a highly scalable and fully probabilistic filtering method that\nresolves spatial features at all scales. We prove that the MRF matrices exhibit\na particular block-sparse multi-resolution structure that is preserved under\nfiltering operations through time. We also discuss inference on time-varying\nparameters using an approximate Rao-Blackwellized particle filter, in which the\nintegrated likelihood is computed using the MRF. We compare the MRF to existing\napproaches in a simulation study and a real satellite-data application.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 18:24:49 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 16:25:27 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Jurek", "Marcin", ""], ["Katzfuss", "Matthias", ""]]}, {"id": "1810.04286", "submitter": "Tamara Fernandez", "authors": "Tamara Fern\\'andez and Arthur Gretton", "title": "A maximum-mean-discrepancy goodness-of-fit test for censored data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a kernel-based goodness-of-fit test for censored data, where\nobservations may be missing in random time intervals: a common occurrence in\nclinical trials and industrial life-testing. The test statistic is\nstraightforward to compute, as is the test threshold, and we establish\nconsistency under the null. Unlike earlier approaches such as the Log-rank\ntest, we make no assumptions as to how the data distribution might differ from\nthe null, and our test has power against a very rich class of alternatives. In\nexperiments, our test outperforms competing approaches for periodic and Weibull\nhazard functions (where risks are time dependent), and does not show the\nfailure modes of tests that rely on user-defined features. Moreover, in cases\nwhere classical tests are provably most powerful, our test performs almost as\nwell, while being more general.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 22:36:09 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Fern\u00e1ndez", "Tamara", ""], ["Gretton", "Arthur", ""]]}, {"id": "1810.04443", "submitter": "Samuel Orso", "authors": "St\\'ephane Guerrier, Mucyo Karemera, Samuel Orso, Maria-Pia\n  Victoria-Feser", "title": "On the Properties of Simulation-based Estimators in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Considering the increasing size of available data, the need for statistical\nmethods that control the finite sample bias is growing. This is mainly due to\nthe frequent settings where the number of variables is large and allowed to\nincrease with the sample size bringing standard inferential procedures to incur\nsignificant loss in terms of performance. Moreover, the complexity of\nstatistical models is also increasing thereby entailing important computational\nchallenges in constructing new estimators or in implementing classical ones. A\ntrade-off between numerical complexity and statistical properties is often\naccepted. However, numerically efficient estimators that are altogether\nunbiased, consistent and asymptotically normal in high dimensional problems\nwould generally be ideal. In this paper, we set a general framework from which\nsuch estimators can easily be derived for wide classes of models. This\nframework is based on the concepts that underlie simulation-based estimation\nmethods such as indirect inference. The approach allows various extensions\ncompared to previous results as it is adapted to possibly inconsistent\nestimators and is applicable to discrete models and/or models with a large\nnumber of parameters. We consider an algorithm, namely the Iterative Bootstrap\n(IB), to efficiently compute simulation-based estimators by showing its\nconvergence properties. Within this framework we also prove the properties of\nsimulation-based estimators, more specifically the unbiasedness, consistency\nand asymptotic normality when the number of parameters is allowed to increase\nwith the sample size. Therefore, an important implication of the proposed\napproach is that it allows to obtain unbiased estimators in finite samples.\nFinally, we study this approach when applied to three common models, namely\nlogistic regression, negative binomial regression and lasso regression.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 10:12:54 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 07:37:42 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Karemera", "Mucyo", ""], ["Orso", "Samuel", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "1810.04448", "submitter": "Chuan-Long Xie", "authors": "Heng Peng, Chuanlong Xie, Jingxin Zhao", "title": "Fast Inference Procedures for Semivarying Coefficient Models via Local\n  Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semivarying coefficient models are widely used in the application of\nfinance, economics, medical science and many other areas. The functional\ncoefficients are commonly estimated by local smoothing methods, e.g. local\nlinear estimator. This implies that one should implement the estimation\nprocedure for hundreds of times to obtain an estimate of one function. So the\ncomputation cost is very severe. In this paper, we give an insight to the\ntrade-off between statistical efficiency and computation simplicity, and\nproposes a fast inference procedure for semivarying coefficient model. In our\nmethod, the coefficient functions are approximated by piecewise constants,\nwhich is a simple and rough approximation. This makes our estimators easy to\nimplement and avoid repeat estimation. In this work, we shall show that though\nthese estimators are not asymptotically optimal, they are efficient enough for\nbuilding further inference procedure. Furthermore, three tests are brought out\nto check whether certain coefficient is constant. Our results clearly show that\nwhen the room for improving the asymptotic efficiency is limited, a proper\ntrade-off between statistical efficiency and computation simplicity can be\ntaken into consideration to improve the performance of the inference procedure.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 10:33:51 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 15:25:54 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Peng", "Heng", ""], ["Xie", "Chuanlong", ""], ["Zhao", "Jingxin", ""]]}, {"id": "1810.04567", "submitter": "Edward Frees", "authors": "Edward W. Frees, Catalina Bolanc\\'e, Montserrat Guillen, Emiliano\n  Valdez", "title": "Copula Modeling of Multivariate Longitudinal Data with Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint multivariate longitudinal and time-to-event data are gaining increasing\nattention in the biomedical sciences where subjects are followed over time to\nmonitor the progress of a disease or medical condition. In the insurance\ncontext, claims outcomes may be related to a policyholder's dropout or decision\nto lapse a policy. This paper introduces a generalized method of moments\ntechnique to estimate dependence parameters where associations are represented\nusing copulas. A simulation study demonstrates the viability of the approach.\nThe paper describes how the joint model provides new information that insurers\ncan use to better manage their portfolios of risks using illustrative data from\na Spanish insurer.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 14:58:11 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 13:24:12 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Frees", "Edward W.", ""], ["Bolanc\u00e9", "Catalina", ""], ["Guillen", "Montserrat", ""], ["Valdez", "Emiliano", ""]]}, {"id": "1810.04630", "submitter": "Jing Miao", "authors": "Jing Miao, Hongyuan Yuan, Zhenyu Yan", "title": "Equivalence Test in Multi-dimensional Space with Applications in A/B\n  Testing", "comments": "9 pages; double-column", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a statistical testing framework to check whether a\nrandom sample splitting in a multi-dimensional space is carried out in a valid\nway, which could be directly applied to A/B testing and multivariate testing to\nensure the online traffic split is truly random with respect to the covariates.\nWe believe this is an important step of quality control that is missing in many\nreal world online experiments. Here, we propose a randomized chi-square test\nmethod, compared with propensity score and distance components (DISCO) test\nmethods, to test the hypothesis that the post-split categorical data sets have\nthe same multi-dimensional distribution. The methods can be easily generalized\nto continuous data. We also propose a resampling procedure to adjust for\nmultiplicity which in practice often has higher power than some existing method\nsuch as Holm's procedure. We try the three methods on both simulated and real\ndata sets from Adobe Experience Cloud and show that each method has its own\nadvantage while all of them establish promising power. To our knowledge, we are\namong the first ones to formulate the validity of A/B testing into a\npost-experiments statistical testing problem. Our methodology is non-parametric\nand requires minimum assumption on the data, so it can also have a wide range\nof application in other areas such as clinical trials, medicine, and\nrecommendation system where random data splitting is needed.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 17:12:41 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Miao", "Jing", ""], ["Yuan", "Hongyuan", ""], ["Yan", "Zhenyu", ""]]}, {"id": "1810.04651", "submitter": "Jingyi Kenneth Tay", "authors": "J. Kenneth Tay, Jerome Friedman and Robert Tibshirani", "title": "Principal component-guided sparse regression", "comments": "Update to acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for supervised learning, especially suited to wide\ndata where the number of features is much greater than the number of\nobservations. The method combines the lasso ($\\ell_1$) sparsity penalty with a\nquadratic penalty that shrinks the coefficient vector toward the leading\nprincipal components of the feature matrix. We call the proposed method the\n\"principal components lasso\" (\"pcLasso\"). The method can be especially powerful\nif the features are pre-assigned to groups (such as cell-pathways, assays or\nprotein interaction networks). In that case, pcLasso shrinks each group-wise\ncomponent of the solution toward the leading principal components of that\ngroup. In the process, it also carries out selection of the feature groups. We\nprovide some theory for this method and illustrate it on a number of simulated\nand real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 17:18:46 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 02:46:45 GMT"}, {"version": "v3", "created": "Wed, 24 Oct 2018 06:38:27 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Tay", "J. Kenneth", ""], ["Friedman", "Jerome", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1810.04671", "submitter": "Cristina Mollica", "authors": "Cristina Mollica and Luca Tardella", "title": "Bayesian analysis of ranking data with the constrained Extended\n  Plackett-Luce model", "comments": "20 pages, 4 figures, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1803.02881", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multistage ranking models, including the popular Plackett-Luce distribution\n(PL), rely on the assumption that the ranking process is performed\nsequentially, by assigning the positions from the top to the bottom one\n(forward order). A recent contribution to the ranking literature relaxed this\nassumption with the addition of the discrete-valued reference order parameter,\nyielding the novel Extended Plackett-Luce model (EPL). Inference on the EPL and\nits generalization into a finite mixture framework was originally addressed\nfrom the frequentist perspective. In this work, we propose the Bayesian\nestimation of the EPL with order constraints on the reference order parameter.\nThe proposed restrictions reflect a meaningful rank assignment process. By\ncombining the restrictions with the data augmentation strategy and the\nconjugacy of the Gamma prior distribution with the EPL, we facilitate the\nconstruction of a tuned joint Metropolis-Hastings algorithm within Gibbs\nsampling to simulate from the posterior distribution. The Bayesian approach\nallows to address more efficiently the inference on the additional\ndiscrete-valued parameter and the assessment of its estimation uncertainty. The\nusefulness of the proposal is illustrated with applications to simulated and\nreal datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 08:30:52 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 22:54:35 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Mollica", "Cristina", ""], ["Tardella", "Luca", ""]]}, {"id": "1810.04725", "submitter": "Richard Chen", "authors": "Richard Y. Chen", "title": "Inference for Volatility Functionals of Multivariate It\\^o\n  Semimartingales Observed with Jump and Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the nonparametric inference for nonlinear volatility\nfunctionals of general multivariate It\\^o semimartingales, in high-frequency\nand noisy setting. Pre-averaging and truncation enable simultaneous handling of\nnoise and jumps. Second-order expansion reveals explicit biases and a pathway\nto bias correction. Estimators based on this framework achieve the optimal\nconvergence rate. A class of stable central limit theorems are attained with\nestimable asymptotic covariance matrices. This paper form a basis for infill\nasymptotic results of, for example, the realized Laplace transform, the\nrealized principal component analysis, the continuous-time linear regression,\nand the generalized method of integrated moments, hence helps to extend the\napplication scopes to more frequently sampled noisy data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 19:37:15 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 05:24:55 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Chen", "Richard Y.", ""]]}, {"id": "1810.04785", "submitter": "Rahul Ghosal", "authors": "Sedigheh Mirzaei Salehabadi, Debasis Sengupta, Rahul Ghosal", "title": "Estimating menarcheal age distribution from partially recalled data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a cross-sectional study, adolescent and young adult females were asked to\nrecall the time of menarche, if experienced. Some respondents recalled the date\nexactly, some recalled only the month or the year of the event, and some were\nunable to recall anything. We consider estimation of the menarcheal age\ndistribution from this interval censored data. A~complicated interplay between\nage-at-event and calendar time, together with the evident fact of memory fading\nwith time, makes the censoring informative. We propose a model where the\nprobabilities of various types of recall would depend on the time since\nmenarche. For parametric estimation we model these probabilities using\nmultinomial regression function. Establishing consistency and asymptotic\nnormality of the parametric MLE requires a bit of tweaking of the standard\nasymptotic theory, as the data format varies from case to case. We also provide\na non-parametric MLE, propose a computationally simpler approximation, and\nestablish the consistency of both these estimators under mild conditions. We\nstudy the small sample performance of the parametric and non-parametric\nestimators through Monte Carlo simulations. Moreover, we provide a graphical\ncheck of the assumption of the multinomial model for the recall probabilities,\nwhich appears to hold for the menarcheal data set. Our analysis shows that the\nuse of the partially recalled part of the data indeed leads to smaller\nconfidence intervals of the survival function.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 23:49:50 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 20:37:58 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Salehabadi", "Sedigheh Mirzaei", ""], ["Sengupta", "Debasis", ""], ["Ghosal", "Rahul", ""]]}, {"id": "1810.04806", "submitter": "Tamara Fernandez", "authors": "Tamara Fern\\'andez and Nicol\\'as Rivera", "title": "Kaplan-Meier V- and U-statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study Kaplan-Meier V- and U-statistics respectively defined\nas $\\theta(\\widehat{F}_n)=\\sum_{i,j}K(X_{[i:n]},X_{[j:n]})W_iW_j$ and\n$\\theta_U(\\widehat{F}_n)=\\sum_{i\\neq j}K(X_{[i:n]},X_{[j:n]})W_iW_j/\\sum_{i\\neq\nj}W_iW_j$, where $\\widehat{F}_n$ is the Kaplan-Meier estimator,\n$\\{W_1,\\ldots,W_n\\}$ are the Kaplan-Meier weights and $K:(0,\\infty)^2\\to\\mathbb\nR$ is a symmetric kernel. As in the canonical setting of uncensored data, we\ndifferentiate between two asymptotic behaviours for $\\theta(\\widehat{F}_n)$ and\n$\\theta_U(\\widehat{F}_n)$. Additionally, we derive an asymptotic canonical\nV-statistic representation of the Kaplan-Meier V- and U-statistics. By using\nthis representation we study properties of the asymptotic distribution.\nApplications to hypothesis testing are given.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 00:58:53 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 16:41:34 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Fern\u00e1ndez", "Tamara", ""], ["Rivera", "Nicol\u00e1s", ""]]}, {"id": "1810.04808", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts, Andrea Tancredi, and Brunero Liseo", "title": "Generalized Bayesian Record Linkage and Regression with Exact Error\n  Propagation", "comments": "18 pages, 5 figures", "journal-ref": "Steorts, R.C., Tancredi, A., Liseo, B. Generalized Bayesian Record\n  Linkage and Regression with Exact Error Propagation, PSD (2018)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage (de-duplication or entity resolution) is the process of\nmerging noisy databases to remove duplicate entities. While record linkage\nremoves duplicate entities from such databases, the downstream task is any\ninferential, predictive, or post-linkage task on the linked data. One goal of\nthe downstream task is obtaining a larger reference data set, allowing one to\nperform more accurate statistical analyses. In addition, there is inherent\nrecord linkage uncertainty passed to the downstream task. Motivated by the\nabove, we propose a generalized Bayesian record linkage method and consider\nmultiple regression analysis as the downstream task. Records are linked via a\nrandom partition model, which allows for a wide class to be considered. In\naddition, we jointly model the record linkage and downstream task, which allows\none to account for the record linkage uncertainty exactly. Moreover, one is\nable to generate a feedback propagation mechanism of the information from the\nproposed Bayesian record linkage model into the downstream task. This feedback\neffect is essential to eliminate potential biases that can jeopardize resulting\ndownstream task. We apply our methodology to multiple linear regression, and\nillustrate empirically that the \"feedback effect\" is able to improve the\nperformance of record linkage.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 01:13:04 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Steorts", "Rebecca C.", ""], ["Tancredi", "Andrea", ""], ["Liseo", "Brunero", ""]]}, {"id": "1810.04842", "submitter": "Sharon Lee", "authors": "Sharon X. Lee, Geoffrey J. McLachlan", "title": "On formulations of skew factor models: skew errors versus skew factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, there have been a number of proposals for generalizing\nthe factor analysis (FA) model and its mixture version (known as mixtures of\nfactor analyzers (MFA)) using non-normal and asymmetric distributions. These\nmodels adopt various types of skew densities for either the factors or the\nerrors. While the relationships between various choices of skew distributions\nhave been discussed in the literature, the differences between placing the\nassumption of skewness on the factors or on the errors have not been closely\nstudied. This paper examines these formulations and discusses the connections\nbetween these two types of formulations for skew factor models. In doing so, we\nintroduce a further formulation that unifies these two formulations; that is,\nplacing a skew distribution on both the factors and the errors.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 05:12:03 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 08:01:08 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Lee", "Sharon X.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1810.04996", "submitter": "Junpei Komiyama", "authors": "Junpei Komiyama and Takanori Maehara", "title": "A Simple Way to Deal with Cherry-picking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical hypothesis testing serves as statistical evidence for scientific\ninnovation. However, if the reported results are intentionally biased,\nhypothesis testing no longer controls the rate of false discovery. In\nparticular, we study such selection bias in machine learning models where the\nreporter is motivated to promote an algorithmic innovation. When the number of\npossible configurations (e.g., datasets) is large, we show that the reporter\ncan falsely report an innovation even if there is no improvement at all. We\npropose a `post-reporting' solution to this issue where the bias of the\nreported results is verified by another set of results. The theoretical\nfindings are supported by experimental results with synthetic and real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 13:06:48 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Komiyama", "Junpei", ""], ["Maehara", "Takanori", ""]]}, {"id": "1810.05042", "submitter": "Fr\\'ed\\'eric Bertrand", "authors": "Hiba Alawieh, Fr\\'ed\\'eric Bertrand, Myriam Maumy-Bertrand, Nicolas\n  Wicker and Baydaa Al Ayoubi", "title": "A random model for multidimensional fitting method", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional fitting (MDF) method is a multivariate data analysis method\nrecently developed and based on the fitting of distances. Two matrices are\navailable: one contains the coordinates of the points and the second contains\nthe distances between the same points. The idea of MDF is to modify the\ncoordinates through modification vectors in order to fit the new distances\ncalculated on the modified coordinates to the given distances. In the previous\nworks, the modification vectors are taken as deterministic variables, so here\nwe want to take into account the random effects that can be produce during the\nmodification. An application in the sensometric domain is also given.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 14:20:30 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Alawieh", "Hiba", ""], ["Bertrand", "Fr\u00e9d\u00e9ric", ""], ["Maumy-Bertrand", "Myriam", ""], ["Wicker", "Nicolas", ""], ["Ayoubi", "Baydaa Al", ""]]}, {"id": "1810.05099", "submitter": "Bart J. A. Mertens", "authors": "B. J. A. Mertens, E. Banzato, L.C. de Wreede", "title": "Construction and assessment of prediction rules for binary outcome in\n  the presence of missing predictor data using multiple imputation: theoretical\n  perspective and data-based evaluation", "comments": "This paper is under review at Biometrical Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of calibration and assessment of predictive rules\nin prognostic designs when missing values are present in the predictors. Our\npaper has two key objectives which are entwined. The first is to investigate\nhow the calibration of the prediction rule can be combined with the use of\nmultiple imputation to account for missing predictor observations. The second\nobjective is to propose such methods that can be implemented with current\nmultiple imputation software, while allowing for unbiased predictive assessment\nthrough validation on new observations for which outcome is not yet available.\n  To inform the definition of methodology, we commence with a review of the\ntheoretical background of multiple imputation as a model estimation approach as\nopposed to a purely algorithmic description. We specifically contrast\napplication of multiple imputation for parameter (effect) estimation with\npredictive calibration. Based on this review, two approaches are formulated, of\nwhich the second utilizes application of the classical Rubin's rules for\nparameter estimation, while the first approach averages probabilities from\nmodels fitted on single imputations to directly approximate the predictive\ndensity for future observations. We present implementations using current\nsoftware which allow for validatory or cross-validatory estimation of\nperformance measures, as well as imputation of missing data in predictors on\nthe future data where outcome is by definition as yet unobserved.\n  We restrict discussion to binary outcome and logistic regression throughout,\nthough the principles discussed are generally applicable. We present two data\nsets as examples from our regular consultative practice. Results show little\ndifference between methods for accuracy but substantial reductions in variation\nof calibrated probabilities when using the first approach.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 16:04:43 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Mertens", "B. J. A.", ""], ["Banzato", "E.", ""], ["de Wreede", "L. C.", ""]]}, {"id": "1810.05374", "submitter": "Aki Vehtari", "authors": "Aki Vehtari, Daniel P. Simpson, Yuling Yao, Andrew Gelman", "title": "Limitations of \"Limitations of Bayesian leave-one-out cross-validation\n  for model selection\"", "comments": "To appear in Computational Brain & Behavior", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is an invited discussion of the article by Gronau and\nWagenmakers (2018) that can be found at\nhttps://dx.doi.org/10.1007/s42113-018-0011-7.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 06:45:46 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Vehtari", "Aki", ""], ["Simpson", "Daniel P.", ""], ["Yao", "Yuling", ""], ["Gelman", "Andrew", ""]]}, {"id": "1810.05450", "submitter": "Paul Kirk", "authors": "Oliver M. Crook, Laurent Gatto, Paul D. W. Kirk", "title": "Fast approximate inference for variable selection in Dirichlet process\n  mixtures, with an application to pan-cancer proteomics", "comments": "27 pages, 8 figures. For associated R package, see\n  https://github.com/ococrook/sugsvarsel", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dirichlet Process (DP) mixture model has become a popular choice for\nmodel-based clustering, largely because it allows the number of clusters to be\ninferred. The sequential updating and greedy search (SUGS) algorithm (Wang and\nDunson, 2011) was proposed as a fast method for performing approximate Bayesian\ninference in DP mixture models, by posing clustering as a Bayesian model\nselection (BMS) problem and avoiding the use of computationally costly Markov\nchain Monte Carlo methods. Here we consider how this approach may be extended\nto permit variable selection for clustering, and also demonstrate the benefits\nof Bayesian model averaging (BMA) in place of BMS. Through an array of\nsimulation examples and well-studied examples from cancer transcriptomics, we\nshow that our method performs competitively with the current state-of-the-art,\nwhile also offering computational benefits. We apply our approach to\nreverse-phase protein array (RPPA) data from The Cancer Genome Atlas (TCGA) in\norder to perform a pan-cancer proteomic characterisation of 5,157 tumour\nsamples. We have implemented our approach, together with the original SUGS\nalgorithm, in an open-source R package named sugsvarsel, which accelerates\nanalysis by performing intensive computations in C++ and provides automated\nparallel processing. The R package is freely available from:\nhttps://github.com/ococrook/sugsvarsel\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 11:17:17 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Crook", "Oliver M.", ""], ["Gatto", "Laurent", ""], ["Kirk", "Paul D. W.", ""]]}, {"id": "1810.05679", "submitter": "Xu Shi", "authors": "Xu Shi, Xiaoou Li, Tianxi Cai", "title": "Spherical Regression under Mismatch Corruption with Application to\n  Automated Knowledge Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a series of applications in data integration, language\ntranslation, bioinformatics, and computer vision, we consider spherical\nregression with two sets of unit-length vectors when the data are corrupted by\na small fraction of mismatch in the response-predictor pairs. We propose a\nthree-step algorithm in which we initialize the parameters by solving an\northogonal Procrustes problem to estimate a translation matrix $\\mathbb{W}$\nignoring the mismatch. We then estimate a mapping matrix aiming to correct the\nmismatch using hard-thresholding to induce sparsity, while incorporating\npotential group information. We eventually obtain a refined estimate for\n$\\mathbb{W}$ by removing the estimated mismatched pairs. We derive the error\nbound for the initial estimate of $\\mathbb{W}$ in both fixed and\nhigh-dimensional setting. We demonstrate that the refined estimate of\n$\\mathbb{W}$ achieves an error rate that is as good as if no mismatch is\npresent. We show that our mapping recovery method not only correctly\ndistinguishes one-to-one and one-to-many correspondences, but also consistently\nidentifies the matched pairs and estimates the weight vector for combined\ncorrespondence. We examine the finite sample performance of the proposed method\nvia extensive simulation studies, and with application to the unsupervised\ntranslation of medical codes using electronic health records data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 18:57:17 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 14:45:35 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Shi", "Xu", ""], ["Li", "Xiaoou", ""], ["Cai", "Tianxi", ""]]}, {"id": "1810.05973", "submitter": "Hao Chen", "authors": "Lynna Chu and Hao Chen", "title": "Sequential Change-point Detection for High-dimensional and non-Euclidean\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many modern applications, high-dimensional/non-Euclidean data sequences\nare collected to study complicated phenomena over time and it is of scientific\nimportance to detect anomaly events as the data are being collected. We studied\na nonparametric framework that utilizes nearest neighbor information among the\nobservations and can be applied to such sequences. We considered new test\nstatistics under this framework that can make more positive detections and can\ndetect anomaly events sooner than the existing test under many common scenarios\nwith the false discovery rate controlled at the same level. Analytic formulas\nfor approximate the average run lengths of the new approaches are derived to\nmake them fast applicable to large datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 05:12:48 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Chu", "Lynna", ""], ["Chen", "Hao", ""]]}, {"id": "1810.06089", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban, Sifan Liu", "title": "Asymptotics for Sketching in Least Squares Regression", "comments": null, "journal-ref": "Updated manuscript to be consistent with version at NeurIPS 2019", "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.NA math.NA stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a least squares regression problem where the data has been\ngenerated from a linear model, and we are interested to learn the unknown\nregression parameters. We consider \"sketch-and-solve\" methods that randomly\nproject the data first, and do regression after. Previous works have analyzed\nthe statistical and computational performance of such methods. However, the\nexisting analysis is not fine-grained enough to show the fundamental\ndifferences between various methods, such as the Subsampled Randomized Hadamard\nTransform (SRHT) and Gaussian projections. In this paper, we make progress on\nthis problem, working in an asymptotic framework where the number of datapoints\nand dimension of features goes to infinity. We find the limits of the accuracy\nloss (for estimation and test error) incurred by popular sketching methods. We\nshow separation between different methods, so that SRHT is better than Gaussian\nprojections. Our theoretical results are verified on both real and synthetic\ndata. The analysis of SRHT relies on novel methods from random matrix theory\nthat may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 19:48:05 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 19:25:12 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Dobriban", "Edgar", ""], ["Liu", "Sifan", ""]]}, {"id": "1810.06167", "submitter": "Wenyu Zhang", "authors": "Wenyu Zhang, Daniel Gilbert, David Matteson", "title": "ABACUS: Unsupervised Multivariate Change Detection via Bayesian Source\n  Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection involves segmenting sequential data such that observations\nin the same segment share some desired properties. Multivariate change\ndetection continues to be a challenging problem due to the variety of ways\nchange points can be correlated across channels and the potentially poor\nsignal-to-noise ratio on individual channels. In this paper, we are interested\nin locating additive outliers (AO) and level shifts (LS) in the unsupervised\nsetting. We propose ABACUS, Automatic BAyesian Changepoints Under Sparsity, a\nBayesian source separation technique to recover latent signals while also\ndetecting changes in model parameters. Multi-level sparsity achieves both\ndimension reduction and modeling of signal changes. We show ABACUS has\ncompetitive or superior performance in simulation studies against\nstate-of-the-art change detection methods and established latent variable\nmodels. We also illustrate ABACUS on two real application, modeling genomic\nprofiles and analyzing household electricity consumption.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 03:12:01 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Zhang", "Wenyu", ""], ["Gilbert", "Daniel", ""], ["Matteson", "David", ""]]}, {"id": "1810.06191", "submitter": "Armeen Taeb", "authors": "Daniel Sanz-Alonso and Andrew M. Stuart and Armeen Taeb", "title": "Inverse Problems and Data Assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These notes are designed with the aim of providing a clear and concise\nintroduction to the subjects of Inverse Problems and Data Assimilation, and\ntheir inter-relations, together with citations to some relevant literature in\nthis area. The first half of the notes is dedicated to studying the Bayesian\nframework for inverse problems. Techniques such as importance sampling and\nMarkov Chain Monte Carlo (MCMC) methods are introduced; these methods have the\ndesirable property that in the limit of an infinite number of samples they\nreproduce the full posterior distribution. Since it is often computationally\nintensive to implement these methods, especially in high dimensional problems,\napproximate techniques such as approximating the posterior by a Dirac or a\nGaussian distribution are discussed. The second half of the notes cover data\nassimilation. This refers to a particular class of inverse problems in which\nthe unknown parameter is the initial condition of a dynamical system, and in\nthe stochastic dynamics case the subsequent states of the system, and the data\ncomprises partial and noisy observations of that (possibly stochastic)\ndynamical system. We will also demonstrate that methods developed in data\nassimilation may be employed to study generic inverse problems, by introducing\nan artificial time to generate a sequence of probability measures interpolating\nfrom the prior to the posterior.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 06:09:12 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 16:30:01 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Sanz-Alonso", "Daniel", ""], ["Stuart", "Andrew M.", ""], ["Taeb", "Armeen", ""]]}, {"id": "1810.06234", "submitter": "Alexis Derumigny", "authors": "Alexis Derumigny and Jean-David Fermanian", "title": "On kernel-based estimation of conditional Kendall's tau: finite-distance\n  bounds and asymptotic behavior", "comments": "29 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1802.07613", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonparametric estimators of conditional Kendall's tau, a measure of\nconcordance between two random variables given some covariates. We prove\nnon-asymptotic bounds with explicit constants, that hold with high\nprobabilities. We provide \"direct proofs\" of the consistency and the asymptotic\nlaw of conditional Kendall's tau. A simulation study evaluates the numerical\nperformance of such nonparametric estimators.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 09:10:50 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 17:49:00 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Derumigny", "Alexis", ""], ["Fermanian", "Jean-David", ""]]}, {"id": "1810.06334", "submitter": "Erik-Jan van Kesteren", "authors": "Erik-Jan van Kesteren and Daniel L. Oberski", "title": "Exploratory Mediation Analysis with Many Potential Mediators", "comments": "R code and package are available online as supplementary material at\n  https://github.com/vankesteren/cmfilter and\n  https://github.com/vankesteren/ema_simulations", "journal-ref": "Structural Equation Modeling: A Multidisciplinary Journal,26:5,\n  710-723 (2019)", "doi": "10.1080/10705511.2019.1588124", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social and behavioral scientists are increasingly employing technologies such\nas fMRI, smartphones, and gene sequencing, which yield 'high-dimensional'\ndatasets with more columns than rows. There is increasing interest, but little\nsubstantive theory, in the role the variables in these data play in known\nprocesses. This necessitates exploratory mediation analysis, for which\nstructural equation modeling is the benchmark method. However, this method\ncannot perform mediation analysis with more variables than observations. One\noption is to run a series of univariate mediation models, which incorrectly\nassumes independence of the mediators. Another option is regularization, but\nthe available implementations may lead to high false positive rates. In this\npaper, we develop a hybrid approach which uses components of both filter and\nregularization: the 'Coordinate-wise Mediation Filter'. It performs filtering\nconditional on the other selected mediators. We show through simulation that it\nimproves performance over existing methods. Finally, we provide an empirical\nexample, showing how our method may be used for epigenetic research.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 13:11:37 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 08:10:24 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["van Kesteren", "Erik-Jan", ""], ["Oberski", "Daniel L.", ""]]}, {"id": "1810.06433", "submitter": "Geoff Nicholls", "authors": "Jeong Eun Lee, Geoff K. Nicholls, Robin J. Ryder", "title": "Calibration procedures for approximate Bayesian credible sets", "comments": "28 pages, 6 Figures, 1 Table, 4 Algorithm boxes. Revision improves\n  clarity of presentation and adds relevant citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and apply two calibration procedures for checking the coverage of\napproximate Bayesian credible sets including intervals estimated using Monte\nCarlo methods. The user has an ideal prior and likelihood, but generates a\ncredible set for an approximate posterior which is not proportional to the\nproduct of ideal likelihood and prior. We estimate the realised posterior\ncoverage achieved by the approximate credible set. This is the coverage of the\nunknown ``true'' parameter if the data are a realisation of the user's ideal\nobservation model conditioned on the parameter, and the parameter is a draw\nfrom the user's ideal prior. In one approach we estimate the posterior coverage\nat the data by making a semi-parametric logistic regression of binary coverage\noutcomes on simulated data against summary statistics evaluated on simulated\ndata. In another we use Importance Sampling from the approximate posterior,\nwindowing simulated data to fall close to the observed data. We illustrate our\nmethods on four examples.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 14:56:17 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 14:16:50 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Lee", "Jeong Eun", ""], ["Nicholls", "Geoff K.", ""], ["Ryder", "Robin J.", ""]]}, {"id": "1810.06474", "submitter": "M. Ros\\'ario Oliveira", "authors": "M. Ros\\'ario Oliveira, Margarida Azeitona, Ant\\'onio Pacheco, Rui\n  Valadas", "title": "Association measures for interval variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic Data Analysis (SDA) is a relatively new field of statistics that\nextends conventional data analysis by taking into account intrinsic data\nvariability and structure. Unlike conventional data analysis, in SDA the\nfeatures characterizing the data can be multi-valued, such as intervals or\nhistograms. SDA has been mainly approached from a sampling perspective. In this\nwork, we propose a model that links the micro-data and macro-data of\ninterval-valued symbolic variables, which takes a populational perspective.\nUsing this model, we derive the micro-data assumptions underlying the various\ndefinitions of symbolic covariance matrices proposed in the literature, and\nshow that these assumptions can be too restrictive, raising applicability\nconcerns. We analyze the various definitions using worked examples and four\ndatasets. Our results show that the existence/absence of correlations in the\nmacro-data may not be correctly captured by the definitions of symbolic\ncovariance matrices and that, in real data, there can be a strong divergence\nbetween these definitions. Thus, in order to select the most appropriate\ndefinition, one must have some knowledge about the micro-data structure.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 15:45:45 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 13:08:13 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Oliveira", "M. Ros\u00e1rio", ""], ["Azeitona", "Margarida", ""], ["Pacheco", "Ant\u00f3nio", ""], ["Valadas", "Rui", ""]]}, {"id": "1810.06587", "submitter": "Runjing Liu", "authors": "Runjing Liu, Ryan Giordano, Michael I. Jordan, and Tamara Broderick", "title": "Evaluating Sensitivity to the Stick Breaking Prior in Bayesian\n  Nonparametrics", "comments": "8 pages, 6 figures; submitted to Bayesian Nonparametrics workshop at\n  NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central question in many probabilistic clustering problems is how many\ndistinct clusters are present in a particular dataset. A Bayesian nonparametric\n(BNP) model addresses this question by placing a generative process on cluster\nassignment. However, like all Bayesian approaches, BNP requires the\nspecification of a prior. In practice, it is important to quantitatively\nestablish that the prior is not too informative, particularly when the\nparticular form of the prior is chosen for mathematical convenience rather than\nbecause of a considered subjective belief.\n  We derive local sensitivity measures for a truncated variational Bayes (VB)\napproximation and approximate nonlinear dependence of a VB optimum on prior\nparameters using a local Taylor series approximation. Using a stick-breaking\nrepresentation of a Dirichlet process, we consider perturbations both to the\nscalar concentration parameter and to the functional form of the stick-\nbreaking distribution.\n  Unlike previous work on local Bayesian sensitivity for BNP, we pay special\nattention to the ability of our sensitivity measures to extrapolate to\ndifferent priors, rather than treating the sensitivity as a measure of\nrobustness per se. Extrapolation motivates the use of multiplicative\nperturbations to the functional form of the prior for VB. Additionally, we\nlinearly approximate only the computationally intensive part of inference --\nthe optimization of the global parameters -- and retain the nonlinearity of\neasily computed quantities as functions of the global parameters.\n  We apply our methods to estimate sensitivity of the expected number of\ndistinct clusters present in the Iris dataset to the BNP prior specification.\nWe evaluate the accuracy of our approximations by comparing to the much more\nexpensive process of re-fitting the model.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 18:04:12 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Liu", "Runjing", ""], ["Giordano", "Ryan", ""], ["Jordan", "Michael I.", ""], ["Broderick", "Tamara", ""]]}, {"id": "1810.06608", "submitter": "Yawen Guan", "authors": "Yawen Guan, Christian Sampson, J. Derek Tucker, Won Chang, Anirban\n  Mondal, Murali Haran and Deborah Sulsky", "title": "Computer model calibration based on image warping metrics: an\n  application for sea ice deformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arctic sea ice plays an important role in the global climate. Sea ice models\ngoverned by physical equations have been used to simulate the state of the ice\nincluding characteristics such as ice thickness, concentration, and motion.\nMore recent models also attempt to capture features such as fractures or leads\nin the ice. These simulated features can be partially misaligned or misshapen\nwhen compared to observational data, whether due to numerical approximation or\nincomplete physics. In order to make realistic forecasts and improve\nunderstanding of the underlying processes, it is necessary to calibrate the\nnumerical model to field data. Traditional calibration methods based on\ngeneralized least-square metrics are flawed for linear features such as sea ice\ncracks. We develop a statistical emulation and calibration framework that\naccounts for feature misalignment and misshapenness, which involves optimally\naligning model output with observed features using cutting edge image\nregistration techniques. This work can also have application to other physical\nmodels which produce coherent structures.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 18:43:28 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 16:08:01 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 13:57:25 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Guan", "Yawen", ""], ["Sampson", "Christian", ""], ["Tucker", "J. Derek", ""], ["Chang", "Won", ""], ["Mondal", "Anirban", ""], ["Haran", "Murali", ""], ["Sulsky", "Deborah", ""]]}, {"id": "1810.06655", "submitter": "Yaqing Chen", "authors": "Yaqing Chen, Matthew Dawson and Hans-Georg M\\\"uller", "title": "Rank Dynamics for Functional Data", "comments": "To appear in Computational Statistics & Data Analysis", "journal-ref": null, "doi": "10.1016/j.csda.2020.106963", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of the dynamic behavior of cross-sectional ranks over time for\nfunctional data and the ranks of the observed curves at each time point and\ntheir temporal evolution can yield valuable insights into the time dynamics of\nfunctional data. This approach is of interest in various application areas. For\nthe analysis of the dynamics of ranks, estimation of the cross-sectional ranks\nof functional data is a first step. Several statistics of interest for ranked\nfunctional data are proposed. To quantify the evolution of ranks over time, a\nmodel for rank derivatives is introduced, where rank dynamics are decomposed\ninto two components. One component corresponds to population changes and the\nother to individual changes that both affect the rank trajectories of\nindividuals. The joint asymptotic normality for suitable estimates of these two\ncomponents is established. The proposed approaches are illustrated with\nsimulations and three longitudinal data sets: Growth curves obtained from the\nZ\\\"urich Longitudinal Growth Study, monthly house price data in the US from\n1996 to 2015 and Major League Baseball offensive data for the 2017 season.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 20:01:33 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 19:36:26 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Chen", "Yaqing", ""], ["Dawson", "Matthew", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1810.06738", "submitter": "Sinead Williamson", "authors": "Sinead A. Williamson and Mauricio Tec", "title": "Random clique covers for graphs with local density and global sparsity", "comments": "Appears in UAI 2019. This version includes appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large real-world graphs tend to be sparse, but they often contain many\ndensely connected subgraphs and exhibit high clustering coefficients. While\nrecent random graph models can capture this sparsity, they ignore the local\ndensity, or vice versa. We develop a Bayesian nonparametric graph model based\non random edge clique covers, and show that this model can capture power law\ndegree distribution, global sparsity and non-vanishing local clustering\ncoefficient. This distribution can be used directly as a prior on observed\ngraphs, or as part of a hierarchical Bayesian model for inferring latent graph\nstructures.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 22:50:49 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 15:45:31 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Williamson", "Sinead A.", ""], ["Tec", "Mauricio", ""]]}, {"id": "1810.06803", "submitter": "Gal Mishne", "authors": "Gal Mishne, Eric C. Chi, Ronald R. Coifman", "title": "Co-manifold learning with missing data", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is typically applied to only one mode of a data\nmatrix, either its rows or columns. Yet in many applications, there is an\nunderlying geometry to both the rows and the columns. We propose utilizing this\ncoupled structure to perform co-manifold learning: uncovering the underlying\ngeometry of both the rows and the columns of a given matrix, where we focus on\na missing data setting. Our unsupervised approach consists of three components.\nWe first solve a family of optimization problems to estimate a complete matrix\nat multiple scales of smoothness. We then use this collection of smooth matrix\nestimates to compute pairwise distances on the rows and columns based on a new\nmulti-scale metric that implicitly introduces a coupling between the rows and\nthe columns. Finally, we construct row and column representations from these\nmulti-scale metrics. We demonstrate that our approach outperforms competing\nmethods in both data visualization and clustering.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 04:01:45 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Mishne", "Gal", ""], ["Chi", "Eric C.", ""], ["Coifman", "Ronald R.", ""]]}, {"id": "1810.06989", "submitter": "Marianna Pensky", "authors": "Rasika Rajapakshage and Marianna Pensky", "title": "Clustering in statistical ill-posed linear inverse problems", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical linear inverse problems, one needs to recover classes of\nsimilar curves from their noisy images under an operator that does not have a\nbounded inverse. Problems of this kind appear in many areas of application.\nRoutinely, in such problems clustering is carried out at the pre-processing\nstep and then the inverse problem is solved for each of the cluster averages\nseparately. As a result, the errors of the procedures are usually examined for\nthe estimation step only. The objective of this paper is to examine, both\ntheoretically and via simulations, the effect of clustering on the accuracy of\nthe solutions of general ill-posed linear inverse problems. In particular, we\nassume that one observes $X_m = A f_m + \\delta \\epsilon_m$, $m=1, \\cdots, M$,\nwhere functions $f_m$ can be grouped into $K$ classes and one needs to recover\na vector function ${\\bf f}= (f_1,\\cdots, f_M)^T$. We construct an estimators\nfor ${\\bf f}$ as a solution of a penalized optimization problem and derive an\noracle inequality for its precision. By deriving upper and minimax lower bounds\nfor the error, we confirm that the estimator is minimax optimal or nearly\nminimax optimal up to a logarithmic factor of the number of observations. One\nof the advantages of our estimation procedure is that we do not assume that the\nnumber of clusters is known in advance. We conclude that clustering at the\npre-processing step is beneficial when the problem is moderately ill-posed. It\nshould be applied with extreme care when the problem is severely ill-posed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 13:37:33 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 14:42:15 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 01:02:24 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Rajapakshage", "Rasika", ""], ["Pensky", "Marianna", ""]]}, {"id": "1810.07240", "submitter": "My-Nhi Nguyen", "authors": "Majid Mojirsheibani, My-Nhi Nguyen and Crystal Shaw", "title": "Statistical classification for partially observed functional data via\n  filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with the problem of functional classification for\nL2-valued random covariates when some of the covariates may have missing or\nunobservable fragments. Here, it is allowed for both the training sample as\nwell as the new unclassified observation to have missing fragments in their\nfunctional covariates. Furthermore, unlike most previous results in the\nliterature, where covariate fragments are typically assumed to be missing\ncompletely at random, we do not impose any such assumptions here. Given the\nobserved segments of the curves, we construct a kernel-type classifier which is\nquite straightforward to implement in practice. We also establish the strong\nconsistency of the proposed classifier and provide some numerical examples to\nassess its performance in practice.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 19:15:03 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 23:10:18 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Mojirsheibani", "Majid", ""], ["Nguyen", "My-Nhi", ""], ["Shaw", "Crystal", ""]]}, {"id": "1810.07245", "submitter": "Sedigheh Mirzaei Salehabadi", "authors": "Sedigheh Mirzaei Salehabadi, Edwina Yeung, Germaine M. Buck Louis,\n  Rajeshwari Sundaram", "title": "Assessing the distribution of discrete survival time in presence of\n  recall error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrospectively ascertained survival time may be subject to recall error. An\nexample of discrete survival time with such recall error is time-to-pregnancy\n(TTP), the number of months non-contracepting couples require to get pregnant\nwhich is a measure of human fecundity. The epidemiological literature has\ndemonstrated that retrospective TTP is subject to recall error and statistical\nmodels focusing on TTP have not accounted for the recall error. We propose a\nmultistage model that utilizes women's retrospectively-reported TTP and\nassociated certainty to estimate the TTP distribution. Our proposed model\nutilizes a discrete survival function that accounts for random heterogeneity\narising from between women TTP data as well as a multinomial regression model\nto account for her certainty as accuracy may decline over time, i.e., depends\non time since pregnancy in estimating the TTP distribution. Other novel\nfeatures of the model include attention to whether the pregnancy was\n(un)planned as well as providing an approach to predict survival function for\nwomen without a reported TTP. Our model allows for the consideration of\ncovariates for each of the underlying factors of (un)planned pregnancy, measure\nof certainty and TTP distribution. The proposed model is applicable for any\ndiscrete survival time when certainty in reporting may be a consideration. We\nuse Monte Carlo simulations to assess the finite sample performance for the\nproposed estimators. We illustrate our proposed method using data from Upstate\nKIDS Study.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 19:26:11 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Salehabadi", "Sedigheh Mirzaei", ""], ["Yeung", "Edwina", ""], ["Louis", "Germaine M. Buck", ""], ["Sundaram", "Rajeshwari", ""]]}, {"id": "1810.07403", "submitter": "Behrooz Ghorbani", "authors": "David L. Donoho, Behrooz Ghorbani", "title": "Optimal Covariance Estimation for Condition Number Loss in the Spiked\n  Model", "comments": "85 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study estimation of the covariance matrix under relative condition number\nloss $\\kappa(\\Sigma^{-1/2} \\hat{\\Sigma} \\Sigma^{-1/2})$, where $\\kappa(\\Delta)$\nis the condition number of matrix $\\Delta$, and $\\hat{\\Sigma}$ and $\\Sigma$ are\nthe estimated and theoretical covariance matrices. Optimality in $\\kappa$-loss\nprovides optimal guarantees in two stylized applications: Multi-User Covariance\nEstimation and Multi-Task Linear Discriminant Analysis. We assume the so-called\nspiked covariance model for $\\Sigma$, and exploit recent advances in\nunderstanding that model, to derive a nonlinear shrinker which is\nasymptotically optimal among orthogonally-equivariant procedures. In our\nasymptotic study, the number of variables $p$ is comparable to the number of\nobservations $n$. The form of the optimal nonlinearity depends on the aspect\nratio $\\gamma=p/n$ of the data matrix and on the top eigenvalue of $\\Sigma$.\nFor $\\gamma > 0.618...$, even dependence on the top eigenvalue can be avoided.\nThe optimal shrinker has two notable properties. First, when $p/n \\rightarrow\n\\gamma \\gg 1$ is large, it shrinks even very large eigenvalues substantially,\nby a factor $1/(1+\\gamma)$. Second, even for moderate $\\gamma$, certain highly\nstatistically significant eigencomponents will be completely suppressed. We\nshow that when $\\gamma \\gg 1$ is large, purely diagonal covariance matrices can\nbe optimal, despite the top eigenvalues being large and the empirical\neigenvalues being highly statistically significant. This aligns with\npractitioner experience. We identify intuitively reasonable procedures with\nsmall worst-case relative regret - the simplest being generalized soft\nthresholding having threshold at the bulk edge and slope $(1+\\gamma)^{-1}$\nabove the bulk. For $\\gamma < 2$ it has at most a few percent relative regret.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 07:11:02 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Donoho", "David L.", ""], ["Ghorbani", "Behrooz", ""]]}, {"id": "1810.07509", "submitter": "Celia Garc\\'ia-Pareja", "authors": "Celia Garc\\'ia-Pareja and Matteo Bottai", "title": "Mean survival by ordered fractions of population with censored data", "comments": "6 pages", "journal-ref": "Proceedings of the 33rd International Workshop on Statistical\n  Modelling (IWSM 2018), Volume 2, pp. 56--61", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for estimating mean survival time in the presence\nof censored data, in which we divide the population under study into\nsurvival-ordered fractions defined by a set of proportions, and compute the\nmean survival time for each fraction separately. Our approach provides a\ndetailed picture of the distribution of the time variable while preserving the\nappealing interpretation of the mean. Our measure proves to be of great use in\napplications, particularly those where we are able to detect differences in\nmean survival across groups for certain fractions of the population that would\nhave been overlooked using other available methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 12:59:09 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Garc\u00eda-Pareja", "Celia", ""], ["Bottai", "Matteo", ""]]}, {"id": "1810.07668", "submitter": "Shota Gugushvili", "authors": "Shota Gugushvili, Frank van der Meulen, Moritz Schauer, Peter Spreij", "title": "Bayesian wavelet de-noising with the caravan prior", "comments": "32 pages, 15 figures, 4 tables", "journal-ref": "ESAIM Probab. Stat., Volume 23, pages 947-978, 2019", "doi": "10.1051/ps/2019019", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to both domain expert knowledge and empirical evidence, wavelet\ncoefficients of real signals tend to exhibit clustering patterns, in that they\ncontain connected regions of coefficients of similar magnitude (large or\nsmall). A wavelet de-noising approach that takes into account such a feature of\nthe signal may in practice outperform other, more vanilla methods, both in\nterms of the estimation error and visual appearance of the estimates. Motivated\nby this observation, we present a Bayesian approach to wavelet de-noising,\nwhere dependencies between neighbouring wavelet coefficients are a priori\nmodelled via a Markov chain-based prior, that we term the caravan prior.\nPosterior computations in our method are performed via the Gibbs sampler. Using\nrepresentative synthetic and real data examples, we conduct a detailed\ncomparison of our approach with a benchmark empirical Bayes de-noising method\n(due to Johnstone and Silverman). We show that the caravan prior fares well and\nis therefore a useful addition to the wavelet de-noising toolbox.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 17:03:34 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 08:12:53 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Gugushvili", "Shota", ""], ["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""], ["Spreij", "Peter", ""]]}, {"id": "1810.07677", "submitter": "Mario Fordellone Dr", "authors": "Mario Fordellone and Maurizio Vichi", "title": "Structural Equation Modeling and simultaneous clustering through the\n  Partial Least Squares algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of different homogeneous groups of observations and their\nappropriate analysis in PLS-SEM has become a critical issue in many appli-\ncation fields. Usually, both SEM and PLS-SEM assume the homogeneity of all\nunits on which the model is estimated, and approaches of segmentation present\nin literature, consist in estimating separate models for each segments of\nstatistical units, which have been obtained either by assigning the units to\nsegments a priori defined. However, these approaches are not fully accept- able\nbecause no causal structure among the variables is postulated. In other words,\na modeling approach should be used, where the obtained clusters are homogeneous\nwith respect to the structural causal relationships. In this paper, a new\nmethodology for simultaneous non-hierarchical clus- tering and PLS-SEM is\nproposed. This methodology is motivated by the fact that the sequential\napproach of applying first SEM or PLS-SEM and second the clustering algorithm\nsuch as K-means on the latent scores of the SEM/PLS-SEM may fail to find the\ncorrect clustering structure existing in the data. A simulation study and an\napplication on real data are included to evaluate the performance of the\nproposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 17:26:36 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Fordellone", "Mario", ""], ["Vichi", "Maurizio", ""]]}, {"id": "1810.07809", "submitter": "Juntang Zhuang", "authors": "Juntang Zhuang, Nicha C. Dvornek, Qingyu Zhao, Xiaoxiao Li, Pamela\n  Ventola, James S. Duncan", "title": "Prediction of treatment outcome for autism from structure of the brain\n  based on sure independence screening", "comments": null, "journal-ref": "2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI\n  2019) 2019 Apr 8 (pp. 404-408). IEEE", "doi": null, "report-no": null, "categories": "q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism spectrum disorder (ASD) is a complex neurodevelopmental disorder, and\nbehavioral treatment interventions have shown promise for young children with\nASD. However, there is limited progress in understanding the effect of each\ntype of treatment. In this project, we aim to detect structural changes in the\nbrain after treatment and select structural features associated with treatment\noutcomes. The difficulty in building large databases of patients who have\nreceived specific treatments and the high dimensionality of medical image\nanalysis problems are the challenges in this work. To select predictive\nfeatures and build accurate models, we use the sure independence screening\n(SIS) method. SIS is a theoretically and empirically validated method for\nultra-high dimensional general linear models, and it achieves both predictive\naccuracy and correct feature selection by iterative feature selection. Compared\nwith step-wise feature selection methods, SIS removes multiple features in each\niteration and is computationally efficient. Compared with other linear models\nsuch as elastic-net regression, support vector regression (SVR) and partial\nleast squares regression (PSLR), SIS achieves higher accuracy. We validated the\nsuperior performance of SIS in various experiments: First, we extract brain\nstructural features from FreeSurfer, including cortical thickness, surface\narea, mean curvature and cortical volume. Next, we predict different measures\nof treatment outcomes based on structural features. We show that SIS achieves\nthe highest correlation between prediction and measurements in all tasks.\nFurthermore, we report regions selected by SIS as biomarkers for ASD.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 21:19:46 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 04:24:53 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhuang", "Juntang", ""], ["Dvornek", "Nicha C.", ""], ["Zhao", "Qingyu", ""], ["Li", "Xiaoxiao", ""], ["Ventola", "Pamela", ""], ["Duncan", "James S.", ""]]}, {"id": "1810.07858", "submitter": "Naoki Egami", "authors": "Naoki Egami", "title": "Identification of Causal Diffusion Effects Under Structural Stationarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although social and biomedical scientists have long been interested in the\nprocess through which ideas and behaviors diffuse, the identification of causal\ndiffusion effects, also known as peer and contagion effects, remains\nchallenging. Many scholars consider the commonly used assumption of no omitted\nconfounders to be untenable due to contextual confounding and homophily bias.\nTo address this long-standing problem, we examine the causal identification\nunder a new assumption of structural stationarity, which formalizes the\nunderlying diffusion process with a class of dynamic causal directed acyclic\ngraphs. First, we develop a statistical test that can detect a wide range of\nbiases, including the two types mentioned above. We then propose a\ndifference-in-differences style estimator that can directly correct biases\nunder an additional parametric assumption. Leveraging the proposed methods, we\nstudy the spatial diffusion of hate crimes against refugees in Germany. After\ncorrecting large upward bias in existing studies, we find hate crimes diffuse\nonly to areas that have a high proportion of school dropouts.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 01:26:52 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 04:47:14 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Egami", "Naoki", ""]]}, {"id": "1810.07897", "submitter": "Nabarun Deb", "authors": "Nabarun Deb, Sujayam Saha, Adityanand Guntuboyina and Bodhisattva Sen", "title": "Two-component Mixture Model in the Presence of Covariates", "comments": "88 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a generalization of the two-groups model in the\npresence of covariates --- a problem that has recently received much attention\nin the statistical literature due to its applicability in multiple hypotheses\ntesting problems. The model we consider allows for infinite dimensional\nparameters and offers flexibility in modeling the dependence of the response on\nthe covariates. We discuss the identifiability issues arising in this model and\nsystematically study several estimation strategies. We propose a tuning\nparameter-free nonparametric maximum likelihood method, implementable via the\nEM algorithm, to estimate the unknown parameters. Further, we derive the rate\nof convergence of the proposed estimators --- in particular, we show that the\nfinite sample Hellinger risk for every `approximate' nonparametric maximum\nlikelihood estimator achieves a near-parametric rate (up to logarithmic\nmultiplicative factors). In addition, we propose and theoretically study two\n`marginal' methods that are more scalable and easily implementable. We\ndemonstrate the efficacy of our procedures through extensive simulation studies\nand relevant data analyses --- one arising from neuroscience and the other from\nastronomy. We also outline the application of our methods to multiple testing.\nThe companion R package $\\texttt{NPMLEmix}$ implements all the procedures\nproposed in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 04:46:25 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 17:40:25 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Deb", "Nabarun", ""], ["Saha", "Sujayam", ""], ["Guntuboyina", "Adityanand", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1810.07902", "submitter": "Mengyun Wu", "authors": "Mengyun Wu, Qingzhao Zhang, Shuangge Ma", "title": "Structured gene-environment interaction analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the etiology, progression, and treatment of complex diseases,\ngene-environment (G-E) interactions have important implications beyond the main\nG and E effects. G-E interaction analysis can be more challenging with the\nhigher dimensionality and need for accommodating the \"main effects,\ninteractions\" hierarchy. In the recent literature, an array of novel methods,\nmany of which are based on the penalization technique, have been developed. In\nmost of these studies, however, the structures of G measurements, for example\nthe adjacency structure of SNPs (attributable to their physical adjacency on\nthe chromosomes) and network structure of gene expressions (attributable to\ntheir coordinated biological functions and correlated measurements), have not\nbeen well accommodated. In this study, we develop the structured G-E\ninteraction analysis, where such structures are accommodated using penalization\nfor both the main G effects and interactions. Penalization is also applied for\nregularized estimation and selection. The proposed structured interaction\nanalysis can be effectively realized. It is shown to have the consistency\nproperties under high dimensional settings. Simulations and the analysis of\nGENEVA diabetes data with SNP measurements and TCGA melanoma data with gene\nexpression measurements demonstrate its competitive practical performance.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 05:30:29 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Wu", "Mengyun", ""], ["Zhang", "Qingzhao", ""], ["Ma", "Shuangge", ""]]}, {"id": "1810.08104", "submitter": "Fr\\'ed\\'eric Bertrand", "authors": "Titin Agustin Nengsih, Fr\\'ed\\'eric Bertrand, Myriam Maumy-Bertrand\n  and Nicolas Meyer", "title": "Determining the Number of Components in PLS Regression on Incomplete\n  Data", "comments": "16 pages, 3 figures", "journal-ref": "Statistical Applications in Genetics and Molecular Biology, 2019", "doi": "10.1515/sagmb-2018-0059", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial least squares regression---or PLS---is a multivariate method in which\nmodels are estimated using either the SIMPLS or NIPALS algorithm. PLS\nregression has been extensively used in applied research because of its\neffectiveness in analysing relationships between an outcome and one or several\ncomponents. Note that the NIPALS algorithm is able to provide estimates on\nincomplete data. Selection of the number of components used to build a\nrepresentative model in PLS regression is an important problem. However, how to\ndeal with missing data when using PLS regression remains a matter of debate.\nSeveral approaches have been proposed in the literature, including the $Q^2$\ncriterion, and the AIC and BIC criteria. Here we study the behavior of the\nNIPALS algorithm when used to fit a PLS regression for various proportions of\nmissing data and for different types of missingness. We compare criteria for\nselecting the number of components for a PLS regression on incomplete data and\non imputed datasets using three imputation methods: multiple imputation by\nchained equations, k-nearest neighbor imputation, and singular value\ndecomposition imputation. Various criteria were tested with different\nproportions of missing data (ranging from 5% to 50%) under different\nmissingness assumptions. Q2-leave-one-out component selection methods gave more\nreliable results than AIC and BIC-based ones.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 15:19:15 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Nengsih", "Titin Agustin", ""], ["Bertrand", "Fr\u00e9d\u00e9ric", ""], ["Maumy-Bertrand", "Myriam", ""], ["Meyer", "Nicolas", ""]]}, {"id": "1810.08140", "submitter": "Alessandro Gasparini", "authors": "Alessandro Gasparini, Mark S. Clements, Keith R. Abrams, Michael J.\n  Crowther", "title": "Impact of model misspecification in shared frailty survival models", "comments": null, "journal-ref": null, "doi": "10.1002/sim.8309", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival models incorporating random effects to account for unmeasured\nheterogeneity are being increasingly used in biostatistical and applied\nresearch. Specifically, unmeasured covariates whose lack of inclusion in the\nmodel would lead to biased, inefficient results are commonly modelled by\nincluding a subject-specific (or cluster-specific) frailty term that follows a\ngiven distribution (e.g. Gamma or log-Normal). Despite that, in the context of\nparametric frailty models little is known about the impact of misspecifying the\nbaseline hazard, the frailty distribution, or both. Therefore, our aim is to\nquantify the impact of such misspecification in a wide variety of clinically\nplausible scenarios via Monte Carlo simulation, using open source software\nreadily available to applied researchers. We generate clustered survival data\nassuming various baseline hazard functions, including mixture distributions\nwith turning points, and assess the impact of sample size, variance of the\nfrailty, baseline hazard function, and frailty distribution. Models compared\ninclude standard parametric distributions and more flexible spline-based\napproaches; we also included semiparametric Cox models. The resulting bias can\nbe clinically relevant. In conclusion, we highlight the importance of fitting\nmodels that are flexible enough and the importance of assessing model fit. We\nillustrate our conclusions with two applications using data on diabetic\nretinopathy and bladder cancer. Our results show the importance of assessing\nmodel fit with respect to the baseline hazard function and the distribution of\nthe frailty: misspecifying the former leads to biased relative and absolute\nrisk estimates while misspecifying the latter affects absolute risk estimates\nand measures of heterogeneity.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 16:33:13 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 16:04:40 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 12:43:22 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Gasparini", "Alessandro", ""], ["Clements", "Mark S.", ""], ["Abrams", "Keith R.", ""], ["Crowther", "Michael J.", ""]]}, {"id": "1810.08240", "submitter": "Steven Howard", "authors": "Steven R. Howard, Aaditya Ramdas, Jon McAuliffe, Jasjeet Sekhon", "title": "Time-uniform, nonparametric, nonasymptotic confidence sequences", "comments": "48 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A confidence sequence is a sequence of confidence intervals that is uniformly\nvalid over an unbounded time horizon. Our work develops confidence sequences\nwhose widths go to zero, with nonasymptotic coverage guarantees under\nnonparametric conditions. We draw connections between the Cram\\'er-Chernoff\nmethod for exponential concentration, the law of the iterated logarithm (LIL),\nand the sequential probability ratio test---our confidence sequences are\ntime-uniform extensions of the first; provide tight, nonasymptotic\ncharacterizations of the second; and generalize the third to nonparametric\nsettings, including sub-Gaussian and Bernstein conditions, self-normalized\nprocesses, and matrix martingales. We illustrate the generality of our proof\ntechniques by deriving an empirical-Bernstein bound growing at a LIL rate, as\nwell as a novel upper LIL for the maximum eigenvalue of a sum of random\nmatrices. Finally, we apply our methods to covariance matrix estimation and to\nestimation of sample average treatment effect under the Neyman-Rubin potential\noutcomes model.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 19:05:00 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 01:50:08 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2019 23:29:35 GMT"}, {"version": "v4", "created": "Tue, 17 Sep 2019 22:05:53 GMT"}, {"version": "v5", "created": "Sat, 30 May 2020 04:24:49 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Howard", "Steven R.", ""], ["Ramdas", "Aaditya", ""], ["McAuliffe", "Jon", ""], ["Sekhon", "Jasjeet", ""]]}, {"id": "1810.08259", "submitter": "Vishesh Karwa", "authors": "Vishesh Karwa and Edoardo M. Airoldi", "title": "A systematic investigation of classical causal inference strategies\n  under mis-specification due to network interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We systematically investigate issues due to mis-specification that arise in\nestimating causal effects when (treatment) interference is informed by a\nnetwork available pre-intervention, i.e., in situations where the outcome of a\nunit may depend on the treatment assigned to other units. We develop theory for\nseveral forms of interference through the concept of exposure neighborhood, and\ndevelop the corresponding semi-parametric representation for potential outcomes\nas a function of the exposure neighborhood. Using this representation, we\nextend the definition of two popular classes of causal estimands, marginal and\naverage causal effects, to the case of network interference. We characterize\nthe bias and variance one incurs when combining classical randomization\nstrategies (namely, Bernoulli, Completely Randomized, and Cluster Randomized\ndesigns) and estimators (namely, difference-in-means and Horvitz-Thompson) used\nto estimate average treatment effect and on the total treatment effect, under\nmisspecification due to interference. We illustrate how difference-in-means\nestimators can have arbitrarily large bias when estimating average causal\neffects, depending on the form and strength of interference, which is unknown\nat design stage. Horvitz-Thompson (HT) estimators are unbiased when the correct\nweights are specified. Here, we derive the HT weights for unbiased estimation\nof different estimands, and illustrate how they depend on the design, the form\nof interference, which is unknown at design stage, and the estimand. More\nimportantly, we show that HT estimators are in-admissible for a large class of\nrandomization strategies, in the presence of interference. We develop new\nmodel-assisted and model-dependent strategies to improve HT estimators, and we\ndevelop new randomization strategies for estimating the average treatment\neffect and total treatment effect.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 19:42:27 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Karwa", "Vishesh", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1810.08264", "submitter": "Yichen Zhang", "authors": "Xi Chen, Weidong Liu, Yichen Zhang", "title": "Quantile Regression Under Memory Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the inference problem in quantile regression (QR) for a\nlarge sample size $n$ but under a limited memory constraint, where the memory\ncan only store a small batch of data of size $m$. A natural method is the\nna\\\"ive divide-and-conquer approach, which splits data into batches of size\n$m$, computes the local QR estimator for each batch, and then aggregates the\nestimators via averaging. However, this method only works when $n=o(m^2)$ and\nis computationally expensive. This paper proposes a computationally efficient\nmethod, which only requires an initial QR estimator on a small batch of data\nand then successively refines the estimator via multiple rounds of\naggregations. Theoretically, as long as $n$ grows polynomially in $m$, we\nestablish the asymptotic normality for the obtained estimator and show that our\nestimator with only a few rounds of aggregations achieves the same efficiency\nas the QR estimator computed on all the data. Moreover, our result allows the\ncase that the dimensionality $p$ goes to infinity. The proposed method can also\nbe applied to address the QR problem under distributed computing environment\n(e.g., in a large-scale sensor network) or for real-time streaming data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 20:03:51 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Chen", "Xi", ""], ["Liu", "Weidong", ""], ["Zhang", "Yichen", ""]]}, {"id": "1810.08285", "submitter": "Helton Saulo", "authors": "Helton Saulo and Roberto Vila", "title": "Log-symmetric regression models for correlated errors with an\n  application to mortality data", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-symmetric regression models are particularly useful when the response\nvariable is continuous, strictly positive and asymmetric. In this paper, we\nproposed a class of log-symmetric regression models in the context of\ncorrelated errors. The proposed models provide a novel alternative to the\nexisting log-symmetric regression models due to its flexibility in\naccommodating correlation. We discuss some properties, parameter estimation by\nthe conditional maximum likelihood method and goodness of fit of the proposed\nmodel. We also provide expressions for the observed Fisher information matrix.\nA Monte Carlo simulation study is presented to evaluate the performance of the\nconditional maximum likelihood estimators. Finally, a full analysis of a\nreal-world mortality data set is presented to illustrate the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 21:59:38 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Saulo", "Helton", ""], ["Vila", "Roberto", ""]]}, {"id": "1810.08292", "submitter": "Anne van Delft Dr.", "authors": "Anne van Delft and Holger Dette", "title": "A similarity measure for second order properties of non-stationary\n  functional time series with applications to clustering and testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the surge of data storage techniques, the need for the development of\nappropriate techniques to identify patterns and to extract knowledge from the\nresulting enormous data sets, which can be viewed as collections of dependent\nfunctional data, is of increasing interest in many scientific areas. We develop\na similarity measure for spectral density operators of a collection of\nfunctional time series, which is based on the aggregation of Hilbert-Schmidt\ndifferences of the individual time-varying spectral density operators. Under\nfairly general conditions, the asymptotic properties of the corresponding\nestimator are derived and asymptotic normality is established. The introduced\nstatistic lends itself naturally to quantify (dis)-similarity between\nfunctional time series, which we subsequently exploit in order to build a\nspectral clustering algorithm. Our algorithm is the first of its kind in the\nanalysis of non-stationary (functional) time series and enables to discover\nparticular patterns by grouping together `similar' series into clusters,\nthereby reducing the complexity of the analysis considerably. The algorithm is\nsimple to implement and computationally feasible. As a further application we\nprovide a simple test for the hypothesis that the second order properties of\ntwo non-stationary functional time series coincide.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 22:23:49 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 16:58:44 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["van Delft", "Anne", ""], ["Dette", "Holger", ""]]}, {"id": "1810.08316", "submitter": "Anru R. Zhang", "authors": "Anru R. Zhang and T. Tony Cai and Yihong Wu", "title": "Heteroskedastic PCA: Algorithm, Optimality, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A general framework for principal component analysis (PCA) in the presence of\nheteroskedastic noise is introduced. We propose an algorithm called HeteroPCA,\nwhich involves iteratively imputing the diagonal entries of the sample\ncovariance matrix to remove estimation bias due to heteroskedasticity. This\nprocedure is computationally efficient and provably optimal under the\ngeneralized spiked covariance model. A key technical step is a deterministic\nrobust perturbation analysis on singular subspaces, which can be of independent\ninterest. The effectiveness of the proposed algorithm is demonstrated in a\nsuite of problems in high-dimensional statistics, including singular value\ndecomposition (SVD) under heteroskedastic noise, Poisson PCA, and SVD for\nheteroskedastic and incomplete data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 00:22:25 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 01:29:04 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 14:00:56 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Anru R.", ""], ["Cai", "T. Tony", ""], ["Wu", "Yihong", ""]]}, {"id": "1810.08361", "submitter": "Fang Liu", "authors": "Yinan Li, Xiao Liu, Fang Liu", "title": "AdaPtive Noisy Data Augmentation (PANDA) for Simultaneous Construction\n  of Multiple Graph Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the data augmentation technique PANDA by Li et al. (2018) that\nregularizes single graph estimation to jointly learning multiple graphical\nmodels with various node types in a unified framework. We design two types of\nnoise to augment the observed data: the first type regularizes the estimation\nof each graph while the second type promotes either the structural similarity,\nreferred as the \\joint group lasso regularization, or the numerical similarity,\nreferred as the joint fused ridge regularization, among the edges in the same\nposition across graphs. The computation in PANDA is straightforward and only\ninvolves obtaining maximum likelihood estimator in generalized linear models in\nan iterative manner. The simulation studies demonstrate PANDA is non-inferior\nto existing joint estimation approaches for Gaussian graphical models, and\nsignificantly improves over the naive differencing approach for non-Gaussian\ngraphical models. We apply PANDA to a real-life lung cancer microarray data to\nsimultaneously construct four protein networks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 06:05:30 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 23:39:32 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Li", "Yinan", ""], ["Liu", "Xiao", ""], ["Liu", "Fang", ""]]}, {"id": "1810.08389", "submitter": "Adam Kapelner", "authors": "Adam Kapelner, Abba M. Krieger, Uri Shalit and David Azriel", "title": "Harmonizing Fully Optimal Designs with Classic Randomization in Fixed\n  Trial Experiments", "comments": "19 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a movement in design of experiments away from the classic\nrandomization put forward by Fisher, Cochran and others to one based on\noptimization. In fixed-sample trials comparing two groups, measurements of\nsubjects are known in advance and subjects can be divided optimally into two\ngroups based on a criterion of homogeneity or \"imbalance\" between the two\ngroups. These designs are far from random. This paper seeks to understand the\nbenefits and the costs over classic randomization in the context of different\nperformance criterions such as Efron's worst-case analysis. In the criterion\nthat we motivate, randomization beats optimization. However, the optimal design\nis shown to lie between these two extremes. Much-needed further work will\nprovide a procedure to find this optimal designs in different scenarios in\npractice. Until then, it is best to randomize.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 08:14:36 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Kapelner", "Adam", ""], ["Krieger", "Abba M.", ""], ["Shalit", "Uri", ""], ["Azriel", "David", ""]]}, {"id": "1810.08557", "submitter": "Emil Constantinescu", "authors": "Emil M. Constantinescu, Noemi Petra, Julie Bessac, Cosmin G. Petra", "title": "Statistical Treatment of Inverse Problems Constrained by Differential\n  Equations-Based Models with Stochastic Terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a statistical treatment of inverse problems constrained\nby models with stochastic terms. The solution of the forward problem is given\nby a distribution represented numerically by an ensemble of simulations. The\ngoal is to formulate the inverse problem, in particular the objective function,\nto find the closest forward distribution (i.e., the output of the stochastic\nforward problem) that best explains the distribution of the observations in a\ncertain metric. We use proper scoring rules, a concept employed in statistical\nforecast verification, namely energy, variogram, and hybrid (i.e., combination\nof the two) scores. We study the performance of the proposed formulation in the\ncontext of two applications: a coefficient field inversion for subsurface flow\ngoverned by an elliptic partial differential equation (PDE) with a stochastic\nsource and a parameter inversion for power grid governed by\ndifferential-algebraic equations (DAEs). In both cases we show that the\nvariogram and the hybrid scores show better parameter inversion results than\ndoes the energy score, whereas the energy score leads to better probabilistic\npredictions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 16:28:46 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 00:47:42 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Constantinescu", "Emil M.", ""], ["Petra", "Noemi", ""], ["Bessac", "Julie", ""], ["Petra", "Cosmin G.", ""]]}, {"id": "1810.08564", "submitter": "Mingyuan Zhou", "authors": "Quan Zhang and Mingyuan Zhou", "title": "Nonparametric Bayesian Lomax delegate racing for survival analysis with\n  competing risks", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Lomax delegate racing (LDR) to explicitly model the mechanism of\nsurvival under competing risks and to interpret how the covariates accelerate\nor decelerate the time to event. LDR explains non-monotonic covariate effects\nby racing a potentially infinite number of sub-risks, and consequently relaxes\nthe ubiquitous proportional-hazards assumption which may be too restrictive.\nMoreover, LDR is naturally able to model not only censoring, but also missing\nevent times or event types. For inference, we develop a Gibbs sampler under\ndata augmentation for moderately sized data, along with a stochastic gradient\ndescent maximum a posteriori inference algorithm for big data applications.\nIllustrative experiments are provided on both synthetic and real datasets, and\ncomparison with various benchmark algorithms for survival analysis with\ncompeting risks demonstrates distinguished performance of LDR.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 15:57:22 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 00:48:10 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Zhang", "Quan", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1810.08595", "submitter": "Armeen Taeb", "authors": "Armeen Taeb, Parikshit Shah, Venkat Chandrasekaran", "title": "False Discovery and Its Control in Low Rank Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models specified by low-rank matrices are ubiquitous in contemporary\napplications. In many of these problem domains, the row/column space structure\nof a low-rank matrix carries information about some underlying phenomenon, and\nit is of interest in inferential settings to evaluate the extent to which the\nrow/column spaces of an estimated low-rank matrix signify discoveries about the\nphenomenon. However, in contrast to variable selection, we lack a formal\nframework to assess true/false discoveries in low-rank estimation; in\nparticular, the key source of difficulty is that the standard notion of a\ndiscovery is a discrete one that is ill-suited to the smooth structure\nunderlying low-rank matrices. We address this challenge via a geometric\nreformulation of the concept of a discovery, which then enables a natural\ndefinition in the low-rank case. We describe and analyze a generalization of\nthe Stability Selection method of Meinshausen and B\\\"uhlmann to control for\nfalse discoveries in low-rank estimation, and we demonstrate its utility\ncompared to previous approaches via numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 17:25:59 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 16:13:11 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 12:37:33 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Taeb", "Armeen", ""], ["Shah", "Parikshit", ""], ["Chandrasekaran", "Venkat", ""]]}, {"id": "1810.08635", "submitter": "Jacqueline Hughes-Oliver", "authors": "Jacqueline M. Hughes-Oliver", "title": "Population and Empirical PR Curves for Assessment of Ranking Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ROC curve is widely used to assess the quality of\nprediction/classification/ranking algorithms, and its properties have been\nextensively studied. The precision-recall (PR) curve has become the de facto\nreplacement for the ROC curve in the presence of imbalance, namely where one\nclass is far more likely than the other class. While the PR and ROC curves tend\nto be used interchangeably, they have some very different properties.\nProperties of the PR curve are the focus of this paper. We consider: (1)\npopulation PR curves, where complete distributional assumptions are specified\nfor scores from both classes; and (2) empirical estimators of the PR curve,\nwhere we observe scores and no distributional assumptions are made. The\nproperties have direct consequence on how the PR curve should, and should not,\nbe used. For example, the empirical PR curve is not consistent when scores in\nthe class of primary interest come from discrete distributions. On the other\nhand, a normal approximation can fit quite well for points on the empirical PR\ncurve from continuously-defined scores, but convergence can be heavily\ninfluenced by the distributional setting, the amount of imbalance, and the\npoint of interest on the PR curve.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 18:29:27 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Hughes-Oliver", "Jacqueline M.", ""]]}, {"id": "1810.08778", "submitter": "Francesco Bartolucci", "authors": "Francesco Bartolucci, Antonietta Mira, Stefano Peluso", "title": "Marginal models with individual-specific effects for the analysis of\n  longitudinal bipartite networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new modeling framework for bipartite social networks arising from a\nsequence of partially time-ordered relational events is proposed. We directly\nmodel the joint distribution of the binary variables indicating if each single\nactor is involved or not in an event. The adopted parametrization is based on\nfirst- and second-order effects, formulated as in marginal models for\ncategorical data and free higher order effects. In particular, second-order\neffects are log-odds ratios with meaningful interpretation from the social\nperspective in terms of tendency to cooperate, in contrast to first-order\neffects interpreted in terms of tendency of each single actor to participate in\nan event. These effects are parametrized on the basis of the event times, so\nthat suitable latent trajectories of individual behaviors may be represented.\nInference is based on a composite likelihood function, maximized by an\nalgorithm with numerical complexity proportional to the square of the number of\nunits in the network. A classification composite likelihood is used to cluster\nthe actors, simplifying the interpretation of the data structure. The proposed\napproach is illustrated on a dataset of scientific articles published in four\ntop statistical journals from 2003 to 2012.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 09:48:09 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Bartolucci", "Francesco", ""], ["Mira", "Antonietta", ""], ["Peluso", "Stefano", ""]]}, {"id": "1810.08880", "submitter": "Yong He", "authors": "Mingjuan Zhang and Yong He and Cheng Zhou and Xinsheng Zhang", "title": "High-dimensional Two-sample Precision Matrices Test: An Adaptive\n  Approach through Multiplier Bootstrap", "comments": "30 pages 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision matrix, which is the inverse of covariance matrix, plays an\nimportant role in statistics, as it captures the partial correlation between\nvariables. Testing the equality of two precision matrices in high dimensional\nsetting is a very challenging but meaningful problem, especially in the\ndifferential network modelling. To our best knowledge, existing test is only\npowerful for sparse alternative patterns where two precision matrices differ in\na small number of elements. In this paper we propose a data-adaptive test which\nis powerful against either dense or sparse alternatives. Multiplier bootstrap\napproach is utilized to approximate the limiting distribution of the test\nstatistic. Theoretical properties including asymptotic size and power of the\ntest are investigated. Simulation study verifies that the data-adaptive test\nperforms well under various alternative scenarios. The practical usefulness of\nthe test is illustrated by applying it to a gene expression data set associated\nwith lung cancer.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 02:31:09 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Zhang", "Mingjuan", ""], ["He", "Yong", ""], ["Zhou", "Cheng", ""], ["Zhang", "Xinsheng", ""]]}, {"id": "1810.08918", "submitter": "Antonio Punzo", "authors": "Antonio Punzo and Cristina Tortora", "title": "Multiple Scaled Contaminated Normal Distribution and Its Application in\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate contaminated normal (MCN) distribution represents a simple\nheavy-tailed generalization of the multivariate normal (MN) distribution to\nmodel elliptical contoured scatters in the presence of mild outliers, referred\nto as \"bad\" points. The MCN can also automatically detect bad points. The price\nof these advantages is two additional parameters, both with specific and useful\ninterpretations: proportion of good observations and degree of contamination.\nHowever, points may be bad in some dimensions but good in others. The use of an\noverall proportion of good observations and of an overall degree of\ncontamination is limiting. To overcome this limitation, we propose a multiple\nscaled contaminated normal (MSCN) distribution with a proportion of good\nobservations and a degree of contamination for each dimension. Once the model\nis fitted, each observation has a posterior probability of being good with\nrespect to each dimension. Thanks to this probability, we have a method for\nsimultaneous directional robust estimation of the parameters of the MN\ndistribution based on down-weighting and for the automatic directional\ndetection of bad points by means of maximum a posteriori probabilities. The\nterm \"directional\" is added to specify that the method works separately for\neach dimension. Mixtures of MSCN distributions are also proposed as an\napplication of the proposed model for robust clustering. An extension of the EM\nalgorithm is used for parameter estimation based on the maximum likelihood\napproach. Real and simulated data are used to show the usefulness of our\nmixture with respect to well-established mixtures of symmetric distributions\nwith heavy tails.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 09:37:44 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Punzo", "Antonio", ""], ["Tortora", "Cristina", ""]]}, {"id": "1810.09004", "submitter": "Pallavi Ray", "authors": "Pallavi Ray and Anirban Bhattacharya", "title": "Signal Adaptive Variable Selector for the Horseshoe Prior", "comments": "21 pages (including appendix and references), 11 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a simple method to perform variable selection as\na post model-fitting exercise using continuous shrinkage priors such as the\npopular horseshoe prior. The proposed Signal Adaptive Variable Selector (SAVS)\napproach post-processes a point estimate such as the posterior mean to group\nthe variables into signals and nulls. The approach is completely automated and\ndoes not require specification of any tuning parameters. We carried out a\ncomprehensive simulation study to compare the performance of the proposed SAVS\napproach to frequentist penalization procedures and Bayesian model selection\nprocedures. SAVS was found to be highly competitive across all the settings\nconsidered, and was particularly found to be robust to correlated designs. We\nalso applied SAVS to a genomic dataset with more than 20,000 covariates to\nillustrate its scalability.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 18:36:37 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Ray", "Pallavi", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "1810.09022", "submitter": "Ted Westling", "authors": "Ted Westling, Mark van der Laan, and Marco Carone", "title": "Correcting an estimator of a multivariate monotone function with\n  isotonic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many problems, a sensible estimator of a possibly multivariate monotone\nfunction may itself fail to be monotone. We study the correction of such an\nestimator obtained via projection onto the space of functions monotone over a\nfinite grid in the domain. We demonstrate that this corrected estimator has no\nworse supremal estimation error than the initial estimator, and that\nanalogously corrected confidence bands contain the true function whenever the\ninitial bands do, at no loss to average or maximal band width. Additionally, we\ndemonstrate that the corrected estimator is uniformly asymptotically equivalent\nto the initial estimator provided that the initial estimator satisfies a\nstochastic equicontinuity condition and that the true function is Lipschitz and\nstrictly monotone. We provide simple sufficient conditions for our stochastic\nequicontinuity condition in the important special case that the initial\nestimator is uniformly asymptotically linear, and illustrate the use of these\nresults for estimation of a G-computed distribution function. Our stochastic\nequicontinuity condition is weaker than standard uniform stochastic\nequicontinuity, which has been required for alternative correction procedures.\nCrucially, this allows us to apply our results to the bivariate correction of\nthe local linear estimator of a conditional distribution function known to be\nmonotone in its conditioning argument. Our experiments suggest that the\nprojection step can yield significant practical improvements in performance for\nboth the estimator and confidence band.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 20:36:05 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 01:25:25 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Westling", "Ted", ""], ["van der Laan", "Mark", ""], ["Carone", "Marco", ""]]}, {"id": "1810.09116", "submitter": "Zicheng Liu", "authors": "Z.Liu, D. Lesselier, B. Sudret and J. Wiart", "title": "Surrogate modeling based on resampled polynomial chaos expansions", "comments": null, "journal-ref": null, "doi": "10.1016/j.ress.2020.107008", "report-no": "RSUQ-2018-006", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In surrogate modeling, polynomial chaos expansion (PCE) is popularly utilized\nto represent the random model responses, which are computationally expensive\nand usually obtained by deterministic numerical modeling approaches including\nfinite element and finite-difference time-domain methods. Recently, efforts\nhave been made on improving the prediction performance of the PCE-based model\nand building efficiency by only selecting the influential basis polynomials\n(e.g., via the approach of least angle regression). This paper proposes an\napproach, named as resampled PCE (rPCE), to further optimize the selection by\nmaking use of the knowledge that the true model is fixed despite the\nstatistical uncertainty inherent to sampling in the training. By simulating\ndata variation via resampling ($k$-fold division utilized here) and collecting\nthe selected polynomials with respect to all resamples, polynomials are ranked\nmainly according to the selection frequency. The resampling scheme (the value\nof $k$ here) matters much and various configurations are considered and\ncompared. The proposed resampled PCE is implemented with two popular selection\ntechniques, namely least angle regression and orthogonal matching pursuit, and\na combination thereof. The performance of the proposed algorithm is\ndemonstrated on two analytical examples, a benchmark problem in structural\nmechanics, as well as a realistic case study in computational dosimetry.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 07:14:09 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 17:05:09 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Liu", "Z.", ""], ["Lesselier", "D.", ""], ["Sudret", "B.", ""], ["Wiart", "J.", ""]]}, {"id": "1810.09214", "submitter": "Amadou Barry", "authors": "Amadou Barry, Karim Oualkacha and Arthur Charpentier", "title": "A new GEE method to account for heteroscedasticity, using asymmetric\n  least-square regressions", "comments": "40 pages, 14 figures and all section modified", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized estimating equations (GEE) are widely used to analyze\nlongitudinal data; however, they are not appropriate for heteroscedastic data,\nbecause they only estimate regressor effects on the mean\nresponse{\\textemdash}and therefore do not account for data heterogeneity. Here,\nwe combine the GEE with the asymmetric least squares (expectile) regression to\nderive a new class of estimators, which we call generalized expectile\nestimating equations (GEEE). The GEEE model estimates regressor effects on the\nexpectiles of the response distribution, which provides a detailed view of\nregressor effects on the entire response distribution. In addition to capturing\ndata heteroscedasticity, the GEEE extends the various working correlation\nstructures to account for within-subject dependence. We derive the asymptotic\nproperties of the GEEE estimators and propose a robust estimator of its\ncovariance matrix for inference (see our R package,\ngithub.com/AmBarry/expectgee). Our simulations show that the GEEE estimator is\nnon-biased and efficient, and our real data analysis shows it captures\nheteroscedasticity.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 12:36:58 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 20:27:05 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Barry", "Amadou", ""], ["Oualkacha", "Karim", ""], ["Charpentier", "Arthur", ""]]}, {"id": "1810.09298", "submitter": "Ekaterina Krymova", "authors": "Denis Belomestny and Ekaterina Krymova", "title": "Sparse constrained projection approximation subspace tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the well-known constrained projection approximation\nsubspace tracking algorithm (CPAST) and derive, for the first time,\nnon-asymptotic error bounds. Furthermore, we introduce a novel sparse\nmodification of CPAST which is able to exploit sparsity in the underlying\ncovariance structure. We present a non-asymptotic analysis of the proposed\nalgorithm and study its empirical performance on simulated and real data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 14:01:34 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 19:45:17 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Belomestny", "Denis", ""], ["Krymova", "Ekaterina", ""]]}, {"id": "1810.09447", "submitter": "Babak Barazandeh", "authors": "Babak Barazandeh, Mohammadhussein Rafieisakhaei, Sunwook Kim, Zhenyu\n  (James) Kong, Maury A. Nussbaum", "title": "A Method for Robust Online Classification using Dictionary Learning:\n  Development and Assessment for Monitoring Manual Material Handling Activities\n  Using Wearable Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification methods based on sparse estimation have drawn much attention\nrecently, due to their effectiveness in processing high-dimensional data such\nas images. In this paper, a method to improve the performance of a sparse\nrepresentation classification (SRC) approach is proposed; it is then applied to\nthe problem of online process monitoring of human workers, specifically manual\nmaterial handling (MMH) operations monitored using wearable sensors (involving\n111 sensor channels). Our proposed method optimizes the design matrix (aka\ndictionary) in the linear model used for SRC, minimizing its ill-posedness to\nachieve a sparse solution. This procedure is based on the idea of dictionary\nlearning (DL): we optimize the design matrix formed by training datasets to\nminimize both redundancy and coherency as well as reducing the size of these\ndatasets. Use of such optimized training data can subsequently improve\nclassification accuracy and help decrease the computational time needed for the\nSRC; it is thus more applicable for online process monitoring. Performance of\nthe proposed methodology is demonstrated using wearable sensor data obtained\nfrom manual material handling experiments, and is found to be superior to those\nof benchmark methods in terms of accuracy, while also requiring computational\ntime appropriate for MMH online monitoring.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 03:13:48 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Barazandeh", "Babak", "", "James"], ["Rafieisakhaei", "Mohammadhussein", "", "James"], ["Kim", "Sunwook", "", "James"], ["Zhenyu", "", "", "James"], ["Kong", "", ""], ["Nussbaum", "Maury A.", ""]]}, {"id": "1810.09494", "submitter": "Arman Oganisian", "authors": "Arman Oganisian, Nandita Mitra, Jason Roy", "title": "A Bayesian Nonparametric Model for Zero-Inflated Outcomes: Prediction,\n  Clustering, and Causal Estimation", "comments": null, "journal-ref": null, "doi": "10.1111/biom.13244", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers are often interested in predicting outcomes, conducting\nclustering analysis to detect distinct subgroups of their data, or computing\ncausal treatment effects. Pathological data distributions that exhibit skewness\nand zero-inflation complicate these tasks - requiring highly flexible,\ndata-adaptive modeling. In this paper, we present a fully nonparametric\nBayesian generative model for continuous, zero-inflated outcomes that\nsimultaneously predicts structural zeros, captures skewness, and clusters\npatients with similar joint data distributions. The flexibility of our approach\nyields predictions that capture the joint data distribution better than\ncommonly used zero-inflated methods. Moreover, we demonstrate that our model\ncan be coherently incorporated into a standardization procedure for computing\ncausal effect estimates that are robust to such data pathologies. Uncertainty\nat all levels of this model flow through to the causal effect estimates of\ninterest - allowing easy point estimation, interval estimation, and posterior\npredictive checks verifying positivity, a required causal identification\nassumption. Our simulation results show point estimates to have low bias and\ninterval estimates to have close to nominal coverage under complicated data\nsettings. Under simpler settings, these results hold while incurring lower\nefficiency loss than comparator methods. Lastly, we use our proposed method to\nanalyze zero-inflated inpatient medical costs among endometrial cancer patients\nreceiving either chemotherapy and radiation therapy in the SEER medicare\ndatabase.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 18:35:48 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 00:57:58 GMT"}, {"version": "v3", "created": "Sat, 9 Mar 2019 14:11:57 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Oganisian", "Arman", ""], ["Mitra", "Nandita", ""], ["Roy", "Jason", ""]]}, {"id": "1810.09497", "submitter": "Ali Akbar Jafari", "authors": "Ali Akbar Jafari", "title": "Comparing Two Approaches in Heteroscedastic Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a generalized test approach is proposed by Sadooghi-alvandi et al.\n(2016) and a fiducial approach is proposed by Xu and Li (2018) to test the\nequality of coefficients in several regression models with unequal variances.\nIn this paper, it is shown that the considered quantities in these approaches\nare identically distributed and therefore, these approaches are same. Also,\nthis result satisfies for the one-way ANOVA problem.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 18:39:21 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Jafari", "Ali Akbar", ""]]}, {"id": "1810.09498", "submitter": "Yi Yu", "authors": "Daren Wang, Yi Yu and Alessandro Rinaldo", "title": "Univariate Mean Change Point Detection: Penalization, CUSUM and\n  Optimality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of univariate mean change point detection and localization based\non a sequence of $n$ independent observations with piecewise constant means has\nbeen intensively studied for more than half century, and serves as a blueprint\nfor change point problems in more complex settings. We provide a complete\ncharacterization of this classical problem in a general framework in which the\nupper bound $\\sigma^2$ on the noise variance, the minimal spacing $\\Delta$\nbetween two consecutive change points and the minimal magnitude $\\kappa$ of the\nchanges, are allowed to vary with $n$. We first show that consistent\nlocalization of the change points, when the signal-to-noise ratio $\\frac{\\kappa\n\\sqrt{\\Delta}}{\\sigma} < \\sqrt{\\log(n)}$, is impossible. In contrast, when\n$\\frac{\\kappa \\sqrt{\\Delta}}{\\sigma}$ diverges with $n$ at the rate of at least\n$\\sqrt{\\log(n)}$, we demonstrate that two computationally-efficient change\npoint estimators, one based on the solution to an $\\ell_0$-penalized least\nsquares problem and the other on the popular wild binary segmentation\nalgorithm, are both consistent and achieve a localization rate of the order\n$\\frac{\\sigma^2}{\\kappa^2} \\log(n)$. We further show that such rate is minimax\noptimal, up to a $\\log(n)$ term.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 18:42:05 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 08:51:16 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 15:06:27 GMT"}, {"version": "v4", "created": "Thu, 6 Jun 2019 08:47:03 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Wang", "Daren", ""], ["Yu", "Yi", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1810.09750", "submitter": "Roberta Paroli", "authors": "Roberta Paroli, Guido Consonni", "title": "Objective Bayesian Comparison of Order-Constrained Models in Contingency\n  Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In social and biomedical sciences testing in contingency tables often\ninvolves order restrictions on cell-probabilities parameters. We develop\nobjective Bayes methods for order-constrained testing and model comparison when\nobservations arise under product binomial or multinomial sampling.\nSpecifically, we consider tests for monotone order of the parameters against\nequality of all parameters. Our strategy combines in a unified way both the\nintrinsic prior methodology and the encompassing prior approach in order to\ncompute Bayes factors and posterior model probabilities. Performance of our\nmethod is evaluated on several simulation studies and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 09:58:45 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Paroli", "Roberta", ""], ["Consonni", "Guido", ""]]}, {"id": "1810.09894", "submitter": "Alejandra Avalos-Pacheco", "authors": "Alejandra Avalos-Pacheco, David Rossell and Richard S. Savage", "title": "Heterogeneous large datasets integration using Bayesian factor\n  regression", "comments": "Main manuscript: 34 pages. Supplementary material: 12 pages. Typos\n  corrected, link to the R code added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two key challenges in modern statistical applications are the large amount of\ninformation recorded per individual, and that such data are often not collected\nall at once but in batches. These batch effects can be complex, causing\ndistortions in both mean and variance. We propose a novel sparse latent factor\nregression model to integrate such heterogeneous data. The model provides a\ntool for data exploration via dimensionality reduction while correcting for a\nrange of batch effects. We study the use of several sparse priors (local and\nnon-local) to learn the dimension of the latent factors. Our model is fitted in\na deterministic fashion by means of an EM algorithm for which we derive\nclosed-form updates, contributing a novel scalable algorithm for non-local\npriors of interest beyond the immediate scope of this paper. We present several\nexamples, with a focus on bioinformatics applications. Our results show an\nincrease in the accuracy of the dimensionality reduction, with non-local priors\nsubstantially improving the reconstruction of factor cardinality, as well as\nthe need to account for batch effects to obtain reliable results. Our model\nprovides a novel approach to latent factor regression that balances sparsity\nwith sensitivity and is highly computationally efficient.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 14:48:23 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 23:37:58 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 17:13:49 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Avalos-Pacheco", "Alejandra", ""], ["Rossell", "David", ""], ["Savage", "Richard S.", ""]]}, {"id": "1810.09899", "submitter": "Traiko Dinev", "authors": "Traiko Dinev and Michael U. Gutmann", "title": "Dynamic Likelihood-free Inference via Ratio Estimation (DIRE)", "comments": "For a demo, see https://traiko.com/pages/research/lfire/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric statistical models that are implicitly defined in terms of a\nstochastic data generating process are used in a wide range of scientific\ndisciplines because they enable accurate modeling. However, learning the\nparameters from observed data is generally very difficult because their\nlikelihood function is typically intractable. Likelihood-free Bayesian\ninference methods have been proposed which include the frameworks of\napproximate Bayesian computation (ABC), synthetic likelihood, and its recent\ngeneralization that performs likelihood-free inference by ratio estimation\n(LFIRE). A major difficulty in all these methods is choosing summary statistics\nthat reduce the dimensionality of the data to facilitate inference. While\nseveral methods for choosing summary statistics have been proposed for ABC, the\nliterature for synthetic likelihood and LFIRE is very thin to date. We here\naddress this gap in the literature, focusing on the important special case of\ntime-series models. We show that convolutional neural networks trained to\npredict the input parameters from the data provide suitable summary statistics\nfor LFIRE. On a wide range of time-series models, a single neural network\narchitecture produced equally or more accurate posteriors than alternative\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 15:02:47 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Dinev", "Traiko", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "1810.09909", "submitter": "Sourabh Bhattacharya", "authors": "Minerva Mukhopadhyay and Sourabh Bhattacharya", "title": "Bayes Factor Asymptotics for Variable Selection in the Gaussian Process\n  Framework", "comments": "A very significantly updated version, with extensive treatment of the\n  \"large p, large n\" paradigm, even when p>>n. Substantial methodological\n  development added with TTMCMC based Bayes factor oriented variable selection,\n  along with ample simulation experiments and a real data analysis in the bona\n  fide \"large p, small n\" premise", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although variable selection is one of the most popular areas of modern\nstatistical research, much of its development has taken place in the classical\nparadigm compared to the Bayesian counterpart. Somewhat surprisingly, both the\nparadigms have focussed almost completely on linear models, in spite of the\nvast scope offered by the model liberation movement brought about by modern\nadvancements in studying real, complex phenomena.\n  In this article, we investigate general Bayesian variable selection in models\ndriven by Gaussian processes, which allows us to treat linear, non-linear and\nnonparametric models, in conjunction with even dependent setups, in the same\nvein. We consider the Bayes factor route to variable selection, and develop a\ngeneral asymptotic theory for the Gaussian process framework in the \"large p,\nlarge n\" settings even with p>>n, establishing almost sure exponential\nconvergence of the Bayes factor under appropriately mild conditions. The fixed\np setup is included as a special case.\n  To illustrate, we apply our general result to variable selection in linear\nregression, Gaussian process model with squared exponential covariance function\naccommodating the covariates, and a first order autoregressive process with\ntime-varying covariates. We also follow up our theoretical investigations with\nample simulation experiments in the above regression contexts and variable\nselection in a real, riboflavin data consisting of 71 observations but 4088\ncovariates. For implementation of variable selection using Bayes factors, we\ndevelop a novel and effective general-purpose transdimensional, transformation\nbased Markov chain Monte Carlo algorithm, which has played a crucial role in\nour simulated and real data applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 15:22:50 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 19:42:22 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 15:55:47 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Mukhopadhyay", "Minerva", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1810.09912", "submitter": "Steven Kleinegesse", "authors": "Steven Kleinegesse and Michael Gutmann", "title": "Efficient Bayesian Experimental Design for Implicit Models", "comments": "Added references and fixed typos. Results and figures remain\n  unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian experimental design involves the optimal allocation of resources in\nan experiment, with the aim of optimising cost and performance. For implicit\nmodels, where the likelihood is intractable but sampling from the model is\npossible, this task is particularly difficult and therefore largely unexplored.\nThis is mainly due to technical difficulties associated with approximating\nposterior distributions and utility functions. We devise a novel experimental\ndesign framework for implicit models that improves upon previous work in two\nways. First, we use the mutual information between parameters and data as the\nutility function, which has previously not been feasible. We achieve this by\nutilising Likelihood-Free Inference by Ratio Estimation (LFIRE) to approximate\nposterior distributions, instead of the traditional approximate Bayesian\ncomputation or synthetic likelihood methods. Secondly, we use Bayesian\noptimisation in order to solve the optimal design problem, as opposed to the\ntypically used grid search or sampling-based methods. We find that this\nincreases efficiency and allows us to consider higher design dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 15:24:29 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 13:48:19 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Kleinegesse", "Steven", ""], ["Gutmann", "Michael", ""]]}, {"id": "1810.09996", "submitter": "Beniamino Hadj-Amar", "authors": "Beniamino Hadj-Amar, B\\\"arbel Finkenst\\\"adt, Mark Fiecas, Francis Levi\n  and Robert Huckstepp", "title": "Bayesian Model Search for Nonstationary Periodic Time Series", "comments": "Received 23 Oct 2018, Accepted 12 May 2019", "journal-ref": "Journal of the American Statistical Association, T&M, 2019", "doi": "10.1080/01621459.2019.1623043", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Bayesian methodology for analyzing nonstationary time\nseries that exhibit oscillatory behaviour. We approximate the time series using\na piecewise oscillatory model with unknown periodicities, where our goal is to\nestimate the change-points while simultaneously identifying the potentially\nchanging periodicities in the data. Our proposed methodology is based on a\ntrans-dimensional Markov chain Monte Carlo (MCMC) algorithm that simultaneously\nupdates the change-points and the periodicities relevant to any segment between\nthem. We show that the proposed methodology successfully identifies time\nchanging oscillatory behaviour in two applications which are relevant to\ne-Health and sleep research, namely the occurrence of ultradian oscillations in\nhuman skin temperature during the time of night rest, and the detection of\ninstances of sleep apnea in plethysmographic respiratory traces.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 17:56:37 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 20:43:45 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 18:09:19 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Hadj-Amar", "Beniamino", ""], ["Finkenst\u00e4dt", "B\u00e4rbel", ""], ["Fiecas", "Mark", ""], ["Levi", "Francis", ""], ["Huckstepp", "Robert", ""]]}, {"id": "1810.10041", "submitter": "Giorgos Bakoyannis", "authors": "Giorgos Bakoyannis, Ying Zhang and Constantin T. Yiannoutsos", "title": "Semiparametric regression and risk prediction with competing risks data\n  under missing cause of failure", "comments": null, "journal-ref": null, "doi": "10.1007/s10985-020-09494-1", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cause of failure in cohort studies that involve competing risks is\nfrequently incompletely observed. To address this, several methods have been\nproposed for the semiparametric proportional cause-specific hazards model under\na missing at random assumption. However, these proposals provide inference for\nthe regression coefficients only, and do not consider the infinite dimensional\nparameters, such as the covariate-specific cumulative incidence function.\nNevertheless, the latter quantity is essential for risk prediction in modern\nmedicine. In this paper we propose a unified framework for inference about both\nthe regression coefficients of the proportional cause-specific hazards model\nand the covariate-specific cumulative incidence functions under missing at\nrandom cause of failure. Our approach is based on a novel computationally\nefficient maximum pseudo-partial-likelihood estimation method for the\nsemiparametric proportional cause-specific hazards model. Using modern\nempirical process theory we derive the asymptotic properties of the proposed\nestimators for the regression coefficients and the covariate-specific\ncumulative incidence functions, and provide methodology for constructing\nsimultaneous confidence bands for the latter. Simulation studies show that our\nestimators perform well even in the presence of a large fraction of missing\ncause of failures, and that the regression coefficient estimator can be\nsubstantially more efficient compared to the previously proposed augmented\ninverse probability weighting estimator. The method is applied using data from\nan HIV cohort study and a bladder cancer clinical trial.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 18:37:10 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 00:37:27 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 20:02:21 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Bakoyannis", "Giorgos", ""], ["Zhang", "Ying", ""], ["Yiannoutsos", "Constantin T.", ""]]}, {"id": "1810.10164", "submitter": "Tyler VanderWeele", "authors": "Tyler J. VanderWeele, Maya B. Mathur, Ying Chen", "title": "Outcome-wide longitudinal designs for causal inference: a new template\n  for empirical studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new template for empirical studies intended to\nassess causal effects: the outcome-wide longitudinal design. The approach is an\nextension of what is often done to assess the causal effects of a treatment or\nexposure using confounding control, but now, over numerous outcomes. We discuss\nthe temporal and confounding control principles for such outcome-wide studies,\nmetrics to evaluate robustness or sensitivity to potential unmeasured\nconfounding for each outcome, and approaches to handle multiple testing. We\nargue that the outcome-wide longitudinal design has numerous advantages over\nmore traditional studies of single exposure-outcome relationships including\nresults that are less subject to investigator bias, greater potential to report\nnull effects, greater capacity to compare effect sizes, a tremendous gain in\nthe efficiency for the research community, a greater policy relevance, and a\nmore rapid advancement of knowledge. We discuss both the practical and\ntheoretical justification for the outcome-wide longitudinal design and also the\npragmatic details of its implementation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 03:20:37 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["VanderWeele", "Tyler J.", ""], ["Mathur", "Maya B.", ""], ["Chen", "Ying", ""]]}, {"id": "1810.10172", "submitter": "Qiang Sun", "authors": "Xiucai Ding and Qiang Sun", "title": "Modified Multidimensional Scaling and High Dimensional Clustering", "comments": "There are critical errors in the proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional scaling is an important dimension reduction tool in\nstatistics and machine learning. Yet few theoretical results characterizing its\nstatistical performance exist, not to mention any in high dimensions. By\nconsidering a unified framework that includes low, moderate and high\ndimensions, we study multidimensional scaling in the setting of clustering\nnoisy data. Our results suggest that, the classical multidimensional scaling\ncan be modified to further improve the quality of embedded samples, especially\nwhen the noise level increases. To this end, we propose {\\it modified\nmultidimensional scaling} which applies a nonlinear transformation to the\nsample eigenvalues. The nonlinear transformation depends on the dimensionality,\nsample size and moment of noise. We show that modified multidimensional scaling\nfollowed by various clustering algorithms can achieve exact recovery, i.e., all\nthe cluster labels can be recovered correctly with probability tending to one.\nNumerical simulations and two real data applications lend strong support to our\nproposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 03:51:26 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 23:53:54 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 03:20:14 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2020 01:03:53 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Ding", "Xiucai", ""], ["Sun", "Qiang", ""]]}, {"id": "1810.10213", "submitter": "Marie-Pierre Etienne", "authors": "Th\\'eo Michelot, Marie-Pierre Etienne (IRMAR, LMA2), Pierre Gloaguen\n  (MIA-Paris)", "title": "The Langevin diffusion as a continuous-time model of animal movement and\n  habitat selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  1. The utilisation distribution describes the relative probability of use of\na spatial unit by an animal. It is natural to think of it as the long-term\nconsequence of the animal's short-term movement decisions: it is the\naccumulation of small displacements which, over time, gives rise to global\npatterns of space use. However, most utilisation distribution models either\nignore the underlying movement, assuming the independenceof observed locations,\nor are based on simplistic Brownian motion movement rules. 2. We introduce a\nnew continuous-time model of animal movement, based on the Langevin diffusion.\nThis stochastic process has an explicit stationary distribution, conceptually\nanalogous to the idea of the utilisation distribution, and thus provides an\nintuitive framework to integrate movement and space use. We model the\nstationary (utilisation) distribution with a resource selection function to\nlink the movement to spatial covariates, and allow inference into habitat\nselection. 3. Standard approximation techniques can be used to derive the\npseudo-likelihood of the Langevin diffusion movement model, and to estimate\nhabitat preference and movement parameters from tracking data. We investigate\nthe performance of the method on simulated data, and discuss its sensitivity to\nthe time scale of the sampling. We present an example of its application to\ntracking data of Stellar sea lions (Eumetopiasjubatus). 4. Due to its\ncontinuous-time formulation, this method can be applied to irregular telemetry\ndata. It provides a rigorous framework to estimate long-term habitat selection\nfrom correlated movement data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 06:53:50 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Michelot", "Th\u00e9o", "", "IRMAR, LMA2"], ["Etienne", "Marie-Pierre", "", "IRMAR, LMA2"], ["Gloaguen", "Pierre", "", "MIA-Paris"]]}, {"id": "1810.10239", "submitter": "Colin Griesbach", "authors": "Colin Griesbach, Andreas Mayr and Elisabeth Waldmann", "title": "Extension of the Gradient Boosting Algorithm for Joint Modeling of\n  Longitudinal and Time-to-Event data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various data situations joint models are an efficient tool to analyze\nrelationships between time dependent covariates and event times or to correct\nfor event-dependent dropout occurring in regression analysis. Joint modeling\nconnects a longitudinal and a survival submodel within a single joint\nlikelihood which then can be maximized by standard optimization methods. Main\nburdens of these conventional methods are that the computational effort\nincreases rapidly in higher dimensions and they do not offer special tools for\nproper variable selection. Gradient boosting techniques are well known among\nstatisticians for addressing exactly these problems, hence an initial boosting\nalgorithm to fit a basic joint model based on functional gradient descent\nmethods has been proposed. Aim of this work is to extend this algorithm in\norder to fit a model incorporating baseline covariates affecting solely the\nsurvival part of the model. The extended algorithm is evaluated based on low\nand high dimensional simulation runs as well as a data set on AIDS patients,\nwhere the longitudinal submodel models the underlying profile of the CD4 cell\ncount which then gets included alongside several baseline covariates in the\nsurvival submodel.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 08:23:01 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Griesbach", "Colin", ""], ["Mayr", "Andreas", ""], ["Waldmann", "Elisabeth", ""]]}, {"id": "1810.10276", "submitter": "Gery Geenens", "authors": "Gery Geenens and Pierre Lafaye de Micheaux", "title": "The Hellinger Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the defining properties of a valid measure of the dependence\nbetween two random variables are reviewed and complemented with two original\nones, shown to be more fundamental than other usual postulates. While other\npopular choices are proved to violate some of these requirements, a class of\ndependence measures satisfying all of them is identified. One particular\nmeasure, that we call the Hellinger correlation, appears as a natural choice\nwithin that class due to both its theoretical and intuitive appeal. A simple\nand efficient nonparametric estimator for that quantity is proposed. Synthetic\nand real-data examples finally illustrate the descriptive ability of the\nmeasure, which can also be used as test statistic for exact independence\ntesting.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 10:03:43 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 05:43:26 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2019 07:22:32 GMT"}, {"version": "v4", "created": "Mon, 2 Dec 2019 03:35:48 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Geenens", "Gery", ""], ["de Micheaux", "Pierre Lafaye", ""]]}, {"id": "1810.10292", "submitter": "Hannah Worthington", "authors": "Hannah Worthington, Rachel McCrea, Ruth King and Richard Griffiths", "title": "Estimating abundance from multiple sampling capture-recapture data via a\n  multi-state multi-period stopover model", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collection of capture-recapture data often involves collecting data on\nnumerous capture occasions over a relatively short period of time. For many\nstudy species this process is repeated, for example annually, resulting in\ncapture information spanning multiple sampling periods. The robust design class\nof models provide a convenient framework in which to analyse all of the\navailable capture data in a single likelihood expression. However, these models\ntypically rely either upon the assumption of closure within a sampling period\n(the closed robust design) or condition on the number of individuals captured\nwithin a sampling period (the open robust design). The models we develop in\nthis paper require neither assumption by explicitly modelling the movement of\nindividuals into the population both within and between the sampling periods,\nwhich in turn permits the estimation of abundance. These models are further\nextended to allow parameters to depend not only on capture occasion but also\nthe amount of time since joining the population and to the case of multi-state\ndata where there is individual time-varying discrete covariate information. We\nderive an efficient likelihood expression for the new multi-state multi-period\nstopover model using the hidden Markov model framework. We demonstrate the new\nmodel through a simulation study before considering a dataset on great crested\nnewts, Triturus cristatus.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 11:06:19 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Worthington", "Hannah", ""], ["McCrea", "Rachel", ""], ["King", "Ruth", ""], ["Griffiths", "Richard", ""]]}, {"id": "1810.10495", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee and Sourabh Bhattacharya", "title": "Posterior Convergence of Gaussian and General Stochastic Process\n  Regression Under Possible Misspecifications", "comments": "An updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate posterior convergence in nonparametric\nregression models where the unknown regression function is modeled by some\nappropriate stochastic process. In this regard, we consider two setups. The\nfirst setup is based on Gaussian processes, where the covariates are either\nrandom or non-random and the noise may be either normally or\ndouble-exponentially distributed. In the second setup, we assume that the\nunderlying regression function is modeled by some reasonably smooth, but\nunspecified stochastic process satisfying reasonable conditions. The\ndistribution of the noise is also left unspecified, but assumed to be\nthick-tailed. As in the previous studies regarding the same problems, we do not\nassume that the truth lies in the postulated parameter space, thus explicitly\nallowing the possibilities of misspecification. We exploit the general results\nof Shalizi (2009) for our purpose and establish not only posterior consistency,\nbut also the rates at which the posterior probabilities converge, which turns\nout to be the Kullback-Leibler divergence rate. We also investigate the more\nfamiliar posterior convergence rates. Interestingly, we show that the posterior\npredictive distribution can accurately approximate the best possible predictive\ndistribution in the sense that the Hellinger distance, as well as the total\nvariation distance between the two distributions can tend to zero, in spite of\nmisspecifications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 16:57:10 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 04:42:41 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1810.10520", "submitter": "John Mashford PhD", "authors": "John Mashford", "title": "Stochastic temporal data upscaling using the generalized k-nearest\n  neighbor algorithm", "comments": "Published in International Journal of Stochastic Analysis", "journal-ref": "International Journal of Stochastic Analysis Volume 2018 Article\n  ID 2487947", "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three methods of temporal data upscaling, which may collectively be called\nthe generalized k-nearest neighbor (GkNN) method, are considered. The accuracy\nof the GkNN simulation of month by month yield is considered (where the term\nyield denotes the dependent variable). The notion of an eventually well\ndistributed time series is introduced and on the basis of this assumption some\nproperties of the average annual yield and its variance for a GkNN simulation\nare computed. The total yield over a planning period is determined and a\ngeneral framework for considering the GkNN algorithm based on the notion of\nstochastically dependent time series is described and it is shown that for a\nsufficiently large training set the GkNN simulation has the same statistical\nproperties as the training data. An example of the application of the\nmethodology is given in the problem of simulating yield of a rainwater tank\ngiven monthly climatic data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 22:29:33 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Mashford", "John", ""]]}, {"id": "1810.10559", "submitter": "Paul-Christian B\\\"urkner", "authors": "Paul-Christian B\\\"urkner, Jonah Gabry, Aki Vehtari", "title": "Efficient leave-one-out cross-validation for Bayesian non-factorized\n  normal and Student-t models", "comments": "18 pages, 3 figures", "journal-ref": "Computational Statistics, 2020", "doi": "10.1007/s00180-020-01045-4", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation can be used to measure a model's predictive accuracy for the\npurpose of model comparison, averaging, or selection. Standard leave-one-out\ncross-validation (LOO-CV) requires that the observation model can be factorized\ninto simple terms, but a lot of important models in temporal and spatial\nstatistics do not have this property or are inefficient or unstable when forced\ninto a factorized form. We derive how to efficiently compute and validate both\nexact and approximate LOO-CV for any Bayesian non-factorized model with a\nmultivariate normal or Student-t distribution on the outcome values. We\ndemonstrate the method using lagged simultaneously autoregressive (SAR) models\nas a case study.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 18:03:36 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 19:57:03 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 09:06:45 GMT"}, {"version": "v4", "created": "Fri, 27 Mar 2020 09:09:07 GMT"}, {"version": "v5", "created": "Thu, 1 Oct 2020 12:59:20 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["B\u00fcrkner", "Paul-Christian", ""], ["Gabry", "Jonah", ""], ["Vehtari", "Aki", ""]]}, {"id": "1810.10572", "submitter": "Abhirup Datta", "authors": "Abhirup Datta, Jacob Fiksel, Agbessi Amouzou, Scott Zeger", "title": "Regularized Bayesian transfer learning for population level etiological\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-coded verbal autopsy (CCVA) algorithms predict cause of death from\nhigh-dimensional family questionnaire data (verbal autopsies) of a deceased\nindividual. CCVA algorithms are typically trained on non-local data, then used\nto generate national and regional estimates of cause-specific mortality\nfractions. These estimates may be inaccurate if the non-local training data is\ndifferent from the local population of interest. This problem is a special case\nof transfer learning. However, most transfer learning classification approaches\nare concerned with individual (e.g. a person's) classification within a target\ndomain (e.g. a particular population) with training performed in data from a\nsource domain. Epidemiologists are often more interested in estimating\npopulation-level etiological distributions, using datasets much smaller than\nthose used in common transfer learning applications. We present a parsimonious\nhierarchical Bayesian transfer learning framework to directly estimate\npopulation-level class probabilities in a target domain. To address small\nsample sizes, we introduce a novel shrinkage prior for the transfer error rates\nguaranteeing that, in absence of any labeled target domain data or when the\nbaseline classifier has zero transfer error, the calibrated estimate of class\nprobabilities coincides with the naive estimates from the baseline classifier,\nthereby subsuming the default practice as a special case. A novel Gibbs sampler\nusing data-augmentation enables fast implementation. We extend our approach to\nuse not one, but an ensemble of baseline classifiers. Theoretical and empirical\nresults demonstrate how the ensemble model favors the most accurate baseline\nclassifier. We present extensions allowing class probabilities to vary with\ncovariates, and an EM-algorithm-based MAP estimation. An R-package implementing\nthis method is developed.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 18:33:03 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 22:22:07 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Datta", "Abhirup", ""], ["Fiksel", "Jacob", ""], ["Amouzou", "Agbessi", ""], ["Zeger", "Scott", ""]]}, {"id": "1810.10598", "submitter": "Walter Dempsey", "authors": "Walter Dempsey", "title": "Exchangeable, Markov multi-state survival process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider exchangeable Markov multi-state survival processes -- temporal\nprocesses taking values over a state-space$\\mathcal{S}$ with at least one\nabsorbing failure state $\\flat \\in \\mathcal{S}$ that satisfy natural invariance\nproperties of exchangeability and consistency under subsampling. The set of\nprocesses contains many well-known examples from health and epidemiology --\nsurvival, illness-death, competing risk, and comorbidity processes; an\nextension leads to recurrent event processes. We characterize exchangeable\nMarkov multi-state survival processes in both discrete and continuous time.\nStatistical considerations impose natural constraints on the space of models\nappropriate for applied work. In particular, we describe constraints arising\nfrom the notion of composable systems. We end with an application of the\ndeveloped models to irregularly sampled and potentially censored multi-state\nsurvival data, developing a Markov chain Monte Carlo algorithm for posterior\ncomputation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 20:07:22 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Dempsey", "Walter", ""]]}, {"id": "1810.10705", "submitter": "Xiaowu Dai", "authors": "Xiaowu Dai and Alzheimer's Disease Neuroimaging Initiative", "title": "Alzheimer's Disease Prediction Using Longitudinal and Heterogeneous\n  Magnetic Resonance Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent evidence has shown that structural magnetic resonance imaging (MRI) is\nan effective tool for Alzheimer's disease (AD) prediction and diagnosis. While\ntraditional MRI-based diagnosis uses images acquired at a single time point, a\nlongitudinal study is more sensitive and accurate in detecting early\npathological changes of the AD. Two main difficulties arise in longitudinal\nMRI-based diagnosis: (1) the inconsistent longitudinal scans among subjects\n(i.e., different scanning time and different total number of scans); (2) the\nheterogeneous progressions of high-dimensional regions of interest (ROIs) in\nMRI. In this work, we propose a novel feature selection and estimation method\nwhich can be applied to extract features from the heterogeneous longitudinal\nMRI. A key ingredient of our method is the combination of smoothing splines and\nthe $l_1$-penalty. We perform experiments on the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) database. The results corroborate the advantages\nof the proposed method for AD prediction in longitudinal studies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 03:25:28 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Dai", "Xiaowu", ""], ["Initiative", "Alzheimer's Disease Neuroimaging", ""]]}, {"id": "1810.10722", "submitter": "Matthias Meller", "authors": "Matthias Meller, Jan Beyersmann, Kaspar Rufibach", "title": "Joint modelling of progression-free and overall survival and computation\n  of correlation measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we derive the joint distribution of progression-free and\noverall survival as a function of transition probabilities in a multistate\nmodel. No assumptions on copulae or latent event times are needed and the model\nis allowed to be non-Markov. From the joint distribution, statistics of\ninterest can then be computed. As an example, we provide closed formulas and\nstatistical inference for Pearson's correlation coefficient between\nprogression-free and overall survival in a parametric framework. The example is\ninspired by recent approaches to quantify the dependence between\nprogression-free survival, a common primary outcome in phase III trials in\noncology, and overall survival. We complement these approaches by providing\nmethods of statistical inference while at the same time working within a much\nmore parsimonious modelling framework. Our approach is completely general and\ncan be applied to other measures of dependence. We also discuss extensions to\nnonparametric inference. Our analytical results are illustrated using a large\nrandomized clinical trial in breast cancer.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 05:25:25 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Meller", "Matthias", ""], ["Beyersmann", "Jan", ""], ["Rufibach", "Kaspar", ""]]}, {"id": "1810.10854", "submitter": "Monica Chiogna", "authors": "Thi Kim Hue Nguyen and Monica Chiogna", "title": "Structure learning of undirected graphical models for count data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological processes underlying the basic functions of a cell involve complex\ninteractions between genes. From a technical point of view, these interactions\ncan be represented through a graph where genes and their connections are,\nrespectively, nodes and edges. The main objective of this paper is to develop a\nstatistical framework for modelling the interactions between genes when the\nactivity of genes is measured on a discrete scale. In detail, we define a new\nalgorithm for learning the structure of undirected graphs, PC-LPGM, proving its\ntheoretical consistence in the limit of infinite observations. The proposed\nalgorithm shows promising results when applied to simulated data as well as to\nreal data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 13:04:27 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 08:49:42 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Nguyen", "Thi Kim Hue", ""], ["Chiogna", "Monica", ""]]}, {"id": "1810.10883", "submitter": "Tim van Erven", "authors": "Tim van Erven and Botond Szabo", "title": "Fast Exact Bayesian Inference for Sparse Signals in the Normal Sequence\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider exact algorithms for Bayesian inference with model selection\npriors (including spike-and-slab priors) in the sparse normal sequence model.\nBecause the best existing exact algorithm becomes numerically unstable for\nsample sizes over n=500, there has been much attention for alternative\napproaches like approximate algorithms (Gibbs sampling, variational Bayes,\netc.), shrinkage priors (e.g. the Horseshoe prior and the Spike-and-Slab LASSO)\nor empirical Bayesian methods. However, by introducing algorithmic ideas from\nonline sequential prediction, we show that exact calculations are feasible for\nmuch larger sample sizes: for general model selection priors we reach n=25000,\nand for certain spike-and-slab priors we can easily reach n=100000. We further\nprove a de Finetti-like result for finite sample sizes that characterizes\nexactly which model selection priors can be expressed as spike-and-slab priors.\nThe computational speed and numerical accuracy of the proposed methods are\ndemonstrated in experiments on simulated data, on a differential gene\nexpression data set, and to compare the effect of multiple hyper-parameter\nsettings in the beta-binomial prior. In our experimental evaluation we compute\nguaranteed bounds on the numerical accuracy of all new algorithms, which shows\nthat the proposed methods are numerically reliable whereas an alternative based\non long division is not.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 13:58:27 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 14:24:30 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["van Erven", "Tim", ""], ["Szabo", "Botond", ""]]}, {"id": "1810.10984", "submitter": "Jemima Tabeart", "authors": "Jemima M. Tabeart, Sarah L. Dance, Amos S. Lawless, Nancy K. Nichols\n  and Joanne A. Waller", "title": "Improving the condition number of estimated covariance matrices", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional error covariance matrices and their inverses are used to\nweight the contribution of observation and background information in data\nassimilation procedures. As observation error covariance matrices are often\nobtained by sampling methods, estimates are often degenerate or\nill-conditioned, making it impossible to invert an observation error covariance\nmatrix without the use of techniques to reduce its condition number. In this\npaper we present new theory for two existing methods that can be used to\n'recondition' any covariance matrix: ridge regression, and the minimum\neigenvalue method. We compare these methods with multiplicative variance\ninflation. We investigate the impact of reconditioning on variances and\ncorrelations of a general covariance matrix in both a theoretical and practical\nsetting. Improved theoretical understanding provides guidance to users\nregarding method selection, and choice of target condition number. The new\ntheory shows that, for the same target condition number, both methods increase\nvariances compared to the original matrix, with larger increases for ridge\nregression than the minimum eigenvalue method. We prove that the ridge\nregression method strictly decreases the absolute value of off-diagonal\ncorrelations. Theoretical comparison of the impact of reconditioning and\nmultiplicative variance inflation on the data assimilation objective function\nshows that variance inflation alters information across all scales uniformly,\nwhereas reconditioning has a larger effect on scales corresponding to smaller\neigenvalues. The minimum eigenvalue method results in smaller overall changes\nto the correlation matrix than ridge regression, but can increase off-diagonal\ncorrelations. Data assimilation experiments reveal that reconditioning corrects\nspurious noise in the analysis but underestimates the true signal compared to\nmultiplicative variance inflation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 17:18:11 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 13:26:31 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 08:45:52 GMT"}, {"version": "v4", "created": "Tue, 1 Oct 2019 11:29:52 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Tabeart", "Jemima M.", ""], ["Dance", "Sarah L.", ""], ["Lawless", "Amos S.", ""], ["Nichols", "Nancy K.", ""], ["Waller", "Joanne A.", ""]]}, {"id": "1810.11010", "submitter": "Hanzhong Liu", "authors": "Ran Chen and Hanzhong Liu", "title": "Heterogeneous Treatment Effect Estimation through Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating heterogeneous treatment effect is an important task in causal\ninference with wide application fields. It has also attracted increasing\nattention from machine learning community in recent years. In this work, we\nreinterpret the heterogeneous treatment effect estimation and propose ways to\nborrow strength from neural networks. We analyze the strengths and drawbacks of\nintegrating neural networks into heterogeneous treatment effect estimation and\nclarify the aspects that need to be taken into consideration when designing a\nspecific network. We proposed a specific network under our guidelines. In\nsimulations, we show that our network performs better when the structure of\ndata is complex, and reach a draw under the cases where other methods could be\nproved to be optimal.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 17:56:57 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Chen", "Ran", ""], ["Liu", "Hanzhong", ""]]}, {"id": "1810.11039", "submitter": "Jorge Ignacio Gonz\\'alez C\\'azares", "authors": "Jorge Ignacio Gonz\\'alez C\\'azares, Aleksandar Mijatovi\\'c, Ger\\'onimo\n  Uribe Bravo", "title": "Geometrically Convergent Simulation of the Extrema of L\\'{e}vy Processes", "comments": "Minor revision: reintroduction of the result on the scaling limits.\n  37 pages and 5 figures. Short presentation on: https://youtu.be/P3vHmJUCFbU", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.CP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel approximate simulation algorithm for the joint law of the\nposition, the running supremum and the time of the supremum of a general L\\'evy\nprocess at an arbitrary finite time. We identify the law of the error in simple\nterms. We prove that the error decays geometrically in $L^p$ (for any $p\\geq\n1$) as a function of the computational cost, in contrast with the polynomial\ndecay for the approximations available in the literature. We establish a\ncentral limit theorem and construct non-asymptotic and asymptotic confidence\nintervals for the corresponding Monte Carlo estimator. We prove that the\nmultilevel Monte Carlo estimator has optimal computational complexity (i.e. of\norder $\\epsilon^{-2}$ if the mean squared error is at most $\\epsilon^2$) for\nlocally Lipschitz and barrier-type functionals of the triplet and develop an\nunbiased version of the estimator. We illustrate the performance of the\nalgorithm with numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 18:01:47 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 07:29:29 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 21:22:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["C\u00e1zares", "Jorge Ignacio Gonz\u00e1lez", ""], ["Mijatovi\u0107", "Aleksandar", ""], ["Bravo", "Ger\u00f3nimo Uribe", ""]]}, {"id": "1810.11042", "submitter": "Spencer Woody", "authors": "Spencer Woody, Oscar Hernan Madrid Padilla, and James G. Scott", "title": "Optimal post-selection inference for sparse signals: a nonparametric\n  empirical-Bayes approach", "comments": null, "journal-ref": null, "doi": "10.1093/biomet/asab014", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recently developed Bayesian methods have focused on sparse signal\ndetection. However, much less work has been done addressing the natural\nfollow-up question: how to make valid inferences for the magnitude of those\nsignals after selection. Ordinary Bayesian credible intervals suffer from\nselection bias, owing to the fact that the target of inference is chosen\nadaptively. Existing Bayesian approaches for correcting this bias produce\ncredible intervals with poor frequentist properties, while existing frequentist\napproaches require sacrificing the benefits of shrinkage typical in Bayesian\nmethods, resulting in confidence intervals that are needlessly wide. We address\nthis gap by proposing a nonparametric empirical-Bayes approach for constructing\noptimal selection-adjusted confidence sets. Our method produces confidence sets\nthat are as short as possible on average, while both adjusting for selection\nand maintaining exact frequentist coverage uniformly over the parameter space.\nOur main theoretical result establishes an important consistency property of\nour procedure: that under mild conditions, it asymptotically converges to the\nresults of an oracle-Bayes analysis in which the prior distribution of signal\nsizes is known exactly. Across a series of examples, the method outperforms\nexisting frequentist techniques for post-selection inference, producing\nconfidence sets that are notably shorter but with the same coverage guarantee.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 18:03:32 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 21:58:36 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 19:25:14 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Woody", "Spencer", ""], ["Padilla", "Oscar Hernan Madrid", ""], ["Scott", "James G.", ""]]}, {"id": "1810.11130", "submitter": "Paulo Orenstein", "authors": "Paulo Orenstein", "title": "Robust Importance Sampling with Adaptive Winsorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling is a widely used technique to estimate properties of a\ndistribution. This paper investigates trading-off some bias for variance by\nadaptively winsorizing the importance sampling estimator. The novel winsorizing\nprocedure, based on the Balancing Principle (or Lepskii's Method), chooses a\nthreshold level among a pre-defined set by roughly balancing the bias and\nvariance of the estimator when winsorized at different levels. As a\nconsequence, it provides a principled way to perform winsorization with\nfinite-sample optimality guarantees under minimal assumptions. In various\nexamples, the proposed estimator is shown to have smaller mean squared error\nand mean absolute deviation than leading alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 22:45:45 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 20:30:19 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Orenstein", "Paulo", ""]]}, {"id": "1810.11207", "submitter": "Kartik Ahuja", "authors": "Kartik Ahuja, Mihaela van der Schaar", "title": "Joint Concordance Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing metrics in competing risks survival analysis such as concordance and\naccuracy do not evaluate a model's ability to jointly predict the event type\nand the event time. To address these limitations, we propose a new metric,\nwhich we call the joint concordance. The joint concordance measures a model's\nability to predict the overall risk profile, i.e., risk of death from different\nevent types. We develop a consistent estimator for the new metric that accounts\nfor the censoring bias. We use the new metric to develop a variable importance\nranking approach. Using the real and synthetic data experiments, we show that\nmodels selected using the existing metrics are worse than those selected using\njoint concordance at jointly predicting the event type and event time. We show\nthat the existing approaches for variable importance ranking often fail to\nrecognize the importance of the event-specific risk factors, whereas, the\nproposed approach does not, since it compares risk factors based on their\ncontribution to the prediction of the different event-types. To summarize,\njoint concordance is helpful for model comparisons and variable importance\nranking and has the potential to impact applications such as\nrisk-stratification and treatment planning in multimorbid populations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 07:25:59 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 21:53:35 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Ahuja", "Kartik", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1810.11209", "submitter": "Mingyuan Zhou", "authors": "Dandan Guo, Bo Chen, Hao Zhang, Mingyuan Zhou", "title": "Deep Poisson gamma dynamical systems", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop deep Poisson-gamma dynamical systems (DPGDS) to model sequentially\nobserved multivariate count data, improving previously proposed models by not\nonly mining deep hierarchical latent structure from the data, but also\ncapturing both first-order and long-range temporal dependencies. Using\nsophisticated but simple-to-implement data augmentation techniques, we derived\nclosed-form Gibbs sampling update equations by first backward and upward\npropagating auxiliary latent counts, and then forward and downward sampling\nlatent variables. Moreover, we develop stochastic gradient MCMC inference that\nis scalable to very long multivariate count time series. Experiments on both\nsynthetic and a variety of real-world data demonstrate that the proposed model\nnot only has excellent predictive performance, but also provides highly\ninterpretable multilayer latent structure to represent hierarchical and\ntemporal information propagation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 07:28:20 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 01:01:02 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Guo", "Dandan", ""], ["Chen", "Bo", ""], ["Zhang", "Hao", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1810.11223", "submitter": "Yi Yu", "authors": "Mark Fiecas, Chenlei Leng, Weidong Liu, and Yi Yu", "title": "Spectral Analysis of High-dimensional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A useful approach for analysing multiple time series is via characterising\ntheir spectral density matrix as the frequency domain analog of the covariance\nmatrix. When the dimension of the time series is large compared to their\nlength, regularisation based methods can overcome the curse of dimensionality,\nbut the existing ones lack theoretical justification. This paper develops the\nfirst non-asymptotic result for characterising the difference between the\nsample and population versions of the spectral density matrix, allowing one to\njustify a range of high-dimensional models for analysing time series. As a\nconcrete example, we apply this result to establish the convergence of the\nsmoothed periodogram estimators and sparse estimators of the inverse of\nspectral density matrices, namely precision matrices. These results, novel in\nthe frequency domain time series analysis, are corroborated by simulations and\nan analysis of the Google Flu Trends data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 08:23:22 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Fiecas", "Mark", ""], ["Leng", "Chenlei", ""], ["Liu", "Weidong", ""], ["Yu", "Yi", ""]]}, {"id": "1810.11397", "submitter": "Xinwei Ma", "authors": "Xinwei Ma and Jingshen Wang", "title": "Robust Inference Using Inverse Probability Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse Probability Weighting (IPW) is widely used in empirical work in\neconomics and other disciplines. As Gaussian approximations perform poorly in\nthe presence of \"small denominators,\" trimming is routinely employed as a\nregularization strategy. However, ad hoc trimming of the observations renders\nusual inference procedures invalid for the target estimand, even in large\nsamples. In this paper, we first show that the IPW estimator can have different\n(Gaussian or non-Gaussian) asymptotic distributions, depending on how \"close to\nzero\" the probability weights are and on how large the trimming threshold is.\nAs a remedy, we propose an inference procedure that is robust not only to small\nprobability weights entering the IPW estimator but also to a wide range of\ntrimming threshold choices, by adapting to these different asymptotic\ndistributions. This robustness is achieved by employing resampling techniques\nand by correcting a non-negligible trimming bias. We also propose an\neasy-to-implement method for choosing the trimming threshold by minimizing an\nempirical analogue of the asymptotic mean squared error. In addition, we show\nthat our inference procedure remains valid with the use of a data-driven\ntrimming threshold. We illustrate our method by revisiting a dataset from the\nNational Supported Work program.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 15:47:29 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 19:13:22 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Ma", "Xinwei", ""], ["Wang", "Jingshen", ""]]}, {"id": "1810.11480", "submitter": "Muhyiddin Izadi", "authors": "Muhyiddin Izadi, Sirous Fathimanesh", "title": "Testing Exponentiality Against a Trend Change in Mean Time to Failure in\n  Age Replacement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean time to failure in age replacement evaluates the performance and\neffectiveness of the age replacement policy. In this paper, we propose a test\nfor exponentiality against a trend change in mean time to failure in age\nreplacement. We derive the asymptotic distribution of the test statistics under\nthe null hypothesis to approximate the critical values. We conduct a simulation\nstudy to investigate the performance of the proposed test and compare it with\nsome well known tests in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 18:19:20 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Izadi", "Muhyiddin", ""], ["Fathimanesh", "Sirous", ""]]}, {"id": "1810.11664", "submitter": "Mengyang Gu", "authors": "Mengyang Gu, Kyle Anderson and Erika McPhillips", "title": "Calibration of imperfect mathematical models by multiple sources of data\n  with measurement bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model calibration involves using experimental or field data to estimate the\nunknown parameters of a mathematical model. This task is complicated by\ndiscrepancy between the model and reality, and by possible bias in the data. We\nconsider model calibration in the presence of both model discrepancy and\nmeasurement bias using multiple sources of data. Model discrepancy is often\nestimated using a Gaussian stochastic process (GaSP), but it has been observed\nin many studies that the calibrated mathematical model can be far from the\nreality. Here we show that modeling the discrepancy function via a GaSP often\nleads to an inconsistent estimation of the calibration parameters even if one\nhas an infinite number of repeated experiments and infinite number of\nobservations in a fixed input domain in each experiment. We introduce the\nscaled Gaussian stochastic process (S-GaSP) to model the discrepancy function.\nUnlike the GaSP, the S-GaSP utilizes a non-increasing scaling function which\nassigns more probability mass on the smaller $L_2$ loss between the\nmathematical model and reality, preventing the calibrated mathematical model\nfrom deviating too much from reality.\n  We apply our technique to the calibration of a geophysical model of\nK\\={\\i}lauea Volcano, Hawai`i, using multiple radar satellite interferograms.\nWe compare the use of models calibrated using multiple data sets simultaneously\nwith results obtained using stacks (averages). We derive distributions for the\nmaximum likelihood estimator and Bayesian inference, both implemented in the\n\"RobustCalibration\" package available on CRAN. Analysis of both simulated and\nreal data confirm that our approach can identify the measurement bias and model\ndiscrepancy using multiple sources of data, and provide better estimates of\nmodel parameters.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 15:37:05 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 18:50:53 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gu", "Mengyang", ""], ["Anderson", "Kyle", ""], ["McPhillips", "Erika", ""]]}, {"id": "1810.11721", "submitter": "Abhijit Mandal", "authors": "Taranga Mukherjee, Abhijit Mandal, Ayanendranath Basu", "title": "The B-Exponential Divergence and its Generalizations with Applications\n  to Parametric Estimation", "comments": "28 pages, 7 figures", "journal-ref": null, "doi": "10.1007/s10260-018-00444-8", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new family of minimum divergence estimators based on the\nBregman divergence is proposed, where the defining convex function has an\nexponential nature. These estimators avoid the necessity of using an\nintermediate kernel density and many of them also have strong robustness\nproperties. It is further demonstrated that the proposed approach can be\nextended to construct a class of generalized estimating equations, where the\npool of the resultant estimators encompass a large variety of minimum\ndivergence estimators and range from highly robust to fully efficient based on\nthe choice of the tuning parameters. All of the resultant estimators are\nM-estimators, where the defining functions make explicit use of the form of the\nparametric model. The properties of these estimators are discussed in detail;\nthe theoretical results are substantiated by simulation and real data examples.\nIt is observed that in many cases, certain robust estimators from the above\ngeneralized class provide better compromises between robustness and efficiency\ncompared to the existing standards.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 22:09:07 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Mukherjee", "Taranga", ""], ["Mandal", "Abhijit", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1810.11746", "submitter": "Zhao Liu", "authors": "Zhao Liu", "title": "On buffered double autoregressive time series models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A buffered double autoregressive (BDAR) time series model is proposed in this\npaper to depict the buffering phenomenon of conditional mean and conditional\nvariance in time series. To build this model, a novel flexible regime switching\nmechanism is introduced to modify the classical threshold time series model by\ncapturing the stickiness of signal. Besides, considering the inadequacy of\ntraditional models under the lack of information, a signal retrospection is run\nin this model to provide a more accurate judgment. Moreover, formal proofs\nsuggest strict stationarity and geometric ergodicity of BDAR model under\nseveral sufficient conditions. A Gaussian quasi-maximum likelihood estimation\n(QMLE) is employed and the asymptotic distributions of its estimators are\nderived. It has been demonstrated that the estimated thresholds of the BDAR\nmodel are $n$-consistent, each of which converges weakly to a functional of a\ntwo-sided compound Poisson process. The remaining parameters are\n$\\sqrt{n}$-consistent and asymptotically normal. Furthermore, a model selection\ncriteria and its asymptotic property have been established. Simulation studies\nare constructed to evaluate the finite sample performance of QMLE and model\nselection criteria. Finally, an empirical analysis of Hang Seng Index (HSI)\nusing BDAR model reveals the asymmetry of investors' preference over losses and\ngains as well as the asymmetry of volatility structure.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 01:57:31 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Liu", "Zhao", ""]]}, {"id": "1810.11776", "submitter": "Niklas Pfister", "authors": "Niklas Pfister, Stefan Bauer and Jonas Peters", "title": "Learning stable and predictive structures in kinetic systems: Benefits\n  of a causal approach", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.1905688116", "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning kinetic systems from data is one of the core challenges in many\nfields. Identifying stable models is essential for the generalization\ncapabilities of data-driven inference. We introduce a computationally efficient\nframework, called CausalKinetiX, that identifies structure from discrete time,\nnoisy observations, generated from heterogeneous experiments. The algorithm\nassumes the existence of an underlying, invariant kinetic model, a key\ncriterion for reproducible research. Results on both simulated and real-world\nexamples suggest that learning the structure of kinetic systems benefits from a\ncausal perspective. The identified variables and models allow for a concise\ndescription of the dynamics across multiple experimental settings and can be\nused for prediction in unseen experiments. We observe significant improvements\ncompared to well established approaches focusing solely on predictive\nperformance, especially for out-of-sample generalization.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 07:47:54 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 12:07:30 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Pfister", "Niklas", ""], ["Bauer", "Stefan", ""], ["Peters", "Jonas", ""]]}, {"id": "1810.11861", "submitter": "Andrew Wilson", "authors": "William Herlands, Daniel B. Neill, Hannes Nickisch, Andrew Gordon\n  Wilson", "title": "Change Surfaces for Expressive Multidimensional Changepoints and\n  Counterfactual Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying changes in model parameters is fundamental in machine learning\nand statistics. However, standard changepoint models are limited in\nexpressiveness, often addressing unidimensional problems and assuming\ninstantaneous changes. We introduce change surfaces as a multidimensional and\nhighly expressive generalization of changepoints. We provide a model-agnostic\nformalization of change surfaces, illustrating how they can provide variable,\nheterogeneous, and non-monotonic rates of change across multiple dimensions.\nAdditionally, we show how change surfaces can be used for counterfactual\nprediction. As a concrete instantiation of the change surface framework, we\ndevelop Gaussian Process Change Surfaces (GPCS). We demonstrate counterfactual\nprediction with Bayesian posterior mean and credible sets, as well as massive\nscalability by introducing novel methods for additive non-separable kernels.\nUsing two large spatio-temporal datasets we employ GPCS to discover and\ncharacterize complex changes that can provide scientific and policy relevant\ninsights. Specifically, we analyze twentieth century measles incidence across\nthe United States and discover previously unknown heterogeneous changes after\nthe introduction of the measles vaccine. Additionally, we apply the model to\nrequests for lead testing kits in New York City, discovering distinct spatial\nand demographic patterns.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 19:08:18 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 11:56:42 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Herlands", "William", ""], ["Neill", "Daniel B.", ""], ["Nickisch", "Hannes", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1810.11881", "submitter": "Jize Zhang", "authors": "Jize Zhang, Lizhen Lin", "title": "Bounded Regression with Gaussian Process Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examples with bound information on the regression function and density abound\nin many real applications. We propose a novel approach for estimating such\nfunctions by incorporating the prior knowledge on the bounds. Specially, a\nGaussian process is first imposed on the regression function whose posterior\ndistribution is then projected onto the bounded space. The resulting projected\nmeasure is then used for inference. The projected sample path has closed form\nwhich facilitates efficient computations. In particular, our projection\napproach maintains a comparable computational efficiency with that of the\noriginal GP. The proposed method yield predictions that respects bound\nconstraints everywhere, while allows varying bounds across the input domain. An\nextensive simulation study is carried out which demonstrates that the\nperformance of our approach dominates that of the competitors. An application\nto real data set is also considered.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 20:54:43 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Zhang", "Jize", ""], ["Lin", "Lizhen", ""]]}, {"id": "1810.12161", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi and Bao-Tuyen Huynh", "title": "Regularized Maximum Likelihood Estimation and Feature Selection in\n  Mixtures-of-Experts Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of Experts (MoE) are successful models for modeling heterogeneous\ndata in many statistical learning problems including regression, clustering and\nclassification. Generally fitted by maximum likelihood estimation via the\nwell-known EM algorithm, their application to high-dimensional problems is\nstill therefore challenging. We consider the problem of fitting and feature\nselection in MoE models, and propose a regularized maximum likelihood\nestimation approach that encourages sparse solutions for heterogeneous\nregression data models with potentially high-dimensional predictors. Unlike\nstate-of-the art regularized MLE for MoE, the proposed modelings do not require\nan approximate of the penalty function. We develop two hybrid EM algorithms: an\nExpectation-Majorization-Maximization (EM/MM) algorithm, and an EM algorithm\nwith coordinate ascent algorithm. The proposed algorithms allow to\nautomatically obtaining sparse solutions without thresholding, and avoid matrix\ninversion by allowing univariate parameter updates. An experimental study shows\nthe good performance of the algorithms in terms of recovering the actual sparse\nsolutions, parameter estimation, and clustering of heterogeneous regression\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:42:04 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Huynh", "Bao-Tuyen", ""]]}, {"id": "1810.12169", "submitter": "Marie Szafranski", "authors": "Florent Guinot (LaMME), Marie Szafranski (LaMME), Julien Chiquet\n  (MIA-Paris), Anouk Zancarini, Christine Le Signor, Christophe Mougel (IGEPP),\n  Christophe Ambroise (LaMME)", "title": "Fast Computation of Genome-Metagenome Interaction Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation. Association studies have been widely used to search for\nassociations between common genetic variants observations and a given\nphenotype. However, it is now generally accepted that genes and environment\nmust be examined jointly when estimating phenotypic variance. In this work we\nconsider two types of biological markers: genotypic markers, which characterize\nan observation in terms of inherited genetic information, and metagenomic\nmarker which are related to the environment. Both types of markers are\navailable in their millions and can be used to characterize any observation\nuniquely. Objective. Our focus is on detecting interactions between groups of\ngenetic and metagenomic markers in order to gain a better understanding of the\ncomplex relationship between environment and genome in the expression of a\ngiven phenotype. Contributions. We propose a novel approach for efficiently\ndetecting interactions between complementary datasets in a high-dimensional\nsetting with a reduced computational cost. The method, named SICOMORE, reduces\nthe dimension of the search space by selecting a subset of supervariables in\nthe two complementary datasets. These supervariables are given by a weighted\ngroup structure defined on sets of variables at different scales. A Lasso\nselection is then applied on each type of supervariable to obtain a subset of\npotential interactions that will be explored via linear model testing. Results.\nWe compare SICOMORE with other approaches in simulations, with varying sample\nsizes, noise, and numbers of true interactions. SICOMORE exhibits convincing\nresults in terms of recall, as well as competitive performances with respect to\nrunning time. The method is also used to detect interaction between genomic\nmarkers in Medicago truncatula and metagenomic markers in its rhizosphere\nbacterial community. Software availability. A R package is available, along\nwith its documentation and associated scripts, allowing the reader to reproduce\nthe results presented in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:57:02 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 15:41:15 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 16:19:38 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Guinot", "Florent", "", "LaMME"], ["Szafranski", "Marie", "", "LaMME"], ["Chiquet", "Julien", "", "MIA-Paris"], ["Zancarini", "Anouk", "", "IGEPP"], ["Signor", "Christine Le", "", "IGEPP"], ["Mougel", "Christophe", "", "IGEPP"], ["Ambroise", "Christophe", "", "LaMME"]]}, {"id": "1810.12177", "submitter": "Sebastien Marmin", "authors": "S\\'ebastien Marmin, Maurizio Filippone", "title": "Variational Calibration of Computer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian calibration of black-box computer models offers an established\nframework to obtain a posterior distribution over model parameters. Traditional\nBayesian calibration involves the emulation of the computer model and an\nadditive model discrepancy term using Gaussian processes; inference is then\ncarried out using MCMC. These choices pose computational and statistical\nchallenges and limitations, which we overcome by proposing the use of\napproximate Deep Gaussian processes and variational inference techniques. The\nresult is a practical and scalable framework for calibration, which obtains\ncompetitive performance compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 15:07:07 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Marmin", "S\u00e9bastien", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1810.12452", "submitter": "Kara Rudolph", "authors": "Kara E Rudolph and Oleg Sofrygin and Mark J van der Laan", "title": "Complier stochastic direct effects: identification and robust estimation", "comments": null, "journal-ref": "Journal of the American Statistical Association. 2020", "doi": "10.1080/01621459.2019.1704292", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis is critical to understanding the mechanisms underlying\nexposure-outcome relationships. In this paper, we identify the instrumental\nvariable (IV)-direct effect of the exposure on the outcome not through the\nmediator, using randomization of the instrument. To our knowledge, such an\nestimand has not previously been considered or estimated. We propose and\nevaluate several estimators for this estimand: a ratio of inverse-probability\nof treatment-weighted estimators (IPTW), a ratio of estimating equation\nestimators (EE), a ratio of targeted minimum loss-based estimators (TMLE), and\na TMLE that targets the CSDE directly. These estimators are applicable for a\nvariety of study designs, including randomized encouragement trials, like the\nMTO housing voucher experiment we consider as an illustrative example,\ntreatment discontinuities, and Mendelian randomization. We found the IPTW\nestimator to be the most sensitive to finite sample bias, resulting in bias of\nover 40% even when all models were correctly specified in a sample size of\nN=100. In contrast, the EE estimator and compatible TMLE estimator were far\nless sensitive to finite samples. The EE and TMLE estimators also have\nadvantages over the IPTW estimator in terms of efficiency and reduced reliance\non correct parametric model specification.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 23:10:38 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Rudolph", "Kara E", ""], ["Sofrygin", "Oleg", ""], ["van der Laan", "Mark J", ""]]}, {"id": "1810.12519", "submitter": "Masatoshi Uehara", "authors": "Masatoshi Uehara, Jae Kwang Kim", "title": "Semiparametric response model with nonignorable nonresponse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to deal with nonignorable response is often a challenging problem\nencountered in statistical analysis with missing data. Parametric model\nassumption for the response mechanism is often made and there is no way to\nvalidate the model assumption with missing data. We consider a semiparametric\nresponse model that relaxes the parametric model assumption in the response\nmechanism. Two types of efficient estimators, profile maximum likelihood\nestimator and profile calibration estimator, are proposed and their asymptotic\nproperties are investigated. Two extensive simulation studies are used to\ncompare with some existing methods. We present an application of our method\nusing Korean Labor and Income Panel Survey data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 04:17:21 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Uehara", "Masatoshi", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1810.13094", "submitter": "Nicholas Seewald", "authors": "Nicholas J. Seewald and Kelley M. Kidwell and Inbal Nahum-Shani and\n  Tianshuang Wu and James R. McKay and Daniel Almirall", "title": "Sample size considerations for comparing dynamic treatment regimens in a\n  sequential multiple-assignment randomized trial with a continuous\n  longitudinal outcome", "comments": "Supplementary material at https://osf.io/q7zv8/ This version updates\n  funding information not previously included. Text remains the final accepted\n  version of the manuscript in Statistical Methods in Medical Research", "journal-ref": "Stat. Methods Med. Res. 29 (2020) 1891-1912", "doi": "10.1177/0962280219877520", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinicians and researchers alike are increasingly interested in how best to\npersonalize interventions. A dynamic treatment regimen (DTR) is a sequence of\npre-specified decision rules which can be used to guide the delivery of a\nsequence of treatments or interventions that are tailored to the changing needs\nof the individual. The sequential multiple-assignment randomized trial (SMART)\nis a research tool which allows for the construction of effective DTRs. We\nderive easy-to-use formulae for computing the total sample size for three\ncommon two-stage SMART designs in which the primary aim is to compare mean\nend-of-study outcomes for two embedded DTRs which recommend different\nfirst-stage treatments. The formulae are derived in the context of a regression\nmodel which leverages information from a longitudinal outcome collected over\nthe entire study. We show that the sample size formula for a SMART can be\nwritten as the product of the sample size formula for a standard two-arm\nrandomized trial, a deflation factor that accounts for the increased\nstatistical efficiency resulting from a longitudinal analysis, and an inflation\nfactor that accounts for the design of a SMART. The SMART design inflation\nfactor is typically a function of the anticipated probability of response to\nfirst-stage treatment. We review modeling and estimation for DTR effect\nanalyses using a longitudinal outcome from a SMART, as well as the estimation\nof standard errors. We also present estimators for the covariance matrix for a\nvariety of common working correlation structures. Methods are motivated using\nthe ENGAGE study, a SMART aimed at developing a DTR for increasing motivation\nto attend treatments among alcohol- and cocaine-dependent patients.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 03:56:44 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 20:15:48 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 13:18:45 GMT"}, {"version": "v4", "created": "Sun, 5 Jan 2020 19:37:34 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Seewald", "Nicholas J.", ""], ["Kidwell", "Kelley M.", ""], ["Nahum-Shani", "Inbal", ""], ["Wu", "Tianshuang", ""], ["McKay", "James R.", ""], ["Almirall", "Daniel", ""]]}, {"id": "1810.13321", "submitter": "Clara Happ", "authors": "Clara Happ, Fabian Scheipl, Alice-Agnes Gabriel and Sonja Greven", "title": "A General Framework for Multivariate Functional Principal Component\n  Analysis of Amplitude and Phase Variation", "comments": "Contains Supplementary Files", "journal-ref": null, "doi": "10.1002/sta4.220", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data typically contains amplitude and phase variation. In many\ndata situations, phase variation is treated as a nuisance effect and is removed\nduring preprocessing, although it may contain valuable information. In this\nnote, we focus on joint principal component analysis (PCA) of amplitude and\nphase variation.\n  As the space of warping functions has a complex geometric structure, one key\nelement of the analysis is transforming the warping functions to\n$L^2(\\mathcal{T})$. We present different transformation approaches and show how\nthey fit into a general class of transformations. This allows us to compare\ntheir strengths and limitations. In the context of PCA, our results offer\narguments in favour of the centered log-ratio transformation.\n  We further embed existing approaches from Hadjipantelis et al. (2015) and Lee\nand Jung (2017) for joint PCA of amplitude and phase variation into the\nframework of multivariate functional PCA, where we study the properties of the\nestimators based on an appropriate metric. The approach is illustrated through\nan application from seismology.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 14:58:45 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Happ", "Clara", ""], ["Scheipl", "Fabian", ""], ["Gabriel", "Alice-Agnes", ""], ["Greven", "Sonja", ""]]}, {"id": "1810.13402", "submitter": "Louisa Smith", "authors": "Louisa H. Smith and Tyler J. VanderWeele", "title": "Bounding bias due to selection", "comments": null, "journal-ref": "Epidemiology. 2019;30(4):509-516", "doi": "10.1097/EDE.0000000000001032", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When epidemiologic studies are conducted in a subset of the population,\nselection bias can threaten the validity of causal inference. This bias can\noccur whether or not that selected population is the target population, and can\noccur even in the absence of exposure-outcome confounding. However, it is often\ndifficult to quantify the extent of selection bias, and sensitivity analysis\ncan be challenging to undertake and to understand. In this article we\ndemonstrate that the magnitude of the bias due to selection can be bounded by\nsimple expressions defined by parameters characterizing the relationships\nbetween unmeasured factor(s) responsible for the bias and the measured\nvariables. No functional form assumptions are necessary about those unmeasured\nfactors. Using knowledge about the selection mechanism, researchers can account\nfor the possible extent of selection bias by specifying the size of the\nparameters in the bounds. We also show that the bounds, which differ depending\non the target population, result in summary measures that can be used to\ncalculate the minimum magnitude of the parameters required to shift a risk\nratio to the null. The summary measure can be used to determine the overall\nstrength of selection that would be necessary to explain away a result. We then\nshow that the bounds and summary measures can be simplified in certain contexts\nor with certain assumptions. Using examples with varying selection mechanisms,\nwe also demonstrate how researchers can implement these simple sensitivity\nanalyses.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 16:48:11 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 21:14:48 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Smith", "Louisa H.", ""], ["VanderWeele", "Tyler J.", ""]]}]