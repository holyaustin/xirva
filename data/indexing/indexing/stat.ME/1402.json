[{"id": "1402.0136", "submitter": "Wei Sun", "authors": "Wei Sun, Yufeng Liu, James J. Crowley, Ting-Huei Chen, Hua Zhou,\n  Haitao Chu, Shunping Huang, Pei-Fen Kuan, Yuan Li, Darla Miller, Ginger Shaw,\n  Yichao Wu, Vasyl Zhabotynsky, Leonard McMillan, Fei Zou, Patrick F. Sullivan,\n  Fernando Pardo-Manuel de Villena", "title": "IsoDOT Detects Differential RNA-isoform Expression/Usage with respect to\n  a Categorical or Continuous Covariate with High Sensitivity and Specificity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a statistical method named IsoDOT to assess differential\nisoform expression (DIE) and differential isoform usage (DIU) using RNA-seq\ndata. Here isoform usage refers to relative isoform expression given the total\nexpression of the corresponding gene. IsoDOT performs two tasks that cannot be\naccomplished by existing methods: to test DIE/DIU with respect to a continuous\ncovariate, and to test DIE/DIU for one case versus one control. The latter task\nis not an uncommon situation in practice, e.g., comparing paternal and maternal\nallele of one individual or comparing tumor and normal sample of one cancer\npatient. Simulation studies demonstrate the high sensitivity and specificity of\nIsoDOT. We apply IsoDOT to study the effects of haloperidol treatment on mouse\ntranscriptome and identify a group of genes whose isoform usages respond to\nhaloperidol treatment.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2014 23:21:17 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 14:09:32 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Sun", "Wei", ""], ["Liu", "Yufeng", ""], ["Crowley", "James J.", ""], ["Chen", "Ting-Huei", ""], ["Zhou", "Hua", ""], ["Chu", "Haitao", ""], ["Huang", "Shunping", ""], ["Kuan", "Pei-Fen", ""], ["Li", "Yuan", ""], ["Miller", "Darla", ""], ["Shaw", "Ginger", ""], ["Wu", "Yichao", ""], ["Zhabotynsky", "Vasyl", ""], ["McMillan", "Leonard", ""], ["Zou", "Fei", ""], ["Sullivan", "Patrick F.", ""], ["de Villena", "Fernando Pardo-Manuel", ""]]}, {"id": "1402.0330", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth, Fredrik Lindsten, Thomas B. Sch\\\"on", "title": "Sequential Monte Carlo for Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for how to use sequential Monte Carlo (SMC)\nalgorithms for inference in probabilistic graphical models (PGM). Via a\nsequential decomposition of the PGM we find a sequence of auxiliary\ndistributions defined on a monotonically increasing sequence of probability\nspaces. By targeting these auxiliary distributions using SMC we are able to\napproximate the full joint distribution defined by the PGM. One of the key\nmerits of the SMC sampler is that it provides an unbiased estimate of the\npartition function of the model. We also show how it can be used within a\nparticle Markov chain Monte Carlo framework in order to construct\nhigh-dimensional block-sampling algorithms for general PGMs.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 10:21:18 GMT"}, {"version": "v2", "created": "Mon, 9 Jun 2014 06:32:56 GMT"}, {"version": "v3", "created": "Fri, 8 Aug 2014 08:06:51 GMT"}, {"version": "v4", "created": "Mon, 6 Oct 2014 05:55:20 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1402.0361", "submitter": "Paula Saavedra-Nieves", "authors": "Paula Saavedra-Nieves, Wenceslao Gonz\\'alez-Manteiga and Alberto\n  Rodr\\'iguez-Casal", "title": "A comparative simulation study of data-driven methods for estimating\n  density level sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density level sets are mainly estimated using one of three methodologies:\nplug-in, excess mass, or a hybrid approach. The plug-in methods are based on\nreplacing the unknown density by some nonparametric estimator, usually the\nkernel. Thus, the bandwidth selection is a fundamental problem from a practical\npoint of view. Recently, specific selectors for level sets have been proposed.\nHowever, if some a priori information about the geometry of the level set is\navailable, then excess mass algorithms can be useful. In this case, a density\nestimator is not necessary, and the problem of bandwidth selection can be\navoided. The third methodology is a hybrid of the others. As in the excess mass\nmethod, it assumes a mild geometric restriction on the level set and, like the\nplug-in approach, requires a pilot nonparametric estimator of the density. One\ninteresting open question concerns the practical performance of these methods.\nIn this work, existing methods are reviewed, and two new hybrid algorithms are\nproposed. Their practical behaviour is compared through extensive simulations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 12:33:13 GMT"}, {"version": "v2", "created": "Wed, 5 Mar 2014 12:32:41 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Saavedra-Nieves", "Paula", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["Rodr\u00edguez-Casal", "Alberto", ""]]}, {"id": "1402.0550", "submitter": "Hau-tieng Wu", "authors": "Stefano Marchesini, Yu-Chao Tu, Hau-tieng Wu", "title": "Alternating Projection, Ptychographic Imaging and Phase Synchronization", "comments": null, "journal-ref": "Applied and Computational Harmonic Analysis 41 (3), 815-851(2019)", "doi": "10.1016/j.acha.2015.06.005", "report-no": "LBNL-6583E", "categories": "math.OC physics.optics stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate necessary and sufficient conditions of the local convergence\nof the alternating projection algorithm to a unique solution up to a global\nphase factor. Additionally, for the ptychography imaging problem, we discuss\nphase synchronization and graph connection Laplacian, and show how to construct\nan accurate initial guess to accelerate convergence speed to handle the big\nimaging data in the coming new light source era.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 23:54:58 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2014 17:38:47 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2015 02:21:38 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Marchesini", "Stefano", ""], ["Tu", "Yu-Chao", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1402.0844", "submitter": "Luo Xiao", "authors": "Luo Xiao and Florentina Bunea", "title": "On the theoretic and practical merits of the banding estimator for large\n  covariance matrices", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the banding estimator proposed in Bickel and Levina\n(2008) for estimation of large covariance matrices. We prove that the banding\nestimator achieves rate-optimality under the operator norm, for a class of\napproximately banded covariance matrices, improving the existing results in\nBickel and Levina (2008). In addition, we propose a Stein's unbiased risk\nestimate (Sure)-type approach for selecting the bandwidth for the banding\nestimator. Simulations indicate that the Sure-tuned banding estimator\noutperforms competing estimators.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 19:52:46 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Xiao", "Luo", ""], ["Bunea", "Florentina", ""]]}, {"id": "1402.0943", "submitter": "Ali Saeb Dr.", "authors": "R. Vasudeva and Ali Saeb", "title": "Galton-Watson Process for a class of distributions from Bernoulli to\n  Poisson", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the Galton Watson branching process has been studied for a\nclass of offspring distributions which are in a way sandwiched between the\nBernoulli and Poisson.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 05:53:23 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Vasudeva", "R.", ""], ["Saeb", "Ali", ""]]}, {"id": "1402.1033", "submitter": "Silvia Pandolfi Miss", "authors": "Francesco Bartolucci, Giorgio E. Montanari, and Silvia Pandolfi", "title": "Three-step estimation of latent Markov models with covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a modified version of the three-step estimation method for the\nlatent class model with covariates, which may be used to estimate latent Markov\nmodels for longitudinal data. The three-step estimation approach we propose is\nbased on a preliminary clustering of sample units on the basis of the time\nspecific responses only. This approach represents an useful estimation tool\nwhen a large number of response variables are observed at each time occasion.\nIn such a context, full maximum likelihood estimation, which is typically based\non the Expectation-Maximization algorithm, may have some drawbacks, essentially\ndue to the presence of many local maxima of the model likelihood. Moreover, the\nEM algorithm may be particularly slow to converge, and may become unstable with\ncomplex LM models. We prove the consistency of the proposed three-step\nestimator when the number of response variables tends to infinity. We also show\nthe results of a simulation study aimed at evaluating the performance of the\nproposed alternative approach with respect to the full likelihood method. We\nfinally illustrate an application to a real dataset on the health status of\nelderly people hosted in Italian nursing homes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 13:22:07 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Bartolucci", "Francesco", ""], ["Montanari", "Giorgio E.", ""], ["Pandolfi", "Silvia", ""]]}, {"id": "1402.1138", "submitter": "Kjartan Rimstad", "authors": "Kjartan Rimstad and Henning Omre", "title": "Skew-Gaussian Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skewness is often present in a wide range of spatial prediction problems, and\nmodeling it in the spatial context remains a challenging problem. In this study\na skew-Gaussian random field is considered. The skew-Gaussian random field is\nconstructed by using the multivariate closed skew-normal distribution, which is\na generalization of the traditional normal distribution. We present a\nMetropolis-Hastings algorithm for simulating realizations efficiently from the\nrandom field, and an algorithm for estimating parameters by maximum likelihood\nwith a Monte Carlo approximation of the likelihood. We demonstrate and evaluate\nthe algorithms on synthetic cases. The skewness in the skew-Gaussian random\nfield is found to be strongly influenced by the spatial correlation in the\nfield, and the parameter estimators appear as consistent with increasing size\nof the random field. Moreover, we use the closed skew-normal distribution in a\nmultivariate random field predictive setting on real seismic data from the\nSleipner field in the North Sea.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 19:38:27 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Rimstad", "Kjartan", ""], ["Omre", "Henning", ""]]}, {"id": "1402.1144", "submitter": "Kjartan Rimstad", "authors": "Kjartan Rimstad and Henning Omre", "title": "Generalized Gaussian Random Fields using hidden selections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study non-Gaussian random fields constructed by the selection normal\ndistribution, and we term them selection Gaussian random fields. The selection\nGaussian random field can capture skewness, multi-modality, and to some extend\nheavy tails in the marginal distribution. We present a Metropolis-Hastings\nalgorithm for efficient simulation of realizations from the random field, and a\nnumerical algorithm for estimating model parameters by maximum likelihood. The\nalgorithms are demonstrated and evaluated on synthetic cases and on a real\nseismic data set from the North Sea. In the North Sea data set we are able to\nreduce the mean square prediction error by 20-40% compared to a Gaussian model,\nand we obtain more reliable prediction intervals.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 19:57:49 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Rimstad", "Kjartan", ""], ["Omre", "Henning", ""]]}, {"id": "1402.1253", "submitter": "Debasish Roy", "authors": "Saikat Sarkar and Debasish Roy", "title": "An Ensemble Kushner-Stratonovich (EnKS) Nonlinear Filter: Additive\n  Particle Updates in Non-Iterative and Iterative Forms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the cheap availability of computing resources enabling faster Monte\nCarlo simulations, the potential benefits of particle filtering in revealing\naccurate statistical information on the imprecisely known model parameters or\nmodeling errors of dynamical systems, based on limited time series data, have\nnot been quite realized. A major numerical bottleneck precipitating this\nunder-performance, especially for higher dimensional systems, is the\nprogressive particle impoverishment owing to weight collapse and the aim of the\ncurrent work is to address this problem by replacing weight-based updates\nthrough additive ones. Thus, in the context of nonlinear filtering problems, a\nnovel additive particle update scheme, in its non-iterative and iterative\nforms, is proposed based on manipulations of the innovation integral in the\ngoverning Kushner-Stratonovich equation. Numerical evidence for the\nidentification of nonlinear and large dimensional dynamical systems indicates a\nsubstantively superior performance of the non- iterative version of the EnKS\nvis-\\`a-vis most existing filters. The costlier iterative version, though\nconceptually elegant, mostly appears to effect a marginal improvement in the\nreconstruction accuracy over its non-iterative counterpart. Prominent in the\nreported numerical comparisons are variants of the Ensemble Kalman Filter\n(EnKF) that also use additive updates, albeit with many inherent limitations of\na Kalman filter.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 05:45:31 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Sarkar", "Saikat", ""], ["Roy", "Debasish", ""]]}, {"id": "1402.1472", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss and Dorit Hammerling", "title": "Parallel inference for massive distributed spatial data using low-rank\n  models", "comments": "20 pages; published in Statistics and Computing", "journal-ref": null, "doi": "10.1007/s11222-016-9627-4", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to rapid data growth, statistical analysis of massive datasets often has\nto be carried out in a distributed fashion, either because several datasets\nstored in separate physical locations are all relevant to a given problem, or\nsimply to achieve faster (parallel) computation through a divide-and-conquer\nscheme. In both cases, the challenge is to obtain valid inference that does not\nrequire processing all data at a single central computing node. We show that\nfor a very widely used class of spatial low-rank models, which can be written\nas a linear combination of spatial basis functions plus a fine-scale-variation\ncomponent, parallel spatial inference and prediction for massive distributed\ndata can be carried out exactly, meaning that the results are the same as for a\ntraditional, non-distributed analysis. The communication cost of our\ndistributed algorithms does not depend on the number of data points. After\nextending our results to the spatio-temporal case, we illustrate our\nmethodology by carrying out distributed spatio-temporal particle filtering\ninference on total precipitable water measured by three different satellite\nsensor systems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 20:16:30 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 23:44:54 GMT"}, {"version": "v3", "created": "Sat, 8 Nov 2014 00:15:42 GMT"}, {"version": "v4", "created": "Fri, 5 Feb 2016 17:10:49 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Katzfuss", "Matthias", ""], ["Hammerling", "Dorit", ""]]}, {"id": "1402.1649", "submitter": "Heng Lian", "authors": "Gaorong Li, Peng Lai, Heng Lian", "title": "Variable Selection and Estimation for Partially Linear Single-index\n  Models with Longitudinal Data", "comments": "to appear in Statistics and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the partially linear single-index models with\nlongitudinal data. To deal with the variable selection problem in this context,\nwe propose a penalized procedure combined with two bias correction methods,\nresulting in the bias-corrected generalized estimating equation (GEE) and the\nbias-corrected quadratic inference function (QIF), which can take into account\nthe correlations. Asymptotic properties of these methods are demonstrated. We\nalso evaluate the finite sample performance of the proposed methods via Monte\nCarlo simulation studies and a real data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 14:24:25 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Li", "Gaorong", ""], ["Lai", "Peng", ""], ["Lian", "Heng", ""]]}, {"id": "1402.1694", "submitter": "Patrick Conrad", "authors": "Patrick R. Conrad, Youssef M. Marzouk, Natesh S. Pillai, Aaron Smith", "title": "Accelerating Asymptotically Exact MCMC for Computationally Intensive\n  Models via Local Approximations", "comments": "A major update of the theory and examples", "journal-ref": "Journal of the American Statistical Association, volume 111, issue\n  516, 1591--1607 (2016)", "doi": "10.1080/01621459.2015.1096787", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a new framework for accelerating Markov chain Monte Carlo in\nposterior sampling problems where standard methods are limited by the\ncomputational cost of the likelihood, or of numerical models embedded therein.\nOur approach introduces local approximations of these models into the\nMetropolis-Hastings kernel, borrowing ideas from deterministic approximation\ntheory, optimization, and experimental design. Previous efforts at integrating\napproximate models into inference typically sacrifice either the sampler's\nexactness or efficiency; our work seeks to address these limitations by\nexploiting useful convergence characteristics of local approximations. We prove\nthe ergodicity of our approximate Markov chain, showing that it samples\nasymptotically from the \\emph{exact} posterior distribution of interest. We\ndescribe variations of the algorithm that employ either local polynomial\napproximations or local Gaussian process regressors. Our theoretical results\nreinforce the key observation underlying this paper: when the likelihood has\nsome \\emph{local} regularity, the number of model evaluations per MCMC step can\nbe greatly reduced without biasing the Monte Carlo average. Numerical\nexperiments demonstrate multiple order-of-magnitude reductions in the number of\nforward model evaluations used in representative ODE and PDE inference\nproblems, with both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 16:59:31 GMT"}, {"version": "v2", "created": "Mon, 8 Sep 2014 20:53:57 GMT"}, {"version": "v3", "created": "Wed, 5 Nov 2014 15:20:21 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2015 15:45:09 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Conrad", "Patrick R.", ""], ["Marzouk", "Youssef M.", ""], ["Pillai", "Natesh S.", ""], ["Smith", "Aaron", ""]]}, {"id": "1402.1782", "submitter": "James M. Flegal", "authors": "Roberto C. Crackel and James M. Flegal", "title": "Bayesian inference for a flexible class of bivariate beta distributions", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several bivariate beta distributions have been proposed in the literature. In\nparticular, Olkin and Liu (2003) proposed a 3 parameter bivariate beta model,\nwhich Arnold and Ng (2011) extend to 5 and 8 parameter models. The 3 parameter\nmodel allows for only positive correlation, while the latter models can\naccommodate both positive and negative correlation. However, these come at the\nexpense of a density that is mathematically intractable. The focus of this\nresearch is on Bayesian estimation for the 5 and 8 parameter models. Since the\nlikelihood does not exist in closed form, we apply approximate Bayesian\ncomputation, a likelihood free approach. Simulation studies have been carried\nout for the 5 and 8 parameter cases under various priors and tolerance levels.\nWe apply the 5 parameter model to a real data set by allowing the model to\nserve as a prior to correlated proportions of a bivariate beta binomial model.\nResults and comparisons are then discussed.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 21:58:38 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 20:34:08 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2015 03:31:12 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Crackel", "Roberto C.", ""], ["Flegal", "James M.", ""]]}, {"id": "1402.1872", "submitter": "Roberto Rivera", "authors": "Roberto Rivera and Wendy Meiring", "title": "Spatial prediction variance estimation based on covariance penalty", "comments": "Critical error makes equation (17) incorrect", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice rarely (if ever) is the spatial covariance known in spatial\nprediction problems. Often, prediction is performed after estimated spatial\ncovariance parameters are plugged into the prediction equation. The estimated\nspatial association parameters arealso plugged into the prediction variance of\nthe spatial predictor. However, simply plugging in spatial covariance parameter\nestimates into the prediction variance of the spatial predictor does not take\ninto account the uncertainty in the true values of the spatial covariance\nparameters. Therefore the plug-in prediction variance estimate will\nunderestimate the true prediction variance of the estimated spatial predictor,\nespecially for small datasets. We propose a new way to estimate the prediction\nvariance of the estimated spatial predictor based on a covariance penalty using\nparametric bootstrapping. Our new estimator is compared to three other\nprediction variance estimators proposed in literature. The new prediction\nvariance estimator generally performs better than the plug in method for small\ndatasets with weak spatial association, although sometimes it is second best\namong the four prediction variance estimators compared. Furthermore, the new\nprediction variance estimator could potentially be used in the case of\nnon-normal prediction.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 17:32:33 GMT"}, {"version": "v2", "created": "Wed, 10 Sep 2014 00:36:08 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Rivera", "Roberto", ""], ["Meiring", "Wendy", ""]]}, {"id": "1402.1876", "submitter": "Abraao D. C. Nacimento", "authors": "A. C. Frery, A. D. C. Nascimento, R. J. Cintra", "title": "Information Theory and Image Understanding: An Application to\n  Polarimetric SAR Imagery", "comments": "15 pages, 11 figures", "journal-ref": "Chilean Journal of Statistics, Vol. 2, No. 2, September 2011,\n  81-100", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a comprehensive examination of the use of information\ntheory for understanding Polarimetric Synthetic Aperture Radar (PolSAR) images\nby means of contrast measures that can be used as test statistics. Due to the\nphenomenon called `speckle', common to all images obtained with coherent\nillumination such as PolSAR imagery, accurate modelling is required in their\nprocessing and analysis. The scaled multilook complex Wishart distribution has\nproven to be a successful approach for modelling radar backscatter from forest\nand pasture areas. Classification, segmentation, and image analysis techniques\nwhich depend on this model have been devised, and many of them employ some kind\nof dissimilarity measure. Specifically, we introduce statistical tests for\nanalyzing contrast in such images. These tests are based on the chi-square,\nKullback-Leibler, R\\'enyi, Bhattacharyya, and Hellinger distances. Results\nobtained by Monte Carlo experiments reveal the Kullback-Leibler distance as the\nbest one with respect to the empirical test sizes under several situations\nwhich include pure and contaminated data. The proposed methodology was applied\nto actual data, obtained by an E-SAR sensor over surroundings of\nWe$\\beta$ssling, Bavaria, Germany.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 18:08:48 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Frery", "A. C.", ""], ["Nascimento", "A. D. C.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1402.1888", "submitter": "Stanley Chan", "authors": "Stanley H. Chan and Edoardo M. Airoldi", "title": "A Consistent Histogram Estimator for Exchangeable Graph Models", "comments": "28 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exchangeable graph models (ExGM) subsume a number of popular network models.\nThe mathematical object that characterizes an ExGM is termed a graphon. Finding\nscalable estimators of graphons, provably consistent, remains an open issue. In\nthis paper, we propose a histogram estimator of a graphon that is provably\nconsistent and numerically efficient. The proposed estimator is based on a\nsorting-and-smoothing (SAS) algorithm, which first sorts the empirical degree\nof a graph, then smooths the sorted graph using total variation minimization.\nThe consistency of the SAS algorithm is proved by leveraging sparsity concepts\nfrom compressed sensing.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 20:43:46 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2014 20:31:25 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Chan", "Stanley H.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1402.1909", "submitter": "George Karabatsos Ph.D.", "authors": "George Karabatsos and Stephen G. Walker", "title": "A Bayesian Nonparametric Hypothesis Testing Approach for Regression\n  Discontinuity Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regression discontinuity (RD) design is a popular approach to causal\ninference in non-randomized studies. This is because it can be used to identify\nand estimate causal effects under mild conditions. Specifically, for each\nsubject, the RD design assigns a treatment or non-treatment, depending on\nwhether or not an observed value of an assignment variable exceeds a fixed and\nknown cutoff value.\n  In this paper, we propose a Bayesian nonparametric regression modeling\napproach to RD designs, which exploits a local randomization feature. In this\napproach, the assignment variable is treated as a covariate, and a\nscalar-valued confounding variable is treated as a dependent variable (which\nmay be a multivariate confounder score). Then, over the model's posterior\ndistribution of locally-randomized subjects that cluster around the cutoff of\nthe assignment variable, inference for causal effects are made within this\nrandom cluster, via two-group statistical comparisons of treatment outcomes and\nnon-treatment outcomes.\n  We illustrate the Bayesian nonparametric approach through the analysis of a\nreal educational data set, to investigate the causal link between basic skills\nand teaching ability.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 02:10:47 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Karabatsos", "George", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1402.1937", "submitter": "Heejoon Han", "authors": "Heejoon Han, Oliver Linton, Tatsushi Oka, Yoon-Jae Whang", "title": "The Cross-Quantilogram: Measuring Quantile Dependence and Testing\n  Directional Predictability between Time Series", "comments": null, "journal-ref": "Journal of Econometrics, 2016, 193, 251-270", "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the cross-quantilogram to measure the quantile dependence\nbetween two time series. We apply it to test the hypothesis that one time\nseries has no directional predictability to another time series. We establish\nthe asymptotic distribution of the cross quantilogram and the corresponding\ntest statistic. The limiting distributions depend on nuisance parameters. To\nconstruct consistent confidence intervals we employ the stationary bootstrap\nprocedure; we show the consistency of this bootstrap. Also, we consider the\nself-normalized approach, which is shown to be asymptotically pivotal under the\nnull hypothesis of no predictability. We provide simulation studies and two\nempirical applications. First, we use the cross-quantilogram to detect\npredictability from stock variance to excess stock return. Compared to existing\ntools used in the literature of stock return predictability, our method\nprovides a more complete relationship between a predictor and stock return.\nSecond, we investigate the systemic risk of individual financial institutions,\nsuch as JP Morgan Chase, Goldman Sachs and AIG. This article has supplementary\nmaterials online.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 11:30:20 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 04:46:31 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Han", "Heejoon", ""], ["Linton", "Oliver", ""], ["Oka", "Tatsushi", ""], ["Whang", "Yoon-Jae", ""]]}, {"id": "1402.1976", "submitter": "Mircea Andrecut Dr", "authors": "M. Andrecut", "title": "Decision Making via AHP", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Analytic Hierarchy Process (AHP) is a procedure for establishing\npriorities in multi-criteria decision making problems. Here we discuss the\nLogarithmic Least Squares (LLS) method for the AHP and group-AHP, which\nprovides an exact and unique solution for the priority vector. Also, we show\nthat for the group-AHP, the LLS method is equivalent with the minimization of\nthe weighted sum of generalized Kullback-Leibler divergences, between the\ngroup-priority vector and the priority vector of each expert.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 18:23:51 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Andrecut", "M.", ""]]}, {"id": "1402.2550", "submitter": "Jay Bartroff", "authors": "Jay Bartroff and Tze Leung Lai and Balasubramanian Narasimhan", "title": "A New Approach to Designing Phase I-II Cancer Trials for Cytotoxic\n  Chemotherapies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been much work on early phase cancer designs that\nincorporate both toxicity and efficacy data, called Phase I-II designs because\nthey combine elements of both phases. However, they do not explicitly address\nthe Phase II hypothesis test of $H_0: p\\le p_0$, where $p$ is the probability\nof efficacy at the estimated maximum tolerated dose (MTD) $\\widehat{\\eta}$ from\nPhase I and $p_0$ is the baseline efficacy rate. Standard practice for Phase II\nremains to treat $p$ as a fixed, unknown parameter and to use Simon's 2-stage\ndesign with all patients dosed at $\\widehat{\\eta}$. We propose a Phase I-II\ndesign that addresses the uncertainty in the estimate $p=p(\\widehat{\\eta})$ in\n$H_0$ by using sequential generalized likelihood theory. Combining this with a\nPhase I design that incorporates efficacy data, the Phase I-II design provides\na common framework that can be used all the way from the first dose of Phase I\nthrough the final accept/reject decision about $H_0$ at the end of Phase II,\nutilizing both toxicity and efficacy data throughout. Efficient group\nsequential testing is used in Phase II that allows for early stopping to show\ntreatment effect or futility. The proposed Phase I-II design thus removes the\nartificial barrier between Phase I and Phase II, and fulfills the objectives of\nsearching for the MTD and testing if the treatment has an acceptable response\nrate to enter into a Phase III trial.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 16:27:30 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Bartroff", "Jay", ""], ["Lai", "Tze Leung", ""], ["Narasimhan", "Balasubramanian", ""]]}, {"id": "1402.2679", "submitter": "Wen-Yu Hua", "authors": "Wen-Yu Hua and Debashis Ghosh (for the Alzheimer's Disease\n  Neuroimaging Initiative)", "title": "Equivalence of Kernel Machine Regression and Kernel Distance Covariance\n  for Multidimensional Trait Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associating genetic markers with a multidimensional phenotype is an important\nyet challenging problem. In this work, we establish the equivalence between two\npopular methods: kernel-machine regression (KMR), and kernel distance\ncovariance (KDC). KMR is a semiparametric regression frameworks that models the\ncovariate effects parametrically, while the genetic markers are considered\nnon-parametrically. KDC represents a class of methods that includes distance\ncovariance (DC) and Hilbert-Schmidt Independence Criterion (HSIC), which are\nnonparametric tests of independence. We show the equivalence between the score\ntest of KMR and the KDC statistic under certain conditions. This result leads\nto a novel generalization of the KDC test that incorporates the covariates. Our\ncontributions are three-fold: (1) establishing the equivalence between KMR and\nKDC; (2) showing that the principles of kernel machine regression can be\napplied to the interpretation of KDC; (3) the development of a broader class of\nKDC statistics, that the members are the quantities of different kernels. We\ndemonstrate the proposals using simulation studies. Data from the Alzheimer's\nDisease Neuroimaging Initiative (ADNI) is used to explore the association\nbetween the genetic variants on gene \\emph{FLJ16124} and phenotypes represented\nin 3D structural brain MR images adjusting for age and gender. The results\nsuggest that SNPs of \\emph{FLJ16124} exhibit strong pairwise interaction\neffects that are correlated to the changes of brain region volumes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 21:52:30 GMT"}, {"version": "v2", "created": "Wed, 2 Apr 2014 00:13:00 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Hua", "Wen-Yu", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"], ["Ghosh", "Debashis", "", "for the Alzheimer's Disease\n  Neuroimaging Initiative"]]}, {"id": "1402.2734", "submitter": "Ye Liang", "authors": "Ye Liang", "title": "Graph-based Multivariate Conditional Autoregressive Models", "comments": "31 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conditional autoregressive model is a routinely used statistical model\nfor areal data that arise from, for instances, epidemiological, socio-economic\nor ecological studies. Various multivariate conditional autoregressive models\nhave also been extensively studied in the literature and it has been shown that\nextending from the univariate case to the multivariate case is not trivial. The\ndifficulties lie in many aspects, including validity, interpretability,\nflexibility and computational feasibility of the model. In this paper, we\napproach the multivariate modeling from an element-based perspective instead of\nthe traditional vector-based perspective. We focus on the joint adjacency\nstructure of elements and discuss graphical structures for both the spatial and\nnon-spatial domains. We assume that the graph for the spatial domain is\ngenerally known and fixed while the graph for the non-spatial domain can be\nunknown and random. We propose a very general specification for the\nmultivariate conditional modeling and then focus on three special cases, which\nare linked to well known models in the literature. Bayesian inference for\nparameter learning and graph learning is provided for the focused cases, and\nfinally, an example with public health data is illustrated.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 04:56:02 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2018 23:01:24 GMT"}, {"version": "v3", "created": "Sat, 20 Jul 2019 04:26:01 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Liang", "Ye", ""]]}, {"id": "1402.2755", "submitter": "Alessio Benavoli", "authors": "Alessio Benavoli, Francesca Mangili, Fabrizio Ruggeri and Marco\n  Zaffalon", "title": "Imprecise Dirichlet Process with application to the hypothesis test on\n  the probability that X< Y", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dirichlet process (DP) is one of the most popular Bayesian nonparametric\nmodels. An open problem with the DP is how to choose its infinite dimensional\nparameter (base measure) in case of lack of prior information. In this work we\npresent the Imprecise DP (IDP) -- a prior near-ignorance DP-based model that\ndoes not require any choice of this probability measure. It consists of a class\nof DPs obtained by letting the normalized base measure of the DP vary in the\nset of all probability measures. We discuss the tight connections of this\napproach with Bayesian robustness and in particular prior near-ignorance\nmodeling via sets of probabilities. We use this model to perform a Bayesian\nhypothesis test on the probability P(X<Y). We study the theoretical properties\nof the IDP test (e.g., asymptotic consistency), and compare it with the\nfrequentist Mann-Whitney-Wilcoxon rank test that is commonly employed as a test\non P(X< Y). In particular we will show that our method is more robust, in the\nsense that it is able to isolate instances in which the aforementioned test is\nvirtually guessing at random.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 08:05:17 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2014 10:19:16 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Benavoli", "Alessio", ""], ["Mangili", "Francesca", ""], ["Ruggeri", "Fabrizio", ""], ["Zaffalon", "Marco", ""]]}, {"id": "1402.2775", "submitter": "Anirvan Chakraborty Mr.", "authors": "Anirvan Chakraborty and Probal Chaudhuri (Indian Statistical\n  Institute, Kolkata, India)", "title": "On data depth in infinite dimensional spaces", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": "R4/2012, Stat. Math. Unit", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of data depth leads to a center-outward ordering of multivariate\ndata, and it has been effectively used for developing various data analytic\ntools. While different notions of depth were originally developed for finite\ndimensional data, there have been some recent attempts to develop depth\nfunctions for data in infinite dimensional spaces. In this paper, we consider\nsome notions of depth in infinite dimensional spaces and study their properties\nunder various stochastic models. Our analysis shows that some of the depth\nfunctions available in the literature have degenerate behaviour for some\ncommonly used probability distributions in infinite dimensional spaces of\nsequences and functions. As a consequence, they are not very useful for the\nanalysis of data satisfying such infinite dimensional probability models.\nHowever, some modified versions of those depth functions as well as an infinite\ndimensional extension of the spatial depth do not suffer from such degeneracy,\nand can be conveniently used for analyzing infinite dimensional data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 10:10:46 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Chakraborty", "Anirvan", "", "Indian Statistical\n  Institute, Kolkata, India"], ["Chaudhuri", "Probal", "", "Indian Statistical\n  Institute, Kolkata, India"]]}, {"id": "1402.2905", "submitter": "Marco Scutari", "authors": "Marco Scutari, Phil Howell, David J. Balding, Ian Mackay", "title": "Multiple Quantitative Trait Analysis Using Bayesian Networks", "comments": "28 pages, 1 figure, code at\n  http://www.bnlearn.com/research/genetics14", "journal-ref": "Genetics 2014, 198(1), 129-137", "doi": null, "report-no": null, "categories": "stat.ME q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for genome-wide prediction and association studies usually target a\nsingle phenotypic trait. However, in animal and plant genetics it is common to\nrecord information on multiple phenotypes for each individual that will be\ngenotyped. Modeling traits individually disregards the fact that they are most\nlikely associated due to pleiotropy and shared biological basis, thus providing\nonly a partial, confounded view of genetic effects and phenotypic interactions.\nIn this paper we use data from a Multiparent Advanced Generation Inter-Cross\n(MAGIC) winter wheat population to explore Bayesian networks as a convenient\nand interpretable framework for the simultaneous modeling of multiple\nquantitative traits. We show that they are equivalent to multivariate genetic\nbest linear unbiased prediction (GBLUP), and that they are competitive with\nsingle-trait elastic net and single-trait GBLUP in predictive performance.\nFinally, we discuss their relationship with other additive-effects models and\ntheir advantages in inference and interpretation. MAGIC populations provide an\nideal setting for this kind of investigation because the very low population\nstructure and large sample size result in predictive models with good power and\nlimited confounding due to relatedness.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 17:33:05 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 13:04:00 GMT"}, {"version": "v3", "created": "Tue, 29 Jul 2014 16:24:57 GMT"}, {"version": "v4", "created": "Sun, 7 Dec 2014 20:19:28 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Scutari", "Marco", ""], ["Howell", "Phil", ""], ["Balding", "David J.", ""], ["Mackay", "Ian", ""]]}, {"id": "1402.3019", "submitter": "Georg Hahn", "authors": "Axel Gandy, Georg Hahn", "title": "A Framework for Monte Carlo based Multiple Testing", "comments": null, "journal-ref": "Scand J Stat (2016), 43(4):1046--1063", "doi": "10.1111/sjos.12228", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with a situation in which we would like to test multiple\nhypotheses with tests whose p-values cannot be computed explicitly but can be\napproximated using Monte Carlo simulation. This scenario occurs widely in\npractice. We are interested in obtaining the same rejections and non-rejections\nas the ones obtained if the p-values for all hypotheses had been available. The\npresent article introduces a framework for this scenario by providing a generic\nalgorithm for a general multiple testing procedure. We establish conditions\nwhich guarantee that the rejections and non-rejections obtained through Monte\nCarlo simulations are identical to the ones obtained with the p-values. Our\nframework is applicable to a general class of step-up and step-down procedures\nwhich includes many established multiple testing corrections such as the ones\nof Bonferroni, Holm, Sidak, Hochberg or Benjamini-Hochberg. Moreover, we show\nhow to use our framework to improve algorithms available in the literature in\nsuch a way as to yield theoretical guarantees on their results. These\nmodifications can easily be implemented in practice and lead to a particular\nway of reporting multiple testing results as three sets together with an error\nbound on their correctness, demonstrated exemplarily using a real biological\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 01:47:03 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 19:11:01 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2015 23:53:34 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2016 03:11:44 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Gandy", "Axel", ""], ["Hahn", "Georg", ""]]}, {"id": "1402.3085", "submitter": "Yue Wu", "authors": "Yue Wu, Jose Miguel Hernandez Lobato, Zoubin Ghahramani", "title": "Gaussian Process Volatility Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate prediction of time-changing variances is an important task in\nthe modeling of financial data. Standard econometric models are often limited\nas they assume rigid functional relationships for the variances. Moreover,\nfunction parameters are usually learned using maximum likelihood, which can\nlead to overfitting. To address these problems we introduce a novel model for\ntime-changing variances using Gaussian Processes. A Gaussian Process (GP)\ndefines a distribution over functions, which allows us to capture highly\nflexible functional relationships for the variances. In addition, we develop an\nonline algorithm to perform inference. The algorithm has two main advantages.\nFirst, it takes a Bayesian approach, thereby avoiding overfitting. Second, it\nis much quicker than current offline inference procedures. Finally, our new\nmodel was evaluated on financial data and showed significant improvement in\npredictive performance over current standard models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 10:48:46 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Wu", "Yue", ""], ["Lobato", "Jose Miguel Hernandez", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1402.3093", "submitter": "Julyan Arbel", "authors": "Julyan Arbel, Kerrie Mengersen, Judith Rousseau", "title": "Bayesian nonparametric dependent model for partially replicated data:\n  the influence of fuel spills on species diversity", "comments": "Main Paper: 22 pages, 6 figures. Supplementary Material: 11 pages, 1\n  figure", "journal-ref": "Annals of Applied Statistics, 10(3):1496--1516, 2016", "doi": "10.1214/16-AOAS944", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a dependent Bayesian nonparametric model for the probabilistic\nmodeling of membership of subgroups in a community based on partially\nreplicated data. The focus here is on species-by-site data, i.e. community data\nwhere observations at different sites are classified in distinct species. Our\naim is to study the impact of additional covariates, for instance environmental\nvariables, on the data structure, and in particular on the community diversity.\nTo that purpose, we introduce dependence a priori across the covariates, and\nshow that it improves posterior inference. We use a dependent version of the\nGriffiths-Engen-McCloskey distribution defined via the stick-breaking\nconstruction. This distribution is obtained by transforming a Gaussian process\nwhose covariance function controls the desired dependence. The resulting\nposterior distribution is sampled by Markov chain Monte Carlo. We illustrate\nthe application of our model to a soil microbial dataset acquired across a\nhydrocarbon contamination gradient at the site of a fuel spill in Antarctica.\nThis method allows for inference on a number of quantities of interest in\necotoxicology, such as diversity or effective concentrations, and is broadly\napplicable to the general problem of communities response to environmental\nvariables.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 11:29:45 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 13:30:45 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Arbel", "Julyan", ""], ["Mengersen", "Kerrie", ""], ["Rousseau", "Judith", ""]]}, {"id": "1402.3365", "submitter": "Rosemary  Renaut", "authors": "Saeed Vatankhah, Rosemary A Renaut and Vahid E Ardestani", "title": "Regularization Parameter Estimation for Underdetermined problems by the\n  $\\chi^2$ principle with application to $2D$ focusing gravity inversion", "comments": null, "journal-ref": "Inverse Problems 30 (2014) 085002", "doi": "10.1088/0266-5611/30/8/085002", "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\chi^2$-principle generalizes the Morozov discrepancy principle (MDP) to\nthe augmented residual of the Tikhonov regularized least squares problem.\nWeighting of the data fidelity by a known Gaussian noise distribution on the\nmeasured data, when the regularization term is weighted by unknown inverse\ncovariance information on the model parameters, the minimum of the Tikhonov\nfunctional is a random variable following a $\\chi^2$-distribution with $m+p-n$\ndegrees of freedom, model matrix $G:$ $m \\times n$ and regularizer $L:$\n$p\\times n$. It is proved that the result holds also for $m<n$ when $m+p\\ge n$.\nA Newton root-finding algorithm is used to find the regularization parameter\n$\\alpha$ which yields the optimal inverse covariance weighting in the case of a\nwhite noise assumption on the mapped model data. It is implemented for\nsmall-scale problems using the generalized singular value decomposition.\nNumerical results verify the algorithm for the case of regularizers\napproximating zero to second order derivative approximations, contrasted with\nthe methods of generalized cross validation and unbiased predictive risk\nestimation. The inversion of underdetermined $2D$ focusing gravity data\nproduces models with non-smooth properties, for which typical implementations\nin this field use the iterative minimum support (MS) stabilizer and both\nregularizer and regularizing parameter are updated each iteration. For a\nsimulated data set with noise, the regularization parameter estimation methods\nfor underdetermined data sets are used in this iterative framework, also\ncontrasted with the L-curve and MDP. Experiments demonstrate efficiency and\nrobustness of the $\\chi^2$-principle, moreover the L-curve and MDP are\ngenerally outperformed. Furthermore, the MS is of general use for the\n$\\chi^2$-principle when implemented without the knowledge of a mean value of\nthe model.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 05:14:23 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2014 16:09:29 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Vatankhah", "Saeed", ""], ["Renaut", "Rosemary A", ""], ["Ardestani", "Vahid E", ""]]}, {"id": "1402.3433", "submitter": "Martin Treiber", "authors": "Andy Obermeyer, Martin Treiber, and Christos Evangelinos", "title": "Thresholds in choice behaviour and the size of travel time savings", "comments": "21 pages, 7 figures, submitted to the Journal of Choice Modelling", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Travel time savings are usually the most substantial economic benefit of\ntransport infrastructure projects. However, questions surround whether small\ntime savings are as valuable per unit as larger savings. Thresholds in\nindividual choice behaviour are one reason cited for a discounted unit value\nfor small time savings. We demonstrate different approaches for modelling these\nthresholds using synthetic and stated choice data. We show that the\nconsideration of thresholds is important, even if the discounted unit value for\nsmall travel time savings is rejected for transport project appraisal. If an\nexisting threshold is ignored in model estimation, the value of travel time\nsavings will be biased. The presented procedure might also be useful to model\nthresholds in other contexts of choice behaviour.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 11:12:32 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 19:56:26 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Obermeyer", "Andy", ""], ["Treiber", "Martin", ""], ["Evangelinos", "Christos", ""]]}, {"id": "1402.3478", "submitter": "Pier Francesco Perri", "authors": "Lucio Barabesi, Giancarlo Diana, Pier Francesco Perri", "title": "A functional derivative useful for the linearization of inequality\n  indexes in the design-based framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linearization methods are customarily adopted in sampling surveys to obtain\napproximated variance formulae for estimators of nonlinear functions of finite\npopulation totals - such as ratios, correlation coefficients or measures of\nincome inequality - which can be usually rephrased in terms of statistical\nfunctionals. In the present paper, by considering the Deville (1991) approach\nstemming on the concept of design-based influence curve, we provide a general\nresult for linearizing large families of inequality indexes. As an example, the\nachievement is applied to the Gini, the Amato, the Zenga and the Atkinson\nindexes, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 14:28:40 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Barabesi", "Lucio", ""], ["Diana", "Giancarlo", ""], ["Perri", "Pier Francesco", ""]]}, {"id": "1402.3480", "submitter": "Anirvan Chakraborty", "authors": "Anirvan Chakraborty, Probal Chaudhuri", "title": "The spatial distribution in infinite dimensional spaces and related\n  quantiles and depths", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1226 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 3, 1203-1231", "doi": "10.1214/14-AOS1226", "report-no": "IMS-AOS-AOS1226", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatial distribution has been widely used to develop various\nnonparametric procedures for finite dimensional multivariate data. In this\npaper, we investigate the concept of spatial distribution for data in infinite\ndimensional Banach spaces. Many technical difficulties are encountered in such\nspaces that are primarily due to the noncompactness of the closed unit ball. In\nthis work, we prove some Glivenko-Cantelli and Donsker-type results for the\nempirical spatial distribution process in infinite dimensional spaces. The\nspatial quantiles in such spaces can be obtained by inverting the spatial\ndistribution function. A Bahadur-type asymptotic linear representation and the\nassociated weak convergence results for the sample spatial quantiles in\ninfinite dimensional spaces are derived. A study of the asymptotic efficiency\nof the sample spatial median relative to the sample mean is carried out for\nsome standard probability distributions in function spaces. The spatial\ndistribution can be used to define the spatial depth in infinite dimensional\nBanach spaces, and we study the asymptotic properties of the empirical spatial\ndepth in such spaces. We also demonstrate the spatial quantiles and the spatial\ndepth using some real and simulated functional data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 14:34:18 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 05:31:20 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Chakraborty", "Anirvan", ""], ["Chaudhuri", "Probal", ""]]}, {"id": "1402.3514", "submitter": "Kaveh Vakili", "authors": "E. Schmitt and K. Vakili", "title": "Robust PCA with FastHCS", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is widely used to analyze high-dimensional\ndata, but it is very sensitive to outliers. Robust PCA methods seek fits that\nare unaffected by the outliers and can therefore be trusted to reveal them.\nFastHCS (High-dimensional Congruent Subsets) is a robust PCA algorithm suitable\nfor high-dimensional applications, including cases where the number of\nvariables exceeds the number of observations. After detailing the FastHCS\nalgorithm, we carry out an extensive simulation study and three real data\napplications, the results of which show that FastHCS is systematically more\nrobust to outliers than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 16:13:21 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 09:51:42 GMT"}, {"version": "v3", "created": "Tue, 15 Jul 2014 06:20:07 GMT"}, {"version": "v4", "created": "Tue, 16 Dec 2014 20:28:59 GMT"}, {"version": "v5", "created": "Fri, 19 Dec 2014 04:23:59 GMT"}, {"version": "v6", "created": "Mon, 18 May 2015 09:48:48 GMT"}, {"version": "v7", "created": "Thu, 24 Sep 2015 11:24:08 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Schmitt", "E.", ""], ["Vakili", "K.", ""]]}, {"id": "1402.3559", "submitter": "Tsagris Michail", "authors": "Michail Tsagris, Christina Beneki and Hossein Hassani", "title": "On the folded normal distribution", "comments": "Published in Mathematics. http://www.mdpi.com/2227-7390/2/1/12", "journal-ref": "Mathematics 2014, 2(1), 12-28", "doi": "10.3390/math2010012", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The characteristic function of the folded normal distribution and its moment\nfunction are derived. The entropy of the folded normal distribution and the\nKullback--Leibler from the normal and half normal distributions are\napproximated using Taylor series. The accuracy of the results are also assessed\nusing different criteria. The maximum likelihood estimates and confidence\nintervals for the parameters are obtained using the asymptotic theory and\nbootstrap method. The coverage of the confidence intervals is also examined.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 19:43:46 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Tsagris", "Michail", ""], ["Beneki", "Christina", ""], ["Hassani", "Hossein", ""]]}, {"id": "1402.3580", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Yuting Wu, Daniel J. Holland, Sebastian Nowozin,\n  Mick D. Mantle, Lynn F. Gladden, Andrew Blake", "title": "Bayesian Inference for NMR Spectroscopy with Applications to Chemical\n  Quantification", "comments": "26 pages, 13 figures, 1 table. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear magnetic resonance (NMR) spectroscopy exploits the magnetic\nproperties of atomic nuclei to discover the structure, reaction state and\nchemical environment of molecules. We propose a probabilistic generative model\nand inference procedures for NMR spectroscopy. Specifically, we use a weighted\nsum of trigonometric functions undergoing exponential decay to model free\ninduction decay (FID) signals. We discuss the challenges in estimating the\ncomponents of this general model -- amplitudes, phase shifts, frequencies,\ndecay rates, and noise variances -- and offer practical solutions. We compare\nwith conventional Fourier transform spectroscopy for estimating the relative\nconcentrations of chemicals in a mixture, using synthetic and experimentally\nacquired FID signals. We find the proposed model is particularly robust to low\nsignal to noise ratios (SNR), and overlapping peaks in the Fourier transform of\nthe FID, enabling accurate predictions (e.g., 1% sensitivity at low SNR) which\nare not possible with conventional spectroscopy (5% sensitivity).\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 20:47:58 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2014 02:24:00 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Wu", "Yuting", ""], ["Holland", "Daniel J.", ""], ["Nowozin", "Sebastian", ""], ["Mantle", "Mick D.", ""], ["Gladden", "Lynn F.", ""], ["Blake", "Andrew", ""]]}, {"id": "1402.3740", "submitter": "Zhiwei Qin", "authors": "Zhiwei Qin, Irene Song", "title": "Joint Variable Selection for Data Envelopement Analysis via Group\n  Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study develops a data-driven group variable selection method for data\nenvelopment analysis (DEA), a non-parametric linear programming approach to the\nestimation of production frontiers. The proposed method extends the group Lasso\n(least absolute shrinkage and selection operator) designed for variable\nselection on (often predefined) groups of variables in linear regression models\nto DEA models. In particular, a special constrained version of the group Lasso\nwith the loss function suited for variable selection in DEA models is derived\nand solved by a new tailored algorithm based on the alternating direction\nmethod of multipliers (ADMM). This study further conducts a thorough evaluation\nof the proposed method against two widely used variable selection methods --\nthe efficiency contribution measure (ECM) method and the regression-based (RB)\ntest -- in DEA via Monte Carlo simulations. The simulation results show that\nour method provides more favorable performance compared with its benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2014 00:40:09 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Qin", "Zhiwei", ""], ["Song", "Irene", ""]]}, {"id": "1402.3896", "submitter": "Qijun Fang", "authors": "Qijun Fang, Walter W. Piegorsch, Susan J. Simmons, Xiaosong Li,\n  Cuixian Chen, Yishi Wang", "title": "Bayesian Model-Averaged Benchmark Dose Analysis Via Reparameterized\n  Quantal-Response Models", "comments": "Main document 18 pages, 2 figures. Supplementary document 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important objective in biomedical risk assessment is estimation of minimum\nexposure levels that induce a pre-specified adverse response in a target\npopulation. The exposure/dose points in such settings are known as Benchmark\nDoses (BMDs). Recently, parametric Bayesian estimation for finding BMDs has\nbecome popular. A large variety of candidate dose-response models is available\nfor applying these methods, however, leading to questions of model adequacy and\nuncertainty. Here we enhance the Bayesian estimation technique for BMD analysis\nby applying Bayesian model averaging to produce point estimates and (lower)\ncredible bounds. We include reparameterizations of traditional dose-response\nmodels that allow for more-focused use of elicited prior information when\nbuilding the Bayesian hierarchy. Performance of the method is evaluated via a\nshort simulation study. An example from carcinogenicity testing illustrates the\ncalculations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 05:39:50 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Fang", "Qijun", ""], ["Piegorsch", "Walter W.", ""], ["Simmons", "Susan J.", ""], ["Li", "Xiaosong", ""], ["Chen", "Cuixian", ""], ["Wang", "Yishi", ""]]}, {"id": "1402.4061", "submitter": "Paul von Hippel", "authors": "Paul T. von Hippel, Samuel V. Scarpino, Igor Holas", "title": "Robust estimation of inequality from binned incomes", "comments": "39 pages, 7 tables, 7 figures", "journal-ref": "Sociological Methodology 46(1), 212-251, 2015", "doi": "10.1177/0081175015599807", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers must often estimate income inequality using data that give only\nthe number of cases (e.g., families or households) whose incomes fall in \"bins\"\nsuch as $0-9,999, $10,000-14,999,..., $200,000+. We find that popular methods\nfor estimating inequality from binned incomes are not robust in small samples,\nwhere popular methods can produce infinite, undefined, or arbitrarily large\nestimates. To solve these and other problems, we develop two improved\nestimators: the robust Pareto midpoint estimator (RPME) and the multimodel\ngeneralized beta estimator (MGBE). In a broad evaluation using US national,\nstate, and county data from 1970 to 2009, we find that both estimators produce\nvery good estimates of the mean and Gini, but less accurate estimates of the\nTheil and mean log deviation. Neither estimator is uniformly more accurate, but\nthe RPME is much faster, which may be a consideration when many estimates must\nbe obtained from many datasets. We have made the methods available as the rpme\nand mgbe commands for Stata and the binequality package for R.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 17:09:05 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2014 19:41:59 GMT"}, {"version": "v3", "created": "Sun, 29 Jun 2014 21:31:01 GMT"}, {"version": "v4", "created": "Mon, 9 Feb 2015 21:49:06 GMT"}, {"version": "v5", "created": "Mon, 16 Feb 2015 09:06:01 GMT"}, {"version": "v6", "created": "Thu, 20 Aug 2015 18:24:47 GMT"}, {"version": "v7", "created": "Mon, 6 Jun 2016 20:16:33 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["von Hippel", "Paul T.", ""], ["Scarpino", "Samuel V.", ""], ["Holas", "Igor", ""]]}, {"id": "1402.4102", "submitter": "Tianqi Chen", "authors": "Tianqi Chen, Emily B. Fox, Carlos Guestrin", "title": "Stochastic Gradient Hamiltonian Monte Carlo", "comments": "ICML 2014 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for\ndefining distant proposals with high acceptance probabilities in a\nMetropolis-Hastings framework, enabling more efficient exploration of the state\nspace than standard random-walk proposals. The popularity of such methods has\ngrown significantly in recent years. However, a limitation of HMC methods is\nthe required gradient computation for simulation of the Hamiltonian dynamical\nsystem-such computation is infeasible in problems involving a large sample size\nor streaming data. Instead, we must rely on a noisy gradient estimate computed\nfrom a subset of the data. In this paper, we explore the properties of such a\nstochastic gradient HMC approach. Surprisingly, the natural implementation of\nthe stochastic approximation can be arbitrarily bad. To address this problem we\nintroduce a variant that uses second-order Langevin dynamics with a friction\nterm that counteracts the effects of the noisy gradient, maintaining the\ndesired target distribution as the invariant distribution. Results on simulated\ndata validate our theory. We also provide an application of our methods to a\nclassification task using neural networks and to online Bayesian matrix\nfactorization.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 19:57:59 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 06:38:21 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Chen", "Tianqi", ""], ["Fox", "Emily B.", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1402.4239", "submitter": "Ran Shi", "authors": "Ran Shi, Ying Guo", "title": "Modeling Covariate Effects in Group Independent Component Analysis with\n  Applications to Functional Magnetic Resonance Imaging", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is a powerful computational tool for\nseparating independent source signals from their linear mixtures. ICA has been\nwidely applied in neuroimaging studies to identify and characterize underlying\nbrain functional networks. An important goal in such studies is to assess the\neffects of subjects' clinical and demographic covariates on the spatial\ndistributions of the functional networks. Currently, covariate effects are not\nincorporated in existing group ICA decomposition methods. Hence, they can only\nbe evaluated through ad-hoc approaches which may not be accurate in many cases.\nIn this paper, we propose a hierarchical covariate ICA model that provides a\nformal statistical framework for estimating and testing covariate effects in\nICA decomposition. A maximum likelihood method is proposed for estimating the\ncovariate ICA model. We develop two expectation-maximization (EM) algorithms to\nobtain maximum likelihood estimates. The first is an exact EM algorithm, which\nhas analytically tractable E-step and M-step. Additionally, we propose a\nsubspace-based approximate EM, which can significantly reduce computational\ntime while still retain high model-fitting accuracy. Furthermore, to test\ncovariate effects on the functional networks, we develop a voxel-wise\napproximate inference procedure which eliminates the needs of computationally\nexpensive covariance estimation. The performance of the proposed methods is\nevaluated via simulation studies. The application is illustrated through an\nfMRI study of Zen meditation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 07:23:00 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 07:04:40 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2015 00:09:35 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Shi", "Ran", ""], ["Guo", "Ying", ""]]}, {"id": "1402.4279", "submitter": "Ingmar Schuster", "authors": "Ingmar Schuster", "title": "A Bayesian Model of node interaction in networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We are concerned with modeling the strength of links in networks by taking\ninto account how often those links are used. Link usage is a strong indicator\nof how closely two nodes are related, but existing network models in Bayesian\nStatistics and Machine Learning are able to predict only wether a link exists\nat all. As priors for latent attributes of network nodes we explore the Chinese\nRestaurant Process (CRP) and a multivariate Gaussian with fixed dimensionality.\nThe model is applied to a social network dataset and a word coocurrence\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 10:34:41 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 10:22:12 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Schuster", "Ingmar", ""]]}, {"id": "1402.4281", "submitter": "Jonathan Stroud", "authors": "Jonathan R. Stroud, Michael L. Stein, Shaun Lysen", "title": "Bayesian and Maximum Likelihood Estimation for Gaussian Processes on an\n  Incomplete Lattice", "comments": "29 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach for Bayesian and maximum likelihood\nparameter estimation for stationary Gaussian processes observed on a large\nlattice with missing values. We propose an MCMC approach for Bayesian\ninference, and a Monte Carlo EM algorithm for maximum likelihood inference. Our\napproach uses data augmentation and circulant embedding of the covariance\nmatrix, and provides exact inference for the parameters and the missing data.\nUsing simulated data and an application to satellite sea surface temperatures\nin the Pacific Ocean, we show that our method provides accurate inference on\nlattices of sizes up to 512 x 512, and outperforms two popular methods:\ncomposite likelihood and spectral approximations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 10:38:14 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Stroud", "Jonathan R.", ""], ["Stein", "Michael L.", ""], ["Lysen", "Shaun", ""]]}, {"id": "1402.4296", "submitter": "Catherine Matias", "authors": "Catherine Matias (LaMME, LPMA), St\\'ephane Robin", "title": "Modeling heterogeneity in random graphs through latent space models: a\n  selective review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a selective review on probabilistic modeling of heterogeneity in\nrandom graphs. We focus on latent space models and more particularly on\nstochastic block models and their extensions that have undergone major\ndevelopments in the last five years.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 11:16:44 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 16:56:13 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Matias", "Catherine", "", "LaMME, LPMA"], ["Robin", "St\u00e9phane", ""]]}, {"id": "1402.4306", "submitter": "Amar Shah", "authors": "Amar Shah, Andrew Gordon Wilson and Zoubin Ghahramani", "title": "Student-t Processes as Alternatives to Gaussian Processes", "comments": "13 pages, 6 figures, 1 table. To appear in \"The Seventeenth\n  International Conference on Artificial Intelligence and Statistics (AISTATS),\n  2014.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the Student-t process as an alternative to the Gaussian\nprocess as a nonparametric prior over functions. We derive closed form\nexpressions for the marginal likelihood and predictive distribution of a\nStudent-t process, by integrating away an inverse Wishart process prior over\nthe covariance kernel of a Gaussian process model. We show surprising\nequivalences between different hierarchical Gaussian process models leading to\nStudent-t processes, and derive a new sampling scheme for the inverse Wishart\nprocess, which helps elucidate these equivalences. Overall, we show that a\nStudent-t process can retain the attractive properties of a Gaussian process --\na nonparametric representation, analytic marginal and predictive distributions,\nand easy model selection through covariance kernels -- but has enhanced\nflexibility, and predictive covariances that, unlike a Gaussian process,\nexplicitly depend on the values of training observations. We verify empirically\nthat a Student-t process is especially useful in situations where there are\nchanges in covariance structure, or in applications like Bayesian optimization,\nwhere accurate predictive covariances are critical for good performance. These\nadvantages come at no additional computational cost over Gaussian processes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 11:47:38 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 10:49:16 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Shah", "Amar", ""], ["Wilson", "Andrew Gordon", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1402.4372", "submitter": "Kyle Vincent Ph. D", "authors": "Kyle Vincent and Steve Thompson", "title": "Estimating the size and distribution of networked populations with\n  snowball sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new strategy is introduced for estimating population size and networked\npopulation characteristics. Sample selection is based on a multi-wave snowball\nsampling design. A generalized stochastic block model is posited for the\npopulation's network graph. Inference is based on a Bayesian data augmentation\nprocedure. Applications are provided to an empirical and simulated populations.\nThe results demonstrate that statistically efficient estimates of the size and\ndistribution of the population can be achieved.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 15:29:05 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 12:48:04 GMT"}, {"version": "v3", "created": "Wed, 26 Jul 2017 01:39:10 GMT"}, {"version": "v4", "created": "Mon, 29 Jul 2019 12:05:08 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Vincent", "Kyle", ""], ["Thompson", "Steve", ""]]}, {"id": "1402.4459", "submitter": "Xingye Qiao", "authors": "Xingye Qiao, Yufeng Liu, J. S. Marron", "title": "Significance Analysis for Pairwise Variable Selection in Classification", "comments": "28 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this article is to select important variables that can\ndistinguish one class of data from another. A marginal variable selection\nmethod ranks the marginal effects for classification of individual variables,\nand is a useful and efficient approach for variable selection. Our focus here\nis to consider the bivariate effect, in addition to the marginal effect. In\nparticular, we are interested in those pairs of variables that can lead to\naccurate classification predictions when they are viewed jointly. To accomplish\nthis, we propose a permutation test called Significance test of Joint Effect\n(SigJEff). In the absence of joint effect in the data, SigJEff is similar or\nequivalent to many marginal methods. However, when joint effects exist, our\nmethod can significantly boost the performance of variable selection. Such\njoint effects can help to provide additional, and sometimes dominating,\nadvantage for classification. We illustrate and validate our approach using\nboth simulated example and a real glioblastoma multiforme data set, which\nprovide promising results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 20:06:06 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Qiao", "Xingye", ""], ["Liu", "Yufeng", ""], ["Marron", "J. S.", ""]]}, {"id": "1402.4539", "submitter": "Xingye Qiao", "authors": "Sungkyu Jung and Xingye Qiao", "title": "A Statistical Approach to Set Classification by Feature Selection with\n  Applications to Classification of Histopathology Images", "comments": "44 pages, 4 figures in the main paper", "journal-ref": null, "doi": "10.1111/biom.12164", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set classification problems arise when classification tasks are based on sets\nof observations as opposed to individual observations. In set classification, a\nclassification rule is trained with $N$ sets of observations, where each set is\nlabeled with class information, and the prediction of a class label is\nperformed also with a set of observations. Data sets for set classification\nappear, for example, in diagnostics of disease based on multiple cell nucleus\nimages from a single tissue. Relevant statistical models for set classification\nare introduced, which motivate a set classification framework based on\ncontext-free feature extraction. By understanding a set of observations as an\nempirical distribution, we employ a data-driven method to choose those features\nwhich contain information on location and major variation. In particular, the\nmethod of principal component analysis is used to extract the features of major\nvariation. Multidimensional scaling is used to represent features as\nvector-valued points on which conventional classifiers can be applied. The\nproposed set classification approaches achieve better classification results\nthan competing methods in a number of simulated data examples. The benefits of\nour method are demonstrated in an analysis of histopathology images of cell\nnuclei related to liver cancer.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 01:27:43 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Jung", "Sungkyu", ""], ["Qiao", "Xingye", ""]]}, {"id": "1402.4624", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin and Anju Kambadur and Aurelie C. Lozano and Ronny\n  Luss", "title": "Sparse Quantile Huber Regression for Efficient and Robust Estimation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider new formulations and methods for sparse quantile regression in\nthe high-dimensional setting. Quantile regression plays an important role in\nmany applications, including outlier-robust exploratory analysis in gene\nselection. In addition, the sparsity consideration in quantile regression\nenables the exploration of the entire conditional distribution of the response\nvariable given the predictors and therefore yields a more comprehensive view of\nthe important predictors. We propose a generalized OMP algorithm for variable\nselection, taking the misfit loss to be either the traditional quantile loss or\na smooth version we call quantile Huber, and compare the resulting greedy\napproaches with convex sparsity-regularized formulations. We apply a recently\nproposed interior point methodology to efficiently solve all convex\nformulations as well as convex subproblems in the generalized OMP setting, pro-\nvide theoretical guarantees of consistent estimation, and demonstrate the\nperformance of our approach using empirical studies of simulated and genomic\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 11:18:32 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Kambadur", "Anju", ""], ["Lozano", "Aurelie C.", ""], ["Luss", "Ronny", ""]]}, {"id": "1402.4882", "submitter": "Bin Guo", "authors": "Song Xi Chen and Bin Guo", "title": "Tests for High Dimensional Generalized Linear Models", "comments": "The research paper was stole by someone last November and illegally\n  submitted to arXiv by a person named gong zi jiang nan. We have asked arXiv\n  to withdraw the unfinished paper [arXiv:1311.4043] and it was removed last\n  December. We have collected enough evidences to identify the person and\n  Peking University has begun to investigate the plagiarizer", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider testing regression coefficients in high dimensional generalized\nlinear models. An investigation of the test of Goeman et al. (2011) is\nconducted, which reveals that if the inverse of the link function is unbounded,\nthe high dimensionality in the covariates can impose adverse impacts on the\npower of the test. We propose a test formation which can avoid the adverse\nimpact of the high dimensionality. When the inverse of the link function is\nbounded such as the logistic or probit regression, the proposed test is as good\nas Goeman et al. (2011)'s test. The proposed tests provide p-values for testing\nsignificance for gene-sets as demonstrated in a case study on an acute\nlymphoblastic leukemia dataset.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 03:57:12 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Chen", "Song Xi", ""], ["Guo", "Bin", ""]]}, {"id": "1402.4887", "submitter": "Roberto D. Pascual-Marqui", "authors": "RD Pascual-Marqui, RJ Biscay, J Bosch-Bayard, D Lehmann, K Kochi, N\n  Yamada, T Kinoshita, N Sadato", "title": "Isolated effective coherence (iCoh): causal information flow excluding\n  indirect paths", "comments": "2014-02-21 pre-print, technical report, KEY Institute for Brain-Mind\n  Research, University of Zurich, et al", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A problem of great interest in real world systems, where multiple time series\nmeasurements are available, is the estimation of the intra-system causal\nrelations. For instance, electric cortical signals are used for studying\nfunctional connectivity between brain areas, their directionality, the direct\nor indirect nature of the connections, and the spectral characteristics (e.g.\nwhich oscillations are preferentially transmitted). The earliest spectral\nmeasure of causality was Akaike's (1968) seminal work on the noise contribution\nratio, reflecting direct and indirect connections. Later, a major breakthrough\nwas the partial directed coherence of Baccala and Sameshima (2001) for direct\nconnections. The simple aim of this study consists of two parts: (1) To expose\na major problem with the partial directed coherence, where it is shown that it\nis affected by irrelevant connections to such an extent that it can\nmisrepresent the frequency response, thus defeating the main purpose for which\nthe measure was developed, and (2) To provide a solution to this problem,\nnamely the \"isolated effective coherence\", which consists of estimating the\npartial coherence under a multivariate auto-regressive model, followed by\nsetting all irrelevant associations to zero, other than the particular\ndirectional association of interest. Simple, realistic, toy examples illustrate\nthe severity of the problem with the partial directed coherence, and the\nsolution achieved by the isolated effective coherence. For the sake of\nreproducible research, the software code implementing the methods discussed\nhere (using lazarus free-pascal \"www.lazarus.freepascal.org\"), including the\ntest data as text files, are freely available at:\nhttps://sites.google.com/site/pascualmarqui/home/icoh-isolated-effective-coherence\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 04:31:33 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2014 01:56:35 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2014 02:08:23 GMT"}, {"version": "v4", "created": "Sat, 1 Mar 2014 02:45:48 GMT"}, {"version": "v5", "created": "Tue, 25 Mar 2014 03:17:49 GMT"}, {"version": "v6", "created": "Mon, 12 May 2014 08:57:48 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Pascual-Marqui", "RD", ""], ["Biscay", "RJ", ""], ["Bosch-Bayard", "J", ""], ["Lehmann", "D", ""], ["Kochi", "K", ""], ["Yamada", "N", ""], ["Kinoshita", "T", ""], ["Sadato", "N", ""]]}, {"id": "1402.5090", "submitter": "Yanxun Xu", "authors": "Yanxun Xu, Peter Mueller, Yuan Yuan, Kamalakar Gulukota and Yuan Ji", "title": "MAD Bayes for Tumor Heterogeneity Feature Allocation with Non-Normal\n  Sampling", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2014.995794", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose small-variance asymptotic approximations for the inference of\ntumor heterogeneity (TH) using next-generation sequencing data. Understanding\nTH is an important and open research problem in biology. The lack of\nappropriate statistical inference is a critical gap in existing methods that\nthe proposed approach aims to fill. We build on a hierarchical model with an\nexponential family likelihood and a feature allocation prior. The proposed\napproach generalizes similar small-variance approximations proposed by Kulis\nand Jordan (2012) and Broderick et.al (2012) for inference with Dirichlet\nprocess mixture and Indian buffet prior models under normal sampling. We show\nthat the new algorithm can successfully recover latent structures of different\nsubclones and is also magnitude faster than available Markov chain Monte Carlo\nsamplers, the latter often practically infeasible for high-dimensional genomics\ndata. The proposed approach is scalable, simple to implement and benefits from\nthe flexibility of Bayesian nonparametric models. More importantly, it provides\na useful tool for the biological community for estimating cell subtypes in\ntumor samples.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 18:15:56 GMT"}, {"version": "v2", "created": "Mon, 3 Mar 2014 00:21:38 GMT"}, {"version": "v3", "created": "Mon, 31 Mar 2014 01:24:19 GMT"}, {"version": "v4", "created": "Tue, 2 Dec 2014 23:56:49 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Xu", "Yanxun", ""], ["Mueller", "Peter", ""], ["Yuan", "Yuan", ""], ["Gulukota", "Kamalakar", ""], ["Ji", "Yuan", ""]]}, {"id": "1402.5103", "submitter": "Matthieu Marbac", "authors": "Matthieu Marbac (INRIA Lille - Nord Europe), Christophe Biernacki\n  (INRIA Lille - Nord Europe), Vincent Vandewalle (INRIA Lille - Nord Europe)", "title": "Finite mixture model of conditional dependencies modes to cluster\n  categorical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parsimonious extension of the classical latent class model to\ncluster categorical data by relaxing the class conditional independence\nassumption. Under this new mixture model, named Conditional Modes Model,\nvariables are grouped into conditionally independent blocks. The corresponding\nblock distribution is a parsimonious multinomial distribution where the few\nfree parameters correspond to the most likely modality crossings, while the\nremaining probability mass is uniformly spread over the other modality\ncrossings. Thus, the proposed model allows to bring out the intra-class\ndependency between variables and to summarize each class by a few\ncharacteristic modality crossings. The model selection is performed via a\nMetropolis-within-Gibbs sampler to overcome the computational intractability of\nthe block structure search. As this approach involves the computation of the\nintegrated complete-data likelihood, we propose a new method (exact for the\ncontinuous parameters and approximated for the discrete ones) which avoids the\nbiases of the \\textsc{bic} criterion pointed out by our experiments. Finally,\nthe parameters are only estimated for the best model via an \\textsc{em}\nalgorithm. The characteristics of the new model are illustrated on simulated\ndata and on two biological data sets. These results strengthen the idea that\nthis simple model allows to reduce biases involved by the conditional\nindependence assumption and gives meaningful parameters. Both applications were\nperformed with the R package \\texttt{CoModes}\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 19:04:32 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Marbac", "Matthieu", "", "INRIA Lille - Nord Europe"], ["Biernacki", "Christophe", "", "INRIA Lille - Nord Europe"], ["Vandewalle", "Vincent", "", "INRIA Lille - Nord Europe"]]}, {"id": "1402.5161", "submitter": "Roberto Rossi", "authors": "Roberto Rossi and Steven Prestwich and S. Armagan Tarim", "title": "Statistical Constraints", "comments": null, "journal-ref": "Proceedings of the 21st European Conference on Artificial\n  Intelligence, ECAI 2014, August 18-22, 2014, Prague, Czech Republic, IOS\n  Press, Volume 263, pp. 777-782, 2014", "doi": "10.3233/978-1-61499-419-0-777", "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce statistical constraints, a declarative modelling tool that links\nstatistics and constraint programming. We discuss two statistical constraints\nand some associated filtering algorithms. Finally, we illustrate applications\nto standard problems encountered in statistics and to a novel inspection\nscheduling problem in which the aim is to find inspection plans with desirable\nstatistical properties.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 22:29:59 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2014 21:38:16 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2014 20:16:41 GMT"}, {"version": "v4", "created": "Sun, 1 Jun 2014 14:39:30 GMT"}, {"version": "v5", "created": "Thu, 14 Aug 2014 21:33:04 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Rossi", "Roberto", ""], ["Prestwich", "Steven", ""], ["Tarim", "S. Armagan", ""]]}, {"id": "1402.5190", "submitter": "Lixing Zhu", "authors": "Zhou Yu, Yuexiao Dong and Li-Xing Zhu", "title": "Trace Pursuit: A General Framework for Model-Free Variable Selection", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose trace pursuit for model-free variable selection under the\nsufficient dimension reduction paradigm. Two distinct algorithms are proposed:\nstepwise trace pursuit and forward trace pursuit. Stepwise trace pursuit\nachieves selection consistency with fixed p, and is readily applicable in the\nchallenging setting with p>n. Forward trace pursuit can serve as an initial\nscreening step to speed up the computation in the case of ultrahigh\ndimensionality. The screening consistency property of forward trace pursuit\nbased on sliced inverse regression is established. Finite sample performances\nof trace pursuit and other model-free variable selection methods are compared\nthrough numerical studies.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 02:55:06 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Yu", "Zhou", ""], ["Dong", "Yuexiao", ""], ["Zhu", "Li-Xing", ""]]}, {"id": "1402.5264", "submitter": "Eisa Mahmoudi", "authors": "Eisa Mahmoudi, Afsaneh Sepahdar, Artur Lemonte", "title": "Exponentiated Weibull-logarithmic Distribution: Model, Properties and\n  Applications", "comments": "arXiv admin note: substantial text overlap with arXiv:1212.5586,\n  arXiv:1212.5613, arXiv:1206.4008; and text overlap with arXiv:0809.2703 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new four-parameter generalization of the\nexponentiated Weibull (EW) distribution, called the exponentiated\nWeibull-logarithmic (EWL) distribution, which obtained by compounding EW and\nlogarithmic distributions. The new distribution arises on a latent\ncomplementary risks scenario, in which the lifetime associated with a\nparticular risk is not observable; rather, we observe only the maximum lifetime\nvalue among all risks. The distribution exhibits decreasing, increasing,\nunimodal and bathtub-shaped hazard rate functions, depending on its parameters\nand contains several lifetime sub-models such as: generalized\nexponential-logarithmic (GEL), complementary Weibull-logarithmic (CWL),\ncomplementary exponential-logarithmic (CEL), exponentiated Rayleigh-logarithmic\n(ERL) and Rayleigh-logarithmic (RL) distributions. We study various properties\nof the new distribution and provide numerical examples to show the flexibility\nand potentiality of the model.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 11:48:41 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Mahmoudi", "Eisa", ""], ["Sepahdar", "Afsaneh", ""], ["Lemonte", "Artur", ""]]}, {"id": "1402.5282", "submitter": "Eisa Mahmoudi", "authors": "Eisa Mahmoudi and Ali Akbar Jafari", "title": "The Compound Class of Linear Failure Rate-Power Series Distributions:\n  Model, Properties and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce in this paper a new class of distributions which generalizes the\nlinear failure rate (LFR) distribution and is obtained by compounding the LFR\ndistribution and power series (PS) class of distributions. This new class of\ndistributions is called the linear failure rate-power series (LFRPS)\ndistributions and contains some new distributions such as linear failure rate\ngeometric (LFRG) distribution, linear failure rate Poisson (LFRP) distribution,\nlinear failure rate logarithmic (LFRL) distribution, linear failure rate\nbinomial (LFRB) distribution and Raylight-power series (RPS) class of\ndistributions. Some former works such as exponential-power series (EPS) class\nof distributions, exponential geometric (EG) distribution, exponential Poisson\n(EP) distribution and exponential logarithmic (EL) distribution are special\ncases of the new proposed model.\n  The ability of the LFRPS class of distributions is in covering five possible\nhazard rate function i.e., increasing, decreasing, upside-down bathtub\n(unimodal), bathtub and increasing-decreasing-increasing shaped. Several\nproperties of the LFRPS distributions such as moments, maximum likelihood\nestimation procedure via an EM-algorithm and inference for a large sample, are\ndiscussed in this paper. In order to show the flexibility and potentiality of\nthe new class of distributions, the fitted results of the new class of\ndistributions and some its submodels are compared using a real data set.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 12:44:37 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Mahmoudi", "Eisa", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "1402.5384", "submitter": "Nirian Martin", "authors": "Nirian Mart\\'in, Raquel Mata, Leandro Pardo", "title": "Phi-divergence statistics for the likelihood ratio order: an approach\n  based on log-linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When some treatments are ordered according to the categories of an ordinal\ncategorical variable (e.g., extent of side effects) in a monotone order, one\nmight be interested in knowing wether the treatments are equally effective or\nnot. One way to do that is to test if the likelihood ratio order is strictly\nverified. A method based on log-linear models is derived to make statistical\ninference and phi-divergence test-statistics are proposed for the test of\ninterest. Focussed on loglinear modeling, the theory associated with the\nasymptotic distribution of the phi-divergence test-statistics is developed. An\nillustrative example motivates the procedure and a simulation study for small\nand moderate sample sizes shows that it is possible to find phi-divergence\ntest-statistic with an exact size closer to nominal size and higher power in\ncomparison with the classical likelihood ratio.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 19:13:03 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Mart\u00edn", "Nirian", ""], ["Mata", "Raquel", ""], ["Pardo", "Leandro", ""]]}, {"id": "1402.5397", "submitter": "Adam Kapelner", "authors": "Justin Bleich and Adam Kapelner", "title": "Bayesian Additive Regression Trees With Parametric Models of\n  Heteroskedasticity", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We incorporate heteroskedasticity into Bayesian Additive Regression Trees\n(BART) by modeling the log of the error variance parameter as a linear function\nof prespecified covariates. Under this scheme, the Gibbs sampling procedure for\nthe original sum-of- trees model is easily modified, and the parameters for the\nvariance model are updated via a Metropolis-Hastings step. We demonstrate the\npromise of our approach by providing more appropriate posterior predictive\nintervals than homoskedastic BART in heteroskedastic settings and demonstrating\nthe model's resistance to overfitting. Our implementation will be offered in an\nupcoming release of the R package bartMachine.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 19:58:59 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Bleich", "Justin", ""], ["Kapelner", "Adam", ""]]}, {"id": "1402.5568", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald and Alfred O. Hero III", "title": "Regularized Block Toeplitz Covariance Matrix Estimation via Kronecker\n  Product Expansions", "comments": "To appear at IEEE SSP 2014 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the estimation of spatio-temporal covariance\nmatrices in the low sample non-Gaussian regime. We impose covariance structure\nin the form of a sum of Kronecker products decomposition (Tsiligkaridis et al.\n2013, Greenewald et al. 2013) with diagonal correction (Greenewald et al.),\nwhich we refer to as DC-KronPCA, in the estimation of multiframe covariance\nmatrices. This paper extends the approaches of (Tsiligkaridis et al.) in two\ndirections. First, we modify the diagonally corrected method of (Greenewald et\nal.) to include a block Toeplitz constraint imposing temporal stationarity\nstructure. Second, we improve the conditioning of the estimate in the very low\nsample regime by using Ledoit-Wolf type shrinkage regularization similar to\n(Chen, Hero et al. 2010). For improved robustness to heavy tailed\ndistributions, we modify the KronPCA to incorporate robust shrinkage estimation\n(Chen, Hero et al. 2011). Results of numerical simulations establish benefits\nin terms of estimation MSE when compared to previous methods. Finally, we apply\nour methods to a real-world network spatio-temporal anomaly detection problem\nand achieve superior results.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 01:15:50 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 19:27:14 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1402.5596", "submitter": "Jason Lee", "authors": "Jason D Lee and Jonathan E Taylor", "title": "Exact Post Model Selection Inference for Marginal Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for post model selection inference, via marginal\nscreening, in linear regression. At the core of this framework is a result that\ncharacterizes the exact distribution of linear functions of the response $y$,\nconditional on the model being selected (``condition on selection\" framework).\nThis allows us to construct valid confidence intervals and hypothesis tests for\nregression coefficients that account for the selection procedure. In contrast\nto recent work in high-dimensional statistics, our results are exact\n(non-asymptotic) and require no eigenvalue-like assumptions on the design\nmatrix $X$. Furthermore, the computational cost of marginal regression,\nconstructing confidence intervals and hypothesis testing is negligible compared\nto the cost of linear regression, thus making our methods particularly suitable\nfor extremely large datasets. Although we focus on marginal screening to\nillustrate the applicability of the condition on selection framework, this\nframework is much more broadly applicable. We show how to apply the proposed\nframework to several other selection procedures including orthogonal matching\npursuit, non-negative least squares, and marginal screening+Lasso.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 10:30:21 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2014 00:28:21 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Lee", "Jason D", ""], ["Taylor", "Jonathan E", ""]]}, {"id": "1402.5609", "submitter": "Rajesh  Singh", "authors": "Prayas Sharma and Rajesh Singh", "title": "Efficient class of estimators for population median using auxiliary\n  information", "comments": "16 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article suggests an efficient class of estimators of population median\nof the study variable using an auxiliary variable. Asymptotic expressions of\nbias and mean square error of the proposed class of estimators have been\nobtained. Asymptotic optimum estimator has been investigated along with its\napproximate mean square error. We have shown that proposed class of estimator\nis more efficient than estimator considered by Srivastava (1967), Gross (1980),\nKuk and Mak (1989) Singh et al. (2003b), Al and Chingi (2009) and Singh and\nSolanki (2013). In addition theoretical findings are supported by an empirical\nstudy based on two populations to show the superiority of the constructed\nestimators over others.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 14:08:24 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Sharma", "Prayas", ""], ["Singh", "Rajesh", ""]]}, {"id": "1402.5640", "submitter": "Chris Oates", "authors": "Chris J. Oates, Sach Mukherjee", "title": "Joint Structure Learning of Multiple Non-Exchangeable Networks", "comments": "To appear in Proceedings of the Seventeenth International Conference\n  on Artificial Intelligence and Statistics (AISTATS)", "journal-ref": "JMLR W&CP 33 :687-695, 2014", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have recently been developed for joint structure learning of\nmultiple (related) graphical models or networks. These methods treat individual\nnetworks as exchangeable, such that each pair of networks are equally\nencouraged to have similar structures. However, in many practical applications,\nexchangeability in this sense may not hold, as some pairs of networks may be\nmore closely related than others, for example due to group and sub-group\nstructure in the data. Here we present a novel Bayesian formulation that\ngeneralises joint structure learning beyond the exchangeable case. In addition\nto a general framework for joint learning, we (i) provide a novel default prior\nover the joint structure space that requires no user input; (ii) allow for\nlatent networks; (iii) give an efficient, exact algorithm for the case of time\nseries data and dynamic Bayesian networks. We present empirical results on\nnon-exchangeable populations, including a real data example from biology, where\ncell-line-specific networks are related according to genomic features.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 17:48:16 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Oates", "Chris J.", ""], ["Mukherjee", "Sach", ""]]}, {"id": "1402.5655", "submitter": "Theresa R. Smith", "authors": "Theresa R. Smith, Jon Wakefield, Adrian Dobra", "title": "Restricted Covariance Priors with Applications in Spatial Statistics", "comments": "Published at http://dx.doi.org/10.1214/14-BA927 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 4, 965-990", "doi": "10.1214/14-BA927", "report-no": "VTeX-BA-BA927", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian model for area-level count data that uses Gaussian\nrandom effects with a novel type of G-Wishart prior on the inverse\nvariance--covariance matrix. Specifically, we introduce a new distribution\ncalled the truncated G-Wishart distribution that has support over precision\nmatrices that lead to positive associations between the random effects of\nneighboring regions while preserving conditional independence of\nnon-neighboring regions. We describe Markov chain Monte Carlo sampling\nalgorithms for the truncated G-Wishart prior in a disease mapping context and\ncompare our results to Bayesian hierarchical models based on intrinsic\nautoregression priors. A simulation study illustrates that using the truncated\nG-Wishart prior improves over the intrinsic autoregressive priors when there\nare discontinuities in the disease risk surface. The new model is applied to an\nanalysis of cancer incidence data in Washington State.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 19:32:57 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 13:55:57 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Smith", "Theresa R.", ""], ["Wakefield", "Jon", ""], ["Dobra", "Adrian", ""]]}, {"id": "1402.5723", "submitter": "Jingjing Yang", "authors": "Jingjing Yang, Hongxiao Zhu, Taeryon Choi, Dennis D. Cox", "title": "Smoothing and mean-covariance estimation of functional data with a\n  Bayesian hierarchical model", "comments": "Submitted to Bayesian Analysis", "journal-ref": "Bayesian Analysis. Volume 11, Number 3 (2016), 649-670", "doi": "10.1214/15-BA967", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data, with basic observational units being functions (e.g.,\ncurves, surfaces) varying over a continuum, are frequently encountered in\nvarious applications. While many statistical tools have been developed for\nfunctional data analysis, the issue of smoothing all functional observations\nsimultaneously is less studied. Existing methods often focus on smoothing each\nindividual function separately, at the risk of removing important systematic\npatterns common across functions. We propose a nonparametric Bayesian approach\nto smooth all functional observations simultaneously and nonparametrically. In\nthe proposed approach, we assume that the functional observations are\nindependent Gaussian processes subject to a common level of measurement errors,\nenabling the borrowing of strength across all observations. Unlike most\nGaussian process regression models that rely on pre-specified structures for\nthe covariance kernel, we adopt a hierarchical framework by assuming a Gaussian\nprocess prior for the mean function and an Inverse-Wishart process prior for\nthe covariance function. These prior assumptions induce an automatic\nmean-covariance estimation in the posterior inference in addition to the\nsimultaneous smoothing of all observations. Such a hierarchical framework is\nflexible enough to incorporate functional data with different characteristics,\nincluding data measured on either common or uncommon grids, and data with\neither stationary or nonstationary covariance structures. Simulations and real\ndata analysis demonstrate that, in comparison with alternative methods, the\nproposed Bayesian approach achieves better smoothing accuracy and comparable\nmean-covariance estimation results. Furthermore, it can successfully retain the\nsystematic patterns in the functional observations that are usually neglected\nby the existing functional data analyses based on individual-curve smoothing.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 04:54:14 GMT"}, {"version": "v2", "created": "Thu, 17 Jul 2014 18:42:50 GMT"}, {"version": "v3", "created": "Mon, 3 Nov 2014 22:24:52 GMT"}, {"version": "v4", "created": "Mon, 22 Dec 2014 18:13:05 GMT"}, {"version": "v5", "created": "Tue, 7 Jul 2015 15:59:03 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Yang", "Jingjing", ""], ["Zhu", "Hongxiao", ""], ["Choi", "Taeryon", ""], ["Cox", "Dennis D.", ""]]}, {"id": "1402.5724", "submitter": "Hidetoshi Matsui", "authors": "Hidetoshi Matsui", "title": "Model selection criteria for nonlinear mixed effects modeling", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider constructing model selection criteria for evaluating nonlinear\nmixed effects models via basis expansions. Mean functions and random functions\nin the mixed effects model are expressed by basis expansions, then they are\nestimated by the maximum likelihood method. In order to select numbers of basis\nwe derive a Bayesian model selection criterion for evaluating nonlinear mixed\neffects models estimated by the maximum likelihood method. Simulation results\nshows the effectiveness of the mixed effects modeling.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 05:12:20 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Matsui", "Hidetoshi", ""]]}, {"id": "1402.5847", "submitter": "Toshihiro Hirano", "authors": "Toshihiro Hirano", "title": "Modified Linear Projection for Large Spatial Data Sets", "comments": "29 pages, 5 figures, 4 tables", "journal-ref": "Communications in Statistics - Simulation and Computation (2017),\n  Vol.46, 870-889", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in engineering techniques for spatial data collection\nsuch as geographic information systems have resulted in an increasing need for\nmethods to analyze large spatial data sets. These sorts of data sets can be\nfound in various fields of the natural and social sciences. However, model\nfitting and spatial prediction using these large spatial data sets are\nimpractically time-consuming, because of the necessary matrix inversions.\nVarious methods have been developed to deal with this problem, including a\nreduced rank approach and a sparse matrix approximation. In this paper, we\npropose a modification to an existing reduced rank approach to capture both the\nlarge- and small-scale spatial variations effectively. We have used simulated\nexamples and an empirical data analysis to demonstrate that our proposed\napproach consistently performs well when compared with other methods. In\nparticular, the performance of our new method does not depend on the dependence\nproperties of the spatial covariance functions.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 15:04:18 GMT"}, {"version": "v2", "created": "Sun, 13 Jul 2014 15:19:55 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Hirano", "Toshihiro", ""]]}, {"id": "1402.5979", "submitter": "Renato J Cintra", "authors": "V. A. Coutinho, R. J. Cintra, F. M. Bayer, S. Kulasekera, A.\n  Madanayake", "title": "A Multiplierless Pruned DCT-like Transformation for Image and Video\n  Compression that Requires 10 Additions Only", "comments": "13 pages, 4 figures, 5 tables", "journal-ref": "Journal of Real-Time Image Processing, August 2016, Volume 12,\n  Issue 2, pp 247-255", "doi": "10.1007/s11554-015-0492-8", "report-no": null, "categories": "cs.MM cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multiplierless pruned approximate 8-point discrete cosine transform (DCT)\nrequiring only 10 additions is introduced. The proposed algorithm was assessed\nin image and video compression, showing competitive performance with\nstate-of-the-art methods. Digital implementation in 45 nm CMOS technology up to\nplace-and-route level indicates clock speed of 288 MHz at a 1.1 V supply. The\n8x8 block rate is 36 MHz.The DCT approximation was embedded into HEVC reference\nsoftware; resulting video frames, at up to 327 Hz for 8-bit RGB HEVC, presented\nnegligible image degradation.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 21:04:41 GMT"}, {"version": "v2", "created": "Sun, 11 Dec 2016 19:23:57 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Coutinho", "V. A.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""]]}, {"id": "1402.6034", "submitter": "Renato J Cintra", "authors": "R. J. Cintra and F. M. Bayer", "title": "A DCT Approximation for Image Compression", "comments": "10 pages, 6 figures", "journal-ref": "IEEE Signal Processing Letters, 18(10):579-582, October 2011", "doi": "10.1109/LSP.2011.2163394", "report-no": null, "categories": "cs.MM cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An orthogonal approximation for the 8-point discrete cosine transform (DCT)\nis introduced. The proposed transformation matrix contains only zeros and ones;\nmultiplications and bit-shift operations are absent. Close spectral behavior\nrelative to the DCT was adopted as design criterion. The proposed algorithm is\nsuperior to the signed discrete cosine transform. It could also outperform\nstate-of-the-art algorithms in low and high image compression scenarios,\nexhibiting at the same time a comparable computational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 02:49:01 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""]]}, {"id": "1402.6035", "submitter": "Minh-Ngoc Tran", "authors": "M.-N. Tran, C. Strickland, M. K. Pitt, R. Kohn", "title": "Annealed Important Sampling for Models with Latent Variables", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with Bayesian inference when the likelihood is\nanalytically intractable but can be unbiasedly estimated. We propose an\nannealed importance sampling procedure for estimating expectations with respect\nto the posterior. The proposed algorithm is useful in cases where finding a\ngood proposal density is challenging, and when estimates of the marginal\nlikelihood are required. The effect of likelihood estimation is investigated,\nand the results provide guidelines on how to set up the precision of the\nlikelihood estimation in order to optimally implement the procedure. The\nmethodological results are empirically demonstrated in several simulated and\nreal data examples.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 02:49:13 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Tran", "M. -N.", ""], ["Strickland", "C.", ""], ["Pitt", "M. K.", ""], ["Kohn", "R.", ""]]}, {"id": "1402.6089", "submitter": "David Degras", "authors": "David Degras and Martin A. Lindquist", "title": "A hierarchical model for simultaneous detection and estimation in\n  multi-subject fMRI Studies", "comments": null, "journal-ref": "NeuroImage 98, 2014, p. 61-72", "doi": "10.1016/j.neuroimage.2014.04.052", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new hierarchical model for the simultaneous\ndetection of brain activation and estimation of the shape of the hemodynamic\nresponse in multi-subject fMRI studies. The proposed approach circumvents a\nmajor stumbling block in standard multi-subject fMRI data analysis, in that it\nboth allows the shape of the hemodynamic response function to vary across\nregion and subjects, while still providing a straightforward way to estimate\npopulation-level activation. An efficient estimation algorithm is presented, as\nis an inferential framework that not only allows for tests of activation, but\nalso for tests for deviations from some canonical shape. The model is validated\nthrough simulations and application to a multi-subject fMRI study of thermal\npain.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 08:35:14 GMT"}, {"version": "v2", "created": "Sat, 29 Mar 2014 20:56:11 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Degras", "David", ""], ["Lindquist", "Martin A.", ""]]}, {"id": "1402.6118", "submitter": "James Watson", "authors": "James Watson and Chris Holmes", "title": "Approximate Models and Robust Decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decisions based partly or solely on predictions from probabilistic models may\nbe sensitive to model misspecification. Statisticians are taught from an early\nstage that \"all models are wrong\", but little formal guidance exists on how to\nassess the impact of model approximation on decision making, or how to proceed\nwhen optimal actions appear sensitive to model fidelity. This article presents\nan overview of recent developments across different disciplines to address\nthis. We review diagnostic techniques, including graphical approaches and\nsummary statistics, to help highlight decisions made through minimised expected\nloss that are sensitive to model misspecification. We then consider formal\nmethods for decision making under model misspecification by quantifying\nstability of optimal actions to perturbations to the model within a\nneighbourhood of model space. This neighbourhood is defined in either one of\ntwo ways. Firstly, in a strong sense via an information (Kullback-Leibler)\ndivergence around the approximating model. Or using a nonparametric model\nextension, again centred at the approximating model, in order to `average out'\nover possible misspecifications. This is presented in the context of recent\nwork in the robust control, macroeconomics and financial mathematics\nliterature. We adopt a Bayesian approach throughout although the methods are\nagnostic to this position.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 10:20:04 GMT"}, {"version": "v2", "created": "Sun, 1 Feb 2015 17:44:48 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2015 14:09:57 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Watson", "James", ""], ["Holmes", "Chris", ""]]}, {"id": "1402.6149", "submitter": "Zeng Li", "authors": "Zeng Li, Guangming Pan and Jianfeng Yao", "title": "On singular value distribution of large dimensional auto-covariance\n  matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $(\\varepsilon_j)_{j\\geq 0}$ be a sequence of independent $p-$dimensional\nrandom vectors and $\\tau\\geq1$ a given integer. From a sample\n$\\varepsilon_1,\\cdots,\\varepsilon_{T+\\tau-1},\\varepsilon_{T+\\tau}$ of the\nsequence, the so-called lag $-\\tau$ auto-covariance matrix is\n$C_{\\tau}=T^{-1}\\sum_{j=1}^T\\varepsilon_{\\tau+j}\\varepsilon_{j}^t$. When the\ndimension $p$ is large compared to the sample size $T$, this paper establishes\nthe limit of the singular value distribution of $C_\\tau$ assuming that $p$ and\n$T$ grow to infinity proportionally and the sequence satisfies a Lindeberg\ncondition on fourth order moments. Compared to existing asymptotic results on\nsample covariance matrices developed in random matrix theory, the case of an\nauto-covariance matrix is much more involved due to the fact that the summands\nare dependent and the matrix $C_\\tau$ is not symmetric. Several new techniques\nare introduced for the derivation of the main theorem.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 12:32:28 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Li", "Zeng", ""], ["Pan", "Guangming", ""], ["Yao", "Jianfeng", ""]]}, {"id": "1402.6257", "submitter": "Christian P. Robert", "authors": "Kaniav Kamary (CEREMADE, Universit\\'e Paris-Dauphine) and Christian P.\n  Robert (CEREMADE, Universit\\'e Paris-Dauphine and University of Warwick)", "title": "Reflecting about Selecting Noninformative Priors", "comments": "15 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the critical review of Seaman et al. (2012), we reflect on what is\npresumably the most essential aspect of Bayesian statistics, namely the\nselection of a prior density. In some cases, Bayesian inference remains fairly\nstable under a large range of noninformative prior distributions. However, as\ndiscussed by \\citet{Hd}, there may also be unintended consequences of a choice\nof a noninformative prior and, these authors consider this problem ignored in\nBayesian studies. As they based their argumentation on four examples, we\nreassess these examples and their Bayesian processing via different prior\nchoices. Our conclusion is to lower the degree of worry about the impact of the\nprior, exhibiting an overall stability of the posterior distributions. We thus\nconsider that the warnings of Seaman et al. (2012), while commendable, do not\njeopardize the use of most noninformative priors.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 17:53:54 GMT"}, {"version": "v2", "created": "Fri, 14 Mar 2014 11:32:31 GMT"}, {"version": "v3", "created": "Tue, 22 Jul 2014 09:31:31 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Kamary", "Kaniav", "", "CEREMADE, Universit\u00e9 Paris-Dauphine"], ["Robert", "Christian P.", "", "CEREMADE, Universit\u00e9 Paris-Dauphine and University of Warwick"]]}, {"id": "1402.6350", "submitter": "Matthew Plumlee", "authors": "Matthew Plumlee", "title": "Fast prediction of deterministic functions using sparse grid\n  experimental designs", "comments": "This document is in-press at the Journal of the American Statistical\n  Association. A MATLAB package released along with this document is available\n  at\n  http://www.mathworks.com/matlabcentral/fileexchange/45668-sparse-grid-designs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random field models have been widely employed to develop a predictor of an\nexpensive function based on observations from an experiment. The traditional\nframework for developing a predictor with random field models can fail due to\nthe computational burden it requires. This problem is often seen in cases where\nthe input of the expensive function is high dimensional. While many previous\nworks have focused on developing an approximative predictor to resolve these\nissues, this article investigates a different solution mechanism. We\ndemonstrate that when a general set of designs is employed, the resulting\npredictor is quick to compute and has reasonable accuracy. The fast computation\nof the predictor is made possible through an algorithm proposed by this work.\nThis paper also demonstrates methods to quickly evaluate the likelihood of the\nobservations and describes some fast maximum likelihood estimates for unknown\nparameters of the random field. The computational savings can be several orders\nof magnitude when the input is located in a high dimensional space. Beyond the\nfast computation of the predictor, existing research has demonstrated that a\nsubset of these designs generate predictors that are asymptotically efficient.\nThis work details some empirical comparisons to the more common space-filling\ndesigns that verify the designs are competitive in terms of resulting\nprediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 21:17:34 GMT"}, {"version": "v2", "created": "Sat, 31 May 2014 20:48:32 GMT"}, {"version": "v3", "created": "Wed, 26 Nov 2014 21:23:05 GMT"}, {"version": "v4", "created": "Thu, 4 Dec 2014 00:07:44 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Plumlee", "Matthew", ""]]}, {"id": "1402.6397", "submitter": "Jamie Oaks", "authors": "Jamie R. Oaks, Charles W. Linkem and Jeet Sukumaran", "title": "Implications of uniformly distributed, empirically informed priors for\n  phylogeographical model selection: A reply to Hickerson et al", "comments": "24 pages, 4 figures, 1 table, 14 pages of supporting information with\n  10 supporting figures", "journal-ref": null, "doi": "10.1111/evo.12523", "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing that a set of population-splitting events occurred at the same\ntime can be a potentially persuasive argument that a common process affected\nthe populations. Oaks et al. (2013) assessed the ability of an\napproximate-Bayesian method (msBayes) to estimate such a pattern of\nsimultaneous divergence across taxa, to which Hickerson et al. (2014)\nresponded. Both papers agree the method is sensitive to prior assumptions and\noften erroneously supports shared divergences; the papers differ about the\nexplanation and solution. Oaks et al. (2013) suggested the method's behavior is\ncaused by the strong weight of uniform priors on divergence times leading to\nsmaller marginal likelihoods of models with more divergence-time parameters\n(Hypothesis 1); they proposed alternative priors to avoid strongly weighted\nposteriors. Hickerson et al. (2014) suggested numerical approximation error\ncauses msBayes analyses to be biased toward models of clustered divergences\n(Hypothesis 2); they proposed using narrow, empirical uniform priors. Here, we\ndemonstrate that the approach of Hickerson et al. (2014) does not mitigate the\nmethod's tendency to erroneously support models of clustered divergences, and\noften excludes the true parameter values. Our results also show that the\ntendency of msBayes analyses to support models of shared divergences is\nprimarily due to Hypothesis 1. This series of papers demonstrate that if our\nprior assumptions place too much weight in unlikely regions of parameter space\nsuch that the exact posterior supports the wrong model of evolutionary history,\nno amount of computation can rescue our inference. Fortunately, more flexible\ndistributions that accommodate prior uncertainty about parameters without\nplacing excessive weight in vast regions of parameter space with low likelihood\nincrease the method's robustness and power to detect temporal variation in\ndivergences.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 02:29:29 GMT"}, {"version": "v2", "created": "Mon, 24 Nov 2014 22:41:08 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Oaks", "Jamie R.", ""], ["Linkem", "Charles W.", ""], ["Sukumaran", "Jeet", ""]]}, {"id": "1402.6455", "submitter": "Shuichi Kawano", "authors": "Shuichi Kawano, Hironori Fujisawa, Toyoyuki Takada, Toshihiko\n  Shiroishi", "title": "Sparse principal component regression with adaptive loading", "comments": "24 pages", "journal-ref": "Computational Statistics & Data Analysis 89 (2015) 192-203", "doi": "10.1016/j.csda.2015.03.016", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component regression (PCR) is a two-stage procedure that selects\nsome principal components and then constructs a regression model regarding them\nas new explanatory variables. Note that the principal components are obtained\nfrom only explanatory variables and not considered with the response variable.\nTo address this problem, we propose the sparse principal component regression\n(SPCR) that is a one-stage procedure for PCR. SPCR enables us to adaptively\nobtain sparse principal component loadings that are related to the response\nvariable and select the number of principal components simultaneously. SPCR can\nbe obtained by the convex optimization problem for each of parameters with the\ncoordinate descent algorithm. Monte Carlo simulations and real data analyses\nare performed to illustrate the effectiveness of SPCR.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 08:50:16 GMT"}, {"version": "v2", "created": "Tue, 8 Apr 2014 03:10:22 GMT"}, {"version": "v3", "created": "Tue, 8 Jul 2014 07:39:43 GMT"}, {"version": "v4", "created": "Sat, 1 Nov 2014 03:02:23 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Kawano", "Shuichi", ""], ["Fujisawa", "Hironori", ""], ["Takada", "Toyoyuki", ""], ["Shiroishi", "Toshihiko", ""]]}, {"id": "1402.6536", "submitter": "M{\\aa}ns Thulin", "authors": "M{\\aa}ns Thulin", "title": "On split sample and randomized confidence intervals for binomial\n  proportions", "comments": null, "journal-ref": "Statistics and Probability Letters, 92, 65-71 (2014)", "doi": "10.1016/j.spl.2014.05.005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Split sample methods have recently been put forward as a way to reduce the\ncoverage oscillations that haunt confidence intervals for parameters of lattice\ndistributions, such as the binomial and Poisson distributions. We study split\nsample intervals in the binomial setting, showing that these intervals can be\nviewed as being based on adding discrete random noise to the data. It is shown\nthat they can be improved upon by using noise with a continuous distribution\ninstead, regardless of whether the randomization is determined by the data or\nan external source of randomness. We compare split sample intervals to the\nrandomized Stevens interval, which removes the coverage oscillations\ncompletely, and find the latter interval to have several advantages.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 13:26:13 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Thulin", "M\u00e5ns", ""]]}, {"id": "1402.6574", "submitter": "Nirian Mart\\'in", "authors": "Nirian Mart\\'in, Raquel Mata, Leandro Pardo", "title": "Comparing two treatments in terms of the likelihood ratio order", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper new families of test statistics are introduced and studied for\nthe problem of comparing two treatments in terms of the likelihood ratio order.\nThe considered families are based on phi-divergence measures and arise as\nnatural extensions of the classical likelihood ratio test and Pearson test\nstatistics. It is proven that their asymptotic distribution is a common chi-bar\nrandom variable. An illustrative example is presented and the performance of\nthese statistics is analysed through a simulation study. Through a simulation\nstudy it is shown that, for most of the proposed scenarios adjusted to be small\nor moderate, some members of this new family of test-statistic display clearly\nbetter performance with respect to the power in comparison to the classical\nlikelihood ratio and the Pearson's chi-square test while the exact size remains\nclosed to the nominal size. In view of the exact powers and significance\nlevels, the study also shows that the Wilcoxon test-statistic is not as good as\nthe two classical test-statistics.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 15:36:52 GMT"}, {"version": "v2", "created": "Wed, 26 Mar 2014 21:11:58 GMT"}, {"version": "v3", "created": "Wed, 29 Oct 2014 15:12:25 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Mart\u00edn", "Nirian", ""], ["Mata", "Raquel", ""], ["Pardo", "Leandro", ""]]}, {"id": "1402.6717", "submitter": "Nirian Martin", "authors": "Nirian Mart\\'in, Raquel Mata, Leando Pardo", "title": "Wald type and Phi-divergence based test-statistics for isotonic binomial\n  proportions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper new test statistics are introduced and studied for the\nimportant problem of testing hypothesis that involves inequality constraint on\nproportions when the sample comes from independent binomial random variables:\nWald type and phi-divergence based test-statistics. As a particular case of\nphi-divergence based test-statistics, the classical likelihood ratio test is\nconsidered. An illustrative example is given and the performance of all of them\nfor small and moderate sample sizes is analyzed in an extensive simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 21:31:04 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Mart\u00edn", "Nirian", ""], ["Mata", "Raquel", ""], ["Pardo", "Leando", ""]]}, {"id": "1402.6738", "submitter": "Nirian Martin", "authors": "Nirian Mart\\'in, Raquel Mata", "title": "An efficient asymptotic approach for testing monotone proportions\n  assuming an underlying logit based order dose-response model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an underlying logit based order dose-response model is considered with\nsmall or moderate sample sizes, the Cochran-Armitage (CA) test represents the\nmost efficient test in the framework of the test-statistics applied with\nasymptotic distributions for testing monotone proportions. The Wald and\nlikelihood ratio (LR) test have much worse behaviour in type error I in\ncomparison with the CA test. It suffers, however, from the weakness of not\nmaintaining the nominal size. In this paper a family of test-statistics based\non {\\phi}-divergence measures is proposed and their asymptotic distribution\nunder the null hypothesis is obtained either for one-sided or two-sided\nhypothesis testing. A numerical example based on real data illustrates that the\nproposed test-statistics are simple for computation and moreover, the necessary\ngoodness-of-fit test-statistic are easily calculated from them. The simulation\nstudy shows that the test based on the Cressie and Read (Journal of the Royal\nStatistical Society, Series B, 46, 440-464, 1989) divergence measure usually\nprovides a better nominal size than the CA test for small and moderate sample\nsizes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 22:55:01 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Mart\u00edn", "Nirian", ""], ["Mata", "Raquel", ""]]}, {"id": "1402.6744", "submitter": "Paul McNicholas", "authors": "Katherine Morris, Antonio Punzo, Paul D. McNicholas and Ryan P. Browne", "title": "Asymmetric Clusters and Outliers: Mixtures of Multivariate Contaminated\n  Shifted Asymmetric Laplace Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of multivariate contaminated shifted asymmetric Laplace\ndistributions are developed for handling asymmetric clusters in the presence of\noutliers (also referred to as bad points herein). In addition to the parameters\nof the related non-contaminated mixture, for each (asymmetric) cluster, our\nmodel has one parameter controlling the proportion of outliers and one\nspecifying the degree of contamination. Crucially, these parameters do not have\nto be specified a priori, adding a flexibility to our approach that is absent\nfrom other approaches such as trimming. Moreover, each observation is given a\nposterior probability of belonging to a particular cluster, and of being an\noutlier or not; advantageously, this allows for the automatic detection of\noutliers. An expectation-conditional maximization algorithm is outlined for\nparameter estimation and various implementation issues are discussed. The\nbehaviour of the proposed model is investigated, and compared with\nwell-established finite mixtures, on artificial and real data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 23:29:54 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 00:31:37 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Morris", "Katherine", ""], ["Punzo", "Antonio", ""], ["McNicholas", "Paul D.", ""], ["Browne", "Ryan P.", ""]]}, {"id": "1402.6781", "submitter": "Gael Martin Prof", "authors": "D. S. Poskitt, Gael M. Martin and Simone D. Grose", "title": "Bias Reduction of Long Memory Parameter Estimators via the Pre-filtered\n  Sieve Bootstrap", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the use of bootstrap-based bias correction of\nsemi-parametric estimators of the long memory parameter in fractionally\nintegrated processes. The re-sampling method involves the application of the\nsieve bootstrap to data pre-filtered by a preliminary semi-parametric estimate\nof the long memory parameter. Theoretical justification for using the bootstrap\ntechniques to bias adjust log-periodogram and semi-parametric local Whittle\nestimators of the memory parameter is provided. Simulation evidence comparing\nthe performance of the bootstrap bias correction with analytical bias\ncorrection techniques is also presented. The bootstrap method is shown to\nproduce notable bias reductions, in particular when applied to an estimator for\nwhich analytical adjustments have already been used. The empirical coverage of\nconfidence intervals based on the bias-adjusted estimators is very close to the\nnominal, for a reasonably large sample size, more so than for the comparable\nanalytically adjusted estimators. The precision of inferences (as measured by\ninterval length) is also greater when the bootstrap is used to bias correct\nrather than analytical adjustments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 03:40:15 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Poskitt", "D. S.", ""], ["Martin", "Gael M.", ""], ["Grose", "Simone D.", ""]]}, {"id": "1402.6836", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Rosa M. Crujeiras, Wenceslao\n  Gonz\\'alez-Manteiga", "title": "Central limit theorems for directional and linear random variables with\n  applications", "comments": "Paper: 19 pages, 5 figures, 1 table. Supplementary material: 46\n  pages, 7 figures, 5 tables", "journal-ref": "Statistica Sinica, 25(3):1207-1229, 2015", "doi": "10.5705/ss.2014.153", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A central limit theorem for the integrated squared error of the\ndirectional-linear kernel density estimator is established. The result enables\nthe construction and analysis of two testing procedures based on squared loss:\na nonparametric independence test for directional and linear random variables\nand a goodness-of-fit test for parametric families of directional-linear\ndensities. Limit distributions for both test statistics, and a consistent\nbootstrap strategy for the goodness-of-fit test, are developed for the\ndirectional-linear case and adapted to the directional-directional setting.\nFinite sample performance for the goodness-of-fit test is illustrated in a\nsimulation study. This test is also applied to datasets from biology and\nenvironmental sciences.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 09:41:36 GMT"}, {"version": "v2", "created": "Sun, 28 Sep 2014 03:14:09 GMT"}, {"version": "v3", "created": "Wed, 17 Dec 2014 17:28:51 GMT"}, {"version": "v4", "created": "Wed, 22 Apr 2015 18:26:38 GMT"}, {"version": "v5", "created": "Sun, 20 Sep 2020 23:32:29 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Crujeiras", "Rosa M.", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""]]}, {"id": "1402.7322", "submitter": "Chris Oates", "authors": "Chris J. Oates, Richard Amos, Simon E. F. Spencer", "title": "Quantifying the Multi-Scale Performance of Network Inference Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are widely used to study complex multivariate biological\nsystems. Network inference algorithms aim to reverse-engineer such models from\nnoisy experimental data. It is common to assess such algorithms using\ntechniques from classifier analysis. These metrics, based on ability to\ncorrectly infer individual edges, possess a number of appealing features\nincluding invariance to rank-preserving transformation. However, regulation in\nbiological systems occurs on multiple scales and existing metrics do not take\ninto account the correctness of higher-order network structure. In this paper\nnovel performance scores are presented that share the appealing properties of\nexisting scores, whilst capturing ability to uncover regulation on multiple\nscales. Theoretical results confirm that performance of a network inference\nalgorithm depends crucially on the scale at which inferences are to be made; in\nparticular strong local performance does not guarantee accurate reconstruction\nof higher-order topology. Applying these scores to a large corpus of data from\nthe DREAM5 challenge, we undertake a data-driven assessment of estimator\nperformance. We find that the ``wisdom of crowds'' network, that demonstrated\nsuperior local performance in the DREAM5 challenge, is also among the best\nperforming methodologies for inference of regulation on multiple length scales.\nMATLAB R2013b code \"net_assess\" is provided as Supplement.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 17:10:55 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Oates", "Chris J.", ""], ["Amos", "Richard", ""], ["Spencer", "Simon E. F.", ""]]}, {"id": "1402.7349", "submitter": "Kean Ming Tan", "authors": "Kean Ming Tan, Palma London, Karthik Mohan, Su-In Lee, Maryam Fazel,\n  and Daniela Witten", "title": "Learning Graphical Models With Hubs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a high-dimensional graphical model in\nwhich certain hub nodes are highly-connected to many other nodes. Many authors\nhave studied the use of an l1 penalty in order to learn a sparse graph in\nhigh-dimensional setting. However, the l1 penalty implicitly assumes that each\nedge is equally likely and independent of all other edges. We propose a general\nframework to accommodate more realistic networks with hub nodes, using a convex\nformulation that involves a row-column overlap norm penalty. We apply this\ngeneral framework to three widely-used probabilistic graphical models: the\nGaussian graphical model, the covariance graph model, and the binary Ising\nmodel. An alternating direction method of multipliers algorithm is used to\nsolve the corresponding convex optimization problems. On synthetic data, we\ndemonstrate that our proposed framework outperforms competitors that do not\nexplicitly model hub nodes. We illustrate our proposal on a webpage data set\nand a gene expression data set.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 19:09:11 GMT"}, {"version": "v2", "created": "Sat, 9 Aug 2014 18:33:43 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Tan", "Kean Ming", ""], ["London", "Palma", ""], ["Mohan", "Karthik", ""], ["Lee", "Su-In", ""], ["Fazel", "Maryam", ""], ["Witten", "Daniela", ""]]}]