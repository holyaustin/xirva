[{"id": "1104.0089", "submitter": "Stephane Girard", "authors": "St\\'ephane Girard and Pierre Jacob", "title": "Frontier estimation with local polynomials and high power-transformed\n  data", "comments": null, "journal-ref": "Journal of Multivariate Analysis, 100, 1691--1705, 2009", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for estimating the frontier of a sample. The\nestimator is based on a local polynomial regression on the power-transformed\ndata. We assume that the exponent of the transformation goes to infinity while\nthe bandwidth goes to zero. We give conditions on these two parameters to\nobtain almost complete convergence. The asymptotic conditional bias and\nvariance of the estimator are provided and its good performance is illustrated\non some finite sample situations.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2011 06:54:25 GMT"}], "update_date": "2011-04-04", "authors_parsed": [["Girard", "St\u00e9phane", ""], ["Jacob", "Pierre", ""]]}, {"id": "1104.0145", "submitter": "Stephane Girard", "authors": "C\\'ecile Amblard and St\\'ephane Girard", "title": "Estimation procedures for a semiparametric family of bivariate copulas", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, 14(2), 1-15,\n  2005", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose simple estimation methods dedicated to a\nsemiparametric family of bivariate copulas. These copulas can be simply\nestimated through the estimation of their univariate generating function. We\ntake profit of this result to estimate the associated measures of association\nas well as the high probability regions of the copula. These procedures are\nillustrated on simulations and on real data.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2011 11:31:52 GMT"}], "update_date": "2011-04-04", "authors_parsed": [["Amblard", "C\u00e9cile", ""], ["Girard", "St\u00e9phane", ""]]}, {"id": "1104.0166", "submitter": "Laurent Gardes", "authors": "L. Gardes, S. Girard and A. Lekina", "title": "Functional nonparametric estimation of conditional extreme quantiles", "comments": null, "journal-ref": "Journal of Multivariate Analysis, 101, 419-433, 2010", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the estimation of quantiles from heavy-tailed distributions when\nfunctional covariate information is available and in the case where the order\nof the quantile converges to one as the sample size increases. Such \"extreme\"\nquantiles can be located in the range of the data or near and even beyond the\nboundary of the sample, depending on the convergence rate of their order to\none. Nonparametric estimators of these functional extreme quantiles are\nintroduced, their asymptotic distributions are established and their finite\nsample behavior is investigated.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2011 13:27:57 GMT"}], "update_date": "2011-04-04", "authors_parsed": [["Gardes", "L.", ""], ["Girard", "S.", ""], ["Lekina", "A.", ""]]}, {"id": "1104.0191", "submitter": "Matthias Troffaes", "authors": "Nathan Huntley and Matthias C. M. Troffaes", "title": "Normal form backward induction for decision trees with coherent lower\n  previsions", "comments": "22 pages, 5 figures; v1: added doi and arxiv links", "journal-ref": "Annals of Operations Research 195 (2012) 111-134", "doi": "10.1007/s10479-011-0968-2", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine normal form solutions of decision trees under typical choice\nfunctions induced by lower previsions. For large trees, finding such solutions\nis hard as very many strategies must be considered. In an earlier paper, we\nextended backward induction to arbitrary choice functions, yielding far more\nefficient solutions, and we identified simple necessary and sufficient\nconditions for this to work. In this paper, we show that backward induction\nworks for maximality and E-admissibility, but not for interval dominance and\nGamma-maximin. We also show that, in some situations, a computationally cheap\napproximation of a choice function can be used, even if the approximation\nviolates the conditions for backward induction; for instance, interval\ndominance with backward induction will yield at least all maximal normal form\nsolutions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2011 15:04:21 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2012 11:28:21 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Huntley", "Nathan", ""], ["Troffaes", "Matthias C. M.", ""]]}, {"id": "1104.0341", "submitter": "David R. Bickel", "authors": "David R. Bickel", "title": "Small-scale inference: Empirical Bayes and confidence methods for as few\n  as a single comparison", "comments": "Corrected Figures 1-2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By restricting the possible values of the proportion of null hypotheses that\nare true, the local false discovery rate (LFDR) can be estimated using as few\nas one comparison. The proportion of proteins with equivalent abundance was\nestimated to be about 20% for patient group I and about 90% for group II. The\nsimultaneously-estimated LFDRs give approximately the same inferences as\nindividual-protein confidence levels for group I but are much closer to\nindividual-protein LFDR estimates for group II. Simulations confirm that\nconfidence-based inference or LFDR-based inference performs markedly better for\nlow or high proportions of true null hypotheses, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 2 Apr 2011 22:37:16 GMT"}, {"version": "v2", "created": "Wed, 11 May 2011 20:13:22 GMT"}], "update_date": "2011-05-13", "authors_parsed": [["Bickel", "David R.", ""]]}, {"id": "1104.0763", "submitter": "Laurent Gardes", "authors": "L. Gardes and S. Girard", "title": "A moving window approach for nonparametric estimation of the conditional\n  tail index", "comments": null, "journal-ref": "Journal of Multivariate Analysis, 99, 2368-2388 (2008)", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a nonparametric family of estimators for the tail index of a\nPareto-type distribution when covariate information is available. Our\nestimators are based on a weighted sum of the log-spacings between some\nselected observations. This selection is achieved through a moving window\napproach on the covariate domain and a random threshold on the variable of\ninterest. Asymptotic normality is proved under mild regularity conditions and\nillustrated for some weight functions. Finite sample performances are presented\non a real data study.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 08:17:55 GMT"}], "update_date": "2011-04-06", "authors_parsed": [["Gardes", "L.", ""], ["Girard", "S.", ""]]}, {"id": "1104.0764", "submitter": "Stephane Girard", "authors": "Laurent Gardes and St\\'ephane Girard", "title": "Comparison of Weibull tail-coefficient estimators", "comments": null, "journal-ref": "L. Gardes & S. Girard. \"Comparison of Weibull tail-coefficient\n  estimators\", REVSTAT - Statistical Journal, 4(2), 163-188, 2006", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating the Weibull tail-coefficient which is\nthe regular variation exponent of the inverse failure rate function. We propose\na family of estimators of this coefficient and an associate extreme quantile\nestimator. Their asymptotic normality are established and their asymptotic\nmean-square errors are compared. The results are illustrated on some finite\nsample situations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 08:19:20 GMT"}], "update_date": "2011-04-06", "authors_parsed": [["Gardes", "Laurent", ""], ["Girard", "St\u00e9phane", ""]]}, {"id": "1104.0861", "submitter": "Artin Armagan", "authors": "Artin Armagan, David Dunson, Jaeyong Lee", "title": "Generalized double Pareto shrinkage", "comments": null, "journal-ref": "Statistica Sinica 23 (2013), 119-143", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalized double Pareto prior for Bayesian shrinkage\nestimation and inferences in linear models. The prior can be obtained via a\nscale mixture of Laplace or normal distributions, forming a bridge between the\nLaplace and Normal-Jeffreys' priors. While it has a spike at zero like the\nLaplace density, it also has a Student's $t$-like tail behavior. Bayesian\ncomputation is straightforward via a simple Gibbs sampling algorithm. We\ninvestigate the properties of the maximum a posteriori estimator, as sparse\nestimation plays an important role in many problems, reveal connections with\nsome well-established regularization procedures, and show some asymptotic\nresults. The performance of the prior is tested through simulations and an\napplication.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 14:42:50 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2011 12:36:36 GMT"}, {"version": "v3", "created": "Sat, 21 Jan 2012 18:32:11 GMT"}, {"version": "v4", "created": "Sat, 26 Jan 2013 19:47:16 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Armagan", "Artin", ""], ["Dunson", "David", ""], ["Lee", "Jaeyong", ""]]}, {"id": "1104.0896", "submitter": "Marco Scutari", "authors": "Marco Scutari and Radhakrishnan Nagarajan", "title": "On Identifying Significant Edges in Graphical Models of Molecular\n  Networks", "comments": "21 pages, 9 figures. Presented at the Conference for Artificial\n  Intelligence in Medicine (AIME '11), Workshop on Probabilistic Problem\n  Solving in Biomedicine", "journal-ref": "Artificial Intelligence in Medicine 2013, 57(3):207-217", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Modelling the associations from high-throughput experimental\nmolecular data has provided unprecedented insights into biological pathways and\nsignalling mechanisms. Graphical models and networks have especially proven to\nbe useful abstractions in this regard. Ad-hoc thresholds are often used in\nconjunction with structure learning algorithms to determine significant\nassociations. The present study overcomes this limitation by proposing a\nstatistically-motivated approach for identifying significant associations in a\nnetwork.\n  Methods and Materials: A new method that identifies significant associations\nin graphical models by estimating the threshold minimising the $L_{\\mathrm{1}}$\nnorm between the cumulative distribution function (CDF) of the observed edge\nconfidences and those of its asymptotic counterpart is proposed. The\neffectiveness of the proposed method is demonstrated on popular synthetic data\nsets as well as publicly available experimental molecular data corresponding to\ngene and protein expression profiles.\n  Results: The improved performance of the proposed approach is demonstrated\nacross the synthetic data sets using sensitivity, specificity and accuracy as\nperformance metrics. The results are also demonstrated across varying sample\nsizes and three different structure learning algorithms with widely varying\nassumptions. In all cases, the proposed approach has specificity and accuracy\nclose to 1, while sensitivity increases linearly in the logarithm of the sample\nsize. The estimated threshold systematically outperforms common ad-hoc ones in\nterms of sensitivity while maintaining comparable levels of specificity and\naccuracy. Networks from experimental data sets are reconstructed accurately\nwith respect to the results from the original papers.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 17:08:50 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2011 17:03:45 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2011 10:38:27 GMT"}, {"version": "v4", "created": "Tue, 16 Oct 2012 10:02:27 GMT"}, {"version": "v5", "created": "Tue, 23 Apr 2013 11:27:58 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Scutari", "Marco", ""], ["Nagarajan", "Radhakrishnan", ""]]}, {"id": "1104.1436", "submitter": "Massimiliano Pontil", "authors": "Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil, Lixin\n  Shen, Yuesheng Xu", "title": "Efficient First Order Methods for Linear Composite Regularizers", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide class of regularization problems in machine learning and statistics\nemploy a regularization term which is obtained by composing a simple convex\nfunction \\omega with a linear transformation. This setting includes Group Lasso\nmethods, the Fused Lasso and other total variation methods, multi-task learning\nmethods and many more. In this paper, we present a general approach for\ncomputing the proximity operator of this class of regularizers, under the\nassumption that the proximity operator of the function \\omega is known in\nadvance. Our approach builds on a recent line of research on optimal first\norder optimization methods and uses fixed point iterations for numerically\ncomputing the proximity operator. It is more general than current approaches\nand, as we show with numerical simulations, computationally more efficient than\navailable first order methods which do not achieve the optimal rate. In\nparticular, our method outperforms state of the art O(1/T) methods for\noverlapping Group Lasso and matches optimal O(1/T^2) methods for the Fused\nLasso and tree structured Group Lasso.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2011 20:05:48 GMT"}], "update_date": "2011-04-11", "authors_parsed": [["Argyriou", "Andreas", ""], ["Micchelli", "Charles A.", ""], ["Pontil", "Massimiliano", ""], ["Shen", "Lixin", ""], ["Xu", "Yuesheng", ""]]}, {"id": "1104.1779", "submitter": "Ronny Luss", "authors": "Ronny Luss, Saharon Rosset", "title": "Generalized Isotonic Regression", "comments": "Minor changes. Final version. To appear in the Journal of\n  Computational and Graphical Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational and statistical approach for fitting isotonic\nmodels under convex differentiable loss functions. We offer a recursive\npartitioning algorithm which provably and efficiently solves isotonic\nregression under any such loss function. Models along the partitioning path are\nalso isotonic and can be viewed as regularized solutions to the problem. Our\napproach generalizes and subsumes two previous results: the well-known work of\nBarlow and Brunk (1972) on fitting isotonic regressions subject to specially\nstructured loss functions, and a recursive partitioning algorithm (Spouge et al\n2003) for the case of standard (l2-loss) isotonic regression. We demonstrate\nthe advantages of our generalized algorithm on both real and simulated data in\ntwo settings: fitting count data using negative Poisson log-likelihood loss,\nand fitting robust isotonic regression using Huber's loss.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2011 15:57:29 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2012 19:34:36 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Luss", "Ronny", ""], ["Rosset", "Saharon", ""]]}, {"id": "1104.1807", "submitter": "Audrey Kueh", "authors": "Audrey Kueh", "title": "Locally Adaptive Density Estimation on the Unit Sphere Using Needlets", "comments": null, "journal-ref": "Constructive Approximation, December 2012, Volume 36, Issue 3, pp\n  433-458", "doi": "10.1007/s00365-012-9170-2", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating a probability density function f on the\n(d-1)-dimensional unit sphere S^{d-1} from directional data using the needlet\nframe is considered. It is shown that the decay of needlet coefficients\nsupported near a point of a function f depends only on local H\\\"{o}lder\ncontinuity properties of f at x. This is then used to show that the thresholded\nneedlet estimator introduced in Baldi, Kerkyacharian, Marinucci and Picard\nadapts to the local regularity properties of f. Moreover an adaptive confidence\ninterval for f based on the thresholded needlet estimator is proposed, which is\nasymptotically honest over suitable classes of locally H\\\"{o}lderian densities.\n", "versions": [{"version": "v1", "created": "Sun, 10 Apr 2011 22:29:35 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2012 20:36:21 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2013 10:35:14 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Kueh", "Audrey", ""]]}, {"id": "1104.1897", "submitter": "David A. van Dyk", "authors": "David A. van Dyk, Xiao-Li Meng", "title": "Cross-Fertilizing Strategies for Better EM Mountain Climbing and DA\n  Field Exploration: A Graphical Guide Book", "comments": "Published in at http://dx.doi.org/10.1214/09-STS309 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 4, 429", "doi": "10.1214/09-STS309", "report-no": "IMS-STS-STS309", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a variety of extensions and refinements have been developed\nfor data augmentation based model fitting routines. These developments aim to\nextend the application, improve the speed and/or simplify the implementation of\ndata augmentation methods, such as the deterministic EM algorithm for mode\nfinding and stochastic Gibbs sampler and other auxiliary-variable based methods\nfor posterior sampling. In this overview article we graphically illustrate and\ncompare a number of these extensions, all of which aim to maintain the\nsimplicity and computation stability of their predecessors. We particularly\nemphasize the usefulness of identifying similarities between the deterministic\nand stochastic counterparts as we seek more efficient computational strategies.\nWe also demonstrate the applicability of data augmentation methods for handling\ncomplex models with highly hierarchical structure, using a high-energy\nhigh-resolution spectral imaging model for data from satellite telescopes, such\nas the Chandra X-ray Observatory.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 10:21:52 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["van Dyk", "David A.", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "1104.1923", "submitter": "Nan M. Laird", "authors": "Nan M. Laird", "title": "The EM Algorithm in Genetics, Genomics and Public Health", "comments": "Published in at http://dx.doi.org/10.1214/08-STS270 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 4, 450-457", "doi": "10.1214/08-STS270", "report-no": "IMS-STS-STS270", "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of the EM algorithm owes much to the 1977 paper by Dempster,\nLaird and Rubin. That paper gave the algorithm its name, identified the general\nform and some key properties of the algorithm and established its broad\napplicability in scientific research. This review gives a nontechnical\nintroduction to the algorithm for a general scientific audience, and presents a\nfew examples characteristic of its application.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 12:07:03 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Laird", "Nan M.", ""]]}, {"id": "1104.2057", "submitter": "Jonathan Lilly", "authors": "Jonathan M. Lilly", "title": "Modulated Oscillations in Three Dimensions", "comments": "IEEE Transactions on Signal Processing, 2011", "journal-ref": null, "doi": "10.1109/TSP.2011.2164914", "report-no": null, "categories": "stat.ME physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of the fully three-dimensional and time-varying polarization\ncharacteristics of a modulated trivariate, or three-component, oscillation is\naddressed. The use of the analytic operator enables the instantaneous\nthree-dimensional polarization state of any square-integrable trivariate signal\nto be uniquely defined. Straightforward expressions are given which permit the\nellipse parameters to be recovered from data. The notions of instantaneous\nfrequency and instantaneous bandwidth, generalized to the trivariate case, are\nrelated to variations in the ellipse properties. Rates of change of the ellipse\nparameters are found to be intimately linked to the first few moments of the\nsignal's spectrum, averaged over the three signal components. In particular,\nthe trivariate instantaneous bandwidth---a measure of the instantaneous\ndeparture of the signal from a single pure sinusoidal oscillation---is found to\ncontain five contributions: three essentially two-dimensional effects due to\nthe motion of the ellipse within a fixed plane, and two effects due to the\nmotion of the plane containing the ellipse. The resulting analysis method is an\ninformative means of describing nonstationary trivariate signals, as is\nillustrated with an application to a seismic record.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 20:23:30 GMT"}, {"version": "v2", "created": "Sun, 2 Oct 2011 22:28:02 GMT"}, {"version": "v3", "created": "Tue, 11 Oct 2011 18:27:34 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Lilly", "Jonathan M.", ""]]}, {"id": "1104.2060", "submitter": "Jonathan Lilly", "authors": "Jonathan M. Lilly and Sofia C. Olhede", "title": "Analysis of Modulated Multivariate Oscillations", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2011.2173681", "report-no": null, "categories": "stat.ME physics.ao-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of a common modulated oscillation spanning multiple time series\nis formalized, a method for the recovery of such a signal from potentially\nnoisy observations is proposed, and the time-varying bias properties of the\nrecovery method are derived. The method, an extension of wavelet ridge analysis\nto the multivariate case, identifies the common oscillation by seeking, at each\npoint in time, a frequency for which a bandpassed version of the signal obtains\na local maximum in power. The lowest-order bias is shown to involve a quantity,\ntermed the instantaneous curvature, which measures the strength of local\nquadratic modulation of the signal after demodulation by the common oscillation\nfrequency. The bias can be made to be small if the analysis filter, or wavelet,\ncan be chosen such that the signal's instantaneous curvature changes little\nover the filter time scale. An application is presented to the detection of\nvortex motions in a set of freely-drifting oceanographic instruments tracking\nthe ocean currents.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 20:35:32 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2011 20:50:33 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Lilly", "Jonathan M.", ""], ["Olhede", "Sofia C.", ""]]}, {"id": "1104.2123", "submitter": "Zhangzhang Si", "authors": "Zhangzhang Si, Haifeng Gong, Song-Chun Zhu, Ying Nian Wu", "title": "Learning Active Basis Models by EM-Type Algorithms", "comments": "Published in at http://dx.doi.org/10.1214/09-STS281 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 4, 458-475", "doi": "10.1214/09-STS281", "report-no": "IMS-STS-STS281", "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EM algorithm is a convenient tool for maximum likelihood model fitting when\nthe data are incomplete or when there are latent variables or hidden states. In\nthis review article we explain that EM algorithm is a natural computational\nscheme for learning image templates of object categories where the learning is\nnot fully supervised. We represent an image template by an active basis model,\nwhich is a linear composition of a selected set of localized, elongated and\noriented wavelet elements that are allowed to slightly perturb their locations\nand orientations to account for the deformations of object shapes. The model\ncan be easily learned when the objects in the training images are of the same\npose, and appear at the same location and scale. This is often called\nsupervised learning. In the situation where the objects may appear at different\nunknown locations, orientations and scales in the training images, we have to\nincorporate the unknown locations, orientations and scales as latent variables\ninto the image generation process, and learn the template by EM-type\nalgorithms. The E-step imputes the unknown locations, orientations and scales\nbased on the currently learned template. This step can be considered\nself-supervision, which involves using the current template to recognize the\nobjects in the training images. The M-step then relearns the template based on\nthe imputed locations, orientations and scales, and this is essentially the\nsame as supervised learning. So the EM learning process iterates between\nrecognition and supervised learning. We illustrate this scheme by several\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2011 06:55:31 GMT"}], "update_date": "2011-04-13", "authors_parsed": [["Si", "Zhangzhang", ""], ["Gong", "Haifeng", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1104.2180", "submitter": "Xiaodan Fan", "authors": "Xiaodan Fan, Yuan Yuan, Jun S. Liu", "title": "The EM Algorithm and the Rise of Computational Biology", "comments": "Published in at http://dx.doi.org/10.1214/09-STS312 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 4, 476-491", "doi": "10.1214/09-STS312", "report-no": "IMS-STS-STS312", "categories": "stat.ME q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade computational biology has grown from a cottage industry\nwith a handful of researchers to an attractive interdisciplinary field,\ncatching the attention and imagination of many quantitatively-minded\nscientists. Of interest to us is the key role played by the EM algorithm during\nthis transformation. We survey the use of the EM algorithm in a few important\ncomputational biology problems surrounding the \"central dogma\"; of molecular\nbiology: from DNA to RNA and then to proteins. Topics of this article include\nsequence motif discovery, protein sequence alignment, population genetics,\nevolutionary models and mRNA expression microarray data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2011 12:08:54 GMT"}], "update_date": "2011-04-13", "authors_parsed": [["Fan", "Xiaodan", ""], ["Yuan", "Yuan", ""], ["Liu", "Jun S.", ""]]}, {"id": "1104.2190", "submitter": "Thoralf Mildenberger", "authors": "Thoralf Mildenberger", "title": "The Discrepancy Principle for Choosing Bandwidths in Kernel Density\n  Estimation", "comments": "17 pages, 3 figures. Section on histograms removed, new (positive and\n  negative) consistency results for kernel density estimators added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the discrepancy principle for choosing smoothing parameters\nfor kernel density estimation. The method is based on the distance between the\nempirical and estimated distribution functions. We prove some new positive and\nnegative results on L_1-consistency of kernel estimators with bandwidths chosen\nusing the discrepancy principle. Consistency crucially depends on a rather weak\nH\\\"older condition on the distribution function. We also unify and extend\nprevious results on the behaviour of the chosen bandwidth under more strict\nsmoothness assumptions. Furthermore, we compare the discrepancy principle to\nstandard methods in a simulation study. Surprisingly, some of the proposals\nwork reasonably well over a large set of different densities and sample sizes,\nand the performance of the methods at least up to n=2500 can be quite different\nfrom their asymptotic behavior.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2011 12:33:49 GMT"}, {"version": "v2", "created": "Mon, 2 May 2011 09:51:23 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2012 13:41:58 GMT"}, {"version": "v4", "created": "Tue, 18 Dec 2012 14:39:55 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Mildenberger", "Thoralf", ""]]}, {"id": "1104.2203", "submitter": "Kenneth Lange", "authors": "Tong Tong Wu, Kenneth Lange", "title": "The MM Alternative to EM", "comments": "Published in at http://dx.doi.org/10.1214/08-STS264 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 4, 492-505", "doi": "10.1214/08-STS264", "report-no": "IMS-STS-STS264", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EM algorithm is a special case of a more general algorithm called the MM\nalgorithm. Specific MM algorithms often have nothing to do with missing data.\nThe first M step of an MM algorithm creates a surrogate function that is\noptimized in the second M step. In minimization, MM stands for\nmajorize--minimize; in maximization, it stands for minorize--maximize. This\ntwo-step process always drives the objective function in the right direction.\nConstruction of MM algorithms relies on recognizing and manipulating\ninequalities rather than calculating conditional expectations. This survey\nwalks the reader through the construction of several specific MM algorithms.\nThe potential of the MM algorithm in solving high-dimensional optimization and\nestimation problems is its most attractive feature. Our applications to random\ngraph models, discriminant analysis and image restoration showcase this\nability.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2011 13:14:51 GMT"}], "update_date": "2011-04-13", "authors_parsed": [["Wu", "Tong Tong", ""], ["Lange", "Kenneth", ""]]}, {"id": "1104.2210", "submitter": "Martin A. Tanner", "authors": "Martin A. Tanner, Wing H. Wong", "title": "From EM to Data Augmentation: The Emergence of MCMC Bayesian Computation\n  in the 1980s", "comments": "Published in at http://dx.doi.org/10.1214/10-STS341 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 4, 506-516", "doi": "10.1214/10-STS341", "report-no": "IMS-STS-STS341", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was known from Metropolis et al. [J. Chem. Phys. 21 (1953) 1087--1092]\nthat one can sample from a distribution by performing Monte Carlo simulation\nfrom a Markov chain whose equilibrium distribution is equal to the target\ndistribution. However, it took several decades before the statistical community\nembraced Markov chain Monte Carlo (MCMC) as a general computational tool in\nBayesian inference. The usual reasons that are advanced to explain why\nstatisticians were slow to catch on to the method include lack of computing\npower and unfamiliarity with the early dynamic Monte Carlo papers in the\nstatistical physics literature. We argue that there was a deeper reason,\nnamely, that the structure of problems in the statistical mechanics and those\nin the standard statistical literature are different. To make the methods\nusable in standard Bayesian problems, one had to exploit the power that comes\nfrom the introduction of judiciously chosen auxiliary variables and collective\nmoves. This paper examines the development in the critical period 1980--1990,\nwhen the ideas of Markov chain simulation from the statistical physics\nliterature and the latent variable formulation in maximum likelihood\ncomputation (i.e., EM algorithm) came together to spark the widespread\napplication of MCMC methods in Bayesian computation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2011 13:45:16 GMT"}], "update_date": "2011-04-13", "authors_parsed": [["Tanner", "Martin A.", ""], ["Wong", "Wing H.", ""]]}, {"id": "1104.2400", "submitter": "Yan Zhou", "authors": "Yan Zhou, Roderick J. A. Little, John D. Kalbfleisch", "title": "Block-Conditional Missing at Random Models for Missing Data", "comments": "Published in at http://dx.doi.org/10.1214/10-STS344 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 4, 517-532", "doi": "10.1214/10-STS344", "report-no": "IMS-STS-STS344", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two major ideas in the analysis of missing data are (a) the EM algorithm\n[Dempster, Laird and Rubin, J. Roy. Statist. Soc. Ser. B 39 (1977) 1--38] for\nmaximum likelihood (ML) estimation, and (b) the formulation of models for the\njoint distribution of the data ${Z}$ and missing data indicators ${M}$, and\nassociated \"missing at random\"; (MAR) condition under which a model for ${M}$\nis unnecessary [Rubin, Biometrika 63 (1976) 581--592]. Most previous work has\ntreated ${Z}$ and ${M}$ as single blocks, yielding selection or pattern-mixture\nmodels depending on how their joint distribution is factorized. This paper\nexplores \"block-sequential\"; models that interleave subsets of the variables\nand their missing data indicators, and then make parameter restrictions based\non assumptions in each block. These include models that are not MAR. We examine\na subclass of block-sequential models we call block-conditional MAR (BCMAR)\nmodels, and an associated block-monotone reduced likelihood strategy that\ntypically yields consistent estimates by selectively discarding some data.\nAlternatively, full ML estimation can often be achieved via the EM algorithm.\nWe examine in some detail BCMAR models for the case of two multinomially\ndistributed categorical variables, and a two block structure where the first\nblock is categorical and the second block arises from a (possibly multivariate)\nexponential family distribution.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2011 07:39:13 GMT"}], "update_date": "2011-04-14", "authors_parsed": [["Zhou", "Yan", ""], ["Little", "Roderick J. A.", ""], ["Kalbfleisch", "John D.", ""]]}, {"id": "1104.2407", "submitter": "Andrew Lewandowski", "authors": "Andrew Lewandowski, Chuanhai Liu, Scott Vander Wiel", "title": "Parameter Expansion and Efficient Inference", "comments": "Published in at http://dx.doi.org/10.1214/10-STS348 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 4, 533-544", "doi": "10.1214/10-STS348", "report-no": "IMS-STS-STS348", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This EM review article focuses on parameter expansion, a simple technique\nintroduced in the PX-EM algorithm to make EM converge faster while maintaining\nits simplicity and stability. The primary objective concerns the connection\nbetween parameter expansion and efficient inference. It reviews the statistical\ninterpretation of the PX-EM algorithm, in terms of efficient inference via bias\nreduction, and further unfolds the PX-EM mystery by looking at PX-EM from\ndifferent perspectives. In addition, it briefly discusses potential\napplications of parameter expansion to statistical inference and the broader\nimpact of statistical thinking on understanding and developing other iterative\noptimization algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2011 08:24:53 GMT"}], "update_date": "2011-04-14", "authors_parsed": [["Lewandowski", "Andrew", ""], ["Liu", "Chuanhai", ""], ["Wiel", "Scott Vander", ""]]}, {"id": "1104.2424", "submitter": "Tomokazu Konishi", "authors": "Tomokazu Konishi", "title": "Appropriate Methodology of Statistical Tests According to Prior\n  Probability and Required Objectivity", "comments": "16 pages, 3 figures, and 1 table", "journal-ref": "BMC Systems Biology 2011, 5(Suppl 2):S6", "doi": "10.1186/1752-0509-5-S2-S6", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to its common definition and calculation, interpretation of\np-values diverges among statisticians. Since p-value is the basis of various\nmethodologies, this divergence has led to a variety of test methodologies and\nevaluations of test results. This chaotic situation has complicated the\napplication of tests and decision processes. Here, the origin of the divergence\nis found in the prior probability of the test. Effects of difference in Pr(H0 =\ntrue) on the character of p-values are investigated by comparing real\nmicroarray data and its artificial imitations as subjects of Student's t-tests.\nAlso, the importance of the prior probability is discussed in terms of the\napplicability of Bayesian approaches. Suitable methodology is found in\naccordance with the prior probability and purpose of the test.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2011 09:25:15 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Konishi", "Tomokazu", ""]]}, {"id": "1104.2808", "submitter": "Alain Hauser", "authors": "Alain Hauser and Peter B\\\"uhlmann", "title": "Characterization and Greedy Learning of Interventional Markov\n  Equivalence Classes of Directed Acyclic Graphs", "comments": null, "journal-ref": "Journal of Machine Learning Research, 13:2409-2464, 2012", "doi": null, "report-no": null, "categories": "stat.ME cs.DM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The investigation of directed acyclic graphs (DAGs) encoding the same Markov\nproperty, that is the same conditional independence relations of multivariate\nobservational distributions, has a long tradition; many algorithms exist for\nmodel selection and structure learning in Markov equivalence classes. In this\npaper, we extend the notion of Markov equivalence of DAGs to the case of\ninterventional distributions arising from multiple intervention experiments. We\nshow that under reasonable assumptions on the intervention experiments,\ninterventional Markov equivalence defines a finer partitioning of DAGs than\nobservational Markov equivalence and hence improves the identifiability of\ncausal models. We give a graph theoretic criterion for two DAGs being Markov\nequivalent under interventions and show that each interventional Markov\nequivalence class can, analogously to the observational case, be uniquely\nrepresented by a chain graph called interventional essential graph (also known\nas CPDAG in the observational case). These are key insights for deriving a\ngeneralization of the Greedy Equivalence Search algorithm aimed at structure\nlearning from interventional data. This new algorithm is evaluated in a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 15:41:01 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2012 18:01:33 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Hauser", "Alain", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1104.2826", "submitter": "Pietro Berkes", "authors": "Pietro Berkes and Jozsef Fiser", "title": "A frequentist two-sample test based on Bayesian model selection", "comments": "Paper and supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their importance in supporting experimental conclusions, standard\nstatistical tests are often inadequate for research areas, like the life\nsciences, where the typical sample size is small and the test assumptions\ndifficult to verify. In such conditions, standard tests tend to be overly\nconservative, and fail thus to detect significant effects in the data. Here we\ndefine a novel statistical test for the two-sample problem. Several\ncharacteristics make it an attractive alternative to classical two-sample\ntests: 1) It is based on Bayesian model selection, and thus takes into account\nuncertainty about the model's parameters, mitigating the problem of small\nsamples size; 2) The null hypothesis is compared with several alternative\nhypotheses, making the test suitable in different experimental scenarios; 3)\nThe test is constructed as a frequentist test, and defines significance with\nthe conventional bound on Type I errors. We analyze the power of the test and\nfind that it is higher than the power of other standard options, like the\nt-test (up to 25% higher) for a wide range of sample and effect sizes, and is\nat most 1% lower when the assumptions of the t-test are perfectly matched. We\ndiscuss and evaluate two variants of the test, that define different prior\ndistributions over the parameters of the hypotheses.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 16:44:18 GMT"}], "update_date": "2011-04-15", "authors_parsed": [["Berkes", "Pietro", ""], ["Fiser", "Jozsef", ""]]}, {"id": "1104.2930", "submitter": "Donghui Yan", "authors": "Donghui Yan, Aiyou Chen, Michael I. Jordan", "title": "Cluster Forests", "comments": "23 pages, 6 figures", "journal-ref": "Computational Statistics and Data Analysis 2013, Vol. 66, 178-192", "doi": "10.1016/j.csda.2013.04.010", "report-no": "COMSTA5571", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With inspiration from Random Forests (RF) in the context of classification, a\nnew clustering ensemble method---Cluster Forests (CF) is proposed.\nGeometrically, CF randomly probes a high-dimensional data cloud to obtain \"good\nlocal clusterings\" and then aggregates via spectral clustering to obtain\ncluster assignments for the whole dataset. The search for good local\nclusterings is guided by a cluster quality measure kappa. CF progressively\nimproves each local clustering in a fashion that resembles the tree growth in\nRF. Empirical studies on several real-world datasets under two different\nperformance metrics show that CF compares favorably to its competitors.\nTheoretical analysis reveals that the kappa measure makes it possible to grow\nthe local clustering in a desirable way---it is \"noise-resistant\". A\nclosed-form expression is obtained for the mis-clustering rate of spectral\nclustering under a perturbation model, which yields new insights into some\naspects of spectral clustering.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 21:29:10 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2011 05:06:04 GMT"}, {"version": "v3", "created": "Thu, 23 May 2013 21:17:26 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Yan", "Donghui", ""], ["Chen", "Aiyou", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1104.2975", "submitter": "Ana M. Pires", "authors": "Ana M. Pires, Jo\\~ao A. Branco", "title": "A Statistical Model to Explain the Mendel--Fisher Controversy", "comments": "Published in at http://dx.doi.org/10.1214/10-STS342 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 4, 545-565", "doi": "10.1214/10-STS342", "report-no": "IMS-STS-STS342", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1866 Gregor Mendel published a seminal paper containing the foundations of\nmodern genetics. In 1936 Ronald Fisher published a statistical analysis of\nMendel's data concluding that \"the data of most, if not all, of the experiments\nhave been falsified so as to agree closely with Mendel's expectations.\" The\naccusation gave rise to a controversy which has reached the present time. There\nare reasonable grounds to assume that a certain unconscious bias was\nsystematically introduced in Mendel's experimentation. Based on this\nassumption, a probability model that fits Mendel's data and does not offend\nFisher's analysis is given. This reconciliation model may well be the end of\nthe Mendel--Fisher controversy.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2011 07:59:17 GMT"}], "update_date": "2011-04-18", "authors_parsed": [["Pires", "Ana M.", ""], ["Branco", "Jo\u00e3o A.", ""]]}, {"id": "1104.3073", "submitter": "Yingcun Xia", "authors": "Yingcun Xia, Howell Tong", "title": "Feature Matching in Time Series Modeling", "comments": "Published in at http://dx.doi.org/10.1214/10-STS345 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 21-46", "doi": "10.1214/10-STS345", "report-no": "IMS-STS-STS345", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a time series model to mimic an observed time series has a long\nhistory. However, with regard to this objective, conventional estimation\nmethods for discrete-time dynamical models are frequently found to be wanting.\nIn fact, they are characteristically misguided in at least two respects: (i)\nassuming that there is a true model; (ii) evaluating the efficacy of the\nestimation as if the postulated model is true. There are numerous examples of\nmodels, when fitted by conventional methods, that fail to capture some of the\nmost basic global features of the data, such as cycles with good matching\nperiods, singularities of spectral density functions (especially at the origin)\nand others. We argue that the shortcomings need not always be due to the model\nformulation but the inadequacy of the conventional fitting methods. After all,\nall models are wrong, but some are useful if they are fitted properly. The\npractical issue becomes one of how to best fit the model to data. Thus, in the\nabsence of a true model, we prefer an alternative approach to conventional\nmodel fitting that typically involves one-step-ahead prediction errors. Our\nprimary aim is to match the joint probability distribution of the observable\ntime series, including long-term features of the dynamics that underpin the\ndata, such as cycles, long memory and others, rather than short-term\nprediction. For want of a better name, we call this specific aim feature\nmatching.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2011 14:37:22 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2012 11:56:32 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Xia", "Yingcun", ""], ["Tong", "Howell", ""]]}, {"id": "1104.3476", "submitter": "Vincent Dubourg", "authors": "V. Dubourg and F. Deheeger and B. Sudret", "title": "Metamodel-based importance sampling for the simulation of rare events", "comments": "8 pages, 3 figures, 1 table. Preprint submitted to ICASP11\n  Mini-symposia entitled \"Meta-models/surrogate models for uncertainty\n  propagation, sensitivity and reliability analysis\"", "journal-ref": "Proceedings of the 11th International Conference on Applications\n  of Statistics and Probability in Civil Engineering (ICASP11). Zurich,\n  Switzerland, August 2011", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of structural reliability, the Monte-Carlo estimator is\nconsidered as the reference probability estimator. However, it is still\nuntractable for real engineering cases since it requires a high number of runs\nof the model. In order to reduce the number of computer experiments, many other\napproaches known as reliability methods have been proposed. A certain approach\nconsists in replacing the original experiment by a surrogate which is much\nfaster to evaluate. Nevertheless, it is often difficult (or even impossible) to\nquantify the error made by this substitution. In this paper an alternative\napproach is developed. It takes advantage of the kriging meta-modeling and\nimportance sampling techniques. The proposed alternative estimator is finally\napplied to a finite element based structural reliability analysis.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2011 13:30:16 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Dubourg", "V.", ""], ["Deheeger", "F.", ""], ["Sudret", "B.", ""]]}, {"id": "1104.3479", "submitter": "Bruno Sudret", "authors": "V. Dubourg and J.-M. Bourinet and B. Sudret", "title": "Reliability-based design optimization of shells with uncertain geometry\n  using adaptive Kriging metamodels", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2017-002", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal design under uncertainty has gained much attention in the past ten\nyears due to the ever increasing need for manufacturers to build robust systems\nat the lowest cost. Reliability-based design optimization (RBDO) allows the\nanalyst to minimize some cost function while ensuring some minimal performances\ncast as admissible failure probabilities for a set of performance functions. In\norder to address real-world engineering problems in which the performance is\nassessed through computational models (e.g., finite element models in\nstructural mechanics) metamodeling techniques have been developed in the past\ndecade. This paper introduces adaptive Kriging surrogate models to solve the\nRBDO problem. The latter is cast in an augmented space that \"sums up\" the range\nof the design space and the aleatory uncertainty in the design parameters and\nthe environmental conditions. The surrogate model is used (i) for evaluating\nrobust estimates of the failure probabilities (and for enhancing the\ncomputational experimental design by adaptive sampling) in order to achieve the\nrequested accuracy and (ii) for applying a gradient-based optimization\nalgorithm to get optimal values of the design parameters. The approach is\napplied to the optimal design of ring-stiffened cylindrical shells used in\nsubmarine engineering under uncertain geometric imperfections. For this\napplication the performance of the structure is related to buckling which is\naddressed here by means of a finite element solution based on the asymptotic\nnumerical method.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2011 13:33:58 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 14:19:34 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Dubourg", "V.", ""], ["Bourinet", "J. -M.", ""], ["Sudret", "B.", ""]]}, {"id": "1104.3667", "submitter": "Vincent Dubourg", "authors": "V. Dubourg, B. Sudret and J.-M. Bourinet", "title": "Reliability-based design optimization using kriging surrogates and\n  subset simulation", "comments": "20 pages, 6 figures, 5 tables. Preprint submitted to Springer-Verlag", "journal-ref": "Structural Multisciplinary Optimization. 2011", "doi": "10.1007/s00158-011-0653-8", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the present paper is to develop a strategy for solving\nreliability-based design optimization (RBDO) problems that remains applicable\nwhen the performance models are expensive to evaluate. Starting with the\npremise that simulation-based approaches are not affordable for such problems,\nand that the most-probable-failure-point-based approaches do not permit to\nquantify the error on the estimation of the failure probability, an approach\nbased on both metamodels and advanced simulation techniques is explored. The\nkriging metamodeling technique is chosen in order to surrogate the performance\nfunctions because it allows one to genuinely quantify the surrogate error. The\nsurrogate error onto the limit-state surfaces is propagated to the failure\nprobabilities estimates in order to provide an empirical error measure. This\nerror is then sequentially reduced by means of a population-based adaptive\nrefinement technique until the kriging surrogates are accurate enough for\nreliability analysis. This original refinement strategy makes it possible to\nadd several observations in the design of experiments at the same time.\nReliability and reliability sensitivity analyses are performed by means of the\nsubset simulation technique for the sake of numerical efficiency. The adaptive\nsurrogate-based strategy for reliability estimation is finally involved into a\nclassical gradient-based optimization algorithm in order to solve the RBDO\nproblem. The kriging surrogates are built in a so-called augmented reliability\nspace thus making them reusable from one nested RBDO iteration to the other.\nThe strategy is compared to other approaches available in the literature on\nthree academic examples in the field of structural mechanics.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 08:07:38 GMT"}], "update_date": "2011-04-20", "authors_parsed": [["Dubourg", "V.", ""], ["Sudret", "B.", ""], ["Bourinet", "J. -M.", ""]]}, {"id": "1104.3707", "submitter": "Cedric Ginestet", "authors": "Cedric E. Ginestet, Thomas E. Nichols, Ed T. Bullmore, Andrew Simmons", "title": "Brain Network Analysis: Separating Cost from Topology using\n  Cost-integration", "comments": "Accepted for publication in PLoS one, in June 2011", "journal-ref": null, "doi": "10.1371/journal.pone.0021570", "report-no": null, "categories": "q-bio.MN q-bio.NC q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistically principled way of conducting weighted network analysis is\nstill lacking. Comparison of different populations of weighted networks is hard\nbecause topology is inherently dependent on wiring cost, where cost is defined\nas the number of edges in an unweighted graph. In this paper, we evaluate the\nbenefits and limitations associated with using cost-integrated topological\nmetrics. Our focus is on comparing populations of weighted undirected graphs\nusing global efficiency. We evaluate different approaches to the comparison of\nweighted networks that differ in mean association weight. Our key result shows\nthat integrating over cost is equivalent to controlling for any monotonic\ntransformation of the weight set of a weighted graph. That is, when integrating\nover cost, we eliminate the differences in topology that may be due to a\nmonotonic transformation of the weight set. Our result holds for any unweighted\ntopological measure. Cost-integration is therefore helpful in disentangling\ndifferences in cost from differences in topology. By contrast, we show that the\nuse of the weighted version of a topological metric does not constitute a valid\napproach to this problem. Indeed, we prove that, under mild conditions, the use\nof the weighted version of global efficiency is equivalent to simply comparing\nweighted costs. Thus, we recommend the reporting of (i) differences in weighted\ncosts and (ii) differences in cost-integrated topological measures. We\ndemonstrate the application of these techniques in a re-analysis of an fMRI\nworking memory task. Finally, we discuss the limitations of integrating\ntopology over cost, which may pose problems when some weights are zero, when\nmultiplicities exist in the ranks of the weights, and when one expects subtle\ncost-dependent topological differences, which could be masked by\ncost-integration.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 10:56:44 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2011 05:49:51 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Ginestet", "Cedric E.", ""], ["Nichols", "Thomas E.", ""], ["Bullmore", "Ed T.", ""], ["Simmons", "Andrew", ""]]}, {"id": "1104.3733", "submitter": "Nikolai Gagunashvili", "authors": "Nikolai Gagunashvili", "title": "CHIWEI: A code of goodness of fit tests for weighted and unweighted\n  histograms", "comments": "7 pages", "journal-ref": "Computer Physics Communications, Volume 183, Issue 2, 2012, Pages\n  418-421", "doi": "10.1016/j.cpc.2011.10.009", "report-no": null, "categories": "physics.data-an astro-ph.IM hep-ex nucl-ex stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A self-contained Fortran-77 program for goodness of fit tests for histograms\nwith weighted entries as well as with unweighted entries is presented. The code\ncalculates test statistics for case of histogram with normalized weights of\nevents and in case of unnormalized weights of events.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 12:45:10 GMT"}, {"version": "v2", "created": "Mon, 30 May 2011 13:36:56 GMT"}, {"version": "v3", "created": "Mon, 10 Oct 2011 12:24:32 GMT"}], "update_date": "2011-11-18", "authors_parsed": [["Gagunashvili", "Nikolai", ""]]}, {"id": "1104.3889", "submitter": "Lior Pachter", "authors": "Lior Pachter", "title": "Models for transcript quantification from RNA-Seq", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RNA-Seq is rapidly becoming the standard technology for transcriptome\nanalysis. Fundamental to many of the applications of RNA-Seq is the\nquantification problem, which is the accurate measurement of relative\ntranscript abundances from the sequenced reads. We focus on this problem, and\nreview many recently published models that are used to estimate the relative\nabundances. In addition to describing the models and the different approaches\nto inference, we also explain how methods are related to each other. A key\nresult is that we show how inference with many of the models results in\nidentical estimates of relative abundances, even though model formulations can\nbe very different. In fact, we are able to show how a single general model\ncaptures many of the elements of previously published methods. We also review\nthe applications of RNA-Seq models to differential analysis, and explain why\naccurate relative transcript abundance estimates are crucial for downstream\nanalyses.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 21:46:46 GMT"}, {"version": "v2", "created": "Fri, 13 May 2011 00:18:18 GMT"}], "update_date": "2011-05-16", "authors_parsed": [["Pachter", "Lior", ""]]}, {"id": "1104.4049", "submitter": "Stefan Fremdt Dipl. Wirt.-Math.", "authors": "Stefan Fremdt, Lajos Horv\\'ath, Piotr Kokoszka, Josef G. Steinebach", "title": "Testing the Equality of Covariance Operators in Functional Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust test for the equality of the covariance structures in two\nfunctional samples. The test statistic has a chi-square asymptotic distribution\nwith a known number of degrees of freedom, which depends on the level of\ndimension reduction needed to represent the data. Detailed analysis of the\nasymptotic properties is developed. Finite sample performance is examined by a\nsimulation study and an application to egg-laying curves of fruit flies.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2011 14:55:38 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2011 06:09:43 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Fremdt", "Stefan", ""], ["Horv\u00e1th", "Lajos", ""], ["Kokoszka", "Piotr", ""], ["Steinebach", "Josef G.", ""]]}, {"id": "1104.4135", "submitter": "Artin Armagan", "authors": "Artin Armagan, David B. Dunson, Jaeyong Lee, Waheed U. Bajwa, Nate\n  Strawn", "title": "Posterior consistency in linear models under shrinkage priors", "comments": "To appear in Biometrika", "journal-ref": "Biometrika, vol. 100, no. 4, pp. 1011-1018, Dec. 2013", "doi": "10.1093/biomet/ast028", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the asymptotic behavior of posterior distributions of\nregression coefficients in high-dimensional linear models as the number of\ndimensions grows with the number of observations. We show that the posterior\ndistribution concentrates in neighborhoods of the true parameter under simple\nsufficient conditions. These conditions hold under popular shrinkage priors\ngiven some sparsity assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2011 21:58:52 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2011 12:39:15 GMT"}, {"version": "v3", "created": "Sat, 7 Apr 2012 22:45:19 GMT"}, {"version": "v4", "created": "Sun, 19 May 2013 14:06:17 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Armagan", "Artin", ""], ["Dunson", "David B.", ""], ["Lee", "Jaeyong", ""], ["Bajwa", "Waheed U.", ""], ["Strawn", "Nate", ""]]}, {"id": "1104.4218", "submitter": "Monica Musio", "authors": "Valentina Mameli and Monica Musio", "title": "A Generalization of the Skew-Normal Distribution: The Beta Skew-Normal", "comments": "21 pages, 3 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this article is to introduce a new family of distributions, which\ngeneralizes the skew normal distribution (SN). This new family, called Beta\nskew-normal (BSN), arises naturally when we consider the distributions of order\nstatistics of the SN. The BSN can also be obtained as a special case of the\nBeta generated distribution (Jones (2004)). In this work we pay attention to\nthree other generalizations of the SN distribution: the Balakrishnan\nskew-normal (SNB) (Balakrishnan (2002), as a discussant of Arnold and Beaver\n(2002), Gupta and Gupta (2004), Sharafi and Behboodian (2008)), the generalized\nBalakrishnan skew-normal (GBSN) (Yadegari et al. (2008)) and a two parameter\ngeneralization of the Balakrishnan skew-normal (TBSN) (Bahrami et al. (2009)).\nThe above three extensions are related to the Beta skew-normal distribution for\nparticular values of the parameters. The paper is organized as follows: after\ndescribing briefly, in Section 2, the skew-normal distribution, its\ngeneralizations and listing their most important properties, in Section 3 we\npresent some generalizations of the Beta distribution. In the last Section we\ndefine the Beta skew-normal distribution, we present its properties and some\nspecial cases. In particular the BSN contains the Beta half-normal distribution\n(Pescim et al. (2010)) as limiting case. Besides, we investigate its shape\nproperties. We derive its moment generating function and we also compute\nnumerically the first moment, the variance, the skewness and the kurtosis. We\npresent two different methods which allow to simulate a BSN distribution. We\nexplore its relationships with the other generalizations of the SN. We give\nsome results concerning the SNB distribution. In particular we derive the exact\ndistributions of the largest order statistic from SNB with parameters m and 1\nand the shortest order statistic SNB with parameters m and -1.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 09:53:52 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Mameli", "Valentina", ""], ["Musio", "Monica", ""]]}, {"id": "1104.4338", "submitter": "Eben Kenah", "authors": "Eben Kenah", "title": "Nonparametric survival analysis of epidemic data", "comments": "30 pages, 6 figures", "journal-ref": "Journal of the Royal Statistical Society B 75(2): 277-303 (2013)", "doi": "10.1111/j.1467-9868.2012.01042.x", "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops nonparametric methods for the survival analysis of\nepidemic data based on contact intervals. The contact interval from person i to\nperson j is the time between the onset of infectiousness in i and infectious\ncontact from i to j, where we define infectious contact as a contact sufficient\nto infect a susceptible individual. We show that the Nelson-Aalen estimator\nproduces an unbiased estimate of the contact interval cumulative hazard\nfunction when who-infects-whom is observed. When who-infects-whom is not\nobserved, we average the Nelson-Aalen estimates from all transmission networks\nconsistent with the observed data using an EM algorithm. This converges to a\nnonparametric MLE of the contact interval cumulative hazard function that we\ncall the marginal Nelson-Aalen estimate. We study the behavior of these methods\nin simulations and use them to analyze household surveillance data from the\n2009 influenza A(H1N1) pandemic. In an appendix, we show that these methods\nextend chain-binomial models to continuous time.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 19:18:14 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Kenah", "Eben", ""]]}, {"id": "1104.4376", "submitter": "Vikram Krishnamurthy", "authors": "Alex Wang and Vikram Krishnamurthy and Bhashyam Balaji", "title": "Intent Inference and Syntactic Tracking with GMTI Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional target tracking systems, human operators use the estimated\ntarget tracks to make higher level inference of the target behaviour/intent.\nThis paper develops syntactic filtering algorithms that assist human operators\nby extracting spatial patterns from target tracks to identify\nsuspicious/anomalous spatial trajectories. The targets' spatial trajectories\nare modeled by a stochastic context free grammar (SCFG) and a switched mode\nstate space model. Bayesian filtering algorithms for stochastic context free\ngrammars are presented for extracting the syntactic structure and illustrated\nfor a ground moving target indicator (GMTI) radar example. The performance of\nthe algorithms is tested with the experimental data collected using DRDC\nOttawa's X-band Wideband Experimental Airborne Radar (XWEAR).\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2011 02:27:45 GMT"}], "update_date": "2011-04-25", "authors_parsed": [["Wang", "Alex", ""], ["Krishnamurthy", "Vikram", ""], ["Balaji", "Bhashyam", ""]]}, {"id": "1104.4580", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Ivan Fernandez-Val, and Amanda Kowalski", "title": "Quantile Regression with Censoring and Endogeneity", "comments": "52 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new censored quantile instrumental variable\n(CQIV) estimator and describe its properties and computation. The CQIV\nestimator combines Powell (1986) censored quantile regression (CQR) to deal\nwith censoring, with a control variable approach to incorporate endogenous\nregressors. The CQIV estimator is obtained in two stages that are non-additive\nin the unobservables. The first stage estimates a non-additive model with\ninfinite dimensional parameters for the control variable, such as a quantile or\ndistribution regression model. The second stage estimates a non-additive\ncensored quantile regression model for the response variable of interest,\nincluding the estimated control variable to deal with endogeneity. For\ncomputation, we extend the algorithm for CQR developed by Chernozhukov and Hong\n(2002) to incorporate the estimation of the control variable. We give generic\nregularity conditions for asymptotic normality of the CQIV estimator and for\nthe validity of resampling methods to approximate its asymptotic distribution.\nWe verify these conditions for quantile and distribution regression estimation\nof the control variable. Our analysis covers two-stage (uncensored) quantile\nregression with non-additive first stage as an important special case. We\nillustrate the computation and applicability of the CQIV estimator with a\nMonte-Carlo numerical example and an empirical application on estimation of\nEngel curves for alcohol.\n", "versions": [{"version": "v1", "created": "Sat, 23 Apr 2011 17:43:04 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2013 02:15:45 GMT"}, {"version": "v3", "created": "Fri, 14 Mar 2014 02:33:34 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fernandez-Val", "Ivan", ""], ["Kowalski", "Amanda", ""]]}, {"id": "1104.4937", "submitter": "James Scott", "authors": "Nicholas G. Polson and James G. Scott", "title": "On the half-Cauchy prior for a global scale parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper argues that the half-Cauchy distribution should replace the\ninverse-Gamma distribution as a default prior for a top-level scale parameter\nin Bayesian hierarchical models, at least for cases where a proper prior is\nnecessary. Our arguments involve a blend of Bayesian and frequentist reasoning,\nand are intended to complement the original case made by Gelman (2006) in\nsupport of the folded-t family of priors. First, we generalize the half-Cauchy\nprior to the wider class of hypergeometric inverted-beta priors. We derive\nexpressions for posterior moments and marginal densities when these priors are\nused for a top-level normal variance in a Bayesian hierarchical model. We go on\nto prove a proposition that, together with the results for moments and\nmarginals, allows us to characterize the frequentist risk of the Bayes\nestimators under all global-shrinkage priors in the class. These theoretical\nresults, in turn, allow us to study the frequentist properties of the\nhalf-Cauchy prior versus a wide class of alternatives. The half-Cauchy occupies\na sensible 'middle ground' within this class: it performs very well near the\norigin, but does not lead to drastic compromises in other parts of the\nparameter space. This provides an alternative, classical justification for the\nrepeated, routine use of this prior. We also consider situations where the\nunderlying mean vector is sparse, where we argue that the usual conjugate\nchoice of an inverse-gamma prior is particularly inappropriate, and can lead to\nhighly distorted posterior inferences. Finally, we briefly summarize some open\nissues in the specification of default priors for scale terms in hierarchical\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2011 14:42:15 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2011 02:50:17 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""]]}, {"id": "1104.5414", "submitter": "Korbinian Strimmer", "authors": "Bernd Klaus and Korbinian Strimmer", "title": "Learning false discovery rates by fitting sigmoidal threshold functions", "comments": "13 pages, 3 figures, 1 table", "journal-ref": "Journal de la Soci\\'et\\'e Fran\\c{c}aise de Statistique 2011, Vol.\n  152, Issue 2, pp. 39-50", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  False discovery rates (FDR) are typically estimated from a mixture of a null\nand an alternative distribution. Here, we study a complementary approach\nproposed by Rice and Spiegelhalter (2008) that uses as primary quantities the\nnull model and a parametric family for the local false discovery rate.\nSpecifically, we consider the half-normal decay and the beta-uniform mixture\nmodels as FDR threshold functions. Using simulations and analysis of real data\nwe compare the performance of the Rice-Spiegelhalter approach with that of\ncompeting FDR estimation procedures. If the alternative model is misspecified\nand an empirical null distribution is employed the accuracy of FDR estimation\ndegrades substantially. Hence, while being a very elegant formalism, the FDR\nthreshold approach requires special care in actual application.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 14:57:53 GMT"}], "update_date": "2011-08-03", "authors_parsed": [["Klaus", "Bernd", ""], ["Strimmer", "Korbinian", ""]]}, {"id": "1104.5429", "submitter": "Caroline Berard", "authors": "Caroline B\\'erard, Marie-Laure Martin-Magniette, V\\'eronique Brunaud,\n  S\\'ebastien Aubourg and St\\'ephane Robin", "title": "Unsupervised Classification for Tiling Arrays: ChIP-chip and\n  Transcriptome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tiling arrays make possible a large scale exploration of the genome thanks to\nprobes which cover the whole genome with very high density until 2 000 000\nprobes. Biological questions usually addressed are either the expression\ndifference between two conditions or the detection of transcribed regions. In\nthis work we propose to consider simultaneously both questions as an\nunsupervised classification problem by modeling the joint distribution of the\ntwo conditions. In contrast to previous methods, we account for all available\ninformation on the probes as well as biological knowledge like annotation and\nspatial dependence between probes. Since probes are not biologically relevant\nunits we propose a classification rule for non-connected regions covered by\nseveral probes. Applications to transcriptomic and ChIP-chip data of\nArabidopsis thaliana obtained with a NimbleGen tiling array highlight the\nimportance of a precise modeling and the region classification.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2011 15:42:24 GMT"}], "update_date": "2011-04-29", "authors_parsed": [["B\u00e9rard", "Caroline", ""], ["Martin-Magniette", "Marie-Laure", ""], ["Brunaud", "V\u00e9ronique", ""], ["Aubourg", "S\u00e9bastien", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1104.5617", "submitter": "Diego Colombo", "authors": "Diego Colombo, Marloes H. Maathuis, Markus Kalisch, Thomas S.\n  Richardson", "title": "Learning high-dimensional directed acyclic graphs with latent and\n  selection variables", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS940 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 1, 294-321", "doi": "10.1214/11-AOS940", "report-no": "IMS-AOS-AOS940", "categories": "stat.ME cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning causal information between random\nvariables in directed acyclic graphs (DAGs) when allowing arbitrarily many\nlatent and selection variables. The FCI (Fast Causal Inference) algorithm has\nbeen explicitly designed to infer conditional independence and causal\ninformation in such settings. However, FCI is computationally infeasible for\nlarge graphs. We therefore propose the new RFCI algorithm, which is much faster\nthan FCI. In some situations the output of RFCI is slightly less informative,\nin particular with respect to conditional independence information. However, we\nprove that any causal information in the output of RFCI is correct in the\nasymptotic limit. We also define a class of graphs on which the outputs of FCI\nand RFCI are identical. We prove consistency of FCI and RFCI in sparse\nhigh-dimensional settings, and demonstrate in simulations that the estimation\nperformances of the algorithms are very similar. All software is implemented in\nthe R-package pcalg.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2011 12:57:10 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2011 08:09:46 GMT"}, {"version": "v3", "created": "Tue, 29 May 2012 13:02:17 GMT"}], "update_date": "2012-05-30", "authors_parsed": [["Colombo", "Diego", ""], ["Maathuis", "Marloes H.", ""], ["Kalisch", "Markus", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1104.5667", "submitter": "Eduardo Mendes", "authors": "Eduardo F. Mendes", "title": "Model Selection Consistency for Cointegrating Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the asymptotic properties of the adaptive Lasso in cointegration\nregressions in the case where all covariates are weakly exogenous. We assume\nthe number of candidate I(1) variables is sub-linear with respect to the sample\nsize (but possibly larger) and the number of candidate I(0) variables is\npolynomial with respect to the sample size. We show that, under classical\nconditions used in cointegration analysis, this estimator asymptotically\nchooses the correct subset of variables in the model and its asymptotic\ndistribution is the same as the distribution of the OLS estimate given the\nvariables in the model were known in beforehand (oracle property). We also\nderive an algorithm based on the local quadratic approximation and present a\nnumerical study to show the adequacy of the method in finite samples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2011 15:46:28 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2011 06:01:27 GMT"}, {"version": "v3", "created": "Mon, 10 Oct 2011 11:02:19 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Mendes", "Eduardo F.", ""]]}]