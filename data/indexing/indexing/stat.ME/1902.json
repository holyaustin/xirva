[{"id": "1902.00161", "submitter": "Andrea Arf\\'e", "authors": "Andrea Arf\\'e and Brian Alexander and Lorenzo Trippa", "title": "Optimality of testing procedures for survival data", "comments": "Accepted for publication in Biometrics on May 27, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most statistical tests for treatment effects used in randomized clinical\ntrials with survival outcomes are based on the proportional hazards assumption,\nwhich often fails in practice. Data from early exploratory studies may provide\nevidence of non-proportional hazards which can guide the choice of alternative\ntests in the design of practice-changing confirmatory trials. We study a test\nto detect treatment effects in a late-stage trial which accounts for the\ndeviations from proportional hazards suggested by early-stage data. Conditional\non early-stage data, among all tests which control the frequentist Type I error\nrate at a fixed $\\alpha$ level, our testing procedure maximizes the Bayesian\nprediction of the finite-sample power. Hence, the proposed test provides a\nuseful benchmark for other tests commonly used in presence of non-proportional\nhazards, for example weighted log-rank tests. We illustrate the approach in a\nsimulations based on data from a published cancer immunotherapy phase III\ntrial.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 03:27:31 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 20:57:36 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 12:52:05 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Arf\u00e9", "Andrea", ""], ["Alexander", "Brian", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "1902.00181", "submitter": "Ursula Laa", "authors": "Ursula Laa, Dianne Cook", "title": "Using tours to visually investigate properties of new projection pursuit\n  indexes with application to problems in physics", "comments": "39 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM hep-ex hep-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projection pursuit is used to find interesting low-dimensional projections of\nhigh-dimensional data by optimizing an index over all possible projections.\nMost indexes have been developed to detect departure from known distributions,\nsuch as normality, or to find separations between known groups. Here, we are\ninterested in finding projections revealing potentially complex bivariate\npatterns, using new indexes constructed from scagnostics and a maximum\ninformation coefficient, with a purpose to detect unusual relationships between\nmodel parameters describing physics phenomena. The performance of these indexes\nis examined with respect to ideal behaviour, using simulated data, and then\napplied to problems from gravitational wave astronomy. The implementation\nbuilds upon the projection pursuit tools available in the R package, tourr,\nwith indexes constructed from code in the R packages, scagnostics, minerva and\nmbgraphic.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 04:46:36 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 04:07:27 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Laa", "Ursula", ""], ["Cook", "Dianne", ""]]}, {"id": "1902.00197", "submitter": "Martin Zhang", "authors": "Martin J. Zhang, James Zou, David Tse", "title": "Adaptive Monte Carlo Multiple Testing via Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo (MC) permutation test is considered the gold standard for\nstatistical hypothesis testing, especially when standard parametric assumptions\nare not clear or likely to fail. However, in modern data science settings where\na large number of hypothesis tests need to be performed simultaneously, it is\nrarely used due to its prohibitive computational cost. In genome-wide\nassociation studies, for example, the number of hypothesis tests $m$ is around\n$10^6$ while the number of MC samples $n$ for each test could be greater than\n$10^8$, totaling more than $nm$=$10^{14}$ samples. In this paper, we propose\nAdaptive MC multiple Testing (AMT) to estimate MC p-values and control false\ndiscovery rate in multiple testing. The algorithm outputs the same result as\nthe standard full MC approach with high probability while requiring only\n$\\tilde{O}(\\sqrt{n}m)$ samples. This sample complexity is shown to be optimal.\nOn a Parkinson GWAS dataset, the algorithm reduces the running time from 2\nmonths for full MC to an hour. The AMT algorithm is derived based on the theory\nof multi-armed bandits.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 06:28:38 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 06:47:11 GMT"}, {"version": "v3", "created": "Sat, 18 May 2019 07:16:26 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Zhang", "Martin J.", ""], ["Zou", "James", ""], ["Tse", "David", ""]]}, {"id": "1902.00242", "submitter": "Ingeborg Gullikstad Hem", "authors": "Geir-Arne Fuglstad, Ingeborg Gullikstad Hem, Alexander Knight,\n  H{\\aa}vard Rue and Andrea Riebler", "title": "Intuitive joint priors for variance parameters", "comments": "64 pages, 33 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance parameters in additive models are typically assigned independent\npriors that do not account for model structure. We present a new framework for\nprior selection based on a hierarchical decomposition of the total variance\nalong a tree structure to the individual model components. For each split in\nthe tree, an analyst may be ignorant or have a sound intuition on how to\nattribute variance to the branches. In the former case a Dirichlet prior is\nappropriate to use, while in the latter case a penalised complexity (PC) prior\nprovides robust shrinkage. A bottom-up combination of the conditional priors\nresults in a proper joint prior. We suggest default values for the\nhyperparameters and offer intuitive statements for eliciting the\nhyperparameters based on expert knowledge. The prior framework is applicable\nfor R packages for Bayesian inference such as INLA and RStan.\n  Three simulation studies show that, in terms of the application-specific\nmeasures of interest, PC priors improve inference over Dirichlet priors when\nused to penalise different levels of complexity in splits. However, when\nexpressing ignorance in a split, Dirichlet priors perform equally well and are\npreferred for their simplicity. We find that assigning current state-of-the-art\ndefault priors for each variance parameter individually is less transparent and\ndoes not perform better than using the proposed joint priors. We demonstrate\npractical use of the new framework by analysing spatial heterogeneity in\nneonatal mortality in Kenya in 2010-2014 based on complex survey data.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 09:25:07 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 11:27:03 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 09:32:39 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Fuglstad", "Geir-Arne", ""], ["Hem", "Ingeborg Gullikstad", ""], ["Knight", "Alexander", ""], ["Rue", "H\u00e5vard", ""], ["Riebler", "Andrea", ""]]}, {"id": "1902.00482", "submitter": "Lulu Kang", "authors": "Victoria Pokhiko and Qiong Zhang and Lulu Kang and D'arcy P. Mays", "title": "D-optimal Design for Network A/B Testing", "comments": "24 pages, 5 figures, 2 tables", "journal-ref": "Journal of Statistical Theory and Practice, 2019", "doi": "10.1007/s42519-019-0058-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A/B testing refers to the statistical procedure of conducting an experiment\nto compare two treatments, A and B, applied to different testing subjects. It\nis widely used by technology companies such as Facebook, LinkedIn, and Netflix,\nto compare different algorithms, web-designs, and other online products and\nservices. The subjects participating these online A/B testing experiments are\nusers who are connected in different scales of social networks. Two connected\nsubjects are similar in terms of their social behaviors, education and\nfinancial background, and other demographic aspects. Hence, it is only natural\nto assume that their reactions to the online products and services are related\nto their network adjacency. In this paper, we propose to use the conditional\nauto-regressive model to present the network structure and include the network\neffects in the estimation and inference of the treatment effect. A D-optimal\ndesign criterion is developed based on the proposed model. Mixed integer\nprogramming formulations are developed to obtain the D-optimal designs. The\neffectiveness of the proposed method is shown through numerical results with\nsynthetic networks and real social networks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 17:58:28 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Pokhiko", "Victoria", ""], ["Zhang", "Qiong", ""], ["Kang", "Lulu", ""], ["Mays", "D'arcy P.", ""]]}, {"id": "1902.00535", "submitter": "Qing Zhou", "authors": "Kun Zhou, Ker-Chau Li, and Qing Zhou", "title": "Honest confidence sets for high-dimensional regression by projection and\n  shrinkage", "comments": "36 pages, 7 figures", "journal-ref": "Journal of the American Statistical Association, 2021", "doi": "10.1080/01621459.2021.1938581", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of honesty in constructing confidence sets arises in nonparametric\nregression. While optimal rate in nonparametric estimation can be achieved and\nutilized to construct sharp confidence sets, severe degradation of confidence\nlevel often happens after estimating the degree of smoothness. Similarly, for\nhigh-dimensional regression, oracle inequalities for sparse estimators could be\nutilized to construct sharp confidence sets. Yet the degree of sparsity itself\nis unknown and needs to be estimated, causing the honesty problem. To resolve\nthis issue, we develop a novel method to construct honest confidence sets for\nsparse high-dimensional linear regression. The key idea in our construction is\nto separate signals into a strong and a weak group, and then construct\nconfidence sets for each group separately. This is achieved by a projection and\nshrinkage approach, the latter implemented via Stein estimation and the\nassociated Stein unbiased risk estimate. Our confidence set is honest over the\nfull parameter space without any sparsity constraints, while its diameter\nadapts to the optimal rate of $n^{-1/4}$ when the true parameter is indeed\nsparse. Through extensive numerical comparisons, we demonstrate that our method\noutperforms other competitors with big margins for finite samples, including\noracle methods built upon the true sparsity of the underlying model.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 19:38:24 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 16:53:56 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhou", "Kun", ""], ["Li", "Ker-Chau", ""], ["Zhou", "Qing", ""]]}, {"id": "1902.00653", "submitter": "Catia  Scricciolo", "authors": "Catia Scricciolo", "title": "On asymptotically efficient maximum likelihood estimation of linear\n  functionals in Laplace measurement error models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimation of linear functionals in the inverse problem of\ndeconvolution is considered. Given observations of a random sample from a\ndistribution $P_0\\equiv P_{F_0}$ indexed by a (potentially\ninfinite-dimensional) parameter $F_0$, which is the distribution of the latent\nvariable in a standard additive Laplace measurement error model, one wants to\nestimate a linear functional of $F_0$. Asymptotically efficient maximum\nlikelihood estimation (MLE) of integral linear functionals of the mixing\ndistribution $F_0$ in a convolution model with the Laplace kernel density is\ninvestigated. Situations are distinguished in which the functional of interest\ncan be consistently estimated at $n^{-1/2}$-rate by the plug-in MLE, which is\nasymptotically normal and efficient, in the sense of achieving the variance\nlower bound, from those in which no integral linear functional can be estimated\nat parametric rate, which precludes any possibility for asymptotic efficiency.\nThe $\\sqrt{n}$-convergence of the MLE, valid in the case of a degenerate mixing\ndistribution at a single location point, fails in general, as does asymptotic\nnormality. It is shown that there exists no regular estimator sequence for\nintegral linear functionals of the mixing distribution that, when recentered\nabout the estimand and $\\sqrt{n}$-rescaled, is asymptotically efficient,\n\\emph{viz}., has Gaussian limit distribution with minimum variance. One can\nthus only expect estimation with some slower rate and, often, with a\nnon-Gaussian limit distribution.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 06:55:59 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Scricciolo", "Catia", ""]]}, {"id": "1902.00772", "submitter": "Jelena Bradic", "authors": "Yuqian Zhang and Jelena Bradic", "title": "High-dimensional semi-supervised learning: in search for optimal\n  inference of the mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a high-dimensional semi-supervised inference framework focused on\nthe mean and variance of the response. Our data are comprised of an extensive\nset of observations regarding the covariate vectors and a much smaller set of\nlabeled observations where we observe both the response as well as the\ncovariates. We allow the size of the covariates to be much larger than the\nsample size and impose weak conditions on a statistical form of the data. We\nprovide new estimators of the mean and variance of the response that extend\nsome of the recent results presented in low-dimensional models. In particular,\nat times we will not necessitate consistent estimation of the functional form\nof the data. Together with estimation of the population mean and variance, we\nprovide their asymptotic distribution and confidence intervals where we\nshowcase gains in efficiency compared to the sample mean and variance. Our\nprocedure, with minor modifications, is then presented to make important\ncontributions regarding inference about average treatment effects. We also\ninvestigate the robustness of estimation and coverage and showcase widespread\napplicability and generality of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 19:28:50 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Zhang", "Yuqian", ""], ["Bradic", "Jelena", ""]]}, {"id": "1902.00791", "submitter": "Marta Crispino", "authors": "Julyan Arbel, Marta Crispino and St\\'ephane Girard", "title": "Dependence properties and Bayesian inference for asymmetric multivariate\n  copulas", "comments": null, "journal-ref": "Journal of Multivariate Analysis, 2019", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a broad class of asymmetric copulas introduced by Liebscher (2008)\nas a combination of multiple - usually symmetric - copulas. The main thrust of\nthe paper is to provide new theoretical properties including exact tail\ndependence expressions and stability properties. A subclass of Liebscher\ncopulas obtained by combining Fr\\'echet copulas is studied in more details. We\nestablish further dependence properties for copulas of this class and show that\nthey are characterized by an arbitrary number of singular components.\nFurthermore, we introduce a novel iterative representation for general\nLiebscher copulas which de facto insures uniform margins, thus relaxing a\nconstraint of Liebscher's original construction. Besides, we show that this\niterative construction proves useful for inference by developing an Approximate\nBayesian computation sampling scheme. This inferential procedure is\ndemonstrated on simulated data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 20:56:31 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 07:36:08 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Arbel", "Julyan", ""], ["Crispino", "Marta", ""], ["Girard", "St\u00e9phane", ""]]}, {"id": "1902.00892", "submitter": "Ruth Heller", "authors": "Ruth Heller and Saharon Rosset", "title": "Optimal control of false discovery criteria in the two-group model", "comments": null, "journal-ref": "J R Stat Soc Series B (2020)", "doi": "10.1111/rssb.12403", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The highly influential two-group model in testing a large number of\nstatistical hypotheses assumes that the test statistics are drawn independently\nfrom a mixture of a high probability null distribution and a low probability\nalternative. Optimal control of the marginal false discovery rate (mFDR), in\nthe sense that it provides maximal power (expected true discoveries) subject to\nmFDR control, is known to be achieved by thresholding the local false discovery\nrate (locFDR), i.e., the probability of the hypothesis being null given the set\nof test statistics, with a fixed threshold. We address the challenge of\ncontrolling optimally the popular false discovery rate (FDR) or positive FDR\n(pFDR) rather than mFDR in the general two-group model, which also allows for\ndependence between the test statistics. These criteria are less conservative\nthan the mFDR criterion, so they make more rejections in expectation. We derive\ntheir optimal multiple testing (OMT) policies, which turn out to be\nthresholding the locFDR with a threshold that is a function of the entire set\nof statistics. We develop an efficient algorithm for finding these policies,\nand use it for problems with thousands of hypotheses. We illustrate these\nprocedures on gene expression studies.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 13:32:12 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 13:30:21 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 17:10:51 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Heller", "Ruth", ""], ["Rosset", "Saharon", ""]]}, {"id": "1902.00931", "submitter": "Radoslav Paulen", "authors": "Anwesh Reddy Gottu Mukkula, Radoslav Paulen", "title": "Optimal Experiment Design in Nonlinear Parameter Estimation with Exact\n  Confidence Regions", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": "10.1016/j.jprocont.2019.01.004", "report-no": null, "categories": "math.OC cs.SY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model-based optimal experiment design (OED) of nonlinear systems is\nstudied. OED represents a methodology for optimizing the geometry of the\nparametric joint-confidence regions (CRs), which are obtained in an a\nposteriori analysis of the least-squares parameter estimates. The optimal\ndesign is achieved by using the available (experimental) degrees of freedom\nsuch that more informative measurements are obtained. Unlike the commonly used\napproaches, which base the OED procedure upon the linearized CRs, we explore a\npath where we explicitly consider the exact CRs in the OED framework. We use a\nmethodology for a finite parametrization of the exact CRs within the OED\nproblem and we introduce a novel approximation technique of the exact CRs using\ninner- and outer-approximating ellipsoids as a computationally less demanding\nalternative. The employed techniques give the OED problem as a\nfinite-dimensional mathematical program of bilevel nature. We use two\nsmall-scale illustrative case studies to study various OED criteria and compare\nthe resulting optimal designs with the commonly used linearization-based\napproach. We also assess the performance of two simple heuristic numerical\nschemes for bilevel optimization within the studied problems.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 17:13:04 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 13:01:34 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Mukkula", "Anwesh Reddy Gottu", ""], ["Paulen", "Radoslav", ""]]}, {"id": "1902.01011", "submitter": "Pritam Ranjan", "authors": "Feng Yang, C. Devon Lin, Pritam Ranjan", "title": "Global Fitting of the Response Surface via Estimating Multiple Contours\n  of a Simulator", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer simulators are nowadays widely used to understand complex physical\nsystems in many areas such as aerospace, renewable energy, climate modeling,\nand manufacturing. One fundamental issue in the study of computer simulators is\nknown as experimental design, that is, how to select the input settings where\nthe computer simulator is run and the corresponding response is collected.\nExtra care should be taken in the selection process because computer simulators\ncan be computationally expensive to run. The selection shall acknowledge and\nachieve the goal of the analysis. This article focuses on the goal of producing\nmore accurate prediction which is important for risk assessment and decision\nmaking. We propose two new methods of design approaches that sequentially\nselect input settings to achieve this goal. The approaches make novel\napplications of simultaneous and sequential contour estimations. Numerical\nexamples are employed to demonstrate the effectiveness of the proposed\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 02:29:46 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Yang", "Feng", ""], ["Lin", "C. Devon", ""], ["Ranjan", "Pritam", ""]]}, {"id": "1902.01075", "submitter": "Claire Lacour", "authors": "Suzanne Varet (LM-Orsay), Claire Lacour (LAMA), Pascal Massart\n  (LM-Orsay), Vincent Rivoirard (CEREMADE)", "title": "Numerical performance of Penalized Comparison to Overfitting for\n  multivariate kernel density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel density estimation is a well known method involving a smoothing\nparameter (the bandwidth) that needs to be tuned by the user. Although this\nmethod has been widely used the bandwidth selection remains a challenging issue\nin terms of balancing algorithmic performance and statistical relevance. The\npurpose of this paper is to compare a recently developped bandwidth selection\nmethod for kernel density estimation to those which are commonly used by now\n(at least those which are implemented in the R-package). This new method is\ncalled Penalized Comparison to Overfitting (PCO). It has been proposed by some\nof the authors of this paper in a previous work devoted to its statistical\nrelevance from a purely theoretical perspective. It is compared here to other\nusual bandwidth selection methods for univariate and also multivariate kernel\ndensity estimation on the basis of intensive simulation studies. In particular,\ncross-validation and plug-in criteria are numerically investigated and compared\nto PCO. The take home message is that PCO can outperform the classical methods\nwithout algorithmic additionnal cost.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 08:14:46 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Varet", "Suzanne", "", "LM-Orsay"], ["Lacour", "Claire", "", "LAMA"], ["Massart", "Pascal", "", "LM-Orsay"], ["Rivoirard", "Vincent", "", "CEREMADE"]]}, {"id": "1902.01237", "submitter": "Marco Oesting", "authors": "Marco Oesting and Alexander Schnurr", "title": "Ordinal Patterns in Clusters of Subsequent Extremes of Regularly Varying\n  Time Series", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate temporal clusters of extremes defined as\nsubsequent exceedances of high thresholds in a stationary time series. Two\nmeaningful features of these clusters are the probability distribution of the\ncluster size and the ordinal patterns within a cluster. Since these patterns\ntake only the ordinal structure of consecutive data points into account the\nmethod is robust under monotone transformations and measurement errors. We\nverify the existence of the corresponding limit distributions in the framework\nof regularly varying time series, develop non-parametric estimators and show\ntheir asymptotic normality under appropriate mixing conditions. The performance\nof the estimators is demonstrated in a simulated example and a real data\napplication to discharge data of the river Rhine.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 15:07:37 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 11:19:42 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Oesting", "Marco", ""], ["Schnurr", "Alexander", ""]]}, {"id": "1902.01289", "submitter": "Evan Baker", "authors": "Evan Baker, Peter Challenor, Matt Eames", "title": "Diagnostics for Stochastic Gaussian Process Emulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer models, also known as simulators, can be computationally expensive\nto run, and for this reason statistical surrogates, known as emulators, are\noften used. Any statistical model, including an emulator, should be validated\nbefore being used, otherwise resulting decisions can be misguided. We discuss\nhow current methods for validating Gaussian process emulators of deterministic\nmodels are insufficient for emulators of stochastic computer models and develop\na framework for diagnosing problems in stochastic emulators. These diagnostics\nare based on independently validating the mean and variance predictions using\nout-of-sample, replicated, simulator runs. We then also use a building\nperformance simulator as a case study example.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 16:31:28 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 12:57:32 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Baker", "Evan", ""], ["Challenor", "Peter", ""], ["Eames", "Matt", ""]]}, {"id": "1902.01290", "submitter": "Evan Baker", "authors": "Evan Baker, Peter Challenor, Matt Eames", "title": "Predicting the Output From a Stochastic Computer Model When a\n  Deterministic Approximation is Available", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2020.1750416", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of computer models can be aided by the construction of surrogate\nmodels, or emulators, that statistically model the numerical computer model.\nIncreasingly, computer models are becoming stochastic, yielding different\noutputs each time they are run, even if the same input values are used.\nStochastic computer models are more difficult to analyse and more difficult to\nemulate - often requiring substantially more computer model runs to fit. We\npresent a method of using deterministic approximations of the computer model to\nbetter construct an emulator. The method is applied to numerous toy examples,\nas well as an idealistic epidemiology model, and a model from the building\nperformance field.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 16:33:18 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Baker", "Evan", ""], ["Challenor", "Peter", ""], ["Eames", "Matt", ""]]}, {"id": "1902.01330", "submitter": "David Miller", "authors": "David L. Miller", "title": "Bayesian views of generalized additive modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Links between frequentist and Bayesian approaches to smoothing were\nhighlighted early on in the smoothing literature, and power much of the\nmachinery that underlies the modern generalized additive modelling framework\n(implemented in software such as the R package $\\texttt{mgcv}$), but they tend\nto be unknown or under appreciated. This article aims to highlight useful links\nbetween Bayesian and frequentist approaches to smoothing, and their practical\napplications (with a somewhat $\\texttt{mgcv}$-centric viewpoint).\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 17:35:28 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Miller", "David L.", ""]]}, {"id": "1902.01352", "submitter": "Vasiliki Koutra Dr", "authors": "Vasiliki Koutra, Steven G. Gilmour, Ben M. Parker", "title": "Optimal block designs for experiments on networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for constructing optimal block designs for experiments on\nnetworks. The response model for a given network interference structure extends\nthe linear network effects model to incorporate blocks. The optimality criteria\nare chosen to reflect the experimental objectives and an exchange algorithm is\nused to search across the design space for obtaining an efficient design when\nan exhaustive search is not possible. Our interest lies in estimating the\ndirect comparisons among treatments, in the presence of nuisance network\neffects that stem from the underlying network interference structure governing\nthe experimental units, or in the network effects themselves. Comparisons of\noptimal designs under different models, including the standard treatment\nmodels, are examined by comparing the variance and bias of treatment effect\nestimators. We also suggest a way of defining blocks, while taking into account\nthe interrelations of groups of experimental units within a network, using\nspectral clustering techniques to achieve optimal modularity. We expect\nconnected units within closed-form communities to behave similarly to an\nexternal stimulus. We provide evidence that our approach can lead to efficiency\ngains over conventional designs such as randomized designs that ignore the\nnetwork structure and we illustrate its usefulness for experiments on networks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 18:14:37 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 13:16:57 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Koutra", "Vasiliki", ""], ["Gilmour", "Steven G.", ""], ["Parker", "Ben M.", ""]]}, {"id": "1902.01377", "submitter": "Forrest Crawford", "authors": "Forrest W. Crawford, Olga Morozova, Ashley L. Buchanan, and Donna\n  Spiegelman", "title": "Interpretation of the individual effect under treatment spillover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some interventions may include important spillover or dissemination effects\nbetween study participants. For example, vaccines, cash transfers, and\neducation programs may exert a causal effect on participants beyond those to\nwhom individual treatment is assigned. In a recent paper, Buchanan et al.\nprovide a causal definition of the \"individual effect\" of an intervention in\nnetworks of people who inject drugs. In this short note, we discuss the\ninterpretation of the individual effect when a spillover or dissemination\neffect exists.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 18:44:36 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Crawford", "Forrest W.", ""], ["Morozova", "Olga", ""], ["Buchanan", "Ashley L.", ""], ["Spiegelman", "Donna", ""]]}, {"id": "1902.01443", "submitter": "Eli Sherman", "authors": "Eli Sherman, Ilya Shpitser", "title": "Identification and Estimation of Causal Effects from Dependent Data", "comments": "Advances in neural information processing systems. 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption that data samples are independent and identically distributed\n(iid) is standard in many areas of statistics and machine learning.\nNevertheless, in some settings, such as social networks, infectious disease\nmodeling, and reasoning with spatial and temporal data, this assumption is\nfalse. An extensive literature exists on making causal inferences under the iid\nassumption [18, 12, 28, 22], even when unobserved confounding bias may be\npresent. But, as pointed out in [20], causal inference in non-iid contexts is\nchallenging due to the presence of both unobserved confounding and data\ndependence. In this paper we develop a general theory describing when causal\ninferences are possible in such scenarios. We use segregated graphs [21], a\ngeneralization of latent projection mixed graphs [30], to represent causal\nmodels of this type and provide a complete algorithm for non-parametric\nidentification in these models. We then demonstrate how statistical inference\nmay be performed on causal parameters identified by this algorithm. In\nparticular, we consider cases where only a single sample is available for parts\nof the model due to full interference, i.e., all units are pathwise dependent\nand neighbors' treatments affect each others' outcomes [26]. We apply these\ntechniques to a synthetic data set which considers users sharing fake news\narticles given the structure of their social network, user activity levels, and\nbaseline demographics and socioeconomic covariates.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 19:56:06 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Sherman", "Eli", ""], ["Shpitser", "Ilya", ""]]}, {"id": "1902.01587", "submitter": "Torsten Hothorn", "authors": "Natalia Korepanova and Heidi Seibold and Verena Steffen and Torsten\n  Hothorn", "title": "Survival Forests under Test: Impact of the Proportional Hazards\n  Assumption on Prognostic and Predictive Forests for ALS Survival", "comments": null, "journal-ref": "Statistical Methods in Medical Research (2020)", "doi": "10.1177/0962280219862586", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the effect of the proportional hazards assumption on\nprognostic and predictive models of the survival time of patients suffering\nfrom amyotrophic lateral sclerosis (ALS). We theoretically compare the\nunderlying model formulations of several variants of survival forests and\nimplementations thereof, including random forests for survival, conditional\ninference forests, Ranger, and survival forests with $L_1$ splitting, with two\nnovel variants, namely distributional and transformation survival forests.\nTheoretical considerations explain the low power of log-rank-based splitting in\ndetecting patterns in non-proportional hazards situations in survival trees and\ncorresponding forests. This limitation can potentially be overcome by the\nalternative split procedures suggested herein. We empirically investigated this\neffect using simulation experiments and a re-analysis of the PRO-ACT database\nof ALS survival, giving special emphasis to both prognostic and predictive\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 08:23:52 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Korepanova", "Natalia", ""], ["Seibold", "Heidi", ""], ["Steffen", "Verena", ""], ["Hothorn", "Torsten", ""]]}, {"id": "1902.01781", "submitter": "Lawrence Middleton", "authors": "Lawrence Middleton, George Deligiannidis, Arnaud Doucet, Pierre E.\n  Jacob", "title": "Unbiased Smoothing using Particle Independent Metropolis-Hastings", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the approximation of expectations with respect to the\ndistribution of a latent Markov process given noisy measurements. This is known\nas the smoothing problem and is often approached with particle and Markov chain\nMonte Carlo (MCMC) methods. These methods provide consistent but biased\nestimators when run for a finite time. We propose a simple way of coupling two\nMCMC chains built using Particle Independent Metropolis-Hastings (PIMH) to\nproduce unbiased smoothing estimators. Unbiased estimators are appealing in the\ncontext of parallel computing, and facilitate the construction of confidence\nintervals. The proposed scheme only requires access to off-the-shelf Particle\nFilters (PF) and is thus easier to implement than recently proposed unbiased\nsmoothers. The approach is demonstrated on a L\\'evy-driven stochastic\nvolatility model and a stochastic kinetic model.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 16:41:00 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Middleton", "Lawrence", ""], ["Deligiannidis", "George", ""], ["Doucet", "Arnaud", ""], ["Jacob", "Pierre E.", ""]]}, {"id": "1902.01978", "submitter": "David Albers", "authors": "DJ Albers, M Levine, L Mamykina and G Hripcsak", "title": "The Parameter Houlihan: a solution to high-throughput identifiability\n  indeterminacy for brutally ill-posed problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to interject knowledge into clinically impactful forecasting is to\nuse data assimilation, a nonlinear regression that projects data onto a\nmechanistic physiologic model, instead of a set of functions, such as neural\nnetworks. Such regressions have an advantage of being useful with particularly\nsparse, non-stationary clinical data. However, physiological models are often\nnonlinear and can have many parameters, leading to potential problems with\nparameter identifiability, or the ability to find a unique set of parameters\nthat minimize forecasting error. The identifiability problems can be minimized\nor eliminated by reducing the number of parameters estimated, but reducing the\nnumber of estimated parameters also reduces the flexibility of the model and\nhence increases forecasting error. We propose a method, the parameter Houlihan,\nthat combines traditional machine learning techniques with data assimilation,\nto select the right set of model parameters to minimize forecasting error while\nreducing identifiability problems. The method worked well: the data\nassimilation-based glucose forecasts and estimates for our cohort using the\nHoulihan-selected parameter sets generally also minimize forecasting errors\ncompared to other parameter selection methods such as by-hand parameter\nselection. Nevertheless, the forecast with the lowest forecast error does not\nalways accurately represent physiology, but further advancements of the\nalgorithm provide a path for improving physiologic fidelity as well. Our hope\nis that this methodology represents a first step toward combining machine\nlearning with data assimilation and provides a lower-threshold entry point for\nusing data assimilation with clinical data by helping select the right\nparameters to estimate.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 23:42:25 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Albers", "DJ", ""], ["Levine", "M", ""], ["Mamykina", "L", ""], ["Hripcsak", "G", ""]]}, {"id": "1902.02412", "submitter": "Quinten Meertens", "authors": "Q. A. Meertens, C. G. H. Diks, H. J. van den Herik, F W Takes", "title": "A Bayesian Approach for Accurate Classification-Based Aggregates", "comments": "9 pages, 5 figures, accepted conference paper, SIAM International\n  Conference on Data Mining 2019 (SDM19)", "journal-ref": null, "doi": "10.1137/1.9781611975673.35", "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the accuracy of values aggregated over classes\npredicted by a classification algorithm. The problem is that the resulting\naggregates (e.g., sums of a variable) are known to be biased. The bias can be\nlarge even for highly accurate classification algorithms, in particular when\ndealing with class-imbalanced data. To correct this bias, the algorithm's\nclassification error rates have to be estimated. In this estimation, two issues\narise when applying existing bias correction methods. First, inaccuracies in\nestimating classification error rates have to be taken into account. Second,\nimpermissible estimates, such as a negative estimate for a positive value, have\nto be dismissed. We show that both issues are relevant in applications where\nthe true labels are known only for a small set of data points. We propose a\nnovel bias correction method using Bayesian inference. The novelty of our\nmethod is that it imposes constraints on the model parameters. We show that our\nmethod solves the problem of biased classification-based aggregates as well as\nthe two issues above, in the general setting of multi-class classification. In\nthe empirical evaluation, using a binary classifier on a real-world dataset of\ncompany tax returns, we show that our method outperforms existing methods in\nterms of mean squared error.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 22:18:42 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Meertens", "Q. A.", ""], ["Diks", "C. G. H.", ""], ["Herik", "H. J. van den", ""], ["Takes", "F W", ""]]}, {"id": "1902.02624", "submitter": "Philipp Frank", "authors": "Philipp Frank, Reimar Leike, and Torsten A. En{\\ss}lin", "title": "Field dynamics inference for local and causal interactions", "comments": "NIFTy5 release paper, 25 pages, 11 figures, code is part of NIFTy5\n  release at https://gitlab.mpcdf.mpg.de/ift/nifty", "journal-ref": null, "doi": "10.1002/andp.202000486", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inference of fields defined in space and time from observational data is a\ncore discipline in many scientific areas. This work approaches the problem in a\nBayesian framework. The proposed method is based on statistically homogeneous\nrandom fields defined in space and time and demonstrates how to reconstruct the\nfield together with its prior correlation structure from data. The prior model\nof the correlation structure is described in a non-parametric fashion and\nsolely builds on fundamental physical assumptions such as space-time\nhomogeneity, locality, and causality. These assumptions are sufficient to\nsuccessfully infer the field and its prior correlation structure from noisy and\nincomplete data of a single realization of the process as demonstrated via\nmultiple numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 10:39:46 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 11:19:02 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 09:48:59 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Frank", "Philipp", ""], ["Leike", "Reimar", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1902.02774", "submitter": "Nikolaos Ignatiadis", "authors": "Nikolaos Ignatiadis, Stefan Wager", "title": "Confidence Intervals for Nonparametric Empirical Bayes Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an empirical Bayes analysis, we use data from repeated sampling to imitate\ninferences made by an oracle Bayesian with extensive knowledge of the\ndata-generating distribution. Existing results provide a comprehensive\ncharacterization of when and why empirical Bayes point estimates accurately\nrecover oracle Bayes behavior. In this paper, we develop flexible and practical\nconfidence intervals that provide asymptotic frequentist coverage of empirical\nBayes estimands, such as the posterior mean or the local false sign rate. The\ncoverage statements hold even when the estimands are only partially identified\nor when empirical Bayes point estimates converge very slowly.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 18:57:36 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 00:43:56 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 18:29:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Ignatiadis", "Nikolaos", ""], ["Wager", "Stefan", ""]]}, {"id": "1902.02776", "submitter": "Bryan Martin", "authors": "Bryan D. Martin, Daniela Witten, Amy D. Willis", "title": "Modeling microbial abundances and dysbiosis with beta-binomial\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a sample from a population to estimate the proportion of the population\nwith a certain category label is a broadly important problem. In the context of\nmicrobiome studies, this problem arises when researchers wish to use a sample\nfrom a population of microbes to estimate the population proportion of a\nparticular taxon, known as the taxon's relative abundance. In this paper, we\npropose a beta-binomial model for this task. Like existing models, our model\nallows for a taxon's relative abundance to be associated with covariates of\ninterest. However, unlike existing models, our proposal also allows for the\noverdispersion in the taxon's counts to be associated with covariates of\ninterest. We exploit this model in order to propose tests not only for\ndifferential relative abundance, but also for differential variability. The\nlatter is particularly valuable in light of speculation that dysbiosis, the\nperturbation from a normal microbiome that can occur in certain disease\nconditions, may manifest as a loss of stability, or increase in variability, of\nthe counts associated with each taxon. We demonstrate the performance of our\nproposed model using a simulation study and an application to soil microbial\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 18:57:41 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Martin", "Bryan D.", ""], ["Witten", "Daniela", ""], ["Willis", "Amy D.", ""]]}, {"id": "1902.02824", "submitter": "Alexey Zaytsev", "authors": "S. Kumbhakar, A. Peresetsky, Y. Shchetynin, A. Zaytsev", "title": "Technical efficiency and inefficiency: SFA misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effect of external factors $z$ on technical inefficiency ($TI$) in\nstochastic frontier (SF) production models is often specified through the\nvariance of inefficiency term $u$. In this setup the signs of marginal effects\nof $z$ on $TI$ and technical efficiency $TE$ identify how one should control\n$z$ to increase $TI$ or decrease $TE$. We prove that these signs for $TI$ and\n$TE$ are opposite for typical setups with normally distributed random error $v$\nand exponentially or half-normally distributed $u$ for both conditional and\nunconditional cases.\n  On the other hand, we give an example to show that signs of the marginal\neffects of $z$ on $TI$ and $TE$ may coincide, at least for some ranges of $z$.\nIn our example, the distribution of $u$ is a mixture of two distributions, and\nthe proportion of the mixture is a function of $z$. Thus if the real data comes\nfrom this mixture distribution, and we estimate model parameters with an\nexponential or half-normal distribution for $u$, the estimated efficiency and\nthe marginal effect of $z$ on $TE$ would be wrong. Moreover, for a misspecified\nmodel, the rank correlations between the true and the estimated values of TE\ncould be small and even negative for some subsamples of data. These results are\ndemonstrated by simulations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 20:05:42 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 22:08:04 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2019 20:22:51 GMT"}, {"version": "v4", "created": "Mon, 10 Feb 2020 07:11:17 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Kumbhakar", "S.", ""], ["Peresetsky", "A.", ""], ["Shchetynin", "Y.", ""], ["Zaytsev", "A.", ""]]}, {"id": "1902.03027", "submitter": "Mathias Raschke -", "authors": "Mathias Raschke", "title": "Alternative modelling and inference methods for claim size distributions", "comments": null, "journal-ref": "Ann. actuar. sci. 14 (2020) 1-19", "doi": "10.1017/S1748499519000010", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The upper tail of a claim size distribution of a property line of business is\nfrequently modelled by Pareto distribution. However, the upper tail does not\nneed to be Pareto distributed, extraordinary shapes are possible. Here, the\nopportunities for the modelling of loss distributions are extended. The basic\nidea is the adjustment of a base distribution for their tails. The\n(generalised) Pareto distribution is used as base distribution for different\nreasons. The upper tail is in the focus and can be modelled well for special\ncases by a discrete mixture of the base distribution with a combination of the\nbase distribution with an adapting distribution via the product of their\nsurvival functions. A kind of smoothed step is realised in this way in the\noriginal line function between logarithmic loss and logarithmic exceedance\nprobability. The lower tail can also be adjusted. The new approaches offer the\nopportunity for stochastic interpretation and are applied to observed losses.\nFor parameter estimation, a modification of the minimum Anderson Darling\ndistance method is used. A new test is suggested to exclude that the observed\nupper tail is better modelled by a simple Pareto distribution. Q-Q plots are\napplied, and secondary results are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 11:38:58 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 15:42:05 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Raschke", "Mathias", ""]]}, {"id": "1902.03095", "submitter": "Daniela De Canditiis", "authors": "Daniela De Canditiis and Italia De Feis", "title": "Simultaneous nonparametric regression in RADWT dictionaries", "comments": "38 pages, 8 figures", "journal-ref": "Computational Statistics & Data Analysis 134: 36-57 (2019)", "doi": "10.1016/j.csda.2018.11.003", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new technique for nonparametric regression of multichannel signals is\npresented. The technique is based on the use of the Rational-Dilation Wavelet\nTransform (RADWT), equipped with a tunable Q-factor able to provide sparse\nrepresentations of functions with different oscillations persistence. In\nparticular, two different frames are obtained by two RADWT with different\nQ-factors that give sparse representations of functions with low and high\nresonance. It is assumed that the signals are measured simultaneously on\nseveral independent channels and that they share the low resonance component\nand the spectral characteristics of the high resonance component. Then, a\nregression analysis is performed by means of the grouped lasso penalty.\nFurthermore, a result of asymptotic optimality of the estimator is presented\nusing reasonable assumptions and exploiting recent results on group-lasso like\nprocedures. Numerical experiments show the performance of the proposed method\nin different synthetic scenarios as well as in a real case example for the\nanalysis and joint detection of sleep spindles and K-complex events for\nmultiple electroencephalogram (EEG) signals.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 14:27:46 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["De Canditiis", "Daniela", ""], ["De Feis", "Italia", ""]]}, {"id": "1902.03116", "submitter": "Daniela De Canditiis", "authors": "Daniela De Canditiis and Armando Guardasole", "title": "Learning Gaussian Graphical Models by symmetric parallel regression\n  technique", "comments": "12 pages, 5 figures", "journal-ref": "2018 IMACS Series in Computational and Applied Mathematics ISSN\n  1098-870X", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution we deal with the problem of learning an undirected graph\nwhich encodes the conditional dependence relationship between variables of a\ncomplex system, given a set of observations of this system. This is a very\ncentral problem of modern data analysis and it comes out every time we want to\ninvestigate a deeper relationship between random variables, which is different\nfrom the classical dependence usually measured by the covariance.\n  In particular, in this contribution we deal with the case of Gaussian\nGraphical Models (GGMs) for which the system of variables has a multivariate\ngaussian distribution. We study all the existing techniques for such a problem\nand propose a smart implementation of the symmetric parallel regression\ntechnique which turns out to be very competitive for learning sparse GGMs under\nhigh dimensional data regime.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 14:48:57 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["De Canditiis", "Daniela", ""], ["Guardasole", "Armando", ""]]}, {"id": "1902.03175", "submitter": "Edwin Fong", "authors": "Edwin Fong, Simon Lyddon, Chris Holmes", "title": "Scalable Nonparametric Sampling from Multimodal Posteriors with the\n  Posterior Bootstrap", "comments": "Accepted at International Conference on Machine Learning (ICML) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly complex datasets pose a number of challenges for Bayesian\ninference. Conventional posterior sampling based on Markov chain Monte Carlo\ncan be too computationally intensive, is serial in nature and mixes poorly\nbetween posterior modes. Further, all models are misspecified, which brings\ninto question the validity of the conventional Bayesian update. We present a\nscalable Bayesian nonparametric learning routine that enables posterior\nsampling through the optimization of suitably randomized objective functions. A\nDirichlet process prior on the unknown data distribution accounts for model\nmisspecification, and admits an embarrassingly parallel posterior bootstrap\nalgorithm that generates independent and exact samples from the nonparametric\nposterior distribution. Our method is particularly adept at sampling from\nmultimodal posterior distributions via a random restart mechanism. We\ndemonstrate our method on Gaussian mixture model and sparse logistic regression\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 16:37:25 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 14:51:22 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Fong", "Edwin", ""], ["Lyddon", "Simon", ""], ["Holmes", "Chris", ""]]}, {"id": "1902.03241", "submitter": "Natsumi Makigusa", "authors": "Natsumi Makigusa and Kanta Naito", "title": "Asymptotics and practical aspects of testing normality with kernel\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with testing normality in a Hilbert space based on\nthe maximum mean discrepancy. Specifically, we discuss the behavior of the test\nfrom two standpoints: asymptotics and practical aspects. Asymptotic normality\nof the test under a fixed alternative hypothesis is developed, which implies\nthat the test has consistency. Asymptotic distribution of the test under a\nsequence of local alternatives is also derived, from which asymptotic null\ndistribution of the test is obtained. A concrete expression for the integral\nkernel associated with the null distribution is derived under the use of the\nGaussian kernel, allowing the implementation of a reliable approximation of the\nnull distribution. Simulations and applications to real data sets are reported\nwith emphasis on high-dimension low-sample size cases.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 04:11:33 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Makigusa", "Natsumi", ""], ["Naito", "Kanta", ""]]}, {"id": "1902.03291", "submitter": "Xianyang Zhang", "authors": "Changbo Zhu, Shun Yao, Xianyang Zhang and Xiaofeng Shao", "title": "Distance-based and RKHS-based Dependence Metrics in High Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study distance covariance, Hilbert-Schmidt covariance (aka\nHilbert-Schmidt independence criterion [Gretton et al. (2008)]) and related\nindependence tests under the high dimensional scenario. We show that the sample\ndistance/Hilbert-Schmidt covariance between two random vectors can be\napproximated by the sum of squared componentwise sample cross-covariances up to\nan asymptotically constant factor, which indicates that the\ndistance/Hilbert-Schmidt covariance based test can only capture linear\ndependence in high dimension. As a consequence, the distance correlation based\nt-test developed by Szekely and Rizzo (2013) for independence is shown to have\ntrivial limiting power when the two random vectors are nonlinearly dependent\nbut component-wisely uncorrelated. This new and surprising phenomenon, which\nseems to be discovered for the first time, is further confirmed in our\nsimulation study. As a remedy, we propose tests based on an aggregation of\nmarginal sample distance/Hilbert-Schmidt covariances and show their superior\npower behavior against their joint counterparts in simulations. We further\nextend the distance correlation based t-test to those based on Hilbert-Schmidt\ncovariance and marginal distance/Hilbert-Schmidt covariance. A novel unified\napproach is developed to analyze the studentized sample\ndistance/Hilbert-Schmidt covariance as well as the studentized sample marginal\ndistance covariance under both null and alternative hypothesis. Our theoretical\nand simulation results shed light on the limitation of distance/Hilbert-Schmidt\ncovariance when used jointly in the high dimensional setting and suggest the\naggregation of marginal distance/Hilbert-Schmidt covariance as a useful\nalternative.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 21:01:29 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Zhu", "Changbo", ""], ["Yao", "Shun", ""], ["Zhang", "Xianyang", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "1902.03295", "submitter": "Subhajit Dutta Dr.", "authors": "Sarbojit Roy, Soham Sarkar, Subhajit Dutta and Anil K. Ghosh", "title": "On Generalizations of Some Distance Based Classifiers for HDLSS Data", "comments": "37 pages, full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimension, low sample size (HDLSS) settings, classifiers based on\nEuclidean distances like the nearest neighbor classifier and the average\ndistance classifier perform quite poorly if differences between locations of\nthe underlying populations get masked by scale differences. To rectify this\nproblem, several modifications of these classifiers have been proposed in the\nliterature. However, existing methods are confined to location and scale\ndifferences only, and often fail to discriminate among populations differing\noutside of the first two moments. In this article, we propose some simple\ntransformations of these classifiers resulting into improved performance even\nwhen the underlying populations have the same location and scale. We further\npropose a generalization of these classifiers based on the idea of grouping of\nvariables. The high-dimensional behavior of the proposed classifiers is studied\ntheoretically. Numerical experiments with a variety of simulated examples as\nwell as an extensive analysis of real data sets exhibit advantages of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 21:26:36 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 07:31:41 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 16:21:32 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Roy", "Sarbojit", ""], ["Sarkar", "Soham", ""], ["Dutta", "Subhajit", ""], ["Ghosh", "Anil K.", ""]]}, {"id": "1902.03308", "submitter": "Siliang Gong", "authors": "Siliang Gong, Kai Zhang, and Yufeng Liu", "title": "Penalized linear regression with high-dimensional pairwise screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In variable selection, most existing screening methods focus on marginal\neffects and ignore dependence between covariates. To improve the performance of\nselection, we incorporate pairwise effects in covariates for screening and\npenalization. We achieve this by studying the asymptotic distribution of the\nmaximal absolute pairwise sample correlation among independent covariates. The\nnovelty of the theory is in that the convergence is with respect to the\ndimensionality $p$, and is uniform with respect to the sample size $n$.\nMoreover, we obtain an upper bound for the maximal pairwise R squared when\nregressing the response onto two different covariates. Based on these extreme\nvalue results, we propose a screening procedure to detect covariates pairs that\nare potentially correlated and associated with the response. We further combine\nthe pairwise screening with Sure Independence Screening and develop a new\nregularized variable selection procedure. Numerical studies show that our\nmethod is very competitive in terms of both prediction accuracy and variable\nselection accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 22:15:39 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Gong", "Siliang", ""], ["Zhang", "Kai", ""], ["Liu", "Yufeng", ""]]}, {"id": "1902.03316", "submitter": "Youngseok Kim", "authors": "Youngseok Kim, Chao Gao", "title": "Bayesian Model Selection with Graph Structured Sparsity", "comments": null, "journal-ref": "Journal of Machine Learning Research 21(109):1-61, 2020", "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general algorithmic framework for Bayesian model selection. A\nspike-and-slab Laplacian prior is introduced to model the underlying structural\nassumption. Using the notion of effective resistance, we derive an EM-type\nalgorithm with closed-form iterations to efficiently explore possible\ncandidates for Bayesian model selection. The deterministic nature of the\nproposed algorithm makes it more scalable to large-scale and high-dimensional\ndata sets compared with existing stochastic search algorithms. When applied to\nsparse linear regression, our framework recovers the EMVS algorithm [Rockova\nand George, 2014] as a special case. We also discuss extensions of our\nframework using tools from graph algebra to incorporate complex Bayesian models\nsuch as biclustering and submatrix localization. Extensive simulation studies\nand real data applications are conducted to demonstrate the superior\nperformance of our methods over its frequentist competitors such as $\\ell_0$ or\n$\\ell_1$ penalization.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 22:51:03 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 03:17:07 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Kim", "Youngseok", ""], ["Gao", "Chao", ""]]}, {"id": "1902.03327", "submitter": "Jelena Bradic", "authors": "Alexander Hanbo Li, Jelena Bradic", "title": "Censored Quantile Regression Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests are powerful non-parametric regression method but are severely\nlimited in their usage in the presence of randomly censored observations, and\nnaively applied can exhibit poor predictive performance due to the incurred\nbiases. Based on a local adaptive representation of random forests, we develop\nits regression adjustment for randomly censored regression quantile models.\nRegression adjustment is based on new estimating equations that adapt to\ncensoring and lead to quantile score whenever the data do not exhibit\ncensoring. The proposed procedure named censored quantile regression forest,\nallows us to estimate quantiles of time-to-event without any parametric\nmodeling assumption. We establish its consistency under mild model\nspecifications. Numerical studies showcase a clear advantage of the proposed\nprocedure.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 23:29:50 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Li", "Alexander Hanbo", ""], ["Bradic", "Jelena", ""]]}, {"id": "1902.03347", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen", "title": "Asymptotic normality of the time-domain generalized least squares\n  estimator for linear regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In linear models, the generalized least squares (GLS) estimator is applicable\nwhen the structure of the error dependence is known. When it is unknown, such\nstructure must be approximated and estimated in a manner that may lead to\nmisspecification. The large-sample analysis of incorrectly-specified GLS (IGLS)\nestimators requires careful asymptotic manipulations. When performing\nestimation in the frequency domain, the asymptotic normality of the IGLS\nestimator, under the so-called Grenander assumptions, has been proved for a\nbroad class of error dependence models. Under the same assumptions, asymptotic\nnormality results for the time-domain IGLS estimator are only available for a\nlimited class of error structures. We prove that the time-domain IGLS estimator\nis asymptotically normal for a general class of dependence models.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 01:21:51 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Nguyen", "Hien D", ""]]}, {"id": "1902.03456", "submitter": "Holger Dette", "authors": "Kathrin M\\\"ollenhoff, Frank Bretz and Holger Dette", "title": "Equivalence of regression curves sharing common parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical trials the comparison of two different populations is a\nfrequently addressed problem. Non-linear (parametric) regression models are\ncommonly used to describe the relationship between covariates as the dose and a\nresponse variable in the two groups. In some situations it is reasonable to\nassume some model parameters to be the same, for instance the placebo effect or\nthe maximum treatment effect. In this paper we develop a (parametric) bootstrap\ntest to establish the similarity of two regression curves sharing some common\nparameters. We show by theoretical arguments and by means of a simulation study\nthat the new test controls its level and achieves a reasonable power. Moreover,\nit is demonstrated that under the assumption of common parameters a\nconsiderable more powerful test can be constructed compared to the test which\ndoes not use this assumption. Finally, we illustrate potential applications of\nthe new methodology by a clinical trial example.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 17:42:31 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["M\u00f6llenhoff", "Kathrin", ""], ["Bretz", "Frank", ""], ["Dette", "Holger", ""]]}, {"id": "1902.03525", "submitter": "Min Zhou", "authors": "Min Zhou, Mingwei Dai, Yuan Yao, Jin Liu, Can Yang, Heng Peng", "title": "BOLT-SSI: A Statistical Approach to Screening Interaction Effects for\n  Ultra-High Dimensional Data", "comments": "56 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting interaction effects among predictors on the response variable is a\ncrucial step in various applications. In this paper, we first propose a simple\nmethod for sure screening interactions (SSI). Although its computation\ncomplexity is $O(p^2n)$, SSI works well for problems of moderate dimensionality\n(e.g., $p=10^3\\sim10^4$), without the heredity assumption. To ultra-high\ndimensional problems (e.g., $p = 10^6$), motivated by discretization associated\nBoolean representation and operations and the contingency table for discrete\nvariables, we propose a fast algorithm, named \"BOLT-SSI\". The statistical\ntheory has been established for SSI and BOLT-SSI, guaranteeing their sure\nscreening property. The performance of SSI and BOLT-SSI are evaluated by\ncomprehensive simulation and real case studies. Numerical results demonstrate\nthat SSI and BOLT-SSI can often outperform their competitors in terms of\ncomputational efficiency and statistical accuracy. The proposed method can be\napplied for fully detecting interactions with more than 300,000 predictors.\nBased on this study, we believe that there is a great need to rethink the\nrelationship between statistical accuracy and computational efficiency. We have\nshown that the computational performance of a statistical method can often be\ngreatly improved by exploring the advantages of computational architecture with\na tolerable loss of statistical accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 01:32:32 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 08:32:10 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Zhou", "Min", ""], ["Dai", "Mingwei", ""], ["Yao", "Yuan", ""], ["Liu", "Jin", ""], ["Yang", "Can", ""], ["Peng", "Heng", ""]]}, {"id": "1902.03651", "submitter": "Peyman Jalali", "authors": "Peyman Jalali, Kshitij Khare and George Michailidis", "title": "A Bayesian Approach to Joint Estimation of Multiple Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of joint estimation of multiple graphical models from high\ndimensional data has been studied in the statistics and machine learning\nliterature, due to its importance in diverse fields including molecular\nbiology, neuroscience and the social sciences. This work develops a Bayesian\napproach that decomposes the model parameters across the multiple graphical\nmodels into shared components across subsets of models and edges, and\nidiosyncratic ones. Further, it leverages a novel multivariate prior\ndistribution, coupled with a pseudo-likelihood that enables fast computations\nthrough a robust and efficient Gibbs sampling scheme. We establish strong\nposterior consistency for model selection, as well as estimation of model\nparameters under high dimensional scaling with the number of variables growing\nexponentially with the sample size. The efficacy of the proposed approach is\nillustrated on both synthetic and real data.\n  Keywords: Pseudo-likelihood, Gibbs sampling, posterior consistency, Omics\ndata\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 18:52:09 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 20:14:08 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Jalali", "Peyman", ""], ["Khare", "Kshitij", ""], ["Michailidis", "George", ""]]}, {"id": "1902.03674", "submitter": "Xiaoxiao Sun", "authors": "Xiaoxiao Sun, Pang Du, Xiao Wang, Ping Ma", "title": "Optimal Penalized Function-on-Function Regression under a Reproducing\n  Kernel Hilbert Space Framework", "comments": null, "journal-ref": "Journal of the American Statistical Association, 113:524,\n  1601-1611", "doi": "10.1080/01621459.2017.1356320", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific studies collect data where the response and predictor\nvariables are both functions of time, location, or some other covariate.\nUnderstanding the relationship between these functional variables is a common\ngoal in these studies. Motivated from two real-life examples, we present in\nthis paper a function-on-function regression model that can be used to analyze\nsuch kind of functional data. Our estimator of the 2D coefficient function is\nthe optimizer of a form of penalized least squares where the penalty enforces a\ncertain level of smoothness on the estimator. Our first result is the\nRepresenter Theorem which states that the exact optimizer of the penalized\nleast squares actually resides in a data-adaptive finite dimensional subspace\nalthough the optimization problem is defined on a function space of infinite\ndimensions. This theorem then allows us an easy incorporation of the Gaussian\nquadrature into the optimization of the penalized least squares, which can be\ncarried out through standard numerical procedures. We also show that our\nestimator achieves the minimax convergence rate in mean prediction under the\nframework of function-on-function regression. Extensive simulation studies\ndemonstrate the numerical advantages of our method over the existing ones,\nwhere a sparse functional data extension is also introduced. The proposed\nmethod is then applied to our motivating examples of the benchmark Canadian\nweather data and a histone regulation study.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 21:35:19 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Sun", "Xiaoxiao", ""], ["Du", "Pang", ""], ["Wang", "Xiao", ""], ["Ma", "Ping", ""]]}, {"id": "1902.03932", "submitter": "Andrew Wilson", "authors": "Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, Andrew Gordon\n  Wilson", "title": "Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning", "comments": "Published at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The posteriors over neural network weights are high dimensional and\nmultimodal. Each mode typically characterizes a meaningfully different\nrepresentation of the data. We develop Cyclical Stochastic Gradient MCMC\n(SG-MCMC) to automatically explore such distributions. In particular, we\npropose a cyclical stepsize schedule, where larger steps discover new modes,\nand smaller steps characterize each mode. We also prove non-asymptotic\nconvergence of our proposed algorithm. Moreover, we provide extensive\nexperimental results, including ImageNet, to demonstrate the scalability and\neffectiveness of cyclical SG-MCMC in learning complex multimodal distributions,\nespecially for fully Bayesian inference with modern deep neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 15:03:30 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 20:49:28 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zhang", "Ruqi", ""], ["Li", "Chunyuan", ""], ["Zhang", "Jianyi", ""], ["Chen", "Changyou", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1902.04200", "submitter": "Alexander Keil", "authors": "Alexander P. Keil, Jessie P. Buckley, Katie M. OBrien, Kelly K.\n  Ferguson, Shanshan Zhao, Alexandra J. White", "title": "A quantile-based g-computation approach to addressing the effects of\n  exposure mixtures", "comments": "Main manuscript (3 figures, 4 tables, 7000 words) + appendix", "journal-ref": null, "doi": "10.1289/EHP5838", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposure mixtures frequently occur in data across many domains, particularly\nin the fields of environmental and nutritional epidemiology. Various strategies\nhave arisen to answer questions about mixtures, including methods such as\nweighted quantile sum (WQS) regression that estimate a joint effect of the\nmixture components.We demonstrate a new approach to estimating the joint\neffects of a mixture: quantile g-computation. This approach combines the\ninferential simplicity of WQS regression with the flexibility of g-computation,\na method of causal effect estimation. We use simulations to examine whether\nquantile g-computation and WQS regression can accurately and precisely estimate\neffects of mixtures in common scenarios. We examine the bias, confidence\ninterval coverage, and bias-variance tradeoff of quantile g-computation and WQS\nregression, and how these quantities are impacted by the presence of non-causal\nexposures, exposure correlation, unmeasured confounding, and non-linear\neffects. Quantile g-computation, unlike WQS regression allows inference on\nmixture effects that is unbiased with appropriate confidence interval coverage\nat sample sizes typically encountered in epidemiologic studies and when the\nassumptions of WQS regression are not met. Further, WQS regression can magnify\nbias from unmeasured confounding that might occur if important components of\nthe mixture are omitted. Unlike inferential approaches that examine effects of\nindividual exposures, methods like quantile g-computation that can estimate the\neffect of a mixture are essential for understanding effects of potential public\nhealth actions that act on exposure sources. Our approach may serve to help\nbridge gaps between epidemiologic analysis and interventions such as\nregulations on industrial emissions or mining processes, dietary changes, or\nconsumer behavioral changes that act on multiple exposures simultaneously.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 01:09:33 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 14:12:13 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 19:17:09 GMT"}, {"version": "v4", "created": "Wed, 11 Mar 2020 21:03:49 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Keil", "Alexander P.", ""], ["Buckley", "Jessie P.", ""], ["OBrien", "Katie M.", ""], ["Ferguson", "Kelly K.", ""], ["Zhao", "Shanshan", ""], ["White", "Alexandra J.", ""]]}, {"id": "1902.04349", "submitter": "Sirio Legramanti", "authors": "Sirio Legramanti, Daniele Durante and David B. Dunson", "title": "Bayesian cumulative shrinkage for infinite factorizations", "comments": "8 pages, 2 tables and 1 figure; v2: more theory, algorithms (adaptive\n  Gibbs sampler added) and empirical studies (simulations, sensitivity\n  analyses, real-data application); v3: small typos fixed and GitHub link added\n  to the abstract", "journal-ref": "Biometrika (2020), 107(3), 745-752", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a wide variety of models in which the dimension of the parameter\nspace is unknown. For example, in factor analysis the number of latent factors\nis typically not known and has to be inferred from the observed data. Although\nclassical shrinkage priors are useful in these contexts, increasing shrinkage\npriors can provide a more effective option, which progressively penalizes\nexpansions with growing complexity. In this article we propose a novel\nincreasing shrinkage prior, named the cumulative shrinkage process, for the\nparameters controlling the dimension in over-complete formulations. Our\nconstruction has broad applicability, simple interpretation, and is based on a\nsequence of spike and slab distributions which assign increasing mass to the\nspike as model complexity grows. Using factor analysis as an illustrative\nexample, we show that this formulation has theoretical and practical advantages\nover current competitors, including an improved ability to recover the model\ndimension. An adaptive Markov chain Monte Carlo algorithm is proposed, and the\nmethods are evaluated in simulation studies and applied to personality traits\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 12:03:22 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 07:36:21 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 16:27:23 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Legramanti", "Sirio", ""], ["Durante", "Daniele", ""], ["Dunson", "David B.", ""]]}, {"id": "1902.04673", "submitter": "Henry Lam", "authors": "Henry Lam, Xinyu Zhang, Xuhui Zhang", "title": "Enhanced Balancing of Bias-Variance Tradeoff in Stochastic Estimation: A\n  Minimax Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biased stochastic estimators, such as finite-differences for noisy gradient\nestimation, often contain parameters that need to be properly chosen to balance\nimpacts from the bias and the variance. While the optimal order of these\nparameters in terms of the simulation budget can be readily established, the\nprecise best values depend on model characteristics that are typically unknown\nin advance. We introduce a framework to construct new classes of estimators,\nbased on judicious combinations of simulation runs on sequences of tuning\nparameter values, such that the estimators consistently outperform a given\ntuning parameter choice in the conventional approach, regardless of the unknown\nmodel characteristics. We argue the outperformance via what we call the\nasymptotic minimax risk ratio, obtained by minimizing the worst-case asymptotic\nratio between the mean square errors of our estimators and the conventional\none, where the worst case is over any possible values of the model unknowns. In\nparticular, when the minimax ratio is less than 1, the calibrated estimator is\nguaranteed to perform better asymptotically. We identify this minimax ratio for\ngeneral classes of weighted estimators, and the regimes where this ratio is\nless than 1. Moreover, we show that the best weighting scheme is characterized\nby a sum of two components with distinct decay rates. We explain how this\narises from bias-variance balancing that combats the adversarial selection of\nthe model constants, which can be analyzed via a tractable reformulation of a\nnon-convex optimization problem.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 23:35:38 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Lam", "Henry", ""], ["Zhang", "Xinyu", ""], ["Zhang", "Xuhui", ""]]}, {"id": "1902.04679", "submitter": "Ana Maria Pires", "authors": "A. M. Pires and J. A. Branco", "title": "High dimensionality: The latest challenge to data analysis", "comments": "Keywords: Curse of dimensionality; High dimensional data; Mahalanobis\n  distance", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of modern technology, permitting the measurement of thousands of\ncharacteristics simultaneously, has given rise to floods of data characterized\nby many large or even huge datasets. This new paradigm presents extraordinary\nchallenges to data analysis and the question arises: how can conventional data\nanalysis methods, devised for moderate or small datasets, cope with the\ncomplexities of modern data? The case of high dimensional data is particularly\nrevealing of some of the drawbacks. We look at the case where the number of\ncharacteristics measured in an object is at least the number of observed\nobjects and conclude that this configuration leads to geometrical and\nmathematical oddities and is an insurmountable barrier for the direct\napplication of traditional methodologies. If scientists are going to ignore\nfundamental mathematical results arrived at in this paper and blindly use\nsoftware to analyze data, the results of their analyses may not be trustful,\nand the findings of their experiments may never be validated. That is why new\nmethods together with the wise use of traditional approaches are essential to\nprogress safely through the present reality.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 23:56:26 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Pires", "A. M.", ""], ["Branco", "J. A.", ""]]}, {"id": "1902.04701", "submitter": "Pallavi Ray", "authors": "Pallavi Ray, Debdeep Pati, Anirban Bhattacharya", "title": "Efficient Bayesian shape-restricted function estimation with constrained\n  Gaussian process priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article revisits the problem of Bayesian shape-restricted inference in\nthe light of a recently developed approximate Gaussian process that admits an\nequivalent formulation of the shape constraints in terms of the basis\ncoefficients. We propose a strategy to efficiently sample from the resulting\nconstrained posterior by absorbing a smooth relaxation of the constraint in the\nlikelihood and using circulant embedding techniques to sample from the\nunconstrained modified prior. We additionally pay careful attention to mitigate\nthe computational complexity arising from updating hyperparameters within the\ncovariance kernel of the Gaussian process. The developed algorithm is shown to\nbe accurate and highly efficient in simulated and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 01:52:06 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Ray", "Pallavi", ""], ["Pati", "Debdeep", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "1902.04765", "submitter": "Rhythm Grover", "authors": "Rhythm Grover, Debasis Kundu and Amit Mitra", "title": "An efficient methodology to estimate the parameters of a two-dimensional\n  chirp signal model", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various capacities of statistical signal processing two-dimensional (2-D)\nchirp models have been considered significantly, particularly in image\nprocessing$-$ to model gray-scale and texture images, magnetic resonance\nimaging, optical imaging etc. In this paper we address the problem of\nestimation of the unknown parameters of a 2-D chirp model under the assumption\nthat the errors are independently and identically distributed (i.i.d.). The key\nattribute of the proposed estimation procedure is that it is computationally\nmore efficient than the least squares estimation method. Moreover, the proposed\nestimators are observed to have the same asymptotic properties as the least\nsquares estimators, thus providing computational effectiveness without any\ncompromise on the efficiency of the estimators. We extend the propounded\nestimation method to provide a sequential procedure to estimate the unknown\nparameters of a 2-D chirp model with multiple components and under the\nassumption of i.i.d. errors we study the large sample properties of these\nsequential estimators. Simulation studies and a synthetic data analysis show\nthat the proposed estimators perform satisfactorily.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 06:36:24 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Grover", "Rhythm", ""], ["Kundu", "Debasis", ""], ["Mitra", "Amit", ""]]}, {"id": "1902.04926", "submitter": "Tom\\'a\\v{s} Mrkvi\\v{c}ka", "authors": "Tomas Mrkvicka, Tomas Roskovec, Michael Rost", "title": "A nonparametric graphical tests of significance in functional GLM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new nonparametric graphical test of significance of a covariate in\nfunctional GLM is proposed. Our approach is especially interesting due to its\nfunctional graphical interpretation of the results. As such it is able to find\nnot only if the factor of interest is significant but also which functional\ndomain is responsible for the potential rejection. In the case of functional\nmulti-way main effect ANOVA or functional main effect ANCOVA models it is able\nto find which groups differ (and where they differ), in the case of functional\nfactorial ANOVA or functional factorial ANCOVA models it is able to find which\ncombination of levels (which interactions) differ (and where they differ). The\ndescribed tests are extensions of global envelope tests in the GLM models. It\napplies Freedman-Lane algorithm for the permutation of functions and as such it\napproximately achieve the desired significance level.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 14:36:53 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Mrkvicka", "Tomas", ""], ["Roskovec", "Tomas", ""], ["Rost", "Michael", ""]]}, {"id": "1902.04996", "submitter": "Manuela Zucknick", "authors": "Zhi Zhao and Manuela Zucknick", "title": "Structured penalized regression for drug sensitivity prediction", "comments": "Zhao Z, Zucknick M (2020). Structured penalized regression for drug\n  sensitivity prediction. Journal of the Royal Statistical Society, Series C.\n  19 pages, 6 figures and 2 tables", "journal-ref": null, "doi": "10.1111/rssc.12400", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale {\\it in vitro} drug sensitivity screens are an important tool in\npersonalized oncology to predict the effectiveness of potential cancer drugs.\nThe prediction of the sensitivity of cancer cell lines to a panel of drugs is a\nmultivariate regression problem with high-dimensional heterogeneous multi-omics\ndata as input data and with potentially strong correlations between the outcome\nvariables which represent the sensitivity to the different drugs. We propose a\njoint penalized regression approach with structured penalty terms which allow\nus to utilize the correlation structure between drugs with group-lasso-type\npenalties and at the same time address the heterogeneity between omics data\nsources by introducing data-source-specific penalty factors to penalize\ndifferent data sources differently. By combining integrative penalty factors\n(IPF) with tree-guided group lasso, we create the IPF-tree-lasso method. We\npresent a unified framework to transform more general IPF-type methods to the\noriginal penalized method. Because the structured penalty terms have multiple\nparameters, we demonstrate how the interval-search Efficient Parameter\nSelection via Global Optimization (EPSGO) algorithm can be used to optimize\nmultiple penalty parameters efficiently. Simulation studies show that\nIPF-tree-lasso can improve the prediction performance compared to other\nlasso-type methods, in particular for heterogenous data sources. Finally, we\nemploy the new methods to analyse data from the Genomics of Drug Sensitivity in\nCancer project.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 16:41:41 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 06:54:16 GMT"}, {"version": "v3", "created": "Sun, 8 Mar 2020 22:14:29 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Zhao", "Zhi", ""], ["Zucknick", "Manuela", ""]]}, {"id": "1902.05106", "submitter": "Maryclare Griffin", "authors": "Maryclare Griffin and Peter D. Hoff", "title": "Structured Shrinkage Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many regression settings the unknown coefficients may have some known\nstructure, for instance they may be ordered in space or correspond to a\nvectorized matrix or tensor. At the same time, the unknown coefficients may be\nsparse, with many nearly or exactly equal to zero. However, many commonly used\npriors and corresponding penalties for coefficients do not encourage\nsimultaneously structured and sparse estimates. In this paper we develop\nstructured shrinkage priors that generalize multivariate normal, Laplace,\nexponential power and normal-gamma priors. These priors allow the regression\ncoefficients to be correlated a priori without sacrificing elementwise sparsity\nor shrinkage. The primary challenges in working with these structured shrinkage\npriors are computational, as the corresponding penalties are intractable\nintegrals and the full conditional distributions that are needed to approximate\nthe posterior mode or simulate from the posterior distribution may be\nnon-standard. We overcome these issues using a flexible elliptical slice\nsampling procedure, and demonstrate that these priors can be used to introduce\nstructure while preserving sparsity of the corresponding penalized estimate\ngiven by the posterior mode.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 20:09:00 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 15:55:17 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Griffin", "Maryclare", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1902.05156", "submitter": "Bernard Silverman", "authors": "Lax Chan, Bernard W. Silverman and Kyle Vincent", "title": "Multiple Systems Estimation for Sparse Capture Data: Inferential\n  Challenges when there are Non-Overlapping Lists", "comments": "41 pages, 1 figure", "journal-ref": "Journal of the American Statistical Association, 2020", "doi": "10.1080/01621459.2019.1708748", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple systems estimation strategies have recently been applied to quantify\nhard-to-reach populations, particularly when estimating the number of victims\nof human trafficking and modern slavery. In such contexts, it is not uncommon\nto see sparse or even no overlap between some of the lists on which the\nestimates are based. These create difficulties in model fitting and selection,\nand we develop inference procedures to address these challenges. The approach\nis based on Poisson log-linear regression modeling. Issues investigated in\ndetail include taking proper account of data sparsity in the estimation\nprocedure, as well as the existence and identifiability of maximum likelihood\nestimates. A stepwise method for choosing the most suitable parameters is\ndeveloped, together with a bootstrap approach to finding confidence intervals\nfor the total population size. We apply the strategy to two empirical data sets\nof trafficking in US regions, and find that the approach results in stable,\nreasonable estimates. An accompanying R software implementation has been made\npublicly available.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 23:11:08 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 12:10:10 GMT"}, {"version": "v3", "created": "Sun, 4 Aug 2019 17:12:43 GMT"}, {"version": "v4", "created": "Sat, 14 Dec 2019 08:03:19 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Chan", "Lax", ""], ["Silverman", "Bernard W.", ""], ["Vincent", "Kyle", ""]]}, {"id": "1902.05336", "submitter": "Babak Choodari-Oskooei", "authors": "Babak Choodari-Oskooei, Daniel J Bratton, Melissa R Gannon, Angela M\n  Meade, Matthew R Sydes, Mahesh KB Parmar", "title": "Adding new experimental arms to randomised clinical trials: impact on\n  error rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Experimental treatments pass through various stages of\ndevelopment. If a treatment passes through early phase experiments, the\ninvestigators may want to assess it in a late phase randomised controlled\ntrial. An efficient way to do this is adding it as a new research arm to an\nongoing trial. This allows to add the new treatment while the existing arms\ncontinue. The familywise type I error rate (FWER) is often a key quantity of\ninterest in any multi-arm trial. We set out to clarify how it should be\ncalculated when new arms are added to a trial some time after it has started.\n  Methods: We show how the FWER, any-pair and all-pairs powers can be\ncalculated when a new arm is added to a platform trial. We extend the Dunnett\nprobability and derive analytical formulae for the correlation between the test\nstatistics of the existing pairwise comparison and that of the newly added arm.\nWe also verify our analytical derivation via simulations.\n  Results: Our results indicate that the FWER depends on the shared control arm\ninformation (i.e. individuals in continuous and binary outcomes and primary\noutcome events in time-to-event outcomes) from the common control arm patients\nand the allocation ratio. The FWER is driven more by the number of pairwise\ncomparisons and the corresponding (pairwise) Type I error rates than by the\ntiming of the addition of the new arms. The FWER can be estimated using\n\\v{S}id\\'{a}k's correction if the correlation between the test statistics of\npairwise comparisons is less than 0:30.\n  Conclusions: The findings we present in this article can be used to design\ntrials with pre-planned deferred arms or to design new pairwise comparisons\nwithin an ongoing platform trial where control of the pairwise error rate\n(PWER) or FWER (for a subset of pairwise comparisons) is required.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 13:03:24 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Choodari-Oskooei", "Babak", ""], ["Bratton", "Daniel J", ""], ["Gannon", "Melissa R", ""], ["Meade", "Angela M", ""], ["Sydes", "Matthew R", ""], ["Parmar", "Mahesh KB", ""]]}, {"id": "1902.05428", "submitter": "Hugo Lewi Hammer Dr.", "authors": "Hugo Lewi Hammer, Anis Yazidi and H{\\aa}vard Rue", "title": "Joint Tracking of Multiple Quantiles Through Conditional Quantiles", "comments": "arXiv admin note: text overlap with arXiv:1901.04681", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of quantiles is one of the most fundamental real-time analysis\ntasks. Most real-time data streams vary dynamically with time and incremental\nquantile estimators document state-of-the art performance to track quantiles of\nsuch data streams. However, most are not able to make joint estimates of\nmultiple quantiles in a consistent manner, and estimates may violate the\nmonotone property of quantiles. In this paper we propose the general concept of\n*conditional quantiles* that can extend incremental estimators to jointly track\nmultiple quantiles. We apply the concept to propose two new estimators.\nExtensive experimental results, on both synthetic and real-life data, show that\nthe new estimators clearly outperform legacy state-of-the-art joint quantile\ntracking algorithm and achieve faster adaptivity in dynamically varying data\nstreams.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 07:47:22 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Hammer", "Hugo Lewi", ""], ["Yazidi", "Anis", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1902.05499", "submitter": "Crystal Nguyen", "authors": "Crystal T. Nguyen (1), Daniel J. Luckett (1), Anna R. Kahkoska (2),\n  Grace E. Shearrer (2), Donna Spruijt-Metz (3), Jaimie N. Davis (4), and\n  Michael R. Kosorok (1) ((1) Department of Biostatistics, University of North\n  Carolina, Chapel Hill, North Carolina, U.S.A., (2) Department of Nutrition,\n  University of North Carolina, Chapel Hill, U.S.A., (3) Center of Economic and\n  Social Research, University of Southern California, Los Angeles, California,\n  U.S.A., (4) Department of Nutrition, University of Texas, Austin, Texas,\n  U.S.A.)", "title": "Estimating Individualized Treatment Regimes from Crossover Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of precision medicine aims to tailor treatment based on\npatient-specific factors in a reproducible way. To this end, estimating an\noptimal individualized treatment regime (ITR) that recommends treatment\ndecisions based on patient characteristics to maximize the mean of a\npre-specified outcome is of particular interest. Several methods have been\nproposed for estimating an optimal ITR from clinical trial data in the parallel\ngroup setting where each subject is randomized to a single intervention.\nHowever, little work has been done in the area of estimating the optimal ITR\nfrom crossover study designs. Such designs naturally lend themselves to\nprecision medicine, because they allow for observing the response to multiple\ntreatments for each patient. In this paper, we introduce a method for\nestimating the optimal ITR using data from a 2x2 crossover study with or\nwithout carryover effects. The proposed method is similar to policy search\nmethods such as outcome weighted learning; however, we take advantage of the\ncrossover design by using the difference in responses under each treatment as\nthe observed reward. We establish Fisher and global consistency, present\nnumerical experiments, and analyze data from a feeding trial to demonstrate the\nimproved performance of the proposed method compared to standard methods for a\nparallel study design.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 02:28:33 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Nguyen", "Crystal T.", ""], ["Luckett", "Daniel J.", ""], ["Kahkoska", "Anna R.", ""], ["Shearrer", "Grace E.", ""], ["Spruijt-Metz", "Donna", ""], ["Davis", "Jaimie N.", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1902.05539", "submitter": "Pierre-Alexandre Mattei", "authors": "Pierre-Alexandre Mattei", "title": "A Parsimonious Tour of Bayesian Model Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern statistical software and machine learning libraries are enabling\nsemi-automated statistical inference. Within this context, it appears easier\nand easier to try and fit many models to the data at hand, reversing thereby\nthe Fisherian way of conducting science by collecting data after the scientific\nhypothesis (and hence the model) has been determined. The renewed goal of the\nstatistician becomes to help the practitioner choose within such large and\nheterogeneous families of models, a task known as model selection. The Bayesian\nparadigm offers a systematized way of assessing this problem. This approach,\nlaunched by Harold Jeffreys in his 1935 book Theory of Probability, has\nwitnessed a remarkable evolution in the last decades, that has brought about\nseveral new theoretical and methodological advances. Some of these recent\ndevelopments are the focus of this survey, which tries to present a unifying\nperspective on work carried out by different communities. In particular, we\nfocus on non-asymptotic out-of-sample performance of Bayesian model selection\nand averaging techniques, and draw connections with penalized maximum\nlikelihood. We also describe recent extensions to wider classes of\nprobabilistic frameworks including high-dimensional, unidentifiable, or\nlikelihood-free models.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 18:43:46 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 17:39:07 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Mattei", "Pierre-Alexandre", ""]]}, {"id": "1902.05680", "submitter": "Yanxun Xu", "authors": "Yuliang Li, Dipankar Bandyopadhyay, Fangzheng Xie, Yanxun Xu", "title": "BAREB: A Bayesian repulsive biclustering model for periodontal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preventing periodontal diseases (PD) and maintaining the structure and\nfunction of teeth are important goals for personal oral care. To understand the\nheterogeneity in patients with diverse PD patterns, we develop BAREB, a\nBayesian repulsive biclustering method that can simultaneously cluster the PD\npatients and their tooth sites after taking the patient- and site- level\ncovariates into consideration. BAREB uses the determinantal point process (DPP)\nprior to induce diversity among different biclusters to facilitate parsimony\nand interpretability. Since PD progression is hypothesized to be\nspatially-referenced, BAREB factors in the spatial dependence among tooth\nsites. In addition, since PD is the leading cause for tooth loss, the missing\ndata mechanism is non-ignorable. Such nonrandom missingness is incorporated\ninto BAREB. For the posterior inference, we design an efficient reversible jump\nMarkov chain Monte Carlo sampler. Simulation studies show that BAREB is able to\naccurately estimate the biclusters, and compares favorably to alternatives. For\nreal world application, we apply BAREB to a dataset from a clinical PD study,\nand obtain desirable and interpretable results. A major contribution of this\npaper is the Rcpp implementation of BAREB, available at\nhttps://github.com/YanxunXu/ BAREB.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 04:10:23 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 09:47:58 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Li", "Yuliang", ""], ["Bandyopadhyay", "Dipankar", ""], ["Xie", "Fangzheng", ""], ["Xu", "Yanxun", ""]]}, {"id": "1902.05754", "submitter": "Maxime Vono", "authors": "Maxime Vono and Nicolas Dobigeon and Pierre Chainais", "title": "Asymptotically exact data augmentation: models, properties and\n  algorithms", "comments": "63 pages", "journal-ref": null, "doi": "10.1080/10618600.2020.1826954", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation, by the introduction of auxiliary variables, has become an\nubiquitous technique to improve convergence properties, simplify the\nimplementation or reduce the computational time of inference methods such as\nMarkov chain Monte Carlo ones. Nonetheless, introducing appropriate auxiliary\nvariables while preserving the initial target probability distribution and\noffering a computationally efficient inference cannot be conducted in a\nsystematic way. To deal with such issues, this paper studies a unified\nframework, coined asymptotically exact data augmentation (AXDA), which\nencompasses both well-established and more recent approximate augmented models.\nIn a broader perspective, this paper shows that AXDA models can benefit from\ninteresting statistical properties and yield efficient inference algorithms. In\nnon-asymptotic settings, the quality of the proposed approximation is assessed\nwith several theoretical results. The latter are illustrated on standard\nstatistical problems. Supplementary materials including computer code for this\npaper are available online.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 10:29:53 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 21:11:48 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 19:27:31 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Vono", "Maxime", ""], ["Dobigeon", "Nicolas", ""], ["Chainais", "Pierre", ""]]}, {"id": "1902.05813", "submitter": "Qianqian Zhu", "authors": "Qianqian Zhu and Guodong Li", "title": "Quantile double autoregression", "comments": "This article has 46 pages, 6 tables and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many financial time series have varying structures at different quantile\nlevels, and also exhibit the phenomenon of conditional heteroscedasticity at\nthe same time. In the meanwhile, it is still lack of a time series model to\naccommodate both of the above features simultaneously. This paper fills the gap\nby proposing a novel conditional heteroscedastic model, which is called the\nquantile double autoregression. The strict stationarity of the new model is\nderived, and a self-weighted conditional quantile estimation is suggested. Two\npromising properties of the original double autoregressive model are shown to\nbe preserved. Based on the quantile autocorrelation function and self-weighting\nconcept, two portmanteau tests are constructed, and they can be used in\nconjunction to check the adequacy of fitted conditional quantiles. The\nfinite-sample performance of the proposed inference tools is examined by\nsimulation studies, and the necessity of the new model is further demonstrated\nby analyzing the S&P500 Index.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 14:07:46 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 14:29:28 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhu", "Qianqian", ""], ["Li", "Guodong", ""]]}, {"id": "1902.06020", "submitter": "Luis Nieto-Barajas Dr.", "authors": "Luis Nieto-Barajas and Gabriel Nu\\~nez-Antonio", "title": "Projected P\\'olya Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way of defining probability distributions for circular variables\n(directions in two dimensions) is to radially project probability\ndistributions, originally defined on $\\mathbb{R}^2$, to the unit circle.\nProjected distributions have proved to be useful in the study of circular and\ndirectional data. Although any bivariate distribution can be used to produce a\nprojected circular model, these distributions are typically parametric. In this\narticle we consider a bivariate P\\'olya tree on $\\mathbb{R}^2$ and project it\nto the unit circle to define a new Bayesian nonparametric model for circular\ndata. We study the properties of the proposed model, obtain its posterior\ncharacterisation and show its performance with simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 01:16:37 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 22:37:05 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 00:49:19 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Nieto-Barajas", "Luis", ""], ["Nu\u00f1ez-Antonio", "Gabriel", ""]]}, {"id": "1902.06036", "submitter": "Lin Dong", "authors": "Lin Dong and Sujit K. Ghosh", "title": "Assessing Biosimilarity using Functional Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there have been a lot of interest to test for similarity\nbetween biological drug products, commonly known as biologics. Biologics are\nlarge and complex molecule drugs that are produced by living cells and hence\nthese are sensitive to the environmental changes. In addition, biologics\nusually induce antibodies which raises the safety and efficacy issues. The\nmanufacturing process is also much more complicated and often costlier than the\nsmall-molecule generic drugs. Because of these complexities and inherent\nvariability of the biologics, the testing paradigm of the traditional generic\ndrugs cannot be directly used to test for biosimilarity. Taking into account\nsome of these concerns we propose a functional distance based methodology that\ntakes into consideration the entire time course of the study and is based on a\nclass of flexible semi-parametric models. The empirical results show that the\nproposed approach is more sensitive than the classical equivalence tests\napproach which are usually based on arbitrarily chosen time point. Bootstrap\nbased methodologies are also presented for statistical inference.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 04:08:29 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 00:04:42 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Dong", "Lin", ""], ["Ghosh", "Sujit K.", ""]]}, {"id": "1902.06043", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle, Jerome P. Reiter", "title": "Sequentially additive nonignorable missing data modeling using auxiliary\n  marginal information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of missingness mechanisms, called sequentially additive\nnonignorable, for modeling multivariate data with item nonresponse. These\nmechanisms explicitly allow the probability of nonresponse for each variable to\ndepend on the value of that variable, thereby representing nonignorable\nmissingness mechanisms. These missing data models are identified by making use\nof auxiliary information on marginal distributions, such as marginal\nprobabilities for multivariate categorical variables or moments for numeric\nvariables. We present theory proving identification results, and illustrate the\nuse of these mechanisms in an application.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 05:11:12 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Sadinle", "Mauricio", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1902.06078", "submitter": "Bernard Silverman", "authors": "Bernard W. Silverman", "title": "Model fitting in Multiple Systems Analysis for the quantification of\n  Modern Slavery: Classical and Bayesian approaches", "comments": "31 pages. Version 3. (Original version July 2018)", "journal-ref": "Journal of the Royal Statistical Society, Series A. (2020) with\n  Discussion", "doi": "10.1111/rssa.12505", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple systems estimation is a key approach for quantifying hidden\npopulations such as the number of victims of modern slavery. The UK Government\npublished an estimate of 10,000 to 13,000 victims, constructed by the present\nauthor, as part of the strategy leading to the Modern Slavery Act 2015. This\nestimate was obtained by a stepwise multiple systems method based on six lists.\nFurther investigation shows that a small proportion of the possible models give\nrather different answers, and that other model fitting approaches may choose\none of these. Three data sets collected in the field of modern slavery,\ntogether with a data set about the death toll in the Kosovo conflict, are used\nto investigate the stability and robustness of various multiple systems\nestimate methods. The crucial aspect is the way that interactions between lists\nare modelled, because these can substantially affect the results. Model\nselection and Bayesian approaches are considered in detail, in particular to\nassess their stability and robustness when applied to real modern slavery data.\nA new Markov Chain Monte Carlo Bayesian approach is developed; overall, this\ngives robust and stable results at least for the examples considered. The\nsoftware and datasets are freely and publicly available to facilitate wider\nimplementation and further research.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 10:01:20 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 10:15:30 GMT"}, {"version": "v3", "created": "Sun, 11 Aug 2019 21:36:43 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Silverman", "Bernard W.", ""]]}, {"id": "1902.06080", "submitter": "Issa Dahabreh", "authors": "Issa J. Dahabreh and Miguel A. Hernan and Sarah E. Robertson and\n  Ashley Buchanan and Jon A. Steingrimsson", "title": "Generalizing trial findings using nested trial designs with sub-sampling\n  of non-randomized individuals", "comments": "added acknowledgements, fixed some wording imperfections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To generalize inferences from a randomized trial to the target population of\nall trial-eligible individuals, investigators can use nested trial designs,\nwhere the randomized individuals are nested within a cohort of trial-eligible\nindividuals, including those who are not offered or refuse randomization. In\nthese designs, data on baseline covariates are collected from the entire\ncohort, and treatment and outcome data need only be collected from randomized\nindividuals. In this paper, we describe nested trial designs that improve\nresearch economy by collecting additional baseline covariate data after\nsub-sampling non-randomized individuals (i.e., a two-stage design), using\nsampling probabilities that may depend on the initial set of baseline\ncovariates available from all individuals in the cohort. We propose an\nestimator for the potential outcome mean in the target population of all\ntrial-eligible individuals and show that our estimator is doubly robust, in the\nsense that it is consistent when either the model for the conditional outcome\nmean among randomized individuals or the model for the probability of trial\nparticipation is correctly specified. We assess the impact of sub-sampling on\nthe asymptotic variance of our estimator and examine the estimator's\nfinite-sample performance in a simulation study. We illustrate the methods\nusing data from the Coronary Artery Surgery Study (CASS).\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 10:27:45 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 16:59:59 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Dahabreh", "Issa J.", ""], ["Hernan", "Miguel A.", ""], ["Robertson", "Sarah E.", ""], ["Buchanan", "Ashley", ""], ["Steingrimsson", "Jon A.", ""]]}, {"id": "1902.06115", "submitter": "Molei Liu", "authors": "Tianxi Cai, Molei Liu and Yin Xia", "title": "Individual Data Protected Integrative Regression Analysis of\n  High-dimensional Heterogeneous Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence-based decision making often relies on meta-analyzing multiple\nstudies, which enables more precise estimation and investigation of\ngeneralizability. Integrative analysis of multiple heterogeneous studies is,\nhowever, highly challenging in the ultra high dimensional setting. The\nchallenge is even more pronounced when the individual level data cannot be\nshared across studies, known as DataSHIELD constraint (Wolfson et al., 2010).\nUnder sparse regression models that are assumed to be similar yet not identical\nacross studies, we propose in this paper a novel integrative estimation\nprocedure for data-Shielding High-dimensional Integrative Regression (SHIR).\nSHIR protects individual data through summary-statistics-based integrating\nprocedure, accommodates between study heterogeneity in both the covariate\ndistribution and model parameters, and attains consistent variable selection.\nTheoretically, SHIR is statistically more efficient than the existing\ndistributed approaches that integrate debiased LASSO estimators from the local\nsites. Furthermore, the estimation error incurred by aggregating derived data\nis negligible compared to the statistical minimax rate and SHIR is shown to be\nasymptotically equivalent in estimation to the ideal estimator obtained by\nsharing all data. The finite-sample performance of our method is studied and\ncompared with existing approaches via extensive simulation settings. We further\nillustrate the utility of SHIR to derive phenotyping algorithms for coronary\nartery disease using electronic health records data from multiple chronic\ndisease cohorts.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 16:01:02 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 15:20:04 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 16:56:17 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Cai", "Tianxi", ""], ["Liu", "Molei", ""], ["Xia", "Yin", ""]]}, {"id": "1902.06122", "submitter": "Daniel Baker", "authors": "Daniel H. Baker, Greta Vilidaite, Freya A. Lygo, Anika K. Smith, Tessa\n  R. Flack, Andre D. Gouws and Timothy J. Andrews", "title": "Power contours: optimising sample size and precision in experimental\n  psychology and human neuroscience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When designing experimental studies with human participants, experimenters\nmust decide how many trials each participant will complete, as well as how many\nparticipants to test. Most discussion of statistical power (the ability of a\nstudy design to detect an effect) has focussed on sample size, and assumed\nsufficient trials. Here we explore the influence of both factors on statistical\npower, represented as a two-dimensional plot on which iso-power contours can be\nvisualised. We demonstrate the conditions under which the number of trials is\nparticularly important, i.e. when the within-participant variance is large\nrelative to the between-participants variance. We then derive power contour\nplots using existing data sets for eight experimental paradigms and\nmethodologies (including reaction times, sensory thresholds, fMRI, MEG, and\nEEG), and provide example code to calculate estimates of the within- and\nbetween-participant variance for each method. In all cases, the\nwithin-participant variance was larger than the between-participants variance,\nmeaning that the number of trials has a meaningful influence on statistical\npower in commonly used paradigms. An online tool is provided\n(https://shiny.york.ac.uk/powercontours/) for generating power contours, from\nwhich the optimal combination of trials and participants can be calculated when\ndesigning future studies.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 16:30:59 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 06:43:51 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 15:24:19 GMT"}, {"version": "v4", "created": "Sat, 2 Nov 2019 10:02:27 GMT"}, {"version": "v5", "created": "Tue, 4 Feb 2020 05:57:53 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Baker", "Daniel H.", ""], ["Vilidaite", "Greta", ""], ["Lygo", "Freya A.", ""], ["Smith", "Anika K.", ""], ["Flack", "Tessa R.", ""], ["Gouws", "Andre D.", ""], ["Andrews", "Timothy J.", ""]]}, {"id": "1902.06183", "submitter": "Arnab Chakraborty", "authors": "Arnab Chakraborty and Soumendra Nath Lahiri and Alyson Wilson", "title": "A Statistical Analysis of Noisy Crowdsourced Weather Data", "comments": "Submitted to Annals of Applied Statistics", "journal-ref": "A statistical analysis of noisy crowdsourced weather data, Annals\n  of Applied Statistics 2020, Vol. 14, No. 1, 116-142", "doi": "10.1214/19-AOAS1290", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial prediction of weather-elements like temperature, precipitation, and\nbarometric pressure are generally based on satellite imagery or data collected\nat ground-stations. None of these data provide information at a more granular\nor \"hyper-local\" resolution. On the other hand, crowdsourced weather data,\nwhich are captured by sensors installed on mobile devices and gathered by\nweather-related mobile apps like WeatherSignal and AccuWeather, can serve as\npotential data sources for analyzing environmental processes at a hyper-local\nresolution. However, due to the low quality of the sensors and the\nnon-laboratory environment, the quality of the observations in crowdsourced\ndata is compromised. This paper describes methods to improve hyper-local\nspatial prediction using this varying-quality noisy crowdsourced information.\nWe introduce a reliability metric, namely Veracity Score (VS), to assess the\nquality of the crowdsourced observations using a coarser, but high-quality,\nreference data. A VS-based methodology to analyze noisy spatial data is\nproposed and evaluated through extensive simulations. The merits of the\nproposed approach are illustrated through case studies analyzing crowdsourced\ndaily average ambient temperature readings for one day in the contiguous United\nStates.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 01:31:58 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 15:37:44 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Chakraborty", "Arnab", ""], ["Lahiri", "Soumendra Nath", ""], ["Wilson", "Alyson", ""]]}, {"id": "1902.06194", "submitter": "Chanmin Kim", "authors": "Chanmin Kim, Michael Daniels, Joseph Hogan, Christine Choirat, Corwin\n  Zigler", "title": "Bayesian Methods for Multiple Mediators: Relating Principal\n  Stratification and Causal Mediation in the Analysis of Power Plant Emission\n  Controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emission control technologies installed on power plants are a key feature of\nmany air pollution regulations in the US. While such regulations are predicated\non the presumed relationships between emissions, ambient air pollution, and\nhuman health, many of these relationships have never been empirically verified.\nThe goal of this paper is to develop new statistical methods to quantify these\nrelationships. We frame this problem as one of mediation analysis to evaluate\nthe extent to which the effect of a particular control technology on ambient\npollution is mediated through causal effects on power plant emissions. Since\npower plants emit various compounds that contribute to ambient pollution, we\ndevelop new methods for multiple intermediate variables that are measured\ncontemporaneously, may interact with one another, and may exhibit joint\nmediating effects. Specifically, we propose new methods leveraging two related\nframeworks for causal inference in the presence of mediating variables:\nprincipal stratification and causal mediation analysis. We define principal\neffects based on multiple mediators, and also introduce a new decomposition of\nthe total effect of an intervention on ambient pollution into the natural\ndirect effect and natural indirect effects for all combinations of mediators.\nBoth approaches are anchored to the same observed-data models, which we specify\nwith Bayesian nonparametric techniques. We provide assumptions for estimating\nprincipal causal effects, then augment these with an additional assumption\nrequired for causal mediation analysis. The two analyses, interpreted in\ntandem, provide the first empirical investigation of the presumed causal\npathways that motivate important air quality regulatory policies.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 03:33:55 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kim", "Chanmin", ""], ["Daniels", "Michael", ""], ["Hogan", "Joseph", ""], ["Choirat", "Christine", ""], ["Zigler", "Corwin", ""]]}, {"id": "1902.06241", "submitter": "Yipeng Song", "authors": "Yipeng Song, Johan A. Westerhuis, Age K. Smilde", "title": "Separating common (global and local) and distinct variation in multiple\n  mixed types data sets", "comments": "32 pages, 14 figures", "journal-ref": "Journal of Chemometrics 34 (2020) e3197", "doi": "10.1002/cem.3197", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple sets of measurements on the same objects obtained from different\nplatforms may reflect partially complementary information of the studied\nsystem. The integrative analysis of such data sets not only provides us with\nthe opportunity of a deeper understanding of the studied system, but also\nintroduces some new statistical challenges. First, the separation of\ninformation that is common across all or some of the data sets, and the\ninformation that is specific to each data set is problematic. Furthermore,\nthese data sets are often a mix of quantitative and discrete (binary or\ncategorical) data types, while commonly used data fusion methods require all\ndata sets to be quantitative. In this paper, we propose an exponential family\nsimultaneous component analysis (ESCA) model to tackle the potential mixed data\ntypes problem of multiple data sets. In addition, a structured sparse pattern\nof the loading matrix is induced through a nearly unbiased group concave\npenalty to disentangle the global, local common and distinct information of the\nmultiple data sets. A Majorization-Minimization based algorithm is derived to\nfit the proposed model. Analytic solutions are derived for updating all the\nparameters of the model in each iteration, and the algorithm will decrease the\nobjective function in each iteration monotonically. For model selection, a\nmissing value based cross validation procedure is implemented. The advantages\nof the proposed method in comparison with other approaches are assessed using\ncomprehensive simulations as well as the analysis of real data from a chronic\nlymphocytic leukaemia (CLL) study.\n  Availability: the codes to reproduce the results in this article are\navailable at https://gitlab.com/uvabda.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 10:49:34 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 05:01:10 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Song", "Yipeng", ""], ["Westerhuis", "Johan A.", ""], ["Smilde", "Age K.", ""]]}, {"id": "1902.06269", "submitter": "Vadim Sokolov", "authors": "Nicholas G. Polson and Vadim Sokolov", "title": "Bayesian Regularization: From Tikhonov to Horseshoe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian regularization is a central tool in modern-day statistical and\nmachine learning methods. Many applications involve high-dimensional sparse\nsignal recovery problems. The goal of our paper is to provide a review of the\nliterature on penalty-based regularization approaches, from Tikhonov (Ridge,\nLasso) to horseshoe regularization.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 14:58:46 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Sokolov", "Vadim", ""]]}, {"id": "1902.06281", "submitter": "Paul-Christian B\\\"urkner", "authors": "Paul-Christian B\\\"urkner, Jonah Gabry, Aki Vehtari", "title": "Approximate leave-future-out cross-validation for Bayesian time series\n  models", "comments": "28 pages, 15 figures, 2 tables", "journal-ref": "Journal of Statistical Computation and Simulation (2020)", "doi": "10.1080/00949655.2020.1783262", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the common goals of time series analysis is to use the observed series\nto inform predictions for future observations. In the absence of any actual new\ndata to predict, cross-validation can be used to estimate a model's future\npredictive accuracy, for instance, for the purpose of model comparison or\nselection. Exact cross-validation for Bayesian models is often computationally\nexpensive, but approximate cross-validation methods have been developed, most\nnotably methods for leave-one-out cross-validation (LOO-CV). If the actual\nprediction task is to predict the future given the past, LOO-CV provides an\noverly optimistic estimate because the information from future observations is\navailable to influence predictions of the past. To properly account for the\ntime series structure, we can use leave-future-out cross-validation (LFO-CV).\nLike exact LOO-CV, exact LFO-CV requires refitting the model many times to\ndifferent subsets of the data. Using Pareto smoothed importance sampling, we\npropose a method for approximating exact LFO-CV that drastically reduces the\ncomputational costs while also providing informative diagnostics about the\nquality of the approximation.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 15:57:30 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 22:25:01 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 07:40:28 GMT"}, {"version": "v4", "created": "Wed, 2 Oct 2019 09:21:52 GMT"}, {"version": "v5", "created": "Fri, 8 May 2020 13:19:31 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["B\u00fcrkner", "Paul-Christian", ""], ["Gabry", "Jonah", ""], ["Vehtari", "Aki", ""]]}, {"id": "1902.06303", "submitter": "Gerhard Tutz", "authors": "Gerhard Tutz", "title": "What is an Ordinal Latent Trait Model?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although various polytomous item response models are considered to be ordinal\nmodels there seems no general definition of an ordinal model available.\nAlternative concepts of ordinal models are discussed and it is shown that they\ncoincide for classical unidimensional models. For multidimensional models the\ndefinition of an ordinal model refers to specific traits in the\nmultidimensional space of traits. The objective is to provide a theoretical\nframework for ordinal models. Practical considerations concerning the strength\nof the link between the latent trait and the order of categories are considered\nbriefly.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 18:16:33 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Tutz", "Gerhard", ""]]}, {"id": "1902.06547", "submitter": "Jean Pauphilet", "authors": "Dimitris Bertsimas, Jean Pauphilet, Bart Van Parys", "title": "Sparse Regression: Scalable algorithms and empirical performance", "comments": null, "journal-ref": "Statistical Science 35-4 (2020) 555-578", "doi": "10.1214/19-STS701", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we review state-of-the-art methods for feature selection in\nstatistics with an application-oriented eye. Indeed, sparsity is a valuable\nproperty and the profusion of research on the topic might have provided little\nguidance to practitioners. We demonstrate empirically how noise and correlation\nimpact both the accuracy - the number of correct features selected - and the\nfalse detection - the number of incorrect features selected - for five methods:\nthe cardinality-constrained formulation, its Boolean relaxation, $\\ell_1$\nregularization and two methods with non-convex penalties. A cogent feature\nselection method is expected to exhibit a two-fold convergence, namely the\naccuracy and false detection rate should converge to $1$ and $0$ respectively,\nas the sample size increases. As a result, proper method should recover all and\nnothing but true features. Empirically, the integer optimization formulation\nand its Boolean relaxation are the closest to exhibit this two properties\nconsistently in various regimes of noise and correlation. In addition, apart\nfrom the discrete optimization approach which requires a substantial, yet often\naffordable, computational time, all methods terminate in times comparable with\nthe \\verb|glmnet| package for Lasso. We released code for methods that were not\npublicly implemented. Jointly considered, accuracy, false detection and\ncomputational time provide a comprehensive assessment of each feature selection\nmethod and shed light on alternatives to the Lasso-regularization which are not\nas popular in practice yet.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 12:47:39 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 01:42:24 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Pauphilet", "Jean", ""], ["Van Parys", "Bart", ""]]}, {"id": "1902.06718", "submitter": "David Barajas-Solano", "authors": "David A. Barajas-Solano and Alexandre M. Tartakovsky", "title": "Approximate Bayesian Model Inversion for PDEs with Heterogeneous and\n  State-Dependent Coefficients", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2019.06.010", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two approximate Bayesian inference methods for parameter\nestimation in partial differential equation (PDE) models with space-dependent\nand state-dependent parameters. We demonstrate that these methods provide\naccurate and cost-effective alternatives to Markov Chain Monte Carlo\nsimulation. We assume a parameterized Gaussian prior on the unknown functions,\nand approximate the posterior density by a parameterized multivariate Gaussian\ndensity. The parameters of the prior and posterior are estimated from sparse\nobservations of the PDE model's states and the unknown functions themselves by\nmaximizing the evidence lower bound (ELBO), a lower bound on the log marginal\nlikelihood of the observations. The first method, Laplace-EM, employs the\nexpectation maximization algorithm to maximize the ELBO, with a Laplace\napproximation of the posterior on the E-step, and minimization of a\nKullback-Leibler divergence on the M-step. The second method, DSVI-EB, employs\nthe doubly stochastic variational inference (DSVI) algorithm, in which the ELBO\nis maximized via gradient-based stochastic optimization, with nosiy gradients\ncomputed via simple Monte Carlo sampling and Gaussian backpropagation. We apply\nthese methods to identifying diffusion coefficients in linear and nonlinear\ndiffusion equations, and we find that both methods provide accurate estimates\nof posterior densities and the hyperparameters of Gaussian priors. While the\nLaplace-EM method is more accurate, it requires computing Hessians of the\nphysics model. The DSVI-EB method is found to be less accurate but only\nrequires gradients of the physics model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 18:36:07 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Barajas-Solano", "David A.", ""], ["Tartakovsky", "Alexandre M.", ""]]}, {"id": "1902.06877", "submitter": "Mitchell Krock", "authors": "Mitchell Krock, William Kleiber, and Stephen Becker", "title": "Penalized basis models for very large spatial datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern spatial models express the stochastic variation component as a\nbasis expansion with random coefficients. Low rank models, approximate spectral\ndecompositions, multiresolution representations, stochastic partial\ndifferential equations and empirical orthogonal functions all fall within this\nbasic framework. Given a particular basis, stochastic dependence relies on\nflexible modeling of the coefficients. Under a Gaussianity assumption, we\npropose a graphical model family for the stochastic coefficients by\nparameterizing the precision matrix. Sparsity in the precision matrix is\nencouraged using a penalized likelihood framework. Computations follow from a\nmajorization-minimization approach, a byproduct of which is a connection to the\ngraphical lasso. The result is a flexible nonstationary spatial model that is\nadaptable to very large datasets. We apply the model to two large and\nheterogeneous spatial datasets in statistical climatology and recover\nphysically sensible graphical structures. Moreover, the model performs\ncompetitively against the popular LatticeKrig model in predictive\ncross-validation, but substantially improves the Akaike information criterion\nscore.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 03:40:55 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Krock", "Mitchell", ""], ["Kleiber", "William", ""], ["Becker", "Stephen", ""]]}, {"id": "1902.06994", "submitter": "Augusto Fasano", "authors": "Augusto Fasano, Giovanni Rebaudo, Daniele Durante and Sonia Petrone", "title": "A closed-form filter for binary time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Gaussian state-space models arise in several applications, and within\nthis framework the binary time series setting provides a relevant example.\nHowever, unlike for Gaussian state-space models - where filtering, predictive\nand smoothing distributions are available in closed form - binary state-space\nmodels require approximations or sequential Monte Carlo strategies for\ninference and prediction. This is due to the apparent absence of conjugacy\nbetween the Gaussian states and the likelihood induced by the observation\nequation for the binary data. In this article we prove that the filtering,\npredictive and smoothing distributions in dynamic probit models with Gaussian\nstate variables are, in fact, available and belong to a class of unified\nskew-normals (SUN) whose parameters can be updated recursively in time via\nanalytical expressions. Also the key functionals of these distributions are, in\nprinciple, available, but their calculation requires the evaluation of\nmultivariate Gaussian cumulative distribution functions. Leveraging SUN\nproperties, we address this issue via novel Monte Carlo methods based on\nindependent samples from the smoothing distribution, that can easily be adapted\nto the filtering and predictive case, thus improving state-of-the-art\napproximate and sequential Monte Carlo inference in small-to-moderate\ndimensional studies. Novel sequential Monte Carlo procedures that exploit the\nSUN properties are also developed to deal with online inference in high\ndimensions. Performance gains over competitors are outlined in a financial\napplication.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 11:09:00 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 10:56:10 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 08:40:23 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Fasano", "Augusto", ""], ["Rebaudo", "Giovanni", ""], ["Durante", "Daniele", ""], ["Petrone", "Sonia", ""]]}, {"id": "1902.07037", "submitter": "Martina McMenamin", "authors": "Martina McMenamin, Jessica K. Barrett, Anna Berglind, James M.S. Wason", "title": "Employing latent variable models to improve efficiency in composite\n  endpoint analysis", "comments": "44 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composite endpoints that combine multiple outcomes on different scales are\ncommon in clinical trials, particularly in chronic conditions. In many of these\ncases, patients will have to cross a predefined responder threshold in each of\nthe outcomes to be classed as a responder overall. One instance of this occurs\nin systemic lupus erythematosus (SLE), where the responder endpoint combines\ntwo continuous, one ordinal and one binary measure. The overall binary\nresponder endpoint is typically analysed using logistic regression, resulting\nin a substantial loss of information. We propose a latent variable model for\nthe SLE endpoint, which assumes that the discrete outcomes are manifestations\nof latent continuous measures and can proceed to jointly model the components\nof the composite. We perform a simulation study and find the method to offer\nlarge efficiency gains over the standard analysis. We find that the magnitude\nof the precision gains are highly dependent on which components are driving\nresponse. Bias is introduced when joint normality assumptions are not\nsatisfied, which we correct for using a bootstrap procedure. The method is\napplied to the Phase IIb MUSE trial in patients with moderate to severe SLE. We\nshow that it estimates the treatment effect 2.5 times more precisely, offering\na 60% reduction in required sample size.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 13:10:17 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["McMenamin", "Martina", ""], ["Barrett", "Jessica K.", ""], ["Berglind", "Anna", ""], ["Wason", "James M. S.", ""]]}, {"id": "1902.07074", "submitter": "Salvatore Miccich\\`e", "authors": "Salvatore Miccich\\`e and Rosario Nunzio Mantegna", "title": "A primer on statistically validated networks", "comments": "13 pages, 2 figures. Lecture notes for the International School of\n  Physics \"Enrico Fermi\" Computational Social Science and Complex Systems 16-21\n  July 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution we discuss some approaches of network analysis providing\ninformation about single links or single nodes with respect to a null\nhypothesis taking into account the heterogeneity of the system empirically\nobserved. With this approach, a selection of nodes and links is feasible when\nthe null hypothesis is statistically rejected. We focus our discussion on\napproaches using (i) the so-called disparity filter and (ii) statistically\nvalidated network in bipartite networks. For both methods we discuss the\nimportance of using multiple hypothesis test correction. Specific applications\nof statistically validated networks are discussed. We also discuss how\nstatistically validated networks can be used to (i) pre-process large sets of\ndata and (ii) detect cores of communities that are forming the most close-knit\nand stable subsets of clusters of nodes present in a complex system.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 14:36:16 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Miccich\u00e8", "Salvatore", ""], ["Mantegna", "Rosario Nunzio", ""]]}, {"id": "1902.07154", "submitter": "Elena Kulinskaya", "authors": "Ilyas Bakbergenuly, David C. Hoaglin and Elena Kulinskaya", "title": "Simulation study of estimating between-study variance and overall effect\n  in meta-analysis of odds-ratios", "comments": "13 pages main text, and 4 Appendices containing 300 pages of A4\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random-effects meta-analysis requires an estimate of the between-study\nvariance, $\\tau^2$. We study methods of estimation of $\\tau^2$ and its\nconfidence interval in meta-analysis of odds ratio, and also the performance of\nrelated estimators of the overall effect. We provide results of extensive\nsimulations on five point estimators of $\\tau^2$ (the popular methods of\nDerSimonian-Laird, restricted maximum likelihood, and Mandel and Paule; the\nless-familiar method of Jackson; and the new method (KD) based on the improved\napproximation to the distribution of the Q statistic by Kulinskaya and\nDollinger (2015)); five interval estimators for $\\tau^2$ (profile likelihood,\nQ-profile, Biggerstaff and Jackson, Jackson, and KD), six point estimators of\nthe overall effect (the five inverse-variance estimators related to the point\nestimators of $\\tau^2$ and an estimator (SSW) whose weights use only\nstudy-level sample sizes), and eight interval estimators for the overall effect\n(five based on the point estimators for $\\tau^2$; the\nHartung-Knapp-Sidik-Jonkman (HKSJ) interval; a KD-based modification of HKSJ;\nand an interval based on the sample-size-weighted estimator). Results of our\nsimulations show that none of the point estimators of $\\tau^2$ can be\nrecommended, however the new KD estimator provides a reliable coverage of\n$\\tau^2$. Inverse-variance estimators of the overall effect are substantially\nbiased. The SSW estimator of the overall effect and the related confidence\ninterval provide the reliable point and interval estimation of log-odds-ratio.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 17:27:34 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Bakbergenuly", "Ilyas", ""], ["Hoaglin", "David C.", ""], ["Kulinskaya", "Elena", ""]]}, {"id": "1902.07232", "submitter": "Simon Vandekar", "authors": "Simon Vandekar, Ran Tao, Jeffrey Blume", "title": "A Robust Effect Size Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effect size indices are useful tools in study design and reporting because\nthey are unitless measures of association strength that do not depend on sample\nsize. Existing effect size indices are developed for particular parametric\nmodels or population parameters. Here, we propose a robust effect size index\nbased on M-estimators. This approach yields an index that is very generalizable\nbecause it is unitless across a wide range of models. We demonstrate that the\nnew index is a function of Cohen's $d$, $R^2$, and standardized log odds ratio\nwhen each of the parametric models is correctly specified. We show that\nexisting effect size estimators are biased when the parametric models are\nincorrect (e.g. under unknown heteroskedasticity). We provide simple formulas\nto compute power and sample size and use simulations to assess the bias and\nvariance of the effect size estimator in finite samples. Because the new index\nis invariant across models, it has the potential to make communication and\ncomprehension of effect size uniform across the behavioral sciences.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 19:06:40 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 17:37:24 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Vandekar", "Simon", ""], ["Tao", "Ran", ""], ["Blume", "Jeffrey", ""]]}, {"id": "1902.07279", "submitter": "Changbo Zhu", "authors": "Changbo Zhu and Xiaofeng Shao", "title": "Interpoint Distance Based Two Sample Tests in High Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a class of two sample test statistics based on\ninter-point distances in the high dimensional and low sample size setting. Our\ntest statistics include the well-known energy distance and maximum mean\ndiscrepancy with Gaussian and Laplacian kernels, and the critical values are\nobtained via permutations. We show that all these tests are inconsistent when\nthe two high dimensional distributions correspond to the same marginal\ndistributions but differ in other aspects of the distributions. The tests based\non energy distance and maximum mean discrepancy are mainly targeting the\ndifferences between marginal means and variances, whereas the test based on\n$L^1$-distance can capture the difference in marginal distributions. Our theory\nsheds new light on the limitation of inter-point distance based tests, the\nimpact of different distance metrics, and the behavior of permutation tests in\nhigh dimension. Some simulation results and a real data illustration are also\npresented to corroborate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 20:54:09 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 16:23:03 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zhu", "Changbo", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "1902.07409", "submitter": "Stefan Wager", "authors": "Susan Athey and Stefan Wager", "title": "Estimating Treatment Effects with Causal Forests: An Application", "comments": "This note will appear in an upcoming issue of Observational Studies,\n  Empirical Investigation of Methods for Heterogeneity, that compiles several\n  analyses of the same dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply causal forests to a dataset derived from the National Study of\nLearning Mindsets, and consider resulting practical and conceptual challenges.\nIn particular, we discuss how causal forests use estimated propensity scores to\nbe more robust to confounding, and how they handle data with clustered errors.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 05:17:50 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Athey", "Susan", ""], ["Wager", "Stefan", ""]]}, {"id": "1902.07634", "submitter": "Chelsea Zhang", "authors": "Chelsea Zhang, Sean J. Taylor, Curtiss Cobb, and Jasjeet Sekhon", "title": "Active Matrix Factorization for Surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amid historically low response rates, survey researchers seek ways to reduce\nrespondent burden while measuring desired concepts with precision. We propose\nto ask fewer questions of respondents and impute missing responses via\nprobabilistic matrix factorization. A variance-minimizing active learning\ncriterion chooses the most informative questions per respondent. In simulations\nof our matrix sampling procedure on real-world surveys, as well as a Facebook\nsurvey experiment, we find active question selection achieves efficiency gains\nover baselines. The reduction in imputation error is heterogeneous across\nquestions, and depends on the latent concepts they capture. The imputation\nprocedure can benefit from incorporating respondent side information, modeling\nresponses as ordered logit rather than Gaussian, and accounting for order\neffects. With our method, survey researchers obtain principled suggestions of\nquestions to retain and, if desired, can automate the design of shorter\ninstruments.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 16:46:18 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 17:04:03 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Zhang", "Chelsea", ""], ["Taylor", "Sean J.", ""], ["Cobb", "Curtiss", ""], ["Sekhon", "Jasjeet", ""]]}, {"id": "1902.07692", "submitter": "Olli Saarela", "authors": "Bo Chen, Keith A. Lawson, Antonio Finelli, Olli Saarela", "title": "Causal variance decompositions for institutional comparisons in\n  healthcare", "comments": "Revision of the original manuscript, with new data analysis results\n  and more interpretation. Methods and simulation results unchanged", "journal-ref": "Statistical methods in medical research. 2020 Jul;29(7):1972-86", "doi": "10.1177/0962280219880571", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing interest in comparing institutions delivering healthcare\nin terms of disease-specific quality indicators (QIs) that capture processes or\noutcomes showing variations in the care provided. Such comparisons can be\nframed in terms of causal models, where adjusting for patient case-mix is\nanalogous to controlling for confounding, and exposure is being treated in a\ngiven hospital, for instance. Our goal here is to help identifying good QIs\nrather than comparing hospitals in terms of an already chosen QI, and so we\nfocus on the presence and magnitude of overall variation in care between the\nhospitals rather than the pairwise differences between any two hospitals. We\nconsider how the observed variation in care received at patient level can be\ndecomposed into that causally explained by the hospital performance adjusting\nfor the case-mix, the case-mix itself, and residual variation. For this\npurpose, we derive a three-way variance decomposition, with particular\nattention to its causal interpretation in terms of potential outcome variables.\nWe propose model-based estimators for the decomposition, accommodating\ndifferent link functions and either fixed or random effect models. We evaluate\ntheir performance in a simulation study and demonstrate their use in a real\ndata application.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 18:39:07 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 03:41:09 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Chen", "Bo", ""], ["Lawson", "Keith A.", ""], ["Finelli", "Antonio", ""], ["Saarela", "Olli", ""]]}, {"id": "1902.07711", "submitter": "Isobel Claire Gormley Dr.", "authors": "Isobel Claire Gormley, Yuxin Bai and Lorraine Brennan", "title": "Combining biomarker and self-reported dietary intake data: a review of\n  the state of the art and an exposition of concepts", "comments": "To appear in Statistical Methods in Medical Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical approaches to assessing dietary intake are associated with\nmeasurement error. In an effort to address inherent measurement error in\ndietary self-reported data there is increased interest in the use of dietary\nbiomarkers as objective measures of intake. Furthermore, there is a growing\nconsensus of the need to combine dietary biomarker data with self-reported\ndata.\n  A review of state of the art techniques employed when combining biomarker and\nself-reported data is conducted. Two predominant methods, the calibration\nmethod and the method of triads, emerge as relevant techniques used when\ncombining biomarker and self-reported data to account for measurement errors in\ndietary intake assessment. Both methods crucially assume measurement error\nindependence. To expose and understand the performance of these methods in a\nrange of realistic settings, their underpinning statistical concepts are\nunified and delineated, and thorough simulation studies conducted.\n  Results show that violation of the methods' assumptions negatively impacts\nresulting inference but that this impact is mitigated when the variation of the\nbiomarker around the true intake is small. Thus there is much scope for the\nfurther development of biomarkers and models in tandem to achieve the ultimate\ngoal of accurately assessing dietary intake.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 14:43:17 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Gormley", "Isobel Claire", ""], ["Bai", "Yuxin", ""], ["Brennan", "Lorraine", ""]]}, {"id": "1902.07770", "submitter": "Yunzhang Zhu", "authors": "Shanshan Tu, Yunzhang Zhu, Yoonkyung Lee", "title": "Cross Validation for Penalized Quantile Regression with a Case-Weight\n  Adjusted Solution Path", "comments": "54 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross validation is widely used for selecting tuning parameters in\nregularization methods, but it is computationally intensive in general. To\nlessen its computational burden, approximation schemes such as generalized\napproximate cross validation (GACV) are often employed. However, such\napproximations may not work well when non-smooth loss functions are involved.\nAs a case in point, approximate cross validation schemes for penalized quantile\nregression do not work well for extreme quantiles. In this paper, we propose a\nnew algorithm to compute the leave-one-out cross validation scores exactly for\nquantile regression with ridge penalty through a case-weight adjusted solution\npath. Resorting to the homotopy technique in optimization, we introduce a case\nweight for each individual data point as a continuous embedding parameter and\ndecrease the weight gradually from one to zero to link the estimators based on\nthe full data and those with a case deleted. This allows us to design a\nsolution path algorithm to compute all leave-one-out estimators very\nefficiently from the full-data solution. We show that the case-weight adjusted\nsolution path is piecewise linear in the weight parameter, and using the\nsolution path, we examine case influences comprehensively and observe that\ndifferent modes of case influences emerge, depending on the specified\nquantiles, data dimensions and penalty parameter.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 20:35:41 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Tu", "Shanshan", ""], ["Zhu", "Yunzhang", ""], ["Lee", "Yoonkyung", ""]]}, {"id": "1902.07780", "submitter": "Dionissios Hristopulos Prof.", "authors": "Dionissios T. Hristopulos, Vasiliki D. Agou", "title": "Stochastic Local Interaction Model with Sparse Precision Matrix for\n  Space-Time Interpolation", "comments": "29 pages, 10 figures", "journal-ref": "Spatial Statistics, Available online 31 December 2019, 100403", "doi": "10.1016/j.spasta.2019.100403", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of geostatistical and machine learning methods based on\nGaussian processes to big space-time data is beset by the requirement for\nstoring and numerically inverting large and dense covariance matrices.\nComputationally efficient representations of space-time correlations can be\nconstructed using local models of conditional dependence which can reduce the\ncomputational load. We formulate a stochastic local interaction model for\nregular and scattered space-time data that incorporates interactions within\ncontrolled space-time neighborhoods. The strength of the interaction and the\nsize of the neighborhood are defined by means of kernel functions and adaptive\nlocal bandwidths. Compactly supported kernels lead to finite-size local\nneighborhoods and consequently to sparse precision matrices that admit explicit\nexpression. Hence, the stochastic local interaction model's requirements for\nstorage are modest and the costly covariance matrix inversion is not needed. We\nalso derive a semi-explicit prediction equation and express the conditional\nvariance of the prediction in terms of the diagonal of the precision matrix.\nFor data on regular space-time lattices, the stochastic local interaction model\nis equivalent to a Gaussian Markov Random Field.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 21:12:16 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 17:30:10 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Hristopulos", "Dionissios T.", ""], ["Agou", "Vasiliki D.", ""]]}, {"id": "1902.07789", "submitter": "Carlos Hernandez-Suarez M", "authors": "Carlos Hernandez-Suarez", "title": "Mean and variance of first passage time in Markov chains with unknown\n  parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are known expressions to calculate the moments of the first passage\ntime in Markov chains. Nevertheless, it is commonly forgotten that in most\napplications the parameters of the Markov chain are constructed using estimates\nbased upon empirical data and in those cases the data sample size should play\nan important role in estimating the variance. Here we provide a Monte Carlo\napproach to estimate the first two moments of the passage time in this\nsituation. We illustrate this method with an example using data from the\nbiological field.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 23:43:55 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 01:26:54 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Hernandez-Suarez", "Carlos", ""]]}, {"id": "1902.07884", "submitter": "Snigdha Panigrahi", "authors": "Snigdha Panigrahi, Jonathan Taylor", "title": "Approximate selective inference via maximum likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes post-selective inference for Gaussian models via\napproximate maximum likelihood. Our proposal serves two key goals: (i)\nefficient utility of hold-out information from selection by exploiting\nrandomization; (ii) computational ease by bypassing expensive MCMC samplers\nfrom intractable conditional distributions. At the core of our method is the\nsolution to a convex optimization problem that assumes a separable form across\nmultiple learning queries during selection. Our proposal allows us to tackle\nefficient and tractable inference in many practical scenarios where more than\none query informs inference. We illustrate the potential of our approximate\nmethod in comparisons with existing strategies across wide ranging\nsignal-to-noise regimes and on gene expression data from TCGA (The Cancer\nGenome Atlas).\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 06:45:23 GMT"}, {"version": "v2", "created": "Sun, 16 Jun 2019 20:35:45 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 02:06:43 GMT"}, {"version": "v4", "created": "Tue, 25 May 2021 19:53:57 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Panigrahi", "Snigdha", ""], ["Taylor", "Jonathan", ""]]}, {"id": "1902.07954", "submitter": "Shinpei Imori", "authors": "Shinpei Imori and Hidetoshi Shimodaira", "title": "An information criterion for auxiliary variable selection in incomplete\n  data analysis", "comments": null, "journal-ref": null, "doi": "10.3390/e21030281", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference is considered for variables of interest, called primary\nvariables, when auxiliary variables are observed along with the primary\nvariables. We consider the setting of incomplete data analysis, where some\nprimary variables are not observed. Utilizing a parametric model of joint\ndistribution of primary and auxiliary variables, it is possible to improve the\nestimation of parametric model for the primary variables when the auxiliary\nvariables are closely related to the primary variables. However, the estimation\naccuracy reduces when the auxiliary variables are irrelevant to the primary\nvariables. For selecting useful auxiliary variables, we formulate the problem\nas model selection, and propose an information criterion for predicting primary\nvariables by leveraging auxiliary variables. The proposed information criterion\nis an asymptotically unbiased estimator of the Kullback-Leibler divergence for\ncomplete data of primary variables under some reasonable conditions. We also\nclarify an asymptotic equivalence between the proposed information criterion\nand a variant of leave-one-out cross validation. Performance of our method is\ndemonstrated via a simulation study and a real data example.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 10:47:53 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 12:04:34 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Imori", "Shinpei", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "1902.07963", "submitter": "Rub\\'en G\\'omez Gonz\\'alez", "authors": "Rub\\'en G\\'omez Gonz\\'alez, M. Isabel Parra, Francisco Javier Acero\n  and Jacinto Mart\\'in", "title": "An improved method for the estimation of the Gumbel distribution\n  parameters", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usual estimation methods for the parameters of extreme values distribution\nemploy only a few values, wasting a lot of information. More precisely, in the\ncase of the Gumbel distribution, only the block maxima values are used. In this\nwork, we propose a method to seize all the available information in order to\nincrease the accuracy of the estimations. This intent can be achieved by taking\nadvantage of the existing relationship between the parameters of the baseline\ndistribution, which generates data from the full sample space, and the ones for\nthe limit Gumbel distribution. In this way, an informative prior distribution\ncan be obtained. Different statistical tests are used to compare the behaviour\nof our method with the standard one, showing that the proposed method performs\nwell when dealing with very shortened available data. The empirical\neffectiveness of the approach is demonstrated through a simulation study and a\ncase study. Reduction in the credible interval width and enhancement in\nparameter location show that the results with improved prior adapt to very\nshortened data better than standard method does.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 11:03:01 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Gonz\u00e1lez", "Rub\u00e9n G\u00f3mez", ""], ["Parra", "M. Isabel", ""], ["Acero", "Francisco Javier", ""], ["Mart\u00edn", "Jacinto", ""]]}, {"id": "1902.08229", "submitter": "Changyu Shen", "authors": "Changyu Shen, Xiaochun Li", "title": "Towards More Flexible False Positive Control in Phase III Randomized\n  Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase III randomized clinical trials play a monumentally critical role in the\nevaluation of new medical products. Because of the intrinsic nature of\nuncertainty embedded in our capability in assessing the efficacy of a medical\nproduct, interpretation of trial results relies on statistical principles to\ncontrol the error of false positives below desirable level. The\nwell-established statistical hypothesis testing procedure suffers from two\nmajor limitations, namely, the lack of flexibility in the thresholds to claim\nsuccess and the lack of capability of controlling the total number of false\npositives that could be yielded by the large volume of trials. We propose two\ngeneral theoretical frameworks based on the conventional frequentist paradigm\nand Bayesian perspectives, which offer realistic, flexible and effective\nsolutions to these limitations. Our methods are based on the distribution of\nthe effect sizes of the population of trials of interest. The estimation of\nthis distribution is practically feasible as clinicaltrials.gov provides a\ncentralized data repository with unbiased coverage of clinical trials. We\nprovide a detailed development of the two frameworks with numerical results\nobtained for industry sponsored Phase III randomized clinical trials.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 19:45:17 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Shen", "Changyu", ""], ["Li", "Xiaochun", ""]]}, {"id": "1902.08246", "submitter": "Aven Samareh", "authors": "Aven Samareh and Shuai Huang", "title": "UQ-CHI: An Uncertainty Quantification-Based Contemporaneous Health Index\n  for Degenerative Disease Monitoring", "comments": "Submitted to the Journal of Biomedical Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing knowledge-driven contemporaneous health index (CHI) that can\nprecisely reflect the underlying patient across the course of the condition's\nprogression holds a unique value, like facilitating a range of clinical\ndecision-making opportunities. This is particularly important for monitoring\ndegenerative condition such as Alzheimer's disease (AD), where the condition of\nthe patient will decay over time. Detecting early symptoms and progression\nsign, and continuous severity evaluation, are all essential for disease\nmanagement. While a few methods have been developed in the literature,\nuncertainty quantification of those health index models has been largely\nneglected. To ensure the continuity of the care, we should be more explicit\nabout the level of confidence in model outputs. Ideally, decision-makers should\nbe provided with recommendations that are robust in the face of substantial\nuncertainty about future outcomes. In this paper, we aim at filling this gap by\ndeveloping an uncertainty quantification based contemporaneous longitudinal\nindex, named UQ-CHI, with a particular focus on continuous patient monitoring\nof degenerative conditions. Our method is to combine convex optimization and\nBayesian learning using the maximum entropy learning (MEL) framework,\nintegrating uncertainty on labels as well. Our methodology also provides\nclosed-form solutions in some important decision making tasks, e.g., such as\npredicting the label of a new sample. Numerical studies demonstrate the\neffectiveness of the propose UQ-CHI method in prediction accuracy, monitoring\nefficacy, and unique advantages if uncertainty quantification is enabled\npractice.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 20:19:20 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Samareh", "Aven", ""], ["Huang", "Shuai", ""]]}, {"id": "1902.08290", "submitter": "Ian Dryden", "authors": "Katie E. Severn, Ian L. Dryden and Simon P. Preston", "title": "Manifold valued data analysis of samples of networks, with applications\n  in corpus linguistics", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks arise in many applications, such as in the analysis of text\ndocuments, social interactions and brain activity. We develop a general\nframework for extrinsic statistical analysis of samples of networks, motivated\nby networks representing text documents in corpus linguistics. We identify\nnetworks with their graph Laplacian matrices, for which we define metrics,\nembeddings, tangent spaces, and a projection from Euclidean space to the space\nof graph Laplacians. This framework provides a way of computing means,\nperforming principal component analysis and regression, and carrying out\nhypothesis tests, such as for testing for equality of means between two samples\nof networks. We apply the methodology to the set of novels by Jane Austen and\nCharles Dickens.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 22:29:00 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 09:57:39 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Severn", "Katie E.", ""], ["Dryden", "Ian L.", ""], ["Preston", "Simon P.", ""]]}, {"id": "1902.08590", "submitter": "Lisandro Ferm\\'in", "authors": "H\\'ector Araya, Natalia Bahamonde, Lisandro Ferm\\'in, Tania Roa,\n  Soledad Torres", "title": "Parameter estimation for random sampled Regression Model with Long\n  Memory Noise", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present the least squares estimator for the drift\nparameter in a linear regression model driven by the increment of a fractional\nBrownian motion sampled at random times. For two different random times,\nJittered and renewal process sampling, consistency of the estimator is proven.\nA simulation study is provided to illustrate the performance of the estimator\nunder different values of the Hurst parameter H.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 18:14:38 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Araya", "H\u00e9ctor", ""], ["Bahamonde", "Natalia", ""], ["Ferm\u00edn", "Lisandro", ""], ["Roa", "Tania", ""], ["Torres", "Soledad", ""]]}, {"id": "1902.08741", "submitter": "Shuang Jiang", "authors": "Qiwei Li, Shuang Jiang, Andrew Y. Koh, Guanghua Xiao, Xiaowei Zhan", "title": "Bayesian Modeling of Microbiome Data for Differential Abundance Analysis", "comments": "58 pages including the main text and the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advances of next-generation sequencing technology have accelerated study\nof the microbiome and stimulated the high throughput profiling of metagenomes.\nThe large volume of sequenced data has encouraged the rise of various studies\nfor detecting differentially abundant taxonomic features across healthy and\ndiseased populations, with the ultimate goal of deciphering the relationship\nbetween the microbiome diversity and health conditions. As the microbiome data\nare high-dimensional, typically featuring by uneven sampling depth,\noverdispersion and a huge amount of zeros, these data characteristics often\nhamper the downstream analysis. Moreover, the taxonomic features are implicitly\nimposed by the phylogenetic tree structure and often ignored. To overcome these\nchallenges, we propose a Bayesian hierarchical modeling framework for the\nanalysis of microbiome count data for differential abundance analysis. Under\nthis framework, we introduce a bi-level Bayesian hierarchical model that allows\na flexible choice of the count generating process, and hyperpriors in the\nfeature selection scheme. We particularly focus on employing a zero-inflated\nnegative binomial model with a Bayesian nonparametric prior model on the bottom\nlevel, and applying Gaussian mixture models for differentially abundant taxa\ndetection on the top level. Our method allows for the simultaneous modeling of\nsample heterogeneity and detecting differentially abundant taxa. We conducted\ncomprehensive simulations and summarized the improved statistical performances\nof the proposed model. We applied the model in two real microbiome study\ndatasets and successfully identified biologically validated differentially\nabundant taxa. We hope that the proposed framework and model can facilitate\nfurther microbiome studies and elucidate disease etiology.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 06:04:22 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 22:01:17 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Qiwei", ""], ["Jiang", "Shuang", ""], ["Koh", "Andrew Y.", ""], ["Xiao", "Guanghua", ""], ["Zhan", "Xiaowei", ""]]}, {"id": "1902.08828", "submitter": "Massimo Ventrucci", "authors": "Massimo Ventrucci, Daniela Cocchi, Gemma Burgazzi, Alex Laini", "title": "PC priors for residual correlation parameters in one-factor mixed models", "comments": null, "journal-ref": null, "doi": "10.1007/s10260-019-00501-w", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lack of independence in the residuals from linear regression motivates the\nuse of random effect models in many applied fields. We start from the one-way\nanova model and extend it to a general class of one-factor Bayesian mixed\nmodels, discussing several correlation structures for the within group\nresiduals. All the considered group models are parametrized in terms of a\nsingle correlation (hyper-)parameter, controlling the shrinkage towards the\ncase of independent residuals (iid). We derive a penalized complexity (PC)\nprior for the correlation parameter of a generic group model. This prior has\ndesirable properties from a practical point of view: i) it ensures appropriate\nshrinkage to the iid case; ii) it depends on a scaling parameter whose choice\nonly requires a prior guess on the proportion of total variance explained by\nthe grouping factor; iii) it is defined on a distance scale common to all group\nmodels, thus the scaling parameter can be chosen in the same manner regardless\nthe adopted group model. We show the benefit of using these PC priors in a case\nstudy in community ecology where different group models are compared.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 18:51:28 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 14:33:31 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Ventrucci", "Massimo", ""], ["Cocchi", "Daniela", ""], ["Burgazzi", "Gemma", ""], ["Laini", "Alex", ""]]}, {"id": "1902.08877", "submitter": "Vladimir Minin", "authors": "Mingwei Tang, Gytis Dudas, Trevor Bedford, Vladimir N. Minin", "title": "Fitting stochastic epidemic models to gene genealogies using linear\n  noise approximation", "comments": "43 pages, 6 figures in the main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylodynamics is a set of population genetics tools that aim at\nreconstructing demographic history of a population based on molecular sequences\nof individuals sampled from the population of interest. One important task in\nphylodynamics is to estimate changes in (effective) population size. When\napplied to infectious disease sequences such estimation of population size\ntrajectories can provide information about changes in the number of infections.\nTo model changes in the number of infected individuals, current phylodynamic\nmethods use non-parametric approaches, parametric approaches, and stochastic\nmodeling in conjunction with likelihood-free Bayesian methods. The first class\nof methods yields results that are hard-to-interpret epidemiologically. The\nsecond class of methods provides estimates of important epidemiological\nparameters, such as infection and removal/recovery rates, but ignores variation\nin the dynamics of infectious disease spread. The third class of methods is the\nmost advantageous statistically, but relies on computationally intensive\nparticle filtering techniques that limits its applications. We propose a\nBayesian model that combines phylodynamic inference and stochastic epidemic\nmodels, and achieves computational tractability by using a linear noise\napproximation (LNA) --- a technique that allows us to approximate probability\ndensities of stochastic epidemic model trajectories. LNA opens the door for\nusing modern Markov chain Monte Carlo tools to approximate the joint posterior\ndistribution of the disease transmission parameters and of high dimensional\nvectors describing unobserved changes in the stochastic epidemic model\ncompartment sizes (e.g., numbers of infectious and susceptible individuals). We\napply our estimation technique to Ebola genealogies estimated using viral\ngenetic data from the 2014 epidemic in Sierra Leone and Liberia.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 02:24:16 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Tang", "Mingwei", ""], ["Dudas", "Gytis", ""], ["Bedford", "Trevor", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1902.08944", "submitter": "Zhonglei Wang", "authors": "Jae-kwang Kim, J. N. K. Rao and Zhonglei Wang", "title": "Hypotheses Testing from Complex Survey Data Using Bootstrap Weights: A\n  Unified Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard statistical methods that do not take proper account of the\ncomplexity of survey design can lead to erroneous inferences when applied to\nsurvey data due to unequal selection probabilities, clustering, and other\ndesign features. In particular, the actual type I error rates of tests of\nhypotheses using standard methods can be much bigger than the nominal\nsignificance level. Methods that take account of survey design features in\ntesting hypotheses have been proposed, including Wald tests and quasi-score\ntests that involve the estimated covariance matrices of parameter estimates. In\nthis paper, we present a unified approach to hypothesis testing that does not\nrequire computing the covariance matrices by constructing bootstrap\napproximations to weighted likelihood ratio statistics and weighted quasi-score\nstatistics and establish the asymptotic validity of the proposed bootstrap\ntests. In addition, we also consider hypothesis testing from categorical data\nand present a bootstrap procedure for testing simple goodness of fit and\nindependence in a two-way table. In the simulation studies, the type I error\nrates of the proposed approach are much closer to their nominal significance\nlevel compared with the naive likelihood-ratio test and quasi-score test. An\napplication to data from an educational survey under a logistic regression\nmodel is also presented.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 13:32:43 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 00:55:03 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Kim", "Jae-kwang", ""], ["Rao", "J. N. K.", ""], ["Wang", "Zhonglei", ""]]}, {"id": "1902.09205", "submitter": "Manuele Leonelli", "authors": "Chiara Lattanzi and Manuele Leonelli", "title": "A changepoint approach for the identification of financial extreme\n  regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference over tails is usually performed by fitting an appropriate limiting\ndistribution over observations that exceed a fixed threshold. However, the\nchoice of such threshold is critical and can affect the inferential results.\nExtreme value mixture models have been defined to estimate the threshold using\nthe full dataset and to give accurate tail estimates. Such models assume that\nthe tail behavior is constant for all observations. However, the extreme\nbehavior of financial returns often changes considerably in time and such\nchanges occur by sudden shocks of the market. Here we extend the extreme value\nmixture model class to formally take into account distributional extreme\nchangepoints, by allowing for the presence of regime-dependent parameters\nmodelling the tail of the distribution. This extension formally uses the full\ndataset to both estimate the thresholds and the extreme changepoint locations,\ngiving uncertainty measures for both quantities. Estimation of functions of\ninterest in extreme value analyses is performed via MCMC algorithms. Our\napproach is evaluated through a series of simulations, applied to real data\nsets and assessed against competing approaches. Evidence demonstrates that the\ninclusion of different extreme regimes outperforms both static and dynamic\ncompeting approaches in financial applications.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 11:45:04 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Lattanzi", "Chiara", ""], ["Leonelli", "Manuele", ""]]}, {"id": "1902.09230", "submitter": "Marco Oesting", "authors": "Marco Oesting, Martin Schlather, Claudia Schillings", "title": "Sampling Sup-Normalized Spectral Functions for Brown-Resnick Processes", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sup-normalized spectral functions form building blocks of max-stable and\nPareto processes and therefore play an important role in modeling spatial\nextremes. For one of the most popular examples, the Brown-Resnick process,\nsimulation is not straightforward. In this paper, we generalize two approaches\nfor simulation via Markov Chain Monte Carlo methods and rejection sampling by\nintroducing new classes of proposal densities. In both cases, we provide an\noptimal choice of the proposal density with respect to sampling efficiency. The\nperformance of the procedures is demonstrated in an example.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 12:34:24 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Oesting", "Marco", ""], ["Schlather", "Martin", ""], ["Schillings", "Claudia", ""]]}, {"id": "1902.09276", "submitter": "Shovan Chowdhury", "authors": "Asok K. Nanda, Sanjib Gayen, and Shovan Chowdhury", "title": "Errors Due to Departure from Independence in Exponential Series System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reliability and life testing when the exponentially distributed components\nare put in series, it is generally assumed that the lifetimes of the components\nare independently distributed, which leads to some errors if they are not\nactually independent. In this paper, we study the relative errors incurred in\ndifferent reliability measures due to such assumptions when actually they\nfollow some bivariate exponential distributions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:32:36 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Nanda", "Asok K.", ""], ["Gayen", "Sanjib", ""], ["Chowdhury", "Shovan", ""]]}, {"id": "1902.09304", "submitter": "Leah Comment", "authors": "Leah Comment, Fabrizia Mealli, Sebastien Haneuse, Corwin Zigler", "title": "Survivor average causal effects for continuous time: a principal\n  stratification approach to causal inference with semicompeting risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In semicompeting risks problems, nonterminal time-to-event outcomes such as\ntime to hospital readmission are subject to truncation by death. These settings\nare often modeled with illness-death models for the hazards of the terminal and\nnonterminal events, but evaluating causal treatment effects with hazard models\nis problematic due to conditioning on survival (a post-treatment outcome) that\nis embedded in the definition of a hazard. Extending an existing survivor\naverage causal effect (SACE) estimand, we frame the evaluation of treatment\neffects in the context of semicompeting risks with principal stratification and\nintroduce two new causal estimands: the time-varying survivor average causal\neffect (TV-SACE) and the restricted mean survivor average causal effect\n(RM-SACE). These principal causal effects are defined among units that would\nsurvive regardless of assigned treatment. We adopt a Bayesian estimation\nprocedure that parameterizes illness-death models for both treatment arms. We\noutline a frailty specification that can accommodate within-person correlation\nbetween nonterminal and terminal event times, and we discuss potential avenues\nfor adding model flexibility. The method is demonstrated in the context of\nhospital readmission among late-stage pancreatic cancer patients.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 21:22:22 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Comment", "Leah", ""], ["Mealli", "Fabrizia", ""], ["Haneuse", "Sebastien", ""], ["Zigler", "Corwin", ""]]}, {"id": "1902.09321", "submitter": "Laura Jula Vanegas", "authors": "Laura Jula Vanegas, Merle Behr, Axel Munk", "title": "Multiscale quantile segmentation", "comments": "39 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new methodology for analyzing serial data by quantile\nregression assuming that the underlying quantile function consists of constant\nsegments. The procedure does not rely on any distributional assumption besides\nserial independence. It is based on a multiscale statistic, which allows to\ncontrol the (finite sample) probability for selecting the correct number of\nsegments S at a given error level, which serves as a tuning parameter. For a\nproper choice of this parameter, this tends exponentially fast to the true S,\nas sample size increases. We further show that the location and size of\nsegments are estimated at minimax optimal rate (compared to a Gaussian setting)\nup to a log-factor. Thereby, our approach leads to (asymptotically) uniform\nconfidence bands for the entire quantile regression function in a fully\nnonparametric setup. The procedure is efficiently implemented using dynamic\nprogramming techniques with double heap structures, and software is provided.\nSimulations and data examples from genetic sequencing and ion channel\nrecordings confirm the robustness of the proposed procedure, which at the same\nhand reliably detects changes in quantiles from arbitrary distributions with\nprecise statistical guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 14:55:21 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 08:58:22 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 15:44:07 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2020 18:44:18 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Vanegas", "Laura Jula", ""], ["Behr", "Merle", ""], ["Munk", "Axel", ""]]}, {"id": "1902.09353", "submitter": "Xuan Cao", "authors": "Xuan Cao, Shaojun Zhang", "title": "A permutation-based Bayesian approach for inverse covariance estimation", "comments": "The proof for posterior convergence rate for DAG-Wishart priors in\n  this work can be found in arXiv:1611.01205", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance estimation and selection for multivariate datasets in a\nhigh-dimensional regime is a fundamental problem in modern statistics. Gaussian\ngraphical models are a popular class of models used for this purpose. Current\nBayesian methods for inverse covariance matrix estimation under Gaussian\ngraphical models require the underlying graph and hence the ordering of\nvariables to be known. However, in practice, such information on the true\nunderlying model is often unavailable. We therefore propose a novel\npermutation-based Bayesian approach to tackle the unknown variable ordering\nissue. In particular, we utilize multiple maximum a posteriori estimates under\nthe DAG-Wishart prior for each permutation, and subsequently construct the\nfinal estimate of the inverse covariance matrix. The proposed estimator has\nsmaller variability and yields order-invariant property. We establish posterior\nconvergence rates under mild assumptions and illustrate that our method\noutperforms existing approaches in estimating the inverse covariance matrices\nvia simulation studies.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 02:00:37 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 01:58:52 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Cao", "Xuan", ""], ["Zhang", "Shaojun", ""]]}, {"id": "1902.09486", "submitter": "Yipeng Song", "authors": "Yipeng Song, Johan A. Westerhuis, Age K. Smilde", "title": "Logistic principal component analysis via non-convex singular value\n  thresholding", "comments": "19 pages, 14 figures", "journal-ref": "Chemometrics and Intelligent Laboratory Systems 204 (2020) 104089", "doi": "10.1016/j.chemolab.2020.104089", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate binary data is becoming abundant in current biological research.\nLogistic principal component analysis (PCA) is one of the commonly used tools\nto explore the relationships inside a multivariate binary data set by\nexploiting the underlying low rank structure. We re-expressed the logistic PCA\nmodel based on the latent variable interpretation of the generalized linear\nmodel on binary data. The multivariate binary data set is assumed to be the\nsign observation of an unobserved quantitative data set, on which a low rank\nstructure is assumed to exist. However, the standard logistic PCA model (using\nexact low rank constraint) is prone to overfitting, which could lead to\ndivergence of some estimated parameters towards infinity. We propose to fit a\nlogistic PCA model through non-convex singular value thresholding to alleviate\nthe overfitting issue. An efficient Majorization-Minimization algorithm is\nimplemented to fit the model and a missing value based cross validation (CV)\nprocedure is introduced for the model selection. Our experiments on realistic\nsimulations of imbalanced binary data and low signal to noise ratio show that\nthe CV error based model selection procedure is successful in selecting the\nproposed model. Furthermore, the selected model demonstrates superior\nperformance in recovering the underlying low rank structure compared to models\nwith convex nuclear norm penalty and exact low rank constraint. A binary copy\nnumber aberration data set is used to illustrate the proposed methodology in\npractice.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 18:03:11 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Song", "Yipeng", ""], ["Westerhuis", "Johan A.", ""], ["Smilde", "Age K.", ""]]}, {"id": "1902.09608", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Richard K. Crump, Max H. Farrell, Yingjie Feng", "title": "On Binscatter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binscatter is very popular in applied microeconomics. It provides a flexible,\nyet parsimonious way of visualizing and summarizing large data sets in\nregression settings, and it is often used for informal evaluation of\nsubstantive hypotheses such as linearity or monotonicity of the regression\nfunction. This paper presents a foundational, thorough analysis of binscatter:\nwe give an array of theoretical and practical results that aid both in\nunderstanding current practices (i.e., their validity or lack thereof) and in\noffering theory-based guidance for future applications. Our main results\ninclude principled number of bins selection, confidence intervals and bands,\nhypothesis tests for parametric and shape restrictions of the regression\nfunction, and several other new methods, applicable to canonical binscatter as\nwell as higher-order polynomial, covariate-adjusted and smoothness-restricted\nextensions thereof. In particular, we highlight important methodological\nproblems related to covariate adjustment methods used in current practice. We\nalso discuss extensions to clustered data. Our results are illustrated with\nsimulated and real data throughout. Companion general-purpose software packages\nfor \\texttt{Stata} and \\texttt{R} are provided. Finally, from a technical\nperspective, new theoretical results for partitioning-based series estimation\nare obtained that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 20:53:04 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Crump", "Richard K.", ""], ["Farrell", "Max H.", ""], ["Feng", "Yingjie", ""]]}, {"id": "1902.09614", "submitter": "Guilherme Pumi", "authors": "Guilherme Pumi, Taiane Schaedler Prass and Rafael Rig\\~ao Souza", "title": "A Dynamic Model for Double Bounded Time Series With Chaotic Driven\n  Conditional Averages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DS stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a class of dynamic models for time series taking\nvalues on the unit interval. The proposed model follows a generalized linear\nmodel approach where the random component, conditioned on the past information,\nfollows a beta distribution, while the conditional mean specification may\ninclude covariates and also an extra additive term given by the iteration of a\nmap that can present chaotic behavior. The resulting model is very flexible and\nits systematic component can accommodate short and long range dependence,\nperiodic behavior, laminar phases, etc. We derive easily verifiable conditions\nfor the stationarity of the proposed model, as well as conditions for the law\nof large numbers and a Birkhoff-type theorem to hold. A Monte Carlo simulation\nstudy is performed to assess the finite sample behavior of the partial maximum\nlikelihood approach for parameter estimation in the proposed model. Finally, an\napplication to the proportion of stored hydroelectrical energy in Southern\nBrazil is presented.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 20:58:51 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 20:50:58 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Pumi", "Guilherme", ""], ["Prass", "Taiane Schaedler", ""], ["Souza", "Rafael Rig\u00e3o", ""]]}, {"id": "1902.09653", "submitter": "Indranil Sahoo", "authors": "Indranil Sahoo, Joseph Guinness and Brian J. Reich", "title": "Estimating Atmospheric Motion Winds from Satellite Image Data using\n  Space-time Drift Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geostationary satellites collect high-resolution weather data comprising a\nseries of images which can be used to estimate wind speed and direction at\ndifferent altitudes. The Derived Motion Winds (DMW) Algorithm is commonly used\nto process these data and estimate atmospheric winds by tracking features in\nimages taken by the GOES-R series of the NOAA geostationary meteorological\nsatellites. However, the wind estimates from the DMW Algorithm are sparse and\ndo not come with uncertainty measures. This motivates us to statistically model\nwind motions as a spatial process drifting in time. We propose a covariance\nfunction that depends on spatial and temporal lags and a drift parameter to\ncapture the wind speed and wind direction. We estimate the parameters by local\nmaximum likelihood. Our method allows us to compute standard errors of the\nestimates, enabling spatial smoothing of the estimates using a Gaussian kernel\nweighted by the inverses of the estimated variances. We conduct extensive\nsimulation studies to determine the situations where our method performs well.\nThe proposed method is applied to the GOES-15 brightness temperature data over\nColorado and reduces prediction error of brightness temperature compared to the\nDMW Algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 23:08:16 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 05:12:08 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 16:05:08 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Sahoo", "Indranil", ""], ["Guinness", "Joseph", ""], ["Reich", "Brian J.", ""]]}, {"id": "1902.09862", "submitter": "Marco Marani", "authors": "Marco Marani and Enrico Zorzetto", "title": "Doubly stochastic distributions of extreme events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of block maxima of sequences of independent and\nidentically-distributed random variables is used to model extreme values in\nmany disciplines. The traditional extreme value (EV) theory derives a\nclosed-form expression for the distribution of block maxima under asymptotic\nassumptions, and is generally fitted using annual maxima or excesses over a\nhigh threshold, thereby discarding a large fraction of the available\nobservations. The recently-introduced Metastatistical Extreme Value\nDistribution (MEVD), a non-asymptotic formulation based on doubly stochastic\ndistributions, has been shown to offer several advantages compared to the\ntraditional EV theory. In particular, MEVD explicitly accounts for the\nvariability of the process generating the extreme values, and uses all the\navailable information to perform high-quantile inferences. Here we review the\nderivation of the MEVD, analyzing its assumptions in detail, and show that its\ngeneral formulation includes other doubly stochastic approaches to extreme\nvalue analysis that have been recently proposed.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 11:16:26 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Marani", "Marco", ""], ["Zorzetto", "Enrico", ""]]}, {"id": "1902.09978", "submitter": "Keisuke Takahata", "authors": "Keisuke Takahata and Takahiro Hoshino", "title": "Semiparametric estimation of heterogeneous treatment effects under the\n  nonignorable assignment condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semiparametric two-stage least square estimator for the\nheterogeneous treatment effects (HTE). HTE is the solution to certain integral\nequation which belongs to the class of Fredholm integral equations of the first\nkind, which is known to be ill-posed problem. Naive semi/nonparametric methods\ndo not provide stable solution to such problems. Then we propose to approximate\nthe function of interest by orthogonal series under the constraint which makes\nthe inverse mapping of integral to be continuous and eliminates the\nill-posedness. We illustrate the performance of the proposed estimator through\nsimulation experiments.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 14:52:26 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Takahata", "Keisuke", ""], ["Hoshino", "Takahiro", ""]]}, {"id": "1902.10009", "submitter": "Serveh Sharifi", "authors": "Serveh Sharifi Far, Michail Papathomas, Ruth King", "title": "Parameter Redundancy and the Existence of Maximum Likelihood Estimates\n  in Log-linear Models", "comments": "20 pages + Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-linear models are typically fitted to contingency table data to describe\nand identify the relationship between different categorical variables. However,\nthe data may include observed zero cell entries. The presence of zero cell\nentries can have an adverse effect on the estimability of parameters, due to\nparameter redundancy. We describe a general approach for determining whether a\ngiven log-linear model is parameter redundant for a pattern of observed zeros\nin the table, prior to fitting the model to the data. We derive the estimable\nparameters or functions of parameters and also explain how to reduce the\nunidentifiable model to an identifiable one. Parameter redundant models have a\nflat ridge in their likelihood function. We further explain when this ridge\nimposes some additional parameter constraints on the model, which can lead to\nobtaining unique maximum likelihood estimates for parameters that otherwise\nwould not have been estimable. In contrast to other frameworks, the proposed\nnovel approach informs on those constraints, elucidating the model that is\nactually being fitted.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 15:44:20 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 13:13:39 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Far", "Serveh Sharifi", ""], ["Papathomas", "Michail", ""], ["King", "Ruth", ""]]}, {"id": "1902.10060", "submitter": "Lingjiao Zhang", "authors": "Lingjiao Zhang, Xiruo Ding, Yanyuan Ma, Naveen Muthu, Imran Ajmal,\n  Jason H. Moore, Daniel S. Herman, Jinbo Chen", "title": "Electronic Health Record Phenotyping with Internally Assessable\n  Performance (PhIAP) using Anchor-Positive and Unlabeled Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building phenotype models using electronic health record (EHR) data\nconventionally requires manually labeled cases and controls. Assigning labels\nis labor intensive and, for some phenotypes, identifying gold-standard controls\nis prohibitive. To facilitate comprehensive clinical decision support and\nresearch, we sought to develop an accurate EHR phenotyping approach that\nassesses its performance without a validation set. Our framework relies on\nspecifying a random subset of cases, potentially using an anchor variable that\nhas excellent positive predictive value and sensitivity that is independent of\npredictors. We developed a novel maximum likelihood approach that efficiently\nleverages data from anchor-positive and unlabeled patients to develop logistic\nregression phenotyping models. Additionally, we described novel statistical\nmethods for estimating phenotyping prevalence and assessing model calibration\nand predictive performance measures. Theoretical and simulation studies\nindicated our method generates accurate predicted probabilities, leading to\nexcellent discrimination and calibration, and consistent estimates of phenotype\nprevalence and anchor sensitivity. The method appeared robust to minor\nlack-of-fit and the proposed calibration assessment detected major lack-of-fit.\nWe applied our method to EHR data to develop a preliminary model for\nidentifying patients with primary aldosteronism, which achieved an AUC of 0.99\nand PPV of 0.8. We developed novel statistical methods for accurate model\ndevelopment and validation with minimal manual labeling, facilitating\ndevelopment of scalable, transferable, semi-automated case labeling and\npractice-specific models. Our EHR phenotyping approach decreases\nlabor-intensive manual phenotyping and annotation, which should enable broader\nmodel development and dissemination for EHR clinical decision support and\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 17:26:18 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Zhang", "Lingjiao", ""], ["Ding", "Xiruo", ""], ["Ma", "Yanyuan", ""], ["Muthu", "Naveen", ""], ["Ajmal", "Imran", ""], ["Moore", "Jason H.", ""], ["Herman", "Daniel S.", ""], ["Chen", "Jinbo", ""]]}, {"id": "1902.10066", "submitter": "Alexey Shutov", "authors": "A.V. Shutov, A.A. Kaygorodtseva", "title": "Parameter identification in elasto-plasticity: distance between\n  parameters and impact of measurement errors", "comments": "11 pages, 2 figures, 4 tables", "journal-ref": "ZAMM, 2019", "doi": "10.1002/zamm.201800340", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A special aspect of parameter identification in finite-strain\nelasto-plasticity is considered. Namely, we analyze the impact of the\nmeasurement errors on the resulting set of material parameters. In order to\ndefine the sensitivity of parameters with respect to the measurement errors, a\nmechanics-based distance between two sets of parameters is introduced. Using\nthis distance function, we assess the reliability of certain parameter\nidentification procedures. The assessment involves introduction of artificial\nnoise to the experimental data; the noise can be both correlated and\nuncorrelated. An analytical procedure to speed up Monte Carlo simulations is\npresented. As a result, a simple tool for estimating the robustness of\nparameter identification is obtained. The efficiency of the approach is\nillustrated using a model of finite-strain elasto-plasticity, which accounts\nfor combined isotropic and kinematic hardening. It is shown that dealing with\ncorrelated measurement errors, most stable identification results are obtained\nfor non-diagonal weighting matrix. At the same time, there is a conflict\nbetween the stability and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 16:55:08 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Shutov", "A. V.", ""], ["Kaygorodtseva", "A. A.", ""]]}, {"id": "1902.10142", "submitter": "Feras Saad", "authors": "Feras A. Saad, Cameron E. Freer, Nathanael L. Ackerman, Vikash K.\n  Mansinghka", "title": "A Family of Exact Goodness-of-Fit Tests for High-Dimensional Discrete\n  Distributions", "comments": "20 pages, 6 figures. Appearing in AISTATS 2019", "journal-ref": "Proceedings of the 22nd International Conference on Artificial\n  Intelligence and Statistics, PMLR 89:1640-1649, 2019", "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of goodness-of-fit testing is to assess whether a dataset of\nobservations is likely to have been drawn from a candidate probability\ndistribution. This paper presents a rank-based family of goodness-of-fit tests\nthat is specialized to discrete distributions on high-dimensional domains. The\ntest is readily implemented using a simulation-based, linear-time procedure.\nThe testing procedure can be customized by the practitioner using knowledge of\nthe underlying data domain. Unlike most existing test statistics, the proposed\ntest statistic is distribution-free and its exact (non-asymptotic) sampling\ndistribution is known in closed form. We establish consistency of the test\nagainst all alternatives by showing that the test statistic is distributed as a\ndiscrete uniform if and only if the samples were drawn from the candidate\ndistribution. We illustrate its efficacy for assessing the sample quality of\napproximate sampling algorithms over combinatorially large spaces with\nintractable probabilities, including random partitions in Dirichlet process\nmixture models and random lattices in Ising models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 15:45:34 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Saad", "Feras A.", ""], ["Freer", "Cameron E.", ""], ["Ackerman", "Nathanael L.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1902.10195", "submitter": "Georg Zimmermann", "authors": "Georg Zimmermann and Markus Pauly and Arne C. Bathke", "title": "Multivariate analysis of covariance when standard assumptions are\n  violated", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmva.2020.104594", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applied research, it is often sensible to account for one or several\ncovariates when testing for differences between multivariate means of several\ngroups. However, the \"classical\" parametric multivariate analysis of covariance\n(MANCOVA) tests (e.g., Wilks' Lambda) are based on quite restrictive\nassumptions (homoscedasticity and normality of the errors), which might be\ndifficult to justify in small sample size settings. Furthermore, existing\npotential remedies (e.g., heteroskedasticity-robust approaches) become\ninappropriate in cases where the covariance matrices are singular.\nNevertheless, such scenarios are frequently encountered in the life sciences\nand other fields, when for example, in the context of standardized assessments,\na summary performance measure as well as its corresponding subscales are\nanalyzed. In the present manuscript, we consider a general MANCOVA model,\nallowing for potentially heteroskedastic and even singular covariance matrices\nas well as non-normal errors. We combine heteroskedasticity-consistent\ncovariance matrix estimation methods with our proposed modified MANCOVA\nANOVA-type statistic (MANCATS) and apply two different bootstrap approaches. We\nprovide the proofs of the asymptotic validity of the respective testing\nprocedures as well as the results from an extensive simulation study, which\nindicate that especially the parametric bootstrap version of the MANCATS\noutperforms its competitors in most scenarios, both in terms of type I error\nrates and power. These considerations are further illustrated and substantiated\nby examining real-life data from standardized achievement tests.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 20:13:24 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zimmermann", "Georg", ""], ["Pauly", "Markus", ""], ["Bathke", "Arne C.", ""]]}, {"id": "1902.10327", "submitter": "Aleksey Buzmakov", "authors": "Aleksey Buzmakov", "title": "Machine learning for subgroup discovery under treatment effect", "comments": "32 pages, in Russian, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical tasks it is needed to estimate an effect of treatment on\nindividual level. For example, in medicine it is essential to determine the\npatients that would benefit from a certain medicament. In marketing, knowing\nthe persons that are likely to buy a new product would reduce the amount of\nspam. In this chapter, we review the methods to estimate an individual\ntreatment effect from a randomized trial, i.e., an experiment when a part of\nindividuals receives a new treatment, while the others do not. Finally, it is\nshown that new efficient methods are needed in this domain.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 04:41:34 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Buzmakov", "Aleksey", ""]]}, {"id": "1902.10347", "submitter": "Raj Agrawal", "authors": "Raj Agrawal, Chandler Squires, Karren Yang, Karthik Shanmugam,\n  Caroline Uhler", "title": "ABCD-Strategy: Budgeted Experimental Design for Targeted Causal\n  Structure Discovery", "comments": "To appear in AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the causal structure of a set of variables is critical for both\nscientific inquiry and decision-making. However, this is often challenging in\npractice due to limited interventional data. Given that randomized experiments\nare usually expensive to perform, we propose a general framework and theory\nbased on optimal Bayesian experimental design to select experiments for\ntargeted causal discovery. That is, we assume the experimenter is interested in\nlearning some function of the unknown graph (e.g., all descendants of a target\nnode) subject to design constraints such as limits on the number of samples and\nrounds of experimentation. While it is in general computationally intractable\nto select an optimal experimental design strategy, we provide a tractable\nimplementation with provable guarantees on both approximation and optimization\nquality based on submodularity. We evaluate the efficacy of our proposed method\non both synthetic and real datasets, thereby demonstrating that our method\nrealizes considerable performance gains over baseline strategies such as random\nsampling.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 06:14:36 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Agrawal", "Raj", ""], ["Squires", "Chandler", ""], ["Yang", "Karren", ""], ["Shanmugam", "Karthik", ""], ["Uhler", "Caroline", ""]]}, {"id": "1902.10393", "submitter": "David  Nott", "authors": "David J. Nott, Max Seah, Luai Al-Labadi, Michael Evans, Hui Khoon Ng,\n  Berthold-Georg Englert", "title": "Using prior expansions for prior-data conflict checking", "comments": "Accepted version, to appear in Bayesian Analysis", "journal-ref": null, "doi": "10.1214/20-BA1204", "report-no": null, "categories": "stat.ME quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any Bayesian analysis involves combining information represented through\ndifferent model components, and when different sources of information are in\nconflict it is important to detect this. Here we consider checking for\nprior-data conflict in Bayesian models by expanding the prior used for the\nanalysis into a larger family of priors, and considering a marginal likelihood\nscore statistic for the expansion parameter. Consideration of different\nexpansions can be informative about the nature of any conflict, and extensions\nto hierarchically specified priors and connections with other approaches to\nprior-data conflict checking are discussed. Implementation in complex\nsituations is illustrated with two applications. The first concerns testing for\nthe appropriateness of a LASSO penalty in shrinkage estimation of coefficients\nin linear regression. Our method is compared with a recent suggestion in the\nliterature designed to be powerful against alternatives in the exponential\npower family, and we use this family as the prior expansion for constructing\nour check. A second application concerns a problem in quantum state estimation,\nwhere a multinomial model is considered with physical constraints on the model\nparameters. In this example, the usefulness of different prior expansions is\ndemonstrated for obtaining checks which are sensitive to different aspects of\nthe prior.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 08:51:51 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 05:54:12 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Nott", "David J.", ""], ["Seah", "Max", ""], ["Al-Labadi", "Luai", ""], ["Evans", "Michael", ""], ["Ng", "Hui Khoon", ""], ["Englert", "Berthold-Georg", ""]]}, {"id": "1902.10446", "submitter": "Nadja Klein Prof. Dr.", "authors": "Nadja Klein, Manuel Carlan, Thomas Kneib, Stefan Lang and Helga Wagner", "title": "Bayesian Effect Selection in Structured Additive Distributional\n  Regression Models", "comments": null, "journal-ref": "Bayesian Anal., Advance publication (2020), 29 pages", "doi": "10.1214/20-BA1214", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel spike and slab prior specification with scaled beta prime\nmarginals for the importance parameters of regression coefficients to allow for\ngeneral effect selection within the class of structured additive distributional\nregression. This enables us to model effects on all distributional parameters\nfor arbitrary parametric distributions, and to consider various effect types\nsuch as non-linear or spatial effects as well as hierarchical regression\nstructures. Our spike and slab prior relies on a parameter expansion that\nseparates blocks of regression coefficients into overall scalar importance\nparameters and vectors of standardised coefficients. Hence, we can work with a\nscalar quantity for effect selection instead of a possibly high-dimensional\neffect vector, which yields improved shrinkage and sampling performance\ncompared to the classical normal-inverse-gamma prior. We investigate the\npropriety of the posterior, show that the prior yields desirable shrinkage\nproperties, propose a way of eliciting prior parameters and provide efficient\nMarkov Chain Monte Carlo sampling. Using both simulated and three large-scale\ndata sets, we show that our approach is applicable for data with a potentially\nlarge number of covariates, multilevel predictors accounting for hierarchically\nnested data and non-standard response distributions, such as bivariate normal\nor zero-inflated Poisson.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 10:42:58 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Klein", "Nadja", ""], ["Carlan", "Manuel", ""], ["Kneib", "Thomas", ""], ["Lang", "Stefan", ""], ["Wagner", "Helga", ""]]}, {"id": "1902.10490", "submitter": "Fadhel Ayed", "authors": "Fadhel Ayed, Marco Battiston, Federico Camerlenghi and Stefano Favaro", "title": "A Good-Turing estimator for feature allocation models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature allocation models generalize species sampling models by allowing\nevery observation to belong to more than one species, now called features.\nUnder the popular Bernoulli product model for feature allocation, given $n$\nsamples, we study the problem of estimating the missing mass $M_{n}$, namely\nthe expected number hitherto unseen features that would be observed if one\nadditional individual was sampled. This is motivated by numerous applied\nproblems where the sampling procedure is expensive, in terms of time and/or\nfinancial resources allocated, and further samples can be only motivated by the\npossibility of recording new unobserved features. We introduce a simple, robust\nand theoretically sound nonparametric estimator $\\hat{M}_{n}$ of $M_{n}$.\n$\\hat{M}_{n}$ turns out to have the same analytic form of the popular\nGood-Turing estimator of the missing mass in species sampling models, with the\ndifference that the two estimators have different ranges. We show that\n$\\hat{M}_{n}$ admits a natural interpretation both as a jackknife estimator and\nas a nonparametric empirical Bayes estimator, we give provable guarantees for\nthe performance of $\\hat{M}_{n}$ in terms of minimax rate optimality, and we\nprovide with an interesting connection between $\\hat{M}_{n}$ and the\nGood-Turing estimator for species sampling. Finally, we derive non-asymptotic\nconfidence intervals for $\\hat{M}_{n}$, which are easily computable and do not\nrely on any asymptotic approximation. Our approach is illustrated with\nsynthetic data and SNP data from the ENCODE sequencing genome project.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 12:47:07 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 14:57:30 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ayed", "Fadhel", ""], ["Battiston", "Marco", ""], ["Camerlenghi", "Federico", ""], ["Favaro", "Stefano", ""]]}, {"id": "1902.10570", "submitter": "Jin Yang", "authors": "Jin Yang, Tao Zhang, Chunling Liu, Kam Chuen Yuen, Aiyi Liu", "title": "Profile and Globe Tests of Mean Surfaces for Two-Sample Bivariate\n  Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate functional data has received considerable attention but testing\nfor equality of mean surfaces and its profile has limited progress. The\nexisting literature has tested equality of either mean curves of univariate\nfunctional samples directly, or mean surfaces of bivariate functional data\nsamples but turn into functional curves comparison again. In this paper, we aim\nto develop both the profile and globe tests of mean surfaces for two-sample\nbivariate functional data. We present valid approaches of tests by employing\nthe idea of pooled projection and by developing a novel profile functional\nprincipal component analysis tool. The proposed methodology enjoys the merit of\nreadily interpretability and implementation. Under mild conditions, we derive\nthe asymptotic behaviors of test statistics under null and alternative\nhypotheses. Simulations show that the proposed tests have a good control of the\ntype I error by the size and can detect difference in mean surfaces and its\nprofile effectively in terms of power in finite samples. Finally, we apply the\ntesting procedures to two real data sets associated with the precipitation\nchange affected jointly by time and locations in the Midwest of USA, and the\ntrends in human mortality from European period life tables.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 15:03:18 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 14:15:51 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 14:30:43 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Yang", "Jin", ""], ["Zhang", "Tao", ""], ["Liu", "Chunling", ""], ["Yuen", "Kam Chuen", ""], ["Liu", "Aiyi", ""]]}, {"id": "1902.10613", "submitter": "Leah Comment", "authors": "Leah Comment, Brent A. Coull, Corwin Zigler, Linda Valeri", "title": "Bayesian data fusion for unmeasured confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian causal inference offers a principled approach to policy evaluation\nof proposed interventions on mediators or time-varying exposures. We outline a\ngeneral approach to the estimation of causal quantities for settings with\ntime-varying confounding, such as exposure-induced mediator-outcome\nconfounders. We further extend this approach to propose two Bayesian data\nfusion (BDF) methods for unmeasured confounding. Using informative priors on\nquantities relating to the confounding bias parameters, our methods incorporate\ndata from an external source where the confounder is measured in order to make\ninferences about causal estimands in the main study population. We present\nresults from a simulation study comparing our data fusion methods to two common\nfrequentist correction methods for unmeasured confounding bias in the mediation\nsetting. We also demonstrate our method with an investigation of the role of\nstage at cancer diagnosis in contributing to Black-White colorectal cancer\nsurvival disparities.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 16:12:00 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Comment", "Leah", ""], ["Coull", "Brent A.", ""], ["Zigler", "Corwin", ""], ["Valeri", "Linda", ""]]}, {"id": "1902.10693", "submitter": "Fadhel Ayed", "authors": "Fadhel Ayed and Fran\\c{c}ois Caron", "title": "Nonnegative Bayesian nonparametric factor models with completely random\n  measures for community detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian nonparametric Poisson factorization model for modeling\nnetwork data with an unknown and potentially growing number of overlapping\ncommunities. The construction is based on completely random measures and allows\nthe number of communities to either increase with the number of nodes at a\nspecified logarithmic or polynomial rate, or be bounded. We develop asymptotics\nfor the number of nodes and the degree distribution of the network and derive a\nMarkov chain Monte Carlo algorithm for targeting the exact posterior\ndistribution for this model. The usefulness of the approach is illustrated on\nvarious real networks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 18:59:02 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Ayed", "Fadhel", ""], ["Caron", "Fran\u00e7ois", ""]]}, {"id": "1902.10704", "submitter": "Yanzhi Chen", "authors": "Yanzhi Chen, Michael U. Gutmann", "title": "Adaptive Gaussian Copula ABC", "comments": "8 pages, 5 figures, accepted to AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a set of techniques for Bayesian\ninference when the likelihood is intractable but sampling from the model is\npossible. This work presents a simple yet effective ABC algorithm based on the\ncombination of two classical ABC approaches --- regression ABC and sequential\nABC. The key idea is that rather than learning the posterior directly, we first\ntarget another auxiliary distribution that can be learned accurately by\nexisting methods, through which we then subsequently learn the desired\nposterior with the help of a Gaussian copula. During this process, the\ncomplexity of the model changes adaptively according to the data at hand.\nExperiments on a synthetic dataset as well as three real-world inference tasks\ndemonstrates that the proposed method is fast, accurate, and easy to use.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 12:28:14 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Chen", "Yanzhi", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "1902.10708", "submitter": "Sandra Fortini", "authors": "Sandra Fortini, Sonia Petrone", "title": "Quasi-Bayes properties of a recursive procedure for mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods are attractive and often optimal, yet nowadays pressure for\nfast computations, especially with streaming data and online learning, brings\nrenewed interest in faster, although possibly sub-optimal, solutions. To what\nextent these algorithms may approximate a Bayesian solution is a problem of\ninterest, not always solved. On this background, in this paper we revisit a\nsequential procedure proposed by Smith and Makov (1978) for unsupervised\nlearning and classification in finite mixtures, and developed by M. Newton and\nZhang (1999), for nonparametric mixtures. Newton's algorithm is simple and\nfast, and theoretically intriguing. Although originally proposed as an\napproximation of the Bayesian solution, its quasi-Bayes properties remain\nunclear. We propose a novel methodological approach. We regard the algorithm as\na probabilistic learning rule, that implicitly defines an underlying\nprobabilistic model; and we find this model. We can then prove that it is,\nasymptotically, a Bayesian, exchangeable mixture model. Moreover, while the\nalgorithm only offers a point estimate, our approach allows us to obtain an\nasymptotic posterior distribution and asymptotic credible intervals for the\nmixing distribution. Our results also provide practical hints for tuning the\nalgorithm and obtaining desirable properties, as we illustrate in a simulation\nstudy. Beyond mixture models, our study suggests a theoretical framework that\nmay be of interest for recursive quasi-Bayes methods in other settings.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 17:13:31 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Fortini", "Sandra", ""], ["Petrone", "Sonia", ""]]}, {"id": "1902.10861", "submitter": "Tianchen Qian", "authors": "Tianchen Qian, Predrag Klasnja, Susan A. Murphy", "title": "Linear mixed models with endogenous covariates: modeling sequential\n  treatment effects with application to a mobile health study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile health is a rapidly developing field in which behavioral treatments\nare delivered to individuals via wearables or smartphones to facilitate\nhealth-related behavior change. Micro-randomized trials (MRT) are an\nexperimental design for developing mobile health interventions. In an MRT the\ntreatments are randomized numerous times for each individual over course of the\ntrial. Along with assessing treatment effects, behavioral scientists aim to\nunderstand between-person heterogeneity in the treatment effect. A natural\napproach is the familiar linear mixed model. However, directly applying linear\nmixed models is problematic because potential moderators of the treatment\neffect are frequently endogenous---that is, may depend on prior treatment. We\ndiscuss model interpretation and biases that arise in the absence of additional\nassumptions when endogenous covariates are included in a linear mixed model. In\nparticular, when there are endogenous covariates, the coefficients no longer\nhave the customary marginal interpretation. However, these coefficients still\nhave a conditional-on-the-random-effect interpretation. We provide an\nadditional assumption that, if true, allows scientists to use standard software\nto fit linear mixed model with endogenous covariates, and person-specific\npredictions of effects can be provided. As an illustration, we assess the\neffect of activity suggestion in the HeartSteps MRT and analyze the\nbetween-person treatment effect heterogeneity.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 02:00:30 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 14:05:26 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Qian", "Tianchen", ""], ["Klasnja", "Predrag", ""], ["Murphy", "Susan A.", ""]]}, {"id": "1902.10963", "submitter": "Keisuke Yano", "authors": "Kento Nakamura, Keisuke Yano, Fumiyasu Komaki", "title": "Learning partially ranked data based on graph regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranked data appear in many different applications, including voting and\nconsumer surveys. There often exhibits a situation in which data are partially\nranked. Partially ranked data is thought of as missing data. This paper\naddresses parameter estimation for partially ranked data under a (possibly)\nnon-ignorable missing mechanism. We propose estimators for both complete\nrankings and missing mechanisms together with a simple estimation procedure.\nOur estimation procedure leverages a graph regularization in conjunction with\nthe Expectation-Maximization algorithm. Our estimation procedure is\ntheoretically guaranteed to have the convergence properties. We reduce a\nmodeling bias by allowing a non-ignorable missing mechanism. In addition, we\navoid the inherent complexity within a non-ignorable missing mechanism by\nintroducing a graph regularization. The experimental results demonstrate that\nthe proposed estimators work well under non-ignorable missing mechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 09:18:40 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Nakamura", "Kento", ""], ["Yano", "Keisuke", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "1902.10981", "submitter": "Martina Vittorietti", "authors": "Martina Vittorietti, Piet J.J. Kok, Jilt Sietsma, Wei Li, Geurt\n  Jongbloed", "title": "General framework for testing Poisson-Voronoi assumption for real\n  microstructures", "comments": "29 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling microstructures is an interesting problem not just in Materials\nScience but also in Mathematics and Statistics. The most basic model for steel\nmicrostructure is the Poisson-Voronoi diagram. It has mathematically attractive\nproperties and it has been used in the approximation of single phase steel\nmicrostructures. The aim of this paper is to develop methods that can be used\nto test whether a real steel microstructure can be approximated by such a\nmodel. Therefore, a general framework for testing the Poisson-Voronoi\nassumption based on images of 2D sections of real metals is set out. Following\ntwo different approaches, according to the use or not of periodic boundary\nconditions, three different model tests are proposed. The first two are based\non the coefficient of variation and the cumulative distribution function of the\ncells area. The third exploits tools from to Topological Data Analysis, such as\npersistence landscapes.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 09:57:42 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Vittorietti", "Martina", ""], ["Kok", "Piet J. J.", ""], ["Sietsma", "Jilt", ""], ["Li", "Wei", ""], ["Jongbloed", "Geurt", ""]]}, {"id": "1902.10991", "submitter": "Luca Margaritella", "authors": "Alain Hecq, Luca Margaritella and Stephan Smeekes", "title": "Granger Causality Testing in High-Dimensional VARs: a\n  Post-Double-Selection Procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an LM test for Granger causality in high-dimensional VAR models\nbased on penalized least squares estimations. To obtain a test retaining the\nappropriate size after the variable selection done by the lasso, we propose a\npost-double-selection procedure to partial out effects of nuisance variables\nand establish its uniform asymptotic validity. We conduct an extensive set of\nMonte-Carlo simulations that show our tests perform well under different data\ngenerating processes, even without sparsity. We apply our testing procedure to\nfind networks of volatility spillovers and we find evidence that causal\nrelationships become clearer in high-dimensional compared to standard\nlow-dimensional VARs.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 10:21:27 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 10:07:29 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 12:26:27 GMT"}, {"version": "v4", "created": "Fri, 4 Dec 2020 13:58:35 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Hecq", "Alain", ""], ["Margaritella", "Luca", ""], ["Smeekes", "Stephan", ""]]}, {"id": "1902.11035", "submitter": "Przemyslaw Biecek", "authors": "Alicja Gosiewska, Aleksandra Gacek, Piotr Lubon, Przemyslaw Biecek", "title": "SAFE ML: Surrogate Assisted Feature Extraction for Model Learning", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex black-box predictive models may have high accuracy, but opacity\ncauses problems like lack of trust, lack of stability, sensitivity to concept\ndrift. On the other hand, interpretable models require more work related to\nfeature engineering, which is very time consuming. Can we train interpretable\nand accurate models, without timeless feature engineering? In this article, we\nshow a method that uses elastic black-boxes as surrogate models to create a\nsimpler, less opaque, yet still accurate and interpretable glass-box models.\nNew models are created on newly engineered features extracted/learned with the\nhelp of a surrogate model. We show applications of this method for model level\nexplanations and possible extensions for instance level explanations. We also\npresent an example implementation in Python and benchmark this method on a\nnumber of tabular data sets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 12:00:51 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Gosiewska", "Alicja", ""], ["Gacek", "Aleksandra", ""], ["Lubon", "Piotr", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "1902.11147", "submitter": "Tianchen Qian", "authors": "Tianchen Qian, Constantine Frangakis, Constantin Yiannoutsos", "title": "Deductive semiparametric estimation in Double-Sampling Designs with\n  application to PEPFAR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-ignorable dropout is common in studies with long follow-up time, and it\ncan bias study results unless handled carefully. A double-sampling design\nallocates additional resources to pursue a subsample of the dropouts and find\nout their outcomes, which can address potential biases due to non-ignorable\ndropout. It is desirable to construct semiparametric estimators for the\ndouble-sampling design because of their robustness properties. However,\nobtaining such semiparametric estimators remains a challenge due to the\nrequirement of the analytic form of the efficient influence function (EIF), the\nderivation of which can be ad hoc and difficult for the double-sampling design.\nRecent work has shown how the derivation of EIF can be made deductive and\ncomputerizable using the functional derivative representation of the EIF in\nnonparametric models. This approach, however, requires deriving the mixture of\na continuous distribution and a point mass, which can itself be challenging for\ncomplicated problems such as the double-sampling design. We propose\nsemiparametric estimators for the survival probability in double-sampling\ndesigns by generalizing the deductive and computerizable estimation approach.\nIn particular, we propose to build the semiparametric estimators based on a\ndiscretized support structure, which approximates the possibly continuous\nobserved data distribution and circumvents the derivation of the mixture\ndistribution. Our approach is deductive in the sense that it is expected to\nproduce semiparametric locally efficient estimators within finite steps without\nknowledge of the EIF. We apply the proposed estimators to estimating the\nmortality rate in a double-sampling design component of the President's\nEmergency Plan for AIDS Relief (PEPFAR) program. We evaluate the impact of\ndouble-sampling selection criteria on the mortality rate estimates.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 15:24:11 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 03:51:17 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Qian", "Tianchen", ""], ["Frangakis", "Constantine", ""], ["Yiannoutsos", "Constantin", ""]]}]