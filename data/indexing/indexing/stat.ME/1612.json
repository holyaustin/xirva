[{"id": "1612.00040", "submitter": "{\\L}ukasz Kidzi\\'nski", "authors": "{\\L}ukasz Kidzi\\'nski, Piotr Kokoszka, Neda Mohammadi Jouzdani", "title": "Principal component analysis of periodically correlated functional time\n  series", "comments": "31 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the framework of functional data analysis, we develop principal\ncomponent analysis for periodically correlated time series of functions. We\ndefine the components of the above analysis including periodic, operator-valued\nfilters, score processes and the inversion formulas. We show that these objects\nare defined via convergent series under a simple condition requiring\nsummability of the Hilbert-Schmidt norms of the filter coefficients, and that\nthey poses optimality properties.\n  We explain how the Hilbert space theory reduces to an approximate\nfinite-dimensional setting which is implemented in a custom build R package. A\ndata example and a simulation study show that the new methodology is superior\nto existing tools if the functional time series exhibit periodic\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 21:32:43 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Kidzi\u0144ski", "\u0141ukasz", ""], ["Kokoszka", "Piotr", ""], ["Jouzdani", "Neda Mohammadi", ""]]}, {"id": "1612.00064", "submitter": "Ilja Klebanov", "authors": "Ilja Klebanov, Alexander Sikorski, Christof Sch\\\"utte and Susanna\n  R\\\"oblitz", "title": "Objective Priors in the Empirical Bayes Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with Bayesian inference the choice of the prior often remains a\ndebatable question. Empirical Bayes methods offer a data-driven solution to\nthis problem by estimating the prior itself from an ensemble of data. In the\nnonparametric case, the maximum likelihood estimate is known to overfit the\ndata, an issue that is commonly tackled by regularization. However, the\nmajority of regularizations are ad hoc choices which lack invariance under\nreparametrization of the model and result in inconsistent estimates for\nequivalent models. We introduce a non-parametric, transformation invariant\nestimator for the prior distribution. Being defined in terms of the missing\ninformation similar to the reference prior, it can be seen as an extension of\nthe latter to the data-driven setting. This implies a natural interpretation as\na trade-off between choosing the least informative prior and incorporating the\ninformation provided by the data, a symbiosis between the objective and\nempirical Bayes methodologies.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 22:35:55 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 14:03:16 GMT"}, {"version": "v3", "created": "Wed, 7 Feb 2018 13:36:57 GMT"}, {"version": "v4", "created": "Fri, 4 May 2018 10:34:03 GMT"}, {"version": "v5", "created": "Mon, 11 May 2020 23:34:50 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Klebanov", "Ilja", ""], ["Sikorski", "Alexander", ""], ["Sch\u00fctte", "Christof", ""], ["R\u00f6blitz", "Susanna", ""]]}, {"id": "1612.00068", "submitter": "Rohit Patra", "authors": "Arun Kumar Kuchibhotla and Rohit Kumar Patra", "title": "Efficient Estimation in Single Index Models through Smoothing splines", "comments": "50 pages, 3 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation and inference in a single index regression model with\nan unknown but smooth link function. In contrast to the standard approach of\nusing kernels or regression splines, we use smoothing splines to estimate the\nsmooth link function. We develop a method to compute the penalized least\nsquares estimators (PLSEs) of the parametric and the nonparametric components\ngiven independent and identically distributed (i.i.d.)~data. We prove the\nconsistency and find the rates of convergence of the estimators. We establish\nasymptotic normality under under mild assumption and prove asymptotic\nefficiency of the parametric component under homoscedastic errors. A finite\nsample simulation corroborates our asymptotic theory. We also analyze a car\nmileage data set and a Ozone concentration data set. The identifiability and\nexistence of the PLSEs are also investigated.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 23:00:05 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 19:18:07 GMT"}, {"version": "v3", "created": "Sat, 25 May 2019 18:06:12 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""], ["Patra", "Rohit Kumar", ""]]}, {"id": "1612.00099", "submitter": "Nishant Subramani", "authors": "Nishant Subramani", "title": "PAG2ADMG: An Algorithm for the Complete Causal Enumeration of a Markov\n  Equivalence Class", "comments": "V2 and V1 are different enough to be different papers. V2 will be\n  significantly extended and resubmitted as a different arxiv paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal graphs, such as directed acyclic graphs (DAGs) and partial ancestral\ngraphs (PAGs), represent causal relationships among variables in a model.\nMethods exist for learning DAGs and PAGs from data and for converting DAGs to\nPAGs. However, these methods are significantly limited in that they only output\na single causal graph consistent with the independencies and dependencies (the\nMarkov equivalence class $M$) estimated from the data. This is problematic and\ninsufficient because many distinct graphs may be consistent with $M$. A data\nmodeler may wish to select among these numerous consistent graphs using domain\nknowledge or other model selection algorithms. Enumeration of the set of\nconsistent graphs is the bottleneck. In this paper, we present a method that\nmakes this desired enumeration possible. We introduce PAG2ADMG, the first\nalgorithm for enumerating all causal graphs consistent with $M$. PAG2ADMG\nconverts a given PAG into the complete set of acyclic directed mixed graphs\n(ADMGs) consistent with $M$. We prove the correctness of the approach and\ndemonstrate its efficiency relative to brute-force enumeration.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 01:08:27 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 18:34:29 GMT"}, {"version": "v3", "created": "Thu, 18 Jan 2018 18:19:39 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Subramani", "Nishant", ""]]}, {"id": "1612.00111", "submitter": "Priyam Das", "authors": "Priyam Das and Subhashis Ghosal", "title": "Bayesian Non-parametric Simultaneous Quantile Regression for Complete\n  and Grid Data", "comments": "25 pages", "journal-ref": null, "doi": "10.1016/j.csda.2018.04.0", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider Bayesian methods for non-parametric quantile\nregressions with multiple continuous predictors ranging values in the unit\ninterval. In the first method, the quantile function is assumed to be smooth\nover the explanatory variable and is expanded in tensor product of B-spline\nbasis functions. While in the second method, the distribution function is\nassumed to be smooth over the explanatory variable and is expanded in tensor\nproduct of B-spline basis functions. Unlike other existing methods of\nnon-parametric quantile regressions, the proposed methods estimate the whole\nquantile function instead of estimating on a grid of quantiles. Priors on the\nB-spline coefficients are put in such a way that the monotonicity of the\nestimated quantile levels are maintained unlike local polynomial quantile\nregression methods. The proposed methods have also been modified for quantile\ngrid data where only the percentile range of each response observations are\nknown. Simulations studies have been provided for both complete and quantile\ngrid data. The proposed method has been used to estimate the quantiles of US\nhousehold income data and North Atlantic hurricane intensity data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 01:52:09 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Das", "Priyam", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1612.00328", "submitter": "Holger Dette", "authors": "Holger Dette, Roman Guchenko, Viatcheslav Melas, Weng Kee Wong", "title": "Optimal discrimination designs for semi-parametric models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the work in the literature on optimal discrimination designs assumes\nthat the models of interest are fully specified, apart from unknown parameters\nin some models. Recent work allows errors in the models to be non-normally\ndistributed but still requires the specification of the mean structures. This\nresearch is motivated by the interesting work of Otsu (2008) to discriminate\namong semi-parametric models by generalizing the KL-optimality criterion\nproposed by L\\'opez-Fidalgo et al. (2007) and Tommasi and L\\'opez-Fidalgo\n(2010). In our work we provide further important insights in this interesting\noptimality criterion. In particular, we propose a practical strategy for\nfinding optimal discrimination designs among semi-parametric models that can\nalso be verified using an equivalence theorem. In addition, we study properties\nof such optimal designs and identify important cases where the proposed\nsemi-parametric optimal discrimination designs coincide with the celebrated T\n-optimal designs.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 16:07:18 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Dette", "Holger", ""], ["Guchenko", "Roman", ""], ["Melas", "Viatcheslav", ""], ["Wong", "Weng Kee", ""]]}, {"id": "1612.00424", "submitter": "Joseph Antonelli", "authors": "Joseph Antonelli, Matthew Cefalu, Nathan Palmer, Denis Agniel", "title": "Doubly robust matching estimators for high dimensional confounding\n  adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valid estimation of treatment effects from observational data requires proper\ncontrol of confounding. If the number of covariates is large relative to the\nnumber of observations, then controlling for all available covariates is\ninfeasible. In cases where a sparsity condition holds, variable selection or\npenalization can reduce the dimension of the covariate space in a manner that\nallows for valid estimation of treatment effects. In this article, we propose\nmatching on both the estimated propensity score and the estimated prognostic\nscores when the number of covariates is large relative to the number of\nobservations. We derive asymptotic results for the matching estimator and show\nthat it is doubly robust, in the sense that only one of the two score models\nneed be correct to obtain a consistent estimator. We show via simulation its\neffectiveness in controlling for confounding and highlight its potential to\naddress nonlinear confounding. Finally, we apply the proposed procedure to\nanalyze the effect of gender on prescription opioid use using insurance claims\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 20:41:06 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 22:11:28 GMT"}, {"version": "v3", "created": "Wed, 10 Jan 2018 15:05:17 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Antonelli", "Joseph", ""], ["Cefalu", "Matthew", ""], ["Palmer", "Nathan", ""], ["Agniel", "Denis", ""]]}, {"id": "1612.00476", "submitter": "Vinay Deolalikar", "authors": "Vinay Deolalikar and Hernan Laffitte", "title": "Extensive Large-Scale Study of Error in Samping-Based Distinct Value\n  Estimators for Databases", "comments": "This is the full-length version of a shorter published paper, and\n  includes supplementary material for the published paper. Please cite as\n  \"Vinay Deolalikar and Hernan Laffitte: Extensive Large-Scale Study of Error\n  in Samping-Based Distinct Value Estimators for Databases, IEEE Big Data\n  Conference, Washington DC, December 2016.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of distinct value estimation has many applications. Being a\ncritical component of query optimizers in databases, it also has high\ncommercial impact. Many distinct value estimators have been proposed, using\nvarious statistical approaches. However, characterizing the errors incurred by\nthese estimators is an open problem: existing analytical approaches are not\npowerful enough, and extensive empirical studies at large scale do not exist.\nWe conduct an extensive large-scale empirical study of 11 distinct value\nestimators from four different approaches to the problem over families of\nZipfian distributions whose parameters model real-world applications. Our study\nis the first that \\emph{scales to the size of a billion-rows} that today's\nlarge commercial databases have to operate in. This allows us to characterize\nthe error that is encountered in real-world applications of distinct value\nestimation. By mining the generated data, we show that estimator error depends\non a key latent parameter --- the average uniform class size --- that has not\nbeen studied previously. This parameter also allows us to unearth error\npatterns that were previously unknown. Importantly, ours is the first approach\nthat provides a framework for \\emph{visualizing the error patterns} in distinct\nvalue estimation, facilitating discussion of this problem in enterprise\nsettings. Our characterization of errors can be used for several problems in\ndistinct value estimation, such as the design of hybrid estimators. This work\naims at the practitioner and the researcher alike, and addresses questions\nfrequently asked by both audiences.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 21:33:25 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Deolalikar", "Vinay", ""], ["Laffitte", "Hernan", ""]]}, {"id": "1612.00503", "submitter": "Art Owen", "authors": "Art B. Owen and Tristan Launay", "title": "Multibrand geographic experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a geographic experiment to measure advertising effectiveness, some regions\n(hereafter GEOs) get increased advertising while others do not. This paper\nlooks at running $B>1$ such experiments simultaneously on $B$ different brands\nin $G$ GEOs, and then using shrinkage methods to estimate returns to\nadvertising. There are important practical gains from doing this. Data from any\none brand helps to estimate the return of all other brands. We see this in both\na frequentist and Bayesian formulation. As a result, each individual experiment\ncould be made smaller and less expensive when they are analyzed together. We\nalso provide an experimental design for multibrand experiments where half of\nthe brands have increased spend in each GEO while half of the GEOs have\nincreased spend for each brand. For $G>B$ the design is a two level factorial\nfor each brand and simultaneously a supersaturated design for the GEOs.\nMultiple simultaneous experiments also allow one to identify GEOs in which\nadvertising is generally more effective. That cannot be done in the single\nbrand experiments we consider.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 22:50:33 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Owen", "Art B.", ""], ["Launay", "Tristan", ""]]}, {"id": "1612.00549", "submitter": "Luyan Ji", "authors": "Xiurui Geng, Luyan Ji, Weitun Yang, Fuxiang Wang, Yongchao Zhao", "title": "MF is always superior to CEM", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constrained energy minimization (CEM) and matched filter (MF) are two\nmost frequently used target detection algorithms in the remotely sensed\ncommunity. In this paper, we first introduce an augmented CEM (ACEM) by adding\nan all-one band. According to a recently published conclusion that CEM can\nalways achieve a better performance by adding any linearly independent bands,\nACEM is better than CEM. Further, we prove that ACEM is mathematically\nequivalent to MF. As a result, we can conclude that the classical matched\nfilter (MF) is always superior to the CEM operator.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 02:46:31 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Geng", "Xiurui", ""], ["Ji", "Luyan", ""], ["Yang", "Weitun", ""], ["Wang", "Fuxiang", ""], ["Zhao", "Yongchao", ""]]}, {"id": "1612.00690", "submitter": "Anders Eklund", "authors": "Anders Eklund, Martin A. Lindquist, Mattias Villani", "title": "A Bayesian Heteroscedastic GLM with Application to fMRI Data with Motion\n  Spikes", "comments": null, "journal-ref": "NeuroImage, Volume 155, 354-369 (2017)", "doi": "10.1016/j.neuroimage.2017.04.069", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a voxel-wise general linear model with autoregressive noise and\nheteroscedastic noise innovations (GLMH) for analyzing functional magnetic\nresonance imaging (fMRI) data. The model is analyzed from a Bayesian\nperspective and has the benefit of automatically down-weighting time points\nclose to motion spikes in a data-driven manner. We develop a highly efficient\nMarkov Chain Monte Carlo (MCMC) algorithm that allows for Bayesian variable\nselection among the regressors to model both the mean (i.e., the design matrix)\nand variance. This makes it possible to include a broad range of explanatory\nvariables in both the mean and variance (e.g., time trends, activation stimuli,\nhead motion parameters and their temporal derivatives), and to compute the\nposterior probability of inclusion from the MCMC output. Variable selection is\nalso applied to the lags in the autoregressive noise process, making it\npossible to infer the lag order from the data simultaneously with all other\nmodel parameters. We use both simulated data and real fMRI data from OpenfMRI\nto illustrate the importance of proper modeling of heteroscedasticity in fMRI\ndata analysis. Our results show that the GLMH tends to detect more brain\nactivity, compared to its homoscedastic counterpart, by allowing the variance\nto change over time depending on the degree of head motion.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 14:36:34 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 13:53:30 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Eklund", "Anders", ""], ["Lindquist", "Martin A.", ""], ["Villani", "Mattias", ""]]}, {"id": "1612.00759", "submitter": "Radu V. Craiu", "authors": "Radu V. Craiu and Thierry Duchesne", "title": "A scalable and efficient covariate selection criterion for mixed effects\n  regression models with unknown random effects structure", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, 2018, 117, 154-161", "doi": "10.1016/j.csda.2017.07.011", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model selection criterion for mixed effects regression\nmodels that is computable when the model is fitted with a two-step method, even\nwhen the structure and the distribution of the random effects are unknown. The\ncriterion is especially useful in the early stage of the model building process\nwhen one needs to decide which covariates should be included in a mixed effects\nregression model but has no knowledge of the random effect structure. This is\nparticularly relevant in substantive fields where variable selection is guided\nby information criteria rather than regularization. The calculation of the\ncriterion requires only the evaluation of cluster-level log-likelihoods and\ndoes not rely on heavy numerical integration. We provide theoretical and\nnumerical arguments to justify the method and we illustrate its usefulness by\nanalyzing data on a socio-economic study of young American Indians.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 17:20:25 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 15:54:33 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Craiu", "Radu V.", ""], ["Duchesne", "Thierry", ""]]}, {"id": "1612.00877", "submitter": "Antik Chakraborty", "authors": "Antik Chakraborty, Anirban Bhattacharya, Bani K. Mallick", "title": "Bayesian sparse multiple regression for simultaneous rank reduction and\n  variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian methodology aimed at simultaneously estimating low-rank\nand row-sparse matrices in a high-dimensional multiple-response linear\nregression model. We consider a carefully devised shrinkage prior on the matrix\nof regression coefficients which obviates the need to specify a prior on the\nrank, and shrinks the regression matrix towards low-rank and row-sparse\nstructures. We provide theoretical support to the proposed methodology by\nproving minimax optimality of the posterior mean under the prediction risk in\nultra-high dimensional settings where the number of predictors can grow\nsub-exponentially relative to the sample size. A one-step post-processing\nscheme induced by group lasso penalties on the rows of the estimated\ncoefficient matrix is proposed for variable selection, with default choices of\ntuning parameters. We additionally provide an estimate of the rank using a\nnovel optimization function achieving dimension reduction in the covariate\nspace. We exhibit the performance of the proposed methodology in an extensive\nsimulation study and a real data example.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 22:16:37 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 06:09:45 GMT"}, {"version": "v3", "created": "Thu, 21 Sep 2017 05:32:55 GMT"}, {"version": "v4", "created": "Tue, 9 Apr 2019 03:29:36 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Chakraborty", "Antik", ""], ["Bhattacharya", "Anirban", ""], ["Mallick", "Bani K.", ""]]}, {"id": "1612.00922", "submitter": "Xiaohui Yuan", "authors": "Tianqing Liu, Xiaohui Yuan, Zhaohai Li and Aiyi Liu", "title": "An efficient and doubly robust empirical likelihood approach for\n  estimating equations with missing data", "comments": "31 pages,0 figures,7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers an empirical likelihood inference for parameters defined\nby general estimating equations, when data are missing at random. The\nefficiency of existing estimators depends critically on correctly specifying\nthe conditional expectation of the estimating function given the observed\ncomponents of the random observations. When the conditional expectation is not\ncorrectly specified, the efficiency of estimation can be severely compromised\neven if the propensity function (of missingness) is correctly specified. We\npropose an efficient estimator which enjoys the double-robustness property and\ncan achieve the semiparametric efficiency bound within the class of the\nestimating functions that are generated by the estimating function of\nestimating equations, if both the propensity model and the regression model (of\nthe conditional expectation) are specified correctly. Moreover, if the\npropensity model is specified correctly but the regression model is\nmisspecified, the proposed estimator still achieves a semiparametric efficiency\nlower bound within a more general class of estimating functions. Simulation\nresults suggest that the proposed estimators are robust against\nmisspecification of the propensity model or regression model and outperform\nmany existing competitors in the sense of having smaller mean-square errors.\nMoreover, using our approach for statistical inference requires neither\nresampling nor kernel smoothing. A real data example is used to illustrate the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 04:50:22 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Liu", "Tianqing", ""], ["Yuan", "Xiaohui", ""], ["Li", "Zhaohai", ""], ["Liu", "Aiyi", ""]]}, {"id": "1612.00939", "submitter": "Giovanni Maria Merola", "authors": "Giovanni Maria Merola", "title": "Projection Sparse Principal Component Analysis: an efficient least\n  squares method", "comments": "31 pages, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new sparse principal component analysis (SPCA) method in which\nthe solutions are obtained by projecting the full cardinality principal\ncomponents onto subsets of variables. The resulting components are guaranteed\nto explain a given proportion of variance. The computation of these solutions\nis very efficient. The proposed method compares well with the optimal least\nsquares sparse components. We show that other SPCA methods fail to identify the\nbest sparse approximations of the principal components and explain less\nvariance than our solutions. We illustrate and compare our method with the\nanalysis of a real dataset containing socioeconomic data and the computational\nresults for nine datasets of increasing dimension with up to 16,000 variables.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 08:42:34 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 04:16:19 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 23:37:12 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Merola", "Giovanni Maria", ""]]}, {"id": "1612.00951", "submitter": "Tom Rainforth", "authors": "Tom Rainforth, Robert Cornish, Hongseok Yang, Frank Wood", "title": "On the Pitfalls of Nested Monte Carlo", "comments": "Appearing in NIPS Workshop on Advances in Approximate Bayesian\n  Inference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in estimating expectations outside of the\nclassical inference framework, such as for models expressed as probabilistic\nprograms. Many of these contexts call for some form of nested inference to be\napplied. In this paper, we analyse the behaviour of nested Monte Carlo (NMC)\nschemes, for which classical convergence proofs are insufficient. We give\nconditions under which NMC will converge, establish a rate of convergence, and\nprovide empirical data that suggests that this rate is observable in practice.\nFinally, we prove that general-purpose nested inference schemes are inherently\nbiased. Our results serve to warn of the dangers associated with naive\ncomposition of inference and models.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 10:44:50 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Rainforth", "Tom", ""], ["Cornish", "Robert", ""], ["Yang", "Hongseok", ""], ["Wood", "Frank", ""]]}, {"id": "1612.01040", "submitter": "Tim Kraska", "authors": "Zheguang Zhao, Lorenzo De Stefani, Emanuel Zgraggen, Carsten Binnig,\n  Eli Upfal, Tim Kraska", "title": "Controlling False Discoveries During Interactive Data Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent tools for interactive data exploration significantly increase the\nchance that users make false discoveries. The crux is that these tools\nimplicitly allow the user to test a large body of different hypotheses with\njust a few clicks thus incurring in the issue commonly known in statistics as\nthe multiple hypothesis testing error. In this paper, we propose solutions to\nintegrate multiple hypothesis testing control into interactive data exploration\ntools. A key insight is that existing methods for controlling the false\ndiscovery rate (such as FDR) are not directly applicable for interactive data\nexploration. We therefore discuss a set of new control procedures that are\nbetter suited and integrated them in our system called Aware. By means of\nextensive experiments using both real-world and synthetic data sets we\ndemonstrate how Aware can help experts and novice users alike to efficiently\ncontrol false discoveries.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 00:24:17 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Zhao", "Zheguang", ""], ["De Stefani", "Lorenzo", ""], ["Zgraggen", "Emanuel", ""], ["Binnig", "Carsten", ""], ["Upfal", "Eli", ""], ["Kraska", "Tim", ""]]}, {"id": "1612.01232", "submitter": "Yuta Koike", "authors": "Takaki Hayashi, Yuta Koike", "title": "Wavelet-based methods for high-frequency lead-lag analysis", "comments": "37 pages, 2 figures. To appear in SIAM Journal on Financial\n  Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework to investigate lead-lag relationships between\ntwo financial assets. Our framework bridges a gap between continuous-time\nmodeling based on Brownian motion and the existing wavelet methods for lead-lag\nanalysis based on discrete-time models and enables us to analyze the\nmulti-scale structure of lead-lag effects. We also present a statistical\nmethodology for the scale-by-scale analysis of lead-lag effects in the proposed\nframework and develop an asymptotic theory applicable to a situation including\nstochastic volatilities and irregular sampling. Finally, we report several\nnumerical experiments to demonstrate how our framework works in practice.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 03:01:56 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 13:35:43 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2018 05:31:07 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Hayashi", "Takaki", ""], ["Koike", "Yuta", ""]]}, {"id": "1612.01291", "submitter": "Carlos Matr\\'an", "authors": "P. C. \\'Alvarez-Esteban, E. del Barrio, J. A. Cuesta-Albertos, and C.\n  Matr\\'an", "title": "Models for the assessment of treatment improvement: the ideal and the\n  feasible", "comments": null, "journal-ref": null, "doi": "10.1214/17-STS616", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparisons of different treatments or production processes are the goals of\na significant fraction of applied research. Unsurprisingly, two-sample problems\nplay a main role in Statistics through natural questions such as `Is the the\nnew treatment significantly better than the old?'. However, this is only\npartially answered by some of the usual statistical tools for this task. More\nimportantly, often practitioners are not aware of the real meaning behind these\nstatistical procedures. We analyze these troubles from the point of view of the\norder between distributions, the stochastic order, showing evidence of the\nlimitations of the usual approaches, paying special attention to the classical\ncomparison of means under the normal model. We discuss the unfeasibility of\nstatistically proving stochastic dominance, but show that it is possible,\ninstead, to gather statistical evidence to conclude that slightly relaxed\nversions of stochastic dominance hold.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 10:04:41 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 18:22:51 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["\u00c1lvarez-Esteban", "P. C.", ""], ["del Barrio", "E.", ""], ["Cuesta-Albertos", "J. A.", ""], ["Matr\u00e1n", "C.", ""]]}, {"id": "1612.01403", "submitter": "Ilja Klebanov", "authors": "Ilja Klebanov, Alexander Sikorski, Christof Sch\\\"utte, Susanna\n  R\\\"oblitz", "title": "Empirical Bayes Methods for Prior Estimation in Systems Medicine", "comments": "20 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:1612.00064", "journal-ref": null, "doi": null, "report-no": "ZR-16-57", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main goals of mathematical modeling in systems medicine related to\nmedical applications is to obtain patient-specific parameterizations and model\npredictions. In clinical practice, however, the number of available\nmeasurements for single patients is usually limited due to time and cost\nrestrictions. This hampers the process of making patient-specific predictions\nabout the outcome of a treatment. On the other hand, data are often available\nfor many patients, in particular if extensive clinical studies have been\nperformed. Therefore, before applying Bayes' rule \\emph{separately} to the data\nof each patient (which is typically performed using a non-informative prior),\nit is meaningful to use empirical Bayes methods in order to construct an\ninformative prior from all available data. We compare the performance of four\npriors -- a non-informative prior and priors chosen by nonparametric maximum\nlikelihood estimation (NPMLE), by maximum penalized likelihood estimation\n(MPLE) and by doubly-smoothed maximum likelihood estimation (DS-MLE) -- by\napplying them to a low-dimensional parameter estimation problem in a toy model\nas well as to a high-dimensional ODE model of the human menstrual cycle, which\nrepresents a typical example from systems biology modeling.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 15:47:40 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 14:09:19 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Klebanov", "Ilja", ""], ["Sikorski", "Alexander", ""], ["Sch\u00fctte", "Christof", ""], ["R\u00f6blitz", "Susanna", ""]]}, {"id": "1612.01520", "submitter": "Holger Dette", "authors": "Fumiya Akashi, Holger Dette, Yan Liu", "title": "Change point detection in autoregressive models with no moment\n  assumptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of detecting a change in the parameters\nof an autoregressive process, where the moments of the innovation process do\nnot necessarily exist. An empirical likelihood ratio test for the existence of\na change point is proposed and its asymptotic properties are studied. In\ncontrast to other work on change point tests using empirical likelihood, we do\nnot assume knowledge of the location of the change point. In particular, we\nprove that the maximizer of the empirical likelihood is a consistent estimator\nfor the parameters of the autoregressive model in the case of no change point\nand derive the limiting distribution of the corresponding test statistic under\nthe null hypothesis. We also establish consistency of the new test. A nice\nfeature of the method consists in the fact that the resulting test is\nasymptotically distribution free and does not require an estimate of the long\nrun variance. The asymptotic properties of the test are investigated by means\nof a small simulation study, which demonstrates good finite sample properties\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 4 Dec 2016 11:35:55 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Akashi", "Fumiya", ""], ["Dette", "Holger", ""], ["Liu", "Yan", ""]]}, {"id": "1612.01595", "submitter": "Hyungsuk Tak", "authors": "Hyungsuk Tak, Joseph Kelly, and Carl N. Morris", "title": "Rgbp: An R Package for Gaussian, Poisson, and Binomial Random Effects\n  Models with Frequency Coverage Evaluations", "comments": null, "journal-ref": "Journal of Statistical Software, 78, 5, 1-33 (2017)", "doi": "10.18637/jss.v078.i05", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rgbp is an R package that provides estimates and verifiable confidence\nintervals for random effects in two-level conjugate hierarchical models for\noverdispersed Gaussian, Poisson, and Binomial data. Rgbp models aggregate data\nfrom k independent groups summarized by observed sufficient statistics for each\nrandom effect, such as sample means, possibly with covariates. Rgbp uses\napproximate Bayesian machinery with unique improper priors for the\nhyper-parameters, which leads to good repeated sampling coverage properties for\nrandom effects. A special feature of Rgbp is an option that generates synthetic\ndata sets to check whether the interval estimates for random effects actually\nmeet the nominal confidence levels. Additionally, Rgbp provides inference\nstatistics for the hyper-parameters, e.g., regression coefficients.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 00:03:31 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Tak", "Hyungsuk", ""], ["Kelly", "Joseph", ""], ["Morris", "Carl N.", ""]]}, {"id": "1612.01773", "submitter": "Anna Kiriliouk", "authors": "Anna Kiriliouk and Holger Rootz\\'en and Johan Segers and Jennifer L.\n  Wadsworth", "title": "Peaks over thresholds modelling with multivariate generalized Pareto\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When assessing the impact of extreme events, it is often not just a single\ncomponent, but the combined behaviour of several components which is important.\nStatistical modelling using multivariate generalized Pareto (GP) distributions\nconstitutes the multivariate analogue of univariate peaks over thresholds\nmodelling, which is widely used in finance and engineering. We develop general\nmethods for construction of multivariate GP distributions and use them to\ncreate a variety of new statistical models. A censored likelihood procedure is\nproposed to make inference on these models, together with a threshold selection\nprocedure, goodness-of-fit diagnostics, and a computationally tractable\nstrategy for model selection. The models are fitted to returns of stock prices\nof four UK-based banks and to rainfall data in the context of landslide risk\nestimation. Supplementary materials and codes are available online.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 12:14:59 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 09:46:57 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Kiriliouk", "Anna", ""], ["Rootz\u00e9n", "Holger", ""], ["Segers", "Johan", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "1612.01801", "submitter": "Zhibing He", "authors": "Zhibing He, Yichen Qin, Ben-Chang Shia and Yang Li", "title": "Variable Selection with Scalable Bootstrap in Generalized Linear Model\n  for Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrap is commonly used as a tool for non-parametric statistical inference\nto estimate meaningful parameters in Variable Selection Models. However, for\nmassive dataset that has exponential growth rate, the computation of Bootstrap\nVariable Selection (BootVS) can be a crucial issue. In this paper, we propose\nthe method of Variable Selection with Bag of Little Bootstraps (BLBVS) on\nGeneral Linear Regression and extend it to Generalized Linear Model for\nselecting important parameters and assessing the quality of estimators'\ncomputation efficiency by analyzing results of multiple bootstrap sub-samples.\nThe introduced method best suits large datasets which have parallel and\ndistributed computing structures. To test the performance of BLBVS, we compare\nit with BootVS from different aspects via empirical studies. The results of\nsimulations show our method has excellent performance. A real data analysis,\nRisk Forecast of Credit Cards, is also presented to illustrate the\ncomputational superiority of BLBVS on large scale datasets, and the result\ndemonstrates the usefulness and validity of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 13:50:50 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 15:42:41 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["He", "Zhibing", ""], ["Qin", "Yichen", ""], ["Shia", "Ben-Chang", ""], ["Li", "Yang", ""]]}, {"id": "1612.01907", "submitter": "Jouni Helske", "authors": "Jouni Helske", "title": "KFAS: Exponential Family State Space Models in R", "comments": "39 pages, 7 figures. This is a preprint version of an article to\n  appear in the Journal of Statistical Software. Change to previous version:\n  Added grant number to acknowledgments", "journal-ref": "Journal of Statistical Software, 78(10), 1 - 39 (2017)", "doi": "10.18637/jss.v078.i10", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State space modelling is an efficient and flexible method for statistical\ninference of a broad class of time series and other data. This paper describes\nan R package KFAS for state space modelling with the observations from an\nexponential family, namely Gaussian, Poisson, binomial, negative binomial and\ngamma distributions. After introducing the basic theory behind Gaussian and\nnon-Gaussian state space models, an illustrative example of Poisson time series\nforecasting is provided. Finally, a comparison to alternative R packages\nsuitable for non-Gaussian time series modelling is presented.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 17:00:28 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 14:47:30 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Helske", "Jouni", ""]]}, {"id": "1612.02024", "submitter": "Marinho Bertanha", "authors": "Marinho Bertanha and Marcelo J. Moreira", "title": "Impossible Inference in Econometrics: Theory and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.EC stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies models in which hypothesis tests have trivial power, that\nis, power smaller than size. This testing impossibility, or impossibility type\nA, arises when any alternative is not distinguishable from the null. We also\nstudy settings in which it is impossible to have almost surely bounded\nconfidence sets for a parameter of interest. This second type of impossibility\n(type B) occurs under a condition weaker than the condition for type A\nimpossibility: the parameter of interest must be nearly unidentified. Our\ntheoretical framework connects many existing publications on impossible\ninference that rely on different notions of topologies to show models are not\ndistinguishable or nearly unidentified. We also derive both types of\nimpossibility using the weak topology induced by convergence in distribution.\nImpossibility in the weak topology is often easier to prove, it is applicable\nfor many widely-used tests, and it is useful for robust hypothesis testing. We\nconclude by demonstrating impossible inference in multiple economic\napplications of models with discontinuity and time-series models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 21:15:33 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 03:35:34 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 16:42:39 GMT"}, {"version": "v4", "created": "Fri, 16 Nov 2018 15:10:42 GMT"}, {"version": "v5", "created": "Mon, 17 Feb 2020 21:38:24 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Bertanha", "Marinho", ""], ["Moreira", "Marcelo J.", ""]]}, {"id": "1612.02090", "submitter": "Pedro H. C. Sant'Anna", "authors": "Pedro H. C. Sant'Anna", "title": "Nonparametric Tests for Treatment Effect Heterogeneity with Duration\n  Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes different tests for treatment effect heterogeneity when\nthe outcome of interest, typically a duration variable, may be right-censored.\nThe proposed tests study whether a policy 1) has zero distributional (average)\neffect for all subpopulations defined by covariate values, and 2) has\nhomogeneous average effect across different subpopulations. The proposed tests\nare based on two-step Kaplan-Meier integrals and do not rely on parametric\ndistributional assumptions, shape restrictions, or on restricting the potential\ntreatment effect heterogeneity across different subpopulations. Our framework\nis suitable not only to exogenous treatment allocation but can also account for\ntreatment noncompliance - an important feature in many applications. The\nproposed tests are consistent against fixed alternatives, and can detect\nnonparametric alternatives converging to the null at the parametric\n$n^{-1/2}$-rate, $n$ being the sample size. Critical values are computed with\nthe assistance of a multiplier bootstrap. The finite sample properties of the\nproposed tests are examined by means of a Monte Carlo study and an application\nabout the effect of labor market programs on unemployment duration. Open-source\nsoftware is available for implementing all proposed tests.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 01:35:01 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 18:24:17 GMT"}, {"version": "v3", "created": "Sat, 30 Sep 2017 21:39:23 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2020 19:06:08 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Sant'Anna", "Pedro H. C.", ""]]}, {"id": "1612.02195", "submitter": "Jerzy Rydlewski", "authors": "Daniel Kosiorowski and Dominik Mielczarek and Jerzy P. Rydlewski and\n  Ma{\\l}gorzata Snarska", "title": "Generalized Exponential smoothing in prediction of hierarchical time\n  series", "comments": null, "journal-ref": "STATISTICS IN TRANSITION new series, June 2018 Vol. 19, No. 2, pp.\n  331-350", "doi": "10.21307/stattrans-2018-019", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shang and Hyndman (2017) proposed a grouped functional time series\nforecasting approach as a combination of individual forecasts obtained using\ngeneralized least squares method. We modify their methodology using generalized\nexponential smoothing technique for the most disaggregated functional time\nseries in order to obtain more robust predictor. We discuss some properties of\nour proposals basing on results obtained via simulation studies and analysis of\nreal data related to a prediction of a demand for electricity in Australia in\n2016.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 11:06:15 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 11:54:08 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 16:12:39 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kosiorowski", "Daniel", ""], ["Mielczarek", "Dominik", ""], ["Rydlewski", "Jerzy P.", ""], ["Snarska", "Ma\u0142gorzata", ""]]}, {"id": "1612.02300", "submitter": "Eyal Fisher", "authors": "Eyal Fisher, Regev Schweiger and Saharon Rosset", "title": "Efficient Construction of Test-Inversion Confidence Intervals Using\n  Quantile Regression, With Application To Population Genetics", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern problems in statistics tend to include estimators of high\ncomputational complexity and with complicated distributions. Statistical\ninference on such estimators usually relies on asymptotic normality\nassumptions, however, such assumptions are often not applicable for available\nsample sizes, due to dependencies in the data and other causes. A common\nalternative is the use of re-sampling procedures, such as the bootstrap, but\nthese may be computationally intensive to an extent that renders them\nimpractical for modern problems. In this paper we develop a method for fast\nconstruction of test-inversion bootstrap confidence intervals. Our approach\nuses quantile regression to model the quantile of an estimator conditional on\nthe true value of the parameter, and we apply it on the Watterson estimator of\nmutation rate in a standard coalescent model. We demonstrate an improved\nefficiency of up to 40% from using quantile regression compared to state of the\nart methods based on stochastic approximation, as measured by the number of\nsimulations required to achieve comparable accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 15:45:09 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Fisher", "Eyal", ""], ["Schweiger", "Regev", ""], ["Rosset", "Saharon", ""]]}, {"id": "1612.02405", "submitter": "Maud Delattre", "authors": "Maud Delattre and Marie-Anne Poursat", "title": "An iterative algorithm for joint covariate and random effect selection\n  in mixed effects models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider joint selection of fixed and random effects in general\nmixed-effects models. The interpretation of estimated mixed-effects models is\nchallenging since changing the structure of one set of effects can lead to\ndifferent choices of important covariates in the model. We propose a stepwise\nselection algorithm to perform simultaneous selection of the fixed and random\neffects. It is based on BIC-type criteria whose penalties are adapted to\nmixed-effects models. The proposed procedure performs model selection in both\nlinear and nonlinear models. It should be used in the low-dimension setting\nwhere the number of covariates and the number of random effects are moderate\nwith respect to the total number of observations. The performance of the\nalgorithm is assessed via a simulation study, that includes also a comparative\nstudy with alternatives when available in the literature. The use of the method\nis illustrated in the clinical study of an antibiotic agent kinetics.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 20:30:23 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 13:04:55 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 13:01:29 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Delattre", "Maud", ""], ["Poursat", "Marie-Anne", ""]]}, {"id": "1612.02416", "submitter": "Anna Klimova", "authors": "Anna Klimova and Tam\\'as Rudas", "title": "Testing the fit of relational models", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational models generalize log-linear models to arbitrary discrete sample\nspaces by specifying effects associated with any subsets of their cells. A\nrelational model may include an overall effect, pertaining to every cell after\na reparameterization, and in this case, the properties of the maximum\nlikelihood estimates (MLEs) are analogous to those computed under traditional\nlog-linear models, and the goodness-of-fit tests are also the same. If an\noverall effect is not present in any reparameterization, the properties of the\nMLEs are considerably different, and the Poisson and multinomial MLEs are not\nequivalent. In the Poisson case, if the overall effect is not present, the\nobserved total is not always preserved by the MLE, and thus, the likelihood\nratio statistic is not identical with twice the Kullback-Leibler divergence.\nHowever, as demonstrated, its general form may be obtained from the Bregman\ndivergence. The asymptotic equivalence of the Pearson chi-squared and\nlikelihood ratio statistics holds, but the generality considered here requires\nextended proofs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 20:51:01 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 12:51:21 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Klimova", "Anna", ""], ["Rudas", "Tam\u00e1s", ""]]}, {"id": "1612.02512", "submitter": "Haiqing Xu", "authors": "Zhongjian Lin and Haiqing Xu", "title": "Estimation of social-influence-dependent peer pressures in a large\n  network game", "comments": "Econometric Society Summer Meeting 2014, Texas Econometrics Camp 2014", "journal-ref": null, "doi": "10.1111/ectj.12102", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on peer effects in sociology has been focused for long on social\ninfluence power to investigate the social foundations for social interactions.\nThis paper extends Xu(2011)'s large--network--based game model by allowing for\nsocial-influence-dependent peer effects. In a large network, we use the\nKatz--Bonacich centrality to measure individuals' social influences. To solve\nthe computational burden when the data come from the equilibrium of a large\nnetwork, we extend Aguirregabiria and Mira (2007)'s nested pseudo likelihood\nestimation (NPLE) approach to our large network game model. Using the Add\nHealth dataset, we investigate peer effects on conducting dangerous behaviors\nof high school students. Our results show that peer effects are statistically\nsignificant and positive. Moreover, a student benefits more (statistically\nsignificant at the 5% level) from her conformity, or equivalently, pays more\nfor her disobedience, in terms of peer pressures, if friends have higher\nsocial-influence status.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 02:21:09 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Lin", "Zhongjian", ""], ["Xu", "Haiqing", ""]]}, {"id": "1612.02710", "submitter": "Edward Ionides", "authors": "Edward L. Ionides, Carles Breto, Joonha Park, Richard A. Smith and\n  Aaron A. King", "title": "Monte Carlo profile confidence intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo methods to evaluate and maximize the likelihood function enable\nthe construction of confidence intervals and hypothesis tests, facilitating\nscientific investigation using models for which the likelihood function is\nintractable. When Monte Carlo error can be made small, by sufficiently\nexhaustive computation, then the standard theory and practice of\nlikelihood-based inference applies. As data become larger, and models more\ncomplex, situations arise where no reasonable amount of computation can render\nMonte Carlo error negligible. We develop profile likelihood methodology to\nprovide frequentist inferences that take into account Monte Carlo uncertainty.\nWe investigate the role of this methodology in facilitating inference for\ncomputationally challenging dynamic latent variable models. We present three\nexamples arising in the study of infectious disease transmission. These three\nexamples demonstrate our methodology for inference on nonlinear dynamic models\nusing genetic sequence data, panel time series data, and spatiotemporal data.\nWe also discuss applicability to nonlinear time series analysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 16:13:05 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 19:13:14 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Ionides", "Edward L.", ""], ["Breto", "Carles", ""], ["Park", "Joonha", ""], ["Smith", "Richard A.", ""], ["King", "Aaron A.", ""]]}, {"id": "1612.02848", "submitter": "Nathan Uyttendaele", "authors": "Nathan Uyttendaele, Gildas Mazo", "title": "Extending one-factor copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So far, one-factor copulas induce conditional independence with respect to a\nlatent factor. In this paper, we extend one-factor copulas to conditionally\ndependent models. This is achieved through new representations which allow to\nbuild new parametric factor copulas with a varying conditional dependence\nstructure. We discuss estimation and properties of these representations. In\norder to distinguish between conditionally independent and conditionally\ndependent factor copulas, we provide a novel statistical test which does not\nassume any parametric form for the conditional dependence structure.\nIllustrations of our framework are provided through examples, numerical\nexperiments, as well as a real data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 21:36:10 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Uyttendaele", "Nathan", ""], ["Mazo", "Gildas", ""]]}, {"id": "1612.02875", "submitter": "Debdeep Pati", "authors": "Gautam Sabnis, Debdeep Pati, Barbara Engelhardt, Natesh Pillai", "title": "A Divide and Conquer Strategy for High Dimensional Bayesian Factor\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed computing framework, based on a divide and conquer\nstrategy and hierarchical modeling, to accelerate posterior inference for\nhigh-dimensional Bayesian factor models. Our approach distributes the task of\nhigh-dimensional covariance matrix estimation to multiple cores, solves each\nsubproblem separately via a latent factor model, and then combines these\nestimates to produce a global estimate of the covariance matrix. Existing\ndivide and conquer methods focus exclusively on dividing the total number of\nobservations $n$ into subsamples while keeping the dimension $p$ fixed. Our\napproach is novel in this regard: it includes all of the $n$ samples in each\nsubproblem and, instead, splits the dimension $p$ into smaller subsets for each\nsubproblem. The subproblems themselves can be challenging to solve when $p$ is\nlarge due to the dependencies across dimensions. To circumvent this issue, we\nspecify a novel hierarchical structure on the latent factors that allows for\nflexible dependencies across dimensions, while still maintaining computational\nefficiency. Our approach is readily parallelizable and is shown to have\ncomputational efficiency of several orders of magnitude in comparison to\nfitting a full factor model. We report the performance of our method in\nsynthetic examples and a genomics application.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 00:42:47 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 03:44:51 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Sabnis", "Gautam", ""], ["Pati", "Debdeep", ""], ["Engelhardt", "Barbara", ""], ["Pillai", "Natesh", ""]]}, {"id": "1612.03054", "submitter": "Vishesh Karwa", "authors": "Vishesh Karwa, Sonja Petrovi\\'c and Denis Baji\\'c", "title": "DERGMs: Degeneracy-restricted exponential random graph models", "comments": "Version 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DM cs.SI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential random graph models, or ERGMs, are a flexible class of models for\nnetworks. Recent work highlights difficulties related to the models' ill\nbehavior, dubbed `degeneracy', such as most of the probability mass being\nconcentrated on a very small subset of the parameter space. This behavior\nlimits both the applicability of an ERGM as a model for real data and parameter\nestimation via the usual MCMC algorithms.\n  To address this problem, we propose a new exponential family of models for\nrandom graphs that build on the standard ERGM framework. We resolve the\ndegenerate model behavior by an interpretable support restriction. Namely, we\nintroduce a new parameter based on the graph-theoretic notion of degeneracy, a\nmeasure of sparsity whose value is low in real-worlds networks. We prove this\nsupport restriction does not eliminate too many graphs from the support of an\nERGM, and we also prove that degeneracy of a model is captured precisely by\nstability of its sufficient statistics. We show examples of ERGMs that are\ndegenerate whose counterpart DERGMs are not, both theoretically and by\nsimulations, and we test our model class on a set of real world networks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 15:21:46 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 15:45:19 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 10:00:12 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Karwa", "Vishesh", ""], ["Petrovi\u0107", "Sonja", ""], ["Baji\u0107", "Denis", ""]]}, {"id": "1612.03233", "submitter": "Amir Sepehri", "authors": "Amir Sepehri", "title": "New Tests of Uniformity on the Compact Classical Groups as Diagnostics\n  for Weak-star Mixing of Markov Chains", "comments": "Accepted for publication in Bernoulli", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.NA math.RT stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces two new families of non-parametric tests of\ngoodness-of-fit on the compact classical groups. One of them is a family of\ntests for the eigenvalue distribution induced by the uniform distribution,\nwhich is consistent against all fixed alternatives. The other is a family of\ntests for the uniform distribution on the entire group, which is again\nconsistent against all fixed alternatives. We find the asymptotic distribution\nunder the null and general alternatives. The tests are proved to be\nasymptotically admissible. Local power is derived and the global properties of\nthe power function against local alternatives are explored.\n  The new tests are validated on two random walks for which the mixing-time is\nstudied in the literature. The new tests, and several others, are applied to\nthe Markov chain sampler proposed by \\cite{jones2011randomized}, providing\nstrong evidence supporting the claim that the sampler mixes quickly.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 01:07:20 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 07:50:15 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Sepehri", "Amir", ""]]}, {"id": "1612.03302", "submitter": "Andrew Raim", "authors": "Andrew M. Raim, Nagaraj K. Neerchal, Jorge G. Morel", "title": "An Extension of Generalized Linear Models to Finite Mixture Outcome\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture distributions arise in sampling a heterogeneous population.\nData drawn from such a population will exhibit extra variability relative to\nany single subpopulation. Statistical models based on finite mixtures can\nassist in the analysis of categorical and count outcomes when standard\ngeneralized linear models (GLMs) cannot adequately account for variability\nobserved in the data. We propose an extension of GLM where the response is\nassumed to follow a finite mixture distribution, while the regression of\ninterest is linked to the mixture's mean. This approach may be preferred over a\nfinite mixture of regressions when the population mean is the quantity of\ninterest; here, only a single regression function must be specified and\ninterpreted in the analysis. A technical challenge is that the mean of a finite\nmixture is a composite parameter which does not appear explicitly in the\ndensity. The proposed model is completely likelihood-based and maintains the\nlink to the regression through a certain random effects structure. We consider\ntypical GLM cases where means are either real-valued, constrained to be\npositive, or constrained to be on the unit interval. The resulting model is\napplied to two example datasets through a Bayesian analysis: one with\nsuccess/failure outcomes and one with count outcomes. Supporting the extra\nvariation is seen to improve residual plots and to appropriately widen\nprediction intervals.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 14:47:40 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Raim", "Andrew M.", ""], ["Neerchal", "Nagaraj K.", ""], ["Morel", "Jorge G.", ""]]}, {"id": "1612.03428", "submitter": "Nicolas Honnorat", "authors": "Nicolas Honnorat and Christos Davatzikos", "title": "Riccati-regularized Precision Matrices for Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of graph theory in neuroimaging has pro- vided invaluable\ntools for the study of brain connectivity. These methods require the definition\nof a graph, which is typically derived by estimating the effective connectivity\nbetween brain regions through the optimization of an ill-posed inverse problem.\nConsiderable efforts have been devoted to the development of methods extracting\nsparse connectivity graphs. The present paper aims at highlighting the benefits\nof an alternative ap- proach. We investigate low-rank L2 regularized matrices\nrecently intro- duced under the denomination of Riccati regularized precision\nmatrices. We demonstrate their benefits for the analysis of cortical thickness\nmap and for the extraction of functional biomarkers from resting state fMRI\nscans. In addition, we explain how speed and result quality can be further\nimproved with random projections. The promising results obtained using the\nHuman Connectome Project dataset as well as the numerous possi- ble extensions\nand applications suggest that Riccati precision matrices might usefully\ncomplement current sparse approaches.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 16:04:30 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Honnorat", "Nicolas", ""], ["Davatzikos", "Christos", ""]]}, {"id": "1612.03451", "submitter": "Bryant Chen", "authors": "Bryant Chen, Daniel Kumor, Elias Bareinboim", "title": "Identification and Model Testing in Linear Structural Equation Models\n  using Auxiliary Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a novel approach to identification and model testing in linear\nstructural equation models (SEMs) based on auxiliary variables (AVs), which\ngeneralizes a widely-used family of methods known as instrumental variables.\nThe identification problem is concerned with the conditions under which causal\nparameters can be uniquely estimated from an observational, non-causal\ncovariance matrix. In this paper, we provide an algorithm for the\nidentification of causal parameters in linear structural models that subsumes\nprevious state-of-the-art methods. In other words, our algorithm identifies\nstrictly more coefficients and models than methods previously known in the\nliterature. Our algorithm builds on a graph-theoretic characterization of\nconditional independence relations between auxiliary and model variables, which\nis developed in this paper. Further, we leverage this new characterization for\nallowing identification when limited experimental data or new substantive\nknowledge about the domain is available. Lastly, we develop a new procedure for\nmodel testing using AVs.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 18:34:23 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 04:44:59 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2019 19:04:09 GMT"}, {"version": "v4", "created": "Mon, 7 Oct 2019 20:15:40 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Chen", "Bryant", ""], ["Kumor", "Daniel", ""], ["Bareinboim", "Elias", ""]]}, {"id": "1612.03608", "submitter": "Tom\\'a\\v{s} Mrkvi\\v{c}ka", "authors": "Tomas Mrkvicka, Mari Myllymaki, Milan Jilek, Ute Hahn", "title": "A one-way ANOVA test for functional data with graphical interpretation", "comments": "arXiv admin note: text overlap with arXiv:1506.01646", "journal-ref": "Kybernetika 56 no. 3, 432-458, 2020", "doi": "10.14736/kyb-2020-3-0432", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new functional ANOVA test, with a graphical interpretation of the result,\nis presented. The test is an extension of the global envelope test introduced\nby Myllymaki et al. (2017, Global envelope tests for spatial processes, J. R.\nStatist. Soc. B 79, 381--404, doi: 10.1111/rssb.12172). The graphical\ninterpretation is realized by a global envelope which is drawn jointly for all\nsamples of functions. If a mean function computed from the empirical data is\nout of the given envelope, the null hypothesis is rejected with the\npredetermined significance level $\\alpha$. The advantages of the proposed\none-way functional ANOVA are that it identifies the domains of the functions\nwhich are responsible for the potential rejection. We introduce two versions of\nthis test: the first gives a graphical interpretation of the test results in\nthe original space of the functions and the second immediately offers a\npost-hoc test by identifying the significant pair-wise differences between\ngroups. The proposed tests rely on discretization of the functions, therefore\nthe tests are also applicable in the multidimensional ANOVA problem. In the\nempirical part of the article, we demonstrate the use of the method by\nanalyzing fiscal decentralization in European countries. The aim of the\nempirical analysis is to capture differences between the levels of government\nexpenditure decentralization ratio among different groups of European\ncountries. The idea behind, based on the existing literature, is\nstraightforward: countries with a longer European integration history are\nsupposed to decentralize more of their government expenditure. We use the\ngovernment expenditure centralization ratios of 29 European Union and EFTA\ncountries in period from 1995 to 2016 sorted into three groups according to the\npresumed level of European economic and political integration.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 10:50:55 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 13:46:34 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 11:59:38 GMT"}, {"version": "v4", "created": "Fri, 22 May 2020 10:30:54 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Mrkvicka", "Tomas", ""], ["Myllymaki", "Mari", ""], ["Jilek", "Milan", ""], ["Hahn", "Ute", ""]]}, {"id": "1612.03858", "submitter": "Hyungsuk Tak", "authors": "Hyungsuk Tak", "title": "Frequency Coverage Properties of a Uniform Shrinkage Prior Distribution", "comments": null, "journal-ref": "Journal of Statistical Computation and Simulation, 87 (15),\n  2929-2939, 2017", "doi": "10.1080/00949655.2017.1349769", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A uniform shrinkage prior (USP) distribution on the unknown variance\ncomponent of a random-effects model is known to produce good frequency\nproperties. The USP has a parameter that determines the shape of its density\nfunction, but it has been neglected whether the USP can maintain such good\nfrequency properties regardless of the choice for the shape parameter. We\ninvestigate which choice for the shape parameter of the USP produces Bayesian\ninterval estimates of random effects that meet their nominal confidence levels\nbetter than several existent choices in the literature. Using univariate and\nmultivariate Gaussian hierarchical models, we empirically show that the USP can\nachieve its best frequency properties when its shape parameter makes the USP\nbehave similarly to an improper flat prior distribution on the unknown variance\ncomponent.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 19:24:39 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Tak", "Hyungsuk", ""]]}, {"id": "1612.03974", "submitter": "Marie Kratz", "authors": "Nehla Debbabi, Marie Kratz, Mamadou Mboup", "title": "A self-calibrating method for heavy tailed data modelling. Application\n  in neuroscience and finance", "comments": "30 pages, 9 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling non-homogeneous and multi-component data is a problem that\nchallenges scientific researchers in several fields. In general, it is not\npossible to find a simple and closed form probabilistic model to describe such\ndata. That is why one often resorts to non-parametric approaches. However, when\nthe multiple components are separable, parametric modelling becomes again\ntractable. In this study, we propose a self-calibrating method to model\nmulti-component data that exhibit heavy tails. We introduce a three-component\nhybrid distribution: a Gaussian distribution is linked to a Generalized Pareto\none via an exponential distribution that bridges the gap between mean and tail\nbehaviors. An unsupervised algorithm is then developed for estimating the\nparameters of this model. We study analytically and numerically its\nconvergence. The effectiveness of the self-calibrating method is tested on\nsimulated data, before applying it to real data from neuroscience and finance,\nrespectively. A comparison with other standard Extreme Value Theory approaches\nconfirms the relevance and the practical advantage of this new method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 23:52:59 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 14:21:52 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Debbabi", "Nehla", ""], ["Kratz", "Marie", ""], ["Mboup", "Mamadou", ""]]}, {"id": "1612.04025", "submitter": "Masayo Hirose", "authors": "Masayo Yoshimori Hirose", "title": "Second-order unbiased naive estimator of mean squared error for EBLUP in\n  small-area estimation", "comments": "13 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An empirical best linear unbiased prediction (EBLUP) estimator is utilized\nfor efficient inference in small-area estimation. To measure its uncertainty,\nwe need to estimate its mean squared error (MSE) since the true MSE cannot\ngenerally be derived in a closed form. The \"naive MSE estimator\", one of the\nestimators available for small-area inference, is unlikely to be chosen, since\nit does not achieve the desired asymptotic property, namely second-order\nunbiasedness, although it maintains strict positivity and tractability.\nTherefore, users tend to choose the second-order unbiased MSE estimator. In\nthis paper, we seek a new adjusted maximum-likelihood method to obtain a naive\nMSE estimator that achieves the required asymptotic property. To obtain the\nresult, we also reveal the relationship between the general adjusted\nmaximum-likelihood method for the model variance parameter and the general\nfunctional form of the second-order unbiased, and strictly positive, MSE\nestimator. We also compare the performance of the new method with that of the\nexisting naive estimator through a Monte Carlo simulation study. The results\nshow that the new method remedies the underestimation associated with the\nexisting naive estimator.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 04:33:54 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Hirose", "Masayo Yoshimori", ""]]}, {"id": "1612.04093", "submitter": "Tore Selland Kleppe", "authors": "Tore Selland Kleppe", "title": "Modified Cholesky Riemann Manifold Hamiltonian Monte Carlo: Exploiting\n  Sparsity for Fast Sampling of High-dimensional Targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Riemann manifold Hamiltonian Monte Carlo (RMHMC) has the potential to produce\nhigh-quality Markov chain Monte Carlo-output even for very challenging target\ndistributions. To this end, a symmetric positive definite scaling matrix for\nRMHMC, which derives, via a modified Cholesky factorization, from the\npotentially indefinite negative Hessian of the target log-density is proposed.\nThe methodology is able to exploit the sparsity of the Hessian, stemming from\nconditional independence modeling assumptions, and thus admit fast\nimplementation of RMHMC even for high-dimensional target distributions.\nMoreover, the methodology can exploit log-concave conditional target densities,\noften encountered in Bayesian hierarchical models, for faster sampling and more\nstraight forward tuning. The proposed methodology is compared to alternatives\nfor some challenging targets, and is illustrated by applying a state space\nmodel to real data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 10:45:06 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 07:44:52 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Kleppe", "Tore Selland", ""]]}, {"id": "1612.04467", "submitter": "Wenge Guo", "authors": "Gavin Lynch, Wenge Guo", "title": "On Procedures Controlling the FDR for Testing Hierarchically Ordered\n  Hypotheses", "comments": "34 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex large-scale studies, such as those related to microarray data and\nfMRI studies, often involve testing multiple hierarchically ordered hypotheses.\nHowever, most existing false discovery rate (FDR) controlling procedures do not\nexploit the inherent hierarchical structure among the tested hypotheses. In\nthis paper, we first present a generalized stepwise procedure which generalizes\nthe usual stepwise procedure to the case where each hypothesis is tested with a\ndifferent set of critical constants. This procedure is helpful in creating a\ngeneral framework under which our hierarchical testing procedures are\ndeveloped. Then, we present several hierarchical testing procedures which\ncontrol the FDR under various forms of dependence such as positive dependence\nand block dependence. Our simulation studies show that these proposed methods\ncan be more powerful in some situations than alternative methods such as\nYekutieli's hierarchical testing procedure (Yekutieli, \\emph{JASA} \\textbf{103}\n(2008) 309-316). Finally, we apply our proposed procedures to a real data set\ninvolving abundances of microbes in different ecological environments.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 03:08:16 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Lynch", "Gavin", ""], ["Guo", "Wenge", ""]]}, {"id": "1612.04535", "submitter": "Kari Krizak Halle", "authors": "Kari Krizak Halle, Srdjan Djurovic, Ole Andreas Andreassen, Mette\n  Langaas", "title": "Is the familywise error rate in genomics controlled by methods based on\n  the effective number of independent tests?", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genome-wide association (GWA) studies the goal is to detect association\nbetween one or more genetic markers and a given phenotype. The number of\ngenetic markers in a GWA study can be in the order hundreds of thousands and\ntherefore multiple testing methods are needed. This paper presents a set of\npopular methods to be used to correct for multiple testing in GWA studies. All\nare based on the concept of estimating an effective number of independent\ntests. We compare these methods using simulated data and data from the TOP\nstudy, and show that the effective number of independent tests is not additive\nover blocks of independent genetic markers unless we assume a common value for\nthe local significance level. We also show that the reviewed methods based on\nestimating the effective number of independent tests in general do not control\nthe familywise error rate.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 08:45:05 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 09:58:41 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Halle", "Kari Krizak", ""], ["Djurovic", "Srdjan", ""], ["Andreassen", "Ole Andreas", ""], ["Langaas", "Mette", ""]]}, {"id": "1612.04576", "submitter": "Martin Dirrler", "authors": "Martin Dirrler, Martin Schlather, Kirstin Strokorb", "title": "Conditionally Max-stable Random Fields based on log Gaussian Cox\n  Processes", "comments": "29 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of spatial stochastic processes in the max-domain of\nattraction of familiar max-stable processes. The new class is based on Cox\nprocesses and comprises models with short range dependence. We show that\nstatistical inference is possible within the given framework, at least under\nsome reasonable restrictions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 11:11:32 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Dirrler", "Martin", ""], ["Schlather", "Martin", ""], ["Strokorb", "Kirstin", ""]]}, {"id": "1612.04601", "submitter": "James Tan", "authors": "James PL Tan", "title": "A simple noise reduction method based on nonlinear forecasting", "comments": "21 pages, 10 figures", "journal-ref": "Phys. Rev. E 95, 032218 (2017)", "doi": "10.1103/PhysRevE.95.032218", "report-no": null, "categories": "nlin.CD q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric detrending or noise reduction methods are often employed to\nseparate trends from noisy time series when no satisfactory models exist to fit\nthe data. However, conventional detrending methods depend on subjective choices\nof detrending parameters. Here, we present a simple multivariate detrending\nmethod based on available nonlinear forecasting techniques. These are in turn\nbased on state space reconstruction for which a strong theoretical\njustification exists for their use in non-parametric forecasting. The\ndetrending method presented here is conceptually similar to Schreiber's noise\nreduction method using state space reconstruction. However, we show that\nSchreiber's method contains a minor flaw that can be overcome with forecasting.\nFurthermore, our detrending method contains a simple but nontrivial extension\nto multivariate time series. We apply the detrending method to multivariate\ntime series generated from the Van der Pol oscillator, the Lorenz equations,\nthe Hindmarsh-Rose model of neuronal spiking activity, and a univariate\nreal-life measles data set. It is demonstrated that detrending heuristics can\nbe objectively optimized with in-sample forecasting errors that correlate well\nwith actual detrending errors.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 12:37:40 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Tan", "James PL", ""]]}, {"id": "1612.04615", "submitter": "Wenlin Dai", "authors": "Wenlin Dai and Marc G. Genton", "title": "Directional Outlyingness for Multivariate Functional Data", "comments": "36 pages, 7 figures, Computational Statistics & Data Analysis, 2018", "journal-ref": null, "doi": "10.1016/j.csda.2018.03.017", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The direction of outlyingness is crucial to describing the centrality of\nmultivariate functional data. Motivated by this idea, we generalize classical\ndepth to directional outlyingness for functional data. We investigate\ntheoretical properties of functional directional outlyingness and find that the\ntotal outlyingness can be naturally decomposed into two parts: magnitude\noutlyingness and shape outlyingness which represent the centrality of a curve\nfor magnitude and shape, respectively. Using this decomposition, we provide a\nvisualization tool for the centrality of curves. Furthermore, we design an\noutlier detection procedure based on functional directional outlyingness. This\ncriterion applies to both univariate and multivariate curves and simulation\nstudies show that it outperforms competing methods. Weather and\nelectrocardiogram data demonstrate the practical application of our proposed\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 12:55:51 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 06:14:12 GMT"}, {"version": "v3", "created": "Sun, 30 Apr 2017 10:53:17 GMT"}, {"version": "v4", "created": "Sun, 17 Sep 2017 10:33:22 GMT"}, {"version": "v5", "created": "Sun, 22 Apr 2018 06:48:35 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Dai", "Wenlin", ""], ["Genton", "Marc G.", ""]]}, {"id": "1612.04710", "submitter": "Karen Fuchs", "authors": "Karen Fuchs (1 and 2), Wolfgang P\\\"o{\\ss}necker (2), Gerhard Tutz (2)\n  ((1) Siemens AG, CT RDA SII CPS-DE, Munich, (2) Department of Statistics,\n  Ludwig-Maximilians-Universit\\\"at M\\\"unchen)", "title": "Classification of Functional Data with k-Nearest-Neighbor Ensembles by\n  Fitting Constrained Multinomial Logit Models", "comments": "The first replacement is due to an update of the data links. No other\n  changes took place with respect to the original submission. To reproduce\n  results of the cell chip or phoneme data application, files can now be\n  downloaded from\n  http://agfda.userweb.mwn.de/files/Fuchs_Poessnecker_Tutz_pcMLM_code_cellchip.zip\n  or\n  http://agfda.userweb.mwn.de/files/Fuchs_Poessnecker_Tutz_pcMLM_code_phonemes.zip", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decades, many methods for the analysis of functional data\nincluding classification methods have been developed. Nonetheless, there are\nissues that have not been adressed satisfactorily by currently available\nmethods, as, for example, feature selection combined with variable selection\nwhen using multiple functional covariates. In this paper, a functional ensemble\nis combined with a penalized and constrained multinomial logit model. It is\nshown that this synthesis yields a powerful classification tool for functional\ndata (possibly mixed with non-functional predictors), which also provides\nautomatic variable selection. The choice of an appropriate, sparsity-inducing\npenalty allows to estimate most model coefficients to exactly zero, and permits\nclass-specific coefficients in multiclass problems, such that feature selection\nis obtained. An additional constraint within the multinomial logit model\nensures that the model coefficients can be considered as weights. Thus, the\nestimation results become interpretable with respect to the discriminative\nimportance of the selected features, which is rated by a feature importance\nmeasure. In two application examples, data of a cell chip used for water\nquality monitoring experiments and phoneme data used for speech recognition,\nthe interpretability as well as the selection results are examined. The\nclassification performance is compared to various other classification\napproaches which are in common use.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 16:19:30 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 21:15:20 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Fuchs", "Karen", "", "1 and 2"], ["P\u00f6\u00dfnecker", "Wolfgang", ""], ["Tutz", "Gerhard", ""]]}, {"id": "1612.04717", "submitter": "Tianxi Li", "authors": "Tianxi Li, Elizaveta Levina, Ji Zhu", "title": "Network cross-validation by edge sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many statistical models and methods are now available for network\nanalysis, resampling network data remains a challenging problem.\nCross-validation is a useful general tool for model selection and parameter\ntuning, but is not directly applicable to networks since splitting network\nnodes into groups requires deleting edges and destroys some of the network\nstructure. Here we propose a new network resampling strategy based on splitting\nnode pairs rather than nodes applicable to cross-validation for a wide range of\nnetwork model selection tasks. We provide a theoretical justification for our\nmethod in a general setting and examples of how our method can be used in\nspecific network model selection and parameter tuning tasks. Numerical results\non simulated networks and on a citation network of statisticians show that this\ncross-validation approach works well for model selection.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 16:36:30 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 19:34:19 GMT"}, {"version": "v3", "created": "Sun, 1 Jan 2017 21:03:42 GMT"}, {"version": "v4", "created": "Wed, 13 Sep 2017 20:05:34 GMT"}, {"version": "v5", "created": "Wed, 9 May 2018 18:49:21 GMT"}, {"version": "v6", "created": "Thu, 14 Mar 2019 02:00:51 GMT"}, {"version": "v7", "created": "Fri, 1 May 2020 07:38:59 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Li", "Tianxi", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "1612.04838", "submitter": "Sebastian D\\\"ohler", "authors": "Sebastian D\\\"ohler", "title": "A discrete modification of the Benjamini-Yekutieli procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Benjamini-Yekutieli procedure is a multiple testing method that controls\nthe false discovery rate under arbitrary dependence of the $p$-values. A\nmodification of this and related procedures is proposed for the case when the\ntest statistics are discrete. It is shown that taking discreteness into account\ncan improve upon known procedures. The performance of this new procedure is\nevaluated for pharmacovigilance data and in a simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 21:05:20 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["D\u00f6hler", "Sebastian", ""]]}, {"id": "1612.04873", "submitter": "Carlos Martinez Mr.", "authors": "Carlos Alberto Mart\\'inez, Kshitij Khare, Syed Rahman, Mauricio A.\n  Elzo", "title": "Introducing Gaussian covariance graph models in genome-wide prediction", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several statistical models used in genome-wide prediction assume independence\nof marker allele substitution effects, but it is known that these effects might\nbe correlated. In statistics, graphical models have been identified as a useful\ntool for covariance estimation in high dimensional problems and it is an area\nthat has recently experienced a great expansion. In Gaussian covariance graph\nmodels (GCovGM), the joint distribution of a set of random variables is assumed\nto be Gaussian and the pattern of zeros of the covariance matrix is encoded in\nterms of an undirected graph G. In this study, methods adapting the theory of\nGCovGM to genome-wide prediction were developed (Bayes GCov, Bayes GCov-KR and\nBayes GCov-H). In simulated and real datasets, improvements in correlation\nbetween phenotypes and predicted breeding values and accuracies of predicted\nbreeding values were found. Our models account for correlation of marker\neffects and permit to accommodate general structures as opposed to models\nproposed in previous studies which consider spatial correlation only. In\naddition, they allow incorporation of biological information in the prediction\nprocess through its use when constructing graph G, and their extension to the\nmultiallelic loci case is straightforward.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 22:49:12 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 21:19:46 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 14:43:45 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Mart\u00ednez", "Carlos Alberto", ""], ["Khare", "Kshitij", ""], ["Rahman", "Syed", ""], ["Elzo", "Mauricio A.", ""]]}, {"id": "1612.04911", "submitter": "Ting Wang", "authors": "Ting Wang and Edgar C. Merkle", "title": "Derivative Computations and Robust Standard Errors for Linear Mixed\n  Effects Models in lme4", "comments": "Accepted at Journal of Statistical Software", "journal-ref": null, "doi": "10.18637/jss.v087.c01", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While robust standard errors and related facilities are available in R for\nmany types of statistical models, the facilities are notably lacking for models\nestimated via lme4. This is because the necessary statistical output, including\nthe Hessian and casewise gradient of random effect parameters, is not\nimmediately available from lme4 and is not trivial to obtain. In this article,\nwe supply and describe two new functions to obtain this output from Gaussian\nmixed models: estfun.lmerMod() and vcov.full.lmerMod(). We discuss the\ntheoretical results implemented in the code, focusing on calculation of robust\nstandard errors via package sandwich. We also use the Sleepstudy data to\nillustrate the code and compare it to a benchmark from package lavaan.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 02:56:46 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 19:16:35 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Wang", "Ting", ""], ["Merkle", "Edgar C.", ""]]}, {"id": "1612.04965", "submitter": "Matthieu Wilhelm", "authors": "Yves Till\\'e and Matthieu Wilhelm", "title": "Probability Sampling Designs: Principles for Choice of Design and\n  Balancing", "comments": "Accepted paper, Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is twofold. First, three theoretical principles are\nformalized: randomization, overrepresentation and restriction. We develop these\nprinciples and give a rationale for their use in choosing the sampling design\nin a systematic way. In the model-assisted framework, knowledge of the\npopulation is formalized by modelling the population and the sampling design is\nchosen accordingly. We show how the principles of overrepresentation and of\nrestriction naturally arise from the modelling of the population. The balanced\nsampling then appears as a consequence of the modelling. Second, a review of\nprobability balanced sampling is presented through the model-assisted\nframework. For some basic models, balanced sampling can be shown to be an\noptimal sampling design. Emphasis is placed on new spatial sampling methods and\ntheir related models. An illustrative example shows the advantages of the\ndifferent methods. Throughout the paper, various examples illustrate how the\nthree principles can be applied in order to improve inference.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 08:27:21 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Till\u00e9", "Yves", ""], ["Wilhelm", "Matthieu", ""]]}, {"id": "1612.04990", "submitter": "Olivier Scaillet", "authors": "Patrick Gagliardini, Elisa Ossola and Olivier Scaillet", "title": "A diagnostic criterion for approximate factor structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a simple diagnostic criterion for approximate factor structure in\nlarge cross-sectional equity datasets. Given a model for asset returns with\nobservable factors, the criterion checks whether the error terms are weakly\ncross-sectionally correlated or share at least one unobservable common factor.\nIt only requires computing the largest eigenvalue of the empirical\ncross-sectional covariance matrix of the residuals of a large unbalanced panel.\nA general version of this criterion allows us to determine the number of\nomitted common factors. The panel data model accommodates both time-invariant\nand time-varying factor structures. The theory applies to random coefficient\npanel models with interactive fixed effects under large cross-section and\ntime-series dimensions. The empirical analysis runs on monthly and quarterly\nreturns for about ten thousand US stocks from January 1968 to December 2011 for\nseveral time-invariant and time-varying specifications. For monthly returns, we\ncan choose either among time-invariant specifications with at least four\nfinancial factors, or a scaled three-factor specification. For quarterly\nreturns, we cannot select macroeconomic models without the market factor.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 09:22:05 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 13:18:25 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Gagliardini", "Patrick", ""], ["Ossola", "Elisa", ""], ["Scaillet", "Olivier", ""]]}, {"id": "1612.04996", "submitter": "Pramita Bagchi", "authors": "Pramita Bagchi, Vaidotas Characiejus, Holger Dette", "title": "A simple test for white noise in functional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new procedure for white noise testing of a functional time\nseries. Our approach is based on an explicit representation of the\n$L^2$-distance between the spectral density operator and its best\n($L^2$-)approximation by a spectral density operator corresponding to a white\nnoise process. The estimation of this distance can be easily accomplished by\nsums of periodogram kernels and it is shown that an appropriately standardized\nversion of the estimator is asymptotically normal distributed under the null\nhypothesis (of functional white noise) and under the alternative. As a\nconsequence we obtain a very simple test (using the quantiles of the normal\ndistribution) for the hypothesis of a white noise functional process. In\nparticular the test does neither require the estimation of a long run variance\n(including a fourth order cumulant) nor resampling procedures to calculate\ncritical values. Moreover, in contrast to all other methods proposed in the\nliterature our approach also allows to test for \"relevant\" deviations from\nwhite noise and to construct confidence intervals for a measure which measures\nthe discrepancy of the underlying process from a functional white noise\nprocess.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 09:50:20 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 11:18:00 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Bagchi", "Pramita", ""], ["Characiejus", "Vaidotas", ""], ["Dette", "Holger", ""]]}, {"id": "1612.05179", "submitter": "Colin Fogarty", "authors": "Colin B. Fogarty", "title": "Regression assisted inference for the average treatment effect in paired\n  experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In paired randomized experiments individuals in a given matched pair may\ndiffer on prognostically important covariates despite the best efforts of\npractitioners. We examine the use of regression adjustment as a way to correct\nfor persistent covariate imbalances after randomization, and present two\nregression assisted estimators for the sample average treatment effect in\npaired experiments. Using the potential outcomes framework, we prove that these\nestimators are consistent for the sample average treatment effect under mild\nregularity conditions even if the regression model is improperly specified.\nFurther, we describe how asymptotically conservative confidence intervals can\nbe constructed. We demonstrate that the variances of the regression assisted\nestimators are at least as small as that of the standard difference-in-means\nestimator asymptotically. Through a simulation study, we illustrate the\nappropriateness of the proposed methods in small and moderate samples. The\nanalysis does not require a superpopulation model, a constant treatment effect,\nor the truth of the regression model, and hence provides a mode of inference\nfor the sample average treatment effect with the potential to yield\nimprovements in the power of the resulting analysis over the classical analysis\nwithout imposing potentially unrealistic assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 18:17:34 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 01:28:15 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 20:08:13 GMT"}, {"version": "v4", "created": "Thu, 23 Nov 2017 01:17:25 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Fogarty", "Colin B.", ""]]}, {"id": "1612.05307", "submitter": "Philippe Gagnon", "authors": "Alain Desgagn\\'e and Philippe Gagnon", "title": "Bayesian Robustness to Outliers in Linear Regression and Ratio\n  Estimation", "comments": "To appear in Brazilian Journal of Probability and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole robustness is a nice property to have for statistical models. It\nimplies that the impact of outliers gradually vanishes as they approach plus or\nminus infinity. So far, the Bayesian literature provides results that ensure\nwhole robustness for the location-scale model. In this paper, we make two\ncontributions. First, we generalise the results to attain whole robustness in\nsimple linear regression through the origin, which is a necessary step towards\nresults for general linear regression models. We allow the variance of the\nerror term to depend on the explanatory variable. This flexibility leads to the\nsecond contribution: we provide a simple Bayesian approach to robustly estimate\nfinite population means and ratios. The strategy to attain whole robustness is\nsimple since it lies in replacing the traditional normal assumption on the\nerror term by a super heavy-tailed distribution assumption. As a result, users\ncan estimate the parameters as usual, using the posterior distribution.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 23:31:10 GMT"}, {"version": "v2", "created": "Sun, 12 Aug 2018 12:47:09 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Desgagn\u00e9", "Alain", ""], ["Gagnon", "Philippe", ""]]}, {"id": "1612.05426", "submitter": "Rose Baker", "authors": "Rose Baker", "title": "A new generalization of the beta distribution", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The beta distribution is the best-known distribution for modelling\ndoubly-bounded data, \\eg percentage data or probabilities. A new generalization\nof the beta distribution is proposed, which uses a cubic transformation of the\nbeta random variable. The new distribution is label-invariant like the beta\ndistribution and has rational expressions for the moments. This facilitates its\nuse in mean regression. The properties are discussed, and two examples of\nfitting to data are given. A modification is also explored in which the\nJacobian of the transformation is omitted. This gives rise to messier\nexpressions for the moments but better modal behaviour. In addition, the\nJacobian alone gives rise to a general quadratic distribution that is of\ninterest. The new distributions allow good fitting of unimodal data that fit\npoorly to the beta distribution, and could also be useful as prior\ndistributions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 10:55:56 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Baker", "Rose", ""]]}, {"id": "1612.05462", "submitter": "Michele Nguyen", "authors": "Michele Nguyen and Almut E. D. Veraart", "title": "A note on confidence intervals for parameter estimates of a\n  spatio-temporal Ornstein-Uhlenbeck process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare two ways of constructing confidence intervals for the\nmoments-matching parameter estimates of a Gaussian spatio-temporal\nOrnstein-Uhlenbeck process. It was found that those obtained via pairwise\nlikelihood approximations had lower coverages and were more prone to the curse\nof dimensionality as opposed to those from a parametric bootstrap procedure.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 13:36:29 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 15:56:41 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Nguyen", "Michele", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "1612.05653", "submitter": "Philippe Gagnon", "authors": "Philippe Gagnon and Myl\\`ene B\\'edard and Alain Desgagn\\'e", "title": "Weak Convergence and Optimal Tuning of the Reversible Jump Algorithm", "comments": null, "journal-ref": "Math. Comput. Simulation 161 (2019), pp. 32-51", "doi": "10.1016/j.matcom.2018.06.007", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reversible jump algorithm is a useful Markov chain Monte Carlo method\nintroduced by Green (1995) that allows switches between subspaces of differing\ndimensionality, and therefore, model selection. Although this method is now\nincreasingly used in key areas (e.g. biology and finance), it remains a\nchallenge to implement it. In this paper, we focus on a simple sampling context\nin order to obtain theoretical results that lead to an optimal tuning procedure\nfor the considered reversible jump algorithm, and consequently, to easy\nimplementation. The key result is the weak convergence of the sequence of\nstochastic processes engendered by the algorithm. It represents the main\ncontribution of this paper as it is, to our knowledge, the first weak\nconvergence result for the reversible jump algorithm. The sampler updating the\nparameters according to a random walk, this result allows to retrieve the\nwell-known 0.234 rule for finding the optimal scaling. It also leads to an\nanswer to the question: \"with what probability should a parameter update be\nproposed comparatively to a model switch at each iteration?\"\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 21:02:19 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 20:22:29 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 14:40:48 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Gagnon", "Philippe", ""], ["B\u00e9dard", "Myl\u00e8ne", ""], ["Desgagn\u00e9", "Alain", ""]]}, {"id": "1612.05800", "submitter": "Ander Wilson", "authors": "Ander Wilson, Yueh-Hsiu Mathilda Chiu, Hsiao-Hsien Leon Hsu, Robert O.\n  Wright, Rosalind J. Wright, Brent A. Coull", "title": "Bayesian Distributed Lag Interaction Models to Identify Perinatal\n  Windows of Vulnerability in Children's Health", "comments": null, "journal-ref": "Biostatistics 2007", "doi": "10.1093/biostatistics/kxx002", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemiological research supports an association between maternal exposure to\nair pollution during pregnancy and adverse children's health outcomes. Advances\nin exposure assessment and statistics allow for estimation of both critical\nwindows of vulnerability and exposure effect heterogeneity. Simultaneous\nestimation of windows of vulnerability and effect heterogeneity can be\naccomplished by fitting a distributed lag model (DLM) stratified by subgroup.\nHowever, this can provide an incomplete picture of how effects vary across\nsubgroups because it does not allow for subgroups to have the same window but\ndifferent within-window effects or to have different windows but the same\nwithin-window effect. Because the timing of some developmental processes are\ncommon across subpopulations of infants while for others the timing differs\nacross subgroups, both scenarios are important to consider when evaluating\nhealth risks of prenatal exposures. We propose a new approach that partitions\nthe DLM into a constrained functional predictor that estimates windows of\nvulnerability and a scalar effect representing the within-window effect\ndirectly. The proposed method allows for heterogeneity in only the window, only\nthe within-window effect, or both. In a simulation study we show that a model\nassuming a shared component across groups results in lower bias and mean\nsquared error for the estimated windows and effects when that component is in\nfact constant across groups. We apply the proposed method to estimate windows\nof vulnerability in the association between prenatal exposures to fine\nparticulate matter and each of birth weight and asthma incidence, and estimate\nhow these associations vary by sex and maternal obesity status, in a\nBoston-area prospective pre-birth cohort study.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 18:12:31 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Wilson", "Ander", ""], ["Chiu", "Yueh-Hsiu Mathilda", ""], ["Hsu", "Hsiao-Hsien Leon", ""], ["Wright", "Robert O.", ""], ["Wright", "Rosalind J.", ""], ["Coull", "Brent A.", ""]]}, {"id": "1612.05844", "submitter": "Bruce Desmarais", "authors": "Skyler J. Cranmer, Bruce A. Desmarais", "title": "What can we Learn from Predictive Modeling?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large majority of inferences drawn in empirical political research follow\nfrom model-based associations (e.g. regression). Here, we articulate the\nbenefits of predictive modeling as a complement to this approach. Predictive\nmodels aim to specify a probabilistic model that provides a good fit to testing\ndata that were not used to estimate the model's parameters. Our goals are\nthreefold. First, we review the central benefits of this under-utilized\napproach from a perspective uncommon in the existing literature: we focus on\nhow predictive modeling can be used to complement and augment standard\nassociational analyses. Second, we advance the state of the literature by\nlaying out a simple set of benchmark predictive criteria. Third, we illustrate\nour approach through a detailed application to the prediction of interstate\nconflict.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2016 01:51:47 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Cranmer", "Skyler J.", ""], ["Desmarais", "Bruce A.", ""]]}, {"id": "1612.06029", "submitter": "Yoshiyuki Ninomiya", "authors": "Yoshiyuki Ninomiya, Satoshi Kuriki, Toshihiko Shiroishi, Toyoyuki\n  Takada", "title": "Use of spurious correlation for multiplicity adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider one of the most basic multiple testing problems that compares\nexpectations of multivariate data among several groups. As a test statistic, a\nconventional (approximate) $t$-statistic is considered, and we determine its\nrejection region using a common rejection limit. When there are unknown\ncorrelations among test statistics, the multiplicity adjusted $p$-values are\ndependent on the unknown correlations. They are usually replaced with their\nestimates that are always consistent under any hypothesis. In this paper, we\npropose the use of estimates, which are not necessarily consistent and are\nreferred to as spurious correlations, in order to improve statistical power.\nThrough simulation studies, we verify that the proposed method asymptotically\ncontrols the family-wise error rate and clearly provides higher statistical\npower than existing methods. In addition, the proposed and existing methods are\napplied to a real multiple testing problem that compares quantitative traits\namong groups of mice and the results are compared.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 02:42:41 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Ninomiya", "Yoshiyuki", ""], ["Kuriki", "Satoshi", ""], ["Shiroishi", "Toshihiko", ""], ["Takada", "Toyoyuki", ""]]}, {"id": "1612.06040", "submitter": "Sonja Petrovic", "authors": "Vishesh Karwa, Debdeep Pati, Sonja Petrovi\\'c, Liam Solus, Nikita\n  Alexeev, Mateja Rai\\v{c}, Dane Wilburne, Robert Williams, Bowei Yan", "title": "Exact tests for stochastic block models", "comments": "30 pages, several figures, Discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a finite-sample goodness-of-fit test for \\emph{latent-variable}\nblock models for networks and test it on simulated and real data sets. The main\nbuilding block for the latent block assignment model test is the exact test for\nthe model with observed blocks assignment. The latter is implemented using\nalgebraic statistics. While we focus on three variants of the stochastic block\nmodel, the methodology extends to any mixture of log-linear models on discrete\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 04:05:57 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Karwa", "Vishesh", ""], ["Pati", "Debdeep", ""], ["Petrovi\u0107", "Sonja", ""], ["Solus", "Liam", ""], ["Alexeev", "Nikita", ""], ["Rai\u010d", "Mateja", ""], ["Wilburne", "Dane", ""], ["Williams", "Robert", ""], ["Yan", "Bowei", ""]]}, {"id": "1612.06045", "submitter": "Yang Ni", "authors": "Yang Ni, Peter Mueller, Yitan Zhu, Yuan Ji", "title": "Heterogeneous Reciprocal Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop novel hierarchical reciprocal graphical models to infer gene\nnetworks from heterogeneous data. In the case of data that can be naturally\ndivided into known groups, we propose to connect graphs by introducing a\nhierarchical prior across group-specific graphs, including a correlation on\nedge strengths across graphs. Thresholding priors are applied to induce\nsparsity of the estimated networks. In the case of unknown groups, we cluster\nsubjects into subpopulations and jointly estimate cluster-specific gene\nnetworks, again using similar hierarchical priors across clusters. We\nillustrate the proposed approach by simulation studies and two applications\nwith multiplatform genomic data for multiple cancers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 04:31:40 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 05:14:01 GMT"}, {"version": "v3", "created": "Sun, 21 Jan 2018 21:57:28 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Ni", "Yang", ""], ["Mueller", "Peter", ""], ["Zhu", "Yitan", ""], ["Ji", "Yuan", ""]]}, {"id": "1612.06118", "submitter": "Aurore Archimbaud", "authors": "Aurore Archimbaud, Klaus Nordhausen, Anne Ruiz-Gazen", "title": "ICS for Multivariate Outlier Detection with Application to Quality\n  Control", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, Volume 128, December\n  2018, Pages 184-199", "doi": "10.1016/j.csda.2018.06.011", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high reliability standards fields such as automotive, avionics or\naerospace, the detection of anomalies is crucial. An efficient methodology for\nautomatically detecting multivariate outliers is introduced. It takes advantage\nof the remarkable properties of the Invariant Coordinate Selection (ICS)\nmethod. Based on the simultaneous spectral decomposition of two scatter\nmatrices, ICS leads to an affine invariant coordinate system in which the\nEuclidian distance corresponds to a Mahalanobis Distance (MD) in the original\ncoordinates. The limitations of MD are highlighted using theoretical arguments\nin a context where the dimension of the data is large. Unlike MD, ICS makes it\npossible to select relevant components which removes the limitations. Owing to\nthe resulting dimension reduction, the method is expected to improve the power\nof outlier detection rules such as MD-based criteria. It also greatly\nsimplifies outliers interpretation. The paper includes practical guidelines for\nusing ICS in the context of a small proportion of outliers which is relevant in\nhigh reliability standards fields. The choice of scatter matrices together with\nthe selection of relevant invariant components through parallel analysis and\nnormality tests are addressed. The use of the regular covariance matrix and the\nso called matrix of fourth moments as the scatter pair is recommended. This\nchoice combines the simplicity of implementation together with the possibility\nto derive theoretical results. A simulation study confirms the good properties\nof the proposal and compares it with other scatter pairs. This study also\nprovides a comparison with Principal Component Analysis and MD. The performance\nof our proposal is also evaluated on several real data sets using a\nuser-friendly R package accompanying the paper.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 10:51:30 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 08:54:20 GMT"}, {"version": "v3", "created": "Fri, 2 Feb 2018 20:51:23 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Archimbaud", "Aurore", ""], ["Nordhausen", "Klaus", ""], ["Ruiz-Gazen", "Anne", ""]]}, {"id": "1612.06127", "submitter": "Benedikt M. P\\\"otscher", "authors": "Benedikt M. P\\\"otscher and David Preinerstorfer", "title": "Controlling the Size of Autocorrelation Robust Tests", "comments": "Minor changes including correction of a minor error", "journal-ref": "Journal of Econometrics 207 (2018), 406-431", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autocorrelation robust tests are notorious for suffering from size\ndistortions and power problems. We investigate under which conditions the size\nof autocorrelation robust tests can be controlled by an appropriate choice of\ncritical value.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 11:21:09 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 13:30:37 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 13:56:25 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""], ["Preinerstorfer", "David", ""]]}, {"id": "1612.06131", "submitter": "Stefano Martiniani", "authors": "Daan Frenkel, K. Julian Schrenk, Stefano Martiniani", "title": "Monte Carlo sampling for stochastic weight functions", "comments": "7 pages, 4 figures", "journal-ref": "Proceedings of the National Academy of Sciences, 114(27),\n  6924-6929 (2017)", "doi": "10.1073/pnas.1620497114", "report-no": null, "categories": "cond-mat.stat-mech physics.comp-ph stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional Monte Carlo simulations are stochastic in the sense that the\nacceptance of a trial move is decided by comparing a computed acceptance\nprobability with a random number, uniformly distributed between 0 and 1. Here\nwe consider the case that the weight determining the acceptance probability\nitself is fluctuating. This situation is common in many numerical studies. We\nshow that it is possible to construct a rigorous Monte Carlo algorithm that\nvisits points in state space with a probability proportional to their average\nweight. The same approach has the potential to transform the methodology of a\ncertain class of high-throughput experiments or the analysis of noisy datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 11:32:01 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Frenkel", "Daan", ""], ["Schrenk", "K. Julian", ""], ["Martiniani", "Stefano", ""]]}, {"id": "1612.06146", "submitter": "Ivi Tsantili C", "authors": "Dionissios T. Hristopulos and Ivi C. Tsantili", "title": "Space-Time Covariance Functions based on Linear Response Theory and the\n  Turning Bands Method", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of non-separable, physically motivated covariance functions is\na theme of ongoing research interest, given that only a few classes of such\nfunctions are available. We construct a non-separable space-time covariance\nfunction based on a diffusive Langevin equation. We employ ideas from\nstatistical mechanics to express the response of an equilibrium (i.e., time\nindependent) random field to a driving noise process by means of a linear,\ndiffusive relaxation mechanism. The equilibrium field is assumed to follow an\nexponential joint probability density which is determined by a spatial local\ninteraction model. We then use linear response theory to express the temporal\nevolution of the random field around the equilibrium state in terms of a\nLangevin equation. The latter yields an equation of motion for the space-time\ncovariance function, which can be solved explicitly at certain limits. We use\nthe explicit covariance model obtained in one spatial dimension and time. By\nmeans of the turning bands transform, we derive a non-separable space-time\ncovariance function in three space dimensions and time. We investigate the\nmathematical properties of this space-time covariance function, and we use it\nto model a dataset of daily ozone concentration values from the conterminous\nUSA.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 12:01:09 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Hristopulos", "Dionissios T.", ""], ["Tsantili", "Ivi C.", ""]]}, {"id": "1612.06198", "submitter": "Philippe Gagnon", "authors": "Philippe Gagnon, Alain Desgagn\\'e and Myl\\`ene B\\'edard", "title": "A New Bayesian Approach to Robustness Against Outliers in Linear\n  Regression", "comments": "To appear in Bayesian Analysis", "journal-ref": null, "doi": "10.1214/19-BA1157", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression is ubiquitous in statistical analysis. It is well\nunderstood that conflicting sources of information may contaminate the\ninference when the classical normality of errors is assumed. The contamination\ncaused by the light normal tails follows from an undesirable effect: the\nposterior concentrates in an area in between the different sources with a large\nenough scaling to incorporate them all. The theory of conflict resolution in\nBayesian statistics (O'Hagan and Pericchi (2012)) recommends to address this\nproblem by limiting the impact of outliers to obtain conclusions consistent\nwith the bulk of the data. In this paper, we propose a model with super\nheavy-tailed errors to achieve this. We prove that it is wholly robust, meaning\nthat the impact of outliers gradually vanishes as they move further and further\naway form the general trend. The super heavy-tailed density is similar to the\nnormal outside of the tails, which gives rise to an efficient estimation\nprocedure. In addition, estimates are easily computed. This is highlighted via\na detailed user guide, where all steps are explained through a simulated case\nstudy. The performance is shown using simulation. All required code is given.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 14:26:53 GMT"}, {"version": "v2", "created": "Sun, 24 Dec 2017 16:22:30 GMT"}, {"version": "v3", "created": "Thu, 18 Oct 2018 16:41:01 GMT"}, {"version": "v4", "created": "Tue, 11 Jun 2019 20:40:25 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Gagnon", "Philippe", ""], ["Desgagn\u00e9", "Alain", ""], ["B\u00e9dard", "Myl\u00e8ne", ""]]}, {"id": "1612.06303", "submitter": "Joshua Hewitt", "authors": "Joshua Hewitt, Jennifer A. Hoeting, James Done and Erin Towler", "title": "Remote effects spatial process models for modeling teleconnections", "comments": "34 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most spatial data can be modeled with the assumption that distant\npoints are uncorrelated, some problems require dependence at both far and short\ndistances. We introduce a model to directly incorporate dependence in phenomena\nthat influence a distant response. Spatial climate problems often have such\nmodeling needs as data are influenced by local factors in addition to remote\nphenomena, known as teleconnections. Teleconnections arise from complex\ninteractions between the atmosphere and ocean, of which the El Nino--Southern\nOscillation teleconnection is a well-known example. Our model extends the\nstandard geostatistical modeling framework to account for effects of covariates\nobserved on a spatially remote domain. We frame our model as an extension of\nspatially varying coefficient models. Connections to existing methods are\nhighlighted and further modeling needs are addressed by additionally drawing on\nspatial basis functions and predictive processes. Notably, our approach allows\nusers to model teleconnected data without pre-specifying teleconnection\nindices, which other methods often require. We adopt a hierarchical Bayesian\nframework to conduct inference and make predictions. The method is demonstrated\nby predicting precipitation in Colorado while accounting for local factors and\nteleconnection effects with Pacific Ocean sea surface temperatures. We show how\nthe proposed model improves upon standard methods for estimating teleconnection\neffects and discuss its utility for climate applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 18:39:21 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 19:48:25 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 23:51:35 GMT"}, {"version": "v4", "created": "Mon, 2 Jul 2018 20:07:39 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Hewitt", "Joshua", ""], ["Hoeting", "Jennifer A.", ""], ["Done", "James", ""], ["Towler", "Erin", ""]]}, {"id": "1612.06304", "submitter": "Mohammad Arashi", "authors": "B. Yuzbasi and M. Arashi", "title": "Double shrunken selection operator", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The least absolute shrinkage and selection operator (LASSO) of Tibshirani\n(1996) is a prominent estimator which selects significant (under some sense)\nfeatures and kills insignificant ones. Indeed the LASSO shrinks features lager\nthan a noise level to zero. In this paper, we force LASSO to be shrunken more\nby proposing a Stein-type shrinkage estimator emanating from the LASSO, namely\nthe Stein-type LASSO. The newly proposed estimator proposes good performance in\nrisk sense numerically. Variants of this estimator have smaller relative MSE\nand prediction error, compared to the LASSO, in the analysis of prostate cancer\ndata set.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 18:42:21 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Yuzbasi", "B.", ""], ["Arashi", "M.", ""]]}, {"id": "1612.06404", "submitter": "Benjamin Bloem-Reddy", "authors": "Benjamin Bloem-Reddy and Peter Orbanz", "title": "Random Walk Models of Network Formation and Sequential Monte Carlo\n  Methods for Graphs", "comments": null, "journal-ref": null, "doi": "10.1111/rssb.12289", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of generative network models that insert edges by\nconnecting the starting and terminal vertices of a random walk on the network\ngraph. Within the taxonomy of statistical network models, this class is\ndistinguished by permitting the location of a new edge to explicitly depend on\nthe structure of the graph, but being nonetheless statistically and\ncomputationally tractable. In the limit of infinite walk length, the model\nconverges to an extension of the preferential attachment model---in this sense,\nit can be motivated alternatively by asking what preferential attachment is an\napproximation to. Theoretical properties, including the limiting degree\nsequence, are studied analytically. If the entire history of the graph is\nobserved, parameters can be estimated by maximum likelihood. If only the final\ngraph is available, its history can be imputed using MCMC. We develop a class\nof sequential Monte Carlo algorithms that are more generally applicable to\nsequential network models, and may be of interest in their own right. The model\nparameters can be recovered from a single graph generated by the model.\nApplications to data clarify the role of the random walk length as a length\nscale of interactions within the graph.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 21:01:36 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 20:23:57 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Bloem-Reddy", "Benjamin", ""], ["Orbanz", "Peter", ""]]}, {"id": "1612.06468", "submitter": "Richard Everitt", "authors": "Richard G Everitt and Richard Culliford and Felipe Medina-Aguayo and\n  Daniel J Wilson", "title": "Sequential Monte Carlo with transformations", "comments": null, "journal-ref": "Statistics and Computing, 2020", "doi": "10.1007/s11222-019-09903-y", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces methodology for performing Bayesian inference\nsequentially on a sequence of posteriors on spaces of different dimensions. We\nshow how this may be achieved through the use of sequential Monte Carlo (SMC)\nsamplers (Del Moral et al., 2006, 2007), making use of the full flexibility of\nthis framework in order that the method is computationally efficient. In\nparticular, we introduce the innovation of using deterministic transformations\nto move particles effectively between target distributions with different\ndimensions. This approach, combined with adaptive methods, yields an extremely\nflexible and general algorithm for Bayesian model comparison that is suitable\nfor use in applications where the acceptance rate in reversible jump Markov\nchain Monte Carlo (RJMCMC) is low. We demonstrate this approach on the\nwell-studied problem of model comparison for mixture models, and for the novel\napplication of inferring coalescent trees sequentially, as data arrives.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 00:57:02 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 15:09:36 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 17:47:01 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Everitt", "Richard G", ""], ["Culliford", "Richard", ""], ["Medina-Aguayo", "Felipe", ""], ["Wilson", "Daniel J", ""]]}, {"id": "1612.06547", "submitter": "Vivian Viallon", "authors": "Vivian Viallon and Marine Dufournet", "title": "Can collider bias fully explain the obesity paradox?", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"obesity paradox\" has been reported in several observational studies,\nwhere obesity was shown to be associated to a decreased mortality in\nindividuals suffering from a chronic disease, such as diabetes or heart\nfailure. Causal arguments have recently been given to explain this apparently\nparadoxical fact: because the chronic disease is caused by obesity, the\nobserved \"protective effect\" of obesity among patients with, say, diabetes,\nactually has no causal value. Recently, Sperrin et al. (2016) relaunched the\ndebate and claimed that the resulting bias, the so-called collider bias, was\nunlikely to be the main explanation for the obesity paradox. However, a number\nof issues in their work make their conclusions questionable. In this article,\nwe first study the bias between (i) the association between obesity and early\ndeath among patients suffering from the chronic disease $\\Delta_{AS}$ and (ii)\nthe causal effect considered by Sperrin et al. Under the usual framework of\nstructural causal models, we explain why this bias can be much higher than what\nthese authors reported. We further consider alternative causal effects of\npotential interest and study their difference with $\\Delta_{AS}$. Numerical\nexamples are presented to illustrate the magnitude of these differences under\nrealistic scenarios. We show that it is possible to have a negative\n$\\Delta_{AS}$, while the causal effects we considered are all positive.\nTherefore, even under the very simple generative model we considered, collider\nbias can be the sole cause of the obesity paradox.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 08:34:46 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Viallon", "Vivian", ""], ["Dufournet", "Marine", ""]]}, {"id": "1612.06601", "submitter": "Bruno Ebner", "authors": "Bruno Ebner and Norbert Henze and Joseph E. Yukich", "title": "Multivariate goodness-of-fit on flat and curved spaces via nearest\n  neighbor distances", "comments": "22 pages, 1 figure, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified approach to goodness-of-fit testing in $\\mathbb{R}^d$\nand on lower-dimensional manifolds embedded in $\\mathbb{R}^d$ based on sums of\npowers of weighted volumes of $k$-th nearest neighbor spheres. We prove\nasymptotic normality of a class of test statistics under the null hypothesis\nand under fixed alternatives. Under such alternatives, scaled versions of the\ntest statistics converge to the $\\alpha$-entropy between probability\ndistributions. A simulation study shows that the procedures are serious\ncompetitors to established goodness-of-fit tests.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 10:55:51 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Ebner", "Bruno", ""], ["Henze", "Norbert", ""], ["Yukich", "Joseph E.", ""]]}, {"id": "1612.06850", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Iv\\'an Fern\\'andez-Val, and Tetsuya Kaji", "title": "Extremal Quantile Regression: An Overview", "comments": "32 pages, 4 tables, 7 figures, forthcoming in the Handbook of\n  Quantile Regression", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extremal quantile regression, i.e. quantile regression applied to the tails\nof the conditional distribution, counts with an increasing number of economic\nand financial applications such as value-at-risk, production frontiers,\ndeterminants of low infant birth weights, and auction models. This chapter\nprovides an overview of recent developments in the theory and empirics of\nextremal quantile regression. The advances in the theory have relied on the use\nof extreme value approximations to the law of the Koenker and Bassett (1978)\nquantile regression estimator. Extreme value laws not only have been shown to\nprovide more accurate approximations than Gaussian laws at the tails, but also\nhave served as the basis to develop bias corrected estimators and inference\nmethods using simulation and suitable variations of bootstrap and subsampling.\nThe applicability of these methods is illustrated with two empirical examples\non conditional value-at-risk and financial contagion.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 20:55:07 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 16:49:01 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Kaji", "Tetsuya", ""]]}, {"id": "1612.06879", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Robust mixture of experts modeling using the skew $t$ distribution", "comments": "arXiv admin note: substantial text overlap with arXiv:1506.06707", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of Experts (MoE) is a popular framework in the fields of statistics\nand machine learning for modeling heterogeneity in data for regression,\nclassification and clustering. MoE for continuous data are usually based on the\nnormal distribution. However, it is known that for data with asymmetric\nbehavior, heavy tails and atypical observations, the use of the normal\ndistribution is unsuitable. We introduce a new robust non-normal mixture of\nexperts modeling using the skew $t$ distribution. The proposed skew $t$ mixture\nof experts, named STMoE, handles these issues of the normal mixtures experts\nregarding possibly skewed, heavy-tailed and noisy data. We develop a dedicated\nexpectation conditional maximization (ECM) algorithm to estimate the model\nparameters by monotonically maximizing the observed data log-likelihood. We\ndescribe how the presented model can be used in prediction and in model-based\nclustering of regression data. Numerical experiments carried out on simulated\ndata show the effectiveness and the robustness of the proposed model in fitting\nnon-linear regression functions as well as in model-based clustering. Then, the\nproposed model is applied to the real-world data of tone perception for musical\ndata analysis, and the one of temperature anomalies for the analysis of climate\nchange data. The obtained results confirm the usefulness of the model for\npractical data analysis applications.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 19:25:27 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1612.06928", "submitter": "Haeran Cho Dr", "authors": "Matteo Barigozzi, Haeran Cho, Piotr Fryzlewicz", "title": "Simultaneous multiple change-point and factor analysis for\n  high-dimensional time series", "comments": "64 pages, to appear in the Journal of Econometrics", "journal-ref": null, "doi": "10.1016/j.jeconom.2018.05.003", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first comprehensive treatment of high-dimensional time series\nfactor models with multiple change-points in their second-order structure. We\noperate under the most flexible definition of piecewise stationarity, and\nestimate the number and locations of change-points consistently as well as\nidentifying whether they originate in the common or idiosyncratic components.\nThrough the use of wavelets, we transform the problem of change-point detection\nin the second-order structure of a high-dimensional time series, into the\n(relatively easier) problem of change-point detection in the means of\nhigh-dimensional panel data. Also, our methodology circumvents the difficult\nissue of the accurate estimation of the true number of factors in the presence\nof multiple change-points by adopting a screening procedure. We further show\nthat consistent factor analysis is achieved over each segment defined by the\nchange-points estimated by the proposed methodology. In extensive simulation\nstudies, we observe that factor analysis prior to change-point detection\nimproves the detectability of change-points, and identify and describe an\ninteresting `spillover' effect in which substantial breaks in the idiosyncratic\ncomponents get, naturally enough, identified as change-points in the common\ncomponents, which prompts us to regard the corresponding change-points as also\nacting as a form of `factors'. Our methodology is implemented in the R package\n{\\tt factorcpt}, available from CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 00:30:29 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 14:55:40 GMT"}, {"version": "v3", "created": "Fri, 2 Feb 2018 13:02:20 GMT"}, {"version": "v4", "created": "Tue, 29 May 2018 22:42:11 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Barigozzi", "Matteo", ""], ["Cho", "Haeran", ""], ["Fryzlewicz", "Piotr", ""]]}, {"id": "1612.06968", "submitter": "Yan Li", "authors": "Yan Li, Yang Li, Yichen Qin, Jun Yan", "title": "Copula Modeling for Data with Ties", "comments": null, "journal-ref": null, "doi": "10.4310/SII.2020.v13.n1.a9", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copula modeling has gained much attention in many fields recently with the\nadvantage of separating dependence structure from marginal distributions. In\nreal data, however, serious ties are often present in one or multiple margins,\nwhich cause problems to many rank-based statistical methods developed under the\nassumption of continuous data with no ties. Simple methods such as breaking the\nties at random or using average rank introduce independence into the data and,\nhence, lead to biased estimation. We propose an estimation method that treats\nthe ranks of tied data as being interval censored and maximizes a\npseudo-likelihood based on interval censored pseudo-observations. A parametric\nbootstrap procedure that preserves the observed tied ranks in the data is\nadapted to assess the estimation uncertainty and perform goodness-of-fit tests.\nThe proposed approach is shown to be very competitive in comparison to the\nsimple treatments in a large scale simulation study. Application to a bivariate\ninsurance data illustrates the methodology.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 04:16:38 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Li", "Yan", ""], ["Li", "Yang", ""], ["Qin", "Yichen", ""], ["Yan", "Jun", ""]]}, {"id": "1612.07010", "submitter": "Kari Krizak Halle", "authors": "Kari Krizak Halle and Mette Langaas", "title": "Permutation in genetic association studies with covariates: controlling\n  the familywise error rate with score tests in generalized linear models", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genome-wide association (GWA) studies the goal is to detect associations\nbetween genetic markers and a given phenotype. The number of genetic markers\ncan be large and effective methods for control of the overall error rate is a\ncentral topic when analyzing GWA data. The Bonferroni method is known to be\nconservative when the tests are dependent. Permutation methods give exact\ncontrol of the overall error rate when the assumption of exchangeability is\nsatisfied, but are computationally intensive for large datasets. For regression\nmodels the exchangeability assumption is in general not satisfied and there is\nno standard solution on how to do permutation testing, except some approximate\nmethods. In this paper we will discuss permutation methods for control of the\nfamilywise error rate in genetic association studies and present an approximate\nsolution. These methods will be compared using simulated data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 08:32:34 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 07:32:39 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Halle", "Kari Krizak", ""], ["Langaas", "Mette", ""]]}, {"id": "1612.07034", "submitter": "Anders Eklund", "authors": "Bertil Wegmann, Anders Eklund, Mattias Villani", "title": "Bayesian Non-Central Chi Regression For Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a regression model for non-central $\\chi$ (NC-$\\chi$) distributed\nfunctional magnetic resonance imaging (fMRI) and diffusion weighted imaging\n(DWI) data, with the heteroscedastic Rician regression model as a prominent\nspecial case. The model allows both parameters in the NC-$\\chi$ distribution to\nbe linked to explanatory variables, with the relevant covariates automatically\nchosen by Bayesian variable selection. A highly efficient Markov chain Monte\nCarlo (MCMC) algorithm is proposed for simulating from the joint Bayesian\nposterior distribution of all model parameters and the binary covariate\nselection indicators. Simulated fMRI data is used to demonstrate that the\nRician model is able to localize brain activity much more accurately than the\ntraditionally used Gaussian model at low signal-to-noise ratios. Using a\ndiffusion dataset from the Human Connectome Project, it is also shown that the\ncommonly used approximate Gaussian noise model underestimates the mean\ndiffusivity (MD) and the fractional anisotropy (FA) in the single-diffusion\ntensor model compared to the theoretically correct Rician model.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 09:52:13 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Wegmann", "Bertil", ""], ["Eklund", "Anders", ""], ["Villani", "Mattias", ""]]}, {"id": "1612.07072", "submitter": "Minh-Ngoc Tran", "authors": "P. Choppala, D. Gunawan, J. Chen, M.-N. Tran, R. Kohn", "title": "Bayesian Inference for State Space Models using Block and Correlated\n  Pseudo Marginal Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the problem of efficient Bayesian inference in dynamic\nsystems using particle methods and makes a number of contributions. First, we\ndevelop a correlated pseudo-marginal (CPM) approach for Bayesian inference in\nstate space (SS) models that is based on filtering the disturbances, rather\nthan the states. This approach is useful when the state transition density is\nintractable or inefficient to compute, and also when the dimension of the\ndisturbance is lower than the dimension of the state. Second, we propose a\nblock pseudo-marginal (BPM) method that uses as the estimate of the likelihood\nthe average of G independent unbiased estimates of the likelihood. We associate\na set of underlying uniform of standard normal random numbers used to construct\neach of the individual unbiased likelihood estimates and then use\ncomponent-wise Markov Chain Monte Carlo to update the parameter vector jointly\nwith one set of these random numbers at a time. This induces a correlation of\napproximately 1-1/G between the logs of the estimated likelihood at the\nproposed and current values of the model parameters. Third, we show for some\nnon-stationary state space models that the BPM approach is much more efficient\nthan the CPM approach, because it is difficult to translate the high\ncorrelation in the underlying random numbers to high correlation between the\nlogs of the likelihood estimates. Although our focus has been on applying the\nBPM method to state space models, our results and approach can be used in a\nwide range of applications of the PM method, such as panel data models,\nsubsampling problems and approximate Bayesian computation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 12:11:46 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Choppala", "P.", ""], ["Gunawan", "D.", ""], ["Chen", "J.", ""], ["Tran", "M. -N.", ""], ["Kohn", "R.", ""]]}, {"id": "1612.07190", "submitter": "Daniel Cooley", "authors": "Daniel Cooley and Emeric Thibaud", "title": "Decompositions of Dependence for High-Dimensional Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Employing the framework of regular variation, we propose two decompositions\nwhich help to summarize and describel high-dimensional tail dependence. Via\ntransformation, we define a vector space on the positive orthant, yielding the\nnotion of basis. With a suitably-chosen transformation, we show that\ntransformed-linear operations applied to regularly varying random vectors\npreserve regular variation. Rather than model regular-variation's angular\nmeasure, we summarize tail dependence via a matrix of pairwise tail dependence\nmetrics. This matrix is positive semidefinite, and eigendecomposition allows\none to interpret tail dependence via the resulting eigenbasis. Additionally\nthis matrix is completely positive, and a resulting decomposition allows one to\neasily construct regularly varying random vectors which share the same pairwise\ntail dependencies. We illustrate our methods with Swiss rainfall data and\nfinancial return data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 21:32:32 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 15:48:14 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 20:46:24 GMT"}, {"version": "v4", "created": "Thu, 26 Apr 2018 02:22:49 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Cooley", "Daniel", ""], ["Thibaud", "Emeric", ""]]}, {"id": "1612.07216", "submitter": "Housen Li", "authors": "Housen Li, Axel Munk, Hannes Sieling, Guenther Walther", "title": "The Essential Histogram", "comments": "Extension to discrete data is included. A R-package \"essHist\" is\n  available from https://CRAN.R-project.org/package=essHist", "journal-ref": "Biometrika, 2020", "doi": "10.1093/biomet/asz081", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The histogram is widely used as a simple, exploratory display of data, but it\nis usually not clear how to choose the number and size of bins. We construct a\nconfidence set of distribution functions that optimally address the two main\ntasks of the histogram: estimating probabilities and detecting features such as\nincreases and modes in the distribution. We define the essential histogram as\nthe histogram in the confidence set with the fewest bins. Thus the essential\nhistogram is the simplest visualization of the data that optimally achieves the\nmain tasks of the histogram. The only assumption we make is that the data are\nindependent and identically distributed. We provide a fast algorithm for the\nessential histogram, and illustrate our methodology with examples. An R-package\nis available on CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 16:14:11 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 22:13:31 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 08:38:10 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Li", "Housen", ""], ["Munk", "Axel", ""], ["Sieling", "Hannes", ""], ["Walther", "Guenther", ""]]}, {"id": "1612.07222", "submitter": "Kevin Jiao", "authors": "Xi Chen, Kevin Jiao, Qihang Lin", "title": "Bayesian Decision Process for Cost-Efficient Dynamic Ranking via\n  Crowdsourcing", "comments": null, "journal-ref": "Journal of Machine Learning Research 17 (2016) 1-40", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank aggregation based on pairwise comparisons over a set of items has a wide\nrange of applications. Although considerable research has been devoted to the\ndevelopment of rank aggregation algorithms, one basic question is how to\nefficiently collect a large amount of high-quality pairwise comparisons for the\nranking purpose. Because of the advent of many crowdsourcing services, a crowd\nof workers are often hired to conduct pairwise comparisons with a small\nmonetary reward for each pair they compare. Since different workers have\ndifferent levels of reliability and different pairs have different levels of\nambiguity, it is desirable to wisely allocate the limited budget for\ncomparisons among the pairs of items and workers so that the global ranking can\nbe accurately inferred from the comparison results. To this end, we model the\nactive sampling problem in crowdsourced ranking as a Bayesian Markov decision\nprocess, which dynamically selects item pairs and workers to improve the\nranking accuracy under a budget constraint. We further develop a\ncomputationally efficient sampling policy based on knowledge gradient as well\nas a moment matching technique for posterior approximation. Experimental\nevaluations on both synthetic and real data show that the proposed policy\nachieves high ranking accuracy with a lower labeling cost.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 16:24:27 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Chen", "Xi", ""], ["Jiao", "Kevin", ""], ["Lin", "Qihang", ""]]}, {"id": "1612.07463", "submitter": "Martin Seilmayer", "authors": "Martin Seilmayer, Matthias Ratajczak", "title": "A guide on spectral methods applied to discrete data -- Part I:\n  One-dimensional signals", "comments": null, "journal-ref": null, "doi": "10.1155/2017/5108946", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral analysis in conjunction with discrete data in one and more\ndimensions can become a challenging task, because the methods are sometimes\ndifficult to understand. This paper intends to provide an overview about the\nusage of the Fourier transform, its related methods and focuses on the\nsubtleties to which the users must pay attention. Typical questions, which are\noften addressed to the data, will be discussed. Such a problem can be the issue\nof frequency or band limitation of the signal. Or the source of artifacts might\nbe of interest, when a Fourier transform is carried out. Another topic is the\nissue with fragmented data. Here, the Lomb-Scargle method will be explained\nwith an illustrative example to deal with this special type of signal.\nFurthermore, a challenge encountered very often is the time-dependent spectral\nanalysis, with which one can evaluate the point in time when a certain\nfrequency appears in the signal. The information to solve such problems and to\nanswer this questions is spread over many disciplines ranging from mathematics,\nelectrical engineering, economic science to astrophysics. The goal of the first\npart of this paper is to collect the important information about the common\nmethods to give the reader a guide on how to use these for application on\none-dimensional data. The second part of this paper will then address the two-\nand more-dimensional data. The introduced methods are supported by the spectral\npackage, which has been published for the statistical environment R prior this\narticle.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 07:00:04 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Seilmayer", "Martin", ""], ["Ratajczak", "Matthias", ""]]}, {"id": "1612.07471", "submitter": "Alain Durmus", "authors": "Alain Durmus, Eric Moulines, Marcelo Pereyra", "title": "Efficient Bayesian computation by proximal Markov chain Monte Carlo:\n  when Langevin meets Moreau", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern imaging methods rely strongly on Bayesian inference techniques to\nsolve challenging imaging problems. Currently, the predominant Bayesian\ncomputation approach is convex optimisation, which scales very efficiently to\nhigh dimensional image models and delivers accurate point estimation results.\nHowever, in order to perform more complex analyses, for example image\nuncertainty quantification or model selection, it is necessary to use more\ncomputationally intensive Bayesian computation techniques such as Markov chain\nMonte Carlo methods. This paper presents a new and highly efficient Markov\nchain Monte Carlo methodology to perform Bayesian computation for high\ndimensional models that are log-concave and non-smooth, a class of models that\nis central in imaging sciences. The methodology is based on a regularised\nunadjusted Langevin algorithm that exploits tools from convex analysis, namely\nMoreau-Yoshida envelopes and proximal operators, to construct Markov chains\nwith favourable convergence properties. In addition to scaling efficiently to\nhigh dimensions, the method is straightforward to apply to models that are\ncurrently solved by using proximal optimisation algorithms. We provide a\ndetailed theoretical analysis of the proposed methodology, including asymptotic\nand non-asymptotic convergence results with easily verifiable conditions, and\nexplicit bounds on the convergence rates. The proposed methodology is\ndemonstrated with four experiments related to image deconvolution and\ntomographic reconstruction with total-variation and $\\ell_1$ priors, where we\nconduct a range of challenging Bayesian analyses related to uncertainty\nquantification, hypothesis testing, and model selection in the absence of\nground truth.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 07:31:59 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Durmus", "Alain", ""], ["Moulines", "Eric", ""], ["Pereyra", "Marcelo", ""]]}, {"id": "1612.07497", "submitter": "B{\\l}a\\.zej Miasojedow", "authors": "B{\\l}a\\.zej Miasojedow, Wojciech Rejchel", "title": "Sparse estimation in Ising Model via penalized Monte Carlo methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of model selection in high-dimensional binary Markov\nrandom fields. The usefulness of the Ising model in studying systems of complex\ninteractions has been confirmed in many papers. The main drawback of this model\nis the intractable norming constant that makes estimation of parameters very\nchallenging. In the paper we propose a Lasso penalized version of the Monte\nCarlo maximum likelihood method. We prove that our algorithm, under mild\nregularity conditions, recognizes the true dependence structure of the graph\nwith high probability. The efficiency of the proposed method is also\ninvestigated via simulation studies.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 09:16:24 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 10:25:54 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Miasojedow", "B\u0142a\u017cej", ""], ["Rejchel", "Wojciech", ""]]}, {"id": "1612.07498", "submitter": "Heidi Seibold", "authors": "Heidi Seibold, Torsten Hothorn, Achim Zeileis", "title": "Generalised Linear Model Trees with Global Additive Effects", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-018-0342-1", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based trees are used to find subgroups in data which differ with\nrespect to model parameters. In some applications it is natural to keep some\nparameters fixed globally for all observations while asking if and how other\nparameters vary across subgroups. Existing implementations of model-based trees\ncan only deal with the scenario where all parameters depend on the subgroups.\nWe propose partially additive linear model trees (PALM trees) as an extension\nof (generalised) linear model trees (LM and GLM trees, respectively), in which\nthe model parameters are specified a priori to be estimated either globally\nfrom all observations or locally from the observations within the subgroups\ndetermined by the tree. Simulations show that the method has high power for\ndetecting subgroups in the presence of global effects and reliably recovers the\ntrue parameters. Furthermore, treatment-subgroup differences are detected in an\nempirical application of the method to data from a mathematics exam: the PALM\ntree is able to detect a small subgroup of students that had a disadvantage in\nan exam with two versions while adjusting for overall ability effects.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 09:19:02 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 08:24:03 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Seibold", "Heidi", ""], ["Hothorn", "Torsten", ""], ["Zeileis", "Achim", ""]]}, {"id": "1612.07561", "submitter": "Robin Ristl", "authors": "Robin Ristl, Dong Xi, Ekkehard Glimm, Martin Posch", "title": "Optimal exact tests for multiple binary endpoints", "comments": null, "journal-ref": "Computational Statistics and Data Analysis 122 (2018) 1-17", "doi": "10.1016/j.csda.2018.01.001", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In confirmatory clinical trials with small sample sizes, hypothesis tests\nbased on asymptotic distributions are often not valid and exact non-parametric\nprocedures are applied instead. However, the latter are based on discrete test\nstatistics and can become very conservative, even more so, if adjustments for\nmultiple testing as the Bonferroni correction are applied. We propose improved\nexact multiple testing procedures for the setting where two parallel groups are\ncompared in multiple binary endpoints. Based on the joint conditional\ndistribution of test statistics of Fisher's exact tests, optimal rejection\nregions for intersection hypotheses tests are constructed. To efficiently\nsearch the large space of possible rejection regions, we propose an\noptimization algorithm based on constrained optimization and integer linear\nprogramming. Depending on the optimization objective, the optimal test yields\nmaximal power under a specific alternative, maximal exhaustion of the nominal\ntype I error rate, or the largest possible rejection region controlling the\ntype I error rate. Applying the closed testing principle, we construct\noptimized multiple testing procedures with strong familywise error rate\ncontrol. Furthermore, we propose a greedy algorithm for nearly optimal tests,\nwhich is computationally more efficient. We numerically compare the\nunconditional power of the optimized procedure with alternative approaches and\nillustrate the optimal tests with a clinical trial example in a rare disease.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 12:05:09 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Ristl", "Robin", ""], ["Xi", "Dong", ""], ["Glimm", "Ekkehard", ""], ["Posch", "Martin", ""]]}, {"id": "1612.07811", "submitter": "Jelena Markovic", "authors": "Jelena Markovic and Jonathan Taylor", "title": "Bootstrap inference after using multiple queries for model selection", "comments": "58 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we provide a refinement of the selective CLT result of Tian and\nTaylor (2015), which allows for selective inference in non-parametric settings\nby adjusting for the asymptotic Gaussian limit for selection. Under some\nregularity assumptions on the density of the randomization, including heavier\ntails than Gaussian satisfied by e.g. logistic distribution, we prove the\nselective CLT holds without any assumptions on the underlying parameter,\nallowing for rare selection events. We also show a selective CLT result for\nGaussian randomization, though the quantitative results are qualitatively\ndifferent for the Gaussian randomization as compared to the heavier tailed\nresults. Furthermore, we propose a bootstrap version of this test statistic,\nwhich is provably asymptotically pivotal uniformly across a family of\nnon-parametric distributions. This result can be interpreted as resolving the\nimpossibility results of Leeb and Potscher (2006). We describe several sampling\nmethods involving the projected Langevin Monte Carlo to compute the\nbootstrapped test statistic and the corresponding confidence intervals valid\nafter selection. The applications of our work include valid inferential and\nsampling tools after running various model selection algorithms including their\ncombinations into multiple views/queries framework. We also present a way to do\ndata carving, providing more powerful tests than classical data splitting by\nreusing the information in the data from the first stage.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 21:04:28 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 23:01:13 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Markovic", "Jelena", ""], ["Taylor", "Jonathan", ""]]}, {"id": "1612.07867", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla, Alex Athey, Alex Reinhart, James G. Scott", "title": "Sequential nonparametric tests for a change in distribution: an\n  application to detecting radiological anomalies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sequential nonparametric test for detecting a change in\ndistribution, based on windowed Kolmogorov--Smirnov statistics. The approach is\nsimple, robust, highly computationally efficient, easy to calibrate, and\nrequires no parametric assumptions about the underlying null and alternative\ndistributions. We show that both the false-alarm rate and the power of our\nprocedure are amenable to rigorous analysis, and that the method outperforms\nexisting sequential testing procedures in practice. We then apply the method to\nthe problem of detecting radiological anomalies, using data collected from\nmeasurements of the background gamma-radiation spectrum on a large university\ncampus. In this context, the proposed method leads to substantial improvements\nin time-to-detection for the kind of radiological anomalies of interest in\nlaw-enforcement and border-security applications.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 04:02:44 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Athey", "Alex", ""], ["Reinhart", "Alex", ""], ["Scott", "James G.", ""]]}, {"id": "1612.07971", "submitter": "St\\'ephanie Aerts", "authors": "St\\'ephanie Aerts and Ines Wilms", "title": "Cellwise robust regularized discriminant analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quadratic and Linear Discriminant Analysis (QDA/LDA) are the most often\napplied classification rules under normality. In QDA, a separate covariance\nmatrix is estimated for each group. If there are more variables than\nobservations in the groups, the usual estimates are singular and cannot be used\nanymore. Assuming homoscedasticity, as in LDA, reduces the number of parameters\nto estimate. This rather strong assumption is however rarely verified in\npractice. Regularized discriminant techniques that are computable in\nhigh-dimension and cover the path between the two extremes QDA and LDA have\nbeen proposed in the literature. However, these procedures rely on sample\ncovariance matrices. As such, they become inappropriate in presence of cellwise\noutliers, a type of outliers that is very likely to occur in high-dimensional\ndatasets. In this paper, we propose cellwise robust counterparts of these\nregularized discriminant techniques by inserting cellwise robust covariance\nmatrices. Our methodology results in a family of discriminant methods that (i)\nare robust against outlying cells, (ii) cover the gap between LDA and QDA and\n(iii) are computable in high-dimension. The good performance of the new methods\nis illustrated through simulated and real data examples. As a by-product,\nvisual tools are provided for the detection of outliers.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 13:50:12 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Aerts", "St\u00e9phanie", ""], ["Wilms", "Ines", ""]]}, {"id": "1612.08062", "submitter": "Minjie Fan", "authors": "Minjie Fan, Debashis Paul, Thomas C.M. Lee, Tomoko Matsuo", "title": "Modeling Tangential Vector Fields on a Sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical processes that manifest as tangential vector fields on a sphere are\ncommon in geophysical and environmental sciences. These naturally occurring\nvector fields are often subject to physical constraints, such as being\ncurl-free or divergence-free. We construct a new class of parametric models for\ncross-covariance functions of curl-free and divergence-free vector fields that\nare tangential to the unit sphere. These models are constructed by applying the\nsurface gradient or the surface curl operator to scalar random potential fields\ndefined on the unit sphere. We propose a likelihood-based estimation procedure\nfor the model parameters and show that fast computation is possible even for\nlarge data sets when the observations are on a regular latitude-longitude grid.\nCharacteristics and utility of the proposed methodology are illustrated through\nsimulation studies and by applying it to an ocean surface wind velocity data\nset collected through satellite-based scatterometry remote sensing. We also\ncompare the performance of the proposed model with a class of bivariate\nMat\\'ern models in terms of estimation and prediction, and demonstrate that the\nproposed model is superior in capturing certain physical characteristics of the\nwind fields.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 19:01:43 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Fan", "Minjie", ""], ["Paul", "Debashis", ""], ["Lee", "Thomas C. M.", ""], ["Matsuo", "Tomoko", ""]]}, {"id": "1612.08099", "submitter": "Carlos Martins-Filho", "authors": "Carlos Martins-Filho, Feng Yao, Maximo Torero", "title": "Nonparametric estimation of conditional value-at-risk and expected\n  shortfall based on extreme value theory", "comments": "45 pages, Econometric Theory, 2017", "journal-ref": null, "doi": "10.1017/S0266466616000517", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose nonparametric estimators for conditional value-at-risk (CVaR) and\nconditional expected shortfall (CES) associated with conditional distributions\nof a series of returns on a financial asset. The return series and the\nconditioning covariates, which may include lagged returns and other exogenous\nvariables, are assumed to be strong mixing and follow a nonparametric\nconditional location-scale model. First stage nonparametric estimators for\nlocation and scale are combined with a generalized Pareto approximation for\ndistribution tails proposed by Pickands (1975) to give final estimators for\nCVaR and CES. We provide consistency and asymptotic normality of the proposed\nestimators under suitable normalization. We also present the results of a Monte\nCarlo study that sheds light on their finite sample performance. Empirical\nviability of the model and estimators is investigated through a backtesting\nexercise using returns on future contracts for five agricultural commodities.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 21:18:42 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Martins-Filho", "Carlos", ""], ["Yao", "Feng", ""], ["Torero", "Maximo", ""]]}, {"id": "1612.08114", "submitter": "Maria Francesca  Marino", "authors": "Marco Alfo', Maria Francesca Marino, Maria Giovanna Ranalli, Nicola\n  Salvati, Nikos Tzavidis", "title": "M-quantile regression for multivariate longitudinal data: analysis of\n  the Millennium Cohort Study data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a M-quantile regression model for the analysis of multivariate,\ncontinuous, longitudinal data. M-quantile regression represents an appealing\nalternative to standard regression models, as it combines the robustness of\nquantile and the efficiency of expectile regression, providing a complete\npicture of the response variable distribution. Discrete, individual-specific,\nrandom parameters are used to account for both dependence within the same\nresponse recorded at different times and association between different\nresponses observed on the same sample unit at a given time. A suitable\nparametrisation is also introduced in the linear predictor to account for\npossible dependence between the individual specific random parameters and the\nvector of observed covariates, that is to account for endogeneity of some\ncovariates. An extended EM algorithm is proposed to derive model parameter\nestimates under a maximum likelihood approach. The model is applied to the\nanalysis of the strengths and difficulties questionnaire scores from the\nMillennium Cohort Study in the UK.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 23:41:02 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 10:02:17 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Alfo'", "Marco", ""], ["Marino", "Maria Francesca", ""], ["Ranalli", "Maria Giovanna", ""], ["Salvati", "Nicola", ""], ["Tzavidis", "Nikos", ""]]}, {"id": "1612.08246", "submitter": "Xiaodong Yan", "authors": "Nian-Sheng Tang, Xiao-Dong Yan and Pu-Ying Zhao", "title": "Exponentially tilted likelihood inference on growing dimensional\n  unconditional moment models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing-dimensional data with likelihood unavailable are often encountered in\nvarious fields. This paper presents a penalized exponentially tilted likelihood\n(PETL) for variable selection and parameter estimation for growing dimensional\nunconditional moment models in the presence of correlation among variables and\nmodel misspecifica- tion. Under some regularity conditions, we investigate the\nconsistent and oracle proper- ties of the PETL estimators of parameters, and\nshow that the constrainedly PETL ratio statistic for testing contrast\nhypothesis asymptotically follows the central chi-squared distribution.\nTheoretical results reveal that the PETL approach is robust to model mis-\nspecification. We also study high-order asymptotic properties of the proposed\nPETL estimators. Simulation studies are conducted to investigate the finite\nperformance of the proposed methodologies. An example from the Boston Housing\nStudy is illustrated.\n", "versions": [{"version": "v1", "created": "Sun, 25 Dec 2016 08:06:34 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 07:35:17 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Tang", "Nian-Sheng", ""], ["Yan", "Xiao-Dong", ""], ["Zhao", "Pu-Ying", ""]]}, {"id": "1612.08287", "submitter": "Peter Hoff", "authors": "Chaoyu Yu and Peter D. Hoff", "title": "Adaptive multigroup confidence intervals with constant coverage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence intervals for the means of multiple normal populations are often\nbased on a hierarchical normal model. While commonly used interval procedures\nbased on such a model have the nominal coverage rate on average across a\npopulation of groups, their actual coverage rate for a given group will be\nabove or below the nominal rate, depending on the value of the group mean.\nAlternatively, a coverage rate that is constant as a function of a group's mean\ncan be simply achieved by using a standard $t$-interval, based on data only\nfrom that group. The standard $t$-interval, however, fails to share information\nacross the groups and is therefore not adaptive to easily obtained information\nabout the distribution of group-specific means.\n  In this article we construct confidence intervals that have a constant\nfrequentist coverage rate and that make use of information about across-group\nheterogeneity, resulting in constant-coverage intervals that are narrower than\nstandard $t$-intervals on average across groups. Such intervals are constructed\nby inverting biased tests for the mean of a normal population. Given a prior\ndistribution on the mean, Bayes-optimal biased tests can be inverted to form\nBayes-optimal confidence intervals with frequentist coverage that is constant\nas a function of the mean. In the context of multiple groups, the prior\ndistribution is replaced by a model of across-group heterogeneity. The\nparameters for this model can be estimated using data from all of the groups,\nand used to obtain confidence intervals with constant group-specific coverage\nthat adapt to information about the distribution of group means.\n", "versions": [{"version": "v1", "created": "Sun, 25 Dec 2016 19:19:05 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Yu", "Chaoyu", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1612.08288", "submitter": "Takuya Ura", "authors": "Takuya Ura", "title": "Instrumental Variable Quantile Regression with Misclassification", "comments": null, "journal-ref": "Econom. Theory 37 (2021) 169-204", "doi": "10.1017/S026646662000002X", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the instrumental variable quantile regression model\n(Chernozhukov and Hansen, 2005, 2013) with a binary endogenous treatment. It\noffers two identification results when the treatment status is not directly\nobserved. The first result is that, remarkably, the reduced-form quantile\nregression of the outcome variable on the instrumental variable provides a\nlower bound on the structural quantile treatment effect under the stochastic\nmonotonicity condition (Small and Tan, 2007; DiNardo and Lee, 2011). This\nresult is relevant, not only when the treatment variable is subject to\nmisclassification, but also when any measurement of the treatment variable is\nnot available. The second result is for the structural quantile function when\nthe treatment status is measured with error; I obtain the sharp identified set\nby deriving moment conditions under widely-used assumptions on the measurement\nerror. Furthermore, I propose an inference method in the presence of other\ncovariates.\n", "versions": [{"version": "v1", "created": "Sun, 25 Dec 2016 19:35:15 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 17:20:47 GMT"}, {"version": "v3", "created": "Wed, 13 Nov 2019 19:41:20 GMT"}, {"version": "v4", "created": "Mon, 16 Dec 2019 21:28:45 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Ura", "Takuya", ""]]}, {"id": "1612.08321", "submitter": "Nathan Kallus", "authors": "Nathan Kallus", "title": "Generalized Optimal Matching Methods for Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an encompassing framework for matching, covariate balancing, and\ndoubly-robust methods for causal inference from observational data called\ngeneralized optimal matching (GOM). The framework is given by generalizing a\nnew functional-analytical formulation of optimal matching, giving rise to the\nclass of GOM methods, for which we provide a single unified theory to analyze\ntractability, consistency, and efficiency. Many commonly used existing methods\nare included in GOM and, using their GOM interpretation, can be extended to\noptimally and automatically trade off balance for variance and outperform their\nstandard counterparts. As a subclass, GOM gives rise to kernel optimal matching\n(KOM), which, as supported by new theoretical and empirical results, is notable\nfor combining many of the positive properties of other methods in one. KOM,\nwhich is solved as a linearly-constrained convex-quadratic optimization\nproblem, inherits both the interpretability and model-free consistency of\nmatching but can also achieve the $\\sqrt{n}$-consistency of well-specified\nregression and the efficiency and robustness of doubly robust methods. In\nsettings of limited overlap, KOM enables a very transparent method for interval\nestimation for partial identification and robust coverage. We demonstrate these\nbenefits in examples with both synthetic and real data\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 03:58:42 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 17:50:00 GMT"}, {"version": "v3", "created": "Fri, 27 Oct 2017 14:29:37 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Kallus", "Nathan", ""]]}, {"id": "1612.08363", "submitter": "Xiaodong Yan", "authors": "Yan Xiao-Dong, Xie Jin-Han, Ding Xian-Wen, Wang Zhi-Qiang and Tang\n  Nian-Sheng", "title": "Fused Mean-variance Filter for Feature Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel model-free screening procedure for ultrahigh\ndimensional data analysis. By utilizing slicing technique which has been\nsuccessfully ap- plied to continuous variables, we construct a new index called\nthe fused mean-variance for feature screening. This method has the following\nmerits: (i) it is model-free, i.e., without specifying regression form of\npredictors and response variable; (ii) it can be used to analyze various types\nof variables including discrete, categorical and continuous vari- ables; (iii)\nit still works well even when the covariates/random errors are heavy-tailed or\nthe predictors are strongly dependent. Under some regularity conditions, we\nestablish the sure screening and rank consistency. Simulation studies are\nconducted to assess the performance of the proposed approach. A real data is\nused to illustrate the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 11:06:43 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Xiao-Dong", "Yan", ""], ["Jin-Han", "Xie", ""], ["Xian-Wen", "Ding", ""], ["Zhi-Qiang", "Wang", ""], ["Nian-Sheng", "Tang", ""]]}, {"id": "1612.08365", "submitter": "Xiaodong Yan", "authors": "Yan Xiao-Dong and Zhao Xing-Qiu", "title": "Optimal Model Averaging Estimation in High-dimensional Censored Linear\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers ultrahigh dimensional prediction problems with\ncensored response vari- ables. We propose a two-step model averaging procedure\nfor improving prediction accuracy of the true conditional mean of a censored\nresponse variable. The first step is to construct a class of candidate models,\neach with low-dimensional covariates. For this, a feature screening procedure\nis developed to separate the active and inactive predictors through a fused\nmean- variance index and group covariates with similar size of index together\nto form regression models with censored response variables. The new model-free\nscreening method can easi- ly deal with many types of predictors and response\nvariables, such as discrete, categorical and continuous variables, still works\nwell when predictors have heavy-tailed distributions or strongly dependend on\neach other, and enjoys rank consistency properties under mild regularity\nconditions. The second step is to find the optimal model weights for averaging\nby adapting a delete-one Mallows criterion, where the standard constraint that\nweights sum to one is removed. The theoretical results show that the delete-one\nMallows criterion achieves the lowest possible prediction loss asymptotically.\nNumerical studies demonstrate the su- perior performance of the proposed\nvariable screening and model averaging procedures over existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 11:13:35 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 05:32:51 GMT"}, {"version": "v3", "created": "Wed, 21 Jun 2017 04:34:51 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Xiao-Dong", "Yan", ""], ["Xing-Qiu", "Zhao", ""]]}, {"id": "1612.08468", "submitter": "Daniel Apley", "authors": "Daniel W. Apley and Jingyu Zhu", "title": "Visualizing the Effects of Predictor Variables in Black Box Supervised\n  Learning Models", "comments": "The R package ALEPlot is available on CRAN. The new version contains\n  refined definitions of ALE effects, a new illustrative example, theorems and\n  proofs of asymptotic properties of ALE effects and estimators, and extra\n  implementation details", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When fitting black box supervised learning models (e.g., complex trees,\nneural networks, boosted trees, random forests, nearest neighbors, local\nkernel-weighted methods, etc.), visualizing the main effects of the individual\npredictor variables and their low-order interaction effects is often important,\nand partial dependence (PD) plots are the most popular approach for\naccomplishing this. However, PD plots involve a serious pitfall if the\npredictor variables are far from independent, which is quite common with large\nobservational data sets. Namely, PD plots require extrapolation of the response\nat predictor values that are far outside the multivariate envelope of the\ntraining data, which can render the PD plots unreliable. Although marginal\nplots (M plots) do not require such extrapolation, they produce substantially\nbiased and misleading results when the predictors are dependent, analogous to\nthe omitted variable bias in regression. We present a new visualization\napproach that we term accumulated local effects (ALE) plots, which inherits the\ndesirable characteristics of PD and M plots, without inheriting their preceding\nshortcomings. Like M plots, ALE plots do not require extrapolation; and like PD\nplots, they are not biased by the omitted variable phenomenon. Moreover, ALE\nplots are far less computationally expensive than PD plots.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 01:08:55 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 20:27:57 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Apley", "Daniel W.", ""], ["Zhu", "Jingyu", ""]]}, {"id": "1612.08488", "submitter": "Chao Wang Dr", "authors": "Richard Gerlach, Chao Wang", "title": "Bayesian Semi-parametric Realized-CARE Models for Tail Risk Forecasting\n  Incorporating Realized Measures", "comments": "40 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new model framework called Realized Conditional Autoregressive Expectile\n(Realized-CARE) is proposed, through incorporating a measurement equation into\nthe conventional CARE model, in a manner analogous to the Realized-GARCH model.\nCompeting realized measures (e.g. Realized Variance and Realized Range) are\nemployed as the dependent variable in the measurement equation and to drive\nexpectile dynamics. The measurement equation here models the contemporaneous\ndependence between the realized measure and the latent conditional expectile.\nWe also propose employing the quantile loss function as the target criterion,\ninstead of the conventional violation rate, during the expectile level grid\nsearch. For the proposed model, the usual search procedure and asymmetric least\nsquares (ALS) optimization to estimate the expectile level and CARE parameters\nproves challenging and often fails to convergence. We incorporate a fast random\nwalk Metropolis stochastic search method, combined with a more targeted grid\nsearch procedure, to allow reasonably fast and improved accuracy in estimation\nof this level and the associated model parameters. Given the convergence issue,\nBayesian adaptive Markov Chain Monte Carlo methods are proposed for estimation,\nwhilst their properties are assessed and compared with ALS via a simulation\nstudy. In a real forecasting study applied to 7 market indices and 2 individual\nasset returns, compared to the original CARE, the parametric GARCH and\nRealized-GARCH models, one-day-ahead Value-at-Risk and Expected Shortfall\nforecasting results favor the proposed Realized-CARE model, especially when\nincorporating the Realized Range and the sub-sampled Realized Range as the\nrealized measure in the model.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 03:35:49 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Gerlach", "Richard", ""], ["Wang", "Chao", ""]]}, {"id": "1612.08490", "submitter": "Yuan Ke", "authors": "Jianqing Fan, Yuan Ke, Kaizheng Wang", "title": "Factor-Adjusted Regularized Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies model selection consistency for high dimensional sparse\nregression when data exhibits both cross-sectional and serial dependency. Most\ncommonly-used model selection methods fail to consistently recover the true\nmodel when the covariates are highly correlated. Motivated by econometric\nstudies, we consider the case where covariate dependence can be reduced through\nfactor model, and propose a consistent strategy named Factor-Adjusted\nRegularized Model Selection (FarmSelect). By separating the latent factors from\nidiosyncratic components, we transform the problem from model selection with\nhighly correlated covariates to that with weakly correlated variables. Model\nselection consistency as well as optimal rates of convergence are obtained\nunder mild conditions. Numerical studies demonstrate the nice finite sample\nperformance in terms of both model selection and out-of-sample prediction.\nMoreover, our method is flexible in a sense that it pays no price for weakly\ncorrelated and uncorrelated cases. Our method is applicable to a wide range of\nhigh dimensional sparse regression problems. An R-package FarmSelect is also\nprovided for implementation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 03:40:55 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 23:20:15 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Fan", "Jianqing", ""], ["Ke", "Yuan", ""], ["Wang", "Kaizheng", ""]]}, {"id": "1612.08549", "submitter": "Zhaoqiang Liu", "authors": "Zhaoqiang Liu and Vincent Y. F. Tan", "title": "Rank-One NMF-Based Initialization for NMF and Relative Error Bounds\n  under a Geometric Assumption", "comments": "Accepted by the IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2713761", "report-no": null, "categories": "stat.ML cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a geometric assumption on nonnegative data matrices such that\nunder this assumption, we are able to provide upper bounds (both deterministic\nand probabilistic) on the relative error of nonnegative matrix factorization\n(NMF). The algorithm we propose first uses the geometric assumption to obtain\nan exact clustering of the columns of the data matrix; subsequently, it employs\nseveral rank-one NMFs to obtain the final decomposition. When applied to data\nmatrices generated from our statistical model, we observe that our proposed\nalgorithm produces factor matrices with comparable relative errors vis-\\`a-vis\nclassical NMF algorithms but with much faster speeds. On face image and\nhyperspectral imaging datasets, we demonstrate that our algorithm provides an\nexcellent initialization for applying other NMF algorithms at a low\ncomputational cost. Finally, we show on face and text datasets that the\ncombinations of our algorithm and several classical NMF algorithms outperform\nother algorithms in terms of clustering performance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 09:27:10 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 08:04:07 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Liu", "Zhaoqiang", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "1612.08699", "submitter": "Kirk Bansak", "authors": "Kirk Bansak", "title": "Comparative Causal Mediation and Relaxing the Assumption of No\n  Mediator-Outcome Confounding: An Application to International Law and\n  Audience Costs", "comments": "This manuscript has been accepted for publication by Political\n  Analysis and will appear in a revised form subject to peer review and/or\n  input from the journal's editor. End-users of this manuscript may only make\n  use of it for private research and study and may not distribute it further", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experiments often include multiple treatments, with the primary goal to\ncompare the causal effects of those treatments. This study focuses on comparing\nthe causal anatomies of multiple treatments through the use of causal mediation\nanalysis. It proposes a novel set of comparative causal mediation (CCM)\nestimands that compare the mediation effects of different treatments via a\ncommon mediator. Further, it derives the properties of a set of estimators for\nthe CCM estimands and shows these estimators to be consistent (or conservative)\nunder assumptions that do not require the absence of unobserved confounding of\nthe mediator-outcome relationship, which is a strong and nonrefutable\nassumption that must typically be made for consistent estimation of individual\ncausal mediation effects. To illustrate the method, the study presents an\noriginal application investigating whether and how the international legal\nstatus of a foreign policy commitment can increase the domestic political\n\"audience costs\" that democratic governments suffer for violating such a\ncommitment. The results provide novel evidence that international legalization\ncan enhance audience costs via multiple causal channels, including by\namplifying the perceived immorality of violating the commitment.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 18:19:42 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 19:53:47 GMT"}, {"version": "v3", "created": "Wed, 4 Apr 2018 20:34:39 GMT"}, {"version": "v4", "created": "Mon, 1 Jul 2019 19:48:17 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Bansak", "Kirk", ""]]}, {"id": "1612.08776", "submitter": "Alexey Kurennoy", "authors": "Alexey Kurennoy", "title": "Estimating the Distribution of Displacements", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for estimating the distribution of time\ndifferences between connected events (such as ad impressions and corresponding\ncustomer calls). A special feature of this method is that it does not require\nmatching those connected events with each other. The method is very simple to\nuse as it essentially consists of computing an ordinary least squares\nestimator.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 00:28:22 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 22:41:24 GMT"}, {"version": "v3", "created": "Tue, 6 Feb 2018 14:31:54 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Kurennoy", "Alexey", ""]]}, {"id": "1612.09121", "submitter": "Soham Sarkar", "authors": "Soham Sarkar and Anil K. Ghosh", "title": "On perfect clustering of high dimension, low sample size data", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2019", "doi": "10.1109/TPAMI.2019.2912599", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular clustering algorithms based on usual distance functions (e.g.,\nEuclidean distance) often suffer in high dimension, low sample size (HDLSS)\nsituations, where concentration of pairwise distances has adverse effects on\ntheir performance. In this article, we use a dissimilarity measure based on the\ndata cloud, called MADD, which takes care of this problem. MADD uses the\ndistance concentration phenomenon to its advantage, and as a result, clustering\nalgorithms based on MADD usually perform better for high dimensional data.\nUsing theoretical and numerical results, we amply demonstrate it in this\narticle.\n  We also address the problem of estimating the number of clusters. This is a\nvery challenging problem in cluster analysis, and several algorithms have been\nproposed for it. We show that many of these existing algorithms have superior\nperformance in high dimensions when MADD is used instead of the Euclidean\ndistance. We also construct a new estimator based on penalized Dunn index and\nprove its consistency in the HDLSS asymptotic regime, where the sample size\nremains fixed and the dimension grows to infinity. Several simulated and real\ndata sets are analyzed to demonstrate the importance of MADD for cluster\nanalysis of high dimensional data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 12:29:08 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 14:02:30 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Sarkar", "Soham", ""], ["Ghosh", "Anil K.", ""]]}, {"id": "1612.09207", "submitter": "Kosuke Morikawa", "authors": "Kosuke Morikawa and Jae Kwang Kim", "title": "Semiparametric Optimal Estimation With Nonignorable Nonresponse Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the response mechanism is believed to be not missing at random (NMAR), a\nvalid analysis requires stronger assumptions on the response mechanism than\nstandard statistical methods would otherwise require. Semiparametric estimators\nhave been developed under the model assumptions on the response mechanism. In\nthis paper, a new statistical test is proposed to guarantee model\nidentifiability without using any instrumental variable. Furthermore, we\ndevelop optimal semiparametric estimation for parameters such as the population\nmean. Specifically, we propose two semiparametric optimal estimators that do\nnot require any model assumptions other than the response mechanism. Asymptotic\nproperties of the proposed estimators are discussed. An extensive simulation\nstudy is presented to compare with some existing methods. We present an\napplication of our method using Korean Labor and Income Panel Survey data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 17:16:18 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 07:27:29 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 06:04:23 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Morikawa", "Kosuke", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1612.09413", "submitter": "Mingyuan Zhou", "authors": "Quan Zhang and Mingyuan Zhou", "title": "Permuted and Augmented Stick-Breaking Bayesian Multinomial Regression", "comments": null, "journal-ref": "Journal of Machine Learning Research, vol. 18, pp. 1-33, 2018", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To model categorical response variables given their covariates, we propose a\npermuted and augmented stick-breaking (paSB) construction that one-to-one maps\nthe observed categories to randomly permuted latent sticks. This new\nconstruction transforms multinomial regression into regression analysis of\nstick-specific binary random variables that are mutually independent given\ntheir covariate-dependent stick success probabilities, which are parameterized\nby the regression coefficients of their corresponding categories. The paSB\nconstruction allows transforming an arbitrary cross-entropy-loss binary\nclassifier into a Bayesian multinomial one. Specifically, we parameterize the\nnegative logarithms of the stick failure probabilities with a family of\ncovariate-dependent softplus functions to construct nonparametric Bayesian\nmultinomial softplus regression, and transform Bayesian support vector machine\n(SVM) into Bayesian multinomial SVM. These Bayesian multinomial regression\nmodels are not only capable of providing probability estimates, quantifying\nuncertainty, increasing robustness, and producing nonlinear classification\ndecision boundaries, but also amenable to posterior simulation. Example results\ndemonstrate their attractive properties and performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 07:54:49 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2017 23:45:04 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 07:00:49 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Zhang", "Quan", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1612.09474", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Michael Golden, Eduardo Garc\\'ia-Portugu\\'es, Michael S{\\o}rensen,\n  Kanti V. Mardia, Thomas Hamelryck, Jotun Hein", "title": "A generative angular model of protein structure evolution", "comments": "23 pages, 10 figures. Supplementary material: 5 pages, 4 figures", "journal-ref": "Molecular Biology and Evolution, 34(8):2085-2100, 2017", "doi": "10.1093/molbev/msx137", "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recently described stochastic models of protein evolution have demonstrated\nthat the inclusion of structural information in addition to amino acid\nsequences leads to a more reliable estimation of evolutionary parameters. We\npresent a generative, evolutionary model of protein structure and sequence that\nis valid on a local length scale. The model concerns the local dependencies\nbetween sequence and structure evolution in a pair of homologous proteins. The\nevolutionary trajectory between the two structures in the protein pair is\ntreated as a random walk in dihedral angle space, which is modelled using a\nnovel angular diffusion process on the two-dimensional torus. Coupling sequence\nand structure evolution in our model allows for modelling both \"smooth\"\nconformational changes and \"catastrophic\" conformational jumps, conditioned on\nthe amino acid changes. The model has interpretable parameters and is\ncomparatively more realistic than previous stochastic models, providing new\ninsights into the relationship between sequence and structure evolution. For\nexample, using the trained model we were able to identify an apparent\nsequence-structure evolutionary motif present in a large number of homologous\nprotein pairs. The generative nature of our model enables us to evaluate its\nvalidity and its ability to simulate aspects of protein evolution conditioned\non an amino acid sequence, a related amino acid sequence, a related structure\nor any combination thereof.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 12:25:05 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 20:08:18 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 12:45:09 GMT"}, {"version": "v4", "created": "Mon, 21 Sep 2020 09:42:02 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Golden", "Michael", ""], ["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["S\u00f8rensen", "Michael", ""], ["Mardia", "Kanti V.", ""], ["Hamelryck", "Thomas", ""], ["Hein", "Jotun", ""]]}, {"id": "1612.09585", "submitter": "Yifei Sun", "authors": "Arnaud Dupuy, Alfred Galichon, Yifei Sun", "title": "Estimating matching affinity matrix under low-rank constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of estimating transport surplus (a.k.a.\nmatching affinity) in high dimensional optimal transport problems. Classical\noptimal transport theory specifies the matching affinity and determines the\noptimal joint distribution. In contrast, we study the inverse problem of\nestimating matching affinity based on the observation of the joint\ndistribution, using an entropic regularization of the problem. To accommodate\nhigh dimensionality of the data, we propose a novel method that incorporates a\nnuclear norm regularization which effectively enforces a rank constraint on the\naffinity matrix. The low-rank matrix estimated in this way reveals the main\nfactors which are relevant for matching.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 20:18:07 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Dupuy", "Arnaud", ""], ["Galichon", "Alfred", ""], ["Sun", "Yifei", ""]]}]