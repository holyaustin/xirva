[{"id": "2008.00127", "submitter": "Jinghao Sun", "authors": "Jinghao Sun (1), Luk Van Baelen (2), Els Plettinckx (2), Forrest W.\n  Crawford (1 and 3 and 4 and 5) ((1) Department of Biostatistics, Yale School\n  of Public Health, (2) Directorate of Epidemiology and Public Health,\n  Sciensano, Belgium, (3) Department of Statistics and Data Science, Yale\n  University, (4) Department of Ecology and Evolutionary Biology, Yale\n  University, (5) Yale School of Management)", "title": "Partial identification and dependence-robust confidence intervals for\n  capture-recapture surveys", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capture-recapture (CRC) surveys are widely used to estimate the size of a\npopulation whose members cannot be enumerated directly. When $k$ capture\nsamples are obtained, counts of unit captures in subsets of samples are\nrepresented naturally by a $2^k$ contingency table in which one element -- the\nnumber of individuals appearing in none of the samples -- remains unobserved.\nIn the absence of additional assumptions, the population size is not\npoint-identified. Assumptions about independence between samples are often used\nto achieve point-identification. However, real-world CRC surveys often use\nconvenience samples in which independence cannot be guaranteed, and population\nsize estimates under independence assumptions may lack empirical credibility.\nIn this work, we apply the theory of partial identification to show that weak\nassumptions or qualitative knowledge about the nature of dependence between\nsamples can be used to characterize a non-trivial set in which the true\npopulation size lies with high probability. We construct confidence sets for\nthe population size under bounds on pairwise capture probabilities, and bounds\non the highest order interaction term in a log-linear model using two methods:\ntest inversion bootstrap confidence intervals, and profile likelihood\nconfidence intervals. We apply these methods to recent survey data to estimate\nthe number of people who inject drugs in Brussels, Belgium.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 23:56:23 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Sun", "Jinghao", "", "1 and 3 and 4 and 5"], ["Van Baelen", "Luk", "", "1 and 3 and 4 and 5"], ["Plettinckx", "Els", "", "1 and 3 and 4 and 5"], ["Crawford", "Forrest W.", "", "1 and 3 and 4 and 5"]]}, {"id": "2008.00142", "submitter": "Yea Seul Kim", "authors": "Yea-Seul Kim, Paula Kayongo, Madeleine Grunde-McLaughlin and Jessica\n  Hullman", "title": "Bayesian-Assisted Inference from Visualized Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian view of data interpretation suggests that a visualization user\nshould update their existing beliefs about a parameter's value in accordance\nwith the amount of information about the parameter value captured by the new\nobservations. Extending recent work applying Bayesian models to understand and\nevaluate belief updating from visualizations, we show how the predictions of\nBayesian inference can be used to guide more rational belief updating. We\ndesign a Bayesian inference-assisted uncertainty analogy that numerically\nrelates uncertainty in observed data to the user's subjective uncertainty, and\na posterior visualization that prescribes how a user should update their\nbeliefs given their prior beliefs and the observed data. In a pre-registered\nexperiment on 4,800 people, we find that when a newly observed data sample is\nrelatively small (N=158), both techniques reliably improve people's Bayesian\nupdating on average compared to the current best practice of visualizing\nuncertainty in the observed data. For large data samples (N=5208), where\npeople's updated beliefs tend to deviate more strongly from the prescriptions\nof a Bayesian model, we find evidence that the effectiveness of the two forms\nof Bayesian assistance may depend on people's proclivity toward trusting the\nsource of the data. We discuss how our results provide insight into individual\nprocesses of belief updating and subjective uncertainty, and how understanding\nthese aspects of interpretation paves the way for more sophisticated\ninteractive visualizations for analysis and communication.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 01:12:25 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 22:32:29 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Kim", "Yea-Seul", ""], ["Kayongo", "Paula", ""], ["Grunde-McLaughlin", "Madeleine", ""], ["Hullman", "Jessica", ""]]}, {"id": "2008.00163", "submitter": "Avanti Athreya", "authors": "Konstantinos Pantazis, Avanti Athreya, Jes\\'us Arroyo, William N.\n  Frost, Evan S. Hill, and Vince Lyzinski", "title": "The Importance of Being Correlated: Implications of Dependence in Joint\n  Spectral Inference across Multiple Networks", "comments": "44 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral inference on multiple networks is a rapidly-developing subfield of\ngraph statistics. Recent work has demonstrated that joint, or simultaneous,\nspectral embedding of multiple independent networks can deliver more accurate\nestimation than individual spectral decompositions of those same networks. Such\ninference procedures typically rely heavily on independence assumptions across\nthe multiple network realizations, and even in this case, little attention has\nbeen paid to the induced network correlation in such joint embeddings. Here, we\npresent a generalized omnibus embedding methodology and provide a detailed\nanalysis of this embedding across both independent and correlated networks, the\nlatter of which significantly extends the reach of such procedures. We describe\nhow this omnibus embedding can itself induce correlation, leading us to\ndistinguish between inherent correlation -- the correlation that arises\nnaturally in multisample network data -- and induced correlation, which is an\nartifice of the joint embedding methodology. We show that the generalized\nomnibus embedding procedure is flexible and robust, and prove both consistency\nand a central limit theorem for the embedded points. We examine how induced and\ninherent correlation can impact inference for network time series data, and we\nprovide network analogues of classical questions such as the effective sample\nsize for more generally correlated data. Further, we show how an appropriately\ncalibrated generalized omnibus embedding can detect changes in real biological\nnetworks that previous embedding procedures could not discern, confirming that\nthe effect of inherent and induced correlation can be subtle and\ntransformative, with import in theory and practice.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 03:43:52 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 05:19:14 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 17:10:29 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Pantazis", "Konstantinos", ""], ["Athreya", "Avanti", ""], ["Arroyo", "Jes\u00fas", ""], ["Frost", "William N.", ""], ["Hill", "Evan S.", ""], ["Lyzinski", "Vince", ""]]}, {"id": "2008.00235", "submitter": "Alessandra Cabassi", "authors": "Alessandra Cabassi, Denis Seyres, Mattia Frontini, Paul D. W. Kirk", "title": "Two-step penalised logistic regression for multi-omic data with an\n  application to cardiometabolic syndrome", "comments": "Manuscript: 22 pages, 6 figures. Supplement: 24 pages, 20 figures.\n  For associated R code, see\n  https://github.com/acabassi/logistic-regression-for-multi-omic-data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building classification models that predict a binary class label on the basis\nof high dimensional multi-omics datasets poses several challenges, due to the\ntypically widely differing characteristics of the data layers in terms of\nnumber of predictors, type of data, and levels of noise. Previous research has\nshown that applying classical logistic regression with elastic-net penalty to\nthese datasets can lead to poor results (Liu et al., 2018). We implement a\ntwo-step approach to multi-omic logistic regression in which variable selection\nis performed on each layer separately and a predictive model is then built\nusing the variables selected in the first step. Here, our approach is compared\nto other methods that have been developed for the same purpose, and we adapt\nexisting software for multi-omic linear regression (Zhao and Zucknick, 2020) to\nthe logistic regression setting. Extensive simulation studies show that our\napproach should be preferred if the goal is to select as many relevant\npredictors as possible, as well as achieving prediction performances comparable\nto those of the best competitors. Our motivating example is a cardiometabolic\nsyndrome dataset comprising eight 'omic data types for 2 extreme phenotype\ngroups (10 obese and 10 lipodystrophy individuals) and 185 blood donors. Our\nproposed approach allows us to identify features that characterise\ncardiometabolic syndrome at the molecular level. R code is available at\nhttps://github.com/acabassi/logistic-regression-for-multi-omic-data.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 10:36:27 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Cabassi", "Alessandra", ""], ["Seyres", "Denis", ""], ["Frontini", "Mattia", ""], ["Kirk", "Paul D. W.", ""]]}, {"id": "2008.00249", "submitter": "Jun Luo", "authors": "L. Jeff Hong, Weiwei Fan, Jun Luo", "title": "Review on Ranking and Selection: A New Perspective", "comments": "52 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we briefly review the development of ranking-and-selection\n(R&S) in the past 70 years, especially the theoretical achievements and\npractical applications in the last 20 years. Different from the frequentist and\nBayesian classifications adopted by Kim and Nelson (2006b) and Chick (2006) in\ntheir review articles, we categorize existing R&S procedures into\nfixed-precision and fixed-budget procedures, as in Hunter and Nelson (2017). We\nshow that these two categories of procedures essentially differ in the\nunderlying methodological formulations, i.e., they are built on hypothesis\ntesting and dynamic-programming, respectively. In light of this variation, we\nreview in detail some well-known procedures in the literature and show how they\nfit into these two formulations. In addition, we discuss the use of R&S\nprocedures in solving various practical problems and propose what we think are\nthe important research questions in the field.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 11:36:16 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 16:54:58 GMT"}, {"version": "v3", "created": "Sun, 14 Mar 2021 04:08:50 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Hong", "L. Jeff", ""], ["Fan", "Weiwei", ""], ["Luo", "Jun", ""]]}, {"id": "2008.00254", "submitter": "Serena Ng", "authors": "Jushan Bai and Serena Ng", "title": "Simpler Proofs for Approximate Factor Models of Large Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimates of the approximate factor model are increasingly used in empirical\nwork. Their theoretical properties, studied some twenty years ago, also laid\nthe ground work for analysis on large dimensional panel data models with\ncross-section dependence. This paper presents simplified proofs for the\nestimates by using alternative rotation matrices, exploiting properties of low\nrank matrices, as well as the singular value decomposition of the data in\naddition to its covariance structure. These simplifications facilitate\ninterpretation of results and provide a more friendly introduction to\nresearchers new to the field. New results are provided to allow linear\nrestrictions to be imposed on factor models.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 12:03:46 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Bai", "Jushan", ""], ["Ng", "Serena", ""]]}, {"id": "2008.00262", "submitter": "Amanda Fern\\'andez-Fontelo Dr.", "authors": "Amanda Fern\\'andez-Fontelo, David Mori\\~na, Alejandra Caba\\~na,\n  Argimiro Arratia, Pere Puig", "title": "Estimating the real burden of disease under a pandemic situation: The\n  SARS-CoV2 case", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0242956", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper introduces a new model used to study and analyse the severe\nacute respiratory syndrome coronavirus 2 (SARS-CoV2) epidemic-reported-data\nfrom Spain. This is a Hidden Markov Model whose hidden layer is a regeneration\nprocess with Poisson immigration, Po-INAR(1), together with a mechanism that\nallows the estimation of the under-reporting in non-stationary count time\nseries. A novelty of the model is that the expectation of the innovations in\nthe unobserved process is a time-dependent function defined in such a way that\ninformation about the spread of an epidemic, as modelled through a\nSusceptible-Infectious-Removed dynamical system, is incorporated into the\nmodel. In addition, the parameter controlling the intensity of the\nunder-reporting is also made to vary with time to adjust to possible\nseasonality or trend in the data. Maximum likelihood methods are used to\nestimate the parameters of the model.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 13:18:48 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Fern\u00e1ndez-Fontelo", "Amanda", ""], ["Mori\u00f1a", "David", ""], ["Caba\u00f1a", "Alejandra", ""], ["Arratia", "Argimiro", ""], ["Puig", "Pere", ""]]}, {"id": "2008.00300", "submitter": "Emily Roberts", "authors": "Emily Roberts and Lili Zhao", "title": "A Bayesian Mixture Model for Changepoint Estimation Using Ordinal\n  Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression models, predictor variables with inherent ordering, such as\ntumor staging ranging and ECOG performance status, are commonly seen in medical\nsettings. Statistically, it may be difficult to determine the functional form\nof an ordinal predictor variable. Often, such a variable is dichotomized based\non whether it is above or below a certain cutoff. Other methods conveniently\ntreat the ordinal predictor as a continuous variable and assume a linear\nrelationship with the outcome. However, arbitrarily choosing a method may lead\nto inaccurate inference and treatment. In this paper, we propose a Bayesian\nmixture model to simultaneously assess the appropriate form of the predictor in\nregression models by considering the presence of a changepoint through the lens\nof a threshold detection problem. By using a mixture model framework to\nconsider both dichotomous and linear forms for the variable, the estimate is a\nweighted average of linear and binary parameterizations. This method is\napplicable to continuous, binary, and survival outcomes, and easily amenable to\npenalized regression. We evaluated the proposed method using simulation studies\nand apply it to two real datasets. We provide JAGS code for easy\nimplementation.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 17:14:37 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Roberts", "Emily", ""], ["Zhao", "Lili", ""]]}, {"id": "2008.00313", "submitter": "Moo K. Chung", "authors": "Moo K. Chung", "title": "Sparse Network Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been many attempts to identify high-dimensional network features\nvia multivariate approaches. Specifically, when the number of voxels or nodes,\ndenoted as p, are substantially larger than the number of images, denoted as n,\nit produces an under-determined model with infinitely many possible solutions.\nThe small-n large-p problem is often remedied by regularizing the\nunder-determined system with additional sparse penalties. Popular sparse\nnetwork models include sparse correlations, LASSO, sparse canonical\ncorrelations and graphical-LASSO. These popular sparse models require\noptimizing L1-norm penalties, which has been the major computational bottleneck\nfor solving large-scale problems. Thus, many existing sparse brain network\nmodels in brain imaging have been restricted to a few hundreds nodes or less.\n2527 MRI features used in a LASSO model for Alzheimer's disease is probably the\nlargest number of features used in any sparse model in the brain imaging\nliterature.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 18:29:29 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Chung", "Moo K.", ""]]}, {"id": "2008.00400", "submitter": "Jialiang Mao", "authors": "Jialiang Mao, Li Ma", "title": "Dirichlet-tree multinomial mixtures for clustering microbiome\n  compositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the human microbiome has gained substantial interest in recent\nyears, and a common task in the analysis of these data is to cluster microbiome\ncompositions into subtypes. This subdivision of samples into subgroups serves\nas an intermediary step in achieving personalized diagnosis and treatment. In\napplying existing clustering methods to modern microbiome studies including the\nAmerican Gut Project (AGP) data, we found that this seemingly standard task,\nhowever, is very challenging in the microbiome composition context due to\nseveral key features of such data. Standard distance-based clustering\nalgorithms generally do not produce reliable results as they do not take into\naccount the heterogeneity of the cross-sample variability among the bacterial\ntaxa, while existing model-based approaches do not allow sufficient flexibility\nfor the identification of complex within-cluster variation from cross-cluster\nvariation. Direct applications of such methods generally lead to overly\ndispersed clusters in the AGP data and such phenomenon is common for other\nmicrobiome data. To overcome these challenges, we introduce Dirichlet-tree\nmultinomial mixtures (DTMM) as a Bayesian generative model for clustering\namplicon sequencing data in microbiome studies. DTMM models the microbiome\npopulation with a mixture of Dirichlet-tree kernels that utilizes the\nphylogenetic tree to offer a more flexible covariance structure in\ncharacterizing within-cluster variation, and it provides a means for\nidentifying a subset of signature taxa that distinguish the clusters. We\nperform extensive simulation studies to evaluate the performance of DTMM and\ncompare it to state-of-the-art model-based and distance-based clustering\nmethods in the microbiome context. Finally, we report a case study on the fecal\ndata from the AGP to identify compositional clusters among individuals with\ninflammatory bowel disease and diabetes.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 05:17:02 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 07:11:36 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Mao", "Jialiang", ""], ["Ma", "Li", ""]]}, {"id": "2008.00422", "submitter": "Themistoklis Botsas", "authors": "Themistoklis Botsas, Lachlan R. Mason and Indranil Pan", "title": "Rule-based Bayesian regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel rule-based approach for handling regression problems.\nThe new methodology carries elements from two frameworks: (i) it provides\ninformation about the uncertainty of the parameters of interest using Bayesian\ninference, and (ii) it allows the incorporation of expert knowledge through\nrule-based systems. The blending of those two different frameworks can be\nparticularly beneficial for various domains (e.g. engineering), where, even\nthough the significance of uncertainty quantification motivates a Bayesian\napproach, there is no simple way to incorporate researcher intuition into the\nmodel. We validate our models by applying them to synthetic applications: a\nsimple linear regression problem and two more complex structures based on\npartial differential equations. Finally, we review the advantages of our\nmethodology, which include the simplicity of the implementation, the\nuncertainty reduction due to the added information and, in some occasions, the\nderivation of better point predictions, and we address limitations, mainly from\nthe computational complexity perspective, such as the difficulty in choosing an\nappropriate algorithm and the added computational burden.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 07:20:45 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Botsas", "Themistoklis", ""], ["Mason", "Lachlan R.", ""], ["Pan", "Indranil", ""]]}, {"id": "2008.00507", "submitter": "James Robins", "authors": "James Robins, Mariela Sued, Quanhong Lei-Gomez, Andrea Rotnitzky", "title": "Double-robust and efficient methods for estimating the causal effects of\n  a binary treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the effects of a binary treatment on a\ncontinuous outcome of interest from observational data in the absence of\nconfounding by unmeasured factors. We provide a new estimator of the population\naverage treatment effect (ATE) based on the difference of novel double-robust\n(DR) estimators of the treatment-specific outcome means. We compare our new\nestimator with previously estimators both theoretically and via simulation.\nDR-difference estimators may have poor finite sample behavior when the\nestimated propensity scores in the treated and untreated do not overlap. We\ntherefore propose an alternative approach, which can be used even in this\nunfavorable setting, based on locally efficient double-robust estimation of a\nsemiparametric regression model for the modification on an additive scale of\nthe magnitude of the treatment effect by the baseline covariates $X$. In\ncontrast with existing methods, our approach simultaneously provides estimates\nof: i) the average treatment effect in the total study population, ii) the\naverage treatment effect in the random subset of the population with\noverlapping estimated propensity scores, and iii) the treatment effect at each\nlevel of the baseline covariates $X$.\n  When the covariate vector $X$ is high dimensional, one cannot be certain,\nowing to lack of power, that given models for the propensity score and for the\nregression of the outcome on treatment and $X$ used in constructing our DR\nestimators are nearly correct, even if they pass standard goodness of fit\ntests. Therefore to select among candidate models, we propose a novel approach\nto model selection that leverages the DR-nature of our treatment effect\nestimator and that outperforms cross-validation in a small simulation study.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 15:43:55 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Robins", "James", ""], ["Sued", "Mariela", ""], ["Lei-Gomez", "Quanhong", ""], ["Rotnitzky", "Andrea", ""]]}, {"id": "2008.00532", "submitter": "Suvra Pal", "authors": "Suvra Pal", "title": "A Simplified Stochastic EM Algorithm for Cure Rate Model with Negative\n  Binomial Competing Risks: An Application to Breast Cancer Data", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a long-term survival model under competing risks is\nconsidered. The unobserved number of competing risks is assumed to follow a\nnegative binomial distribution that can capture both over- and\nunder-dispersion. Considering the latent competing risks as missing data, a\nvariation of the well-known expectation maximization (EM) algorithm, called the\nstochastic EM algorithm (SEM), is developed. It is shown that the SEM algorithm\navoids calculation of complicated expectations, which is a major advantage of\nthe SEM algorithm over the EM algorithm. The proposed procedure also allows the\nobjective function to be split into two simpler functions, one corresponding to\nthe parameters associated with the cure rate and the other corresponding to the\nparameters associated with the progression times. The advantage of this\napproach is that each simple function, with lower parameter dimension, can be\nmaximized independently. An extensive Monte Carlo simulation study is carried\nout to compare the performances of the SEM and EM algorithms. Finally, a breast\ncancer survival data is analyzed and it is shown that the SEM algorithm\nperforms better than the EM algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 17:45:14 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 03:10:49 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 23:20:45 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Pal", "Suvra", ""]]}, {"id": "2008.00547", "submitter": "Arvind Krishna", "authors": "Arvind Krishna (1), V. Roshan Joseph (1), Shan Ba (2), William A.\n  Brenneman (3), William R. Myers (3) ((1) Georgia Institute of Technology, (2)\n  LinkedIn Corporation, (3) Procter & Gamble Company)", "title": "Robust Experimental Designs for Model Calibration", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": "10.1080/00224065.2021.1930618", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computer model can be used for predicting an output only after specifying\nthe values of some unknown physical constants known as calibration parameters.\nThe unknown calibration parameters can be estimated from real data by\nconducting physical experiments. This paper presents an approach to optimally\ndesign such a physical experiment. The problem of optimally designing physical\nexperiment, using a computer model, is similar to the problem of finding\noptimal design for fitting nonlinear models. However, the problem is more\nchallenging than the existing work on nonlinear optimal design because of the\npossibility of model discrepancy, that is, the computer model may not be an\naccurate representation of the true underlying model. Therefore, we propose an\noptimal design approach that is robust to potential model discrepancies. We\nshow that our designs are better than the commonly used physical experimental\ndesigns that do not make use of the information contained in the computer model\nand other nonlinear optimal designs that ignore potential model discrepancies.\nWe illustrate our approach using a toy example and a real example from\nindustry.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 19:31:18 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Krishna", "Arvind", ""], ["Joseph", "V. Roshan", ""], ["Ba", "Shan", ""], ["Brenneman", "William A.", ""], ["Myers", "William R.", ""]]}, {"id": "2008.00559", "submitter": "Chris von Csefalvay", "authors": "Chris von Csefalvay", "title": "Vector quantisation and partitioning of COVID-19 temporal dynamics in\n  the United States", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical dynamics of a pathogen within a population depend on a range\nof factors: population density, the effectiveness and investment into social\ndistancing, public policy measures and non-pharmaceutical interventions (NPIs)\nare only some examples of factors that influence the number of cases over time\nby state. This paper outlines an analysis of time series vector quantisation\nand paritioning of COVID-19 cases in the United States, using a soft-DTW\n(Dynamic Time Warping) k-means clustering and a k-shape based clustering\nalgorithm to identify internally consistent clusters of case counts over time.\nThe identification of characteristic types of time-dependent variations can\nlead to the identification of patterns within sets of time series. This, in\nturn, can help discern the future of infectious dynamics in an area and,\nthrough identifying the most likely cluster-wise trajectory by calculating the\ncluster barycenter, inform public health decision-making.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 20:12:14 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["von Csefalvay", "Chris", ""]]}, {"id": "2008.00602", "submitter": "Ashesh Rambachan", "authors": "Ashesh Rambachan and Jonathan Roth", "title": "Design-Based Uncertainty for Quasi-Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social scientists are often interested in estimating causal effects in\nsettings where all units in the population are observed (e.g. all 50 US\nstates). Design-based approaches, which view the treatment as the random object\nof interest, may be more appealing than standard sampling-based approaches in\nsuch contexts. This paper develops a design-based theory of uncertainty\nsuitable for quasi-experimental settings, in which the researcher estimates the\ntreatment effect as if treatment was randomly assigned, but in reality\ntreatment probabilities may depend in unknown ways on the potential outcomes.\nWe first study the properties of the simple difference-in-means (SDIM)\nestimator. The SDIM is unbiased for a finite-population design-based analog to\nthe average treatment effect on the treated (ATT) if treatment probabilities\nare uncorrelated with the potential outcomes in a finite population sense. We\nfurther derive expressions for the variance of the SDIM estimator and a central\nlimit theorem under sequences of finite populations with growing sample size.\nWe then show how our results can be applied to analyze the distribution and\nestimand of difference-in-differences (DiD) and two-stage least squares (2SLS)\nfrom a design-based perspective when treatment is not completely randomly\nassigned.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 01:12:35 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 01:38:29 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Rambachan", "Ashesh", ""], ["Roth", "Jonathan", ""]]}, {"id": "2008.00707", "submitter": "Costanza Tort\\`u", "authors": "Falco J. Bargagli-Stoffi, Costanza Tort\\`u, Laura Forastiere", "title": "Heterogeneous Treatment and Spillover Effects under Clustered Network\n  Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bulk of causal inference studies rules out the presence of interference\nbetween units. However, in many real-world settings units are interconnected by\nsocial, physical or virtual ties and the effect of a treatment can spill from\none unit to other connected individuals in the network. In these settings,\ninterference should be taken into account to avoid biased estimates of the\ntreatment effect, but it can also be leveraged to save resources and provide\nthe intervention to a lower percentage of the population where the treatment is\nmore effective and where the effect can spill over to other susceptible\nindividuals. In fact, different people might respond differently not only to\nthe treatment received but also to the treatment received by their network\ncontacts. Understanding the heterogeneity of treatment and spillover effects\ncan help policy-makers in the scale-up phase of the intervention, it can guide\nthe design of targeting strategies with the ultimate goal of making the\ninterventions more cost-effective, and it might even allow generalizing the\nlevel of treatment spillover effects in other populations. In this paper, we\ndevelop a machine learning method that makes use of tree-based algorithms and\nan Horvitz-Thompson estimator to assess the heterogeneity of treatment and\nspillover effects with respect to individual, neighborhood and network\ncharacteristics in the context of clustered network interference. We illustrate\nhow the proposed binary tree methodology performs in a Monte Carlo simulation\nstudy. Additionally, we provide an application on a randomized experiment aimed\nat assessing the heterogeneous effects of information sessions on the uptake of\na new weather insurance policy in rural China.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 08:22:04 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 15:46:08 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Bargagli-Stoffi", "Falco J.", ""], ["Tort\u00f9", "Costanza", ""], ["Forastiere", "Laura", ""]]}, {"id": "2008.00747", "submitter": "Ke Zhu", "authors": "Donghang Luo, Ke Zhu, Huan Gong, Dong Li", "title": "Testing error distribution by kernelized Stein discrepancy in\n  multivariate time series models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing the error distribution is important in many multivariate time series\napplications. To alleviate the risk of error distribution mis-specification,\ntesting methodologies are needed to detect whether the chosen error\ndistribution is correct. However, the majority of the existing tests only deal\nwith the multivariate normal distribution for some special multivariate time\nseries models, and they thus can not be used to testing for the often observed\nheavy-tailed and skewed error distributions in applications. In this paper, we\nconstruct a new consistent test for general multivariate time series models,\nbased on the kernelized Stein discrepancy. To account for the estimation\nuncertainty and unobserved initial values, a bootstrap method is provided to\ncalculate the critical values. Our new test is easy-to-implement for a large\nscope of multivariate error distributions, and its importance is illustrated by\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 10:00:45 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Luo", "Donghang", ""], ["Zhu", "Ke", ""], ["Gong", "Huan", ""], ["Li", "Dong", ""]]}, {"id": "2008.00782", "submitter": "Ioannis Kalogridis Mr", "authors": "Ioannis Kalogridis and Stefan Van Aelst", "title": "Robust optimal estimation of location from discretely sampled functional\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating location is a central problem in functional data analysis, yet\nmost current estimation procedures either unrealistically assume completely\nobserved trajectories or lack robustness with respect to the many kinds of\nanomalies one can encounter in the functional setting. To remedy these\ndeficiencies we introduce the first class of optimal robust location estimators\nbased on discretely sampled functional data. The proposed method is based on\nM-type smoothing spline estimation with repeated measurements and is suitable\nfor both commonly and independently observed trajectories that are subject to\nmeasurement error. We show that under suitable assumptions the proposed family\nof estimators is minimax rate optimal both for commonly and independently\nobserved trajectories and we illustrate its highly competitive performance and\npractical usefulness in a Monte-Carlo study and a real-data example involving\nrecent Covid-19 data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 11:19:57 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 07:34:54 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 14:52:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kalogridis", "Ioannis", ""], ["Van Aelst", "Stefan", ""]]}, {"id": "2008.00848", "submitter": "Tahani Coolen-Maturi Dr", "authors": "Tahani Coolen-Maturi and Frank P.A. Coolen", "title": "A monotonicity property of weighted log-rank tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The logrank test is a well-known nonparametric test which is often used to\ncompare the survival distributions of two samples including right censored\nobservations, it is also known as the Mantel-Haenszel test. The $G^{\\rho}$\nfamily of tests, introduced by Harrington and Fleming (1982), generalizes the\nlogrank test by using weights assigned to observations. In this paper, we\npresent a monotonicity property for the $G^{\\rho}$ family of tests, which was\nmotivated by the need to derive bounds for the test statistic in case of\nimprecise data observations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:53:50 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Coolen-Maturi", "Tahani", ""], ["Coolen", "Frank P. A.", ""]]}, {"id": "2008.00977", "submitter": "\\'Angel Gonz\\'alez-Prieto", "authors": "\\'Angel Gonz\\'alez-Prieto, Jorge Perez, Jessica Diaz, Daniel\n  L\\'opez-Fern\\'andez", "title": "Reliability in Software Engineering Qualitative Research through\n  Inter-Coder Agreement: A guide using Krippendorff's $\\alpha$ & Atlas.ti", "comments": "35 pages, 15 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the research on empirical software engineering that uses\nqualitative data analysis (e.g., cases studies, interview surveys, and grounded\ntheory studies) is increasing. However, most of this research does not deep\ninto the reliability and validity of findings, specifically in the reliability\nof coding in which these methodologies rely on, despite there exist a variety\nof statistical techniques known as Inter-Coder Agreement (ICA) for analyzing\nconsensus in team coding. This paper aims to establish a novel theoretical\nframework that enables a methodological approach for conducting this validity\nanalysis. This framework is based on a set of coefficients for measuring the\ndegree of agreement that different coders achieve when judging a common matter.\nWe analyze different reliability coefficients and provide detailed examples of\ncalculation, with special attention to Krippendorff's $\\alpha$ coefficients. We\nsystematically review several variants of Krippendorff's $\\alpha$ reported in\nthe literature and provide a novel common mathematical framework in which all\nof them are unified through a universal $\\alpha$ coefficient. Finally, this\npaper provides a detailed guide of the use of this theoretical framework in a\nlarge case study on DevOps culture. We explain how $\\alpha$ coefficients are\ncomputed and interpreted using a widely used software tool for qualitative\nanalysis like Atlas.ti. We expect that this work will help empirical\nresearchers, particularly in software engineering, to improve the quality and\ntrustworthiness of their studies.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:10:51 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 18:15:52 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Gonz\u00e1lez-Prieto", "\u00c1ngel", ""], ["Perez", "Jorge", ""], ["Diaz", "Jessica", ""], ["L\u00f3pez-Fern\u00e1ndez", "Daniel", ""]]}, {"id": "2008.01007", "submitter": "Nina Kudryashova", "authors": "Nina Kudryashova, Theoklitos Amvrosiadis, Nathalie Dupuy, Nathalie\n  Rochefort, Arno Onken", "title": "Parametric Copula-GP model for analyzing multidimensional neuronal and\n  behavioral relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in current systems neuroscience is the analysis of\nhigh-dimensional neuronal and behavioral data that are characterized by\ndifferent statistics and timescales of the recorded variables. We propose a\nparametric copula model which separates the statistics of the individual\nvariables from their dependence structure, and escapes the curse of\ndimensionality by using vine copula constructions. We use a Bayesian framework\nwith Gaussian Process (GP) priors over copula parameters, conditioned on a\ncontinuous task-related variable. We validate the model on synthetic data and\ncompare its performance in estimating mutual information against the commonly\nused non-parametric algorithms.\n  Our model provides accurate information estimates when the dependencies in\nthe data match the parametric copulas used in our framework. When the exact\ndensity estimation with a parametric model is not possible, our Copula-GP model\nis still able to provide reasonable information estimates, close to the ground\ntruth and comparable to those obtained with a neural network estimator.\nFinally, we apply our framework to real neuronal and behavioral recordings\nobtained in awake mice. We demonstrate the ability of our framework to\n  1) produce accurate and interpretable bivariate models for the analysis of\ninter-neuronal noise correlations or behavioral modulations;\n  2) expand to more than 100 dimensions and measure information content in the\nwhole-population statistics. These results demonstrate that the Copula-GP\nframework is particularly useful for the analysis of complex multidimensional\nrelationships between neuronal, sensory and behavioral data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 16:44:29 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kudryashova", "Nina", ""], ["Amvrosiadis", "Theoklitos", ""], ["Dupuy", "Nathalie", ""], ["Rochefort", "Nathalie", ""], ["Onken", "Arno", ""]]}, {"id": "2008.01019", "submitter": "Zoe Guan", "authors": "Zoe Guan, Theodore Huang, Anne Marie McCarthy, Kevin S. Hughes, Alan\n  Semine, Hajime Uno, Lorenzo Trippa, Giovanni Parmigiani, Danielle Braun", "title": "Combining Breast Cancer Risk Prediction Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate risk stratification is key to reducing cancer morbidity through\ntargeted screening and preventative interventions. Numerous breast cancer risk\nprediction models have been developed, but they often give predictions with\nconflicting clinical implications. Integrating information from different\nmodels may improve the accuracy of risk predictions, which would be valuable\nfor both clinicians and patients. BRCAPRO and BCRAT are two widely used models\nbased on largely complementary sets of risk factors. BRCAPRO is a Bayesian\nmodel that uses detailed family history information to estimate the probability\nof carrying a BRCA1/2 mutation, as well as future risk of breast and ovarian\ncancer, based on mutation prevalence and penetrance (age-specific probability\nof developing cancer given genotype). BCRAT uses a relative hazard model based\non first-degree family history and non-genetic risk factors. We consider two\napproaches for combining BRCAPRO and BCRAT: 1) modifying the penetrance\nfunctions in BRCAPRO using relative hazard estimates from BCRAT, and 2)\ntraining an ensemble model that takes as input BRCAPRO and BCRAT predictions.\nWe show that the combination models achieve performance gains over BRCAPRO and\nBCRAT in simulations and data from the Cancer Genetics Network.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:36:21 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Guan", "Zoe", ""], ["Huang", "Theodore", ""], ["McCarthy", "Anne Marie", ""], ["Hughes", "Kevin S.", ""], ["Semine", "Alan", ""], ["Uno", "Hajime", ""], ["Trippa", "Lorenzo", ""], ["Parmigiani", "Giovanni", ""], ["Braun", "Danielle", ""]]}, {"id": "2008.01029", "submitter": "Nicole Pashley", "authors": "Nicole E. Pashley, Guillaume W. Basse, and Luke W. Miratrix", "title": "Conditional As-If Analyses in Randomized Experiments", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The injunction to `analyze the way you randomize' is well-known to\nstatisticians since Fisher advocated for randomization as the basis of\ninference. Yet even those convinced by the merits of randomization-based\ninference seldom follow this injunction to the letter. Bernoulli randomized\nexperiments are often analyzed as completely randomized experiments, and\ncompletely randomized experiments are analyzed as if they had been stratified;\nmore generally, it is not uncommon to analyze an experiment as if it had been\nrandomized differently. This paper examines the theoretical foundation behind\nthis practice within a randomization-based framework. Specifically, we ask when\nis it legitimate to analyze an experiment randomized according to one design as\nif it had been randomized according to some other design. We show that a\nsufficient condition for this type of analysis to be valid is that the design\nused for analysis be derived from the original design by an appropriate form of\nconditioning. We use our theory to justify certain existing methods, question\nothers, and finally suggest new methodological insights such as conditioning on\napproximate covariate balance.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:08:25 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 16:01:47 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Pashley", "Nicole E.", ""], ["Basse", "Guillaume W.", ""], ["Miratrix", "Luke W.", ""]]}, {"id": "2008.01038", "submitter": "Yiran Wang", "authors": "Yiran Wang, Minh Tang and Soumendra Nath Lahiri", "title": "Two-sample Testing on Latent Distance Graphs With Unknown Link Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a valid and consistent test for the hypothesis that two latent\ndistance random graphs on the same vertex set have the same generating latent\npositions, up to some unidentifiable similarity transformations. Our test\nstatistic is based on first estimating the edge probabilities matrices by\ntruncating the singular value decompositions of the averaged adjacency matrices\nin each population and then computing a Spearman rank correlation coefficient\nbetween these estimates. Experimental results on simulated data indicate that\nthe test procedure has power even when there is only one sample from each\npopulation, provided that the number of vertices is not too small. Application\non a dataset of neural connectome graphs showed that we can distinguish between\nscans from different age groups while application on a dataset of epileptogenic\nrecordings showed that we can discriminate between seizure and non-seizure\nevents.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:24:00 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Yiran", ""], ["Tang", "Minh", ""], ["Lahiri", "Soumendra Nath", ""]]}, {"id": "2008.01069", "submitter": "Ethan Neil", "authors": "William I. Jay and Ethan T. Neil", "title": "Bayesian model averaging for analysis of lattice field theory results", "comments": "30 pages, 6 figures. v2: updated refs and other minor changes. v3:\n  final update to journal version, including new appendices", "journal-ref": "Phys. Rev. D 103, 114502 (2021)", "doi": "10.1103/PhysRevD.103.114502", "report-no": "FERMILAB-PUB-20-374-T", "categories": "stat.ME hep-lat", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling is a key component in the extraction of physical results\nfrom lattice field theory calculations. Although the general models used are\noften strongly motivated by physics, many model variations can frequently be\nconsidered for the same lattice data. Model averaging, which amounts to a\nprobability-weighted average over all model variations, can incorporate\nsystematic errors associated with model choice without being overly\nconservative. We discuss the framework of model averaging from the perspective\nof Bayesian statistics, and give useful formulae and approximations for the\nparticular case of least-squares fitting, commonly used in modeling lattice\nresults. In addition, we frame the common problem of data subset selection\n(e.g. choice of minimum and maximum time separation for fitting a two-point\ncorrelation function) as a model selection problem and study model averaging as\na straightforward alternative to manual selection of fit ranges. Numerical\nexamples involving both mock and real lattice data are given.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:58:58 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 21:09:46 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 15:26:10 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Jay", "William I.", ""], ["Neil", "Ethan T.", ""]]}, {"id": "2008.01200", "submitter": "Han Yu", "authors": "Han Yu, Alan D. Hutson", "title": "A Robust Spearman Correlation Coefficient Permutation Test", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we show that Spearman's correlation coefficient test about\n$H_0:\\rho_s=0$ found in most statistical software packages is theoretically\nincorrect and performs poorly when bivariate normality assumptions are not met\nor the sample size is small. The historical works about these tests make an\nunverifiable assumption that the approximate bivariate normality of original\ndata justifies using classic approximations. In general, there is common\nmisconception that the tests about $\\rho_s=0$ are robust to deviations from\nbivariate normality. In fact, we found under certain scenarios violation of the\nbivariate normality assumption has severe effects on type I error control for\nthe most commonly utilized tests. To address this issue, we developed a robust\npermutation test for testing the general hypothesis $H_0: \\rho_s=0$. The\nproposed test is based on an appropriately studentized statistic. We will show\nthat the test is theoretically asymptotically valid in the general setting when\ntwo paired variables are uncorrelated but dependent. This desired property was\ndemonstrated across a range of distributional assumptions and sample sizes in\nsimulation studies, where the proposed test exhibits robust type I error\ncontrol across a variety of settings, even when the sample size is small. We\ndemonstrated the application of this test in real world examples of\ntranscriptomic data of the TCGA breast cancer patients and a data set of PSA\nlevels and age.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 21:17:59 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Yu", "Han", ""], ["Hutson", "Alan D.", ""]]}, {"id": "2008.01314", "submitter": "Shogo Kato Ph.D.", "authors": "Shogo Kato, Toshinao Yoshiba and Shinto Eguchi", "title": "Copula-based measures of asymmetry between the lower and upper tail\n  probabilities", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a copula-based measure of asymmetry between the lower and upper\ntail probabilities of bivariate distributions. The proposed measure has a\nsimple form and possesses some desirable properties as a measure of asymmetry.\nThe limit of the proposed measure as the index goes to the boundary of its\ndomain can be expressed in a simple form under certain conditions on copulas. A\nsample analogue of the proposed measure for a sample from a copula is presented\nand its weak convergence to a Gaussian process is shown. Another sample\nanalogue of the presented measure, which is based on a sample from a\ndistribution on $\\mathbb{R}^2$, is given. Simple methods for interval\nestimation and nonparametric testing based on the two sample analogues are\npresented. As an example, the presented measure is applied to daily returns of\nS&P500 and Nikkei225.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 03:57:57 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Kato", "Shogo", ""], ["Yoshiba", "Toshinao", ""], ["Eguchi", "Shinto", ""]]}, {"id": "2008.01375", "submitter": "Hongsong Yuan", "authors": "Fengnan Gao, Zongming Ma, Hongsong Yuan", "title": "Community detection in sparse latent space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a simple community detection algorithm originated from\nstochastic blockmodel literature achieves consistency, and even optimality, for\na broad and flexible class of sparse latent space models. The class of models\nincludes latent eigenmodels (arXiv:0711.1146). The community detection\nalgorithm is based on spectral clustering followed by local refinement via\nnormalized edge counting.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 07:12:15 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Gao", "Fengnan", ""], ["Ma", "Zongming", ""], ["Yuan", "Hongsong", ""]]}, {"id": "2008.01400", "submitter": "Chiara Piazzola", "authors": "Chiara Piazzola, Lorenzo Tamellini, Ra\\'ul Tempone", "title": "A note on tools for prediction under uncertainty and identifiability of\n  SIR-like dynamical systems for epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an overview of the methods that can be used for prediction under\nuncertainty and data fitting of dynamical systems, and of the fundamental\nchallenges that arise in this context. The focus is on SIR-like models, that\nare being commonly used when attempting to predict the trend of the COVID-19\npandemic. In particular, we raise a warning flag about identifiability of the\nparameters of SIR-like models; often, it might be hard to infer the correct\nvalues of the parameters from data, even for very simple models, making it\nnon-trivial to use these models for meaningful predictions. Most of the points\nthat we touch upon are actually generally valid for inverse problems in more\ngeneral setups.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 08:20:32 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 10:46:12 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 14:53:30 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Piazzola", "Chiara", ""], ["Tamellini", "Lorenzo", ""], ["Tempone", "Ra\u00fal", ""]]}, {"id": "2008.01479", "submitter": "Ingrid Kockum", "authors": "Jesse Huang, Ingrid Kockum, Pernilla Stridh", "title": "Interaction between two exposures: determining odds ratios and\n  confidence intervals for risk estimates", "comments": "9 pages, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In epidemiological research, it is common to investigate the interaction\nbetween risk factors for an outcome such as a disease and hence to estimate the\nrisk associated with being exposed for either or both of two risk factors under\ninvestigation. Interactions can be estimated both on the additive and\nmultiplicative scale using the same regression model. We here present a review\nfor calculating interaction and estimating the risk and confidence interval of\ntwo exposures using a single regression model and the relationship between\nmeasures, particularly the standard error for the combined exposure risk group.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 12:11:30 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Huang", "Jesse", ""], ["Kockum", "Ingrid", ""], ["Stridh", "Pernilla", ""]]}, {"id": "2008.01496", "submitter": "Xinyu Zhang", "authors": "Xinyu Zhang, Howell Tong", "title": "Some Cautionary Comments on Principal Component Analysis for Time Series\n  Data", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a most frequently used statistical tool\nin almost all branches of data science. However, like many other statistical\ntools, there is sometimes the risk of misuse or even abuse. In this short note,\nwe highlight possible pitfalls in using the theoretical results of PCA based on\nthe assumption of independent data when the data are time series. For the\nlatter, we state a central limit theorem of the eigenvalues and eigenvectors,\ngive analytical and bootstrap methods to estimate the covariance, and assess\ntheir efficacy via simulation. An empirical example is given to illustrate the\npitfalls of a common misuse of PCA. We conclude that while the conventional\nscree plot continues to be useful for time series data, the interpretation of\nthe principal component loadings requires careful attention.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 13:09:41 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Zhang", "Xinyu", ""], ["Tong", "Howell", ""]]}, {"id": "2008.01760", "submitter": "Saptarshi Chakraborty", "authors": "Saptarshi Chakraborty and Jason Xu", "title": "Biconvex Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex clustering has recently garnered increasing interest due to its\nattractive theoretical and computational properties, but its merits become\nlimited in the face of high-dimensional data. In such settings, pairwise\naffinity terms that rely on $k$-nearest neighbors become poorly specified and\nEuclidean measures of fit provide weaker discriminating power. To surmount\nthese issues, we propose to modify the convex clustering objective so that\nfeature weights are optimized jointly with the centroids. The resulting problem\nbecomes biconvex, and as such remains well-behaved statistically and\nalgorithmically. In particular, we derive a fast algorithm with closed form\nupdates and convergence guarantees, and establish finite-sample bounds on its\nprediction error. Under interpretable regularity conditions, the error bound\nanalysis implies consistency of the proposed estimator. Biconvex clustering\nperforms feature selection throughout the clustering task: as the learned\nweights change the effective feature representation, pairwise affinities can be\nupdated adaptively across iterations rather than precomputed within a dubious\nfeature space. We validate the contributions on real and simulated data,\nshowing that our method effectively addresses the challenges of dimensionality\nwhile reducing dependence on carefully tuned heuristics typical of existing\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 18:25:56 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 09:09:11 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Chakraborty", "Saptarshi", ""], ["Xu", "Jason", ""]]}, {"id": "2008.01779", "submitter": "Mark Tygert", "authors": "Mark Tygert", "title": "Cumulative deviation of a subpopulation from the full population", "comments": "70 pages, 51 figures, 2 tables; the new versions of the paper merge\n  in most of arXiv:2006.02504", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing equity in treatment of a subpopulation often involves assigning\nnumerical \"scores\" to all individuals in the full population such that similar\nindividuals get similar scores; matching via propensity scores or appropriate\ncovariates is common, for example. Given such scores, individuals with similar\nscores may or may not attain similar outcomes independent of the individuals'\nmemberships in the subpopulation. The traditional graphical methods for\nvisualizing inequities are known as \"reliability diagrams\" or \"calibrations\nplots,\" which bin the scores into a partition of all possible values, and for\neach bin plot both the average outcomes for only individuals in the\nsubpopulation as well as the average outcomes for all individuals; comparing\nthe graph for the subpopulation with that for the full population gives some\nsense of how the averages for the subpopulation deviate from the averages for\nthe full population. Unfortunately, real data sets contain only finitely many\nobservations, limiting the usable resolution of the bins, and so the\nconventional methods can obscure important variations due to the binning.\nFortunately, plotting cumulative deviation of the subpopulation from the full\npopulation as proposed in this paper sidesteps the problematic coarse binning.\nThe cumulative plots encode subpopulation deviation directly as the slopes of\nsecant lines for the graphs. Slope is easy to perceive even when the constant\noffsets of the secant lines are irrelevant. The cumulative approach avoids\nbinning that smooths over deviations of the subpopulation from the full\npopulation. Such cumulative aggregation furnishes both high-resolution\ngraphical methods and simple scalar summary statistics (analogous to those of\nKuiper and of Kolmogorov and Smirnov used in statistical significance testing\nfor comparing probability distributions).\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 19:30:02 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 22:37:32 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 17:26:47 GMT"}, {"version": "v4", "created": "Sun, 16 May 2021 17:20:41 GMT"}, {"version": "v5", "created": "Wed, 7 Jul 2021 16:40:53 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Tygert", "Mark", ""]]}, {"id": "2008.01889", "submitter": "Trevor Harris", "authors": "Trevor Harris, Bo Li, James Derek Tucker", "title": "Scalable Multiple Changepoint Detection for Functional Data Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Multiple Changepoint Isolation (MCI) method for detecting\nmultiple changes in the mean and covariance of a functional process. We first\nintroduce a pair of projections to represent the high and low frequency\nfeatures of the data. We then apply total variation denoising and introduce a\nnew regionalization procedure to split the projections into multiple regions.\nDenoising and regionalizing act to isolate each changepoint into its own\nregion, so that the classical univariate CUSUM statistic can be applied\nregion-wise to find all changepoints. Simulations show that our method\naccurately detects the number and locations of changepoints under many\ndifferent scenarios. These include light and heavy tailed data, data with\nsymmetric and skewed distributions, sparsely and densely sampled changepoints,\nand both mean and covariance changes. We show that our method outperforms a\nrecent multiple functional changepoint detector and several univariate\nchangepoint detectors applied to our proposed projections. We also show that\nthe MCI is more robust than existing approaches, and scales linearly with\nsample size. Finally, we demonstrate our method on a large time series of water\nvapor mixing ratio profiles from atmospheric emitted radiance interferometer\nmeasurements.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 00:36:11 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 16:29:36 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Harris", "Trevor", ""], ["Li", "Bo", ""], ["Tucker", "James Derek", ""]]}, {"id": "2008.02204", "submitter": "Sumi Seo", "authors": "Yi Li, Sumi Seo, Kyu Ha Lee", "title": "Bayesian Survival Analysis Using Gamma Processes with Adaptive Time\n  Partition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian semi-parametric analyses of time-to-event data, non-parametric\nprocess priors are adopted for the baseline hazard function or the cumulative\nbaseline hazard function for a given finite partition of the time axis.\nHowever, it would be controversial to suggest a general guideline to construct\nan optimal time partition. While a great deal of research has been done to\nrelax the assumption of the fixed split times for other non-parametric\nprocesses, to our knowledge, no methods have been developed for a gamma process\nprior, which is one of the most widely used in Bayesian survival analysis. In\nthis paper, we propose a new Bayesian framework for proportional hazards models\nwhere the cumulative baseline hazard function is modeled a priori by a gamma\nprocess. A key feature of the proposed framework is that the number and\nposition of interval cutpoints are treated as random and estimated based on\ntheir posterior distributions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 16:07:22 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Li", "Yi", ""], ["Seo", "Sumi", ""], ["Lee", "Kyu Ha", ""]]}, {"id": "2008.02243", "submitter": "Michael Robbins", "authors": "Michael W. Robbins", "title": "A flexible and efficient algorithm for joint imputation of general data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imputation of data with general structures (e.g., data with continuous,\nbinary, unordered categorical, and ordinal variables) is commonly performed\nwith fully conditional specification (FCS) instead of joint modeling. A key\ndrawback of FCS is that it does not invoke an appropriate data augmentation\nmechanism and as such convergence of the resulting Markov chain Monte Carlo\nprocedure is not assured. Methods that use joint modeling lack these drawbacks\nbut have not been efficiently implemented in data of general structures. We\naddress these issues by developing a new method, the so-called GERBIL\nalgorithm, that draws imputations from a latent joint multivariate normal model\nthat underpins the generally structured data. This model is constructed using a\nsequence of flexible conditional linear models that enables the resulting\nprocedure to be efficiently implemented on high dimensional datasets in\npractice. Simulations show that GERBIL performs well when compared to those\nthat utilize FCS. Furthermore, the new method is computationally efficient\nrelative to existing FCS procedures.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 17:17:19 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 17:12:06 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Robbins", "Michael W.", ""]]}, {"id": "2008.02341", "submitter": "William Artman", "authors": "William J. Artman, Ashkan Ertefaie, Kevin G. Lynch, James R. McKay", "title": "Bayesian Set of Best Dynamic Treatment Regimes and Sample Size\n  Determination for SMARTs with Binary Outcomes", "comments": "20 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main goals of sequential, multiple assignment, randomized trials\n(SMART) is to find the most efficacious design embedded dynamic treatment\nregimes. The analysis method known as multiple comparisons with the best (MCB)\nallows comparison between dynamic treatment regimes and identification of a set\nof optimal regimes in the frequentist setting for continuous outcomes, thereby,\ndirectly addressing the main goal of a SMART. In this paper, we develop a\nBayesian generalization to MCB for SMARTs with binary outcomes. Furthermore, we\nshow how to choose the sample size so that the inferior embedded DTRs are\nscreened out with a specified power. We compare log-odds between different DTRs\nusing their exact distribution without relying on asymptotic normality in\neither the analysis or the power calculation. We conduct extensive simulation\nstudies under two SMART designs and illustrate our method's application to the\nAdaptive Treatment for Alcohol and Cocaine Dependence (ENGAGE) trial.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 20:11:31 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Artman", "William J.", ""], ["Ertefaie", "Ashkan", ""], ["Lynch", "Kevin G.", ""], ["McKay", "James R.", ""]]}, {"id": "2008.02365", "submitter": "Junmo Song", "authors": "Junmo Song", "title": "Sequential change point test in the presence of outliers: the density\n  power divergence based approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we consider a problem of monitoring parameter changes\nparticularly in the presence of outliers. To propose a sequential procedure\nthat is robust against outliers, we use the density power divergence to derive\na detector and stopping time that make up our procedure. We first investigate\nthe asymptotic properties of our sequential procedure for i.i.d. sequences, and\nthen extend the proposed procedure to stationary time series models, where we\nprovide a set of sufficient conditions under which the proposed procedure has\nan asymptotically controlled size and consistency in power. As an application,\nour procedure is applied to the GARCH models. We demonstrate the validity and\nrobustness of the proposed procedure through a simulation study. Finally, two\nreal data analyses are provided to illustrate the usefulness of the proposed\nsequential procedure.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 21:03:47 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 04:39:54 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Song", "Junmo", ""]]}, {"id": "2008.02442", "submitter": "Yanyan Zhao", "authors": "Yanyan Zhao and Lei Sun", "title": "A stable and adaptive polygenic signal detection method based on\n  repeated sample splitting", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Focusing on polygenic signal detection in high dimensional genetic\nassociation studies of complex traits, we develop an adaptive test for\ngeneralized linear models to accommodate different alternatives. To facilitate\nvalid post-selection inference for high dimensional data, our study here\nadheres to the original sampling-splitting principle but does so, repeatedly,\nto increase stability of the inference. We show the asymptotic null\ndistributions of the proposed test for both fixed and diverging number of\nvariants. We also show the asymptotic properties of the proposed test under\nlocal alternatives, providing insights on why power gain attributed to variable\nselection and weighting can compensate for efficiency loss due to sample\nsplitting. We support our analytical findings through extensive simulation\nstudies and two applications. The proposed procedure is computationally\nefficient and has been implemented as the R package DoubleCauchy.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 03:24:12 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 19:25:53 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zhao", "Yanyan", ""], ["Sun", "Lei", ""]]}, {"id": "2008.02479", "submitter": "Mikkel Slot Nielsen", "authors": "Richard A. Davis and Mikkel S. Nielsen", "title": "Modeling of time series using random forests: theoretical developments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study asymptotic properties of random forests within the\nframework of nonlinear time series modeling. While random forests have been\nsuccessfully applied in various fields, the theoretical justification has not\nbeen considered for their use in a time series setting. Under mild conditions,\nwe prove a uniform concentration inequality for regression trees built on\nnonlinear autoregressive processes and, subsequently, we use this result to\nprove consistency for a large class of random forests. The results are\nsupported by various simulations.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 07:02:10 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Davis", "Richard A.", ""], ["Nielsen", "Mikkel S.", ""]]}, {"id": "2008.02509", "submitter": "Frank De Vocht", "authors": "Frank de Vocht, Srinivasa Vittal Katikireddi, Cheryl McQuire, Kate\n  Tilling, Matthew Hickman, Peter Craig", "title": "Conceptualising Natural and Quasi Experiments in Public Health", "comments": "28 pages, 1 figure, 1 table, 1 supplementary table", "journal-ref": null, "doi": "10.1186/s12874-021-01224-x", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Natural or quasi experiments are appealing for public health\nresearch because they enable the evaluation of events or interventions that are\ndifficult or impossible to manipulate experimentally, such as many policy and\nhealth system reforms. However, there remains ambiguity in the literature about\ntheir definition and how they differ from randomised controlled experiments and\nfrom other observational designs.\n  Methods: We conceptualise natural experiments in in the context of public\nhealth evaluations, align the study design to the Target Trial Framework, and\nprovide recommendation for improvement of their design and reporting.\n  Results: Natural experiment studies combine features of experiments and\nnon-experiments. They differ from RCTs in that exposure allocation is not\ncontrolled by researchers while they differ from other observational designs in\nthat they evaluate the impact of event or exposure changes. As a result they\nare, in theory, less susceptible to bias than other observational study\ndesigns. Importantly, the strength of causal inferences relies on the\nplausibility that the exposure allocation can be considered \"as-if randomised\".\nThe target trial framework provides a systematic basis for assessing the\nplausibility of such claims, and enables a structured method for assessing\nother design elements.\n  Conclusions: Natural experiment studies should be considered a distinct study\ndesign rather than a set of tools for analyses of non-randomised interventions.\nAlignment of natural experiments to the Target Trial framework will clarify the\nstrength of evidence underpinning claims about the effectiveness of public\nhealth interventions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 08:10:01 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["de Vocht", "Frank", ""], ["Katikireddi", "Srinivasa Vittal", ""], ["McQuire", "Cheryl", ""], ["Tilling", "Kate", ""], ["Hickman", "Matthew", ""], ["Craig", "Peter", ""]]}, {"id": "2008.02641", "submitter": "Benjamin Coleman", "authors": "Louis Abraham, Gary Becigneul, Benjamin Coleman, Bernhard Scholkopf,\n  Anshumali Shrivastava, Alexander Smola", "title": "Bloom Origami Assays: Practical Group Testing", "comments": "arXiv admin note: text overlap with arXiv:2005.06413", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem usually referred to as group testing in the context of\nCOVID-19. Given n samples collected from patients, how should we select and\ntest mixtures of samples to maximize information and minimize the number of\ntests? Group testing is a well-studied problem with several appealing\nsolutions, but recent biological studies impose practical constraints for\nCOVID-19 that are incompatible with traditional methods. Furthermore, existing\nmethods use unnecessarily restrictive solutions, which were devised for\nsettings with more memory and compute constraints than the problem at hand.\nThis results in poor utility. In the new setting, we obtain strong solutions\nfor small values of n using evolutionary strategies. We then develop a new\nmethod combining Bloom filters with belief propagation to scale to larger\nvalues of n (more than 100) with good empirical results. We also present a more\naccurate decoding algorithm that is tailored for specific COVID-19 settings.\nThis work demonstrates the practical gap between dedicated algorithms and\nwell-known generic solutions. Our efforts results in a new and practical\nmultiplex method yielding strong empirical performance without mixing more than\na chosen number of patients into the same probe. Finally, we briefly discuss\nadaptive methods, casting them into the framework of adaptive sub-modularity.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 19:31:41 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Abraham", "Louis", ""], ["Becigneul", "Gary", ""], ["Coleman", "Benjamin", ""], ["Scholkopf", "Bernhard", ""], ["Shrivastava", "Anshumali", ""], ["Smola", "Alexander", ""]]}, {"id": "2008.02658", "submitter": "Florian Pein", "authors": "Florian Pein, Annika Bartsch, Claudia Steinem, Axel Munk", "title": "Heterogeneous Idealization of Ion Channel Recordings -- Open Channel\n  Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model-free segmentation method for idealizing ion channel\nrecordings. This method is designed to deal with heterogeneity of measurement\nerrors. This in particular applies to open channel noise which, in general, is\nparticularly difficult to cope with for model-free approaches. Our methodology\nis able to deal with lowpass filtered data which provides a further\ncomputational challenge. To this end we propose a multiresolution testing\napproach, combined with local deconvolution to resolve the lowpass filter.\nSimulations and statistical theory confirm that the proposed idealization\nrecovers the underlying signal very accurately at presence of heterogeneous\nnoise, even when events are shorter than the filter length. The method is\ncompared to existing approaches in computer experiments and on real data. We\nfind that it is the only one which allows to identify openings of the PorB\nporine at two different temporal scales. An implementation is available as an R\npackage.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 13:50:05 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Pein", "Florian", ""], ["Bartsch", "Annika", ""], ["Steinem", "Claudia", ""], ["Munk", "Axel", ""]]}, {"id": "2008.02662", "submitter": "Julia Fukuyama", "authors": "Julia Fukuyama", "title": "Local biplots for multi-dimensional scaling, with application to the\n  microbiome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present local biplots, a an extension of the classic principal components\nbiplot to multi-dimensional scaling. Noticing that principal components biplots\nhave an interpretation as the Jacobian of a map from data space to the\nprincipal subspace, we define local biplots as the Jacobian of the analogous\nmap for multi-dimensional scaling. In the process, we show a close relationship\nbetween our local biplot axes, generalized Euclidean distances, and generalized\nprincipal components. In simulations and real data we show how local biplots\ncan shed light on what variables or combinations of variables are important for\nthe low-dimensional embedding provided by multi-dimensional scaling. They give\nparticular insight into a class of phylogenetically-informed distances commonly\nused in the analysis of microbiome data, showing that different variants of\nthese distances can be interpreted as implicitly smoothing the data along the\nphylogenetic tree and that the extent of this smoothing is variable.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 13:52:04 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Fukuyama", "Julia", ""]]}, {"id": "2008.02703", "submitter": "Zhichao Jiang", "authors": "Zhichao Jiang, Peng Ding", "title": "Identification of Causal Effects Within Principal Strata Using Auxiliary\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In causal inference, principal stratification is a framework for dealing with\na posttreatment intermediate variable between a treatment and an outcome, in\nwhich the principal strata are defined by the joint potential values of the\nintermediate variable. Because the principal strata are not fully observable,\nthe causal effects within them, also known as the principal causal effects, are\nnot identifiable without additional assumptions. Several previous empirical\nstudies leveraged auxiliary variables to improve the inference of principal\ncausal effects. We establish a general theory for identification and estimation\nof the principal causal effects with auxiliary variables, which provides a\nsolid foundation for statistical inference and more insights for model building\nin empirical research. In particular, we consider two commonly-used strategies\nfor principal stratification problems: principal ignorability, and the\nconditional independence between the auxiliary variable and the outcome given\nprincipal strata and covariates. For these two strategies, we give\nnon-parametric and semi-parametric identification results without modeling\nassumptions on the outcome. When the assumptions for neither strategies are\nplausible, we propose a large class of flexible parametric and semi-parametric\nmodels for identifying principal causal effects. Our theory not only\nestablishes formal identification results of several models that have been used\nin previous empirical studies but also generalizes them to allow for different\ntypes of outcomes and intermediate variables.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 15:12:09 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 22:43:55 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Jiang", "Zhichao", ""], ["Ding", "Peng", ""]]}, {"id": "2008.02906", "submitter": "Kengo Kamatani", "authors": "Alexandros Beskos and Kengo Kamatani", "title": "MCMC Algorithms for Posteriors on Matrix Spaces", "comments": "35 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Markov chain Monte Carlo (MCMC) algorithms for target distributions\ndefined on matrix spaces. Such an important sampling problem has yet to be\nanalytically explored. We carry out a major step in covering this gap by\ndeveloping the proper theoretical framework that allows for the identification\nof ergodicity properties of typical MCMC algorithms, relevant in such a\ncontext. Beyond the standard Random-Walk Metropolis (RWM) and preconditioned\nCrank--Nicolson (pCN), a contribution of this paper in the development of a\nnovel algorithm, termed the `Mixed' pCN (MpCN). RWM and pCN are shown not to be\ngeometrically ergodic for an important class of matrix distributions with heavy\ntails. In contrast, MpCN has very good empirical performance within this class.\nGeometric ergodicity for MpCN is not fully proven in this work, as some\nremaining drift conditions are quite challenging to obtain owing to the\ncomplexity of the state space. We do, however, make a lot of progress towards a\nproof, and show in detail the last steps left for future work. We illustrate\nthe computational performance of the various algorithms through simulation\nstudies, first for the trivial case of an Inverse-Wishart target, and then for\na challenging model arising in financial statistics.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 00:05:38 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 15:37:49 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Beskos", "Alexandros", ""], ["Kamatani", "Kengo", ""]]}, {"id": "2008.02915", "submitter": "Xiaowu Dai", "authors": "Xiaowu Dai and Lexin Li", "title": "Kernel Ordinary Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary differential equations (ODE) are widely used in modeling biological\nand physical processes in science. In this article, we propose a new\nreproducing kernel-based approach for estimation and inference of ODEs given\nthe noisy observations. We do not restrict the functional forms in ODE to be\nlinear or additive, and we allow pairwise interactions. We perform sparse\nestimation to select individual functionals, and construct confidence intervals\nfor the estimated signal trajectories. We establish the estimation optimality\nand selection consistency of kernel ODE under both the low-dimensional and\nhigh-dimensional settings, where the number of unknown functionals can be\nsmaller or larger than the sample size. Our proposal builds upon the smoothing\nspline analysis of variance (SS-ANOVA) framework, but tackles several important\nproblems that are not yet fully addressed, and thus extends the scope of\nexisting SS-ANOVA too. We demonstrate the efficacy of our method through\nnumerous ODE examples.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 00:24:29 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Dai", "Xiaowu", ""], ["Li", "Lexin", ""]]}, {"id": "2008.02926", "submitter": "Richard Warr", "authors": "M. S. Hamada, T. L. Graves, N. W. Hengartner, D. M. Higdon, A. V.\n  Huzurbazar, E. C. Lawrence, C. D. Linkletter, C. S. Reese, D. W. Scott, R. R.\n  Sitter, R. L. Warr, B. J. Williams", "title": "A Note on Using Discretized Simulated Data to Estimate Implicit\n  Likelihoods in Bayesian Analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a Bayesian inferential method where the likelihood for\na model is unknown but where data can easily be simulated from the model. We\ndiscretize simulated (continuous) data to estimate the implicit likelihood in a\nBayesian analysis employing a Markov chain Monte Carlo algorithm. Three\nexamples are presented as well as a small study on some of the method's\nproperties.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 01:22:17 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Hamada", "M. S.", ""], ["Graves", "T. L.", ""], ["Hengartner", "N. W.", ""], ["Higdon", "D. M.", ""], ["Huzurbazar", "A. V.", ""], ["Lawrence", "E. C.", ""], ["Linkletter", "C. D.", ""], ["Reese", "C. S.", ""], ["Scott", "D. W.", ""], ["Sitter", "R. R.", ""], ["Warr", "R. L.", ""], ["Williams", "B. J.", ""]]}, {"id": "2008.03033", "submitter": "Timo Dimitriadis", "authors": "Timo Dimitriadis, Tilmann Gneiting, Alexander I. Jordan", "title": "Evaluating probabilistic classifiers: Reliability diagrams and score\n  decompositions revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A probability forecast or probabilistic classifier is reliable or calibrated\nif the predicted probabilities are matched by ex post observed frequencies, as\nexamined visually in reliability diagrams. The classical binning and counting\napproach to plotting reliability diagrams has been hampered by a lack of\nstability under unavoidable, ad hoc implementation decisions. Here we introduce\nthe CORP approach, which generates provably statistically Consistent, Optimally\nbinned, and Reproducible reliability diagrams in an automated way. CORP is\nbased on non-parametric isotonic regression and implemented via the\nPool-adjacent-violators (PAV) algorithm - essentially, the CORP reliability\ndiagram shows the graph of the PAV- (re)calibrated forecast probabilities. The\nCORP approach allows for uncertainty quantification via either resampling\ntechniques or asymptotic theory, furnishes a new numerical measure of\nmiscalibration, and provides a CORP based Brier score decomposition that\ngeneralizes to any proper scoring rule. We anticipate that judicious uses of\nthe PAV algorithm yield improved tools for diagnostics and inference for a very\nwide range of statistical and machine learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 08:22:26 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Dimitriadis", "Timo", ""], ["Gneiting", "Tilmann", ""], ["Jordan", "Alexander I.", ""]]}, {"id": "2008.03073", "submitter": "Clement Lee", "authors": "Clement Lee and Emma Eastoe", "title": "From the power law to extreme value mixture distributions", "comments": "35 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power law is useful in describing count phenomena such as network degrees\nand word frequencies. With a single parameter, it captures the main feature\nthat the frequencies are linear on the log-log scale. Nevertheless, there have\nbeen criticisms of the power law, and various approaches have been proposed to\nresolve issues such as selecting the required threshold and quantifying the\nuncertainty around it, and to test hypotheses on whether the data could have\ncome from the power law. As extreme value theory generalises the (continuous)\npower law, it is natural to consider the former as a solution to these problems\naround the latter. In this paper, we propose two extreme value mixture\ndistributions, in one of which the power law is incorporated, without the need\nof pre-specifying the threshold. The proposed distributions are shown to fit\nthe data well, quantify the threshold uncertainty in a natural way, and\nsatisfactorily answer whether the power law is useful enough.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 10:14:34 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 17:22:02 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 11:38:47 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Lee", "Clement", ""], ["Eastoe", "Emma", ""]]}, {"id": "2008.03221", "submitter": "Zsigmond Benk\\H{o}", "authors": "Zsigmond Benk\\H{o}, Marcell Stippinger, Roberta Rehus, Attila Bencze,\n  D\\'aniel Fab\\'o, Bogl\\'arka Hajnal, Lor\\'and Er\\H{o}ss, Andr\\'as Telcs,\n  Zolt\\'an Somogyv\\'ari", "title": "Manifold-adaptive dimension estimation revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data dimensionality informs us about data complexity and sets limit on the\nstructure of successful signal processing pipelines. In this work we revisit\nand improve the manifold-adaptive Farahmand-Szepesv\\'ari-Audibert (FSA)\ndimension estimator, making it one of the best nearest neighbor-based dimension\nestimators available. We compute the probability density function of local FSA\nestimates, if the local manifold density is uniform. Based on the probability\ndensity function, we propose to use the median of local estimates as a basic\nglobal measure of intrinsic dimensionality, and we demonstrate the advantages\nof this asymptotically unbiased estimator over the previously proposed\nstatistics: the mode and the mean. Additionally, from the probability density\nfunction, we derive the maximum likelihood formula for global intrinsic\ndimensionality, if i.i.d. holds. We tackle edge and finite-sample effects with\nan exponential correction formula, calibrated on hypercube datasets. We compare\nthe performance of the corrected-median-FSA estimator with kNN estimators:\nmaximum likelihood (ML, Levina-Bickel) and two implementations of DANCo (R and\nmatlab). We show that corrected-median-FSA estimator beats the ML estimator and\nit is on equal footing with DANCo for standard synthetic benchmarks according\nto mean percentage error and error rate metrics. With the median-FSA algorithm,\nwe reveal diverse changes in the neural dynamics while resting state and during\nepileptic seizures. We identify brain areas with lower-dimensional dynamics\nthat are possible causal sources and candidates for being seizure onset zones.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 15:27:26 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 10:04:22 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Benk\u0151", "Zsigmond", ""], ["Stippinger", "Marcell", ""], ["Rehus", "Roberta", ""], ["Bencze", "Attila", ""], ["Fab\u00f3", "D\u00e1niel", ""], ["Hajnal", "Bogl\u00e1rka", ""], ["Er\u0151ss", "Lor\u00e1nd", ""], ["Telcs", "Andr\u00e1s", ""], ["Somogyv\u00e1ri", "Zolt\u00e1n", ""]]}, {"id": "2008.03235", "submitter": "Thibaud Rahier", "authors": "Thibaud Rahier, Am\\'elie H\\'eliou, Matthieu Martin, Christophe\n  Renaudin and Eustache Diemert", "title": "Individual Treatment Prescription Effect Estimation in a Low Compliance\n  Setting", "comments": "28 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual Treatment Effect (ITE) estimation is an extensively researched\nproblem, with applications in various domains. We model the case where there\nexists heterogeneous non-compliance to a randomly assigned treatment, a typical\nsituation in health (because of non-compliance to prescription) or digital\nadvertising (because of competition and ad blockers for instance). The lower\nthe compliance, the more the effect of treatment prescription, or individual\nprescription effect (IPE), signal fades away and becomes hard to estimate. We\npropose a new approach for the estimation of the IPE that takes advantage of\nobserved compliance information to prevent signal fading. Using the Structural\nCausal Model framework and do-calculus, we define a general mediated causal\neffect setting and propose a corresponding estimator which consistently\nrecovers the IPE with asymptotic variance guarantees. Finally, we conduct\nexperiments on both synthetic and real-world datasets that highlight the\nbenefit of the approach, which consistently improves state-of-the-art in low\ncompliance settings\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 15:53:00 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 15:30:12 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Rahier", "Thibaud", ""], ["H\u00e9liou", "Am\u00e9lie", ""], ["Martin", "Matthieu", ""], ["Renaudin", "Christophe", ""], ["Diemert", "Eustache", ""]]}, {"id": "2008.03271", "submitter": "Young Lee", "authors": "Young Lee, Wicher P. Bergsma, Marie-Abele C. Bind", "title": "Bayesian causal inference for count potential outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature for count modeling provides useful tools to conduct causal\ninference when outcomes take non-negative integer values. Applied to the\npotential outcomes framework, we link the Bayesian causal inference literature\nto statistical models for count data. We discuss the general architectural\nconsiderations for constructing the predictive posterior of the missing\npotential outcomes. Special considerations for estimating average treatment\neffects are discussed, some generalizing certain relationships and some not yet\nencountered in the causal inference literature.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 17:11:04 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Lee", "Young", ""], ["Bergsma", "Wicher P.", ""], ["Bind", "Marie-Abele C.", ""]]}, {"id": "2008.03282", "submitter": "Yetkin Tuac", "authors": "\\c{S}enay \\\"Ozdemir, Ye\\c{s}im G\\\"uney, Yetkin Tua\\c{c} and Olcay\n  Arslan", "title": "Empirical Likelihood Estimation for Linear Regression Models with AR(p)\n  Error Terms", "comments": "18 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression models are useful statistical tools to analyze data sets in\nseveral different fields. There are several methods to estimate the parameters\nof a linear regression model. These methods usually perform under normally\ndistributed and uncorrelated errors with zero mean and constant variance.\nHowever, for some data sets error terms may not satisfy these or some of these\nassumptions. If error terms are correlated, such as the regression models with\nautoregressive (AR(p)) error terms, the Conditional Maximum Likelihood (CML)\nunder normality assumption or the Least Square (LS) methods are often used to\nestimate the parameters of interest. For CML estimation a distributional\nassumption on error terms is needed to carry on estimation, but, in practice,\nsuch distributional assumptions on error terms may not be plausible. Therefore,\nin such cases some alternative distribution free methods are needed to conduct\nthe parameter estimation. In this paper, we propose to estimate the parameters\nof a linear regression model with AR(p) error term using the Empirical\nLikelihood (EL) method, which is one of the distribution free estimation\nmethods. A small simulation study and a numerical example are provided to\nevaluate the performance of the proposed estimation method over the CML method.\nThe results of simulation study show that the proposed estimators based on EL\nmethod are remarkably better than the estimators obtained from the CML method\nin terms of mean squared errors (MSE) and bias in almost all the simulation\nconfigurations. These findings are also confirmed by the results of the\nnumerical and real data examples.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 20:15:03 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["\u00d6zdemir", "\u015eenay", ""], ["G\u00fcney", "Ye\u015fim", ""], ["Tua\u00e7", "Yetkin", ""], ["Arslan", "Olcay", ""]]}, {"id": "2008.03288", "submitter": "Lin Liu L", "authors": "Lin Liu and Rajarshi Mukherjee and James M. Robins", "title": "Rejoinder: On nearly assumption-free tests of nominal confidence\n  interval coverage for causal parameters estimated by machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the rejoinder to the discussion by Kennedy, Balakrishnan and\nWasserman on the paper \"On nearly assumption-free tests of nominal confidence\ninterval coverage for causal parameters estimated by machine learning\"\npublished in Statistical Science.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 17:38:54 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Liu", "Lin", ""], ["Mukherjee", "Rajarshi", ""], ["Robins", "James M.", ""]]}, {"id": "2008.03349", "submitter": "Micha\\\"el Lalancette", "authors": "Micha\\\"el Lalancette, Sebastian Engelke, Stanislav Volgushev", "title": "Rank-based Estimation under Asymptotic Dependence and Independence, with\n  Applications to Spatial Extremes", "comments": "64 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate extreme value theory is concerned with modeling the joint tail\nbehavior of several random variables. Existing work mostly focuses on\nasymptotic dependence, where the probability of observing a large value in one\nof the variables is of the same order as observing a large value in all\nvariables simultaneously. However, there is growing evidence that asymptotic\nindependence is equally important in real world applications. Available\nstatistical methodology in the latter setting is scarce and not well understood\ntheoretically. We revisit non-parametric estimation and introduce rank-based\nM-estimators for parametric models that simultaneously work under asymptotic\ndependence and asymptotic independence, without requiring prior knowledge on\nwhich of the two regimes applies. Asymptotic normality of the proposed\nestimators is established under weak regularity conditions. We further show how\nbivariate estimators can be leveraged to obtain parametric estimators in\nspatial tail models, and again provide a thorough theoretical justification for\nour approach.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 19:46:21 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 18:18:13 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Lalancette", "Micha\u00ebl", ""], ["Engelke", "Sebastian", ""], ["Volgushev", "Stanislav", ""]]}, {"id": "2008.03379", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, H. M. de Oliveira, C. O. Cintra", "title": "Rounded Hartley Transform: A Quasi-involution", "comments": "6 pages. Manuscript originally published in 2002 at the International\n  Telecommunications Symposium ITS 2002. Readers are encouraged to access newer\n  results at arXiv:2007.02232", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.MM eess.IV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new multiplication-free transform derived from DHT is introduced: the RHT.\nInvestigations on the properties of the RHT led us to the concept of\nweak-inversion. Using new constructs, we show that RHT is not involutional like\nthe DHT, but exhibits quasi-involutional property, a new definition derived\nfrom the periodicity of matrices. Thus instead of using the actual inverse\ntransform, the RHT is viewed as an involutional transform, allowing the use of\ndirect (multiplication-free) to evaluate the inverse. A fast algorithm to\ncompute RHT is presented. This algorithm show embedded properties. We also\nextended RHT to the two-dimensional case. This permitted us to perform a\npreliminary analysis on the effects of RHT on images. Despite of some SNR loss,\nRHT can be very interesting for applications involving image monitoring\nassociated to decision making, such as military applications or medical\nimaging.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 21:10:19 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Cintra", "R. J.", ""], ["de Oliveira", "H. M.", ""], ["Cintra", "C. O.", ""]]}, {"id": "2008.03392", "submitter": "Kefei Liu", "authors": "Kefei Liu, Qi Long, Li Shen", "title": "Grouping effects of sparse CCA models in variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse canonical correlation analysis (SCCA) is a bi-multivariate\nassociation model that finds sparse linear combinations of two sets of\nvariables that are maximally correlated with each other. In addition to the\nstandard SCCA model, a simplified SCCA criterion which maixmizes the\ncross-covariance between a pair of canonical variables instead of their\ncross-correlation, is widely used in the literature due to its computational\nsimplicity. However, the behaviors/properties of the solutions of these two\nmodels remain unknown in theory. In this paper, we analyze the grouping effect\nof the standard and simplified SCCA models in variable selection. In\nhigh-dimensional settings, the variables often form groups with high\nwithin-group correlation and low between-group correlation. Our theoretical\nanalysis shows that for grouped variable selection, the simplified SCCA jointly\nselects or deselects a group of variables together, while the standard SCCA\nrandomly selects a few dominant variables from each relevant group of\ncorrelated variables. Empirical results on synthetic data and real imaging\ngenetics data verify the finding of our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 22:27:31 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Liu", "Kefei", ""], ["Long", "Qi", ""], ["Shen", "Li", ""]]}, {"id": "2008.03481", "submitter": "F. Richard Guo", "authors": "F. Richard Guo and Emilija Perkovi\\'c", "title": "Efficient Least Squares for Estimating Total Effects under Linearity and\n  Causal Sufficiency", "comments": "Edits to Introduction and Discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive linear structural equation models are widely used to postulate\ncausal mechanisms underlying observational data. In these models, each variable\nequals a linear combination of a subset of the remaining variables plus an\nerror term. When there is no unobserved confounding or selection bias, the\nerror terms are assumed to be independent. We consider estimating a total\ncausal effect in this setting. The causal structure is assumed to be known only\nup to a maximally oriented partially directed acyclic graph (MPDAG), a general\nclass of graphs that can represent a Markov equivalence class of directed\nacyclic graphs (DAGs) with added background knowledge. We propose a simple\nestimator based on recursive least squares, which can consistently estimate any\nidentified total causal effect, under point or joint intervention. We show that\nthis estimator is the most efficient among all regular estimators that are\nbased on the sample covariance, which includes covariate adjustment and the\nestimators employed by the joint-IDA algorithm. Notably, our result holds\nwithout assuming Gaussian errors.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 09:10:23 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 02:05:47 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 00:11:52 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Guo", "F. Richard", ""], ["Perkovi\u0107", "Emilija", ""]]}, {"id": "2008.03600", "submitter": "Andrii Babii", "authors": "Andrii Babii and Ryan T. Ball and Eric Ghysels and Jonas Striaukas", "title": "Machine Learning Panel Data Regressions with an Application to\n  Nowcasting Price Earnings Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces structured machine learning regressions for prediction\nand nowcasting with panel data consisting of series sampled at different\nfrequencies. Motivated by the empirical problem of predicting corporate\nearnings for a large cross-section of firms with macroeconomic, financial, and\nnews time series sampled at different frequencies, we focus on the sparse-group\nLASSO regularization. This type of regularization can take advantage of the\nmixed frequency time series panel data structures and we find that it\nempirically outperforms the unstructured machine learning methods. We obtain\noracle inequalities for the pooled and fixed effects sparse-group LASSO panel\ndata estimators recognizing that financial and economic data exhibit heavier\nthan Gaussian tails. To that end, we leverage on a novel Fuk-Nagaev\nconcentration inequality for panel data consisting of heavy-tailed\n$\\tau$-mixing processes which may be of independent interest in other\nhigh-dimensional panel data settings.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 21:12:33 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Babii", "Andrii", ""], ["Ball", "Ryan T.", ""], ["Ghysels", "Eric", ""], ["Striaukas", "Jonas", ""]]}, {"id": "2008.03604", "submitter": "Shuaimin Kang", "authors": "Shuaimin Kang, Krista Gile, Pedro Mateu-Gelabert, Honoria Guarino", "title": "Clustering Network Tree Data From Respondent-driven sampling with\n  application to opioid users in New York City", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is great interest in finding meaningful subgroups of attributed network\ndata. There are many available methods for clustering complete network.\nUnfortunately, much network data is collected through sampling, and therefore\nincomplete. Respondent-driven sampling (RDS) is a widely used method for\nsampling hard-to-reach human populations based on tracing links in the\nunderlying unobserved social network. The resulting data therefore have tree\nstructure representing a sub-sample of the network, along with many nodal\nattributes. In this paper, we introduce an approach to adjust mixture models\nfor general network clustering for samplings by RDS. We apply our model to data\non opioid users in New York City, and detect communities reflecting group\ncharacteristics of interest for intervention activities, including drug use\npatterns, social connections and other community variables\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 21:48:56 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Kang", "Shuaimin", ""], ["Gile", "Krista", ""], ["Mateu-Gelabert", "Pedro", ""], ["Guarino", "Honoria", ""]]}, {"id": "2008.03652", "submitter": "Tianxi Li", "authors": "Tianxi Li, Elizaveta Levina, Ji Zhu", "title": "Community models for networks observed through edge nominations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communities are a common and widely studied structure in networks, typically\nunder the assumption that the network is fully and correctly observed. In\npractice, network data are often collected by querying nodes about their\nconnections. In some settings, all edges of a sampled node will be recorded,\nand in others, a node may be asked to name its connections. These sampling\nmechanisms introduce noise and bias which can obscure the community structure\nand invalidate assumptions underlying standard community detection methods. We\npropose a general model for a class of network sampling mechanisms based on\nrecording edges via querying nodes, designed to improve community detection for\nnetwork data collected in this fashion. We model edge sampling probabilities as\na function of both individual preferences and community parameters, and show\ncommunity detection can be performed by spectral clustering under this general\nclass of models. We also propose, as a special case of the general framework, a\nparametric model for directed networks we call the nomination stochastic block\nmodel, which allows for meaningful parameter interpretations and can be fitted\nby the method of moments. Both spectral clustering and the method of moments in\nthis case are computationally efficient and come with theoretical guarantees of\nconsistency. We evaluate the proposed model in simulation studies on both\nunweighted and weighted networks and apply it to a faculty hiring dataset,\ndiscovering a meaningful hierarchy of communities among US business schools.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 04:53:13 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 03:25:11 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Li", "Tianxi", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "2008.03689", "submitter": "Huang Huang", "authors": "Huang Huang, Ying Sun, Marc G. Genton", "title": "Visualization of Covariance Structures for Multivariate Spatio-Temporal\n  Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of multivariate space-time data collected from monitoring\nnetworks and satellites or generated from numerical models has brought much\nattention to multivariate spatio-temporal statistical models, where the\ncovariance function plays a key role in modeling, inference, and prediction.\nFor multivariate space-time data, understanding the spatio-temporal\nvariability, within and across variables, is essential in employing a realistic\ncovariance model. Meanwhile, the complexity of generic covariances often makes\nmodel fitting very challenging, and simplified covariance structures, including\nsymmetry and separability, can reduce the model complexity and facilitate the\ninference procedure. However, a careful examination of these properties is\nneeded in real applications. In the work presented here, we formally define\nthese properties for multivariate spatio-temporal random fields and use\nfunctional data analysis techniques to visualize them, hence providing\nintuitive interpretations. We then propose a rigorous rank-based testing\nprocedure to conclude whether the simplified properties of covariance are\nsuitable for the underlying multivariate space-time data. The good performance\nof our method is illustrated through synthetic data, for which we know the true\nstructure. We also investigate the covariance of bivariate wind speed, a key\nvariable in renewable energy, over a coastal and an inland area in Saudi\nArabia.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 08:33:38 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Huang", "Huang", ""], ["Sun", "Ying", ""], ["Genton", "Marc G.", ""]]}, {"id": "2008.03733", "submitter": "Xin Zhang", "authors": "Lexin Li, Jing Zeng, Xin Zhang", "title": "Generalized Liquid Association Analysis for Multimodal Data Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal data are now prevailing in scientific research. A central question\nin multimodal integrative analysis is to understand how two data modalities\nassociate and interact with each other given another modality or demographic\nvariables. The problem can be formulated as studying the associations among\nthree sets of random variables, a question that has received relatively less\nattention in the literature. In this article, we propose a novel generalized\nliquid association analysis method, which offers a new and unique angle to this\nimportant class of problems of studying three-way associations. We extend the\nnotion of liquid association of \\citet{li2002LA} from the univariate setting to\nthe sparse, multivariate, and high-dimensional setting. We establish a\npopulation dimension reduction model, transform the problem to sparse Tucker\ndecomposition of a three-way tensor, and develop a higher-order orthogonal\niteration algorithm for parameter estimation. We derive the non-asymptotic\nerror bound and asymptotic consistency of the proposed estimator, while\nallowing the variable dimensions to be larger than and diverge with the sample\nsize. We demonstrate the efficacy of the method through both simulations and a\nmultimodal neuroimaging application for Alzheimer's disease research.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 14:22:42 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 03:10:24 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Li", "Lexin", ""], ["Zeng", "Jing", ""], ["Zhang", "Xin", ""]]}, {"id": "2008.03738", "submitter": "Shulei Wang", "authors": "Ruoqi Yu and Shulei Wang", "title": "Treatment Effects Estimation by Uniform Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, balancing covariates in different treatment groups\nis essential to estimate treatment effects. One of the most commonly used\nmethods for such purposes is weighting. The performance of this class of\nmethods usually depends on strong regularity conditions for the underlying\nmodel, which might not hold in practice. In this paper, we investigate\nweighting methods from a functional estimation perspective and argue that the\nweights needed for covariate balancing could differ from those needed for\ntreatment effects estimation under low regularity conditions. Motivated by this\nobservation, we introduce a new framework of weighting that directly targets\nthe treatment effects estimation. Unlike existing methods, the resulting\nestimator for a treatment effect under this new framework is a simple\nkernel-based $U$-statistic after applying a data-driven transformation to the\nobserved covariates. We characterize the theoretical properties of the new\nestimators of treatment effects under a nonparametric setting and show that\nthey are able to work robustly under low regularity conditions. The new\nframework is also applied to several numerical examples to demonstrate its\npractical merits.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 14:56:55 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 03:17:52 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Yu", "Ruoqi", ""], ["Wang", "Shulei", ""]]}, {"id": "2008.03786", "submitter": "Eben Kenah", "authors": "Eben Kenah", "title": "A potential outcomes approach to selection bias", "comments": "25 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection bias occurs when the association between exposure and disease in\nthe study population differs from that in the population eligible for\ninclusion. Along with confounding, it is one of the fundamental threats to the\nvalidity of epidemiologic research. In this paper, we propose a definition of\nselection bias in terms of potential outcomes. This approach generalizes the\nstructural approach of Hernan et al. (2004), which defines selection bias as a\ndistortion of the exposure-disease association that is caused by conditioning\non a collider. Both approaches agree in all situations where the structural\napproach identifies selection bias, but the potential outcomes approach\nidentifies selection bias in situations where the earlier approach does not.\nSelection bias defined by potential outcomes can involve a collider at\nexposure, a collider at disease, or no collider at all. This broader definition\nof selection bias does not depend on the parameterization of the association\nbetween exposure and disease, so it can be analyzed using nonparametric\nsingle-world intervention graphs (SWIGs) both under the null hypothesis and\naway from it. It provides a more nuanced interpretation of the role of\nrandomization in clinical trials, simplifies the analysis of matched studies\nand case cohort studies, and distinguishes more clearly between the estimation\nof causal effects within the study population and generalization to the\neligible population. This analysis of selection bias is an important\ntheoretical and practical application of SWIGs in epidemiology.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 18:54:33 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Kenah", "Eben", ""]]}, {"id": "2008.03838", "submitter": "Tonglin Zhang", "authors": "Tonglin Zhang and Ge Lin", "title": "Generalized k-Means in GLMs with Applications to the Outbreak of\n  COVID-19 in the United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized $k$-means can be incorporated with any similarity or\ndissimilarity measure for clustering. By choosing the dissimilarity measure as\nthe well known likelihood ratio or $F$-statistic, this work proposes a method\nbased on generalized $k$-means to group statistical models. Given the number of\nclusters $k$, the method is established under hypothesis tests between\nstatistical models. If $k$ is unknown, then the method can be combined with GIC\nto automatically select the best $k$ for clustering. The article investigates\nboth AIC and BIC as the special cases. Theoretical and simulation results show\nthat the number of clusters can be identified by BIC but not AIC. The resulting\nmethod for GLMs is used to group the state-level time series patterns for the\noutbreak of COVID-19 in the United States. A further study shows that the\nstatistical models between the clusters are significantly different from each\nother. This study confirms the result given by the proposed method based on\ngeneralized $k$-means.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 23:31:31 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhang", "Tonglin", ""], ["Lin", "Ge", ""]]}, {"id": "2008.03974", "submitter": "Anthony J  Webster", "authors": "Anthony J. Webster", "title": "Clustering parametric models and normally distributed data", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent UK Biobank study clustered 156 parameterised models associating risk\nfactors with common diseases, to identify shared causes of disease. Parametric\nmodels are often more familiar and interpretable than clustered data, can\nbuild-in prior knowledge, adjust for known confounders, and use marginalisation\nto emphasise parameters of interest. Estimates include a Maximum Likelihood\nEstimate (MLE) that is (approximately) normally distributed, and its\ncovariance. Clustering models rarely consider the covariances of data points,\nthat are usually unavailable. Here a clustering model is formulated that\naccounts for covariances of the data, and assumes that all MLEs in a cluster\nare the same. The log-likelihood is exactly calculated in terms of the fitted\nparameters, with the unknown cluster means removed by marginalisation. The\nprocedure is equivalent to calculating the Bayesian Information Criterion (BIC)\nwithout approximation, and can be used to assess the optimum number of clusters\nfor a given clustering algorithm. The log-likelihood has terms to penalise poor\nfits and model complexity, and can be maximised to determine the number and\ncomposition of clusters. Results can be similar to using the ad-hoc \"elbow\ncriterion\", but are less subjective. The model is also formulated as a\nDirichlet process mixture model (DPMM). The overall approach is equivalent to a\nmulti-layer algorithm that characterises features through the normally\ndistributed MLEs of a fitted model, and then clusters the normal distributions.\nExamples include simulated data, and clustering of diseases in UK Biobank data\nusing estimated associations with risk factors. The results can be applied\ndirectly to measured data and their estimated covariances, to the output from\nclustering models, or the DPMM implementation can be used to cluster fitted\nmodels directly.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 09:18:14 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 16:14:33 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 14:49:18 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Webster", "Anthony J.", ""]]}, {"id": "2008.04099", "submitter": "David Frazier", "authors": "David T. Frazier, Christopher Drovandi, Ruben Loaiza-Maya", "title": "Robust Approximate Bayesian Computation: An Adjustment Approach", "comments": "arXiv admin note: text overlap with arXiv:1904.04551", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to approximate Bayesian computation (ABC) that\nseeks to cater for possible misspecification of the assumed model. This new\napproach can be equally applied to rejection-based ABC and to popular\nregression adjustment ABC. We demonstrate that this new approach mitigates the\npoor performance of regression adjusted ABC that can eventuate when the model\nis misspecified. In addition, this new adjustment approach allows us to detect\nwhich features of the observed data can not be reliably reproduced by the\nassumed model. A series of simulated and empirical examples illustrate this new\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 06:05:54 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Frazier", "David T.", ""], ["Drovandi", "Christopher", ""], ["Loaiza-Maya", "Ruben", ""]]}, {"id": "2008.04257", "submitter": "Yun Li", "authors": "Yun Li, Irina Bondarenko, Michael R. Elliott, Timothy P. Hofer and\n  Jeremy M.G. Taylor", "title": "Using Multiple Imputation to Classify Potential Outcomes Subgroups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With medical tests becoming increasingly available, concerns about\nover-testing and over-treatment dramatically increase. Hence, it is important\nto understand the influence of testing on treatment selection in general\npractice. Most statistical methods focus on average effects of testing on\ntreatment decisions. However, this may be ill-advised, particularly for patient\nsubgroups that tend not to benefit from such tests. Furthermore, missing data\nare common, representing large and often unaddressed threats to the validity of\nstatistical methods. Finally, it is desirable to conduct analyses that can be\ninterpreted causally. We propose to classify patients into four potential\noutcomes subgroups, defined by whether or not a patient's treatment selection\nis changed by the test result and by the direction of how the test result\nchanges treatment selection. This subgroup classification naturally captures\nthe differential influence of medical testing on treatment selections for\ndifferent patients, which can suggest targets to improve the utilization of\nmedical tests. We can then examine patient characteristics associated with\npatient potential outcomes subgroup memberships. We used multiple imputation\nmethods to simultaneously impute the missing potential outcomes as well as\nregular missing values. This approach can also provide estimates of many\ntraditional causal quantities. We find that explicitly incorporating causal\ninference assumptions into the multiple imputation process can improve the\nprecision for some causal estimates of interest. We also find that bias can\noccur when the potential outcomes conditional independence assumption is\nviolated; sensitivity analyses are proposed to assess the impact of this\nviolation. We applied the proposed methods to examine the influence of 21-gene\nassay, the most commonly used genomic test, on chemotherapy selection among\nbreast cancer patients.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 16:59:52 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Li", "Yun", ""], ["Bondarenko", "Irina", ""], ["Elliott", "Michael R.", ""], ["Hofer", "Timothy P.", ""], ["Taylor", "Jeremy M. G.", ""]]}, {"id": "2008.04267", "submitter": "Suyash Gupta", "authors": "Maxime Cauchois, Suyash Gupta, Alnur Ali and John C. Duchi", "title": "Robust Validation: Confident Predictions Even When Distributions Shift", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the traditional viewpoint in machine learning and statistics assumes\ntraining and testing samples come from the same population, practice belies\nthis fiction. One strategy---coming from robust statistics and\noptimization---is thus to build a model robust to distributional perturbations.\nIn this paper, we take a different approach to describe procedures for robust\npredictive inference, where a model provides uncertainty estimates on its\npredictions rather than point predictions. We present a method that produces\nprediction sets (almost exactly) giving the right coverage level for any test\ndistribution in an $f$-divergence ball around the training population. The\nmethod, based on conformal inference, achieves (nearly) valid coverage in\nfinite samples, under only the condition that the training data be\nexchangeable. An essential component of our methodology is to estimate the\namount of expected future data shift and build robustness to it; we develop\nestimators and prove their consistency for protection and validity of\nuncertainty estimates under shifts. By experimenting on several large-scale\nbenchmark datasets, including Recht et al.'s CIFAR-v4 and ImageNet-V2 datasets,\nwe provide complementary empirical results that highlight the importance of\nrobust predictive validity.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 17:09:16 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Cauchois", "Maxime", ""], ["Gupta", "Suyash", ""], ["Ali", "Alnur", ""], ["Duchi", "John C.", ""]]}, {"id": "2008.04269", "submitter": "Abhimanyu Gupta", "authors": "Abhimanyu Gupta and Javier Hidalgo", "title": "Nonparametric prediction with spatial data", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a nonparametric prediction algorithm for spatial data. The\nalgorithm is based on a flexible exponential representation of the model\ncharacterized via the spectral density function. We provide theoretical results\ndemonstrating that our predictors have desired asymptotic properties. Finite\nsample performance is assessed in a Monte Carlo study that also compares our\nalgorithm to a rival nonparametric method based on the infinite AR\nrepresentation of the dynamics of the data. We apply our method to a real data\nset in an empirical example that predicts house prices in Los Angeles.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 17:10:01 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Gupta", "Abhimanyu", ""], ["Hidalgo", "Javier", ""]]}, {"id": "2008.04394", "submitter": "Eli Ben-Michael", "authors": "Eli Ben-Michael, Avi Feller, and Jesse Rothstein", "title": "Varying impacts of letters of recommendation on college admissions:\n  Approximate balancing weights for subgroup effects in observational studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a pilot program during the 2016-17 admissions cycle, the University of\nCalifornia, Berkeley invited many applicants for freshman admission to submit\nletters of recommendation. We use this pilot as the basis for an observational\nstudy of the impact of submitting letters of recommendation on subsequent\nadmission, with the goal of estimating how impacts vary across pre-defined\nsubgroups. Understanding this variation is challenging in observational\nstudies, however, because estimated impacts reflect both actual treatment\neffect variation and differences in covariate balance across groups. To address\nthis, we develop balancing weights that directly optimize for ``local balance''\nwithin subgroups while maintaining global covariate balance between treated and\ncontrol units. We then show that this approach has a dual representation as a\nform of inverse propensity score weighting with a hierarchical propensity score\nmodel. In the UC Berkeley pilot study, our proposed approach yields excellent\nlocal and global balance, unlike more traditional weighting methods, which fail\nto balance covariates within subgroups. We find that the impact of letters of\nrecommendation increases with the predicted probability of admission, with\nmixed evidence of differences for under-represented minority applicants.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 20:07:52 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 15:38:39 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ben-Michael", "Eli", ""], ["Feller", "Avi", ""], ["Rothstein", "Jesse", ""]]}, {"id": "2008.04443", "submitter": "Olivier Binette", "authors": "Olivier Binette and Rebecca C. Steorts", "title": "(Almost) All of Entity Resolution", "comments": "53 pages, includes supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether the goal is to estimate the number of people that live in a\ncongressional district, to estimate the number of individuals that have died in\nan armed conflict, or to disambiguate individual authors using bibliographic\ndata, all these applications have a common theme - integrating information from\nmultiple sources. Before such questions can be answered, databases must be\ncleaned and integrated in a systematic and accurate way, commonly known as\nrecord linkage, de-duplication, or entity resolution. In this article, we\nreview motivational applications and seminal papers that have led to the growth\nof this area. Specifically, we review the foundational work that began in the\n1940's and 50's that have led to modern probabilistic record linkage. We review\nclustering approaches to entity resolution, semi- and fully supervised methods,\nand canonicalization, which are being used throughout industry and academia in\napplications such as human rights, official statistics, medicine, citation\nnetworks, among others. Finally, we discuss current research topics of\npractical importance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 22:41:20 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Binette", "Olivier", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "2008.04456", "submitter": "Li-Pang Chen", "authors": "Li-Pang Chen", "title": "A note of feature screening via rank-based coefficient of correlation", "comments": "5 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature screening is useful and popular to detect informative predictors for\nultrahigh-dimensional data before developing proceeding statistical analysis or\nconstructing statistical models. While a large body of feature screening\nprocedures has been developed, most of them are restricted on examining either\ncontinuous or discrete responses. Moreover, even though many model-free feature\nscreening methods have been proposed, additional assumptions are imposed in\nthose methods to ensure their theoretical results. To address those\ndifficulties and provide simple implementation, in this paper we extend the\nrank-based coefficient of correlation proposed by Chatterjee (2020) to develop\nfeature screening procedure. We show that this new screening criterion is able\nto deal with continuous and discrete responses. Theoretically, sure screening\nproperty is established to justify the proposed method. Simulation studies\ndemonstrate that the predictors with nonlinear and oscillatory trajectory are\nsuccessfully detected regardless of the distribution of the response.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 23:58:30 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Chen", "Li-Pang", ""]]}, {"id": "2008.04475", "submitter": "Mar\\'ia Fernanda Gil-Leyva Villa", "authors": "Mar\\'ia F. Gil-Leyva, Rams\\'es H. Mena", "title": "Stick-breaking processes with exchangeable length variables", "comments": "Accepted for publication by the Journal of the American Statistical\n  Association. 44 pages, 11 figures, supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our object of study is the general class of stick-breaking processes with\nexchangeable length variables. These generalize well-known Bayesian\nnon-parametric priors in an unexplored direction. We give conditions to assure\nthe respective species sampling process is proper and the corresponding prior\nhas full support. For a rich sub-class we explain how, by tuning a single\n$[0,1]$-valued parameter, the stochastic ordering of the weights can be\nmodulated, and Dirichlet and Geometric priors can be recovered. A general\nformula for the distribution of the latent allocation variables is derived and\nan MCMC algorithm is proposed for density estimation purposes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 02:00:04 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 07:09:07 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gil-Leyva", "Mar\u00eda F.", ""], ["Mena", "Rams\u00e9s H.", ""]]}, {"id": "2008.04522", "submitter": "Yoshiko Hayashi", "authors": "Yoshiko Hayashi", "title": "Bayesian Analysis on Limiting the Student-$t$ Linear Regression Model", "comments": "12pages, 2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the outlier problem in linear regression models, the Student-$t$ linear\nregression model is one of the common methods for robust modeling and is widely\nadopted in the literature. However, most of them applies it without careful\ntheoretical consideration. This study provides the practically useful and quite\nsimple conditions to ensure that the Student-$t$ linear regression model is\nrobust against an outlier in the $y$-direction using regular variation theory.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 05:34:13 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 03:57:39 GMT"}, {"version": "v3", "created": "Sun, 25 Apr 2021 05:40:32 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Hayashi", "Yoshiko", ""]]}, {"id": "2008.04544", "submitter": "Myung Hwan Seo Prof.", "authors": "Myung Hwan Seo", "title": "Frequent or Systematic Changes? discussion on \"Detecting possibly\n  frequent change-points: Wild Binary Segmentation 2 and steepest-drop model\n  selection.\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss Fryzlewicz's (2020) that proposes WBS2.SDLL approach to detect\npossibly frequent changes in mean of a series. Our focus is on the potential\nissues related to the model misspecification. We present some numerical\nexamples such as the self-exciting threshold autoregression and the unit root\nprocess, that can be confused as a frequent change-points model.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 06:41:46 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Seo", "Myung Hwan", ""]]}, {"id": "2008.04631", "submitter": "Angela Andreella", "authors": "Angela Andreella, Livio Finos", "title": "Procrustes analysis for high-dimensional data", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Procrustes-based perturbation model \\citep{Goodall} allows to minimize\nthe Frobenius distance between matrices by similarity transformation. However,\nit suffers from non-identifiability, critical interpretation of the transformed\nmatrices, and non-applicability in high-dimensional data. We provide an\nextension of the perturbation model focused on the high-dimensional data\nframework, called the ProMises (Procrustes von Mises-Fisher) model. The\nill-posed and interpretability problems are solved by imposing a proper prior\ndistribution for the orthogonal matrix parameter, i.e., the von Mises-Fisher\ndistribution, which is a conjugate prior, resulting in a fast estimation\nprocess. Furthermore, we present the Efficient ProMises model for the\nhigh-dimensional framework, useful in neuroimaging, where the problem has much\nmore than three dimensions. We found a great improvement in functional Magnetic\nResonance Imaging connectivity analysis since the ProMises model permits to\nincorporate topological brain information in the alignment's estimation\nprocess.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 11:20:38 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 13:49:29 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 08:34:08 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Andreella", "Angela", ""], ["Finos", "Livio", ""]]}, {"id": "2008.04688", "submitter": "Piotr Zwiernik", "authors": "Steffen Lauritzen and Piotr Zwiernik", "title": "Locally associated graphical models and mixed convex exponential\n  families", "comments": "Supplementary material available at\n  http://econ.upf.edu/~piotr/supps/2020-LZ-golazo.zip", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of multivariate total positivity has proved to be useful in\nfinance and psychology but may be too restrictive in other applications. In\nthis paper we propose a concept of local association, where highly connected\ncomponents in a graphical model are positively associated and study its\nproperties. Our main motivation comes from gene expression data, where\ngraphical models have become a popular exploratory tool. The models are\ninstances of what we term mixed convex exponential families and we show that a\nmixed dual likelihood estimator has simple exact properties for such families\nas well as asymptotic properties similar to the maximum likelihood estimator.\nWe further relax the positivity assumption by penalizing negative partial\ncorrelations in what we term the positive graphical lasso. Finally, we develop\na GOLAZO algorithm based on block-coordinate descent that applies to a number\nof optimization procedures that arise in the context of graphical models,\nincluding the estimation problems described above. We derive results on\nexistence of the optimum for such problems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:21:45 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 09:22:54 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Lauritzen", "Steffen", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "2008.04692", "submitter": "Tetsuto Himeno", "authors": "Takayuki Yamada, Tetsuto Himeno, Annika Tillander, Tatjana Pavlenko", "title": "Test for mean matrix in GMANOVA model under heteroscedasticity and\n  non-normality for high-dimensional data", "comments": "Supplementary is available as ancillary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the testing bilateral linear hypothesis on the\nmean matrix in the context of the generalized multivariate analysis of variance\n(GMANOVA) model when the dimensions of the observed vector may exceed the\nsample size, the design may become unbalanced, the population may not be\nnormal, or the true covariance matrices may be unequal. The suggested testing\nmethodology can treat many problems such as the one- and two-way MANOVA tests,\nthe test for parallelism in profile analysis, etc., as specific ones. We\npropose a bias-corrected estimator of the Frobenius norm for the mean matrix,\nwhich is a key component of the test statistic. The null and non-null\ndistributions are derived under a general high-dimensional asymptotic framework\nthat allows the dimensionality to arbitrarily exceed the sample size of a\ngroup, thereby establishing consistency for the testing criterion. The accuracy\nof the proposed test in a finite sample is investigated through simulations\nconducted for several high-dimensional scenarios and various underlying\npopulation distributions in combination with different within-group covariance\nstructures. Finally, the proposed test is applied to a high-dimensional two-way\nMANOVA problem for DNA microarray data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:26:55 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 04:06:24 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Yamada", "Takayuki", ""], ["Himeno", "Tetsuto", ""], ["Tillander", "Annika", ""], ["Pavlenko", "Tatjana", ""]]}, {"id": "2008.04733", "submitter": "Zheng Zhao", "authors": "Zheng Zhao and Muhammad Emzir and Simo S\\\"arkk\\\"a", "title": "Deep State-Space Gaussian Processes", "comments": "Submitted to Statistics and Computing. The code will be revealed at\n  https://github.com/zgbkdlm/SS-DGP upon acceptance", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with a state-space approach to deep Gaussian process\n(DGP) regression. We construct the DGP by hierarchically putting transformed\nGaussian process (GP) priors on the length scales and magnitudes of the next\nlevel of Gaussian processes in the hierarchy. The idea of the state-space\napproach is to represent the DGP as a non-linear hierarchical system of linear\nstochastic differential equations (SDEs), where each SDE corresponds to a\nconditional GP. The DGP regression problem then becomes a state estimation\nproblem, and we can estimate the state efficiently with sequential methods by\nusing the Markov property of the state-space DGP. The computational complexity\nscales linearly with respect to the number of measurements. Based on this, we\nformulate state-space MAP as well as Bayesian filtering and smoothing solutions\nto the DGP regression problem. We demonstrate the performance of the proposed\nmodels and methods on synthetic non-stationary signals and apply the\nstate-space DGP to detection of the gravitational waves from LIGO measurements.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 14:50:07 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Zhao", "Zheng", ""], ["Emzir", "Muhammad", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2008.04734", "submitter": "Xinyu Zhang", "authors": "Xinyu Zhang", "title": "Error Bounds for Generalized Group Sparsity", "comments": "23 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2006.06172", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional statistical inference, sparsity regularizations have\nshown advantages in consistency and convergence rates for coefficient\nestimation. We consider a generalized version of Sparse-Group Lasso which\ncaptures both element-wise sparsity and group-wise sparsity simultaneously. We\nstate one universal theorem which is proved to obtain results on consistency\nand convergence rates for different forms of double sparsity regularization.\nThe universality of the results lies in an generalization of various\nconvergence rates for single regularization cases such as LASSO and group LASSO\nand also double regularization cases such as sparse-group LASSO. Our analysis\nidentifies a generalized norm of $\\epsilon$-norm, which provides a dual\nformulation for our double sparsity regularization.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 03:52:05 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Zhang", "Xinyu", ""]]}, {"id": "2008.05021", "submitter": "Vojtech Kejzlar", "authors": "Vojtech Kejzlar, Mookyong Son, Shrijita Bhattacharya, Tapabrata Maiti", "title": "A Fast and Calibrated Computer Model Emulator: An Empirical Bayes\n  Approach", "comments": null, "journal-ref": "Stat Comput 31, 49 (2021)", "doi": "10.1007/s11222-021-10024-8", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical models implemented on a computer have become the driving force\nbehind the acceleration of the cycle of scientific processes. This is because\ncomputer models are typically much faster and economical to run than physical\nexperiments. In this work, we develop an empirical Bayes approach to\npredictions of physical quantities using a computer model, where we assume that\nthe computer model under consideration needs to be calibrated and is\ncomputationally expensive. We propose a Gaussian process emulator and a\nGaussian process model for the systematic discrepancy between the computer\nmodel and the underlying physical process. This allows for closed-form and\neasy-to-compute predictions given by a conditional distribution induced by the\nGaussian processes. We provide a rigorous theoretical justification of the\nproposed approach by establishing posterior consistency of the estimated\nphysical process. The computational efficiency of the methods is demonstrated\nin an extensive simulation study and a real data example. The newly established\napproach makes enhanced use of computer models both from practical and\ntheoretical standpoints.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 22:26:03 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 15:59:41 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Kejzlar", "Vojtech", ""], ["Son", "Mookyong", ""], ["Bhattacharya", "Shrijita", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2008.05109", "submitter": "Xingchen Yu", "authors": "Xingchen Yu, Abel Rodriguez", "title": "A Bayesian Approach to Spherical Factor Analysis for Binary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor models are widely used across diverse areas of application for\npurposes that include dimensionality reduction, covariance estimation, and\nfeature engineering. Traditional factor models can be seen as an instance of\nlinear embedding methods that project multivariate observations onto a lower\ndimensional Euclidean latent space. This paper discusses a new class of\ngeometric embedding models for multivariate binary data in which the embedding\nspace correspond to a spherical manifold, with potentially unknown dimension.\nThe resulting models include traditional factor models as a special case, but\nprovide additional flexibility. Furthermore, unlike other techniques for\ngeometric embedding, the models are easy to interpret, and the uncertainty\nassociated with the latent features can be properly quantified. These\nadvantages are illustrated using both simulation studies and real data on\nvoting records from the U.S. Senate.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 04:55:18 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Yu", "Xingchen", ""], ["Rodriguez", "Abel", ""]]}, {"id": "2008.05191", "submitter": "Christof Str\\\"ahl", "authors": "Christof Str\\\"ahl", "title": "Log-concave Ridge Estimation", "comments": "29 pages, It is part of the author's PhD dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a density ridge search algorithm based on a novel density ridge\ndefinition. This definition is based on a conditional variance matrix and the\nmode in the lower dimensional subspace. It is compared to the subspace\nconstraint mean shift algorithm, based on the gradient and Hessian of the\nunderlying probability density function. We show the advantages of the new\nalgorithm in a simulation study and estimate galaxy filaments from a data set\nof the Baryon Oscillation Spectroscopic Survey.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 09:13:09 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Str\u00e4hl", "Christof", ""]]}, {"id": "2008.05337", "submitter": "Till Hoffmann", "authors": "Till Hoffmann and Nick S. Jones", "title": "Inference of a universal social scale and segregation measures using\n  social connectivity kernels", "comments": "Article: 23 pages, 3 figures. Supplementary material: 8 pages, 1\n  figure", "journal-ref": "J. R. Soc. Interface. 17: 20200638 (2020)", "doi": "10.1098/rsif.2020.0638", "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How people connect with one another is a fundamental question in the social\nsciences, and the resulting social networks can have a profound impact on our\ndaily lives. Blau offered a powerful explanation: people connect with one\nanother based on their positions in a social space. Yet a principled measure of\nsocial distance, allowing comparison within and between societies, remains\nelusive. We use the connectivity kernel of conditionally-independent edge\nmodels to develop a family of segregation statistics with desirable properties:\nthey offer an intuitive and universal characteristic scale on social space\n(facilitating comparison across datasets and societies), are applicable to\nmultivariate and mixed node attributes, and capture segregation at the level of\nindividuals, pairs of individuals, and society as a whole. We show that the\nsegregation statistics can induce a metric on Blau space (a space spanned by\nthe attributes of the members of society) and provide maps of two societies.\nUnder a Bayesian paradigm, we infer the parameters of the connectivity kernel\nfrom eleven ego-network datasets collected in four surveys in the United\nKingdom and United States. The importance of different dimensions of Blau space\nis similar across time and location, suggesting a macroscopically stable social\nfabric. Physical separation and age differences have the most significant\nimpact on segregation within friendship networks with implications for\nintergenerational mixing and isolation in later stages of life.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 14:15:53 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 10:48:23 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Hoffmann", "Till", ""], ["Jones", "Nick S.", ""]]}, {"id": "2008.05338", "submitter": "Eni Musta", "authors": "Eni Musta, Valentin Patilea and Ingrid Van Keilegom", "title": "A presmoothing approach for estimation in semiparametric mixture cure\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A challenge when dealing with survival analysis data is accounting for a cure\nfraction, meaning that some subjects will never experience the event of\ninterest. Mixture cure models have been frequently used to estimate both the\nprobability of being cured and the time to event for the susceptible subjects,\nby usually assuming a parametric (logistic) form of the incidence. We propose a\nnew estimation procedure for a parametric cure rate that relies on a\npreliminary smooth estimator and is independent of the model assumed for the\nlatency. We investigate the theoretical properties of the estimators and show\nthrough simulations that, in the logistic/Cox model, presmoothing leads to more\naccurate results compared to the maximum likelihood estimator. To illustrate\nthe practical use, we apply the new estimation procedure to two studies of\nmelanoma survival data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 14:16:46 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 14:19:01 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Musta", "Eni", ""], ["Patilea", "Valentin", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "2008.05367", "submitter": "Wei Deng", "authors": "Wei Deng, Qi Feng, Liyao Gao, Faming Liang, Guang Lin", "title": "Non-convex Learning via Replica Exchange Stochastic Gradient MCMC", "comments": "Accepted by ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replica exchange Monte Carlo (reMC), also known as parallel tempering, is an\nimportant technique for accelerating the convergence of the conventional Markov\nChain Monte Carlo (MCMC) algorithms. However, such a method requires the\nevaluation of the energy function based on the full dataset and is not scalable\nto big data. The na\\\"ive implementation of reMC in mini-batch settings\nintroduces large biases, which cannot be directly extended to the stochastic\ngradient MCMC (SGMCMC), the standard sampling method for simulating from deep\nneural networks (DNNs). In this paper, we propose an adaptive replica exchange\nSGMCMC (reSGMCMC) to automatically correct the bias and study the corresponding\nproperties. The analysis implies an acceleration-accuracy trade-off in the\nnumerical discretization of a Markov jump process in a stochastic environment.\nEmpirically, we test the algorithm through extensive experiments on various\nsetups and obtain the state-of-the-art results on CIFAR10, CIFAR100, and SVHN\nin both supervised learning and semi-supervised learning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 15:02:59 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 02:28:35 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 15:55:25 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Deng", "Wei", ""], ["Feng", "Qi", ""], ["Gao", "Liyao", ""], ["Liang", "Faming", ""], ["Lin", "Guang", ""]]}, {"id": "2008.05506", "submitter": "Raphael Saavedra", "authors": "Guilherme Bodin, Raphael Saavedra, Cristiano Fernandes, Alexandre\n  Street", "title": "ScoreDrivenModels.jl: a Julia Package for Generalized Autoregressive\n  Score Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score-driven models, also known as generalized autoregressive score models,\nrepresent a class of observation-driven time series models. They possess\npowerful properties, such as the ability to model different conditional\ndistributions and to consider time-varying parameters within a flexible\nframework. In this paper, we present ScoreDrivenModels.jl, an open-source Julia\npackage for modeling, forecasting, and simulating time series using the\nframework of score-driven models. The package is flexible with respect to model\ndefinition, allowing the user to specify the lag structure and which parameters\nare time-varying or constant. It is also possible to consider several\ndistributions, including Beta, Exponential, Gamma, Lognormal, Normal, Poisson,\nStudent's t, and Weibull. The provided interface is flexible, allowing\ninterested users to implement any desired distribution and parametrization.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 18:12:30 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 10:15:58 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Bodin", "Guilherme", ""], ["Saavedra", "Raphael", ""], ["Fernandes", "Cristiano", ""], ["Street", "Alexandre", ""]]}, {"id": "2008.05574", "submitter": "James Theiler", "authors": "James Theiler", "title": "A few brief notes on the equivalence of two expressions for statistical\n  significance in point source detections", "comments": "5 pages, no figures; written in 1998, and never published (until now)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of point source detection in Poisson-limited count maps has been\naddressed by two recent papers [M. Lampton, ApJ 436, 784 (1994); D. E.\nAlexandreas, et al., Nucl. Instr. Meth. Phys. Res. A 328, 570 (1993)]. Both\npapers consider the problem of determining whether there are significantly more\ncounts in a source region than would be expected given the number of counts\nobserved in a background region. The arguments in the two papers are quite\ndifferent (one takes a Bayesian point of view and the other does not), and the\nsuggested formulas for computing p-values appear to be different as well. It is\nshown here that the expressions provided by the authors of these two articles\nare in fact equivalent.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 21:11:53 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Theiler", "James", ""]]}, {"id": "2008.05578", "submitter": "Lulu Kang", "authors": "Yiou Li and Lulu Kang and Xiao Huang", "title": "Covariate Balancing Based on Kernel Density Estimates for Controlled\n  Experiments", "comments": "26 page, 2 figures, 1 table", "journal-ref": null, "doi": "10.1080/24754269.2021.1878742", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlled experiments are widely used in many applications to investigate\nthe causal relationship between input factors and experimental outcomes. A\ncompletely randomized design is usually used to randomly assign treatment\nlevels to experimental units. When covariates of the experimental units are\navailable, the experimental design should achieve covariate balancing among the\ntreatment groups, such that the statistical inference of the treatment effects\nis not confounded with any possible effects of covariates. However, covariate\nimbalance often exists, because the experiment is carried out based on a single\nrealization of the complete randomization. It is more likely to occur and\nworsen when the size of the experimental units is small or moderate. In this\npaper, we introduce a new covariate balancing criterion, which measures the\ndifferences between kernel density estimates of the covariates of treatment\ngroups. To achieve covariate balance before the treatments are randomly\nassigned, we partition the experimental units by minimizing the criterion, then\nrandomly assign the treatment levels to the partitioned groups. Through\nnumerical examples, we show that the proposed partition approach can improve\nthe accuracy of the difference-in-mean estimator and outperforms the complete\nrandomization and rerandomization approaches.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 21:34:34 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 16:50:36 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Li", "Yiou", ""], ["Kang", "Lulu", ""], ["Huang", "Xiao", ""]]}, {"id": "2008.05606", "submitter": "Shenyi Pan", "authors": "Shenyi Pan, Harry Joe, Guofu Li", "title": "Conditional Inferences Based on Vine Copulas with Applications to Credit\n  Spread Data of Corporate Bonds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the dependence relationship of credit spreads of corporate\nbonds is important for risk management. Vine copula models with tail dependence\nare used to analyze a credit spread dataset of Chinese corporate bonds,\nunderstand the dependence among different sectors and perform conditional\ninferences. It is shown how the effect of tail dependence affects risk\ntransfer, or the conditional distributions given one variable is extreme. Vine\ncopula models also provide more accurate cross prediction results compared with\nlinear regressions. These conditional inference techniques are a statistical\ncontribution for analysis of bond credit spreads of investment portfolios\nconsisting of corporate bonds from various sectors.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 23:17:41 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Pan", "Shenyi", ""], ["Joe", "Harry", ""], ["Li", "Guofu", ""]]}, {"id": "2008.05649", "submitter": "Qihuang Zhang", "authors": "Qihuang Zhang and Grace Y. Yi", "title": "Sensitivity Analysis of Error-Contaminated Time Series Data under\n  Autoregressive Models with Application of COVID-19 Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive (AR) models are useful tools in time series analysis.\nInferences under such models are distorted in the presence of measurement\nerror, which is very common in practice. In this article, we establish\nanalytical results for quantifying the biases of the parameter estimation in AR\nmodels if the measurement error effects are neglected. We propose two\nmeasurement error models to describe different processes of data contamination.\nAn estimating equation approach is proposed for the estimation of the model\nparameters with measurement error effects accounted for. We further discuss\nforecasting using the proposed method. Our work is inspired by COVID-19 data,\nwhich are error-contaminated due to multiple reasons including the asymptomatic\ncases and varying incubation periods. We implement our proposed method by\nconducting sensitivity analyses and forecasting of the mortality rate of\nCOVID-19 over time for the four most populated provinces in Canada. The results\nsuggest that incorporating or not incorporating measurement error effects\nyields rather different results for parameter estimation and forecasting.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 02:19:26 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Zhang", "Qihuang", ""], ["Yi", "Grace Y.", ""]]}, {"id": "2008.05854", "submitter": "Elias Raninen", "authors": "Elias Raninen and David E. Tyler and Esa Ollila", "title": "Linear pooling of sample covariance matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider covariance matrix estimation in a setting, where there are\nmultiple classes (populations). We propose to estimate each class covariance\nmatrix as a linear combination of all of the class sample covariance matrices.\nThis approach is shown to reduce the estimation error when the sample sizes are\nlimited and the true class covariance matrices share a similar structure. We\ndevelop an effective method for estimating the minimum mean squared error\ncoefficients for the linear combination when the samples are drawn from\n(unspecified) elliptically symmetric distributions with finite fourth-order\nmoments. To this end, we utilize the spatial sign covariance matrix, which we\nshow (under rather general conditions) to be a consistent estimator of the\ntrace normalized covariance matrix as both the sample size and the dimension\ngrow to infinity. We also show how the proposed method can be used in choosing\nthe regularization parameters for multiple target matrices in a single class\ncovariance matrix estimation problem. We assess the proposed method via\nnumerical simulation studies including an application in global minimum\nvariance portfolio optimization using real stock data, where it is shown to\noutperform several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 12:20:18 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Raninen", "Elias", ""], ["Tyler", "David E.", ""], ["Ollila", "Esa", ""]]}, {"id": "2008.05909", "submitter": "Yan Chu", "authors": "Tongtong Huang, Yan Chu, Shayan Shams, Yejin Kim, Genevera Allen,\n  Ananth V Annapragada, Devika Subramanian, Ioannis Kakadiaris, Assaf Gottlieb,\n  Xiaoqian Jiang", "title": "Population stratification enables modeling effects of reopening policies\n  on mortality and hospitalization rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: We study the influence of local reopening policies on the\ncomposition of the infectious population and their impact on future\nhospitalization and mortality rates. Materials and Methods: We collected\ndatasets of daily reported hospitalization and cumulative morality of COVID 19\nin Houston, Texas, from May 1, 2020 until June 29, 2020. These datasets are\nfrom multiple sources (USA FACTS, Southeast Texas Regional Advisory Council\nCOVID 19 report, TMC daily news, and New York Times county level mortality\nreporting). Our model, risk stratified SIR HCD uses separate variables to model\nthe dynamics of local contact (e.g., work from home) and high contact (e.g.,\nwork on site) subpopulations while sharing parameters to control their\nrespective $R_0(t)$ over time. Results: We evaluated our models forecasting\nperformance in Harris County, TX (the most populated county in the Greater\nHouston area) during the Phase I and Phase II reopening. Not only did our model\noutperform other competing models, it also supports counterfactual analysis to\nsimulate the impact of future policies in a local setting, which is unique\namong existing approaches. Discussion: Local mortality and hospitalization are\nsignificantly impacted by quarantine and reopening policies. No existing model\nhas directly accounted for the effect of these policies on local trends in\ninfections, hospitalizations, and deaths in an explicit and explainable manner.\nOur work is an attempt to close this important technical gap to support\ndecision making. Conclusion: Despite several limitations, we think it is a\ntimely effort to rethink about how to best model the dynamics of pandemics\nunder the influence of reopening policies.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 04:39:59 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Huang", "Tongtong", ""], ["Chu", "Yan", ""], ["Shams", "Shayan", ""], ["Kim", "Yejin", ""], ["Allen", "Genevera", ""], ["Annapragada", "Ananth V", ""], ["Subramanian", "Devika", ""], ["Kakadiaris", "Ioannis", ""], ["Gottlieb", "Assaf", ""], ["Jiang", "Xiaoqian", ""]]}, {"id": "2008.05926", "submitter": "Berent {\\AA}nund Str{\\o}mnes Lunde", "authors": "Berent {\\AA}nund Str{\\o}mnes Lunde, Tore Selland Kleppe, Hans Julius\n  Skaug", "title": "An information criterion for automatic gradient tree boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An information theoretic approach to learning the complexity of\nclassification and regression trees and the number of trees in gradient tree\nboosting is proposed. The optimism (test loss minus training loss) of the\ngreedy leaf splitting procedure is shown to be the maximum of a\nCox-Ingersoll-Ross process, from which a generalization-error based information\ncriterion is formed. The proposed procedure allows fast local model selection\nwithout cross validation based hyper parameter tuning, and hence efficient and\nautomatic comparison among the large number of models performed during each\nboosting iteration. Relative to xgboost, speedups on numerical experiments\nranges from around 10 to about 1400, at similar predictive-power measured in\nterms of test-loss.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 14:24:27 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Lunde", "Berent \u00c5nund Str\u00f8mnes", ""], ["Kleppe", "Tore Selland", ""], ["Skaug", "Hans Julius", ""]]}, {"id": "2008.05951", "submitter": "Antonio Remiro-Az\\'ocar Mr.", "authors": "Antonio Remiro-Az\\'ocar, Anna Heath, Gianluca Baio", "title": "Marginalization of Regression-Adjusted Treatment Effects in Indirect\n  Comparisons with Limited Patient-Level Data", "comments": "87 pages (28 of supplementary appendices and references), 5 figures.\n  arXiv admin note: text overlap with arXiv:2004.14800", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population adjustment methods such as matching-adjusted indirect comparison\n(MAIC), based on propensity score weighting, are increasingly used to compare\nmarginal treatment effects when there are cross-trial differences in effect\nmodifiers and limited patient-level data. Current outcome regression-based\nalternatives target a conditional treatment effect that is incompatible in the\nindirect comparison. When adjusting for covariates, one must integrate or\naverage the conditional estimate over the population of interest to recover a\ncompatible marginal treatment effect. We propose a marginalization method based\non the ideas of parametric G-computation that can be easily applied where the\noutcome regression is a generalized linear model or a Cox model. In addition,\nwe introduce a novel general-purpose method based on the ideas underlying\nmultiple imputation, which we term multiple imputation marginalization (MIM)\nand is applicable to a wide range of models, including parametric survival\nmodels. Both methods can accommodate a Bayesian statistical framework which\nnaturally integrates the analysis into a probabilistic framework, typically\nrequired for health technology assessment. A simulation study provides\nproof-of-principle for the methods and benchmarks their performance against\nMAIC and the conventional outcome regression. The simulations are based on\nscenarios with binary outcomes and continuous covariates, with the log-odds\nratio as the measure of effect. The marginalized outcome regression approaches\nachieve more precise and more accurate estimates than MAIC, particularly when\ncovariate overlap is poor, and yield unbiased treatment effect estimates under\nno failures of assumptions. Furthermore, the marginalized covariate-adjusted\nestimates provide greater precision than the conditional estimates produced by\nthe conventional outcome regression.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 10:56:15 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 09:55:24 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 18:00:32 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 21:12:55 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Remiro-Az\u00f3car", "Antonio", ""], ["Heath", "Anna", ""], ["Baio", "Gianluca", ""]]}, {"id": "2008.05980", "submitter": "Adam Kapelner", "authors": "Abba M. Krieger, David Azriel, Michael Sklar and Adam Kapelner", "title": "Improving the Power of the Randomization Test", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of evaluating designs for a two-arm randomized\nexperiment with the criterion being the power of the randomization test for the\none-sided null hypothesis. Our evaluation assumes a response that is linear in\none observed covariate, an unobserved component and an additive treatment\neffect where the only randomness comes from the treatment allocations. It is\nwell-known that the power depends on the allocations' imbalance in the observed\ncovariate and this is the reason for the classic restricted designs such as\nrerandomization. We show that power is also affected by two other design\nchoices: the number of allocations in the design and the degree of linear\ndependence among the allocations. We prove that the more allocations, the\nhigher the power and the lower the variability in the power. Designs that\nfeature greater independence of allocations are also shown to have higher\nperformance.\n  Our theoretical findings and extensive simulation studies imply that the\ndesigns with the highest power provide thousands of highly independent\nallocations that each provide nominal imbalance in the observed covariates.\nThese high powered designs exhibit less randomization than complete\nrandomization and more randomization than recently proposed designs based on\nnumerical optimization. Model choices for a practicing experimenter are\nrerandomization and greedy pair switching, where both outperform complete\nrandomization and numerical optimization. The tradeoff we find also provides a\nmeans to specify the imbalance threshold parameter when rerandomizing.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 15:55:41 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Krieger", "Abba M.", ""], ["Azriel", "David", ""], ["Sklar", "Michael", ""], ["Kapelner", "Adam", ""]]}, {"id": "2008.05990", "submitter": "Thomas Nagler", "authors": "Thomas Nagler, Daniel Kr\\\"uger, Aleksey Min", "title": "Stationary vine copula models for multivariate time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time series exhibit two types of dependence: across variables\nand across time points. Vine copulas are graphical models for the dependence\nand can conveniently capture both types of dependence in the same model. We\nderive the maximal class of graph structures that guarantee stationarity under\na natural and verifiable condition called translation invariance. We propose\ncomputationally efficient methods for estimation, simulation, prediction, and\nuncertainty quantification and show their validity by asymptotic results and\nsimulations. The theoretical results allow for misspecified models and, even\nwhen specialized to the iid case, go beyond what is available in the\nliterature. Their proofs are based on new results for general semiparametric\nmethod-of-moment estimators, which shall be of independent interest. The new\nmodel class is illustrated by an application to forecasting returns of a\nportfolio of 20 stocks, where they show excellent forecast performance. The\npaper is accompanied by an open source software implementation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 16:07:48 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 10:11:41 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Nagler", "Thomas", ""], ["Kr\u00fcger", "Daniel", ""], ["Min", "Aleksey", ""]]}, {"id": "2008.06017", "submitter": "Ilya Shpitser", "authors": "Ilya Shpitser, Thomas S. Richardson, James M. Robins", "title": "Multivariate Counterfactual Systems And Causal Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among Judea Pearl's many contributions to Causality and Statistics, the\ngraphical d-separation} criterion, the do-calculus and the mediation formula\nstand out. In this chapter we show that d-separation} provides direct insight\ninto an earlier causal model originally described in terms of potential\noutcomes and event trees. In turn, the resulting synthesis leads to a\nsimplification of the do-calculus that clarifies and separates the underlying\nconcepts, and a simple counterfactual formulation of a complete identification\nalgorithm in causal models with hidden variables.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 17:11:21 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Shpitser", "Ilya", ""], ["Richardson", "Thomas S.", ""], ["Robins", "James M.", ""]]}, {"id": "2008.06019", "submitter": "Ilya Shpitser", "authors": "James M. Robins, Thomas S. Richardson, and Ilya Shpitser", "title": "An Interventionist Approach to Mediation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Judea Pearl's insight that, when errors are assumed independent, the Pure\n(aka Natural) Direct Effect (PDE) is non-parametrically identified via the\nMediation Formula was `path-breaking' in more than one sense! In the same paper\nPearl described a thought-experiment as a way to motivate the PDE. Analysis of\nthis experiment led Robins \\& Richardson to a novel way of conceptualizing\ndirect effects in terms of interventions on an expanded graph in which\ntreatment is decomposed into multiple separable components. We further develop\nthis novel theory here, showing that it provides a self-contained framework for\ndiscussing mediation without reference to cross-world (nested) counterfactuals\nor interventions on the mediator. The theory preserves the dictum `no causation\nwithout manipulation' and makes questions of mediation empirically testable in\nfuture Randomized Controlled Trials. Even so, we prove the interventionist and\nnested counterfactual approaches remain tightly coupled under a Non-Parametric\nStructural Equation Model except in the presence of a `recanting witness.' In\nfact, our analysis also leads to a simple sound and complete algorithm for\ndetermining identification in the (non-interventionist) theory of path-specific\ncounterfactuals.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 17:18:29 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Robins", "James M.", ""], ["Richardson", "Thomas S.", ""], ["Shpitser", "Ilya", ""]]}, {"id": "2008.06130", "submitter": "Neil Shephard", "authors": "Neil Shephard", "title": "An estimator for predictive regression: reliable inference for financial\n  economics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating linear regression using least squares and reporting robust\nstandard errors is very common in financial economics, and indeed, much of the\nsocial sciences and elsewhere. For thick tailed predictors under\nheteroskedasticity this recipe for inference performs poorly, sometimes\ndramatically so. Here, we develop an alternative approach which delivers an\nunbiased, consistent and asymptotically normal estimator so long as the means\nof the outcome and predictors are finite. The new method has standard errors\nunder heteroskedasticity which are easy to reliably estimate and tests which\nare close to their nominal size. The procedure works well in simulations and in\nan empirical exercise. An extension is given to quantile regression.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 23:23:25 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Shephard", "Neil", ""]]}, {"id": "2008.06190", "submitter": "Kyoungjae Lee", "authors": "Kyoungjae Lee and Xuan Cao", "title": "Bayesian joint inference for multiple directed acyclic graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, data often arise from multiple groups that may share\nsimilar characteristics. A joint estimation method that models several groups\nsimultaneously can be more efficient than estimating parameters in each group\nseparately. We focus on unraveling the dependence structures of data based on\ndirected acyclic graphs and propose a Bayesian joint inference method for\nmultiple graphs. To encourage similar dependence structures across all groups,\na Markov random field prior is adopted. We establish the joint selection\nconsistency of the fractional posterior in high dimensions, and benefits of the\njoint inference are shown under the common support assumption. This is the\nfirst Bayesian method for joint estimation of multiple directed acyclic graphs.\nThe performance of the proposed method is demonstrated using simulation\nstudies, and it is shown that our joint inference outperforms other\ncompetitors. We apply our method to an fMRI data for simultaneously inferring\nmultiple brain functional networks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 04:47:03 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Lee", "Kyoungjae", ""], ["Cao", "Xuan", ""]]}, {"id": "2008.06213", "submitter": "Seonghyun Jeong", "authors": "Seonghyun Jeong, Taeyoung Park, and David A. van Dyk", "title": "Bayesian model selection in additive partial linear models via locally\n  adaptive splines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a model selection problem for additive partial linear models that\nprovide a flexible framework allowing both linear and nonlinear additive\ncomponents. In practice, it is challenging to determine which additive\ncomponents should be excluded from the model and simultaneously determine\nwhether nonzero nonlinear components can be further simplified to linear\ncomponents in the final model. In this paper, we propose a new Bayesian\nframework for data-driven model selection by conducting careful model\nspecification, including the choice of prior distribution and of nonparametric\nmodel for the nonlinear additive components, and propose new sophisticated\ncomputational strategies. Our models, methods, and algorithms are deployed on a\nsuite of numerical studies and applied to a nutritional epidemiology study. The\nnumerical results show that the proposed methodology outperforms previously\navailable methodologies in terms of effective sample sizes of the Markov chain\nsamplers and the overall misclassification rates, especially in\nhigh-dimensional and large-sample cases.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 07:15:51 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Jeong", "Seonghyun", ""], ["Park", "Taeyoung", ""], ["van Dyk", "David A.", ""]]}, {"id": "2008.06234", "submitter": "Peter B\\\"uhlmann", "authors": "Peter B\\\"uhlmann, Domagoj \\'Cevid", "title": "Deconfounding and Causal Regularization for Stability and External\n  Validity", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review some recent work on removing hidden confounding and causal\nregularization from a unified viewpoint. We describe how simple and\nuser-friendly techniques improve stability, replicability and distributional\nrobustness in heterogeneous data. In this sense, we provide additional thoughts\nto the issue on concept drift, raised by Efron (2020), when the data generating\ndistribution is changing.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 08:08:41 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["B\u00fchlmann", "Peter", ""], ["\u0106evid", "Domagoj", ""]]}, {"id": "2008.06327", "submitter": "Ji\\v{r}\\'i Dvo\\v{r}\\'ak", "authors": "Ji\\v{r}\\'i Dvo\\v{r}\\'ak and Tom\\'a\\v{s} Mrkvi\\v{c}ka", "title": "Graphical tests of independence for general distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two model-free, permutation-based tests of independence between a\npair of random variables. The tests can be applied to samples from any\nbivariate distribution: continuous, discrete or mixture of those, with light\ntails or heavy tails, \\ldots The tests take advantage of the recent development\nof the global envelope tests in the context of spatial statistics. Apart from\nthe broad applicability of the tests, their main benefit lies in the graphical\ninterpretation of the test outcome: in case of rejection of the null hypothesis\nof independence, the combinations of quantiles in the two marginals are\nindicated for which the deviation from independence is significant. This\ninformation can be used to gain more insight into the properties of the\nobserved data and as a guidance for proposing more complicated models and\nhypotheses. We assess the performance of the proposed tests in a simulation\nstudy and compare them to several well-established tests of independence.\nFurthermore, we illustrate the use of the tests and the interpretation of the\ntest outcome in two real datasets consisting of meteorological reports (daily\nmean temperature and total daily precipitation, having an atomic component at 0\nmillimeters) and road accidents reports (type of road and the weather\nconditions, both variables having categorical distribution).\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 12:37:50 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Dvo\u0159\u00e1k", "Ji\u0159\u00ed", ""], ["Mrkvi\u010dka", "Tom\u00e1\u0161", ""]]}, {"id": "2008.06344", "submitter": "Maria D. Ruiz-Medina", "authors": "A. Torres-Signes, M.P. Fr\\'ias and M.D. Ruiz-Medina", "title": "COVID-19 mortality analysis from soft-data multivariate curve regression\n  and machine learning", "comments": "This paper is currently submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multiple objective space-time forecasting approach is presented involving\ncyclical curve log-regression, and multivariate time series spatial residual\ncorrelation analysis. Specifically, the mean quadratic loss function is\nminimized in the framework of trigonometric regression. While, in our\nsubsequent spatial residual correlation analysis, maximization of the\nlikelihood allows us to compute the posterior mode in a Bayesian multivariate\ntime series soft-data framework. The presented approach is applied to the\nanalysis of COVID-19 mortality in the first wave affecting the Spanish\nCommunities, since March, 8, 2020 until May, 13, 2020. An empirical comparative\nstudy with Machine Learning (ML) regression, based on random k-fold\ncross-validation, and bootstrapping confidence interval and probability density\nestimation, is carried out. This empirical analysis also investigates the\nperformance of ML regression models in a hard- and soft- data frameworks. The\nresults could be extrapolated to other counts, countries, and posterior\nCOVID-19 waves.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 15:59:10 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 12:09:21 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 08:11:31 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Torres-Signes", "A.", ""], ["Fr\u00edas", "M. P.", ""], ["Ruiz-Medina", "M. D.", ""]]}, {"id": "2008.06400", "submitter": "Likun Zhang", "authors": "Likun Zhang and Benjamin Shaby", "title": "Uniqueness and global optimality of the maximum likelihood estimator for\n  the generalized extreme value distribution", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The three-parameter generalized extreme value distribution arises from\nclassical univariate extreme value theory and is in common use for analyzing\nthe far tail of observed phenomena. Curiously, important asymptotic properties\nof likelihood-based estimation under this standard model have yet to be\nestablished. In this paper, we formally prove that the maximum likelihood\nestimator is global and unique. An interesting secondary result entails the\nuniform consistency of a class of limit relations in a tight neighborhood of\nthe shape parameter.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 14:57:07 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Zhang", "Likun", ""], ["Shaby", "Benjamin", ""]]}, {"id": "2008.06423", "submitter": "Rajbir-Singh Nirwan", "authors": "Rajbir-Singh Nirwan, Nils Bertschinger", "title": "Bayesian Quantile Matching Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to increased awareness of data protection and corresponding laws many\ndata, especially involving sensitive personal information, are not publicly\naccessible. Accordingly, many data collecting agencies only release aggregated\ndata, e.g. providing the mean and selected quantiles of population\ndistributions. Yet, research and scientific understanding, e.g. for medical\ndiagnostics or policy advice, often relies on data access. To overcome this\ntension, we propose a Bayesian method for learning from quantile information.\nBeing based on order statistics of finite samples our method adequately and\ncorrectly reflects the uncertainty of empirical quantiles. After outlining the\ntheory, we apply our method to simulated as well as real world examples.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 15:39:51 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Nirwan", "Rajbir-Singh", ""], ["Bertschinger", "Nils", ""]]}, {"id": "2008.06461", "submitter": "Alicia Curth", "authors": "Alicia Curth and Ahmed M. Alaa and Mihaela van der Schaar", "title": "Estimating Structural Target Functions using Machine Learning and\n  Influence Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to construct a class of learning algorithms that are of practical\nvalue to applied researchers in fields such as biostatistics, epidemiology and\neconometrics, where the need to learn from incompletely observed information is\nubiquitous. We propose a new framework for statistical machine learning of\ntarget functions arising as identifiable functionals from statistical models,\nwhich we call `IF-learning' due to its reliance on influence functions (IFs).\nThis framework is problem- and model-agnostic and can be used to estimate a\nbroad variety of target parameters of interest in applied statistics: we can\nconsider any target function for which an IF of a population-averaged version\nexists in analytic form. Throughout, we put particular focus on so-called\ncoarsening at random/doubly robust problems with partially unobserved\ninformation. This includes problems such as treatment effect estimation and\ninference in the presence of missing outcome data. Within this framework, we\npropose two general learning algorithms that build on the idea of nonparametric\nplug-in bias removal via IFs: the 'IF-learner' which uses pseudo-outcomes\nmotivated by uncentered IFs for regression in large samples and outputs entire\ntarget functions without confidence bands, and the 'Group-IF-learner', which\noutputs only approximations to a function but can give confidence estimates if\nsufficient information on coarsening mechanisms is available. We apply both in\na simulation study on inferring treatment effects.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 16:48:29 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 16:27:18 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 13:15:52 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Curth", "Alicia", ""], ["Alaa", "Ahmed M.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2008.06473", "submitter": "Andrew Spieker", "authors": "Andrew J. Spieker and Robert A. Greevy and Lyndsay A. Nelson and\n  Lindsay S. Mayberry", "title": "Bounding the local average treatment effect in an instrumental variable\n  analysis of engagement with a mobile intervention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of local average treatment effects in randomized trials typically\nrequires an assumption known as the exclusion restriction in cases where we are\nunwilling to rule out unmeasured confounding. Under this assumption, any\nbenefit from treatment would be mediated through the post-randomization\nvariable being conditioned upon, and would be directly attributable to neither\nthe randomization itself nor its latent descendants. Recently, there has been\ninterest in mobile health interventions to provide healthcare support; such\nstudies can feature one-way content and/or two-way content, the latter of which\nallowing subjects to engage with the intervention in a way that can be\nobjectively measured on a subject-specific level (e.g., proportion of text\nmessages receiving a response). It is hence highly likely that a benefit\nachieved by the intervention could be explained in part by receipt of the\nintervention content and in part by engaging with/responding to it. When\nseeking to characterize average causal effects conditional on\npost-randomization engagement, the exclusion restriction is therefore all but\nsurely violated. In this paper, we propose a conceptually intuitive sensitivity\nanalysis procedure for this setting that gives rise to sharp bounds on local\naverage treatment effects. A wide array of simulation studies reveal this\napproach to have very good finite-sample behavior and to recover local average\ntreatment effects under correct specification of the sensitivity parameter. We\napply our methodology to a randomized trial evaluating a text message-delivered\nintervention for Type 2 diabetes self-care.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 17:20:58 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Spieker", "Andrew J.", ""], ["Greevy", "Robert A.", ""], ["Nelson", "Lyndsay A.", ""], ["Mayberry", "Lindsay S.", ""]]}, {"id": "2008.06475", "submitter": "Yiou Li", "authors": "Yiou Li, Lulu Kang, Xinwei Deng", "title": "A Maximin $\\Phi_{p}$-Efficient Design for Multivariate GLM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental designs for a generalized linear model (GLM) often depend on the\nspecification of the model, including the link function, the predictors, and\nunknown parameters, such as the regression coefficients. To deal with\nuncertainties of these model specifications, it is important to construct\noptimal designs with high efficiency under such uncertainties. Existing methods\nsuch as Bayesian experimental designs often use prior distributions of model\nspecifications to incorporate model uncertainties into the design criterion.\nAlternatively, one can obtain the design by optimizing the worst-case design\nefficiency with respect to uncertainties of model specifications. In this work,\nwe propose a new Maximin $\\Phi_p$-Efficient (or Mm-$\\Phi_p$ for short) design\nwhich aims at maximizing the minimum $\\Phi_p$-efficiency under model\nuncertainties. Based on the theoretical properties of the proposed criterion,\nwe develop an efficient algorithm with sound convergence properties to\nconstruct the Mm-$\\Phi_p$ design. The performance of the proposed Mm-$\\Phi_p$\ndesign is assessed through several numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 17:23:47 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Li", "Yiou", ""], ["Kang", "Lulu", ""], ["Deng", "Xinwei", ""]]}, {"id": "2008.06476", "submitter": "Lulu Kang", "authors": "Qiong Zhang and Lulu Kang", "title": "Optimal Design for A/B Testing in the Presence of Covariates and Network\n  Connection", "comments": "27 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A/B testing, also known as controlled experiments, refers to the statistical\nprocedure of conducting an experiment to compare two treatments applied to\ndifferent testing subjects. For example, many companies offering online\nservices frequently to conduct A/B testing on their users who are connected in\nsocial networks. Since two connected users usually share some similar traits,\nwe assume that their measurements are related to their network adjacency. In\nthis paper, we assume that the users, or the test subjects of the experiments,\nare connected on an undirected network. The subjects' responses are affected by\nthe treatment assignment, the observed covariate features, as well as the\nnetwork connection. We include the variation from these three sources in a\nconditional autoregressive model. Based on this model, we propose a design\ncriterion on treatment allocation that minimizes the variance of the estimated\ntreatment effect. Since the design criterion depends on an unknown network\ncorrelation parameter, we propose a Bayesian optimal design method and a hybrid\nsolution approach to obtain the optimal design. Examples via synthetic and real\nsocial networks are shown to demonstrate the performances of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 17:25:36 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Zhang", "Qiong", ""], ["Kang", "Lulu", ""]]}, {"id": "2008.06525", "submitter": "Lulu Kang", "authors": "Xiaoning Kang and Shyam Ranganathan and Lulu Kang and Julia Gohlke and\n  Xinwei Deng", "title": "Bayesian Auxiliary Variable Model for Birth Records Data with\n  Qualitative and Quantitative Responses", "comments": "27 pages, 3 figures. 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications involve data with qualitative and quantitative responses.\nWhen there is an association between the two responses, a joint model will\nprovide improved results than modeling them separately. In this paper, we\npropose a Bayesian method to jointly model such data. The joint model links the\nqualitative and quantitative responses and can assess their dependency strength\nvia a latent variable. The posterior distributions of parameters are obtained\nthrough an efficient MCMC sampling algorithm. The simulation shows that the\nproposed method can improve the prediction capacity for both responses. We\napply the proposed joint model to the birth records data acquired by the\nVirginia Department of Health and study the mutual dependence between preterm\nbirth of infants and their birth weights.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 18:11:26 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Kang", "Xiaoning", ""], ["Ranganathan", "Shyam", ""], ["Kang", "Lulu", ""], ["Gohlke", "Julia", ""], ["Deng", "Xinwei", ""]]}, {"id": "2008.06615", "submitter": "Kevin Josey", "authors": "Kevin P. Josey, Fan Yang, Debashis Ghosh, Sridharan Raghavan", "title": "A Calibration Approach to Transportability with Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important consideration in clinical research studies is proper evaluation\nof internal and external validity. While randomized clinical trials can\novercome several threats to internal validity, they may be prone to poor\nexternal validity. Conversely, large prospective observational studies sampled\nfrom a broadly generalizable population may be externally valid, yet\nsusceptible to threats to internal validity, particularly confounding. Thus,\nmethods that address confounding and enhance transportability of study results\nacross populations are essential for internally and externally valid causal\ninference, respectively. We develop a weighting method which estimates the\neffect of an intervention on an outcome in an observational study which can\nthen be transported to a second, possibly unrelated target population. The\nproposed methodology employs calibration estimators to generate complementary\nbalancing and sampling weights to address confounding and transportability,\nrespectively, enabling valid estimation of the target population average\ntreatment effect. A simulation study is conducted to demonstrate the advantages\nand similarities of the calibration approach against alternative techniques. We\nalso test the performance of the calibration estimator-based inference in a\nmotivating real data example comparing whether the effect of biguanides versus\nsulfonylureas - the two most common oral diabetes medication classes for\ninitial treatment - on all-cause mortality described in a historical cohort\napplies to a contemporary cohort of US Veterans with diabetes.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 00:53:10 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 20:31:12 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Josey", "Kevin P.", ""], ["Yang", "Fan", ""], ["Ghosh", "Debashis", ""], ["Raghavan", "Sridharan", ""]]}, {"id": "2008.06642", "submitter": "Yifan Lin", "authors": "Yifan Lin, Yuxuan Ren, Jingyuan Wan, Massey Cashore, Jiayue Wan, Yujia\n  Zhang, Peter Frazier, Enlu Zhou", "title": "Group Testing Enables Asymptomatic Screening for COVID-19 Mitigation:\n  Feasibility and Optimal Pool Size Selection with Dilution Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repeated asymptomatic screening for SARS-CoV-2 promises to control spread of\nthe virus but would require too many resources to implement at scale. Group\ntesting is promising for screening more people with fewer test resources:\nmultiple samples tested together in one pool can be excluded with one negative\ntest result. Existing approaches to group testing design for SARS-CoV-2\nasymptomatic screening, however, do not consider dilution effects: that false\nnegatives become more common with larger pools. As a consequence, they may\nrecommend pool sizes that are too large or misestimate the benefits of\nscreening. Modeling dilution effects, we derive closed-form expressions for the\nexpected number of tests and false negative/positives per person screened under\ntwo popular group testing methods: the linear and square array methods. We find\nthat test error correlation induced by a common viral load across an\nindividual's samples results in many fewer false negatives than would be\nexpected from less realistic but more widely assumed independent errors. This\ninsight also suggests that false positives can be controlled through repeated\ntests without significantly increasing false negatives. Using these closed-form\nexpressions to trace a Pareto frontier over error rates and tests, we design\ntesting protocols for repeated asymptomatic screening of a large population. We\nminimize disease prevalence by optimizing a time-varying pool sizes and\nscreening frequency constrained by daily test capacity and a false positive\nlimit. This provides a testing protocol practitioners can use for mitigating\nCOVID-19. In a case study, we demonstrate the effectiveness of this methodology\nin controlling spread.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 03:44:25 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 19:11:27 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Lin", "Yifan", ""], ["Ren", "Yuxuan", ""], ["Wan", "Jingyuan", ""], ["Cashore", "Massey", ""], ["Wan", "Jiayue", ""], ["Zhang", "Yujia", ""], ["Frazier", "Peter", ""], ["Zhou", "Enlu", ""]]}, {"id": "2008.06773", "submitter": "Kaixu Yang", "authors": "Kaixu Yang, Tapabrata Maiti", "title": "Ultra high dimensional generalized additive model: Unified Theory and\n  Methods", "comments": null, "journal-ref": null, "doi": "10.1111/sjos.12548", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized additive model is a powerful statistical learning and predictive\nmodeling tool that has been applied in a wide range of applications. The need\nof high-dimensional additive modeling is eminent in the context of dealing with\nhigh through-put data such as genetic data analysis. In this article, we\nstudied a two step selection and estimation method for ultra high dimensional\ngeneralized additive models. The first step applies group lasso on the expanded\nbases of the functions. With high probability this selects all nonzero\nfunctions without having too much over selection. The second step uses adaptive\ngroup lasso with any initial estimators, including the group lasso estimator,\nthat satisfies some regular conditions. The adaptive group lasso estimator is\nshown to be selection consistent with improved convergence rates. Tuning\nparameter selection is also discussed and shown to select the true model\nconsistently under GIC procedure. The theoretical properties are supported by\nextensive numerical study.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 19:58:03 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Yang", "Kaixu", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2008.06874", "submitter": "Ryan Martin", "authors": "Chuanhai Liu and Ryan Martin", "title": "Inferential models and possibility measures", "comments": "21 pages, 3 figures. Comments welcome at\n  https://www.researchers.one/article/2020-08-29", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inferential model (IM) framework produces data-dependent, non-additive\ndegrees of belief about the unknown parameter that are provably valid. The\nvalidity property guarantees, among other things, that inference procedures\nderived from the IM control frequentist error rates at the nominal level. A\ntechnical complication is that IMs are built on a relatively unfamiliar theory\nof random sets. Here we develop an alternative---and practically\nequivalent---formulation, based on a theory of possibility measures, which is\nsimpler in many respects. This new perspective also sheds light on the\nrelationship between IMs and Fisher's fiducial inference, as well as on the\nconstruction of optimal IMs.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 10:34:16 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Liu", "Chuanhai", ""], ["Martin", "Ryan", ""]]}, {"id": "2008.06911", "submitter": "Marcos Prates O", "authors": "Douglas Roberto Mesquita Azevedo and Marcos Oliveira Prates and\n  Dipankar Bandyopadhyay", "title": "Alleviating Spatial Confounding in Spatial Frailty Models", "comments": "21 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial confounding is how is called the confounding between fixed and\nspatial random effects. It has been widely studied and it gained attention in\nthe past years in the spatial statistics literature, as it may generate\nunexpected results in modeling. The projection-based approach, also known as\nrestricted models, appears as a good alternative to overcome the spatial\nconfounding in generalized linear mixed models. However, when the support of\nfixed effects is different from the spatial effect one, this approach can no\nlonger be applied directly. In this work, we introduce a method to alleviate\nthe spatial confounding for the spatial frailty models family. This class of\nmodels can incorporate spatially structured effects and it is usual to observe\nmore than one sample unit per area which means that the support of fixed and\nspatial effects differs. In this case, we introduce a two folded\nprojection-based approach projecting the design matrix to the dimension of the\nspace and then projecting the random effect to the orthogonal space of the new\ndesign matrix. To provide fast inference in our analysis we employ the\nintegrated nested Laplace approximation methodology. The method is illustrated\nwith an application with lung and bronchus cancer in California - US that\nconfirms that the methodology efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 13:51:53 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Azevedo", "Douglas Roberto Mesquita", ""], ["Prates", "Marcos Oliveira", ""], ["Bandyopadhyay", "Dipankar", ""]]}, {"id": "2008.06987", "submitter": "Soumik Purkayastha", "authors": "Soumik Purkayastha and Ayanendranath Basu", "title": "On minimum Bregman divergence inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper a new family of minimum divergence estimators based on the\nBregman divergence is proposed. The popular density power divergence (DPD)\nclass of estimators is a sub-class of Bregman divergences. We propose and study\na new sub-class of Bregman divergences called the exponentially weighted\ndivergence (EWD). Like the minimum DPD estimator, the minimum EWD estimator is\nrecognised as an M-estimator. This characterisation is useful while discussing\nthe asymptotic behaviour as well as the robustness properties of this class of\nestimators. Performances of the two classes are compared -- both through\nsimulations as well as through real life examples. We develop an estimation\nprocess not only for independent and homogeneous data, but also for\nnon-homogeneous data. General tests of parametric hypotheses based on the\nBregman divergences are also considered. We establish the asymptotic null\ndistribution of our proposed test statistic and explore its behaviour when\napplied to real data. The inference procedures generated by the new EWD\ndivergence appear to be competitive or better that than the DPD based\nprocedures.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 20:10:03 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Purkayastha", "Soumik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "2008.07005", "submitter": "Tiandong Wang", "authors": "Tiandong Wang and Sidney I. Resnick", "title": "A Directed Preferential Attachment Model with Poisson Measurement", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling a directed social network, one choice is to use the traditional\npreferential attachment model, which generates power-law tail distributions. In\na traditional directed preferential attachment, every new edge is added\nsequentially into the network. However, for real datasets, it is common to only\nhave coarse timestamps available, which means several new edges are created at\nthe same timestamp. Previous analyses on the evolution of social networks\nreveal that after reaching a stable phase, the growth of edge counts in a\nnetwork follows a non-homogeneous Poisson process with a constant rate across\nthe day but varying rates from day to day. Taking such empirical observations\ninto account, we propose a modified preferential attachment model with Poisson\nmeasurement, and study its asymptotic behavior. This modified model is then\nfitted to real datasets, and we see it provides a better fit than the\ntraditional one.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 21:35:45 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Tiandong", ""], ["Resnick", "Sidney I.", ""]]}, {"id": "2008.07077", "submitter": "Francesco Denti", "authors": "Francesco Denti and Federico Camerlenghi and Michele Guindani and\n  Antonietta Mira", "title": "A Common Atom Model for the Bayesian Nonparametric Analysis of Nested\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of high-dimensional data for targeted therapeutic interventions\nrequires new ways to characterize the heterogeneity observed across subgroups\nof a specific population. In particular, models for partially exchangeable data\nare needed for inference on nested datasets, where the observations are assumed\nto be organized in different units and some sharing of information is required\nto learn distinctive features of the units. In this manuscript, we propose a\nnested Common Atoms Model (CAM) that is particularly suited for the analysis of\nnested datasets where the distributions of the units are expected to differ\nonly over a small fraction of the observations sampled from each unit. The\nproposed CAM allows a two-layered clustering at the distributional and\nobservational level and is amenable to scalable posterior inference through the\nuse of a computationally efficient nested slice-sampler algorithm. We further\ndiscuss how to extend the proposed modeling framework to handle discrete\nmeasurements, and we conduct posterior inference on a real microbiome dataset\nfrom a diet swap study to investigate how the alterations in intestinal\nmicrobiota composition are associated with different eating habits. We further\ninvestigate the performance of our model in capturing true distributional\nstructures in the population by means of a simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 04:03:17 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Denti", "Francesco", ""], ["Camerlenghi", "Federico", ""], ["Guindani", "Michele", ""], ["Mira", "Antonietta", ""]]}, {"id": "2008.07107", "submitter": "Yang Ning", "authors": "Yang Ning, Guang Cheng", "title": "Sparse Confidence Sets for Normal Mean Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new framework to construct confidence sets for a\n$d$-dimensional unknown sparse parameter $\\theta$ under the normal mean model\n$X\\sim N(\\theta,\\sigma^2I)$. A key feature of the proposed confidence set is\nits capability to account for the sparsity of $\\theta$, thus named as {\\em\nsparse} confidence set. This is in sharp contrast with the classical methods,\nsuch as Bonferroni confidence intervals and other resampling based procedures,\nwhere the sparsity of $\\theta$ is often ignored. Specifically, we require the\ndesired sparse confidence set to satisfy the following two conditions: (i)\nuniformly over the parameter space, the coverage probability for $\\theta$ is\nabove a pre-specified level; (ii) there exists a random subset $S$ of\n$\\{1,...,d\\}$ such that $S$ guarantees the pre-specified true negative rate\n(TNR) for detecting nonzero $\\theta_j$'s. To exploit the sparsity of $\\theta$,\nwe define that the confidence interval for $\\theta_j$ degenerates to a single\npoint 0 for any $j\\notin S$. Under this new framework, we first consider\nwhether there exist sparse confidence sets that satisfy the above two\nconditions. To address this question, we establish a non-asymptotic minimax\nlower bound for the non-coverage probability over a suitable class of sparse\nconfidence sets. The lower bound deciphers the role of sparsity and minimum\nsignal-to-noise ratio (SNR) in the construction of sparse confidence sets.\nFurthermore, under suitable conditions on the SNR, a two-stage procedure is\nproposed to construct a sparse confidence set. To evaluate the optimality, the\nproposed sparse confidence set is shown to attain a minimax lower bound of some\nproperly defined risk function up to a constant factor. Finally, we develop an\nadaptive procedure to the unknown sparsity and SNR. Numerical studies are\nconducted to verify the theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 06:16:18 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ning", "Yang", ""], ["Cheng", "Guang", ""]]}, {"id": "2008.07110", "submitter": "Didong Li", "authors": "Debolina Paul, Saptarshi Chakraborty, Didong Li and David Dunson", "title": "Principal Ellipsoid Analysis (PEA): Efficient non-linear dimension\n  reduction & clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even with the rise in popularity of over-parameterized models, simple\ndimensionality reduction and clustering methods, such as PCA and k-means, are\nstill routinely used in an amazing variety of settings. A primary reason is the\ncombination of simplicity, interpretability and computational efficiency. The\nfocus of this article is on improving upon PCA and k-means, by allowing\nnon-linear relations in the data and more flexible cluster shapes, without\nsacrificing the key advantages. The key contribution is a new framework for\nPrincipal Elliptical Analysis (PEA), defining a simple and computationally\nefficient alternative to PCA that fits the best elliptical approximation\nthrough the data. We provide theoretical guarantees on the proposed PEA\nalgorithm using Vapnik-Chervonenkis (VC) theory to show strong consistency and\nuniform concentration bounds. Toy experiments illustrate the performance of\nPEA, and the ability to adapt to non-linear structure and complex cluster\nshapes. In a rich variety of real data clustering applications, PEA is shown to\ndo as well as k-means for simple datasets, while dramatically improving\nperformance in more complex settings.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 06:25:50 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 03:23:24 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Paul", "Debolina", ""], ["Chakraborty", "Saptarshi", ""], ["Li", "Didong", ""], ["Dunson", "David", ""]]}, {"id": "2008.07214", "submitter": "Yunxiao Chen", "authors": "Siliang Zhang and Yunxiao Chen", "title": "Computation for Latent Variable Model Estimation: A Unified Stochastic\n  Proximal Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models have been playing a central role in psychometrics and\nrelated fields. In many modern applications, the inference based on latent\nvariable models involves one or several of the following features: (1) the\npresence of many latent variables, (2) the observed and latent variables being\ncontinuous, discrete, or a combination of both, (3) constraints on parameters,\nand (4) penalties on parameters to impose model parsimony. The estimation often\ninvolves maximizing an objective function based on a marginal\nlikelihood/pseudo-likelihood, possibly with constraints and/or penalties on\nparameters. Solving this optimization problem is highly non-trivial, due to the\ncomplexities brought by the features mentioned above. Although several\nefficient algorithms have been proposed, there lacks a unified computational\nframework that takes all these features into account. In this paper, we fill\nthe gap. Specifically, we provide a unified formulation for the optimization\nproblem and then propose a quasi-Newton stochastic proximal algorithm.\nTheoretical properties of the proposed algorithms are established. The\ncomputational efficiency and robustness are shown by simulation studies under\nvarious settings for latent variable model estimation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 10:49:29 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 16:00:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhang", "Siliang", ""], ["Chen", "Yunxiao", ""]]}, {"id": "2008.07283", "submitter": "Sergio Garrido", "authors": "Sergio Garrido, Stanislav S. Borysov, Jeppe Rich, Francisco C. Pereira", "title": "Estimating Causal Effects with the Neural Autoregressive Density\n  Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of causal effects is fundamental in situations were the underlying\nsystem will be subject to active interventions. Part of building a causal\ninference engine is defining how variables relate to each other, that is,\ndefining the functional relationship between variables given conditional\ndependencies. In this paper, we deviate from the common assumption of linear\nrelationships in causal models by making use of neural autoregressive density\nestimators and use them to estimate causal effects within the Pearl's\ndo-calculus framework. Using synthetic data, we show that the approach can\nretrieve causal effects from non-linear systems without explicitly modeling the\ninteractions between the variables.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 13:12:38 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 13:03:20 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Garrido", "Sergio", ""], ["Borysov", "Stanislav S.", ""], ["Rich", "Jeppe", ""], ["Pereira", "Francisco C.", ""]]}, {"id": "2008.07361", "submitter": "Luis John", "authors": "Luis H. John, Jan A. Kors, Jenna M. Reps, Patrick B. Ryan, Peter R.\n  Rijnbeek", "title": "How little data do we need for patient-level prediction?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Provide guidance on sample size considerations for developing\npredictive models by empirically establishing the adequate sample size, which\nbalances the competing objectives of improving model performance and reducing\nmodel complexity as well as computational requirements.\n  Materials and Methods: We empirically assess the effect of sample size on\nprediction performance and model complexity by generating learning curves for\n81 prediction problems in three large observational health databases, requiring\ntraining of 17,248 prediction models. The adequate sample size was defined as\nthe sample size for which the performance of a model equalled the maximum model\nperformance minus a small threshold value.\n  Results: The adequate sample size achieves a median reduction of the number\nof observations between 9.5% and 78.5% for threshold values between 0.001 and\n0.02. The median reduction of the number of predictors in the models at the\nadequate sample size varied between 8.6% and 68.3%, respectively.\n  Discussion: Based on our results a conservative, yet significant, reduction\nin sample size and model complexity can be estimated for future prediction\nwork. Though, if a researcher is willing to generate a learning curve a much\nlarger reduction of the model complexity may be possible as suggested by a\nlarge outcome-dependent variability.\n  Conclusion: Our results suggest that in most cases only a fraction of the\navailable data was sufficient to produce a model close to the performance of\none developed on the full data set, but with a substantially reduced model\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 11:00:13 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["John", "Luis H.", ""], ["Kors", "Jan A.", ""], ["Reps", "Jenna M.", ""], ["Ryan", "Patrick B.", ""], ["Rijnbeek", "Peter R.", ""]]}, {"id": "2008.07478", "submitter": "Akisato Suzuki Dr", "authors": "Akisato Suzuki", "title": "Presenting the Probabilities of Different Effect Sizes: Towards a Better\n  Understanding and Communication of Statistical Uncertainty", "comments": "working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should social scientists understand and communicate the uncertainty of\nstatistically estimated causal effects? It is well-known that the conventional\nsignificance-vs.-insignificance approach is associated with misunderstandings\nand misuses. Behavioral research suggests people understand uncertainty more\nappropriately in a numerical, continuous scale than in a verbal, discrete\nscale. Motivated by these backgrounds, I propose presenting the probabilities\nof different effect sizes. Probability as an intuitive continuous measure of\nuncertainty allows researchers to better understand and communicate the\nuncertainty of the statistically estimated effects. In addition, my approach\nneeds no decision threshold for an uncertainty measure or effect size, unlike\nthe conventional approaches, allowing researchers to be agnostic about a\ndecision threshold such as p<5% and a justification for that. I apply my\napproach to a previous social scientific study, showing it enables richer\ninference than the significance-vs.-insignificance approach taken by the\noriginal study. The accompanying R package makes my approach easy to implement.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 17:09:53 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Suzuki", "Akisato", ""]]}, {"id": "2008.07581", "submitter": "Hoang Anh Ngo", "authors": "Hoang Anh Ngo (\\'Ecole Polytechnique, Palaiseau, France) and Thai Nam\n  HOANG (Beloit College, Wisconsin, United States)", "title": "A Rolling Optimized Nonlinear Grey Bernoulli Model RONGBM(1,1) and\n  application in predicting total COVID-19 infected cases", "comments": "Accepted paper at the 2020 International Congress of Grey Systems and\n  Uncertainty Analysis (GSUA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nonlinear Grey Bernoulli Model NGBM(1, 1) is a recently developed grey\nmodel which has various applications in different fields, mainly due to its\naccuracy in handling small time-series datasets with nonlinear variations. In\nthis paper, to fully improve the accuracy of this model, a novel model is\nproposed, namely Rolling Optimized Nonlinear Grey Bernoulli Model RONGBM(1, 1).\nThis model combines the rolling mechanism with the simultaneous optimization of\nall model parameters (exponential, background value and initial condition). The\naccuracy of this new model has significantly been proven through forecasting\nVietnam's GDP from 2013 to 2018, before it is applied to predict the total\nCOVID-19 infected cases globally by day.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 15:09:08 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ngo", "Hoang Anh", "", "\u00c9cole Polytechnique, Palaiseau, France"], ["HOANG", "Thai Nam", "", "Beloit College, Wisconsin, United States"]]}, {"id": "2008.07687", "submitter": "Liangyuan Hu", "authors": "Liangyuan Hu, Chenyang Gu", "title": "Estimation of causal effects of multiple treatments in healthcare\n  database studies with rare outcomes", "comments": "15 pages, 3 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The preponderance of large-scale healthcare databases provide abundant\nopportunities for comparative effectiveness research. Evidence necessary to\nmaking informed treatment decisions often relies on comparing effectiveness of\nmultiple treatment options on outcomes of interest observed in a small number\nof individuals. Causal inference with multiple treatments and rare outcomes is\na subject that has been treated sparingly in the literature. This paper designs\nthree sets of simulations, representative of the structure of our healthcare\ndatabase study, and propose causal analysis strategies for such settings. We\ninvestigate and compare the operating characteristics of three types of methods\nand their variants: Bayesian Additive Regression Trees (BART), regression\nadjustment on multivariate spline of generalized propensity scores (RAMS) and\ninverse probability of treatment weighting (IPTW) with multinomial logistic\nregression or generalized boosted models. Our results suggest that BART and\nRAMS provide lower bias and mean squared error, and the widely used IPTW\nmethods deliver unfavorable operating characteristics. We illustrate the\nmethods using a case study evaluating the comparative effectiveness of\nrobotic-assisted surgery, video-assisted thoracoscopic surgery and open\nthoracotomy for treating non-small cell lung cancer.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 01:26:36 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 03:25:54 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hu", "Liangyuan", ""], ["Gu", "Chenyang", ""]]}, {"id": "2008.07713", "submitter": "Roland Matsouaka", "authors": "Roland A. Matsouaka, Folefac D. Atem", "title": "Regression with a right-censored predictor, using inverse probability\n  weighting methods", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a longitudinal study, measures of key variables might be incomplete or\npartially recorded due to drop-out, loss to follow-up, or early termination of\nthe study occurring before the advent of the event of interest. In this paper,\nwe focus primarily on the implementation of a regression model with a randomly\ncensored predictor. We examine, particularly, the use of inverse probability\nweighting methods in a generalized linear model (GLM), when the predictor of\ninterest is right-censored, to adjust for censoring. To improve the performance\nof the complete-case analysis and prevent selection bias, we consider three\ndifferent weighting schemes: inverse censoring probability weights,\nKaplan-Meier weights, and Cox proportional hazards weights. We use Monte Carlo\nsimulation studies to evaluate and compare the empirical properties of\ndifferent weighting estimation methods. Finally, we apply these methods to the\nFramingham Heart Study data as an illustrative example to estimate the\nrelationship between age of onset of a clinically diagnosed cardiovascular\nevent and low-density lipoprotein (LDL) among cigarette smokers.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 02:57:53 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Matsouaka", "Roland A.", ""], ["Atem", "Folefac D.", ""]]}, {"id": "2008.07859", "submitter": "Giles Hooker", "authors": "Giles Hooker and Hanlin Shang", "title": "Selecting the Derivative of a Functional Covariate in Scalar-on-Function\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents tests to formally choose between regression models using\ndifferent derivatives of a functional covariate in scalar-on-function\nregression. We demonstrate that for linear regression, models using different\nderivatives can be nested within a model that includes point-impact effects at\nthe end-points of the observed functions. Contrasts can then be employed to\ntest the specification of different derivatives. When nonlinear regression\nmodels are defined, we apply a $J$ test to determine the statistical\nsignificance of the nonlinear structure between a functional covariate and a\nscalar response. The finite-sample performance of these methods is verified in\nsimulation, and their practical application is demonstrated using a chemometric\ndata set.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 11:15:25 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Hooker", "Giles", ""], ["Shang", "Hanlin", ""]]}, {"id": "2008.07954", "submitter": "Hao Chen Dr.", "authors": "Hao Chen, Lanshan Han and Alvin Lim", "title": "A Note on the Sum of Non-Identically Distributed Doubly Truncated Normal\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is proved that the sum of n independent but non-identically distributed\ndoubly truncated Normal distributions converges in distribution to a Normal\ndistribution. It is also shown how the result can be applied in estimating a\nconstrained mixed effects model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 14:33:07 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 19:44:26 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 15:15:12 GMT"}, {"version": "v4", "created": "Thu, 15 Jul 2021 14:52:55 GMT"}, {"version": "v5", "created": "Sat, 17 Jul 2021 23:21:59 GMT"}, {"version": "v6", "created": "Wed, 21 Jul 2021 00:12:33 GMT"}, {"version": "v7", "created": "Thu, 22 Jul 2021 20:59:17 GMT"}, {"version": "v8", "created": "Thu, 29 Jul 2021 01:58:49 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chen", "Hao", ""], ["Han", "Lanshan", ""], ["Lim", "Alvin", ""]]}, {"id": "2008.07966", "submitter": "Debanjan Mitra", "authors": "Debasis Kundu, Debanjan Mitra, Ayon Ganguly", "title": "Analysis of Left Truncated and Right Censored Competing Risks Data", "comments": null, "journal-ref": "Computational Statistics and Data Analysis, vol. 108, pages\n  12--26, 2017", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, the analysis of left truncated and right censored competing\nrisks data is carried out, under the assumption of the latent failure times\nmodel. It is assumed that there are two competing causes of failures, although\nmost of the results can be extended for more than two causes of failures. The\nlifetimes corresponding to the competing causes of failures are assumed to\nfollow Weibull distributions with the same shape parameter but different scale\nparameters. The maximum likelihood estimation procedure of the model parameters\nis discussed, and confidence intervals are provided using the bootstrap\napproach. When the common shape parameter is known, the maximum likelihood\nestimators of the scale parameters can be obtained in explicit forms, and when\nit is unknown we provide a simple iterative procedure to compute the maximum\nlikelihood estimator of the shape parameter. The Bayes estimates and the\nassociated credible intervals of unknown parameters are also addressed under a\nvery flexible set of priors on the shape and scale parameters. Extensive Monte\nCarlo simulations are performed to compare the performances of the different\nmethods. A numerical example is provided for illustrative purposes. Finally the\nresults have been extended when the two competing causes of failures are\nassumed to be independent Weibull distributions with different shape\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 14:54:54 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Kundu", "Debasis", ""], ["Mitra", "Debanjan", ""], ["Ganguly", "Ayon", ""]]}, {"id": "2008.08048", "submitter": "Youssef Aboutaleb", "authors": "Youssef M. Aboutaleb, Moshe Ben-Akiva, Patrick Jaillet", "title": "Learning Structure in Nested Logit Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new data-driven methodology for nested logit\nstructure discovery. Nested logit models allow the modeling of positive\ncorrelations between the error terms of the utility specifications of the\ndifferent alternatives in a discrete choice scenario through the specification\nof a nesting structure. Current nested logit model estimation practices require\nan a priori specification of a nesting structure by the modeler. In this we\nwork we optimize over all possible specifications of the nested logit model\nthat are consistent with rational utility maximization. We formulate the\nproblem of learning an optimal nesting structure from the data as a mixed\ninteger nonlinear programming (MINLP) optimization problem and solve it using a\nvariant of the linear outer approximation algorithm. We exploit the tree\nstructure of the problem and utilize the latest advances in integer\noptimization to bring practical tractability to the optimization problem we\nintroduce. We demonstrate the ability of our algorithm to correctly recover the\ntrue nesting structure from synthetic data in a Monte Carlo experiment. In an\nempirical illustration using a stated preference survey on modes of\ntransportation in the U.S. state of Massachusetts, we use our algorithm to\nobtain an optimal nesting tree representing the correlations between the\nunobserved effects of the different travel mode choices. We provide our\nimplementation as a customizable and open-source code base written in the Julia\nprogramming language.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 17:15:43 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Aboutaleb", "Youssef M.", ""], ["Ben-Akiva", "Moshe", ""], ["Jaillet", "Patrick", ""]]}, {"id": "2008.08176", "submitter": "Esam Mahdi", "authors": "Esam Mahdi", "title": "Mixed Portmanteau Tests for Simultaneous Linear and Nonlinear Dependency\n  in Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnibus portmanteau tests, for detecting simultaneous linear and nonlinear\ndependence structures in time series, are proposed. The tests are based on\ncombining the autocorrelation function of the conditional residuals, the\nautocorrelation function of the conditional square residuals, and the\ncross-correlation function between the conditional residuals and their squares.\nThe quasi maximum likelihood estimate is used to derive the asymptotic\ndistribution as a chi-squared distribution under a general class of time series\nmodels including ARMA, arch, and other linear and nonlinear models. The\nsimulation results show that the proposed tests successfully control the Type I\nerror probability and tend to be more powerful than other tests in many cases.\nThe efficacy of the proposed tests is demonstrated through the analysis of\nFacebook Inc., daily log returns.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 22:48:24 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 22:26:50 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Mahdi", "Esam", ""]]}, {"id": "2008.08240", "submitter": "Julius Juodakis", "authors": "Julius Juodakis and Stephen Marsland", "title": "Epidemic changepoint detection in the presence of nuisance changes", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many time series problems feature epidemic changes - segments where a\nparameter deviates from a background baseline. The number and location of such\nchanges can be estimated in a principled way by existing detection methods,\nproviding that the background level is stable and known. However, practical\ndata often contains nuisance changes in background level, which interfere with\nstandard estimation techniques. Furthermore, such changes often differ from the\ntarget segments only in duration, and appear as false alarms in the detection\nresults. To solve these issues, we propose a two-level detector that models and\nseparates nuisance and signal changes. As part of this method, we developed a\nnew, efficient approach to simultaneously estimate unknown, but fixed,\nbackground level and detect epidemic changes. The analytic and computational\nproperties of the proposed methods are established, including consistency and\nconvergence. We demonstrate via simulations that our two-level detector\nprovides accurate estimation of changepoints under a nuisance process, while\nother state-of-the-art detectors fail. Using real-world genomic and demographic\ndatasets, we demonstrate that our method can identify and localise target\nevents while separating out seasonal variations and experimental artefacts.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 03:32:51 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Juodakis", "Julius", ""], ["Marsland", "Stephen", ""]]}, {"id": "2008.08366", "submitter": "Nabaneet Das", "authors": "Nabaneet Das, Subir Kumar Bhandari", "title": "Observation on F.W.E.R. and F.D.R. for correlated normal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we have attempted to study the behaviour of the family wise\nerror rate (FWER) for Bonferroni's procedure and false discovery rate (FDR) of\nthe Benjamini-Hodgeberg procedure for simultaneous testing problem with\nequicorrelated normal observations. By simulation study, we have shown that\nF.W.E.R. is a concave function for small no. of hypotheses and asymptotically\nbecomes a convex function of the correlation. The plots of F.W.E.R. and F.D.R.\nconfirms that if non-negative correlation is present, then these procedures\ncontrol the type-I error rate at a much smaller rate than the desired level of\nsignificance. This confirms the conservative nature of these popular methods\nwhen correlation is present and provides a scope for improvement in power by\nappropriate adjustment for correlation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 10:28:29 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Das", "Nabaneet", ""], ["Bhandari", "Subir Kumar", ""]]}, {"id": "2008.08397", "submitter": "Nicolas Rivera", "authors": "Tamara Fernandez, Nicolas Rivera, Wenkai Xu and Arthur Gretton", "title": "Kernelized Stein Discrepancy Tests of Goodness-of-fit for Time-to-Event\n  Data", "comments": "Proceedings of the International Conference on Machine Learning, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival Analysis and Reliability Theory are concerned with the analysis of\ntime-to-event data, in which observations correspond to waiting times until an\nevent of interest such as death from a particular disease or failure of a\ncomponent in a mechanical system. This type of data is unique due to the\npresence of censoring, a type of missing data that occurs when we do not\nobserve the actual time of the event of interest but, instead, we have access\nto an approximation for it given by random interval in which the observation is\nknown to belong. Most traditional methods are not designed to deal with\ncensoring, and thus we need to adapt them to censored time-to-event data. In\nthis paper, we focus on non-parametric goodness-of-fit testing procedures based\non combining the Stein's method and kernelized discrepancies. While for\nuncensored data, there is a natural way of implementing a kernelized Stein\ndiscrepancy test, for censored data there are several options, each of them\nwith different advantages and disadvantages. In this paper, we propose a\ncollection of kernelized Stein discrepancy tests for time-to-event data, and we\nstudy each of them theoretically and empirically; our experimental results show\nthat our proposed methods perform better than existing tests, including\nprevious tests based on a kernelized maximum mean discrepancy.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 12:27:43 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 18:13:45 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Fernandez", "Tamara", ""], ["Rivera", "Nicolas", ""], ["Xu", "Wenkai", ""], ["Gretton", "Arthur", ""]]}, {"id": "2008.08648", "submitter": "Feifang Hu Dr", "authors": "Yifan Zhou, Yang Liu, Ping Li and Feifang Hu", "title": "Cluster-Adaptive Network A/B Testing: From Randomization to Estimation", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A/B testing is an important decision-making tool in product development for\nevaluating user engagement or satisfaction from a new service, feature or\nproduct. The goal of A/B testing is to estimate the average treatment effects\n(ATE) of a new change, which becomes complicated when users are interacting.\nWhen the important assumption of A/B testing, the Stable Unit Treatment Value\nAssumption (SUTVA), which states that each individual's response is affected by\ntheir own treatment only, is not valid, the classical estimate of the ATE\nusually leads to a wrong conclusion. In this paper, we propose a\ncluster-adaptive network A/B testing procedure, which involves a sequential\ncluster-adaptive randomization and a cluster-adjusted estimator. The\ncluster-adaptive randomization is employed to minimize the cluster-level\nMahalanobis distance within the two treatment groups, so that the variance of\nthe estimate of the ATE can be reduced. In addition, the cluster-adjusted\nestimator is used to eliminate the bias caused by network interference,\nresulting in a consistent estimation for the ATE. Numerical studies suggest our\ncluster-adaptive network A/B testing achieves consistent estimation with higher\nefficiency. An empirical study is conducted based on a real world network to\nillustrate how our method can benefit decision-making in application.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 19:42:06 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Zhou", "Yifan", ""], ["Liu", "Yang", ""], ["Li", "Ping", ""], ["Hu", "Feifang", ""]]}, {"id": "2008.08741", "submitter": "Xiaoke Zhang", "authors": "Xiaoke Zhang, Wu Xue, and Qiyue Wang", "title": "Functional Data Analysis with Causation in Observational Studies:\n  Covariate Balancing Functional Propensity Score for Functional Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis, which handles data arising from curves, surfaces,\nvolumes, manifolds and beyond in a variety of scientific fields, is a rapidly\ndeveloping area in modern statistics and data science in the recent decades.\nThe effect of a functional variable on an outcome is an essential theme in\nfunctional data analysis, but a majority of related studies are restricted to\ncorrelational effects rather than causal effects. This paper makes the first\nattempt to study the causal effect of a functional variable as a treatment in\nobservational studies. Despite the lack of a probability density function for\nthe functional treatment, the propensity score is properly defined in terms of\na multivariate substitute. Two covariate balancing methods are proposed to\nestimate the propensity score, which minimize the correlation between the\ntreatment and covariates. The appealing performance of the proposed method in\nboth covariate balance and causal effect estimation is demonstrated by a\nsimulation study. The proposed method is applied to study the causal effect of\nbody shape on human visceral adipose tissue.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 02:44:45 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Zhang", "Xiaoke", ""], ["Xue", "Wu", ""], ["Wang", "Qiyue", ""]]}, {"id": "2008.08824", "submitter": "Weiyu Li", "authors": "Lu Lin and Weiyu Li and Jun Lu", "title": "Unified Rules of Renewable Weighted Sums for Various Online Updating\n  Estimations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes unified frameworks of renewable weighted sums (RWS)\nfor various online updating estimations in the models with streaming data sets.\nThe newly defined RWS lays the foundation of online updating likelihood, online\nupdating loss function, online updating estimating equation and so on. The idea\nof RWS is intuitive and heuristic, and the algorithm is computationally simple.\nThis paper chooses nonparametric model as an exemplary setting. The RWS applies\nto various types of nonparametric estimators, which include but are not limited\nto nonparametric likelihood, quasi-likelihood and least squares. Furthermore,\nthe method and the theory can be extended into the models with both parameter\nand nonparametric function. The estimation consistency and asymptotic normality\nof the proposed renewable estimator are established, and the oracle property is\nobtained. Moreover, these properties are always satisfied, without any\nconstraint on the number of data batches, which means that the new method is\nadaptive to the situation where streaming data sets arrive perpetually. The\nbehavior of the method is further illustrated by various numerical examples\nfrom simulation experiments and real data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 07:59:49 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Lin", "Lu", ""], ["Li", "Weiyu", ""], ["Lu", "Jun", ""]]}, {"id": "2008.09083", "submitter": "Soumendu Sundar Mukherjee", "authors": "Shyamal K. De and Soumendu Sundar Mukherjee", "title": "Exact Tests for Offline Changepoint Detection in Multichannel Binary and\n  Count Data with Application to Networks", "comments": "31 pages, 9 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider offline detection of a single changepoint in binary and count\ntime-series. We compare exact tests based on the cumulative sum (CUSUM) and the\nlikelihood ratio (LR) statistics, and a new proposal that combines exact\ntwo-sample conditional tests with multiplicity correction, against standard\nasymptotic tests based on the Brownian bridge approximation to the CUSUM\nstatistic. We see empirically that the exact tests are much more powerful in\nsituations where normal approximations driving asymptotic tests are not\ntrustworthy: (i) small sample settings; (ii) sparse parametric settings; (iii)\ntime-series with changepoint near the boundary.\n  We also consider a multichannel version of the problem, where channels can\nhave different changepoints. Controlling the False Discovery Rate (FDR), we\nsimultaneously detect changes in multiple channels. This \"local\" approach is\nshown to be more advantageous than multivariate global testing approaches when\nthe number of channels with changepoints is much smaller than the total number\nof channels.\n  As a natural application, we consider network-valued time-series and use our\napproach with (a) edges as binary channels and (b) node-degrees or other local\nsubgraph statistics as count channels. The local testing approach is seen to be\nmuch more informative than global network changepoint algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:16:05 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["De", "Shyamal K.", ""], ["Mukherjee", "Soumendu Sundar", ""]]}, {"id": "2008.09091", "submitter": "Ekaterina Poliakova", "authors": "Ekaterina Poliakova", "title": "Maximum likelihood estimation of parameters of spherical particle size\n  distributions from profile size measurements and its application for small\n  samples", "comments": "A shortened and partly rewritten version of this paper with more\n  focus on the novel approximation has been submitted to Image Analysis &\n  Stereology. The paper is not being considered for any other journal, neither\n  in that shortened, nor in this detailed, nor in any other version. Compared\n  to the previous version, there were corrected many misprints and several\n  sentences were clarified", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microscopy research often requires recovering particle-size distributions in\nthree dimensions from only a few (10 - 200) profile measurements in the\nsection. This problem is especially relevant for petrographic and mineralogical\nstudies, where parametric assumptions are reasonable and finding distribution\nparameters from the microscopic study of small sections is essential. This\npaper deals with the specific case where particles are approximately spherical\n(i.e. Wicksell's problem). The paper presents a novel approximation of the\nprobability density of spherical particle profile sizes. This approximation\nuses the actual non-smoothness of mineral particles rather than perfect\nspheres. The new approximation facilitates the numerically efficient use of the\nmaximum likelihood method, a generally powerful method that provides the\ndistribution parameter estimates of the minimal variance in most practical\ncases. The variance and bias of the estimates by the maximum likelihood method\nwere compared numerically for several typical particle-size distributions with\nthose by alternative parametric methods (method of moments and minimum distance\nestimation), and the maximum likelihood estimation was found to be preferable\nfor both small and large samples. The maximum likelihood method, along with the\nsuggested approximation, may also be used for selecting a model, for\nconstructing narrow confidence intervals for distribution parameters using all\nthe profiles without random sampling and for including the measurements of the\nprofiles intersected by section boundaries. The utility of the approach is\nillustrated using an example from glacier ice petrography.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:28:04 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 15:08:55 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 13:57:23 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Poliakova", "Ekaterina", ""]]}, {"id": "2008.09111", "submitter": "Th\\'eo Michelot", "authors": "Th\\'eo Michelot, Richard Glennie, Catriona Harris, Len Thomas", "title": "Varying-coefficient stochastic differential equations with applications\n  in ecology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic differential equations (SDEs) are popular tools to analyse time\nseries data in many areas, such as mathematical finance, physics, and biology.\nThey provide a mechanistic description of the phenomeon of interest, and their\nparameters often have a clear interpretation. These advantages come at the cost\nof requiring a relatively simple model specification. We propose a flexible\nmodel for SDEs with time-varying dynamics where the parameters of the process\nare non-parametric functions of covariates, similar to generalized additive\nmodels. Combining the SDEs and non-parametric approaches allows for the SDE to\ncapture more detailed, non-stationary, features of the data-generating process.\nWe present a computationally efficient method of approximate inference, where\nthe SDE parameters can vary according to fixed covariate effects, random\neffects, or basis-penalty smoothing splines. We demonstrate the versatility and\nutility of this approach with three applications in ecology, where there is\noften a modelling trade-off between interpretability and flexibility.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:55:30 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 15:17:17 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Michelot", "Th\u00e9o", ""], ["Glennie", "Richard", ""], ["Harris", "Catriona", ""], ["Thomas", "Len", ""]]}, {"id": "2008.09407", "submitter": "Maciej Ber\\k{e}sewicz", "authors": "Maciej Ber\\k{e}sewicz, Katarzyna Pawlukiewicz", "title": "Estimation of the number of irregular foreigners in Poland using\n  non-linear count regression models", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population size estimation requires access to unit-level data in order to\ncorrectly apply capture-recapture methods. Unfortunately, for reasons of\nconfidentiality access to such data may be limited. To overcome this issue we\napply and extend the hierarchical Poisson-Gamma model proposed by Zhang (2008),\nwhich initially was used to estimate the number of irregular foreigners in\nNorway.\n  The model is an alternative to the current capture-recapture approach as it\ndoes not require linking multiple sources and is solely based on aggregated\nadministrative data that include (1) the number of apprehended irregular\nforeigners, (2) the number of foreigners who faced criminal charges and (3) the\nnumber of foreigners registered in the central population register. The model\nexplicitly assumes a relationship between the unauthorized and registered\npopulation, which is motivated by the interconnection between these two groups.\nThis makes the estimation conditionally dependent on the size of regular\npopulation, provides interpretation with analogy to registered population and\nmakes the estimated parameter more stable over time.\n  In this paper, we modify the original idea to allow for covariates and\nflexible count distributions in order to estimate the number of irregular\nforeigners in Poland in 2019. We also propose a parametric bootstrap for\nestimating standard errors of estimates. Based on the extended model we\nconclude that in as of 31.03.2019 and 30.09.2019 around 15,000 and 20,000\nforeigners and were residing in Poland without valid permits. This means that\nthose apprehended by the Polish Border Guard account for around 15-20% of the\ntotal.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 10:26:49 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Ber\u0119sewicz", "Maciej", ""], ["Pawlukiewicz", "Katarzyna", ""]]}, {"id": "2008.09434", "submitter": "Anton Alyakin", "authors": "Anton A. Alyakin, Joshua Agterberg, Hayden S. Helm and Carey E. Priebe", "title": "Correcting a Nonparametric Two-sample Graph Hypothesis Test for Graphs\n  with Different Numbers of Vertices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random graphs are statistical models that have many applications, ranging\nfrom neuroscience to social network analysis. Of particular interest in some\napplications is the problem of testing two random graphs for equality of\ngenerating distributions. Tang et al. (2017) propose a test for this setting.\nThis test consists of embedding the graph into a low-dimensional space via the\nadjacency spectral embedding (ASE) and subsequently using a kernel two-sample\ntest based on the maximum mean discrepancy. However, if the two graphs being\ncompared have an unequal number of vertices, the test of Tang et al. (2017) may\nnot be valid. We demonstrate the intuition behind this invalidity and propose a\ncorrection that makes any subsequent kernel- or distance-based test valid. Our\nmethod relies on sampling based on the asymptotic distribution for the ASE. We\ncall these altered embeddings the corrected adjacency spectral embeddings\n(CASE). We show that CASE remedies the exchangeability problem of the original\ntest and demonstrate the validity and consistency of the test that uses CASE\nvia a simulation study. We also apply our proposed test to the problem of\ndetermining equivalence of generating distributions of subgraphs in Drosophila\nconnectome.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 12:01:47 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Alyakin", "Anton A.", ""], ["Agterberg", "Joshua", ""], ["Helm", "Hayden S.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2008.09498", "submitter": "Alexis Derumigny", "authors": "Alexis Derumigny, Jean-David Fermanian and Aleksey Min", "title": "Testing for equality between conditional copulas given discretized\n  conditioning events", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several procedures have been recently proposed to test the simplifying\nassumption for conditional copulas. Instead of considering pointwise\nconditioning events, we study the constancy of the conditional dependence\nstructure when some covariates belong to general borelian conditioning subsets.\nSeveral test statistics based on the equality of conditional Kendall's tau are\nintroduced, and we derive their asymptotic distributions under the null. When\nsuch conditioning events are not fixed ex ante, we propose a data-driven\nprocedure to recursively build such relevant subsets. It is based on decision\ntrees that maximize the differences between the conditional Kendall's taus\ncorresponding to the leaves of the trees. The performances of such tests are\nillustrated in a simulation experiment. Moreover, a study of the conditional\ndependence between financial stock returns is managed, given some clustering of\ntheir past values. The last application deals with the conditional dependence\nbetween coverage amounts in an insurance dataset.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 14:26:05 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Derumigny", "Alexis", ""], ["Fermanian", "Jean-David", ""], ["Min", "Aleksey", ""]]}, {"id": "2008.09633", "submitter": "Renato J Cintra", "authors": "A. Borges Jr., R. J. Cintra, D. F. G. Coelho, V. S. Dimitrov", "title": "Low-complexity Architecture for AR(1) Inference", "comments": "7 pages, 3 tables, 4 figures", "journal-ref": "Electronics Letters 56 (14), 732-734, 2020", "doi": "10.1049/el.2019.4030", "report-no": null, "categories": "eess.SP cs.AR stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this Letter, we propose a low-complexity estimator for the correlation\ncoefficient based on the signed $\\operatorname{AR}(1)$ process. The introduced\napproximation is suitable for implementation in low-power hardware\narchitectures. Monte Carlo simulations reveal that the proposed estimator\nperforms comparably to the competing methods in literature with maximum error\nin order of $10^{-2}$. However, the hardware implementation of the introduced\nmethod presents considerable advantages in several relevant metrics, offering\nmore than 95% reduction in dynamic power and doubling the maximum operating\nfrequency when compared to the reference method.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 18:16:37 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Borges", "A.", "Jr."], ["Cintra", "R. J.", ""], ["Coelho", "D. F. G.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "2008.09650", "submitter": "Mari Myllym\\\"aki", "authors": "Mari Myllym\\\"aki and Tom\\'a\\v{s} Mrkvi\\v{c}ka", "title": "Comparison of non-parametric global envelopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a simulation study to compare different non-parametric\nglobal envelopes that are refinements of the rank envelope proposed by\nMyllym\\\"aki et al. (2017, Global envelope tests for spatial processes, J. R.\nStatist. Soc. B 79, 381-404, doi: 10.1111/rssb.12172). The global envelopes are\nconstructed for a set of functions or vectors. For a large number of vectors,\nall the refinements lead to the same outcome as the global rank envelope. For\nsmaller numbers of vectors the refinement playes a role, where different\nrefinements are sensitive to different types of extremeness of a vector among\nthe set of vectors. The performance of the different alternatives are compared\nin a simulation study with respect to the numbers of available vectors, the\ndimensionality of the vectors, the amount of dependence between the vector\nelements and the expected type of extremeness.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 18:57:47 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Myllym\u00e4ki", "Mari", ""], ["Mrkvi\u010dka", "Tom\u00e1\u0161", ""]]}, {"id": "2008.09818", "submitter": "Anand Deo", "authors": "Anand Deo and Karthyek Murthy", "title": "Optimizing tail risks using an importance sampling based extrapolation\n  for heavy-tailed objectives", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the prominence of Conditional Value-at-Risk (CVaR) as a measure\nfor tail risk in settings affected by uncertainty, we develop a new formula for\napproximating CVaR based optimization objectives and their gradients from\nlimited samples. A key difficulty that limits the widespread practical use of\nthese optimization formulations is the large amount of data required by the\nstate-of-the-art sample average approximation schemes to approximate the CVaR\nobjective with high fidelity. Unlike the state-of-the-art sample average\napproximations which require impractically large amounts of data in tail\nprobability regions, the proposed approximation scheme exploits the\nself-similarity of heavy-tailed distributions to extrapolate data from suitable\nlower quantiles. The resulting approximations are shown to be statistically\nconsistent and are amenable for optimization by means of conventional gradient\ndescent. The approximation is guided by means of a systematic\nimportance-sampling scheme whose asymptotic variance reduction properties are\nrigorously examined. Numerical experiments demonstrate the superiority of the\nproposed approximations and the ease of implementation points to the\nversatility of settings to which the approximation scheme can be applied.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 11:46:58 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Deo", "Anand", ""], ["Murthy", "Karthyek", ""]]}, {"id": "2008.09858", "submitter": "Ankit Sharma", "authors": "Ankit Sharma, Garima Gupta, Ranjitha Prasad, Arnab Chatterjee,\n  Lovekesh Vig, Gautam Shroff", "title": "Hi-CI: Deep Causal Inference in High Dimensions", "comments": "23 pages, 5 figures, Accepted in Causal Discovery Workshop - KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of counterfactual regression using causal inference\n(CI) in observational studies consisting of high dimensional covariates and\nhigh cardinality treatments. Confounding bias, which leads to inaccurate\ntreatment effect estimation, is attributed to covariates that affect both\ntreatments and outcome. The presence of high-dimensional co-variates\nexacerbates the impact of bias as it is harder to isolate and measure the\nimpact of these confounders. In the presence of high-cardinality treatment\nvariables, CI is rendered ill-posed due to the increase in the number of\ncounterfactual outcomes to be predicted. We propose Hi-CI, a deep neural\nnetwork (DNN) based framework for estimating causal effects in the presence of\nlarge number of covariates, and high-cardinal and continuous treatment\nvariables. The proposed architecture comprises of a decorrelation network and\nan outcome prediction network. In the decorrelation network, we learn a data\nrepresentation in lower dimensions as compared to the original covariates and\naddresses confounding bias alongside. Subsequently, in the outcome prediction\nnetwork, we learn an embedding of high-cardinality and continuous treatments,\njointly with the data representation. We demonstrate the efficacy of causal\neffect prediction of the proposed Hi-CI network using synthetic and real-world\nNEWS datasets.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 15:41:59 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 10:52:23 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 11:56:02 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Sharma", "Ankit", ""], ["Gupta", "Garima", ""], ["Prasad", "Ranjitha", ""], ["Chatterjee", "Arnab", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""]]}, {"id": "2008.09865", "submitter": "Serge Aleshin-Guendel", "authors": "Serge Aleshin-Guendel", "title": "On the Identifiability of Latent Class Models for Multiple-Systems\n  Estimation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent class models have recently become popular for multiple-systems\nestimation in human rights applications. However, it is currently unknown when\na given family of latent class models is identifiable in this context. We\nprovide necessary and sufficient conditions on the number of latent classes\nneeded for a family of latent class models to be identifiable. Along the way we\nprovide a mechanism for verifying identifiability in a class of\nmultiple-systems estimation models that allow for individual heterogeneity.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 16:14:52 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Aleshin-Guendel", "Serge", ""]]}, {"id": "2008.09882", "submitter": "Colin Cros", "authors": "Colin Cros, Pierre-Olivier Amblard, Jonathan H. Manton", "title": "VAR estimators using binary measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, two novel algorithms to estimate a Gaussian Vector\nAutoregressive (VAR) model from 1-bit measurements are introduced. They are\nbased on the Yule-Walker scheme modified to account for quantisation. The\nscalar case has been studied before. The main difficulty when going from the\nscalar to the vector case is how to estimate the ratios of the variances of\npairwise components of the VAR model. The first method overcomes this\ndifficulty by requiring the quantisation to be non-symmetric: each component of\nthe VAR model output is replaced by a binary \"zero\" or a binary \"one\" depending\non whether its value is greater than a strictly positive threshold. Different\ncomponents of the VAR model can have different thresholds. As the choice of\nthese thresholds has a strong influence on the performance, this first method\nis best suited for applications where the variance of each time series is\napproximately known prior to choosing the corresponding threshold. The second\nmethod relies instead on symmetric quantisations of not only each component of\nthe VAR model but also on the pairwise differences of the components. These\nadditional measurements are equivalent to a ranking of the instantaneous VAR\nmodel output, from the smallest component to the largest component. This avoids\nthe need for choosing thresholds but requires additional hardware for\nquantising the components in pairs. Numerical simulations show the efficiency\nof both schemes.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 17:15:47 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Cros", "Colin", ""], ["Amblard", "Pierre-Olivier", ""], ["Manton", "Jonathan H.", ""]]}, {"id": "2008.09885", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es and Javier \\'Alvarez-Li\\'ebana and\n  Gonzalo \\'Alvarez-P\\'erez and Wenceslao Gonz\\'alez-Manteiga", "title": "Goodness-of-fit tests for functional linear models based on integrated\n  projections", "comments": "7 pages, 2 figures", "journal-ref": "In Aneiros, G., Horov\\'a, I., Hu\\v{s}kov\\'a, M. and Vieu, P.,\n  editors, Functional and High-Dimensional Statistics and Related Fields, pages\n  107-114. Springer, 2020", "doi": "10.1007/978-3-030-47756-1_15", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Functional linear models are one of the most fundamental tools to assess the\nrelation between two random variables of a functional or scalar nature. This\ncontribution proposes a goodness-of-fit test for the functional linear model\nwith functional response that neatly adapts to functional/scalar\nresponses/predictors. In particular, the new goodness-of-fit test extends a\nprevious proposal for scalar response. The test statistic is based on a\nconvenient regularized estimator, is easy to compute, and is calibrated through\nan efficient bootstrap resampling. A graphical diagnostic tool, useful to\nvisualize the deviations from the model, is introduced and illustrated with a\nnovel data application. The R package goffda implements the proposed methods\nand allows for the reproducibility of the data application.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 17:30:46 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["\u00c1lvarez-Li\u00e9bana", "Javier", ""], ["\u00c1lvarez-P\u00e9rez", "Gonzalo", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""]]}, {"id": "2008.09897", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Paula Navarro-Esteban, Juan A.\n  Cuesta-Albertos", "title": "On a projection-based class of uniformity tests on the hypersphere", "comments": "26 pages, 3 figures, 6 tables. Supplementary material: 26 pages, 2\n  figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a projection-based class of uniformity tests on the hypersphere\nusing statistics that integrate, along all possible directions, the weighted\nquadratic discrepancy between the empirical cumulative distribution function of\nthe projected data and the projected uniform distribution. Simple expressions\nfor several test statistics are obtained for the circle and sphere, and\nrelatively tractable forms for higher dimensions. Despite its different origin,\nthe proposed class is shown to be related with the well-studied Sobolev class\nof uniformity tests. Our new class proves itself advantageous by allowing to\nderive new tests for hyperspherical data that neatly extend the circular tests\nby Watson, Ajne, and Rothman, and by introducing the first instance of an\nAnderson-Darling-like test for such data. The asymptotic distributions and the\nlocal optimality against certain alternatives of the new tests are obtained. A\nsimulation study evaluates the theoretical findings and evidences that, for\ncertain scenarios, the new tests are competitive against previous proposals.\nThe new tests are employed in three astronomical applications.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 18:28:14 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 10:25:28 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Navarro-Esteban", "Paula", ""], ["Cuesta-Albertos", "Juan A.", ""]]}, {"id": "2008.09938", "submitter": "Cheng Zeng", "authors": "Cheng Zeng and Leo L. Duan", "title": "Quasi-Bernoulli Stick-breaking: Infinite Mixture with Cluster\n  Consistency", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mixture modeling and clustering application, the number of components is\noften not known. The stick-breaking model is an appealing construction that\nassumes infinitely many components, while shrinking most of the redundant\nweights to near zero. However, it has been discovered that such a shrinkage is\nunsatisfactory: even when the component distribution is correctly specified,\nsmall and spurious weights will appear and give an inconsistent estimate on the\ncluster number. In this article, we propose a simple solution that gains\nstronger control on the redundant weights -- when breaking each stick into two\npieces, we adjust the length of the second piece by multiplying it to a\nquasi-Bernoulli random variable, supported at one and a positive constant close\nto zero. This substantially increases the chance of shrinking {\\em all} the\nredundant weights to almost zero, leading to a consistent estimator on the\ncluster number; at the same time, it avoids the singularity due to assigning an\nexactly zero weight, and maintains a support in the infinite-dimensional space.\nAs a stick-breaking model, its posterior computation can be carried out\nefficiently via the classic blocked Gibbs sampler, allowing straightforward\nextension of using non-Gaussian components. Compared to existing methods, our\nmodel demonstrates much superior performances in the simulations and data\napplication, showing a substantial reduction in the number of clusters.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 01:13:33 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 22:18:48 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Zeng", "Cheng", ""], ["Duan", "Leo L.", ""]]}, {"id": "2008.10055", "submitter": "Guodong Chen", "authors": "Guodong Chen, Jes\\'us Arroyo, Avanti Athreya, Joshua Cape, Joshua T.\n  Vogelstein, Youngser Park, Chris White, Jonathan Larson, Weiwei Yang and\n  Carey E. Priebe", "title": "Multiple Network Embedding for Anomaly Detection in Time Series of\n  Graphs", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the graph signal processing problem of anomaly detection\nin time series of graphs. We examine two related, complementary inference\ntasks: the detection of anomalous graphs within a time series, and the\ndetection of temporally anomalous vertices. We approach these tasks via the\nadaptation of statistically principled methods for joint graph inference,\nspecifically multiple adjacency spectral embedding (MASE) and omnibus embedding\n(OMNI). We demonstrate that these two methods are effective for our inference\ntasks. Moreover, we assess the performance of these methods in terms of the\nunderlying nature of detectable anomalies. Our results delineate the relative\nstrengths and limitations of these procedures, and provide insight into their\nuse. Applied to a large-scale commercial search engine time series of graphs,\nour approaches demonstrate their applicability and identify the anomalous\nvertices beyond just large degree change.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 14:56:59 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chen", "Guodong", ""], ["Arroyo", "Jes\u00fas", ""], ["Athreya", "Avanti", ""], ["Cape", "Joshua", ""], ["Vogelstein", "Joshua T.", ""], ["Park", "Youngser", ""], ["White", "Chris", ""], ["Larson", "Jonathan", ""], ["Yang", "Weiwei", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2008.10104", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen, Yi-Hsuan Lee and Xiaoou Li", "title": "Item Quality Control in Educational Testing: Change Point Model,\n  Compound Risk, and Sequential Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In standardized educational testing, test items are reused in multiple test\nadministrations. To ensure the validity of test scores, psychometric properties\nof items should remain unchanged over time. In this paper, we consider the\nsequential monitoring of test items, in particular, the detection of abrupt\nchanges to their psychometric properties, where a change can be caused by, for\nexample, leakage of the item or change of corresponding curriculum. We propose\na statistical framework for the detection of abrupt changes in individual\nitems. This framework consists of (1) a multi-stream Bayesian change point\nmodel describing sequential changes in items, (2) a compound risk function\nquantifying the risk in sequential decisions, and (3) sequential decision rules\nthat control the compound risk. Throughout the sequential decision process, the\nproposed decision rule balances the trade-off between two sources of errors,\nthe false detection of pre-change items and the non-detection of post-change\nitems. An item-specific monitoring statistic is proposed based on an item\nresponse theory model that eliminates the confounding from the examinee\npopulation which changes over time. Sequential decision rules and their\ntheoretical properties are developed under two settings: the oracle setting\nwhere the Bayesian change point model is completely known and a more realistic\nsetting where some parameters of the model are unknown. Simulation studies are\nconducted under settings that mimic real operational tests.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 20:46:48 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chen", "Yunxiao", ""], ["Lee", "Yi-Hsuan", ""], ["Li", "Xiaoou", ""]]}, {"id": "2008.10108", "submitter": "Mari Myllym\\\"aki", "authors": "Tom\\'a\\v{s} Mrkvi\\v{c}ka and Mari Myllym\\\"aki", "title": "False discovery rate envelope for functional test statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  False discovery rate (FDR) is a common way to control the number of false\ndiscoveries in multiple testing. In this paper, the focus is on functional test\nstatistics which are discretized into $m$ highly correlated hypotheses and thus\nresampling based methods are investigated. The aim is to find a graphical\nenvelope that detects the outcomes of all individual hypotheses by a simple\nrule: the hypothesis is rejected if and only if the empirical test statistic is\noutside of the envelope. Such an envelope offers a straightforward\ninterpretation of the test results similarly as in global envelope testing\nrecently developed for controlling the family-wise error rate. Two different\nalgorithms are developed to fulfill this aim. The proposed algorithms are\nadaptive single threshold procedures which include the estimation of the true\nnull hypotheses. The new methods are illustrated by two real data examples.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 21:31:00 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Mrkvi\u010dka", "Tom\u00e1\u0161", ""], ["Myllym\u00e4ki", "Mari", ""]]}, {"id": "2008.10109", "submitter": "Raaz Dwivedi", "authors": "Raaz Dwivedi, Yan Shuo Tan, Briton Park, Mian Wei, Kevin Horgan, David\n  Madigan, Bin Yu", "title": "Stable discovery of interpretable subgroups via calibration in causal\n  studies", "comments": "Raaz Dwivedi and Yan Shuo Tan are joint first authors and contributed\n  equally to this work. 52 pages, 8 Figures, 9 Tables. To appear in\n  International Statistical Review, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on Yu and Kumbier's PCS framework and for randomized experiments, we\nintroduce a novel methodology for Stable Discovery of Interpretable Subgroups\nvia Calibration (StaDISC), with large heterogeneous treatment effects. StaDISC\nwas developed during our re-analysis of the 1999-2000 VIGOR study, an 8076\npatient randomized controlled trial (RCT), that compared the risk of adverse\nevents from a then newly approved drug, Rofecoxib (Vioxx), to that from an\nolder drug Naproxen. Vioxx was found to, on average and in comparison to\nNaproxen, reduce the risk of gastrointestinal (GI) events but increase the risk\nof thrombotic cardiovascular (CVT) events. Applying StaDISC, we fit 18 popular\nconditional average treatment effect (CATE) estimators for both outcomes and\nuse calibration to demonstrate their poor global performance. However, they are\nlocally well-calibrated and stable, enabling the identification of patient\ngroups with larger than (estimated) average treatment effects. In fact, StaDISC\ndiscovers three clinically interpretable subgroups each for the GI outcome\n(totaling 29.4% of the study size) and the CVT outcome (totaling 11.0%).\nComplementary analyses of the found subgroups using the 2001-2004 APPROVe\nstudy, a separate independently conducted RCT with 2587 patients, provides\nfurther supporting evidence for the promise of StaDISC.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 21:35:37 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 02:55:05 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Dwivedi", "Raaz", ""], ["Tan", "Yan Shuo", ""], ["Park", "Briton", ""], ["Wei", "Mian", ""], ["Horgan", "Kevin", ""], ["Madigan", "David", ""], ["Yu", "Bin", ""]]}, {"id": "2008.10118", "submitter": "Juan Sosa", "authors": "Brenda Betancourt, Juan Sosa, Abel Rodr\\'iguez", "title": "A Prior for Record Linkage Based on Allelic Partitions", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In database management, record linkage aims to identify multiple records that\ncorrespond to the same individual. This task can be treated as a clustering\nproblem, in which a latent entity is associated with one or more noisy database\nrecords. However, in contrast to traditional clustering applications, a large\nnumber of clusters with a few observations per cluster is expected in this\ncontext. In this paper, we introduce a new class of prior distributions based\non allelic partitions that is specially suited for the small cluster setting of\nrecord linkage. Our approach makes it straightforward to introduce prior\ninformation about the cluster size distribution at different scales, and\nnaturally enforces sublinear growth of the maximum cluster size -known as the\nmicroclustering property. We also introduce a set of novel microclustering\nconditions in order to impose further constraints on the cluster sizes a\npriori. We evaluate the performance of our proposed class of priors using\nsimulated data and three official statistics data sets, and show that our\nmodels provide competitive results compared to state-of-the-art microclustering\nmodels in the record linkage literature. Moreover, we compare the performance\nof different loss functions for optimal point estimation of the partitions\nusing decision-theoretical based approaches recently proposed in the\nliterature.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 22:09:49 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 20:01:54 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Betancourt", "Brenda", ""], ["Sosa", "Juan", ""], ["Rodr\u00edguez", "Abel", ""]]}, {"id": "2008.10211", "submitter": "Quoc Dung Cao", "authors": "Quoc D. Cao, Scott B. Miles, Youngjun Choe", "title": "Infrastructure Recovery Curve Estimation Using Gaussian Process\n  Regression on Expert Elicited Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrastructure recovery time estimation is critical to disaster management\nand planning. Inspired by recent resilience planning initiatives, we consider a\nsituation where experts are asked to estimate the time for different\ninfrastructure systems to recover to certain functionality levels after a\nscenario hazard event. We propose a methodological framework to use\nexpert-elicited data to estimate the expected recovery time curve of a\nparticular infrastructure system. This framework uses the Gaussian process\nregression (GPR) to capture the experts' estimation-uncertainty and satisfy\nknown physical constraints of recovery processes. The framework is designed to\nfind a balance between the data collection cost of expert elicitation and the\nprediction accuracy of GPR. We evaluate the framework on realistically\nsimulated expert-elicited data concerning the two case study events, the 1995\nGreat Hanshin-Awaji Earthquake and the 2011 Great East Japan Earthquake.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 06:17:17 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Cao", "Quoc D.", ""], ["Miles", "Scott B.", ""], ["Choe", "Youngjun", ""]]}, {"id": "2008.10296", "submitter": "Tuomas Sivula", "authors": "Tuomas Sivula (1), M{\\aa}ns Magnusson (2), Aki Vehtari (1) ((1) Aalto\n  University Finland, (2) Uppsala University Sweden)", "title": "Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model\n  Comparison", "comments": "88 pages, 19 figures. Update 2020-10-10: minor fixes for typos,\n  wordings, and y-label in Figure 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leave-one-out cross-validation (LOO-CV) is a popular method for comparing\nBayesian models based on their estimated predictive performance on new, unseen,\ndata. Estimating the uncertainty of the resulting LOO-CV estimate is a complex\ntask and it is known that the commonly used standard error estimate is often\ntoo small. We analyse the frequency properties of the LOO-CV estimator and\nstudy the uncertainty related to it. We provide new results of the properties\nof the uncertainty both theoretically and empirically and discuss the\nchallenges of estimating it. We show that problematic cases include: comparing\nmodels with similar predictions, misspecified models, and small data. In these\ncases, there is a weak connection in the skewness of the sampling distribution\nand the distribution of the error of the LOO-CV estimator. We show that it is\npossible that the problematic skewness of the error distribution, which occurs\nwhen the models make similar predictions, does not fade away when the data size\ngrows to infinity in certain situations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 10:04:31 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 10:50:56 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Sivula", "Tuomas", ""], ["Magnusson", "M\u00e5ns", ""], ["Vehtari", "Aki", ""]]}, {"id": "2008.10437", "submitter": "Jake Grainger", "authors": "Jake P. Grainger, Adam M. Sykulski, Philip Jonathan and Kevin Ewans", "title": "Estimating the parameters of ocean wave spectra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph physics.data-an stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wind-generated waves are often treated as stochastic processes. There is\nparticular interest in their spectral density functions, which are often\nexpressed in some parametric form. Such spectral density functions are used as\ninputs when modelling structural response or other engineering concerns.\nTherefore, accurate and precise recovery of the parameters of such a form, from\nobserved wave records, is important. Current techniques are known to struggle\nwith recovering certain parameters, especially the peak enhancement factor and\nspectral tail decay. We introduce an approach from the statistical literature,\nknown as the de-biased Whittle likelihood, and address some practical concerns\nregarding its implementation in the context of wind-generated waves. We\ndemonstrate, through numerical simulation, that the de-biased Whittle\nlikelihood outperforms current techniques, such as least squares fitting, both\nin terms of accuracy and precision of the recovered parameters. We also provide\na method for estimating the uncertainty of parameter estimates. We perform an\nexample analysis on a data-set recorded off the coast of New Zealand, to\nillustrate some of the extra practical concerns that arise when estimating the\nparameters of spectra from observed data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 13:42:54 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 10:55:40 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Grainger", "Jake P.", ""], ["Sykulski", "Adam M.", ""], ["Jonathan", "Philip", ""], ["Ewans", "Kevin", ""]]}, {"id": "2008.10461", "submitter": "Sergiy Vorobyov A.", "authors": "Jari Miettinen and Eyal Nitzan and Sergiy A. Vorobyov and Esa Ollila", "title": "Graph Signal Processing Meets Blind Source Separation", "comments": "31 pages, 3 figures, 1 table, submitted to IEEE Trans. Signal\n  Processing on Aug. 2020", "journal-ref": null, "doi": "10.1109/TSP.2021.3073226", "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In graph signal processing (GSP), prior information on the dependencies in\nthe signal is collected in a graph which is then used when processing or\nanalyzing the signal. Blind source separation (BSS) techniques have been\ndeveloped and analyzed in different domains, but for graph signals the research\non BSS is still in its infancy. In this paper, this gap is filled with two\ncontributions. First, a nonparametric BSS method, which is relevant to the GSP\nframework, is refined, the Cram\\'{e}r-Rao bound (CRB) for mixing and unmixing\nmatrix estimators in the case of Gaussian moving average graph signals is\nderived, and for studying the achievability of the CRB, a new parametric method\nfor BSS of Gaussian moving average graph signals is introduced. Second, we also\nconsider BSS of non-Gaussian graph signals and two methods are proposed.\nIdentifiability conditions show that utilizing both graph structure and\nnon-Gaussianity provides a more robust approach than methods which are based on\nonly either graph dependencies or non-Gaussianity. It is also demonstrated by\nnumerical study that the proposed methods are more efficient in separating\nnon-Gaussian graph signals.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 14:05:17 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Miettinen", "Jari", ""], ["Nitzan", "Eyal", ""], ["Vorobyov", "Sergiy A.", ""], ["Ollila", "Esa", ""]]}, {"id": "2008.10547", "submitter": "William Stephenson", "authors": "William T. Stephenson, Madeleine Udell, Tamara Broderick", "title": "Approximate Cross-Validation with Low-Rank Data in High Dimensions", "comments": "19 pages, 6 figures", "journal-ref": "Advances in Neural Information Processing Systems 33 (NeurIPS\n  2020)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent advances in machine learning are driven by a challenging\ntrifecta: large data size $N$; high dimensions; and expensive algorithms. In\nthis setting, cross-validation (CV) serves as an important tool for model\nassessment. Recent advances in approximate cross validation (ACV) provide\naccurate approximations to CV with only a single model fit, avoiding\ntraditional CV's requirement for repeated runs of expensive algorithms.\nUnfortunately, these ACV methods can lose both speed and accuracy in high\ndimensions -- unless sparsity structure is present in the data. Fortunately,\nthere is an alternative type of simplifying structure that is present in most\ndata: approximate low rank (ALR). Guided by this observation, we develop a new\nalgorithm for ACV that is fast and accurate in the presence of ALR data. Our\nfirst key insight is that the Hessian matrix -- whose inverse forms the\ncomputational bottleneck of existing ACV methods -- is ALR. We show that,\ndespite our use of the \\emph{inverse} Hessian, a low-rank approximation using\nthe largest (rather than the smallest) matrix eigenvalues enables fast,\nreliable ACV. Our second key insight is that, in the presence of ALR data,\nerror in existing ACV methods roughly grows with the (approximate, low) rank\nrather than with the (full, high) dimension. These insights allow us to prove\ntheoretical guarantees on the quality of our proposed algorithm -- along with\nfast-to-compute upper bounds on its error. We demonstrate the speed and\naccuracy of our method, as well as the usefulness of our bounds, on a range of\nreal and simulated data sets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 16:34:05 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Stephenson", "William T.", ""], ["Udell", "Madeleine", ""], ["Broderick", "Tamara", ""]]}, {"id": "2008.10552", "submitter": "Leonard Soicher", "authors": "R. A. Bailey and Leonard H. Soicher", "title": "Uniform semi-Latin squares and their pairwise-variance aberrations", "comments": "21 pages; accepted manuscript", "journal-ref": "Journal of Statistical Planning and Inference 213 (2021) 282-291", "doi": "10.1016/j.jspi.2020.12.003", "report-no": null, "categories": "math.ST math.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  For integers $n>2$ and $k>0$, an $(n\\times n)/k$ semi-Latin square is an\n$n\\times n$ array of $k$-subsets (called blocks) of an $nk$-set (of\ntreatments), such that each treatment occurs once in each row and once in each\ncolumn of the array. A semi-Latin square is uniform if every pair of blocks,\nnot in the same row or column, intersect in the same positive number of\ntreatments. We show that when a uniform $(n\\times n)/k$ semi-Latin square\nexists, the Schur optimal $(n\\times n)/k$ semi-Latin squares are precisely the\nuniform ones. We then compare uniform semi-Latin squares using the criterion of\npairwise-variance (PV) aberration, introduced by J.P. Morgan for affine\nresolvable designs, and determine the uniform $(n\\times n)/k$ semi-Latin\nsquares with minimum PV aberration when there exist $n-1$ mutually orthogonal\nLatin squares (MOLS) of order $n$. These do not exist when $n=6$, and the\nsmallest uniform semi-Latin squares in this case have size $(6\\times 6)/10$. We\npresent a complete classification of the uniform $(6\\times 6)/10$ semi-Latin\nsquares, and display the one with least PV aberration. We give a construction\nproducing a uniform $((n+1)\\times (n+1))/((n-2)n)$ semi-Latin square when there\nexist $n-1$ MOLS of order $n$, and determine the PV aberration of such a\nuniform semi-Latin square. Finally, we describe how certain affine resolvable\ndesigns and balanced incomplete-block designs (BIBDs) can be constructed from\nuniform semi-Latin squares. From the uniform $(6\\times 6)/10$ semi-Latin\nsquares we classified, we obtain (up to block design isomorphism) exactly 16875\naffine resolvable designs for 72 treatments in 36 blocks of size 12 and 8615\nBIBDs for 36 treatments in 84 blocks of size 6. In particular, this shows that\nthere are at least 16875 pairwise non-isomorphic orthogonal arrays\n$\\mathrm{OA}(72,6,6,2)$.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 18:15:03 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 16:22:56 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Bailey", "R. A.", ""], ["Soicher", "Leonard H.", ""]]}, {"id": "2008.10706", "submitter": "Ranjani Srinivasan", "authors": "Ranjani Srinivasan, Jaron Lee, Rohit Bhattacharya, Narges Ahmidi, Ilya\n  Shpitser", "title": "Path Dependent Structural Equation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal analyses of longitudinal data generally assume that the qualitative\ncausal structure relating variables remains invariant over time. In structured\nsystems that transition between qualitatively different states in discrete time\nsteps, such an approach is deficient on two fronts. First, time-varying\nvariables may have state-specific causal relationships that need to be\ncaptured. Second, an intervention can result in state transitions downstream of\nthe intervention different from those actually observed in the data. In other\nwords, interventions may counterfactually alter the subsequent temporal\nevolution of the system. We introduce a generalization of causal graphical\nmodels, Path Dependent Structural Equation Models (PDSEMs), that can describe\nsuch systems. We show how causal inference may be performed in such models and\nillustrate its use in simulations and data obtained from a septoplasty surgical\nprocedure.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 21:05:04 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 04:35:31 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Srinivasan", "Ranjani", ""], ["Lee", "Jaron", ""], ["Bhattacharya", "Rohit", ""], ["Ahmidi", "Narges", ""], ["Shpitser", "Ilya", ""]]}, {"id": "2008.10741", "submitter": "Arjun Kodialam", "authors": "Arjun Kodialam", "title": "Efficient Detection Of Infected Individuals using Two Stage Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group testing is an efficient method for testing a large population to detect\ninfected individuals. In this paper, we consider an efficient adaptive two\nstage group testing scheme. Using a straightforward analysis, we characterize\nthe efficiency of several two stage group testing algorithms. We determine how\nto pick the parameters of the tests optimally for three schemes with different\ntypes of randomization, and show that the performance of two stage testing\ndepends on the type of randomization employed. Seemingly similar randomization\nprocedures lead to different expected number of tests to detect all infected\nindividuals, we determine what kinds of randomization are necessary to achieve\noptimal performance. We further show that in the optimal setting, our testing\nscheme is robust to errors in the input parameters.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 23:05:10 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kodialam", "Arjun", ""]]}, {"id": "2008.10747", "submitter": "Thien Q. Tran", "authors": "Thien Q. Tran, Kazuto Fukuchi, Youhei Akimoto, Jun Sakuma", "title": "Statistically Significant Pattern Mining with Ordinal Utility", "comments": "Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining (KDD '20), August 23--27, 2020, Virtual Event, CA, USA", "journal-ref": null, "doi": "10.1145/3394486.3403215", "report-no": null, "categories": "stat.ME cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistically significant patterns mining (SSPM) is an essential and\nchallenging data mining task in the field of knowledge discovery in databases\n(KDD), in which each pattern is evaluated via a hypothesis test. Our study aims\nto introduce a preference relation into patterns and to discover the most\npreferred patterns under the constraint of statistical significance, which has\nnever been considered in existing SSPM problems. We propose an iterative\nmultiple testing procedure that can alternately reject a hypothesis and safely\nignore the hypotheses that are less useful than the rejected hypothesis. One\nadvantage of filtering out patterns with low utility is that it avoids\nconsumption of the significance budget by rejection of useless (that is,\nuninteresting) patterns. This allows the significance budget to be focused on\nuseful patterns, leading to more useful discoveries.\n  We show that the proposed method can control the familywise error rate (FWER)\nunder certain assumptions, that can be satisfied by a realistic problem class\nin SSPM.\\@We also show that the proposed method always discovers a set of\npatterns that is at least equally or more useful than those discovered using\nthe standard Tarone-Bonferroni method SSPM.\\@Finally, we conducted several\nexperiments with both synthetic and real-world data to evaluate the performance\nof our method. As a result, in the experiments with real-world datasets, the\nproposed method discovered a larger number of more useful patterns than the\nexisting method for all five conducted tasks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 23:39:15 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Tran", "Thien Q.", ""], ["Fukuchi", "Kazuto", ""], ["Akimoto", "Youhei", ""], ["Sakuma", "Jun", ""]]}, {"id": "2008.10767", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Paula Navarro-Esteban, Juan A.\n  Cuesta-Albertos", "title": "A Cram\\'er-von Mises test of uniformity on the hypersphere", "comments": "8 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Testing uniformity of a sample supported on the hypersphere is one of the\nfirst steps when analysing multivariate data for which only the directions (and\nnot the magnitudes) are of interest. In this work, a projection-based\nCram\\'er-von Mises test of uniformity on the hypersphere is introduced. This\ntest can be regarded as an extension of the well-known Watson test of circular\nuniformity to the hypersphere. The null asymptotic distribution of the test\nstatistic is obtained and, via numerical experiments, shown to be tractable and\npractical. A novel study on the uniformity of the distribution of craters on\nVenus illustrates the usage of the test.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 00:52:42 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 17:26:32 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Navarro-Esteban", "Paula", ""], ["Cuesta-Albertos", "Juan A.", ""]]}, {"id": "2008.10859", "submitter": "Tuomas Sivula", "authors": "Tuomas Sivula (1), M{\\aa}ns Magnusson (2), Aki Vehtari (1) ((1) Aalto\n  University Finland, (2) Uppsala University Sweden)", "title": "Unbiased estimator for the variance of the leave-one-out\n  cross-validation estimator for a Bayesian normal model with fixed variance", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When evaluating and comparing models using leave-one-out cross-validation\n(LOO-CV), the uncertainty of the estimate is typically assessed using the\nvariance of the sampling distribution. It is known, however, that no unbiased\nestimator for the variance can be constructed in a general case. While it has\nnot been discussed before, it could be possible to construct such an estimator\nby considering specific models. In this paper, we show that an unbiased\nsampling distribution variance estimator is obtainable for the Bayesian normal\nmodel with fixed model variance using expected log pointwise predictive density\n(elpd) utility score. Instead of the obtained pointwise LOO-CV estimates, we\nestimate the variance directly from the observations. Motivated by the\npresented unbiased variance estimator, it could be possible to obtain other\nimproved problem-specific estimators, not only unbiased ones, for assessing the\nuncertainty of LOO-CV estimation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 07:39:36 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Sivula", "Tuomas", ""], ["Magnusson", "M\u00e5ns", ""], ["Vehtari", "Aki", ""]]}, {"id": "2008.10876", "submitter": "Yoshihiro Hirose", "authors": "Yoshihiro Hirose", "title": "Regularization Methods Based on the $L_q$-Likelihood for Linear Models\n  with Heavy-Tailed Errors", "comments": null, "journal-ref": null, "doi": "10.3390/e22091036", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose regularization methods for linear models based on the\n$L_q$-likelihood, which is a generalization of the log-likelihood using a power\nfunction. Some heavy-tailed distributions are known as $q$-normal\ndistributions. We find that the proposed methods for linear models with\n$q$-normal errors coincide with the regularization methods that are applied to\nthe normal linear model. The proposed methods work well and efficiently, and\ncan be computed using existing packages. We examine the proposed methods using\nnumerical experiments, showing that the methods perform well, even when the\nerror is heavy-tailed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 08:24:14 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Hirose", "Yoshihiro", ""]]}, {"id": "2008.10903", "submitter": "Akisato Suzuki Dr", "authors": "Akisato Suzuki", "title": "Policy Implications of Statistical Estimates: A General Bayesian\n  Decision-Theoretic Model for Binary Outcomes", "comments": "working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should scholars evaluate the statistically estimated causal effect of a\npolicy intervention? I point out three limitations in the conventional\npractice. First, relying on statistical significance misses the fact that\nuncertainty is a continuous scale. Second, focusing on a standard point\nestimate overlooks variation in plausible effect sizes. Third, the criterion of\nsubstantive significance is rarely explained or justified. To address these\nissues, I propose an original Bayesian decision-theoretic model for binary\noutcomes. I incorporate the posterior distribution of a causal effect reducing\nthe likelihood of an undesirable event, into a loss function over the cost of a\npolicy to realize the effect and the cost of the event. The model can use an\neffect size of interest other than the standard point estimate, and the\nprobability of this effect as a continuous measure of uncertainty. It then\npresents approximately up to what ratio between the two costs an expected loss\nremains smaller if the policy is implemented than if not. I exemplify my model\nthrough three applications and provide an R package for easy implementation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 09:21:58 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 18:32:01 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Suzuki", "Akisato", ""]]}, {"id": "2008.10930", "submitter": "Valentin Courgeau", "authors": "Valentin Courgeau, Almut E.D. Veraart", "title": "High-frequency Estimation of the L\\'evy-driven Graph Ornstein-Uhlenbeck\n  process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Graph Ornstein-Uhlenbeck (GrOU) process observed on a\nnon-uniform discrete time grid and introduce discretised maximum likelihood\nestimators with parameters specific to the whole graph or specific to each\ncomponent, or node. Under a high-frequency sampling scheme, we study the\nasymptotic behaviour of those estimators as the mesh size of the observation\ngrid goes to zero. We prove two stable central limit theorems to the same\ndistribution as in the continuously-observed case under both finite and\ninfinite jump activity for the L\\'evy driving noise. When a graph structure is\nnot explicitly available, the stable convergence allows to consider\npurpose-specific sparse inference procedures, i.e. pruning, on the edges\nthemselves in parallel to the GrOU inference and preserve its asymptotic\nproperties. We apply the new estimators to wind capacity factor measurements,\ni.e. the ratio between the wind power produced locally compared to its rated\npeak power, across fifty locations in Northern Spain and Portugal. We show the\nsuperiority of those estimators compared to the standard least squares\nestimator through a simulation study extending known univariate results across\ngraph configurations, noise types and amplitudes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 10:26:40 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Courgeau", "Valentin", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "2008.10957", "submitter": "Wanfang Chen", "authors": "Wanfang Chen and Marc G. Genton", "title": "Are You All Normal? It Depends!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The assumption of normality has underlain much of the development of\nstatistics, including spatial statistics, and many tests have been proposed. In\nthis work, we focus on the multivariate setting and we first provide a synopsis\nof the recent advances in multivariate normality tests for i.i.d. data, with\nemphasis on the skewness and kurtosis approaches. We show through simulation\nstudies that some of these tests cannot be used directly for testing normality\nof spatial data, since the multivariate sample skewness and kurtosis measures,\nsuch as the Mardia's measures, deviate from their theoretical values under\nGaussianity due to dependence, and some related tests exhibit inflated type I\nerror, especially when the spatial dependence gets stronger. We review briefly\nthe few existing tests under dependence (time or space), and then propose a new\nmultivariate normality test for spatial data by accounting for the spatial\ndependence of the observations in the test statistic. The new test aggregates\nunivariate Jarque-Bera (JB) statistics, which combine skewness and kurtosis\nmeasures, for individual variables. The asymptotic variances of sample skewness\nand kurtosis for standardized observations are derived given the dependence\nstructure of the spatial data. Consistent estimators of the asymptotic\nvariances are then constructed for finite samples. The test statistic is easy\nto compute, without any smoothing involved, and it is asymptotically\n$\\chi^2_{2p}$ under normality, where $p$ is the number of variables. The new\ntest has a good control of the type I error and a high empirical power,\nespecially for large sample sizes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 12:27:35 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Chen", "Wanfang", ""], ["Genton", "Marc G.", ""]]}, {"id": "2008.10982", "submitter": "Esa Ollila", "authors": "Esa Ollila and Ammar Mian", "title": "Block-wise Minimization-Majorization algorithm for Huber's criterion:\n  sparse learning and applications", "comments": "To appear in International Workshop on Machine Learning for Signal\n  Processing (MLSP), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huber's criterion can be used for robust joint estimation of regression and\nscale parameters in the linear model. Huber's (Huber, 1981) motivation for\nintroducing the criterion stemmed from non-convexity of the joint maximum\nlikelihood objective function as well as non-robustness (unbounded influence\nfunction) of the associated ML-estimate of scale. In this paper, we illustrate\nhow the original algorithm proposed by Huber can be set within the block-wise\nminimization majorization framework. In addition, we propose novel\ndata-adaptive step sizes for both the location and scale, which are further\nimproving the convergence. We then illustrate how Huber's criterion can be used\nfor sparse learning of underdetermined linear model using the iterative hard\nthresholding approach. We illustrate the usefulness of the algorithms in an\nimage denoising application and simulation studies.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 13:18:07 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Ollila", "Esa", ""], ["Mian", "Ammar", ""]]}, {"id": "2008.11095", "submitter": "George Wynne", "authors": "George Wynne, Andrew B. Duncan", "title": "A Kernel Two-Sample Test for Functional Data", "comments": "Added to numerics section", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric two-sample test procedure based on Maximum Mean\nDiscrepancy (MMD) for testing the hypothesis that two samples of functions have\nthe same underlying distribution, using kernels defined on function spaces.\nThis construction is motivated by a scaling analysis of the efficiency of\nMMD-based tests for datasets of increasing dimension. Theoretical properties of\nkernels on function spaces and their associated MMD are established and\nemployed to ascertain the efficacy of the newly proposed test, as well as to\nassess the effects of using functional reconstructions based on discretised\nfunction samples. The theoretical results are demonstrated over a range of\nsynthetic and real world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 15:19:02 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 12:02:06 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wynne", "George", ""], ["Duncan", "Andrew B.", ""]]}, {"id": "2008.11175", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee and Sourabh Bhattacharya", "title": "How Ominous is the Future Global Warming Premonition?", "comments": "Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global warming, the phenomenon of increasing global average temperature in\nthe recent decades, is receiving wide attention due to its very significant\nadverse effects on climate. Whether global warming will continue even in the\nfuture, is a question that is most important to investigate. In this regard,\nthe so-called general circulation models (GCMs) have attempted to project the\nfuture climate, and nearly all of them exhibit alarming rates of global\ntemperature rise in the future.\n  Although global warming in the current time frame is undeniable, it is\nimportant to assess the validity of the future predictions of the GCMs. In this\narticle, we attempt such a study using our recently-developed Bayesian multiple\ntesting paradigm for model selection in inverse regression problems. The model\nwe assume for the global temperature time series is based on Gaussian process\nemulation of the black box scenario, realistically treating the dynamic\nevolution of the time series as unknown.\n  We apply our ideas to datasets available from the Intergovernmental Panel on\nClimate Change (IPCC) website. The best GCM models selected by our method under\ndifferent assumptions on future climate change scenarios do not convincingly\nsupport the present global warming pattern when only the future predictions are\nconsidered known. Using our Gaussian process idea, we also forecast the future\ntemperature time series given the current one. Interestingly, our results do\nnot support drastic future global warming predicted by almost all the GCM\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 17:12:38 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "2008.11251", "submitter": "Sangwon Hyun", "authors": "Sangwon Hyun, Mattias Rolf Cape, Francois Ribalet, Jacob Bien", "title": "Modeling Cell Populations Measured By Flow Cytometry With Covariates\n  Using Sparse Mixture of Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ocean is filled with microscopic microalgae called phytoplankton, which\ntogether are responsible for as much photosynthesis as all plants on land\ncombined. Our ability to predict their response to the warming ocean relies on\nunderstanding how the dynamics of phytoplankton populations is influenced by\nchanges in environmental conditions. One powerful technique to study the\ndynamics of phytoplankton is flow cytometry, which measures the optical\nproperties of thousands of individual cells per second. Today, oceanographers\nare able to collect flow cytometry data in real-time onboard a moving ship,\nproviding them with fine-scale resolution of the distribution of phytoplankton\nacross thousands of kilometers. One of the current challenges is to understand\nhow these small and large scale variations relate to environmental conditions,\nsuch as nutrient availability, temperature, light and ocean currents. In this\npaper, we propose a novel sparse mixture of multivariate regressions model to\nestimate the time-varying phytoplankton subpopulations while simultaneously\nidentifying the specific environmental covariates that are predictive of the\nobserved changes to these subpopulations. We demonstrate the usefulness and\ninterpretability of the approach using both synthetic data and real\nobservations collected on an oceanographic cruise conducted in the north-east\nPacific in the spring of 2017.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 20:03:24 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Hyun", "Sangwon", ""], ["Cape", "Mattias Rolf", ""], ["Ribalet", "Francois", ""], ["Bien", "Jacob", ""]]}, {"id": "2008.11430", "submitter": "Carlotta Langer", "authors": "Carlotta Langer and Nihat Ay", "title": "Complexity as Causal Information Integration", "comments": null, "journal-ref": "Langer C, Ay N. Complexity as Causal Information Integration.\n  Entropy. 2020; 22(10):1107", "doi": "10.3390/e22101107", "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complexity measures in the context of the Integrated Information Theory of\nconsciousness try to quantify the strength of the causal connections between\ndifferent neurons. This is done by minimizing the KL-divergence between a full\nsystem and one without causal connections. Various measures have been proposed\nand compared in this setting. We will discuss a class of information geometric\nmeasures that aim at assessing the intrinsic causal influences in a system. One\npromising candidate of these measures, denoted by $\\Phi_{CIS}$, is based on\nconditional independence statements and does satisfy all of the properties that\nhave been postulated as desirable. Unfortunately it does not have a graphical\nrepresentation which makes it less intuitive and difficult to analyze. We\npropose an alternative approach using a latent variable which models a common\nexterior influence. This leads to a measure $\\Phi_{CII}$, Causal Information\nIntegration, that satisfies all of the required conditions. Our measure can be\ncalculated using an iterative information geometric algorithm, the\nem-algorithm. Therefore we are able to compare its behavior to existing\nintegrated information measures.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 07:57:36 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 12:55:31 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Langer", "Carlotta", ""], ["Ay", "Nihat", ""]]}, {"id": "2008.11474", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "A note on data splitting with e-values: online appendix to my comment on\n  Glenn Shafer's \"Testing by betting\"", "comments": "11 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note reanalyzes Cox's idealized example of testing with data splitting\nusing e-values (Shafer's betting scores). Cox's exciting finding was that the\nmethod of data splitting, while allowing flexible data analysis, achieves quite\nhigh efficiencies, of about 80%. The most serious objection to the method was\nthat it involves splitting data at random, and so different people analyzing\nthe same data may get very different answers. Using e-values instead of\np-values remedies this disadvantage.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 10:20:17 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "2008.11477", "submitter": "Rutger-Jan Lange", "authors": "Rutger-Jan Lange", "title": "Bellman filtering for state-space models", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a filter for state-space models based on Bellman's\ndynamic programming principle applied to the mode estimator. The proposed\nBellman filter generalises the Kalman filter including its extended and\niterated versions, while remaining equally inexpensive computationally. The\nBellman filter is also (unlike the Kalman filter) robust under heavy-tailed\nobservation noise and applicable to a wider range of (nonlinear and\nnon-Gaussian) models, involving e.g. count, intensity, duration, volatility and\ndependence. The Bellman-filtered states are shown to be convergent, in\nquadratic mean, towards a small region around the true state. (Hyper)parameters\nare estimated by numerically maximising a filter-implied log-likelihood\ndecomposition, which is an alternative to the classic prediction-error\ndecomposition for linear Gaussian models. Simulation studies reveal that the\nBellman filter performs on par with (or even outperforms) state-of-the-art\nsimulation-based techniques, e.g. particle filters and importance samplers,\nwhile requiring a fraction (e.g. 1%) of the computational cost, being\nstraightforward to implement and offering full scalability to higher\ndimensional state spaces.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 10:32:52 GMT"}, {"version": "v10", "created": "Wed, 3 Mar 2021 09:26:16 GMT"}, {"version": "v11", "created": "Fri, 14 May 2021 10:58:44 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 11:06:58 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 14:16:31 GMT"}, {"version": "v4", "created": "Wed, 2 Sep 2020 14:28:03 GMT"}, {"version": "v5", "created": "Thu, 3 Sep 2020 07:52:03 GMT"}, {"version": "v6", "created": "Wed, 9 Sep 2020 09:56:42 GMT"}, {"version": "v7", "created": "Thu, 7 Jan 2021 16:56:23 GMT"}, {"version": "v8", "created": "Mon, 11 Jan 2021 18:37:58 GMT"}, {"version": "v9", "created": "Fri, 29 Jan 2021 14:50:23 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Lange", "Rutger-Jan", ""]]}, {"id": "2008.11539", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran, Patr\\'icia Szokol and Marianna Szab\\'o", "title": "Truncated generalized extreme value distribution based EMOS model for\n  calibration of wind speed ensemble forecasts", "comments": "30 pages, 12 figures, 8 tables", "journal-ref": "Environmetrics (2021)", "doi": "10.1002/env.2678", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, ensemble weather forecasting have become a routine at all\nmajor weather prediction centres. These forecasts are obtained from multiple\nruns of numerical weather prediction models with different initial conditions\nor model parametrizations. However, ensemble forecasts can often be\nunderdispersive and also biased, so some kind of post-processing is needed to\naccount for these deficiencies. One of the most popular state of the art\nstatistical post-processing techniques is the ensemble model output statistics\n(EMOS), which provides a full predictive distribution of the studied weather\nquantity. We propose a novel EMOS model for calibrating wind speed ensemble\nforecasts, where the predictive distribution is a generalized extreme value\n(GEV) distribution left truncated at zero (TGEV). The truncation corrects the\ndisadvantage of the GEV distribution based EMOS models of occasionally\npredicting negative wind speed values, without affecting its favorable\nproperties. The new model is tested on four data sets of wind speed ensemble\nforecasts provided by three different ensemble prediction systems, covering\nvarious geographical domains and time periods. The forecast skill of the TGEV\nEMOS model is compared with the predictive performance of the truncated normal,\nlog-normal and GEV methods and the raw and climatological forecasts as well.\nThe results verify the advantageous properties of the novel TGEV EMOS approach.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 13:01:27 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 07:28:14 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Szokol", "Patr\u00edcia", ""], ["Szab\u00f3", "Marianna", ""]]}, {"id": "2008.11667", "submitter": "Wei Pan", "authors": "Beilin Jia (1), Donglin Zeng (1), Qing Yang (2), Wei Pan (2) ((1)\n  University of North Carolina at Chapel Hill, (2) Duke University)", "title": "Assessing Impact of Unobserved Confounders with Sensitivity Index\n  Probabilities through Pseudo-Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unobserved confounders are a long-standing issue in causal inference using\npropensity score methods. This study proposed nonparametric indices to quantify\nthe impact of unobserved confounders through pseudo-experiments with an\napplication to real-world data. The study finding suggests that the proposed\nindices can reflect the true impact of confounders. It is hoped that this study\nwill lead to further discussion on this important issue and help move the\nscience of causal inference forward.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:49:49 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Jia", "Beilin", ""], ["Zeng", "Donglin", ""], ["Yang", "Qing", ""], ["Pan", "Wei", ""]]}, {"id": "2008.11682", "submitter": "Zhou Fang", "authors": "Zhou Fang, Ankit Gupta, Mustafa Khammash", "title": "Stochastic filters based on hybrid approximations of multiscale\n  stochastic reaction networks", "comments": "6 pages, 1 figure. Accepted to CDC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the dynamic latent states of an\nintracellular multiscale stochastic reaction network from time-course\nmeasurements of fluorescent reporters. We first prove that accurate solutions\nto the filtering problem can be constructed by solving the filtering problem\nfor a reduced model that represents the dynamics as a hybrid process. The model\nreduction is based on exploiting the time-scale separations in the original\nnetwork, and it can greatly reduce the computational effort required to\nsimulate the dynamics. This enables us to develop efficient particle filters to\nsolve the filtering problem for the original model by applying particle filters\nto the reduced model. We illustrate the accuracy and the computational\nefficiency of our approach using a numerical example.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 17:17:16 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 08:17:50 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Fang", "Zhou", ""], ["Gupta", "Ankit", ""], ["Khammash", "Mustafa", ""]]}, {"id": "2008.11696", "submitter": "David Fan", "authors": "David P. Fan (University of Minnesota, USA)", "title": "Disappearing errors in a conversion model", "comments": "17 pages, 2 figures, to be published in the book \"Measurement Error\n  in Longitudinal Data\" by Oxford University Press, edited by Alexandru Cernat\n  and Joe Sakshaug, scheduled for January 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The same basic differential equation model has been adapted for\ntime-dependent conversions of members of a population among different states.\nThe conversion model has been applied in different contexts such as\nepidemiological infections, the Bass model for the diffusion of innovations,\nand the ideodynamic model for public opinion. For example, the ideodynamic\nversion of the model predicts changes in public opinions in response to\npersuasive messages extending back to an indefinite past. All messages are\nmeasured with error, and this chapter discusses how errors in message\nmeasurements disappear with time so that predicted opinion values gradually\nbecome unaffected by past measurement errors. Prediction uncertainty is\ndiscussed using formal statistics, sensitivity analysis and bootstrap variance\ncalculations. This chapter presents ideodynamic predictions for opinion time\nseries about the Toyota car manufacturer calculated from daily Twitter scores\nover two and half years. During this time, there was a sudden onslaught of bad\nnews for Toyota, and the model could accurately predict the accompanying drop\nin favorable Toyota opinion and rise in unfavorable opinion.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 17:37:45 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Fan", "David P.", "", "University of Minnesota, USA"]]}, {"id": "2008.11750", "submitter": "Francisco Mois\\'es C\\^andido de Medeiros", "authors": "Francisco M.C. Medeiros, Mariana C. Ara\\'ujo, Marcelo Bourguignon", "title": "Improved estimators in beta prime regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the beta prime regression model recently proposed\nby \\cite{bour18}, which is tailored to situations where the response is\ncontinuous and restricted to the positive real line with skewed and long tails\nand the regression structure involves regressors and unknown parameters. We\nconsider two different strategies of bias correction of the maximum-likelihood\nestimators for the parameters that index the model. In particular, we discuss\nbias-corrected estimators for the mean and the dispersion parameters of the\nmodel. Furthermore, as an alternative to the two analytically bias-corrected\nestimators discussed, we consider a bias correction mechanism based on the\nparametric bootstrap. The numerical results show that the bias correction\nscheme yields nearly unbiased estimates. An example with real data is presented\nand discussed.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 18:14:35 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Medeiros", "Francisco M. C.", ""], ["Ara\u00fajo", "Mariana C.", ""], ["Bourguignon", "Marcelo", ""]]}, {"id": "2008.11756", "submitter": "Daniel Eck", "authors": "Jilei Lin and Daniel J. Eck", "title": "Minimizing post-shock forecasting error through aggregation of outside\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a forecasting methodology for providing credible forecasts for\ntime series that have recently undergone a shock. We achieve this by borrowing\nknowledge from other time series that have undergone similar shocks for which\npost-shock outcomes are observed. Three shock effect estimators are motivated\nwith the aim of minimizing average forecast risk. We propose risk-reduction\npropositions that provide conditions that establish when our methodology works.\nBootstrap and leave-one-out cross validation procedures are provided to\nprospectively assess the performance of our methodology. Several simulated data\nexamples, and a real data example of forecasting Conoco Phillips stock price\nare provided for verification and illustration.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 18:33:35 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Lin", "Jilei", ""], ["Eck", "Daniel J.", ""]]}, {"id": "2008.11765", "submitter": "Masoud Faridi", "authors": "Masoud Faridi and Majid Jafari Khaledi", "title": "The polar-generalized normal distribution", "comments": "25 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an extension to the normal distribution through the\npolar method to capture bimodality and asymmetry, which are often observed\ncharacteristics of empirical data. The later two features are entirely\ncontrolled by a separate scalar parameter. Explicit expressions for the\ncumulative distribution function, the density function and the moments were\nderived. The stochastic representation of the distribution facilitates\nimplementing Bayesian estimation via the Markov chain Monte Carlo methods. Some\nreal-life data as well as simulated data are analyzed to illustrate the\nflexibility of the distribution for modeling asymmetric bimodality.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 18:56:07 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Faridi", "Masoud", ""], ["Khaledi", "Majid Jafari", ""]]}, {"id": "2008.11866", "submitter": "Maude Wagner", "authors": "Maude Wagner, Francine Grodstein, Karen Leffondre, C\\'ecilia Samieri,\n  and C\\'ecile Proust-Lima", "title": "Time-varying exposure history and subsequent health outcomes: a\n  two-stage approach to identify critical windows", "comments": "Pages 35, Main Figures 5, Web Figures 13, Work presented at the\n  Alzheimers Association International eConference (2020), eMELODEM conference,\n  MEthods for LOngitudinal studies in DEMentia (2020), and ISCB, International\n  Society for Clinical Biostatistics (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term behavioral and health risk factors constitute a primary focus of\nresearch on the etiology of chronic diseases. Yet, identifying critical\ntime-windows during which risk factors have the strongest impact on disease\nrisk is challenging. To assess the trajectory of association of an exposure\nhistory with an outcome, the weighted cumulative exposure index (WCIE) has been\nproposed, with weights reflecting the relative importance of exposures at\ndifferent times. However, WCIE is restricted to a complete observed error-free\nexposure whereas exposures are often measured with intermittent missingness and\nerror. Moreover, it rarely explores exposure history that is very distant from\nthe outcome as usually sought in life-course epidemiology. We extend the WCIE\nmethodology to (i) exposures that are intermittently measured with error, and\n(ii) contexts where the exposure time-window precedes the outcome time-window\nusing a landmark approach. First, the individual exposure history up to the\nlandmark time is estimated using a mixed model that handles missing data and\nerror in exposure measurement, and the predicted complete error-free exposure\nhistory is derived. Then the WCIE methodology is applied to assess the\ntrajectory of association between the predicted exposure history and the health\noutcome collected after the landmark time. In our context, the health outcome\nis a longitudinal marker analyzed using a mixed model. A simulation study first\ndemonstrates the correct inference obtained with this approach. Then, applied\nto the Nurses' Health Study (19,415 women) to investigate the association\nbetween BMI history (collected from midlife) and subsequent cognitive decline\nafter age 70. In conclusion, this approach, easy to implement, provides a\nflexible tool for studying complex dynamic relationships and identifying\ncritical time windows while accounting for exposure measurement errors.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 00:16:09 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 08:18:41 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 19:17:37 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Wagner", "Maude", ""], ["Grodstein", "Francine", ""], ["Leffondre", "Karen", ""], ["Samieri", "C\u00e9cilia", ""], ["Proust-Lima", "C\u00e9cile", ""]]}, {"id": "2008.11892", "submitter": "Zhou Fan", "authors": "Zhou Fan", "title": "Approximate Message Passing algorithms for rotationally invariant\n  matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Message Passing (AMP) algorithms have seen widespread use across\na variety of applications. However, the precise forms for their Onsager\ncorrections and state evolutions depend on properties of the underlying random\nmatrix ensemble, limiting the extent to which AMP algorithms derived for white\nnoise may be applicable to data matrices that arise in practice.\n  In this work, we study more general AMP algorithms for random matrices $W$\nthat satisfy orthogonal rotational invariance in law, where $W$ may have a\nspectral distribution that is different from the semicircle and Marcenko-Pastur\nlaws characteristic of white noise. The Onsager corrections and state\nevolutions in these algorithms are defined by the free cumulants or rectangular\nfree cumulants of the spectral distribution of $W$. Their forms were derived\npreviously by Opper, \\c{C}akmak, and Winther using non-rigorous dynamic\nfunctional theory techniques, and we provide rigorous proofs.\n  Our motivating application is a Bayes-AMP algorithm for Principal Components\nAnalysis, when there is prior structure for the principal components (PCs) and\npossibly non-white noise. For sufficiently large signal strengths and any\nnon-Gaussian prior distributions for the PCs, we show that this algorithm\nprovably achieves higher estimation accuracy than the sample PCs.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 02:35:16 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 19:40:49 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 23:18:11 GMT"}, {"version": "v4", "created": "Fri, 30 Apr 2021 14:49:14 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Fan", "Zhou", ""]]}, {"id": "2008.11997", "submitter": "Andrew Grant", "authors": "Andrew J. Grant and Stephen Burgess", "title": "Pleiotropy robust methods for multivariable Mendelian randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian randomization is a powerful tool for inferring the presence, or\notherwise, of causal effects from observational data. However, the nature of\ngenetic variants is such that pleiotropy remains a barrier to valid causal\neffect estimation. There are many options in the literature for pleiotropy\nrobust methods when studying the effects of a single risk factor on an outcome.\nHowever, there are few pleiotropy robust methods in the multivariable setting,\nthat is, when there are multiple risk factors of interest. In this paper we\nintroduce three methods which build on common approaches in the univariable\nsetting: MVMR-Robust; MVMR-Median; and MVMR-Lasso. We discuss the properties of\neach of these methods and examine their performance in comparison to existing\napproaches in a simulation study. MVMR-Robust is shown to outperform existing\noutlier robust approaches when there are low levels of pleiotropy. MVMR-Lasso\nprovides the best estimation in terms of mean squared error for moderate to\nhigh levels of pleiotropy, and can provide valid inference in a three sample\nsetting. MVMR-Median performs well in terms of estimation across all scenarios\nconsidered, and provides valid inference up to a moderate level of pleiotropy.\nWe demonstrate the methods in an applied example looking at the effects of\nintelligence, education and household income on the risk of Alzheimer's\ndisease.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 09:05:14 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Grant", "Andrew J.", ""], ["Burgess", "Stephen", ""]]}, {"id": "2008.12138", "submitter": "Galit Shmueli", "authors": "Galit Shmueli", "title": "\"Improving\" prediction of human behavior using behavior modification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fields of statistics and machine learning design algorithms, models, and\napproaches to improve prediction. Larger and richer behavioral data increase\npredictive power, as evident from recent advances in behavioral prediction\ntechnology. Large internet platforms that collect behavioral big data predict\nuser behavior for internal purposes and for third parties (advertisers,\ninsurers, security forces, political consulting firms) who utilize the\npredictions for personalization, targeting and other decision-making. While\nstandard data collection and modeling efforts are directed at improving\npredicted values, internet platforms can minimize prediction error by \"pushing\"\nusers' actions towards their predicted values using behavior modification\ntechniques. The better the platform can make users conform to their predicted\noutcomes, the more it can boast its predictive accuracy and ability to induce\nbehavior change. Hence, platforms are strongly incentivized to \"make\npredictions true\". This strategy is absent from the ML and statistics\nliterature. Investigating its properties requires incorporating causal notation\ninto the correlation-based predictive environment---an integration currently\nmissing. To tackle this void, we integrate Pearl's causal do(.) operator into\nthe predictive framework. We then decompose the expected prediction error given\nbehavior modification, and identify the components impacting predictive power.\nOur derivation elucidates the implications of such behavior modification to\ndata scientists, platforms, their clients, and the humans whose behavior is\nmanipulated. Behavior modification can make users' behavior more predictable\nand even more homogeneous; yet this apparent predictability might not\ngeneralize when clients use predictions in practice. Outcomes pushed towards\ntheir predictions can be at odds with clients' intentions, and harmful to\nmanipulated users.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 12:39:35 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Shmueli", "Galit", ""]]}, {"id": "2008.12395", "submitter": "Abhimanyu Gupta", "authors": "Abhimanyu Gupta", "title": "Efficient closed-form estimation of large spatial autoregressions", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Newton-step approximations to pseudo maximum likelihood estimates of spatial\nautoregressive models with a large number of parameters are examined, in the\nsense that the parameter space grows slowly as a function of sample size. These\nhave the same asymptotic efficiency properties as maximum likelihood under\nGaussianity but are of closed form. Hence they are computationally simple and\nfree from compactness assumptions, thereby avoiding two notorious pitfalls of\nimplicitly defined estimates of large spatial autoregressions. For an initial\nleast squares estimate, the Newton step can also lead to weaker regularity\nconditions for a central limit theorem than those extant in the literature. A\nsimulation study demonstrates excellent finite sample gains from Newton\niterations, especially in large multiparameter models for which grid search is\ncostly. A small empirical illustration shows improvements in estimation\nprecision with real data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 22:32:37 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 11:51:02 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 10:24:47 GMT"}, {"version": "v4", "created": "Sat, 22 May 2021 20:54:09 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Gupta", "Abhimanyu", ""]]}, {"id": "2008.12411", "submitter": "Jae Youn Ahn", "authors": "Jae Youn Ahn and Sebastian Fuchs and Rosy Oh", "title": "A copula transformation in multivariate mixed discrete-continuous models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copulas allow a flexible and simultaneous modeling of complicated dependence\nstructures together with various marginal distributions. Especially if the\ndensity function can be represented as the product of the marginal density\nfunctions and the copula density function, this leads to both an intuitive\ninterpretation of the conditional distribution and convenient estimation\nprocedures. However, this is no longer the case for copula models with mixed\ndiscrete and continuous marginal distributions, because the corresponding\ndensity function cannot be decomposed so nicely. In this paper, we introduce a\ncopula transformation method that allows to represent the density function of a\ndistribution with mixed discrete and continuous marginals as the product of the\nmarginal probability mass/density functions and the copula density function.\nWith the proposed method, conditional distributions can be described\nanalytically and the computational complexity in the estimation procedure can\nbe reduced depending on the type of copula used.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 23:44:12 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Ahn", "Jae Youn", ""], ["Fuchs", "Sebastian", ""], ["Oh", "Rosy", ""]]}, {"id": "2008.12467", "submitter": "Molei Liu", "authors": "Molei Liu", "title": "A Note on Debiased/Double Machine Learning Logistic Partially Linear\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is of particular interests in many application fields to draw doubly\nrobust inference of a logistic partially linear model with the predictor\nspecified as combination of a targeted low dimensional linear parametric\nfunction and a nuisance nonparametric function. In recent, Tan (2019) proposed\na simple and flexible doubly robust estimator for this purpose. They introduced\nthe two nuisance models, i.e. nonparametric component in the logistic model and\nconditional mean of the exposure covariates given the other covariates and\nfixed response, and specified them as fixed dimensional parametric models.\nTheir framework could be potentially extended to machine learning or high\ndimensional nuisance modelling exploited recently, e.g. in Chernozhukovet al.\n(2018a,b) and Smucler et al. (2019); Tan (2020). Motivated by this, we derive\nthe debiased/double machine learning logistic partially linear model in this\nnote. For construction of the nuisance models, we separately consider the use\nof high dimensional sparse parametric models and general machine learning\nmethods. By deriving certain moment equations to calibrate the first order bias\nof the nuisance models, we preserve a model double robustness property on high\ndimensional ultra-sparse nuisance models. We also discuss and compare the\nunderlying assumption of our method with debiased LASSO (Van deGeer et al.,\n2014). To implement the machine learning proposal, we design a full model\nrefitting procedure that allows the use of any blackbox conditional mean\nestimation method in our framework. Under the machine learning setting, our\nmethod is rate doubly robust in a similar sense as Chernozhukov et al. (2018a).\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 03:40:13 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Liu", "Molei", ""]]}, {"id": "2008.12682", "submitter": "Johannes Resin", "authors": "Johannes Resin", "title": "A Simple Algorithm for Exact Multinomial Tests", "comments": "27 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new method for computing acceptance regions of exact\nmultinomial tests. From this an algorithm is derived, which finds exact\np-values for tests of simple multinomial hypotheses. Using concepts from\ndiscrete convex analysis, the method is proven to be exact for various popular\ntest statistics, including Pearson's chi-square and the log-likelihood ratio.\nThe proposed algorithm improves greatly on the naive approach using full\nenumeration of the sample space. However, its use is limited to multinomial\ndistributions with a small number of categories, as the runtime grows\nexponentially in the number of possible outcomes.\n  The method is applied in a simulation study and uses of multinomial tests in\nforecast evaluation are outlined. Additionally, properties of a test statistic\nusing probability ordering, referred to as the \"exact multinomial test\" by some\nauthors, are investigated and discussed. The algorithm is implemented in the\naccompanying R package ExactMultinom.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 14:36:10 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Resin", "Johannes", ""]]}, {"id": "2008.12774", "submitter": "Tianyu Zhan", "authors": "Tianyu Zhan, Yiwang Zhou, Ziqian Geng, Yihua Gu, Jian Kang, Li Wang,\n  Xiaohong Huang and Elizabeth H. Slate", "title": "Deep Historical Borrowing Framework to Prospectively and Simultaneously\n  Synthesize Control Information in Confirmatory Clinical Trials with Multiple\n  Endpoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current clinical trial development, historical information is receiving\nmore attention as providing value beyond sample size calculation.\nMeta-analytic-predictive (MAP) priors and robust MAP priors have been proposed\nfor prospectively borrowing historical data on a single endpoint. To\nsimultaneously synthesize control information from multiple endpoints in\nconfirmatory clinical trials, we propose to approximate posterior probabilities\nfrom a Bayesian hierarchical model and estimate critical values by deep\nlearning to construct pre-specified decision functions before the trial\nconduct. Simulation studies and a case study demonstrate that our method\nadditionally preserves power, and has a satisfactory performance under\nprior-data conflict.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 17:52:46 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Zhan", "Tianyu", ""], ["Zhou", "Yiwang", ""], ["Geng", "Ziqian", ""], ["Gu", "Yihua", ""], ["Kang", "Jian", ""], ["Wang", "Li", ""], ["Huang", "Xiaohong", ""], ["Slate", "Elizabeth H.", ""]]}, {"id": "2008.12807", "submitter": "Olli Saarela", "authors": "Bo Chen, Keith A. Lawson, Antonio Finelli, Olli Saarela", "title": "Causal mediation analysis decomposition of between-hospital variance", "comments": "Health Serv Outcomes Res Method (2021)", "journal-ref": null, "doi": "10.1007/s10742-021-00256-6", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal variance decompositions for a given disease-specific quality indicator\ncan be used to quantify differences in performance between hospitals or health\ncare providers. While variance decompositions can demonstrate variation in\nquality of care, causal mediation analysis can be used to study care pathways\nleading to the differences in performance between the institutions. This raises\nthe question of whether the two approaches can be combined to decompose\nbetween-hospital variation in an outcome type indicator to that mediated\nthrough a given process (indirect effect) and remaining variation due to all\nother pathways (direct effect). For this purpose, we derive a causal mediation\nanalysis decomposition of between-hospital variance, discuss its\ninterpretation, and propose an estimation approach based on generalized linear\nmixed models for the outcome and the mediator. We study the performance of the\nestimators in a simulation study and demonstrate its use in administrative data\non kidney cancer care in Ontario.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 18:33:09 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 22:34:09 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 16:22:22 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chen", "Bo", ""], ["Lawson", "Keith A.", ""], ["Finelli", "Antonio", ""], ["Saarela", "Olli", ""]]}, {"id": "2008.12812", "submitter": "Soojin Park", "authors": "Soojin Park, Chioun Lee, and Xu Qin", "title": "Estimation and Sensitivity Analysis for Causal Decomposition in Heath\n  Disparity Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of disparities research, there has been growing interest in\ndeveloping a counterfactual-based decomposition analysis to identify underlying\nmediating mechanisms that help reduce disparities in populations. Despite rapid\ndevelopment in the area, most prior studies have been limited to\nregression-based methods, undermining the possibility of addressing complex\nmodels with multiple mediators and/or heterogeneous effects. We propose an\nestimation method that effectively addresses complex models. Moreover, we\ndevelop a novel sensitivity analysis for possible violations of identification\nassumptions. The proposed method and sensitivity analysis are demonstrated with\ndata from the Midlife Development in the US study to investigate the degree to\nwhich disparities in cardiovascular health at the intersection of race and\ngender would be reduced if the distributions of education and perceived\ndiscrimination were the same across intersectional groups.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 18:49:24 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Park", "Soojin", ""], ["Lee", "Chioun", ""], ["Qin", "Xu", ""]]}, {"id": "2008.12857", "submitter": "Austin Cole", "authors": "D. Austin Cole, Ryan Christianson, Robert B. Gramacy", "title": "Locally induced Gaussian processes for large-scale simulation\n  experiments", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) serve as flexible surrogates for complex surfaces,\nbut buckle under the cubic cost of matrix decompositions with big training data\nsizes. Geospatial and machine learning communities suggest pseudo-inputs, or\ninducing points, as one strategy to obtain an approximation easing that\ncomputational burden. However, we show how placement of inducing points and\ntheir multitude can be thwarted by pathologies, especially in large-scale\ndynamic response surface modeling tasks. As remedy, we suggest porting the\ninducing point idea, which is usually applied globally, over to a more local\ncontext where selection is both easier and faster. In this way, our proposed\nmethodology hybridizes global inducing point and data subset-based local GP\napproximation. A cascade of strategies for planning the selection of local\ninducing points is provided, and comparisons are drawn to related methodology\nwith emphasis on computer surrogate modeling applications. We show that local\ninducing points extend their global and data-subset component parts on the\naccuracy--computational efficiency frontier. Illustrative examples are provided\non benchmark data and a large-scale real-simulation satellite drag\ninterpolation problem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 21:37:46 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 20:57:43 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Cole", "D. Austin", ""], ["Christianson", "Ryan", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "2008.12882", "submitter": "Bhaswar Bhattacharya", "authors": "Somabha Mukherjee, Jaesung Son, and Bhaswar B. Bhattacharya", "title": "Estimation in Tensor Ising Models", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $p$-tensor Ising model is a one-parameter discrete exponential family for\nmodeling dependent binary data, where the sufficient statistic is a\nmulti-linear form of degree $p \\geq 2$. This is a natural generalization of the\nmatrix Ising model, that provides a convenient mathematical framework for\ncapturing higher-order dependencies in complex relational data. In this paper,\nwe consider the problem of estimating the natural parameter of the $p$-tensor\nIsing model given a single sample from the distribution on $N$ nodes. Our\nestimate is based on the maximum pseudo-likelihood (MPL) method, which provides\na computationally efficient algorithm for estimating the parameter that avoids\ncomputing the intractable partition function. We derive general conditions\nunder which the MPL estimate is $\\sqrt N$-consistent, that is, it converges to\nthe true parameter at rate $1/\\sqrt N$. In particular, we show the $\\sqrt\nN$-consistency of the MPL estimate in the $p$-spin Sherrington-Kirkpatrick (SK)\nmodel, spin systems on general $p$-uniform hypergraphs, and Ising models on the\nhypergraph stochastic block model (HSBM). In fact, for the HSBM we pin down the\nexact location of the phase transition threshold, which is determined by the\npositivity of a certain mean-field variational problem, such that above this\nthreshold the MPL estimate is $\\sqrt N$-consistent, while below the threshold\nno estimator is consistent. Finally, we derive the precise fluctuations of the\nMPL estimate in the special case of the $p$-tensor Curie-Weiss model. An\ninteresting consequence of our results is that the MPL estimate in the\nCurie-Weiss model saturates the Cramer-Rao lower bound at all points above the\nestimation threshold, that is, the MPL estimate incurs no loss in asymptotic\nefficiency, even though it is obtained by minimizing only an approximation of\nthe true likelihood function for computational tractability.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 00:06:58 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Mukherjee", "Somabha", ""], ["Son", "Jaesung", ""], ["Bhattacharya", "Bhaswar B.", ""]]}, {"id": "2008.12885", "submitter": "Xinghao Qiao", "authors": "Jinyuan Chang and Cheng Chen and Xinghao Qiao", "title": "An autocovariance-based learning framework for high-dimensional\n  functional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and economic applications involve the analysis of\nhigh-dimensional functional time series, which stands at the intersection\nbetween functional time series and high-dimensional statistics gathering\nchallenges of infinite-dimensionality with serial dependence and\nnon-asymptotics. In this paper, we model observed functional time series, which\nare subject to errors in the sense that each functional datum arises as the sum\nof two uncorrelated components, one dynamic and one white noise. Motivated from\na simple fact that the autocovariance function of observed functional time\nseries automatically filters out the noise term, we propose an\nautocovariance-based three-step procedure by first performing\nautocovariance-based dimension reduction and then formulating a novel\nautocovariance-based block regularized minimum distance (RMD) estimation\nframework to produce block sparse estimates, from which we can finally recover\nfunctional sparse estimates. We investigate non-asymptotic properties of\nrelevant estimated terms under such autocovariance-based dimension reduction\nframework. To provide theoretical guarantees for the second step, we also\npresent convergence analysis of the block RMD estimator. Finally, we illustrate\nthe proposed autocovariance-based learning framework using applications of\nthree sparse high-dimensional functional time series models. With derived\ntheoretical results, we study convergence properties of the associated\nestimators. We demonstrate via simulated and real datasets that our proposed\nestimators significantly outperform the competitors.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 00:33:26 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chang", "Jinyuan", ""], ["Chen", "Cheng", ""], ["Qiao", "Xinghao", ""]]}, {"id": "2008.12887", "submitter": "Marta Bofill Roig", "authors": "Marta Bofill Roig, Yu Shen and Guadalupe G\\'omez Melis", "title": "Design of phase III trials with long-term survival outcomes based on\n  short-term binary results", "comments": null, "journal-ref": "Statistics in Medicine, 2021", "doi": "10.1002/sim.9018", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pathologic complete response (pCR) is a common primary endpoint for a phase\nII trial or even accelerated approval of neoadjuvant cancer therapy. If\ngranted, a two-arm confirmatory trial is often required to demonstrate the\nefficacy with a time-to-event outcome such as overall survival. However, the\ndesign of a subsequent phase III trial based on prior information on the pCR\neffect is not straightforward. Aiming at designing such phase III trials with\noverall survival as primary endpoint using pCR information from previous\ntrials, we consider a mixture model that incorporates both the survival and the\nbinary endpoints. We propose to base the comparison between arms on the\ndifference of the restricted mean survival times, and show how the effect size\nand sample size for overall survival rely on the probability of the binary\nresponse and the survival distribution by response status, both for each\ntreatment arm. Moreover, we provide the sample size calculation under different\nscenarios and accompany them with an R package where all the computations have\nbeen implemented. We evaluate our proposal with a simulation study, and\nillustrate its application through a neoadjuvant breast cancer trial.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 01:05:37 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 20:38:25 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 07:10:49 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Roig", "Marta Bofill", ""], ["Shen", "Yu", ""], ["Melis", "Guadalupe G\u00f3mez", ""]]}, {"id": "2008.12892", "submitter": "Dominik Rothenh\\\"ausler", "authors": "Dominik Rothenh\\\"ausler", "title": "Model selection for estimation of causal parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular technique for selecting and tuning machine learning estimators is\ncross-validation. Cross-validation evaluates overall model fit, usually in\nterms of predictive accuracy. In causal inference, the optimal choice of\nestimator depends not only on the fitted models but also on assumptions the\nstatistician is willing to make. In this case, the performance of different\n(potentially biased) estimators cannot be evaluated by checking overall model\nfit. We propose a model selection procedure that estimates the squared\nl2-deviation of a finite-dimensional estimator from its target. The procedure\nrelies on knowing an asymptotically unbiased \"benchmark estimator\" of the\nparameter of interest. Under regularity conditions, we investigate bias and\nvariance of the proposed criterion compared to competing procedures and derive\na finite-sample bound for the excess risk compared to an oracle procedure. The\nresulting estimator is discontinuous and does not have a Gaussian limit\ndistribution. Thus, standard asymptotic expansions do not apply. We derive\nasymptotically valid confidence intervals that take into account the model\nselection step. The performance of the approach for estimation and inference\nfor average treatment effects is evaluated on simulated data sets, including\nexperimental data, instrumental variables settings, and observational data with\nselection on observables.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 02:08:11 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 04:51:07 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Rothenh\u00e4usler", "Dominik", ""]]}, {"id": "2008.12919", "submitter": "Jiayi Wang", "authors": "Jiayi Wang, Raymond K. W. Wong, Xiaoke Zhang", "title": "Low-Rank Covariance Function Estimation for Multidimensional Functional\n  Data", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional function data arise from many fields nowadays. The\ncovariance function plays an important role in the analysis of such\nincreasingly common data. In this paper, we propose a novel nonparametric\ncovariance function estimation approach under the framework of reproducing\nkernel Hilbert spaces (RKHS) that can handle both sparse and dense functional\ndata. We extend multilinear rank structures for (finite-dimensional) tensors to\nfunctions, which allow for flexible modeling of both covariance operators and\nmarginal structures. The proposed framework can guarantee that the resulting\nestimator is automatically semi-positive definite, and can incorporate various\nspectral regularizations. The trace-norm regularization in particular can\npromote low ranks for both covariance operator and marginal structures. Despite\nthe lack of a closed form, under mild assumptions, the proposed estimator can\nachieve unified theoretical results that hold for any relative magnitudes\nbetween the sample size and the number of observations per sample field, and\nthe rate of convergence reveals the \"phase-transition\" phenomenon from sparse\nto dense functional data. Based on a new representer theorem, an ADMM algorithm\nis developed for the trace-norm regularization. The appealing numerical\nperformance of the proposed estimator is demonstrated by a simulation study and\nthe analysis of a dataset from the Argo project.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 05:55:31 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Wang", "Jiayi", ""], ["Wong", "Raymond K. W.", ""], ["Zhang", "Xiaoke", ""]]}, {"id": "2008.12927", "submitter": "Kejun He", "authors": "Ya Zhou, Raymond K. W. Wong and Kejun He", "title": "Broadcasted Nonparametric Tensor Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel broadcasting idea to model the nonlinearity in tensor\nregression non-parametrically. Unlike existing non-parametric tensor regression\nmodels, the resulting model strikes a good balance between flexibility and\ninterpretability. A penalized estimation and corresponding algorithm are\nproposed. Our theoretical investigation, which allows the dimensions of the\ntensor covariate to diverge, indicates that the proposed estimation enjoys\ndesirable convergence rate. Numerical experiments are conducted to confirm the\ntheoretical finding and show that the proposed model has advantage over\nexisting linear counterparts.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 07:27:53 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Zhou", "Ya", ""], ["Wong", "Raymond K. W.", ""], ["He", "Kejun", ""]]}, {"id": "2008.12974", "submitter": "Peter Rousseeuw", "authors": "Iwein Vranckx, Jakob Raymaekers, Bart De Ketelaere, Peter J.\n  Rousseeuw, Mia Hubert", "title": "Real-time discriminant analysis in the presence of label and measurement\n  noise", "comments": null, "journal-ref": "Chemometrics and Intelligent Laboratory Systems, 2021, Volume 208", "doi": "10.1016/j.chemolab.2020.104197", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quadratic discriminant analysis (QDA) is a widely used classification\ntechnique. Based on a training dataset, each class in the data is characterized\nby an estimate of its center and shape, which can then be used to assign unseen\nobservations to one of the classes. The traditional QDA rule relies on the\nempirical mean and covariance matrix. Unfortunately, these estimators are\nsensitive to label and measurement noise which often impairs the model's\npredictive ability. Robust estimators of location and scatter are resistant to\nthis type of contamination. However, they have a prohibitive computational cost\nfor large scale industrial experiments. We present a novel QDA method based on\na recent real-time robust algorithm. We additionally integrate an anomaly\ndetection step to classify the most atypical observations into a separate class\nof outliers. Finally, we introduce the label bias plot, a graphical display to\nidentify label and measurement noise in the training data. The performance of\nthe proposed approach is illustrated in a simulation study with huge datasets,\nand on real datasets about diabetes and fruit.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 13:19:54 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 12:18:42 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Vranckx", "Iwein", ""], ["Raymaekers", "Jakob", ""], ["De Ketelaere", "Bart", ""], ["Rousseeuw", "Peter J.", ""], ["Hubert", "Mia", ""]]}, {"id": "2008.12989", "submitter": "Yuanyao Tan", "authors": "Yuanyao Tan, Xialing Wen, Wei Liang, Ying Yan", "title": "Empirical Likelihood Weighted Estimation of Average Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been growing attention on how to effectively and objectively use\ncovariate information when the primary goal is to estimate the average\ntreatment effect (ATE) in randomized clinical trials (RCTs). In this paper, we\npropose an effective weighting approach to extract covariate information based\non the empirical likelihood (EL) method. The resulting two-sample empirical\nlikelihood weighted (ELW) estimator includes two classes of weights, which are\nobtained from a constrained empirical likelihood estimation procedure, where\nthe covariate information is effectively incorporated into the form of general\nestimating equations. Furthermore, this ELW approach separates the estimation\nof ATE from the analysis of the covariate-outcome relationship, which implies\nthat our approach maintains objectivity. In theory, we show that the proposed\nELW estimator is semiparametric efficient. We extend our estimator to tackle\nthe scenarios where the outcomes are missing at random (MAR), and prove the\ndouble robustness and multiple robustness properties of our estimator.\nFurthermore, we derive the semiparametric efficiency bound of all regular and\nasymptotically linear semiparametric ATE estimators under MAR mechanism and\nprove that our proposed estimator attains this bound. We conduct simulations to\nmake comparisons with other existing estimators, which confirm the efficiency\nand multiple robustness property of our proposed ELW estimator. An application\nto the AIDS Clinical Trials Group Protocol 175 (ACTG 175) data is conducted.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 15:00:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Tan", "Yuanyao", ""], ["Wen", "Xialing", ""], ["Liang", "Wei", ""], ["Yan", "Ying", ""]]}, {"id": "2008.12991", "submitter": "Zad Rafi", "authors": "Zad Rafi and Sander Greenland", "title": "Technical Issues in the Interpretation of S-values and Their Relation to\n  Other Information Measures", "comments": "6 pages, 18 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An extended technical discussion of $S$-values and unconditional information\ncan be found in Greenland, 2019. Here we briefly cover several technical topics\nmentioned in our main paper, Rafi & Greenland, 2020: Different units for\n(scaling of) the $S$-value besides base-2 logs (bits); the importance of\nuniformity (validity) of the $P$-value for interpretation of the $S$-value; and\nthe relation of the $S$-value to other measures of statistical information\nabout a test hypothesis or model.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 15:13:42 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 17:03:23 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 16:53:44 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Rafi", "Zad", ""], ["Greenland", "Sander", ""]]}, {"id": "2008.13066", "submitter": "Won Chang", "authors": "Saumya Bhatnagar, Won Chang, Seonjin Kim Jiali Wang", "title": "Computer Model Calibration with Time Series Data using Deep Learning and\n  Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer models play a key role in many scientific and engineering problems.\nOne major source of uncertainty in computer model experiment is input parameter\nuncertainty. Computer model calibration is a formal statistical procedure to\ninfer input parameters by combining information from model runs and\nobservational data. The existing standard calibration framework suffers from\ninferential issues when the model output and observational data are\nhigh-dimensional dependent data such as large time series due to the difficulty\nin building an emulator and the non-identifiability between effects from input\nparameters and data-model discrepancy. To overcome these challenges we propose\na new calibration framework based on a deep neural network (DNN) with\nlong-short term memory layers that directly emulates the inverse relationship\nbetween the model output and input parameters. Adopting the 'learning with\nnoise' idea we train our DNN model to filter out the effects from data model\ndiscrepancy on input parameter inference. We also formulate a new way to\nconstruct interval predictions for DNN using quantile regression to quantify\nthe uncertainty in input parameter estimates. Through a simulation study and\nreal data application with WRF-hydro model we show that our approach can yield\naccurate point estimates and well calibrated interval estimates for input\nparameters.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 22:18:41 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 06:10:09 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Bhatnagar", "Saumya", ""], ["Chang", "Won", ""], ["Wang", "Seonjin Kim Jiali", ""]]}, {"id": "2008.13068", "submitter": "Hsien-Wei Chen", "authors": "Hsien-Wei Chen", "title": "Modeling of Daily Precipitation Amounts Using the Mixed Gamma Weibull\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By recognizing that the main difficulty of the modeling of daily\nprecipitation amounts is the selection of an appropriate probability\ndistribution, this study aims to establish a model selection framework to\nidentify the appropriate probability distribution for the modeling of daily\nprecipitation amounts from the commonly used probability distributions, i.e.\nthe exponential, Gamma, Weibull, and mixed exponential distributions. The mixed\nGamma Weibull (MGW) distribution serves this purpose because all the commonly\nused probability distributions are special cases of the MGW distribution, and\nthe MGW distribution integrates all the commonly used probability distributions\ninto one framework. Finally, via the large sample inference of likelihood\nratios, a model selection criterion can be established to identify the\nappropriate model for the modeling of daily precipitation amounts from the MGW\ndistribution framework.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 22:53:07 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chen", "Hsien-Wei", ""]]}, {"id": "2008.13087", "submitter": "Mingbin Feng", "authors": "Mingbin Ben Feng and Eunhye Song", "title": "Optimal Nested Simulation Experiment Design via Likelihood Ratio Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested simulation arises frequently in {risk management} or uncertainty\nquantification problems, where the performance measure is a function of the\nsimulation output mean conditional on the outer scenario. The standard nested\nsimulation samples $M$ outer scenarios and runs $N$ inner replications at each.\nWe propose a new experiment design framework for a problem whose inner\nreplication's inputs are generated from distributions parameterized by the\nouter scenario. This structure lets us pool replications from an outer scenario\nto estimate another scenario's conditional mean via the likelihood ratio\nmethod. We formulate a bi-level optimization problem to decide not only which\nof $M$ outer scenarios to simulate and how many times to replicate at each, but\nalso how to pool these replications such that the total simulation effort is\nminimized while achieving a target level of {precision}. The resulting optimal\ndesign requires far less simulation effort than $MN$. We provide asymptotic\nanalyses on the convergence rates of the performance measure estimators\ncomputed from the experiment design. Empirical results show that our experiment\ndesign reduces the simulation effort by orders of magnitude compared to the\nstandard nested simulation and outperforms a state-of-the-art regression-based\ndesign that pools replications via regression.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 04:19:39 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 04:09:25 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Feng", "Mingbin Ben", ""], ["Song", "Eunhye", ""]]}, {"id": "2008.13126", "submitter": "Torben Martinussen", "authors": "Torben Martinussen and Mats Julius Stensrud", "title": "Estimation of separable direct and indirect effects in continuous time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many research questions involve time-to-event outcomes that can be prevented\nfrom occurring due to competing events. In these settings, we must be careful\nabout the causal interpretation of classical statistical estimands. In\nparticular, estimands on the hazard scale, such as ratios of cause specific or\nsubdistribution hazards, are fundamentally hard to be interpret causally.\nEstimands on the risk scale, such as contrasts of cumulative incidence\nfunctions, do have a causal interpretation, but they only capture the total\neffect of the treatment on the event of interest; that is, effects both through\nand outside of the competing event. To disentangle causal treatment effects on\nthe event of interest and competing events, the separable direct and indirect\neffects were recently introduced. Here we provide new results on the estimation\nof direct and indirect separable effects in continuous time. In particular, we\nderive the nonparametric influence function in continuous time and use it to\nconstruct an estimator that has certain robustness properties. We also propose\na simple estimator based on semiparametric models for the two cause specific\nhazard functions. We describe the asymptotic properties of these estimators,\nand present results from simulation studies, suggesting that the estimators\nbehave satisfactorily in finite samples. Finally, we re-analyze the prostate\ncancer trial from Stensrud et al (2020).\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 09:25:08 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Martinussen", "Torben", ""], ["Stensrud", "Mats Julius", ""]]}, {"id": "2008.13127", "submitter": "Boya Lai", "authors": "Nozer D. Singpurwalla and Boya Lai", "title": "The Dinegentropy of Diagnostic Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic testing is germane to a variety of scenarios in medicine, pandemic\ntracking, threat detection, and signal processing. This is an expository paper\nwith some original results. Here we first set up a mathematical architecture\nfor diagnostics, and explore its probabilistic underpinnings. Doing so enables\nus to develop new metrics for assessing the efficacy of different kinds of\ndiagnostic tests, and for solving a long standing open problem in diagnostics,\nnamely, comparing tests when their receiver operating characteristic curves\ncross. The first is done by introducing the notion of what we call, a Gini\nCoefficient; the second by invoking the information theoretic notion of\ndinegentropy. Taken together, these may be seen a contribution to the state of\nthe art of diagnostics. The spirit of our work could also be relevant to the\nmuch discussed topic of batch testing, where each batch is defined by the\npartitioning strategy used to create it. However this possibility has not been\nexplored here in any detail. Rather, we invite the attention of other\nresearchers to investigate this idea, as future work.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 09:31:57 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Singpurwalla", "Nozer D.", ""], ["Lai", "Boya", ""]]}, {"id": "2008.13137", "submitter": "Shu Yang", "authors": "Ming-Yueh Huang and Shu Yang", "title": "Robust inference of conditional average treatment effects using\n  dimension reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to make robust inference of the conditional average treatment\neffect from observational data, but this becomes challenging when the\nconfounder is multivariate or high-dimensional. In this article, we propose a\ndouble dimension reduction method, which reduces the curse of dimensionality as\nmuch as possible while keeping the nonparametric merit. We identify the central\nmean subspace of the conditional average treatment effect using dimension\nreduction. A nonparametric regression with prior dimension reduction is also\nused to impute counterfactual outcomes. This step helps improve the stability\nof the imputation and leads to a better estimator than existing methods. We\nthen propose an effective bootstrapping procedure without bootstrapping the\nestimated central mean subspace to make valid inference.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 10:48:33 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Huang", "Ming-Yueh", ""], ["Yang", "Shu", ""]]}, {"id": "2008.13473", "submitter": "Andrea Meil\\'an-Vila", "authors": "Andrea Meil\\'an-Vila, Mario Francisco-Fern\\'andez, Rosa M. Crujeiras", "title": "Goodness-of-fit tests for parametric regression models with circular\n  response", "comments": "38 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing procedures for assessing a parametric regression model with circular\nresponse and $\\mathbb{R}^d$-valued covariate are proposed and analyzed in this\nwork both for independent and for spatially correlated data. The test\nstatistics are based on a circular distance comparing a (non-smoothed or\nsmoothed) parametric circular estimator and a nonparametric one. Properly\ndesigned bootstrap procedures for calibrating the tests in practice are also\npresented. Finite sample performance of the tests in different scenarios with\nindependent and spatially correlated samples, is analyzed by simulations.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 10:33:28 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Meil\u00e1n-Vila", "Andrea", ""], ["Francisco-Fern\u00e1ndez", "Mario", ""], ["Crujeiras", "Rosa M.", ""]]}, {"id": "2008.13558", "submitter": "Santtu Tikka", "authors": "Santtu Tikka, Jussi Hakanen, Mirka Saarela, Juha Karvanen", "title": "Simulation Framework for Realistic Large-scale Individual-level Data\n  Generation with an Application in the Health Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a framework for realistic data generation and simulation of\ncomplex systems and demonstrate its capabilities in the health domain. The main\nuse cases of the framework are predicting the development of risk factors and\ndisease occurrence, evaluating the impact of interventions and policy\ndecisions, and statistical method development. We present the fundamentals of\nthe framework using rigorous mathematical definitions. The framework supports\ncalibration to a real population as well as various manipulations and data\ncollection processes. The freely available open-source implementation in R\nembraces efficient data structures, parallel computing and fast random number\ngeneration which ensure reproducibility and scalability. With the framework it\nis possible to run daily-level simulations for populations of millions of\nindividuals for decades of simulated time. An example on the occurrence of\nstroke, type 2 diabetes and mortality illustrates the usage of the framework in\nthe Finnish context. In the example, we demonstrate the data-collection\nfunctionality by studying the impact of non-participation on the estimated risk\nmodels and interventions related to controlling the additional salt intake.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 12:43:31 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 07:58:03 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tikka", "Santtu", ""], ["Hakanen", "Jussi", ""], ["Saarela", "Mirka", ""], ["Karvanen", "Juha", ""]]}, {"id": "2008.13567", "submitter": "Moo K. Chung", "authors": "Moo K. Chung", "title": "Introduction to logistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For random field theory based multiple comparison corrections In brain\nimaging, it is often necessary to compute the distribution of the supremum of a\nrandom field. Unfortunately, computing the distribution of the supremum of the\nrandom field is not easy and requires satisfying many distributional\nassumptions that may not be true in real data. Thus, there is a need to come up\nwith a different framework that does not use the traditional statistical\nhypothesis testing paradigm that requires to compute p-values. With this as a\nmotivation, we can use a different approach called the logistic regression that\ndoes not require computing the p-value and still be able to localize the\nregions of brain network differences. Unlike other discriminant and\nclassification techniques that tried to classify preselected feature vectors,\nthe method here does not require any preselected feature vectors and performs\nthe classification at each edge level.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 05:18:55 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 17:16:47 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Chung", "Moo K.", ""]]}, {"id": "2008.13619", "submitter": "Jun-Ichi Takeshita", "authors": "Jun-ichi Takeshita and Tomomichi Suzuki", "title": "Precision for binary measurement methods and results under beta-binomial\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To handle typical problems from fields dealing with biological responses,\nthis study develops a new statistical model and method for analysing the\nprecision of binary measurement methods and results from collaborative studies.\nThe model is based on beta-binomial distributions. In other words, we assume\nthat the sensitivity of each laboratory obeys a beta distribution and the\nbinary measurement results under a given sensitivity follow a binomial\ndistribution. We propose the key precision indicators of repeatability and\nreproducibility for the model and derive their unbiased estimates. We further\npropose a confidence interval for repeatability by applying the Jeffreys\ninterval, which utilizes the assumption of beta distributions for sensitivity.\nMoreover, we propose a statistical test for determining laboratory effects,\nusing simultaneous confidence intervals based on the confidence interval of\neach laboratory's sensitivity. Finally, we apply the proposed method to\nreal-world examples in the fields of food safety and chemical risk assessment\nand management.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 14:05:14 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Takeshita", "Jun-ichi", ""], ["Suzuki", "Tomomichi", ""]]}, {"id": "2008.13620", "submitter": "Ruoqi Liu", "authors": "Ruoqi Liu, Changchang Yin, Ping Zhang", "title": "Estimating Individual Treatment Effects with Time-Varying Confounders", "comments": "Accepted to ICDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the individual treatment effect (ITE) from observational data is\nmeaningful and practical in healthcare. Existing work mainly relies on the\nstrong ignorability assumption that no hidden confounders exist, which may lead\nto bias in estimating causal effects. Some studies consider the hidden\nconfounders are designed for static environment and not easily adaptable to a\ndynamic setting. In fact, most observational data (e.g., electronic medical\nrecords) is naturally dynamic and consists of sequential information. In this\npaper, we propose Deep Sequential Weighting (DSW) for estimating ITE with\ntime-varying confounders. Specifically, DSW infers the hidden confounders by\nincorporating the current treatment assignments and historical information\nusing a deep recurrent weighting neural network. The learned representations of\nhidden confounders combined with current observed data are leveraged for\npotential outcome and treatment predictions. We compute the time-varying\ninverse probabilities of treatment for re-weighting the population. We conduct\ncomprehensive comparison experiments on fully-synthetic, semi-synthetic and\nreal-world datasets to evaluate the performance of our model and baselines.\nResults demonstrate that our model can generate unbiased and accurate treatment\neffect by conditioning both time-varying observed and hidden confounders,\npaving the way for personalized medicine.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 02:21:56 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 16:34:59 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Liu", "Ruoqi", ""], ["Yin", "Changchang", ""], ["Zhang", "Ping", ""]]}, {"id": "2008.13651", "submitter": "Yingjie Feng", "authors": "Yingjie Feng", "title": "Causal Inference in Possibly Nonlinear Factor Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a general causal inference method for treatment effects\nmodels with noisily measured confounders. The key feature is that a large set\nof noisy measurements are linked with the underlying latent confounders through\nan unknown, possibly nonlinear factor structure. The main building block is a\nlocal principal subspace approximation procedure that combines $K$-nearest\nneighbors matching and principal component analysis. Estimators of many causal\nparameters, including average treatment effects and counterfactual\ndistributions, are constructed based on doubly-robust score functions.\nLarge-sample properties of these estimators are established, which only require\nrelatively mild conditions on the principal subspace approximation. The results\nare illustrated with an empirical application studying the effect of political\nconnections on stock returns of financial firms, and a Monte Carlo experiment.\nThe main technical and methodological results regarding the general local\nprincipal subspace approximation method may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 14:39:36 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 07:20:38 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Feng", "Yingjie", ""]]}, {"id": "2008.13767", "submitter": "Justin Williams", "authors": "Justin R. Williams and Catherine M. Crespi", "title": "Causal inference for multiple continuous exposures via the multivariate\n  generalized propensity score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized propensity score (GPS) is an extension of the propensity\nscore for use with quantitative or continuous exposures (e.g., dose of\nmedication or years of education). Current GPS methods allow estimation of the\ndose-response relationship between a single continuous exposure and an outcome.\nHowever, in many real-world settings, there are multiple exposures occurring\nsimultaneously that could be causally related to the outcome. We propose a\nmultivariate GPS method (mvGPS) that allows estimation of a dose-response\nsurface that relates the joint distribution of multiple continuous exposure\nvariables to an outcome. The method involves generating weights under a\nmultivariate normality assumption on the exposure variables. Focusing on\nscenarios with two exposure variables, we show via simulation that the mvGPS\nmethod can achieve balance across sets of confounders that may differ for\ndifferent exposure variables and reduces bias of the treatment effect estimates\nunder a variety of data generating scenarios. We apply the mvGPS method to an\nanalysis of the joint effect of two types of intervention strategies to reduce\nchildhood obesity rates.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 17:40:10 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Williams", "Justin R.", ""], ["Crespi", "Catherine M.", ""]]}]