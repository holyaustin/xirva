[{"id": "1908.00099", "submitter": "Bryan Graham", "authors": "Bryan S. Graham, Andrin Pelican", "title": "Testing for Externalities in Network Formation Using Simulation", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a simplified version of the testing problem considered by Pelican\nand Graham (2019): testing for interdependencies in preferences over links\namong N (possibly heterogeneous) agents in a network. We describe an exact test\nwhich conditions on a sufficient statistic for the nuisance parameter\ncharacterizing any agent-level heterogeneity. Employing an algorithm due to\nBlitzstein and Diaconis (2011), we show how to simulate the null distribution\nof the test statistic in order to estimate critical values and/or p-values. We\nillustrate our methods using the Nyakatoke risk-sharing network. We find that\nthe transitivity of the Nyakatoke network far exceeds what can be explained by\ndegree heterogeneity across households alone.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 21:10:04 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Graham", "Bryan S.", ""], ["Pelican", "Andrin", ""]]}, {"id": "1908.00307", "submitter": "Ashis Chakraborty", "authors": "Ashis Kumar Chakraborty, Parna Chatterjee, Poulami Chakraborty and\n  Aleena Chanda", "title": "Optimum Testing Time of Software using Size-Biased Concepts", "comments": "Communicated", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimum software release time problem has been an interesting area of\nresearch for several decades now. We introduce here a new concept of\nsize-biased modelling to solve for the optimum software release time. Bayesian\napproach is used to solve the problem. We also discuss about the applicability\nof the model for a specific data set, though we believe that the model is\napplicable to all kind of software reliability data collected in a discrete\nframework. It has applications in other fields like oil exploration also.\nFinally, we compare favourably our model with another similar model published\nrecently. We also provide in this article some future possibilities of research\nwork.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 10:27:25 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Chakraborty", "Ashis Kumar", ""], ["Chatterjee", "Parna", ""], ["Chakraborty", "Poulami", ""], ["Chanda", "Aleena", ""]]}, {"id": "1908.00336", "submitter": "Han Lin Shang", "authors": "Ufuk Beyaztas and Han Lin Shang", "title": "Forecasting functional time series using weighted likelihood methodology", "comments": "20 pages, 4 figures, to appear in Journal of Statistical Computation\n  and Simulation", "journal-ref": "Journal of Statistical Computation and Simulation, 2019, 89(16),\n  3046-3060", "doi": "10.1080/00949655.2019.1650935", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional time series whose sample elements are recorded sequentially over\ntime are frequently encountered with increasing technology. Recent studies have\nshown that analyzing and forecasting of functional time series can be performed\neasily using functional principal component analysis and existing\nunivariate/multivariate time series models. However, the forecasting\nperformance of such functional time series models may be affected by the\npresence of outlying observations which are very common in many scientific\nfields. Outliers may distort the functional time series model structure, and\nthus, the underlying model may produce high forecast errors. We introduce a\nrobust forecasting technique based on weighted likelihood methodology to obtain\npoint and interval forecasts in functional time series in the presence of\noutliers. The finite sample performance of the proposed method is illustrated\nby Monte Carlo simulations and four real-data examples. Numerical results\nreveal that the proposed method exhibits superior performance compared with the\nexisting method(s).\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 11:29:29 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Beyaztas", "Ufuk", ""], ["Shang", "Han Lin", ""]]}, {"id": "1908.00462", "submitter": "Min Wang", "authors": "Chanseok Park and Haewon Kim and Min Wang", "title": "Investigation of finite-sample properties of robust location and scale\n  estimators", "comments": "A change of title and many minor improvements", "journal-ref": "Communications in Statistics - Simulation and Computation 2020", "doi": "10.1080/03610918.2019.1699114", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the experimental data set is contaminated, we usually employ robust\nalternatives to common location and scale estimators such as the sample median\nand Hodges-Lehmann estimators for location and the sample median absolute\ndeviation and Shamos estimators for scale. It is well known that these\nestimators have high positive asymptotic breakdown points and are\nFisher-consistent as the sample size tends to infinity. To the best of our\nknowledge, the finite-sample properties of these estimators, depending on the\nsample size, have not well been studied in the literature. In this paper, we\nfill this gap by providing their closed-form finite-sample breakdown points and\ncalculating the unbiasing factors and relative efficiencies of the robust\nestimators through the extensive Monte Carlo simulations up to the sample size\n100. The numerical study shows that the unbiasing factor improves the\nfinite-sample performance significantly. In addition, we provide the predicted\nvalues for the unbiasing factors obtained by using the least squares method\nwhich can be used for the case of sample size more than 100.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 15:37:49 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 14:29:52 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Park", "Chanseok", ""], ["Kim", "Haewon", ""], ["Wang", "Min", ""]]}, {"id": "1908.00477", "submitter": "Yongli Sang", "authors": "Yongli Sang, Xin Dang and Yichuan Zhao", "title": "Jackknife Empirical Likelihood Approach for K-sample Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The categorical Gini correlation is an alternative measure of dependence\nbetween a categorical and numerical variables, which characterizes the\nindependence of the variables. A nonparametric test for the equality of K\ndistributions has been developed based on the categorical Gini correlation. By\napplying the jackknife empirical likelihood approach, the standard limiting\nchi-square distribution with degree freedom of $K-1$ is established and is used\nto determine critical value and $p$-value of the test. Simulation studies show\nthat the proposed method is competitive to existing methods in terms of power\nof the tests in most cases. The proposed method is illustrated in an\napplication on a real data set.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 16:09:56 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Sang", "Yongli", ""], ["Dang", "Xin", ""], ["Zhao", "Yichuan", ""]]}, {"id": "1908.00663", "submitter": "Sida Peng", "authors": "Sida Peng", "title": "Heterogeneous Endogenous Effects in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method to identify leaders and followers in a\nnetwork. Prior works use spatial autoregression models (SARs) which implicitly\nassume that each individual in the network has the same peer effects on others.\nMechanically, they conclude the key player in the network to be the one with\nthe highest centrality. However, when some individuals are more influential\nthan others, centrality may fail to be a good measure. I develop a model that\nallows for individual-specific endogenous effects and propose a two-stage LASSO\nprocedure to identify influential individuals in a network. Under an assumption\nof sparsity: only a subset of individuals (which can increase with sample size\nn) is influential, I show that my 2SLSS estimator for individual-specific\nendogenous effects is consistent and achieves asymptotic normality. I also\ndevelop robust inference including uniformly valid confidence intervals. These\nresults also carry through to scenarios where the influential individuals are\nnot sparse. I extend the analysis to allow for multiple types of connections\n(multiple networks), and I show how to use the sparse group LASSO to detect\nwhich of the multiple connection types is more influential. Simulation evidence\nshows that my estimator has good finite sample performance. I further apply my\nmethod to the data in Banerjee et al. (2013) and my proposed procedure is able\nto identify leaders and effective networks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 00:05:05 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Peng", "Sida", ""]]}, {"id": "1908.00823", "submitter": "Hendrik van der Wurp", "authors": "Hendrik van der Wurp, Andreas Groll, Thomas Kneib, Giampiero Marra and\n  Rosalba Radice", "title": "Generalised Joint Regression for Count Data with a Focus on Modelling\n  Football Matches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a versatile joint regression framework for count responses. The\nmethod is implemented in the R add-on package GJRM and allows for modelling\nlinear and non-linear dependence through the use of several copulae. Moreover,\nthe parameters of the marginal distributions of the count responses and of the\ncopula can be specified as flexible functions of covariates. Motivated by a\nfootball application, we also discuss an extension which forces the regression\ncoefficients of the marginal (linear) predictors to be equal via a suitable\npenalisation. Model fitting is based on a trust region algorithm which\nestimates simultaneously all the parameters of the joint models. We investigate\nthe proposal's empirical performance in two simulation studies, the first one\ndesigned for arbitrary count data, the other one reflecting football-specific\nsettings. Finally, the method is applied to FIFA World Cup data, showing its\ncompetitiveness to the standard approach with regard to predictive performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 12:29:45 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 12:36:22 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["van der Wurp", "Hendrik", ""], ["Groll", "Andreas", ""], ["Kneib", "Thomas", ""], ["Marra", "Giampiero", ""], ["Radice", "Rosalba", ""]]}, {"id": "1908.00882", "submitter": "David Blei", "authors": "Rajesh Ranganath and David M. Blei", "title": "Population Predictive Checks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian modeling has become a staple for researchers analyzing data. Thanks\nto recent developments in approximate posterior inference, modern researchers\ncan easily build, use, and revise complicated Bayesian models for large and\nrich data. These new abilities, however, bring into focus the problem of model\nassessment. Researchers need tools to diagnose the fitness of their models, to\nunderstand where a model falls short, and to guide its revision. In this paper\nwe develop a new method for Bayesian model checking, the population predictive\ncheck (Pop-PC). Pop-PCs are built on posterior predictive checks (PPC), a\nseminal method that checks a model by assessing the posterior predictive\ndistribution on the observed data. Though powerful, PPCs use the data\ntwice---both to calculate the posterior predictive and to evaluate it---which\ncan lead to overconfident assessments. Pop-PCs, in contrast, compare the\nposterior predictive distribution to the population distribution of the data.\nThis strategy blends Bayesian modeling with frequentist assessment, leading to\na robust check that validates the model on its generalization. Of course the\npopulation distribution is not usually available; thus we use tools like the\nbootstrap and cross validation to estimate the Pop-PC. Further, we extend\nPop-PCs to hierarchical models. We study Pop-PCs on classical regression and a\nhierarchical model of text. We show that Pop-PCs are robust to overfitting and\ncan be easily deployed on a broad family of models.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 14:37:29 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 01:37:54 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}, {"id": "1908.01044", "submitter": "Alex Ocampo", "authors": "Alex Ocampo, Heinz Schmidli, Peter Quarg, Francesca Callegari,\n  Marcello Pagano", "title": "Identifying Treatment Effects using Trimmed Means when Data are Missing\n  Not at Random", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patients often discontinue treatment in a clinical trial because their health\ncondition is not improving. Consequently, the patients still in the study at\nthe end of the trial have better health outcomes on average than the initial\npatient population would have had if every patient had completed the trial. If\nwe only analyze the patients who complete the trial, then this missing data\nproblem biases the estimator of a medication's efficacy because study outcomes\nare missing not at random (MNAR). One way to overcome this problem - the\ntrimmed means approach for missing data - sets missing values as slightly worse\nthan the worst observed outcome and then trims away a fraction of the\ndistribution from each treatment arm before calculating differences in\ntreatment efficacy (Permutt 2017, Pharmaceutical statistics 16.1:20-28). In\nthis paper we derive sufficient and necessary conditions for when this approach\ncan identify the average population treatment effect in the presence of MNAR\ndata. Numerical studies show the trimmed means approach's ability to\neffectively estimate treatment efficacy when data are MNAR and missingness is\nstrongly associated with an unfavorable outcome, but trimmed means fail when\ndata are missing at random (MAR) when the better approach would be to multiply\nimpute the missing values. If the reasons for discontinuation in a clinical\ntrial are known analysts can improve estimates with a combination of multiple\nimputation (MI) and the trimmed means approach when the assumptions of each\nmissing data mechanism hold. When the assumptions are justifiable, using\ntrimmed means can help identify treatment effects notwithstanding MNAR data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 20:34:46 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 18:26:24 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Ocampo", "Alex", ""], ["Schmidli", "Heinz", ""], ["Quarg", "Peter", ""], ["Callegari", "Francesca", ""], ["Pagano", "Marcello", ""]]}, {"id": "1908.01113", "submitter": "Yuntian Chen", "authors": "Yuntian Chen, Haibin Chang, Meng Jin, Dongxiao Zhang", "title": "Ensemble Neural Networks (ENN): A gradient-free stochastic method", "comments": null, "journal-ref": "Neural Networks, 110, 170-185 (2019)", "doi": "10.1016/j.neunet.2018.11.009", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this study, an efficient stochastic gradient-free method, the ensemble\nneural networks (ENN), is developed. In the ENN, the optimization process\nrelies on covariance matrices rather than derivatives. The covariance matrices\nare calculated by the ensemble randomized maximum likelihood algorithm (EnRML),\nwhich is an inverse modeling method. The ENN is able to simultaneously provide\nestimations and perform uncertainty quantification since it is built under the\nBayesian framework. The ENN is also robust to small training data size because\nthe ensemble of stochastic realizations essentially enlarges the training\ndataset. This constitutes a desirable characteristic, especially for real-world\nengineering applications. In addition, the ENN does not require the calculation\nof gradients, which enables the use of complicated neuron models and loss\nfunctions in neural networks. We experimentally demonstrate benefits of the\nproposed model, in particular showing that the ENN performs much better than\nthe traditional Bayesian neural networks (BNN). The EnRML in ENN is a\nsubstitution of gradient-based optimization algorithms, which means that it can\nbe directly combined with the feed-forward process in other existing (deep)\nneural networks, such as convolutional neural networks (CNN) and recurrent\nneural networks (RNN), broadening future applications of the ENN.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 03:11:32 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Chen", "Yuntian", ""], ["Chang", "Haibin", ""], ["Jin", "Meng", ""], ["Zhang", "Dongxiao", ""]]}, {"id": "1908.01251", "submitter": "Miles Lopes", "authors": "Miles E. Lopes, Suofei Wu, Thomas C. M. Lee", "title": "Measuring the Algorithmic Convergence of Randomized Ensembles: The\n  Regression Setting", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When randomized ensemble methods such as bagging and random forests are\nimplemented, a basic question arises: Is the ensemble large enough? In\nparticular, the practitioner desires a rigorous guarantee that a given ensemble\nwill perform nearly as well as an ideal infinite ensemble (trained on the same\ndata). The purpose of the current paper is to develop a bootstrap method for\nsolving this problem in the context of regression --- which complements our\ncompanion paper in the context of classification (Lopes 2019). In contrast to\nthe classification setting, the current paper shows that theoretical guarantees\nfor the proposed bootstrap can be established under much weaker assumptions. In\naddition, we illustrate the flexibility of the method by showing how it can be\nadapted to measure algorithmic convergence for variable selection. Lastly, we\nprovide numerical results demonstrating that the method works well in a range\nof situations.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 00:45:59 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Lopes", "Miles E.", ""], ["Wu", "Suofei", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "1908.01252", "submitter": "Yuan Liao", "authors": "Jianqing Fan, Yuan Liao", "title": "Learning Latent Factors from Diversified Projections and its\n  Applications to Over-Estimated and Weak Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimations and applications of factor models often rely on the crucial\ncondition that the number of latent factors is consistently estimated, which in\nturn also requires that factors be relatively strong, data are stationary and\nweak serial dependence, and the sample size be fairly large, although in\npractical applications, one or several of these conditions may fail. In these\ncases it is difficult to analyze the eigenvectors of the data matrix. To\naddress this issue, we propose simple estimators of the latent factors using\ncross-sectional projections of the panel data, by weighted averages with\npre-determined weights. These weights are chosen to diversify away the\nidiosyncratic components, resulting in \"diversified factors\". Because the\nprojections are conducted cross-sectionally, they are robust to serial\nconditions, easy to analyze and work even for finite length of time series. We\nformally prove that this procedure is robust to over-estimating the number of\nfactors, and illustrate it in several applications, including post-selection\ninference, big data forecasts, large covariance estimation and factor\nspecification tests. We also recommend several choices for the diversified\nweights.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 01:09:01 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 21:41:46 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 00:51:52 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Fan", "Jianqing", ""], ["Liao", "Yuan", ""]]}, {"id": "1908.01260", "submitter": "Pengfei Li", "authors": "Yukun Liu, Pengfei Li, and Jing Qin", "title": "Full-semiparametric-likelihood-based inference for non-ignorable missing\n  data", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past few decades, missing-data problems have been studied\nextensively, with a focus on the ignorable missing case, where the missing\nprobability depends only on observable quantities. By contrast, research into\nnon-ignorable missing data problems is quite limited. The main difficulty in\nsolving such problems is that the missing probability and the regression\nlikelihood function are tangled together in the likelihood presentation, and\nthe model parameters may not be identifiable even under strong parametric model\nassumptions. In this paper we discuss a semiparametric model for non-ignorable\nmissing data and propose a maximum full semiparametric likelihood estimation\nmethod, which is an efficient combination of the parametric conditional\nlikelihood and the marginal nonparametric biased sampling likelihood. The extra\nmarginal likelihood contribution can not only produce efficiency gain but also\nidentify the underlying model parameters without additional assumptions. We\nfurther show that the proposed estimators for the underlying parameters and the\nresponse mean are semiparametrically efficient. Extensive simulations and a\nreal data analysis demonstrate the advantage of the proposed method over\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 02:30:11 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Liu", "Yukun", ""], ["Li", "Pengfei", ""], ["Qin", "Jing", ""]]}, {"id": "1908.01272", "submitter": "Kyungchul Song", "authors": "Elena Krasnokutskaya, Kyungchul Song, Xun Tang", "title": "Estimating Unobserved Individual Heterogeneity Using Pairwise\n  Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for studying environments with unobserved individual\nheterogeneity. Based on model-implied pairwise inequalities, the method\nclassifies individuals in the sample into groups defined by discrete unobserved\nheterogeneity with unknown support. We establish conditions under which the\ngroups are identified and consistently estimated through our method. We show\nthat the method performs well in finite samples through Monte Carlo simulation.\nWe then apply the method to estimate a model of lowest-price procurement\nauctions with unobserved bidder heterogeneity, using data from the California\nhighway procurement market.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 04:58:24 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 21:19:56 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 16:04:53 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Krasnokutskaya", "Elena", ""], ["Song", "Kyungchul", ""], ["Tang", "Xun", ""]]}, {"id": "1908.01333", "submitter": "Gauri Kamat", "authors": "Gauri Kamat and Jerome P. Reiter", "title": "Leveraging Random Assignment to Impute Missing Covariates in Causal\n  Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Baseline covariates in randomized experiments are often used in the\nestimation of treatment effects, for example, when estimating treatment effects\nwithin covariate-defined subgroups. In practice, however, covariate values may\nbe missing for some data subjects. To handle missing values, analysts can use\nimputation methods to create completed datasets, from which they can estimate\ntreatment effects. Common imputation methods include mean imputation, single\nimputation via regression, and multiple imputation. For each of these methods,\nwe investigate the benefits of leveraging randomized treatment assignment in\nthe imputation routines, that is, making use of the fact that the true\ncovariate distributions are the same across treatment arms. We do so using\nsimulation studies that compare the quality of inferences when we respect or\ndisregard the randomization. We consider this question for imputation routines\nimplemented using covariates only, and imputation routines implemented using\nthe outcome variable. In either case, accounting for randomization offers only\nsmall gains in accuracy for our simulation scenarios. Our results also shed\nlight on the performances of these different procedures for imputing missing\ncovariates in randomized experiments when one seeks to estimate heterogeneous\ntreatment effects.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 12:49:08 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 21:33:42 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Kamat", "Gauri", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1908.01411", "submitter": "Sergey Tarima", "authors": "Sergey Tarima and Nancy Flournoy", "title": "Effect of Interim Adaptations in Group Sequential Designs", "comments": "30 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript investigates unconditional and conditional-on-stopping\nmaximum likelihood estimators (MLEs), information measures and information loss\nassociated with conditioning in group sequential designs (GSDs). The\npossibility of early stopping brings truncation to the distributional form of\nMLEs; sequentially, GSD decisions eliminate some events from the sample space.\nMultiple testing induces mixtures on the adapted sample space. Distributions of\nMLEs are mixtures of truncated distributions. Test statistics that are\nasymptotically normal without GSD, have asymptotic distributions, under GSD,\nthat are non-normal mixtures of truncated normal distributions under local\nalternatives; under fixed alternatives, asymptotic distributions of test\nstatistics are degenerate. Estimation of various statistical quantities such as\ninformation, information fractions, and confidence intervals should account for\nthe effect of planned adaptations. Calculation of adapted information fractions\nrequires substantial computational effort. Therefore, a new GSD is proposed in\nwhich stage-specific sample sizes are fully determined by desired operational\ncharacteristics, and calculation of information fractions is not needed.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 22:02:54 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Tarima", "Sergey", ""], ["Flournoy", "Nancy", ""]]}, {"id": "1908.01446", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Steven Haberman", "title": "Forecasting age distribution of death counts: An application to annuity\n  pricing", "comments": "30 pages, 4 figures, To appear in Annals of Actuarial Science", "journal-ref": "Annals of Actuarial Science, 2020, 14(1), 150-169", "doi": "10.1017/S1748499519000101", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a compositional data analysis approach to forecasting the age\ndistribution of death counts. Using the age-specific period life-table death\ncounts in Australia obtained from the Human Mortality Database, the\ncompositional data analysis approach produces more accurate one- to\n20-step-ahead point and interval forecasts than Lee-Carter method,\nHyndman-Ullah method, and two na\\\"{i}ve random walk methods. The improved\nforecast accuracy of period life-table death counts is of great interest to\ndemographers for estimating survival probabilities and life expectancy, and to\nactuaries for determining temporary annuity prices for various ages and\nmaturities. Although we focus on temporary annuity prices, we consider\nlong-term contracts which make the annuity almost lifetime, in particular when\nthe age at entry is sufficiently high.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 02:32:24 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Shang", "Han Lin", ""], ["Haberman", "Steven", ""]]}, {"id": "1908.01500", "submitter": "Carter Butts", "authors": "Francis Lee and Carter T. Butts", "title": "Incorporating Structural Stigma into Network Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rich literature has explored the modeling of homophily and other forms of\nnonuniform mixing associated with individual-level covariates within the\nexponential family random graph (ERGM) framework. Such differential mixing does\nnot fully explain phenomena such as stigma, however, which involve the active\nmaintenance of social boundaries by ostracism of persons with out-group ties.\nHere, we introduce a new statistic that allows for such effects to be captured,\nmaking it possible to probe for the potential presence of boundary maintenance\nabove and beyond simple differences in nomination rates. We demonstrate this\nstatistic in the context of gender segregation in a school classroom.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 07:53:56 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Lee", "Francis", ""], ["Butts", "Carter T.", ""]]}, {"id": "1908.01720", "submitter": "Felipe Campelo", "authors": "Felipe Campelo, Elizabeth F. Wanner", "title": "Sample size calculations for the experimental comparison of multiple\n  algorithms on multiple problem instances", "comments": "31 pages. 7 Figures. Submitted to the Journal of Heuristics on 5\n  August 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a statistically principled method for estimating the\nrequired number of instances in the experimental comparison of multiple\nalgorithms on a given problem class of interest. This approach generalises\nearlier results by allowing researchers to design experiments based on the\ndesired best, worst, mean or median-case statistical power to detect\ndifferences between algorithms larger than a certain threshold. Holm's\nstep-down procedure is used to maintain the overall significance level\ncontrolled at desired levels, without resulting in overly conservative\nexperiments. This paper also presents an approach for sampling each algorithm\non each instance, based on optimal sample size ratios that minimise the total\nrequired number of runs subject to a desired accuracy in the estimation of\npaired differences. A case study investigating the effect of 21 variants of a\ncustom-tailored Simulated Annealing for a class of scheduling problems is used\nto illustrate the application of the proposed methods for sample size\ncalculations in the experimental comparison of algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 16:53:59 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Campelo", "Felipe", ""], ["Wanner", "Elizabeth F.", ""]]}, {"id": "1908.01823", "submitter": "Lizhen Lin", "authors": "Zifeng Zhao, Li Chen and Lizhen Lin", "title": "Change-point detection in dynamic networks via graphon estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general approach for change-point detection in dynamic networks.\nThe proposed method is model-free and covers a wide range of dynamic networks.\nThe key idea behind our approach is to effectively utilize the network\nstructure in designing change-point detection algorithms. This is done via an\ninitial step of graphon estimation, where we propose a modified neighborhood\nsmoothing~(MNBS) algorithm for estimating the link probability matrices of a\ndynamic network. Based on the initial graphon estimation, we then develop a\nscreening and thresholding algorithm for multiple change-point detection in\ndynamic networks. The convergence rate and consistency for the change-point\ndetection procedure are derived as well as those for MNBS. When the number of\nnodes is large~(e.g., exceeds the number of temporal points), our approach\nyields a faster convergence rate in detecting change-points comparing with an\nalgorithm that simply employs averaged information of the dynamic network\nacross time. Numerical experiments demonstrate robust performance of the\nproposed algorithm for change-point detection under various types of dynamic\nnetworks, and superior performance over existing methods is observed. A real\ndata example is provided to illustrate the effectiveness and practical impact\nof the procedure.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 19:50:20 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Zhao", "Zifeng", ""], ["Chen", "Li", ""], ["Lin", "Lizhen", ""]]}, {"id": "1908.02029", "submitter": "Martin Tveten", "authors": "Martin Tveten and Ingrid K. Glad", "title": "Online Detection of Sparse Changes in High-Dimensional Data Streams\n  Using Tailored Projections", "comments": "26 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When applying principal component analysis (PCA) for dimension reduction, the\nmost varying projections are usually used in order to retain most of the\ninformation. For the purpose of anomaly and change detection, however, the\nleast varying projections are often the most important ones. In this article,\nwe present a novel method that automatically tailors the choice of projections\nto monitor for sparse changes in the mean and/or covariance matrix of\nhigh-dimensional data. A subset of the least varying projections is almost\nalways selected based on a criteria of the projection's sensitivity to changes.\n  Our focus is on online/sequential change detection, where the aim is to\ndetect changes as quickly as possible, while controlling false alarms at a\nspecified level. A combination of tailored PCA and a generalized log-likelihood\nmonitoring procedure displays high efficiency in detecting even very sparse\nchanges in the mean, variance and correlation. We demonstrate on real data that\ntailored PCA monitoring is efficient for sparse change detection also when the\ndata streams are highly auto-correlated and non-normal. Notably, error control\nis achieved without a large validation set, which is needed in most existing\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 09:06:45 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Tveten", "Martin", ""], ["Glad", "Ingrid K.", ""]]}, {"id": "1908.02166", "submitter": "Nir Billfeld", "authors": "Nir Billfeld, Moshe Kim", "title": "Semiparametric Wavelet-based JPEG IV Estimator for endogenously\n  truncated data", "comments": "18 pages", "journal-ref": "IEEE Access, 7, 99602-99621 (2019)", "doi": "10.1109/ACCESS.2019.2929571", "report-no": null, "categories": "stat.ME cs.CV cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new and an enriched JPEG algorithm is provided for identifying redundancies\nin a sequence of irregular noisy data points which also accommodates a\nreference-free criterion function. Our main contribution is by formulating\nanalytically (instead of approximating) the inverse of the transpose of\nJPEGwavelet transform without involving matrices which are computationally\ncumbersome. The algorithm is suitable for the widely-spread situations where\nthe original data distribution is unobservable such as in cases where there is\ndeficient representation of the entire population in the training data (in\nmachine learning) and thus the covariate shift assumption is violated. The\nproposed estimator corrects for both biases, the one generated by endogenous\ntruncation and the one generated by endogenous covariates. Results from\nutilizing 2,000,000 different distribution functions verify the applicability\nand high accuracy of our procedure to cases in which the disturbances are\nneither jointly nor marginally normally distributed.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 13:54:52 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Billfeld", "Nir", ""], ["Kim", "Moshe", ""]]}, {"id": "1908.02218", "submitter": "Christian Hennig", "authors": "M. Iqbal Shamsudheen and Christian Hennig", "title": "Should we test the model assumptions before running a model-based test?", "comments": "31 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Statistical methods are based on model assumptions, and it is statistical\nfolklore that a method's model assumptions should be checked before applying\nit. This can be formally done by running one or more misspecification tests\ntesting model assumptions before running a method that makes these assumptions;\nhere we focus on model-based tests. A combined test procedure can be defined by\nspecifying a protocol in which first model assumptions are tested and then,\nconditionally on the outcome, a test is run that requires or does not require\nthe tested assumptions. Although such an approach is often taken in practice,\nmuch of the literature that investigated this is surprisingly critical of it.\nOur aim is to explore conditions under which model checking is advisable or not\nadvisable. For this, we review results regarding such \"combined procedures\" in\nthe literature, we review and discuss controversial views on the role of model\nchecking in statistics, and we present a general setup in which we can show\nthat preliminary model checking is advantageous, which implies conditions for\nmaking model checking worthwhile.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 15:43:01 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 23:41:53 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 01:24:07 GMT"}, {"version": "v4", "created": "Wed, 17 Mar 2021 00:14:56 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Shamsudheen", "M. Iqbal", ""], ["Hennig", "Christian", ""]]}, {"id": "1908.02344", "submitter": "Hossein Baghishani", "authors": "Mahsa Nadifar, Hossein Baghishani, Afshin Fallah, and Havard Rue", "title": "Statistical modeling of groundwater quality assessment in Iran using a\n  flexible Poisson likelihood", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing water quality and recognizing its associated risks to human health\nand the broader environment is undoubtedly essential. Groundwater is widely\nused to supply water for drinking, industry, and agriculture purposes. The\ngroundwater quality measurements vary for different climates and various human\nbehaviors, and consequently, their spatial variability can be substantial. In\nthis paper, we aim to analyze a groundwater dataset from the Golestan province,\nIran, for November 2003 to November 2013. Our target response variable to\nmonitor the quality of groundwater is the number of counts that the quality of\nwater is good for a drink. Hence, we are facing spatial count data. Due to the\nubiquity of over or underdispersion in count data, we propose a Bayesian\nhierarchical modeling approach based on the renewal theory that relates\nnonexponential waiting times between events and the distribution of the counts,\nrelaxing the assumption of equidispersion at the cost of an additional\nparameter. Particularly, we extend the methodology for the analysis of spatial\ncount data based on the gamma distribution assumption for waiting times. The\nmodel can be formulated as a latent Gaussian model, and therefore, we can carry\nout the fast computation by using the integrated nested Laplace approximation\nmethod. The analysis of the groundwater dataset and a simulation study show a\nsignificant improvement over both Poisson and negative binomial models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 19:49:13 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Nadifar", "Mahsa", ""], ["Baghishani", "Hossein", ""], ["Fallah", "Afshin", ""], ["Rue", "Havard", ""]]}, {"id": "1908.02545", "submitter": "Ta-Hsin Li", "authors": "Ta-Hsin Li", "title": "Quantile-Frequency Analysis and Spectral Divergence Metrics for\n  Diagnostic Checks of Time Series With Nonlinear Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear dynamic volatility has been observed in many financial time series.\nThe recently proposed quantile periodogram offers an alternative way to examine\nthis phenomena in the frequency domain. The quantile periodogram is constructed\nfrom trigonometric quantile regression of time series data at different\nfrequencies and quantile levels. It is a useful tool for quantile-frequency\nanalysis (QFA) of nonlinear serial dependence. This paper introduces a number\nof spectral divergence metrics based on the quantile periodogram for diagnostic\nchecks of financial time series models and model-based discriminant analysis.\nThe parametric bootstrapping technique is employed to compute the $p$-values of\nthe metrics. The usefulness of the proposed method is demonstrated empirically\nby a case study using the daily log returns of the S\\&P 500 index over three\nperiods of time together with their GARCH-type models. The results show that\nthe QFA method is able to provide additional insights into the goodness of fit\nof these financial time series models that may have been missed by conventional\ntests. The results also show that the QFA method offers a more informative way\nof discriminant analysis for detecting regime changes in time series.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 14:50:05 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Li", "Ta-Hsin", ""]]}, {"id": "1908.02552", "submitter": "Yicong Lin", "authors": "Yicong Lin and Hanno Reuvers", "title": "Efficient Estimation by Fully Modified GLS with an Application to the\n  Environmental Kuznets Curve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops the asymptotic theory of a Fully Modified Generalized\nLeast Squares estimator for multivariate cointegrating polynomial regressions.\nSuch regressions allow for deterministic trends, stochastic trends and integer\npowers of stochastic trends to enter the cointegrating relations. Our fully\nmodified estimator incorporates: (1) the direct estimation of the inverse\nautocovariance matrix of the multidimensional errors, and (2) second order bias\ncorrections. The resulting estimator has the intuitive interpretation of\napplying a weighted least squares objective function to filtered data series.\nMoreover, the required second order bias corrections are convenient byproducts\nof our approach and lead to standard asymptotic inference. We also study\nseveral multivariate KPSS-type of tests for the null of cointegration. A\ncomprehensive simulation study shows good performance of the FM-GLS estimator\nand the related tests. As a practical illustration, we reinvestigate the\nEnvironmental Kuznets Curve (EKC) hypothesis for six early industrialized\ncountries as in Wagner et al. (2020).\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 12:17:30 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 17:01:26 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Lin", "Yicong", ""], ["Reuvers", "Hanno", ""]]}, {"id": "1908.02573", "submitter": "Akifumi Okuno", "authors": "Akifumi Okuno, Hidetoshi Shimodaira", "title": "Hyperlink Regression via Bregman Divergence", "comments": "41 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A collection of $U \\: (\\in \\mathbb{N})$ data vectors is called a $U$-tuple,\nand the association strength among the vectors of a tuple is termed as the\n\\emph{hyperlink weight}, that is assumed to be symmetric with respect to\npermutation of the entries in the index. We herein propose Bregman hyperlink\nregression (BHLR), which learns a user-specified symmetric similarity function\nsuch that it predicts the tuple's hyperlink weight from data vectors stored in\nthe $U$-tuple. BHLR is a simple and general framework for hyper-relational\nlearning, that minimizes Bregman-divergence (BD) between the hyperlink weights\nand estimated similarities defined for the corresponding tuples; BHLR\nencompasses various existing methods, such as logistic regression ($U=1$),\nPoisson regression ($U=1$), link prediction ($U=2$), and those for\nrepresentation learning, such as graph embedding ($U=2$), matrix factorization\n($U=2$), tensor factorization ($U \\geq 2$), and their variants equipped with\narbitrary BD. Nonlinear functions (e.g., neural networks), can be employed for\nthe similarity functions. However, there are theoretical challenges such that\nsome of different tuples of BHLR may share data vectors therein, unlike the\ni.i.d. setting of classical regression. We address these theoretical issues,\nand proved that BHLR equipped with arbitrary BD and $U \\in \\mathbb{N}$ is (P-1)\nstatistically consistent, that is, it asymptotically recovers the underlying\ntrue conditional expectation of hyperlink weights given data vectors, and (P-2)\ncomputationally tractable, that is, it is efficiently computed by stochastic\noptimization algorithms using a novel generalized minibatch sampling procedure\nfor hyper-relational data. Consequently, theoretical guarantees for BHLR\nincluding several existing methods, that have been examined experimentally, are\nprovided in a unified manner.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 01:38:21 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 07:34:57 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Okuno", "Akifumi", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "1908.02634", "submitter": "Edward Cohen", "authors": "Edward A.K. Cohen and Alexander J. Gibberd", "title": "Wavelet Spectra for Multivariate Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wavelets provide the flexibility to analyse stochastic processes at different\nscales. Here, we apply them to multivariate point processes as a means of\ndetecting and analysing unknown non-stationarity, both within and across data\nstreams. To provide statistical tractability, a temporally smoothed wavelet\nperiodogram is developed and shown to be equivalent to a multi-wavelet\nperiodogram. Under a stationary assumption, the distribution of the temporally\nsmoothed wavelet periodogram is demonstrated to be asymptotically Wishart, with\nthe centrality matrix and degrees of freedom readily computable from the\nmulti-wavelet formulation. Distributional results extend to wavelet coherence;\na time-scale measure of inter-process correlation. This statistical framework\nis used to construct a test for stationarity in multivariate point-processes.\nThe methodology is applied to neural spike train data, where it is shown to\ndetect and characterise time-varying dependency patterns.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 13:45:53 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 09:57:26 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 09:55:38 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Cohen", "Edward A. K.", ""], ["Gibberd", "Alexander J.", ""]]}, {"id": "1908.02684", "submitter": "Sayantan Banerjee", "authors": "Sayantan Banerjee", "title": "Bayesian Structure Learning in Graphical Models using Shrinkage priors", "comments": "This is an extended abstract version of the ongoing work", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the structure of a high dimensional\nprecision matrix under sparsity assumptions. We propose to use a shrinkage\nprior, called the DL-graphical prior based on the Dirichlet-Laplace prior used\nfor the Gaussian mean problem. A posterior sampling scheme based on Gibbs\nsampling is also provided along with theoretical guarantees of the method by\nobtaining the posterior convergence rate of the precision matrix.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 06:21:52 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Banerjee", "Sayantan", ""]]}, {"id": "1908.02739", "submitter": "Alassane Aw", "authors": "Alassane Aw and Emmanuel Nicolas Cabral", "title": "Bayesian estimation of the functional spatial lag model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The spatial lag model (SLM) has been widely studied in the literature for\nspatialised data modeling in various disciplines such as geography, economics,\ndemography, regional sciences, etc. This is an extension of the classical\nlinear model that takes into account the proximity of spatial units in\nmodeling. The extension of the SLM model in the functional framework (the FSLM\nmodel) as well as its estimation by the truncated maximum likelihood technique\nhave been proposed by \\cite{Ahmed}. In this paper, we propose a Bayesian\nestimation of the FSLM model. The Bayesian MCMC technique is used as estimation\nmethods of the parameters of the model. A simulation study is conducted in\norder to compare the results of the Bayesian estimation method with the\ntruncated maximum likelihood method. As an illustration, the proposed Bayesian\nmethod is used to establish a relationship between the unemployment rate and\nthe curves of illiteracy rate observed in the 45 departments of Senegal.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 17:31:04 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Aw", "Alassane", ""], ["Cabral", "Emmanuel Nicolas", ""]]}, {"id": "1908.02891", "submitter": "Feng Li", "authors": "Xiaoqian Wang, Yanfei Kang, Fotios Petropoulos, Feng Li", "title": "The uncertainty estimation of feature-based forecast combinations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting is an indispensable element of operational research (OR) and an\nimportant aid to planning. The accurate estimation of the forecast uncertainty\nfacilitates several operations management activities, predominantly in\nsupporting decisions in inventory and supply chain management and effectively\nsetting safety stocks. In this paper, we introduce a feature-based framework,\nwhich links the relationship between time series features and the interval\nforecasting performance into providing reliable interval forecasts. We propose\nan optimal threshold ratio searching algorithm and a new weight determination\nmechanism for selecting an appropriate subset of models and assigning\ncombination weights for each time series tailored to the observed features. We\nevaluate our approach using a large set of time series from the M4 competition.\nOur experiments show that our approach significantly outperforms a wide range\nof benchmark models, both in terms of point forecasts as well as prediction\nintervals.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 00:52:55 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 13:11:39 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 02:17:30 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wang", "Xiaoqian", ""], ["Kang", "Yanfei", ""], ["Petropoulos", "Fotios", ""], ["Li", "Feng", ""]]}, {"id": "1908.02922", "submitter": "Aiyou Chen", "authors": "Aiyou Chen and Timothy C. Au", "title": "Robust Causal Inference for Incremental Return on Ad Spend with\n  Randomized Paired Geo Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Evaluating the incremental return on ad spend (iROAS) of a prospective online\nmarketing strategy (i.e., the ratio of the strategy's causal effect on some\nresponse metric of interest relative to its causal effect on the ad spend) has\nbecome increasingly more important. Although randomized ``geo experiments'' are\nfrequently employed for this evaluation, obtaining reliable estimates of iROAS\ncan be challenging as oftentimes only a small number of highly heterogeneous\nunits are used. Moreover, advertisers frequently impose budget constraints on\ntheir ad spends, which further complicates causal inference by introducing\ninterference between the experimental units. In this paper, we formulate a\nnovel statistical framework for inferring the iROAS of online advertising from\nrandomized paired geo experiment which further motivates and provides new\ninsights into Rosenbaum's arguments on instrumental variables, and we propose\nand develop a robust, distribution-free and interpretable estimator ``Trimmed\nMatch'', as well as a data-driven choice of the tuning parameter which may be\nof independent interest. We investigate the sensitivity of Trimmed Match to\nsome violations of its assumptions and show that it can be more efficient than\nsome alternative estimators based on simulated data. We then demonstrate its\npractical utility with real case studies.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 04:03:40 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 21:35:57 GMT"}, {"version": "v3", "created": "Sun, 6 Jun 2021 23:30:25 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chen", "Aiyou", ""], ["Au", "Timothy C.", ""]]}, {"id": "1908.03097", "submitter": "Minh-Ngoc Tran", "authors": "Minh-Ngoc Tran and Dang H. Nguyen and Duy Nguyen", "title": "Variational Bayes on Manifolds", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) has become a widely-used tool for Bayesian inference\nin statistics and machine learning. Nonetheless, the development of the\nexisting VB algorithms is so far generally restricted to the case where the\nvariational parameter space is Euclidean, which hinders the potential broad\napplication of VB methods. This paper extends the scope of VB to the case where\nthe variational parameter space is a Riemannian manifold. We develop an\nefficient manifold-based VB algorithm that exploits both the geometric\nstructure of the constraint parameter space and the information geometry of the\nmanifold of VB approximating probability distributions. Our algorithm is\nprovably convergent and achieves a convergence rate of order $\\mathcal\nO(1/\\sqrt{T})$ and $\\mathcal O(1/T^{2-2\\epsilon})$ for a non-convex evidence\nlower bound function and a strongly retraction-convex evidence lower bound\nfunction, respectively. We develop in particular two manifold VB algorithms,\nManifold Gaussian VB and Manifold Neural Net VB, and demonstrate through\nnumerical experiments that the proposed algorithms are stable, less sensitive\nto initialization and compares favourably to existing VB methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 14:38:31 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 00:48:23 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Nguyen", "Dang H.", ""], ["Nguyen", "Duy", ""]]}, {"id": "1908.03107", "submitter": "Anna Kiriliouk", "authors": "Anna Kiriliouk and Philippe Naveau", "title": "Climate extreme event attribution using multivariate\n  peaks-over-thresholds modeling and counterfactual theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical climate models are complex and combine a large number of physical\nprocesses. They are key tools in quantifying the relative contribution of\npotential anthropogenic causes (e.g., the current increase in greenhouse gases)\non high impact atmospheric variables like heavy rainfall. These so-called\nclimate extreme event attribution problems are particularly challenging in a\nmultivariate context, that is, when the atmospheric variables are measured on a\npossibly high-dimensional grid.\n  In this paper, we leverage two statistical theories to assess causality in\nthe context of multivariate extreme event attribution. As we consider an event\nto be extreme when at least one of the components of the vector of interest is\nlarge, extreme-value theory justifies, in an asymptotical sense, a multivariate\ngeneralized Pareto distribution to model joint extremes. Under this class of\ndistributions, we derive and study probabilities of necessary and sufficient\ncausation as defined by the counterfactual theory of Pearl. To increase causal\nevidence, we propose a dimension reduction strategy based on the optimal linear\nprojection that maximizes such causation probabilities. Our approach is tested\non simulated examples and applied to weekly winter maxima precipitation outputs\nof the French CNRM from the recent CMIP6 experiment.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 15:06:06 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 08:01:05 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kiriliouk", "Anna", ""], ["Naveau", "Philippe", ""]]}, {"id": "1908.03152", "submitter": "Kengo Kato", "authors": "Mingli Chen, Kengo Kato, Chenlei Leng", "title": "Analysis of Networks via the Sparse $\\beta$-Model", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data in the form of networks are increasingly available in a variety of\nareas, yet statistical models allowing for parameter estimates with desirable\nstatistical properties for sparse networks remain scarce. To address this, we\npropose the Sparse $\\beta$-Model (S$\\beta$M), a new network model that\ninterpolates the celebrated Erd\\H{o}s-R\\'enyi model and the $\\beta$-model that\nassigns one different parameter to each node. By a novel reparameterization of\nthe $\\beta$-model to distinguish global and local parameters, our S$\\beta$M can\ndrastically reduce the dimensionality of the $\\beta$-model by requiring some of\nthe local parameters to be zero. We derive the asymptotic distribution of the\nmaximum likelihood estimator of the S$\\beta$M when the support of the parameter\nvector is known. When the support is unknown, we formulate a penalized\nlikelihood approach with the $\\ell_0$-penalty. Remarkably, we show via a\nmonotonicity lemma that the seemingly combinatorial computational problem due\nto the $\\ell_0$-penalty can be overcome by assigning nonzero parameters to\nthose nodes with the largest degrees. We further show that a $\\beta$-min\ncondition guarantees our method to identify the true model and provide excess\nrisk bounds for the estimated parameters. The estimation procedure enjoys good\nfinite sample properties as shown by simulation studies. The usefulness of the\nS$\\beta$M is further illustrated via the analysis of a microfinance take-up\nexample.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 16:25:32 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 12:25:43 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 10:35:49 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Chen", "Mingli", ""], ["Kato", "Kengo", ""], ["Leng", "Chenlei", ""]]}, {"id": "1908.03329", "submitter": "Thierry Mara", "authors": "Thierry A. Mara (PIMENT, GdR MASCOT-NUM)", "title": "Linear regression in the Bayesian framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These notes aim at clarifying different strategies to perform linear\nregression from given dataset. Methods like the weighted and ordinary least\nsquares, ridge regression or LASSO are proposed in the literature. The present\narticle is my understanding of these methods which are, according to me, better\nunified in the Bayesian framework. The formulas to address linear regression\nwith these methods are derived. The KIC for model selection is also derived in\nthe end of the document.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 06:36:31 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Mara", "Thierry A.", "", "PIMENT, GdR MASCOT-NUM"]]}, {"id": "1908.03390", "submitter": "Martin Bladt", "authors": "Martin Bladt, Hansjoerg Albrecher, Jan Beirlant", "title": "Combined Tail Estimation Using Censored Data and Expert Information", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study tail estimation in Pareto-like settings for datasets with a high\npercentage of randomly right-censored data, and where some expert information\non the tail index is available for the censored observations. This setting\narises for instance naturally for liability insurance claims, where actuarial\nexperts build reserves based on the specificity of each open claim, which can\nbe used to improve the estimation based on the already available data points\nfrom closed claims. Through an entropy-perturbed likelihood we derive an\nexplicit estimator and establish a close analogy with Bayesian methods.\nEmbedded in an extreme value approach, asymptotic normality of the estimator is\nshown, and when the expert is clair-voyant, a simple combination formula can be\ndeduced, bridging the classical statistical approach with the expert\ninformation. Following the aforementioned combination formula, a combination of\nquantile estimators can be naturally defined. In a simulation study, the\nestimator is shown to often outperform the Hill estimator for censored\nobservations and recent Bayesian solutions, some of which require more\ninformation than usually available. Finally we perform a case study on a motor\nthird-party liability insurance claim dataset, where Hill-type and quantile\nplots incorporate ultimate values into the estimation procedure in an intuitive\nmanner.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 09:59:52 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 14:53:05 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Bladt", "Martin", ""], ["Albrecher", "Hansjoerg", ""], ["Beirlant", "Jan", ""]]}, {"id": "1908.03531", "submitter": "Guillaume Basse", "authors": "Guillaume Basse, Yi Ding, Panos Toulis", "title": "Minimax designs for causal effects in temporal experiments with\n  treatment habituation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments are the gold standard for estimating the causal\neffects of an intervention. In the simplest setting, each experimental unit is\nrandomly assigned to receive treatment or control, and then the outcomes in\neach treatment arm are compared. In many settings, however, randomized\nexperiments need to be executed over several time periods such that treatment\nassignment happens at each time period. In such temporal experiments, it has\nbeen observed that the effects of an intervention on a given unit may be large\nwhen the unit is first exposed to it, but then it often attenuates, or even\nvanishes, after repeated exposures. This phenomenon is typically due to units'\nhabituation to the intervention, or some other general form of learning, such\nas when users gradually start to ignore repeated mails sent by a promotional\ncampaign. This paper proposes randomized designs for estimating causal effects\nin temporal experiments when habituation is present. We show that our designs\nare minimax optimal in a large class of practical designs. Our analysis is\nbased on the randomization framework of causal inference, and imposes no\nparametric modeling assumptions on the outcomes.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 16:40:05 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 09:52:07 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Basse", "Guillaume", ""], ["Ding", "Yi", ""], ["Toulis", "Panos", ""]]}, {"id": "1908.03606", "submitter": "Jana Jankov\\'a", "authors": "Jana Jankov\\'a, Rajen D. Shah, Peter B\\\"uhlmann, Richard J. Samworth", "title": "Goodness-of-fit testing in high-dimensional generalized linear models", "comments": "40 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of tests to assess the goodness-of-fit of a\nhigh-dimensional generalized linear model. Our framework is flexible and may be\nused to construct an omnibus test or directed against testing specific\nnon-linearities and interaction effects, or for testing the significance of\ngroups of variables. The methodology is based on extracting left-over signal in\nthe residuals from an initial fit of a generalized linear model. This can be\nachieved by predicting this signal from the residuals using modern flexible\nregression or machine learning methods such as random forests or boosted trees.\nUnder the null hypothesis that the generalized linear model is correct, no\nsignal is left in the residuals and our test statistic has a Gaussian limiting\ndistribution, translating to asymptotic control of type I error. Under a local\nalternative, we establish a guarantee on the power of the test. We illustrate\nthe effectiveness of the methodology on simulated and real data examples by\ntesting goodness-of-fit in logistic regression models. Software implementing\nthe methodology is available in the R package `GRPtests'.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 19:30:59 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 20:41:03 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Jankov\u00e1", "Jana", ""], ["Shah", "Rajen D.", ""], ["B\u00fchlmann", "Peter", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1908.03646", "submitter": "Youngjoo Cho", "authors": "Youngjoo Cho (The University of Texas at El Paso), Chen Hu (Johns\n  Hopkins University School of Medicine) and Debashis Ghosh (University of\n  Colorado Anschutz Medical Campus)", "title": "Analysis of regression discontinuity designs using censored data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In medical settings, treatment assignment may be determined by a clinically\nimportant covariate that predicts patients' risk of event. There is a class of\nmethods from the social science literature known as regression discontinuity\n(RD) designs that can be used to estimate the treatment effect in this\nsituation. Under certain assumptions, such an estimand enjoys a causal\ninterpretation. However, few authors have discussed the use of RD for censored\ndata. In this paper, we show how to estimate causal effects under the\nregression discontinuity design for censored data. The proposed estimation\nprocedure employs a class of censoring unbiased transformations that includes\ninverse probability censored weighting and doubly robust transformation\nschemes. Simulation studies demonstrate the utility of the proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 22:08:21 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Cho", "Youngjoo", "", "The University of Texas at El Paso"], ["Hu", "Chen", "", "Johns\n  Hopkins University School of Medicine"], ["Ghosh", "Debashis", "", "University of\n  Colorado Anschutz Medical Campus"]]}, {"id": "1908.03652", "submitter": "Michael Johnson", "authors": "Michael Johnson, Jiongyi Cao, and Hyunseung Kang", "title": "Detecting Heterogeneous Treatment Effect with Instrumental Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an increasing interest in estimating heterogeneity in causal effects\nin randomized and observational studies. However, little research has been\nconducted to understand heterogeneity in an instrumental variables study. In\nthis work, we present a method to estimate heterogeneous causal effects using\nan instrumental variable approach. The method has two parts. The first part\nuses subject-matter knowledge and interpretable machine learning techniques,\nsuch as classification and regression trees, to discover potential effect\nmodifiers. The second part uses closed testing to test for the statistical\nsignificance of the effect modifiers while strongly controlling familywise\nerror rate. We conducted this method on the Oregon Health Insurance Experiment,\nestimating the effect of Medicaid on the number of days an individual's health\ndoes not impede their usual activities, and found evidence of heterogeneity in\nolder men who prefer English and don't self-identify as Asian and younger\nindividuals who have at most a high school diploma or GED and prefer English.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 22:52:44 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 17:05:17 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Johnson", "Michael", ""], ["Cao", "Jiongyi", ""], ["Kang", "Hyunseung", ""]]}, {"id": "1908.03656", "submitter": "Caleb Kwon", "authors": "Caleb Kwon, Eric Mbakop", "title": "Estimation of the Number of Components of Non-Parametric Multivariate\n  Finite Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel estimator for the number of components (denoted by $M$) in\na K-variate non-parametric finite mixture model, where the analyst has repeated\nobservations of $K\\geq2$ variables that are independent given a finitely\nsupported unobserved variable. Under a mild assumption on the joint\ndistribution of the observed and latent variables, we show that an integral\noperator $T$, that is identified from the data, has rank equal to $M$. Using\nthis observation, and the fact that singular values are stable under\nperturbations, the estimator of $M$ that we propose is based on a thresholding\nrule which essentially counts the number of singular values of a consistent\nestimator of $T$ that are greater than a data-driven threshold. We prove that\nour estimator of $M$ is consistent, and establish non-asymptotic results which\nprovide finite sample performance guarantees for our estimator. We present a\nMonte Carlo study which shows that our estimator performs well for samples of\nmoderate size.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 00:24:57 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 02:14:06 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Kwon", "Caleb", ""], ["Mbakop", "Eric", ""]]}, {"id": "1908.03669", "submitter": "Yunan Wu", "authors": "Yunan Wu and Lan Wang", "title": "A Survey of Tuning Parameter Selection for High-dimensional Regression", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized (or regularized) regression, as represented by Lasso and its\nvariants, has become a standard technique for analyzing high-dimensional data\nwhen the number of variables substantially exceeds the sample size. The\nperformance of penalized regression relies crucially on the choice of the\ntuning parameter, which determines the amount of regularization and hence the\nsparsity level of the fitted model. The optimal choice of tuning parameter\ndepends on both the structure of the design matrix and the unknown random error\ndistribution (variance, tail behavior, etc). This article reviews the current\nliterature of tuning parameter selection for high-dimensional regression from\nboth theoretical and practical perspectives. We discuss various strategies that\nchoose the tuning parameter to achieve prediction accuracy or support recovery.\nWe also review several recently proposed methods for tuning-free\nhigh-dimensional regression.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 02:22:42 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Wu", "Yunan", ""], ["Wang", "Lan", ""]]}, {"id": "1908.03717", "submitter": "Shota Gugushvili", "authors": "Itai Dattner and Shota Gugushvili and Harold Ship and Eberhard O. Voit", "title": "Separable nonlinear least-squares parameter estimation for complex\n  dynamic systems", "comments": "17 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear dynamic models are widely used for characterizing functional forms\nof processes that govern complex biological pathway systems. Over the past\ndecade, validation and further development of these models became possible due\nto data collected via high-throughput experiments using methods from molecular\nbiology. While these data are very beneficial, they are typically incomplete\nand noisy, so that inferring parameter values for complex dynamic models is\nassociated with serious computational challenges. Fortunately, many biological\nsystems have embedded linear mathematical features, which may be exploited,\nthereby improving fits and leading to better convergence of optimization\nalgorithms.\n  In this paper, we explore options of inference for dynamic models using a\nnovel method of {\\it separable nonlinear least-squares optimization}, and\ncompare its performance to the traditional nonlinear least-squares method. The\nnumerical results from extensive simulations suggest that the proposed approach\nis at least as accurate as the traditional nonlinear least-squares, but usually\nsuperior, while also enjoying a substantial reduction in computational time.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 09:22:54 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Dattner", "Itai", ""], ["Gugushvili", "Shota", ""], ["Ship", "Harold", ""], ["Voit", "Eberhard O.", ""]]}, {"id": "1908.03836", "submitter": "Yin Xia", "authors": "Yin Xia and Lexin Li", "title": "Hypothesis Testing for Network Data with Power Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing two population means of network data is of paramount importance in\na wide range of scientific applications. Many existing network inference\nsolutions focus on global testing of entire networks, without comparing\nindividual network links. Besides, the observed data often take the form of\nvectors or matrices, and the problem is formulated as comparing two covariance\nor precision matrices under a normal or matrix normal distribution. Moreover,\nmany tests suffer from a limited power under a small sample size. In this\narticle, we tackle the problem of network comparison, both global and\nsimultaneous inferences, when the data come in a different format, i.e., in the\nform of a collection of symmetric matrices, each of which encodes the network\nstructure of an individual subject. Such data format commonly arises in\napplications such as brain connectivity analysis and clinical genomics. We no\nlonger require the underlying data to follow a normal distribution, but instead\nimpose some moment conditions that are easily satisfied for numerous types of\nnetwork data. Furthermore, we propose a power enhancement procedure, and show\nthat it can control the false discovery, while it has the potential to\nsubstantially enhance the power of the test. We investigate the efficacy of our\ntesting procedure through both an asymptotic analysis and a simulation study\nunder a finite sample size. We further illustrate our method with an example of\nbrain structural connectivity analysis.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 01:24:47 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 09:23:20 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Xia", "Yin", ""], ["Li", "Lexin", ""]]}, {"id": "1908.03867", "submitter": "Takashi Arai", "authors": "Takashi Arai", "title": "A new Granger causality measure for eliminating the confounding\n  influence of latent common inputs", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new Granger causality measure which is robust\nagainst the confounding influence of latent common inputs. This measure is\ninspired by partial Granger causality in the literature, and its variant. Using\nnumerical experiments we first show that the test statistics for detecting\ndirected interactions between time series approximately obey the\n$F$-distributions when there are no interactions. Then, we propose a practical\nprocedure for inferring directed interactions, which is based on the idea of\nmultiple statistical test in situations where the confounding influence of\nlatent common inputs may exist. The results of numerical experiments\ndemonstrate that the proposed method successfully eliminates the influence of\nlatent common inputs while the normal Granger causality method detects spurious\ninteractions due to the influence of the confounder.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 08:04:55 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Arai", "Takashi", ""]]}, {"id": "1908.03926", "submitter": "Zhigang Yao", "authors": "Zhigang Yao, Zengyan Fan, Masahito Hayashi, William F. Eddy", "title": "Quantifying Time-Varying Sources in Magnetoencephalography -- A Discrete\n  Approach", "comments": "38 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": "2019", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distribution of brain source from the most advanced brain\nimaging technique, Magnetoencephalography (MEG), which measures the magnetic\nfields outside the human head produced by the electrical activity inside the\nbrain. Common time-varying source localization methods assume the source\ncurrent with a time-varying structure and solve the MEG inverse problem by\nmainly estimating the source moment parameters. These methods use the fact that\nthe magnetic fields linearly depend on the moment parameters of the source, and\nwork well under the linear dynamic system. However, magnetic fields are known\nto be non-linearly related to the location parameters of the source. The\nexisting work on estimating the time-varying unknown location parameters is\nlimited. We are motivated to investigate the source distribution for the\nlocation parameters based on a dynamic framework, where the posterior\ndistribution of the source is computed in a closed form discretely. The new\nframework allows us not only to directly approximate the posterior distribution\nof the source current, where sequential sampling methods may suffer from slow\nconvergence due to the large volume of measurement, but also to quantify the\nsource distribution at any time point from the entire set of measurements\nreflecting the distribution of the source, rather than using only the\nmeasurements up to the time point of interest. Both a dynamic procedure and a\nswitch procedure are proposed for the new discrete approach, balancing\nestimation accuracy and computational efficiency when multiple sources are\npresent. In both simulation and real data, we illustrate that the new method is\nable to provide comprehensive insight into the time evolution of the sources at\ndifferent stages of the MEG and EEG experiment.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 15:46:23 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Yao", "Zhigang", ""], ["Fan", "Zengyan", ""], ["Hayashi", "Masahito", ""], ["Eddy", "William F.", ""]]}, {"id": "1908.03967", "submitter": "Eli Kravitz", "authors": "Eli S. Kravitz, Raymond J. Carroll, David Ruppert", "title": "Sample Splitting as an M-Estimator with Application to Physical Activity\n  Scoring", "comments": "preprint. arXiv admin note: text overlap with arXiv:1908.03968", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample splitting is widely used in statistical applications, including\nclassically in classification and more recently for inference post model\nselection. Motivating by problems in the study of diet, physical activity, and\nhealth, we consider a new application of sample splitting. Physical activity\nresearchers wanted to create a scoring system to quickly assess physical\nactivity levels. A score is created using a large cohort study. Then, using the\nsame data, this score serves as a covariate in a model for the risk of disease\nor mortality. Since the data are used twice in this way, standard errors and\nconfidence intervals from fitting the second model are not valid. To allow for\nproper inference, sample splitting can be used. One builds the score with a\nrandom half of the data and then uses the score when fitting a model to the\nother half of the data. We derive the limiting distribution of the estimators.\nAn obvious question is what happens if multiple sample splits are performed. We\nshow that as the number of sample splits increases, the combination of multiple\nsample splits is effectively equivalent to solving a set of estimating\nequations.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 22:34:01 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kravitz", "Eli S.", ""], ["Carroll", "Raymond J.", ""], ["Ruppert", "David", ""]]}, {"id": "1908.03968", "submitter": "Eli Kravitz", "authors": "Eli S. Kravitz, Raymond J. Carroll, and David Ruppert", "title": "Finite Sample Hypothesis Tests for Stacked Estimating Equations", "comments": "preprint. arXiv admin note: text overlap with arXiv:1908.03967", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose there are two unknown parameters, each parameter is the solution to\nan estimating equation, and the estimating equation of one parameter depends on\nthe other parameter. The parameters can be jointly estimated by \"stacking\"\ntheir estimating equations and solving for both parameters simultaneously.\nAsymptotic confidence intervals are readily available for stacked estimating\nequations. We introduce a bootstrap-based hypothesis test for stacked\nestimating equations which does not rely on asymptotic approximations. Test\nstatistics are constructed by splitting the sample in two, estimating the first\nparameter on a portion of the sample then plugging the result into the second\nestimating equation to solve for the next parameter using the remaining sample.\nTo reduce simulation variability from a single split, we repeatedly split the\nsample and take the sample mean of all the estimates. For parametric models, we\nderive the limiting distribution of sample splitting estimator and show they\nare equivalent to stacked estimating equations.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 22:43:09 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kravitz", "Eli S.", ""], ["Carroll", "Raymond J.", ""], ["Ruppert", "David", ""]]}, {"id": "1908.04002", "submitter": "Ajay Jasra", "authors": "Deng Lu, Maria De Iorio, Ajay Jasra, Gary L. Rosner", "title": "Bayesian Inference for Latent Chain Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider Bayesian inference for partially observed\nAndersson-Madigan-Perlman (AMP) Gaussian chain graph (CG) models. Such models\nare of particular interest in applications such as biological networks and\nfinancial time series. The model itself features a variety of constraints which\nmake both prior modeling and computational inference challenging. We develop a\nframework for the aforementioned challenges, using a sequential Monte Carlo\n(SMC) method for statistical inference. Our approach is illustrated on both\nsimulated data as well as real case studies from university graduation rates\nand a pharmacokinetics study.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 05:00:34 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Lu", "Deng", ""], ["De Iorio", "Maria", ""], ["Jasra", "Ajay", ""], ["Rosner", "Gary L.", ""]]}, {"id": "1908.04053", "submitter": "Ralph Brinks", "authors": "Ralph Brinks", "title": "Estimation of the excess mortality in chronic diseases from prevalence\n  and incidence data", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregated health data such as claims data from health insurances become more\nand more available for research purposes. Estimates of excess mortality from\nprevalence and incidence of a chronic condition have only been possible for\nages 50 years and older and have shown to be unstable in younger ages. The aim\nof this article is to explore the reasons why estimates of excess mortality for\nyounger ages are prone to bias and what can be done to extend the age range to\nages below 50 years.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 08:46:05 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Brinks", "Ralph", ""]]}, {"id": "1908.04217", "submitter": "Michael Robbins", "authors": "Michael W. Robbins, Bonnie Ghosh-Dastidar, and Rajeev Ramchand", "title": "Blending of Probability and Non-Probability Samples: Applications to a\n  Survey of Military Caregivers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probability samples are the preferred method for providing inferences that\nare generalizable to a larger population. However, when a small (or rare)\nsubpopulation is the group of interest, this approach is unlikely to yield a\nsample size large enough to produce precise inferences. Non-probability (or\nconvenience) sampling often provides the necessary sample size to yield\nefficient estimates, but selection bias may compromise the generalizability of\nresults to the broader population. Motivating the exposition is a survey of\nmilitary caregivers; our interest is focused on unpaid caregivers of wounded,\nill, or injured servicemembers and veterans who served in the US armed forces\nfollowing September 11, 2001. An extensive probability sampling effort yielded\nonly 72 caregivers from this subpopulation. Therefore, we consider\nsupplementing the probability sample with a convenience sample from the same\nsubpopulation, and we develop novel methods of statistical weighting that may\nbe used to combine (or blend) the samples. Our analyses show that the\nsubpopulation of interest endures greater hardships than caregivers of veterans\nwith earlier dates of service, and these conclusions are discernably stronger\nwhen blended samples with the proposed weighting schemes are used. We conclude\nwith simulation studies that illustrate the efficacy of the proposed\ntechniques, examine the bias-variance trade-off encountered when using\ninadequately blended data, and show that the gain in precision provided by the\nconvenience sample is lower in circumstances where the outcome is strongly\nrelated to the auxiliary variables used for blending.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 16:06:24 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Robbins", "Michael W.", ""], ["Ghosh-Dastidar", "Bonnie", ""], ["Ramchand", "Rajeev", ""]]}, {"id": "1908.04218", "submitter": "Panos Toulis", "authors": "Panos Toulis", "title": "Life After Bootstrap: Residual Randomization Inference in Regression\n  Models", "comments": "7 figures, 7 tables, R package\n  (https://cran.r-project.org/package=RRI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a randomization-based method for inference in regression models.\nThe basis of inference is an invariance assumption on the regression errors,\nsuch as invariance to permutations or random signs. To test significance, the\nrandomization method repeatedly calculates a suitable test statistic over\ntransformations of the regression residuals according to the invariant.\nInversion of the test can produce confidence intervals. We prove general\nconditions for asymptotic validity of this residual randomization test and\nillustrate in many models, including clustered errors with one-way or two-way\nclustering structure. We also show that finite-sample validity is possible\nunder a suitable construction, and illustrate with an exact test for a case of\nthe Behrens-Fisher problem. The proposed method offers four main advantages\nover the bootstrap: (1) it addresses the inference problem in a unified way,\nwhile bootstrap typically needs to be adapted to the task; (2) it can be more\npowerful by exploiting a richer and more flexible set of invariances than\nexchangeability; (3) it does not rely on asymptotic normality; and (4) it can\nbe valid in finite samples. In extensive empirical evaluations, including high\ndimensional regression and autocorrelated errors, the proposed method performs\nfavorably against many alternatives, including bootstrap variants and\nasymptotic robust error methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 16:09:15 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Toulis", "Panos", ""]]}, {"id": "1908.04427", "submitter": "Chan Park", "authors": "Chan Park and Hyunseung Kang", "title": "A Groupwise Approach for Inferring Heterogeneous Treatment Effects in\n  Causal Inference", "comments": "63 pages including supplementary materials, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been great interest in estimating the conditional average\ntreatment effect using flexible machine learning methods. However, this\nestimate is often difficult to interpret if covariates are high dimensional.\nThe paper propose a groupwise approach to study effect heterogeneity where the\nconditional average treatment effect is partitioned into interpretable\nsubgroups. Our method is simple, only based on linear regression and sample\nsplitting, and is semiparametrically efficient under assumptions. We also\ndiscuss ways to conduct multiple testing across different subgroups. We\nconclude by reanalyzing a get-out-the-vote experiment during the 2014 U.S.\nmidterm elections.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 22:34:18 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 02:28:38 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Park", "Chan", ""], ["Kang", "Hyunseung", ""]]}, {"id": "1908.04695", "submitter": "Lam Yau", "authors": "Ekkehard Glimm, Lillian Yau, and Heike Woehling", "title": "Blinded sample size re-estimation in equivalence testing", "comments": "25 pages, 7 figures", "journal-ref": "Statistics in Biopharmaceutical Research 2021", "doi": "10.1080/19466315.2020.1845232", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates type I error violations that occur when blinded\nsample size reviews are applied in equivalence testing. We give a derivation\nwhich explains why such violations are more pronounced in equivalence testing\nthan in the case of superiority testing. In addition, the amount of type I\nerror inflation is quantified by simulation as well as by some theoretical\nconsiderations. Non-negligible type I error violations arise when blinded\ninterim re-assessments of sample sizes are performed particularly if sample\nsizes are small, but within the range of what is practically relevant.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 15:10:31 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Glimm", "Ekkehard", ""], ["Yau", "Lillian", ""], ["Woehling", "Heike", ""]]}, {"id": "1908.04748", "submitter": "Michele Santacatterina", "authors": "Nathan Kallus, Michele Santacatterina", "title": "Optimal Estimation of Generalized Average Treatment Effects using Kernel\n  Optimal Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In causal inference, a variety of causal effect estimands have been studied,\nincluding the sample, uncensored, target, conditional, optimal subpopulation,\nand optimal weighted average treatment effects. Ad-hoc methods have been\ndeveloped for each estimand based on inverse probability weighting (IPW) and on\noutcome regression modeling, but these may be sensitive to model\nmisspecification, practical violations of positivity, or both. The contribution\nof this paper is twofold. First, we formulate the generalized average treatment\neffect (GATE) to unify these causal estimands as well as their IPW estimates.\nSecond, we develop a method based on Kernel Optimal Matching (KOM) to optimally\nestimate GATE and to find the GATE most easily estimable by KOM, which we term\nthe Kernel Optimal Weighted Average Treatment Effect. KOM provides uniform\ncontrol on the conditional mean squared error of a weighted estimator over a\nclass of models while simultaneously controlling for precision. We study its\ntheoretical properties and evaluate its comparative performance in a simulation\nstudy. We illustrate the use of KOM for GATE estimation in two case studies:\ncomparing spine surgical interventions and studying the effect of peer support\non people living with HIV.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 17:09:02 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 16:21:44 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Kallus", "Nathan", ""], ["Santacatterina", "Michele", ""]]}, {"id": "1908.04822", "submitter": "Imke Mayer", "authors": "Imke Mayer, Aude Sportisse, Julie Josse, Nicholas Tierney and Nathalie\n  Vialaneix", "title": "R-miss-tastic: a unified platform for missing values methods and\n  workflows", "comments": "38 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing values are unavoidable when working with data. Their occurrence is\nexacerbated as more data from different sources become available. However, most\nstatistical models and visualization methods require complete data, and\nimproper handling of missing data results in information loss, or biased\nanalyses. Since the seminal work of Rubin (1976), there has been a burgeoning\nliterature on missing values with heterogeneous aims and motivations. This has\nresulted in the development of various methods, formalizations, and tools\n(including a large number of R packages and Python modules). However, for\npractitioners, it remains challenging to decide which method is most suited for\ntheir problem, partially because handling missing data is still not a topic\nsystematically covered in statistics or data science curricula.\n  To help address this challenge, we have launched a unified platform:\n\"R-miss-tastic\", which aims to provide an overview of standard missing values\nproblems, methods, how to handle them in analyses, and relevant implementations\nof methodologies. In the same perspective, we have also developed several\npipelines in R and Python to allow for a hands-on illustration of how to handle\nmissing values in various statistical tasks such as estimation and prediction,\nwhile ensuring reproducibility of the analyses. This will hopefully also\nprovide some guidance on deciding which method to choose for a specific problem\nand data. The objective of this work is not only to comprehensively organize\nmaterials, but also to create standardized analysis workflows, and to provide a\ncommon ground for discussions among the community. This platform is thus suited\nfor beginners, students, more advanced analysts and researchers.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 18:52:24 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 14:06:22 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Mayer", "Imke", ""], ["Sportisse", "Aude", ""], ["Josse", "Julie", ""], ["Tierney", "Nicholas", ""], ["Vialaneix", "Nathalie", ""]]}, {"id": "1908.04904", "submitter": "Feng Li", "authors": "Xuening Zhu, Feng Li, Hansheng Wang", "title": "Least Squares Approximation for a Distributed System", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics 2021", "doi": "10.1080/10618600.2021.1923517", "report-no": null, "categories": "stat.ME cs.DC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a distributed least squares approximation (DLSA)\nmethod that is able to solve a large family of regression problems (e.g.,\nlinear regression, logistic regression, and Cox's model) on a distributed\nsystem. By approximating the local objective function using a local quadratic\nform, we are able to obtain a combined estimator by taking a weighted average\nof local estimators. The resulting estimator is proved to be statistically as\nefficient as the global estimator. Moreover, it requires only one round of\ncommunication. We further conduct a shrinkage estimation based on the DLSA\nestimation using an adaptive Lasso approach. The solution can be easily\nobtained by using the LARS algorithm on the master node. It is theoretically\nshown that the resulting estimator possesses the oracle property and is\nselection consistent by using a newly designed distributed Bayesian information\ncriterion (DBIC). The finite sample performance and computational efficiency\nare further illustrated by an extensive numerical study and an airline dataset.\nThe airline dataset is 52 GB in size. The entire methodology has been\nimplemented in Python for a {\\it de-facto} standard Spark system. The proposed\nDLSA algorithm on the Spark system takes 26 minutes to obtain a logistic\nregression estimator, which is more efficient and memory friendly than\nconventional methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:05:21 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 01:46:47 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 07:11:52 GMT"}, {"version": "v4", "created": "Tue, 13 Apr 2021 09:53:50 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhu", "Xuening", ""], ["Li", "Feng", ""], ["Wang", "Hansheng", ""]]}, {"id": "1908.04957", "submitter": "Yong He", "authors": "Yong He and Xinbing Kong and Long Yu and Xinsheng Zhang", "title": "Large-dimensional Factor Analysis without Moment Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-dimensional factor model has drawn much attention in the big-data era,\nin order to reduce the dimensionality and extract underlying features using a\nfew latent common factors. Conventional methods for estimating the factor model\ntypically requires finite fourth moment of the data, which ignores the effect\nof heavy-tailedness and thus may result in unrobust or even inconsistent\nestimation of the factor space and common components. In this paper, we propose\nto recover the factor space by performing principal component analysis to the\nspatial Kendall's tau matrix instead of the sample covariance matrix. In a\nsecond step, we estimate the factor scores by the ordinary least square (OLS)\nregression. Theoretically, we show that under the elliptical distribution\nframework the factor loadings and scores as well as the common components can\nbe estimated consistently without any moment constraint. The convergence rates\nof the estimated factor loadings, scores and common components are provided.\nThe finite sample performance of the proposed procedure is assessed through\nthorough simulations. An analysis of a financial data set of asset returns\nshows the superiority of the proposed method over the classical PCA method.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 05:12:52 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 09:05:34 GMT"}, {"version": "v3", "created": "Sat, 30 May 2020 07:21:20 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["He", "Yong", ""], ["Kong", "Xinbing", ""], ["Yu", "Long", ""], ["Zhang", "Xinsheng", ""]]}, {"id": "1908.05065", "submitter": "Heidi S{\\o}gaard Christensen", "authors": "Andreas D. Christoffersen, Jesper M{\\o}ller, Heidi S. Christensen", "title": "Modelling columnarity of pyramidal cells in the human cerebral cortex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For modelling the location of pyramidal cells in the human cerebral cortex we\nsuggest a hierarchical point process in $\\mathbb{R}^3$ that exhibits anisotropy\nin the form of cylinders extending along the $z$-axis. The model consists first\nof a generalised shot noise Cox process for the $xy$-coordinates, providing\ncylindrical clusters, and next of a Markov random field model for the\n$z$-coordinates conditioned on the $xy$-coordinates, providing either\nrepulsion, aggregation, or both within specified areas of interaction. Several\ncases of these hierarchical point processes are fitted to two pyramidal cell\ndatasets, and of these a final model allowing for both repulsion and attraction\nbetween the points seem adequate. We discuss how the final model relates to the\nso-called minicolumn hypothesis in neuroscience.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 10:47:27 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 19:38:15 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 18:43:39 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2020 07:11:44 GMT"}, {"version": "v5", "created": "Tue, 24 Nov 2020 15:53:03 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Christoffersen", "Andreas D.", ""], ["M\u00f8ller", "Jesper", ""], ["Christensen", "Heidi S.", ""]]}, {"id": "1908.05091", "submitter": "Haiyan Zheng", "authors": "Haiyan Zheng, James M.S. Wason", "title": "Borrowing of information across patient subgroups in a basket trial\n  based on distributional discrepancy", "comments": "A submitted paper with 15 pages, 2 figures and 2 tables", "journal-ref": "Biostatistics 2020", "doi": "10.1093/biostatistics/kxaa019", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basket trials have emerged as a new class of efficient approaches in oncology\nto evaluate a new treatment in several patient subgroups simultaneously. In\nthis paper, we extend the key ideas to disease areas outside of oncology,\ndeveloping a robust Bayesian methodology for randomised, placebo-controlled\nbasket trials with a continuous endpoint to enable borrowing of information\nacross subtrials with similar treatment effects. After adjusting for\ncovariates, information from a complementary subtrial can be represented into a\ncommensurate prior for the parameter that underpins the subtrial under\nconsideration. We propose using distributional discrepancy to characterise the\ncommensurability between subtrials for appropriate borrowing of information\nthrough a spike-and-slab prior, which is placed on the prior precision factor.\nWhen the basket trial has at least three subtrials, commensurate priors for\npoint-to-point borrowing are combined into a marginal predictive prior,\naccording to the weights transformed from the pairwise discrepancy measures. In\nthis way, only information from subtrial(s) with the most commensurate\ntreatment effect is leveraged. The marginal predictive prior is updated to a\nrobust posterior by the contemporary subtrial data to inform decision making.\nOperating characteristics of the proposed methodology are evaluated through\nsimulations motivated by a real basket trial in chronic diseases. The proposed\nmethodology has advantages compared to other selected Bayesian analysis models,\nfor (i) identifying the most commensurate source of information, and (ii)\ngauging the degree of borrowing from specific subtrials. Numerical results also\nsuggest that our methodology can improve the precision of estimates and,\npotentially, the statistical power for hypothesis testing.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 12:17:22 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 10:52:43 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 11:14:11 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Zheng", "Haiyan", ""], ["Wason", "James M. S.", ""]]}, {"id": "1908.05097", "submitter": "Nicola Gnecco", "authors": "Nicola Gnecco, Nicolai Meinshausen, Jonas Peters, Sebastian Engelke", "title": "Causal discovery in heavy-tailed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal questions are omnipresent in many scientific problems. While much\nprogress has been made in the analysis of causal relationships between random\nvariables, these methods are not well suited if the causal mechanisms only\nmanifest themselves in extremes. This work aims to connect the two fields of\ncausal inference and extreme value theory. We define the causal tail\ncoefficient that captures asymmetries in the extremal dependence of two random\nvariables. In the population case, the causal tail coefficient is shown to\nreveal the causal structure if the distribution follows a linear structural\ncausal model. This holds even in the presence of latent common causes that have\nthe same tail index as the observed variables. Based on a consistent estimator\nof the causal tail coefficient, we propose a computationally highly efficient\nalgorithm that estimates the causal structure. We prove that our method\nconsistently recovers the causal order and we compare it to other\nwell-established and non-extremal approaches in causal discovery on synthetic\nand real data. The code is available as an open-access R package.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 12:39:00 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2019 18:47:01 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 08:08:38 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Gnecco", "Nicola", ""], ["Meinshausen", "Nicolai", ""], ["Peters", "Jonas", ""], ["Engelke", "Sebastian", ""]]}, {"id": "1908.05272", "submitter": "Niels Olsen", "authors": "Niels Lundtorp Olsen, Alessia Pini, Simone Vantini", "title": "False Discovery Rate for Functional Data", "comments": "22 pages and three pages of appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Benjamini and Hochberg introduced false discovery rate (FDR) in their\nseminal paper, this has become a very popular approach to the multiple\ncomparisons problem. An increasingly popular topic within functional data\nanalysis is local inference, i.e., the continuous statistical testing of a null\nhypothesis along the domain. The principal issue in this topic is the infinite\namount of tested hypotheses, which can be seen as an extreme case of the\nmultiple comparisons problem. In this paper we define and discuss the notion of\nfalse discovery rate in a very general functional data setting. Moreover, a\ncontinuous version of the Benjamini-Hochberg procedure is introduced along with\na definition of adjusted p-value function. Some general conditions are stated,\nunder which the functional Benjamini-Hochberg procedure provides control of the\nfunctional FDR. Two different simulation studies are presented; the first study\nhas a one-dimensional domain and a comparison with another state of the art\nmethod, and the second study has a planar two-dimensional domain. Finally, the\nproposed method is applied to satellite measurements of Earth temperature. In\ndetail, we aim at identifying the regions of the planet where temperature has\nsignificantly increased in the last decades. After adjustment, large areas are\nstill significant.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 05:30:43 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Olsen", "Niels Lundtorp", ""], ["Pini", "Alessia", ""], ["Vantini", "Simone", ""]]}, {"id": "1908.05287", "submitter": "Mohsen Shahhosseini", "authors": "Mohsen Shahhosseini, Guiping Hu, Hieu Pham", "title": "Optimizing Ensemble Weights and Hyperparameters of Machine Learning\n  Models for Regression Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregating multiple learners through an ensemble of models aim to make\nbetter predictions by capturing the underlying distribution of the data more\naccurately. Different ensembling methods, such as bagging, boosting, and\nstacking/blending, have been studied and adopted extensively in research and\npractice. While bagging and boosting focus more on reducing variance and bias,\nrespectively, stacking approaches target both by finding the optimal way to\ncombine base learners. In stacking with the weighted average, ensembles are\ncreated from weighted averages of multiple base learners. It is known that\ntuning hyperparameters of each base learner inside the ensemble weight\noptimization process can produce better performing ensembles. To this end, an\noptimization-based nested algorithm that considers tuning hyperparameters as\nwell as finding the optimal weights to combine ensembles (Generalized Weighted\nEnsemble with Internally Tuned Hyperparameters (GEM-ITH)) is designed. Besides,\nBayesian search was used to speed-up the optimizing process, and a heuristic\nwas implemented to generate diverse and well-performing base learners. The\nalgorithm is shown to be generalizable to real data sets through analyses with\nten publicly available data sets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 18:01:02 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 18:10:57 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 17:50:15 GMT"}, {"version": "v4", "created": "Wed, 11 Sep 2019 22:50:29 GMT"}, {"version": "v5", "created": "Sun, 19 Jan 2020 20:26:46 GMT"}, {"version": "v6", "created": "Sat, 31 Oct 2020 20:28:35 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Shahhosseini", "Mohsen", ""], ["Hu", "Guiping", ""], ["Pham", "Hieu", ""]]}, {"id": "1908.05319", "submitter": "Xiongzhi Chen", "authors": "Xiongzhi Chen and Sanat K. Sarkar", "title": "A grouped, selectively weighted false discovery rate procedure", "comments": "43 pages; 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  False discovery rate (FDR) control in structured hypotheses testing is an\nimportant topic in simultaneous inference. Most existing methods that aim to\nutilize group structure among hypotheses either employ the groupwise mixture\nmodel or weight all p-values or hypotheses. Thus, their powers can be improved\nwhen the groupwise mixture model is inappropriate or when most groups contain\nonly true null hypotheses. Motivated by this, we propose a grouped, selectively\nweighted FDR procedure, which we refer to as \"sGBH\". Specifically, without\nemploying the groupwise mixture model, sGBH identifies groups of hypotheses of\ninterest, weights p-values in each such group only, and tests only the selected\nhypotheses using the weighted p-values. The sGBH subsumes a standard grouped,\nweighted FDR procedure which we refer to as \"GBH\". We provide simple conditions\nto ensure the conservativeness of sGBH, together with empirical evidence on its\nmuch improved power over GBH. The new procedure is applied to a gene expression\nstudy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 19:23:15 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Chen", "Xiongzhi", ""], ["Sarkar", "Sanat K.", ""]]}, {"id": "1908.05340", "submitter": "Joshua Keller", "authors": "Joshua P. Keller, Joanne Katz, Amid K. Pokhrel, Michael N. Bates,\n  James Tielsch, Scott L. Zeger", "title": "A hierarchical model for estimating exposure-response curves from\n  multiple studies", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cookstove replacement trials have found mixed results on their impact on\nrespiratory health. The limited range of concentrations and small sample sizes\nof individual studies are important factors that may be limiting their\nstatistical power. We present a hierarchical approach to modeling exposure\nconcentrations and pooling data from multiple studies in order to estimate a\ncommon exposure-response curve. The exposure concentration model accommodates\ntemporally sparse, clustered longitudinal observations. The exposure-response\ncurve model provides a flexible, semi-parametric estimate of the\nexposure-response relationship while accommodating heterogeneous clustered\ndata. We apply this model to data from three studies of cookstoves and\nrespiratory infections in children in Nepal, which represent three study types:\ncrossover trial, parallel trial, and case-control study. We find evidence of\nincreased odds of disease for particulate matter concentrations between 50 and\n200 $\\mu$g/m$^3$ and a flattening of the exposure-response curve for higher\nexposure concentrations. The model we present can incorporate additional\nstudies and be applied to other settings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 20:37:15 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Keller", "Joshua P.", ""], ["Katz", "Joanne", ""], ["Pokhrel", "Amid K.", ""], ["Bates", "Michael N.", ""], ["Tielsch", "James", ""], ["Zeger", "Scott L.", ""]]}, {"id": "1908.05428", "submitter": "Yaniv Romano", "authors": "Yaniv Romano, Rina Foygel Barber, Chiara Sabatti, Emmanuel J. Cand\\`es", "title": "With Malice Towards None: Assessing Uncertainty via Equalized Coverage", "comments": "14 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important factor to guarantee a fair use of data-driven recommendation\nsystems is that we should be able to communicate their uncertainty to decision\nmakers. This can be accomplished by constructing prediction intervals, which\nprovide an intuitive measure of the limits of predictive performance. To\nsupport equitable treatment, we force the construction of such intervals to be\nunbiased in the sense that their coverage must be equal across all protected\ngroups of interest. We present an operational methodology that achieves this\ngoal by offering rigorous distribution-free coverage guarantees holding in\nfinite samples. Our methodology, equalized coverage, is flexible as it can be\nviewed as a wrapper around any predictive algorithm. We test the applicability\nof the proposed framework on real data, demonstrating that equalized coverage\nconstructs unbiased prediction intervals, unlike competitive methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 05:50:27 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Romano", "Yaniv", ""], ["Barber", "Rina Foygel", ""], ["Sabatti", "Chiara", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1908.05517", "submitter": "Tom Britton", "authors": "Tom Britton", "title": "Epidemic models on social networks -- with inference", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider stochastic models for the spread of an infection in a structured\ncommunity, where this structured community is itself described by a random\nnetwork model. Some common network models and transmission models are defined\nand large population proporties of them are presented. Focus is then shifted to\nstatistical methodology: what can be estimated and how, depending on the\nunderlying network, transmission model and the available data? This survey\npaper discusses several different scenarios, also giving references to\npublications where more details can be found.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 12:44:37 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Britton", "Tom", ""]]}, {"id": "1908.05562", "submitter": "Duncan Wilson", "authors": "Duncan T. Wilson and Rebecca E. A. Walwyn and Julia Brown and Amanda\n  J. Farrin", "title": "A hypothesis test of feasibility for external pilot trials assessing\n  recruitment, follow-up and adherence rates", "comments": null, "journal-ref": null, "doi": "10.1002/sim.9091", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The power of a large clinical trial can be adversely affected by low\nrecruitment, follow-up and adherence rates. External pilot trials estimate\nthese rates and use them, via pre-specified decision rules, to determine if the\ndefinitive trial is feasible and should go ahead. There is little\nmethodological research underpinning how these decision rules, or the sample\nsize of the pilot, should be chosen. In this paper we propose a hypothesis test\nof the feasibility of a definitive trial, to be applied to the external pilot\ndata and used to make progression decisions. We quantify feasibility by the\npower of the planned trial, as a function of recruitment, follow-up and\nadherence rates. We use this measure to define hypotheses to test in the pilot,\npropose a test statistic, and show how the error rates of this test can be\ncalculated for the common scenario of a two-arm parallel group definitive trial\nwith a single normally distributed primary endpoint. We use our method to\nre-design TIGA-CUB, an external pilot trial comparing a psychotherapy with\ntreatment as usual for children with conduct disorders. We then extend our\nformulation to include using the pilot data to estimate the standard deviation\nof the primary endpoint. and incorporate this into the progression decision.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 14:31:15 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wilson", "Duncan T.", ""], ["Walwyn", "Rebecca E. A.", ""], ["Brown", "Julia", ""], ["Farrin", "Amanda J.", ""]]}, {"id": "1908.05607", "submitter": "Weixin Cai", "authors": "Mark J. van der Laan, David Benkeser and Weixin Cai", "title": "Efficient Estimation of Pathwise Differentiable Target Parameters with\n  the Undersmoothed Highly Adaptive Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of a functional parameter of a realistically modeled\ndata distribution based on observing independent and identically distributed\nobservations. We define an $m$-th order Spline Highly Adaptive Lasso Minimum\nLoss Estimator (Spline HAL-MLE) of a functional parameter that is defined by\nminimizing the empirical risk function over an $m$-th order smoothness class of\nfunctions. We show that this $m$-th order smoothness class consists of all\nfunctions that can be represented as an infinitesimal linear combination of\ntensor products of $\\leq m$-th order spline-basis functions, and involves\nassuming $m$-derivatives in each coordinate. By selecting $m$ with\ncross-validation we obtain a Spline-HAL-MLE that is able to adapt to the\nunderlying unknown smoothness of the true function, while guaranteeing a rate\nof convergence faster than $n^{-1/4}$, as long as the true function is cadlag\n(right-continuous with left-hand limits) and has finite sectional variation\nnorm. The $m=0$-smoothness class consists of all cadlag functions with finite\nsectional variation norm and corresponds with the original HAL-MLE defined in\nvan der Laan (2015).\n  In this article we establish that this Spline-HAL-MLE yields an\nasymptotically efficient estimator of any smooth feature of the functional\nparameter under an easily verifiable global undersmoothing condition. A\nsufficient condition for the latter condition is that the minimum of the\nempirical mean of the selected basis functions is smaller than a constant times\n$n^{-1/2}$, which is not parameter specific and enforces the selection of the\n$L_1$-norm in the lasso to be large enough to include sparsely supported basis.\nWe demonstrate our general result for the $m=0$-HAL-MLE of the average\ntreatment effect and of the integral of the square of the data density. We also\npresent simulations for these two examples confirming the theory.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 16:28:37 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 17:32:36 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["van der Laan", "Mark J.", ""], ["Benkeser", "David", ""], ["Cai", "Weixin", ""]]}, {"id": "1908.05654", "submitter": "Wai-Tong Louis Fan", "authors": "Wai-Tong Louis Fan", "title": "Correlation function methods for a system of annihilating Brownian\n  particles", "comments": "9 pages expository note", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math-ph math.MP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this expository note we highlight the correlation function method as a\nunified approach in proving both hydrodynamic limits and fluctuation limits for\nreaction diffusion particle systems. For simplicity we focus on the case when\nthe hydrodynamic limit is $\\partial_t u=\\frac{1}{2}\\Delta u -u^2$, one of the\nsimplest nonlinear reaction-diffusion equations. The outline of the proof\nfollows from Chapter 4 of De Masi and Presutti [7] but to simplify the\npresentation, we consider reflected Brownian motion instead of reflected random\nwalks. We also briefly mention the key ideas in proving the fluctuation result.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 17:54:30 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 02:57:37 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Fan", "Wai-Tong Louis", ""]]}, {"id": "1908.05752", "submitter": "Andrii Babii", "authors": "Andrii Babii and Rohit Kumar", "title": "Isotonic Regression Discontinuity Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the estimation and inference for the isotonic regression\nat the boundary point, an object that is particularly interesting and required\nin the analysis of monotone regression discontinuity designs. We show that the\nisotonic regression is inconsistent in this setting and derive the asymptotic\ndistributions of boundary corrected estimators. Interestingly, the boundary\ncorrected estimators can be bootstrapped without subsampling or additional\nnonparametric smoothing which is not the case for the interior point. The Monte\nCarlo experiments indicate that shape restrictions can improve dramatically the\nfinite-sample performance of unrestricted estimators. Lastly, we apply the\nisotonic regression discontinuity designs to estimate the causal effect of\nincumbency in the U.S. House elections.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 20:44:44 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 20:57:46 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 20:23:32 GMT"}, {"version": "v4", "created": "Wed, 11 Dec 2019 21:55:47 GMT"}, {"version": "v5", "created": "Wed, 11 Mar 2020 17:22:30 GMT"}, {"version": "v6", "created": "Sat, 19 Dec 2020 19:53:09 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Babii", "Andrii", ""], ["Kumar", "Rohit", ""]]}, {"id": "1908.05810", "submitter": "Amanda Kowalski", "authors": "Amanda Kowalski", "title": "A Model of a Randomized Experiment with an Application to the PROWESS\n  Clinical Trial", "comments": "This paper has been combined with arXiv:1908.05811 and superseded by\n  arXiv:1912.06739", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I develop a model of a randomized experiment with a binary intervention and a\nbinary outcome. Potential outcomes in the intervention and control groups give\nrise to four types of participants. Fixing ideas such that the outcome is\nmortality, some participants would live regardless, others would be saved,\nothers would be killed, and others would die regardless. These potential\noutcome types are not observable. However, I use the model to develop\nestimators of the number of participants of each type. The model relies on the\nrandomization within the experiment and on deductive reasoning. I apply the\nmodel to an important clinical trial, the PROWESS trial, and I perform a Monte\nCarlo simulation calibrated to estimates from the trial. The reduced form from\nthe trial shows a reduction in mortality, which provided a rationale for FDA\napproval. However, I find that the intervention killed two participants for\nevery three it saved.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 01:40:05 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 20:08:24 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Kowalski", "Amanda", ""]]}, {"id": "1908.05811", "submitter": "Amanda Kowalski", "authors": "Amanda Kowalski", "title": "Counting Defiers", "comments": "This paper has been combined with arXiv:1908.05810 and superseded by\n  arXiv:1912.06739", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LATE monotonicity assumption of Imbens and Angrist (1994) precludes\n\"defiers,\" individuals whose treatment always runs counter to the instrument,\nin the terminology of Balke and Pearl (1993) and Angrist et al. (1996). I allow\nfor defiers in a model with a binary instrument and a binary treatment. The\nmodel is explicit about the randomization process that gives rise to the\ninstrument. I use the model to develop estimators of the counts of defiers,\nalways takers, compliers, and never takers. I propose separate versions of the\nestimators for contexts in which the parameter of the randomization process is\nunspecified, which I intend for use with natural experiments with virtual\nrandom assignment. I present an empirical application that revisits Angrist and\nEvans (1998), which examines the impact of virtual random assignment of the sex\nof the first two children on subsequent fertility. I find that subsequent\nfertility is much more responsive to the sex mix of the first two children when\ndefiers are allowed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 01:44:57 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 20:05:56 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Kowalski", "Amanda", ""]]}, {"id": "1908.05873", "submitter": "Fan Yin", "authors": "Fan Yin, Nolan Edward Phillips, Carter T. Butts", "title": "Selection of Exponential-Family Random Graph Models via Held-Out\n  Predictive Evaluation (HOPE)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models for networks with complex dependencies pose particular\nchallenges for model selection and evaluation. In particular, many\nwell-established statistical tools for selecting between models assume\nconditional independence of observations and/or conventional asymptotics, and\ntheir theoretical foundations are not always applicable in a network modeling\ncontext. While simulation-based approaches to model adequacy assessment are now\nwidely used, there remains a need for procedures that quantify a model's\nperformance in a manner suitable for selecting among competing models. Here, we\npropose to address this issue by developing a predictive evaluation strategy\nfor exponential family random graph models that is analogous to\ncross-validation. Our approach builds on the held-out predictive evaluation\n(HOPE) scheme introduced by Wang et al. (2016) to assess imputation\nperformance. We systematically hold out parts of the observed network to:\nevaluate how well the model is able to predict the held-out data; identify\nwhere the model performs poorly based on which data are held-out, indicating\ne.g. potential weaknesses; and calculate general summaries of predictive\nperformance that can be used for model selection. As such, HOPE can assist\nresearchers in improving models by indicating where a model performs poorly,\nand by quantitatively comparing predictive performance across competing models.\nThe proposed method is applied to model selection problem of two well-known\ndata sets, and the results are compared to those obtained via nominal AIC and\nBIC scores.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 07:40:56 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 04:29:35 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Yin", "Fan", ""], ["Phillips", "Nolan Edward", ""], ["Butts", "Carter T.", ""]]}, {"id": "1908.05885", "submitter": "Rasmus Br{\\o}ndum", "authors": "Rasmus Froberg Br{\\o}ndum, Thomas Yssing Michaelsen, Martin B{\\o}gsted", "title": "Regression on imperfect class labels derived by unsupervised clustering", "comments": null, "journal-ref": null, "doi": "10.1093/bib/bbaa014", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outcome regressed on class labels identified by unsupervised clustering is\ncustom in many applications. However, it is common to ignore the\nmisclassification of class labels caused by the learning algorithm, which\npotentially leads to serious bias of the estimated effect parameters. Due to\nits generality we suggest to redress the situation by use of the simulation and\nextrapolation method. Performance is illustrated by simulated data from\nGaussian mixture models. Finally, we apply our method to a study which\nregressed overall survival on class labels derived from unsupervised clustering\nof gene expression data from bone marrow samples of multiple myeloma patients.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 08:26:03 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Br\u00f8ndum", "Rasmus Froberg", ""], ["Michaelsen", "Thomas Yssing", ""], ["B\u00f8gsted", "Martin", ""]]}, {"id": "1908.05955", "submitter": "Duncan Wilson", "authors": "Duncan T. Wilson, James M. S. Wason, Julia Brown, Amanda J. Farrin,\n  Rebecca E. A. Walwyn", "title": "Bayesian design and analysis of external pilot trials for complex\n  interventions", "comments": null, "journal-ref": null, "doi": "10.1002/sim.8941", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  External pilot trials of complex interventions are used to help determine if\nand how a confirmatory trial should be undertaken, providing estimates of\nparameters such as recruitment, retention and adherence rates. The decision to\nprogress to the confirmatory trial is typically made by comparing these\nestimates to pre-specified thresholds known as progression criteria, although\nthe statistical properties of such decision rules are rarely assessed. Such\nassessment is complicated by several methodological challenges, including the\nsimultaneous evaluation of multiple endpoints, complex multi-level models,\nsmall sample sizes, and uncertainty in nuisance parameters. In response to\nthese challenges, we describe a Bayesian approach to the design and analysis of\nexternal pilot trials. We show how progression decisions can be made by\nminimising the expected value of a loss function, defined over the whole\nparameter space to allow for preferences and trade-offs between multiple\nparameters to be articulated and used in the decision making process. The\nassessment of preferences is kept feasible by using a piecewise constant\nparameterisation of the loss function, the parameters of which are chosen at\nthe design stage to lead to desirable operating characteristics. We describe a\nflexible, yet computationally intensive, nested Monte Carlo algorithm for\nestimating operating characteristics. The method is used to revisit the design\nof an external pilot trial of a complex intervention designed to increase the\nphysical activity of care home residents.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 12:55:07 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 15:52:10 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Wilson", "Duncan T.", ""], ["Wason", "James M. S.", ""], ["Brown", "Julia", ""], ["Farrin", "Amanda J.", ""], ["Walwyn", "Rebecca E. A.", ""]]}, {"id": "1908.06029", "submitter": "Victor Solo", "authors": "Victor Solo", "title": "Pearson Distance is not a Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Pearson distance between a pair of random variables $X,Y$ with\ncorrelation $\\rho_{xy}$, namely, 1-$\\rho_{xy}$, has gained widespread use,\nparticularly for clustering, in areas such as gene expression analysis, brain\nimaging and cyber security. In all these applications it is implicitly\nassumed/required that the distance measures be metrics, thus satisfying the\ntriangle inequality. We show however, that Pearson distance is not a metric. We\ngo on to show that this can be repaired by recalling the result, (well known in\nother literature) that $\\sqrt{1-\\rho_{xy}}$ is a metric. We similarly show that\na related measure of interest, $1-|\\rho_{xy}|$, which is invariant to the sign\nof $\\rho_{xy}$, is not a metric but that $\\sqrt{1-\\rho_{xy}^2}$ is. We also\ngive generalizations of these results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 06:41:17 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Solo", "Victor", ""]]}, {"id": "1908.06090", "submitter": "Eric Nyarko Mr.", "authors": "Eric Nyarko", "title": "Optimal Paired Comparison Experiments for Second-Order Interactions", "comments": "arXiv admin note: text overlap with arXiv:1811.10973,\n  arXiv:1908.06092", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real life situations often paired comparisons involving alternatives of\neither full or partial profiles to mitigate cognitive burden are presented. For\nthis situation the problem of finding optimal designs is considered in the\npresence of second-order interactions when all attributes have general common\nnumber of levels.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 15:24:57 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Nyarko", "Eric", ""]]}, {"id": "1908.06092", "submitter": "Eric Nyarko Mr.", "authors": "Eric Nyarko", "title": "Optimal $2^K$ Paired Comparison Designs for Third-Order Interactions", "comments": "arXiv admin note: text overlap with arXiv:1811.10973,\n  arXiv:1908.06090", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In psychological research often paired comparisons are used in which either\nfull or partial profiles of the alternatives described by a common set of\ntwo-level attributes are presented. For this situation the problem of finding\noptimal designs is considered in the presence of third-order interactions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 16:19:59 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Nyarko", "Eric", ""]]}, {"id": "1908.06129", "submitter": "Sihai Zhao", "authors": "Sihai Dave Zhao", "title": "Simultaneous estimation of normal means with side information", "comments": "accepted by Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integrative analysis of multiple datasets is an important strategy in\ndata analysis. It is increasingly popular in genomics, which enjoys a wealth of\npublicly available datasets that can be compared, contrasted, and combined in\norder to extract novel scientific insights. This paper studies a stylized\nexample of data integration for a classical statistical problem: leveraging\nside information to estimate a vector of normal means. This task is formulated\nas a compound decision problem, an oracle integrative decision rule is derived,\nand a data-driven estimate of this rule based on minimizing an unbiased\nestimate of its risk is proposed. The data-driven rule is shown to\nasymptotically achieve the minimum possible risk among all separable decision\nrules, and it can outperform existing methods in numerical properties. The\nproposed procedure leads naturally to an integrative high-dimensional\nclassification procedure, which is illustrated by combining data from two\nindependent gene expression profiling studies.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 18:52:27 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 22:12:15 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Zhao", "Sihai Dave", ""]]}, {"id": "1908.06370", "submitter": "Omid Sedehi", "authors": "Omid Sedehi, Lambros S. Katafygiotis, Costas Papadimitriou", "title": "Hierarchical Bayesian Operational Modal Analysis: Theory and\n  Computations", "comments": null, "journal-ref": "Mechanical Systems and Signal Processing, Volume 140, June 2020,\n  106663", "doi": "10.1016/j.ymssp.2020.106663", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a hierarchical Bayesian modeling framework for the\nuncertainty quantification in modal identification of linear dynamical systems\nusing multiple vibration data sets. This novel framework integrates the\nstate-of-the-art Bayesian formulations into a hierarchical setting aiming to\ncapture both the identification precision and the ensemble variability prompted\ndue to modeling errors. Such cutting-edge developments have been absent from\nthe modal identification literature, sustained as a long-standing problem at\nthe research spotlight. Central to this framework is a Gaussian hyper\nprobability model, whose mean and covariance matrix are unknown encapsulating\nthe uncertainty of the modal parameters. Detailed computation of this\nhierarchical model is addressed under two major algorithms using Markov chain\nMonte Carlo (MCMC) sampling and Laplace asymptotic approximation methods. Since\nfor a small number of data sets the hyper covariance matrix is often\nunidentifiable, a practical remedy is suggested through the eigenbasis\ntransformation of the covariance matrix, which effectively reduces the number\nof unknown hyper-parameters. It is also proved that under some conditions the\nmaximum a posteriori (MAP) estimation of the hyper mean and covariance coincide\nwith the ensemble mean and covariance computed using the MAP estimations\ncorresponding to multiple data sets. This interesting finding addresses\nrelevant concerns related to the outcome of the mainstream Bayesian methods in\ncapturing the stochastic variability from dissimilar data sets. Finally, the\ndynamical response of a prototype structure tested on a shaking table subjected\nto Gaussian white noise base excitation and the ambient vibration measurement\nof a cable footbridge are employed to demonstrate the proposed framework.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 03:56:49 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 06:00:29 GMT"}, {"version": "v3", "created": "Sat, 28 Dec 2019 11:26:06 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sedehi", "Omid", ""], ["Katafygiotis", "Lambros S.", ""], ["Papadimitriou", "Costas", ""]]}, {"id": "1908.06400", "submitter": "Forhad Hossain", "authors": "Ummay Salma Shorna and Md. Forhad Hossain", "title": "A New Approach to Determine the Coefficient of Skewness and An\n  Alternative Form of Boxplot", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  To solve the problems in measuring coefficient of skewness related to extreme\nvalue, irregular distance from the middle point and distance between two\nconsecutive numbers, \"Rank skewness\" a new measure of the coefficient of\nskewness has been proposed in this paper. Comparing with other measures of the\ncoefficient of skewness, proposed measure of the coefficient of skewness\nperforms better specially for skewed distribution. An alternative of five point\nsummary boxplot, a four point summary graph has also been proposed which is\nsimpler than the traditional boxplot. It is based on all observation and give\nbetter result than the five point summary.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 08:53:33 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Shorna", "Ummay Salma", ""], ["Hossain", "Md. Forhad", ""]]}, {"id": "1908.06431", "submitter": "Guan'ao Yan", "authors": "Jun Zhao, Guan'ao Yan and Yi Zhang", "title": "Semiparametric Expectile Regression for High-dimensional Heavy-tailed\n  and Heterogeneous Data", "comments": "arXiv admin note: text overlap with arXiv:1601.06000 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, high-dimensional heterogeneous data have attracted a lot of\nattention and discussion. Under heterogeneity, semiparametric regression is a\npopular choice to model data in statistics. In this paper, we take advantages\nof expectile regression in computation and analysis of heterogeneity, and\npropose the regularized partially linear additive expectile regression with\nnonconvex penalty, for example, SCAD or MCP for such high-dimensional\nheterogeneous data. We focus on a more realistic scenario: the regression error\nis heavy-tailed distributed and only has finite moments, which is violated with\nthe classical sub-gaussian distribution assumption and more common in practise.\nUnder some regular conditions, we show that with probability tending to one,\nthe oracle estimator is one of the local minima of our optimization problem.\nThe theoretical study indicates that the dimension cardinality of linear\ncovariates our procedure can handle with is essentially restricted by the\nmoment condition of the regression error. For computation, since the\ncorresponding optimization problem is nonconvex and nonsmooth, we derive a\ntwo-step algorithm to solve this problem. Finally, we demonstrate that the\nproposed method enjoys good performances in estimation accuracy and model\nselection through Monto Carlo simulation studies and a real data example.\nWhat's more, by taking different expectile weights $\\alpha$, we are able to\ndetect heterogeneity and explore the entire conditional distribution of the\nresponse variable, which indicates the usefulness of our proposed method for\nanalyzing high dimensional heterogeneous data.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 12:27:32 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Zhao", "Jun", ""], ["Yan", "Guan'ao", ""], ["Zhang", "Yi", ""]]}, {"id": "1908.06437", "submitter": "Marcos Prates O", "authors": "Zaida C. Quiroz and Marcos O. Prates and Dipak K. Dey and H{\\aa}vard\n  Rue", "title": "Fast Bayesian inference of Block Nearest Neighbor Gaussian process for\n  large data", "comments": "60 pages, 20 figures (including the ones in the Supplementary\n  Material), 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the development of a spatial block-Nearest Neighbor\nGaussian process (block-NNGP) for location-referenced large spatial data. The\nkey idea behind this approach is to divide the spatial domain into several\nblocks which are dependent under some constraints. The cross-blocks capture the\nlarge-scale spatial dependence, while each block captures the small-scale\nspatial dependence. The resulting block-NNGP enjoys Markov properties reflected\non its sparse precision matrix. It is embedded as a prior within the class of\nlatent Gaussian models, thus Bayesian inference is obtained using the\nintegrated nested Laplace approximation (INLA). The performance of the\nblock-NNGP is illustrated on simulated examples and massive real data for\nlocations in the order of $10^4$.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 13:03:10 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 23:42:49 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2021 00:40:22 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Quiroz", "Zaida C.", ""], ["Prates", "Marcos O.", ""], ["Dey", "Dipak K.", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1908.06438", "submitter": "Angelo Mele", "authors": "Angelo Mele and Lingxin Hao and Joshua Cape and Carey E. Priebe", "title": "Spectral inference for large Stochastic Blockmodels with nodal\n  covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications of network analysis, it is important to distinguish\nbetween observed and unobserved factors affecting network structure. To this\nend, we develop spectral estimators for both unobserved blocks and the effect\nof covariates in stochastic blockmodels. On the theoretical side, we establish\nasymptotic normality of our estimators for the subsequent purpose of performing\ninference. On the applied side, we show that computing our estimator is much\nfaster than standard variational expectation--maximization algorithms and\nscales well for large networks. Monte Carlo experiments suggest that the\nestimator performs well under different data generating processes. Our\napplication to Facebook data shows evidence of homophily in gender, role and\ncampus-residence, while allowing us to discover unobserved communities. The\nresults in this paper provide a foundation for spectral estimation of the\neffect of observed covariates as well as unobserved latent community structure\non the probability of link formation in networks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 13:03:13 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 11:26:08 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Mele", "Angelo", ""], ["Hao", "Lingxin", ""], ["Cape", "Joshua", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1908.06486", "submitter": "Ronak Mehta", "authors": "Ronak Mehta, Jaewon Chung, Cencheng Shen, Ting Xu, Joshua T.\n  Vogelstein", "title": "Independence Testing for Multivariate Time Series", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex data structures such as time series are increasingly present in\nmodern data science problems. A fundamental question is whether two such\ntime-series are statistically dependent. Many current approaches make\nparametric assumptions on the random processes, only detect linear association,\nrequire multiple tests, or forfeit power in high-dimensional, nonlinear\nsettings. Estimating the distribution of any test statistic under the null is\nnon-trivial, as the permutation test is invalid. This work juxtaposes distance\ncorrelation (Dcorr) and multiscale graph correlation (MGC) from independence\ntesting literature and block permutation from time series analysis to address\nthese challenges. The proposed nonparametric procedure is valid and consistent,\nbuilding upon prior work by characterizing the geometry of the relationship,\nestimating the time lag at which dependence is maximized, avoiding the need for\nmultiple testing, and exhibiting superior power in high-dimensional, low sample\nsize, nonlinear settings. Neural connectivity is analyzed via fMRI data,\nrevealing linear dependence of signals within the visual network and default\nmode network, and nonlinear relationships in other networks. This work uncovers\na first-resort data analysis tool with open-source code available, directly\nimpacting a wide range of scientific disciplines.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 17:19:16 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 23:29:57 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 00:50:32 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Mehta", "Ronak", ""], ["Chung", "Jaewon", ""], ["Shen", "Cencheng", ""], ["Xu", "Ting", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1908.06514", "submitter": "Felipe Medina-Aguayo", "authors": "Felipe J Medina-Aguayo and Richard G Everitt", "title": "Revisiting the balance heuristic for estimating normalising constants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple importance sampling estimators are widely used for computing\nintractable constants due to its reliability and robustness. The celebrated\nbalance heuristic estimator belongs to this class of methods and has proved\nvery successful in computer graphics. The basic ingredients for computing the\nestimator are: a set of proposal distributions, indexed by some discrete label,\nand a predetermined number of draws from each of these proposals. However, if\nthe number of available proposals is much larger than the number of permitted\nimportance points, one needs to select, possibly at random, which of these\ndistributions will be used. The focus of this work lies within the previous\ncontext, exploring some improvements and variations of the balance heuristic\nvia a novel extended-space representation of the estimator, leading to\nstraightforward annealing schemes for variance reduction purposes. In addition,\nwe also look at the intractable scenario where the proposal density is only\navailable as a joint function with the discrete label, as may be encountered in\nproblems where an ordering is imposed. For this case, we look at combinations\nof correlated unbiased estimators which also fit into the extended-space\nrepresentation and, in turn, will provide other interesting solutions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 21:26:50 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 23:10:39 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Medina-Aguayo", "Felipe J", ""], ["Everitt", "Richard G", ""]]}, {"id": "1908.06597", "submitter": "Yuan Ke", "authors": "Wanjun Liu, Yuan Ke, Jingyuan Liu and Runze Li", "title": "Model-free Feature Screening and FDR Control with Knockoff Features", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2020.1783274", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a model-free and data-adaptive feature screening method\nfor ultra-high dimensional datasets. The proposed method is based on the\nprojection correlation which measures the dependence between two random\nvectors. This projection correlation based method does not require specifying a\nregression model and applies to the data in the presence of heavy-tailed errors\nand multivariate response. It enjoys both sure screening and rank consistency\nproperties under weak assumptions. Further, a two-step approach is proposed to\ncontrol the false discovery rate (FDR) in feature screening with the help of\nknockoff features. It can be shown that the proposed two-step approach enjoys\nboth sure screening and FDR control if the pre-specified FDR level $\\alpha$ is\ngreater or equal to $1/s$, where $s$ is the number of active features. The\nsuperior empirical performance of the proposed methods is justified by various\nnumerical experiments and real data applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 05:12:51 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 01:36:15 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 22:43:45 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Liu", "Wanjun", ""], ["Ke", "Yuan", ""], ["Liu", "Jingyuan", ""], ["Li", "Runze", ""]]}, {"id": "1908.06600", "submitter": "Deepak Nag Ayyala", "authors": "Deepak Nag Ayyala", "title": "High dimensional statistical inference: theoretical development to data\n  analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article is due to appear in the Handbook of Statistics, Vol. 43,\nElsevier/North-Holland, Amsterdam, edited by Arni S. R. Srinivasa Rao and C. R.\nRao.\n  In modern day analytics, there is ever growing need to develop statistical\nmodels to study high dimensional data. Between dimension reduction,\nasymptotics-driven methods and random projection based methods, there are\nseveral approaches developed so far. For high dimensional parametric models,\nestimation and hypothesis testing for mean and covariance matrices have been\nextensively studied. However, practical implementation of these methods are\nfairly limited and are primarily restricted to researchers involved in high\ndimensional inference. With several applied fields such as genomics,\nmetagenomics and social networking, high dimensional inference is a key\ncomponent of big data analytics. In this chapter, a comprehensive overview of\nhigh dimensional inference and its applications in data analytics is provided.\nKey theoretical developments and computational tools are presented, giving\nreaders an in-depth understanding of challenges in big data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 05:42:31 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Ayyala", "Deepak Nag", ""]]}, {"id": "1908.06602", "submitter": "Mar\\'ia Fernanda Gil-Leyva Villa", "authors": "Mar\\'ia F. Gil-Leyva, Rams\\'es H. Mena, Theodoros Nicoleris", "title": "Beta-Binomial stick-breaking non-parametric prior", "comments": "26 pages, 10 figures", "journal-ref": "Electron. J. Statist., Volume 14, Number 1 (2020), 1479-1507", "doi": "10.1214/20-EJS1694", "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of nonparametric prior distributions, termed Beta-Binomial\nstick-breaking process, is proposed. By allowing the underlying length random\nvariables to be dependent through a Beta marginals Markov chain, an appealing\ndiscrete random probability measure arises. The chain's dependence parameter\ncontrols the ordering of the stick-breaking weights, and thus tunes the model's\nlabel-switching ability. Also, by tuning this parameter, the resulting class\ncontains the Dirichlet process and the Geometric process priors as particular\ncases, which is of interest for fast convergence of MCMC implementations. Some\nproperties of the model are discussed and a density estimation algorithm is\nproposed and tested with simulated datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 05:48:11 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 01:42:28 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Gil-Leyva", "Mar\u00eda F.", ""], ["Mena", "Rams\u00e9s H.", ""], ["Nicoleris", "Theodoros", ""]]}, {"id": "1908.06622", "submitter": "Michael Bertolacci", "authors": "Michael Bertolacci, Ori Rosen, Edward Cripps, Sally Cripps", "title": "AdaptSPEC-X: Covariate Dependent Spectral Modeling of Multiple\n  Nonstationary Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for the joint analysis of a panel of possibly\nnonstationary time series. The approach is Bayesian and uses a\ncovariate-dependent infinite mixture model to incorporate multiple time series,\nwith mixture components parameterized by a time varying mean and log spectrum.\nThe mixture components are based on AdaptSPEC, a nonparametric model which\nadaptively divides the time series into an unknown number of segments and\nestimates the local log spectra by smoothing splines. We extend AdaptSPEC to\nhandle missing values, a common feature of time series which can cause\ndifficulties for nonparametric spectral methods. A second extension is to allow\nfor a time varying mean. Covariates, assumed to be time-independent, are\nincorporated via the mixture weights using the logistic stick breaking process.\nThe model can estimate time varying means and spectra at observed and\nunobserved covariate values, allowing for predictive inference. Estimation is\nperformed by Markov chain Monte Carlo (MCMC) methods, combining data\naugmentation, reversible jump, and Riemann manifold Hamiltonian Monte Carlo\ntechniques. We evaluate the methodology using simulated data, and describe\napplications to Australian rainfall data and measles incidence in the US.\nSoftware implementing the method proposed in this paper is available in the R\npackage BayesSpec.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 07:27:15 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 04:13:44 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Bertolacci", "Michael", ""], ["Rosen", "Ori", ""], ["Cripps", "Edward", ""], ["Cripps", "Sally", ""]]}, {"id": "1908.06716", "submitter": "Yuxiang Gao", "authors": "Yuxiang Gao, Lauren Kennedy, Daniel Simpson, Andrew Gelman", "title": "Improving multilevel regression and poststratification with structured\n  priors", "comments": "Minor revision. Added plots showing share of simulations where\n  structured priors outperformed baseline priors in MRP", "journal-ref": "Bayesian Analysis, Advance publication (2020), 26 pages", "doi": "10.1214/20-BA1223", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central theme in the field of survey statistics is estimating\npopulation-level quantities through data coming from potentially\nnon-representative samples of the population. Multilevel Regression and\nPoststratification (MRP), a model-based approach, is gaining traction against\nthe traditional weighted approach for survey estimates. MRP estimates are\nsusceptible to bias if there is an underlying structure that the methodology\ndoes not capture. This work aims to provide a new framework for specifying\nstructured prior distributions that lead to bias reduction in MRP estimates. We\nuse simulation studies to explore the benefit of these prior distributions and\ndemonstrate their efficacy on non-representative US survey data. We show that\nstructured prior distributions offer absolute bias reduction and variance\nreduction for posterior MRP estimates in a large variety of data regimes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 11:56:06 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 16:11:56 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2020 19:45:10 GMT"}, {"version": "v4", "created": "Sat, 23 May 2020 22:08:59 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Gao", "Yuxiang", ""], ["Kennedy", "Lauren", ""], ["Simpson", "Daniel", ""], ["Gelman", "Andrew", ""]]}, {"id": "1908.06733", "submitter": "Christian R\\\"over", "authors": "Moreno Ursino, Christian R\\\"over, Sarah Zohar and Tim Friede", "title": "Random-effects meta-analysis of phase I dose-finding studies using\n  stochastic process priors", "comments": "23 pages, 6 figures, 7 tables", "journal-ref": "The Annals of Applied Statistics, 15(1):174-193, 2021", "doi": "10.1214/20-AOAS1390", "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase I dose-finding studies aim at identifying the maximal tolerated dose\n(MTD). It is not uncommon that several dose-finding studies are conducted,\nalthough often with some variation in the administration mode or dose panel.\nFor instance, sorafenib (BAY 43-900) was used as monotherapy in at least 29\nphase I trials according to a recent search in clinicaltrials.gov. Since the\ntoxicity may not be directly related to the specific indication, synthesizing\nthe information from several studies might be worthwhile. However, this is\nrarely done in practice and only a fixed-effect meta-analysis framework was\nproposed to date. We developed a Bayesian random-effects meta-analysis\nmethodology to pool several phase I trials and suggest the MTD. A curve free\nhierarchical model on the logistic scale with random effects, accounting for\nbetween-trial heterogeneity, is used to model the probability of toxicity\nacross the investigated doses. An Ornstein-Uhlenbeck Gaussian process is\nadopted for the random effects structure. Prior distributions for the curve\nfree model are based on a latent Gamma process. An extensive simulation study\nshowed good performance of the proposed method also under model deviations.\nSharing information between phase I studies can improve the precision of MTD\nselection, at least when the number of trials is reasonably large.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 07:16:40 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ursino", "Moreno", ""], ["R\u00f6ver", "Christian", ""], ["Zohar", "Sarah", ""], ["Friede", "Tim", ""]]}, {"id": "1908.06772", "submitter": "Genya Kobayashi Mr.", "authors": "Genya Kobayashi, Yuta Yamauchi, Kazuhiko Kakamu, Yuki Kawakubo,\n  Shonosuke Sugasawa", "title": "Bayesian approach to Lorenz curve using time series grouped data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is concerned with estimating the inequality measures associated\nwith the underlying hypothetical income distribution from the times series\ngrouped data on the Lorenz curve. We adopt the Dirichlet pseudo likelihood\napproach where the parameters of the Dirichlet likelihood are set to the\ndifferences between the Lorenz curve of the hypothetical income distribution\nfor the consecutive income classes and propose a state space model which\ncombines the transformed parameters of the Lorenz curve through a time series\nstructure. Furthermore, the information on the sample size in each survey is\nintroduced into the originally nuisance Dirichlet precision parameter to take\ninto account the variability from the sampling. From the simulated data and\nreal data on the Japanese monthly income survey, it is confirmed that the\nproposed model produces more efficient estimates on the inequality measures\nthan the existing models without time series structures.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 12:55:55 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Kobayashi", "Genya", ""], ["Yamauchi", "Yuta", ""], ["Kakamu", "Kazuhiko", ""], ["Kawakubo", "Yuki", ""], ["Sugasawa", "Shonosuke", ""]]}, {"id": "1908.06835", "submitter": "Fabrizio Laurini", "authors": "Fabrizio Laurini, Paul Fearnhead, Jonathan A. Tawn", "title": "Evaluation of extremal properties of GARCH(p,q) processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized autoregressive conditionally heteroskedastic (GARCH) processes\nare widely used for modelling features commonly found in observed financial\nreturns. The extremal properties of these processes are of considerable\ninterest for market risk management. For the simplest GARCH(p,q) process, with\nmax(p,q) = 1, all extremal features have been fully characterised. Although the\nmarginal features of extreme values of the process have been theoretically\ncharacterised when max(p, q) >= 2, much remains to be found about both marginal\nand dependence structure during extreme excursions. Specifically, a reliable\nmethod is required for evaluating the tail index, which regulates the marginal\ntail behaviour and there is a need for methods and algorithms for determining\nclustering. In particular, for the latter, the mean number of extreme values in\na short-term cluster, i.e., the reciprocal of the extremal index, has only been\ncharacterised in special cases which exclude all GARCH(p,q) processes that are\nused in practice. Although recent research has identified the multivariate\nregular variation property of stationary GARCH(p,q) processes, currently there\nare no reliable methods for numerically evaluating key components of these\ncharacterisations. We overcome these issues and are able to generate the\nforward tail chain of the process to derive the extremal index and a range of\nother cluster functionals for all GARCH(p, q) processes including integrated\nGARCH processes and processes with unbounded and asymmetric innovations. The\nnew theory and methods we present extend to assessing the strict stationarity\nand extremal properties for a much broader class of stochastic recurrence\nequations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 14:38:23 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Laurini", "Fabrizio", ""], ["Fearnhead", "Paul", ""], ["Tawn", "Jonathan A.", ""]]}, {"id": "1908.06892", "submitter": "Yongli Sang", "authors": "Yongli Sang and Xin Dang", "title": "Empirical Likelihood Test for Diagonal Symmetry", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy distance is a statistical distance between the distributions of random\nvariables, which characterizes the equality of the distributions. Utilizing the\nenergy distance, we develop a nonparametric test for the diagonal symmetry,\nwhich is consistent against any fixed alternatives. The test statistic\ndeveloped in this paper is based on the difference of two $U$-statistics. By\napplying the jackknife empirical likelihood approach, the standard limiting\nchi-square distribution with degree freedom of one is established and is used\nto determine critical value and $p$-value of the test. Simulation studies show\nthat our method is competitive in terms of empirical sizes and empirical\npowers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 15:51:45 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Sang", "Yongli", ""], ["Dang", "Xin", ""]]}, {"id": "1908.06940", "submitter": "Makan Arastuie", "authors": "Makan Arastuie, Subhadeep Paul, Kevin S. Xu", "title": "CHIP: A Hawkes Process Model for Continuous-time Networks with Scalable\n  and Consistent Estimation", "comments": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020), Vancouver, Canada. Source code is available at\n  https://github.com/IdeasLabUT/CHIP-Network-Model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many application settings involving networks, such as messages between\nusers of an on-line social network or transactions between traders in financial\nmarkets, the observed data consist of timestamped relational events, which form\na continuous-time network. We propose the Community Hawkes Independent Pairs\n(CHIP) generative model for such networks. We show that applying spectral\nclustering to an aggregated adjacency matrix constructed from the CHIP model\nprovides consistent community detection for a growing number of nodes and time\nduration. We also develop consistent and computationally efficient estimators\nfor the model parameters. We demonstrate that our proposed CHIP model and\nestimation procedure scales to large networks with tens of thousands of nodes\nand provides superior fits than existing continuous-time network models on\nseveral real networks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 17:24:58 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 00:37:44 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 06:19:42 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Arastuie", "Makan", ""], ["Paul", "Subhadeep", ""], ["Xu", "Kevin S.", ""]]}, {"id": "1908.07176", "submitter": "Michael Newton", "authors": "TIen Vo, Vamsi Ithapu, Vikas Singh, Michael A. Newton", "title": "Dimension constraints improve hypothesis testing for large-scale,\n  graph-associated, brain-image data", "comments": "8 figures; 26 pages sans supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large-scale testing with graph-associated data, we present an empirical\nBayes mixture technique to score local false discovery rates. Compared to\nempirical Bayes procedures that ignore the graph, the proposed method gains\npower in settings where non-null cases form connected subgraphs, and it does so\nby regularizing parameter contrasts between testing units. Simulations show\nthat GraphMM controls the false discovery rate in a variety of settings. On\nmagnetic resonance imaging data from a study of brain changes associated with\nthe onset of Alzheimer's disease, GraphMM produces substantially greater yield\nthan conventional large-scale testing procedures.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 06:05:18 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 03:57:26 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Vo", "TIen", ""], ["Ithapu", "Vamsi", ""], ["Singh", "Vikas", ""], ["Newton", "Michael A.", ""]]}, {"id": "1908.07193", "submitter": "Nicolo Colombo", "authors": "Nicolo Colombo, Ricardo Silva, Soong M Kang, Arthur Gretton", "title": "Counterfactual Distribution Regression for Structured Inference", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider problems in which a system receives external \\emph{perturbations}\nfrom time to time. For instance, the system can be a train network in which\nparticular lines are repeatedly disrupted without warning, having an effect on\npassenger behavior. The goal is to predict changes in the behavior of the\nsystem at particular points of interest, such as passenger traffic around\nstations at the affected rails. We assume that the data available provides\nrecords of the system functioning at its \"natural regime\" (e.g., the train\nnetwork without disruptions) and data on cases where perturbations took place.\nThe inference problem is how information concerning perturbations, with\nparticular covariates such as location and time, can be generalized to predict\nthe effect of novel perturbations. We approach this problem from the point of\nview of a mapping from the counterfactual distribution of the system behavior\nwithout disruptions to the distribution of the disrupted system. A variant on\n\\emph{distribution regression} is developed for this setup.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 07:13:01 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Colombo", "Nicolo", ""], ["Silva", "Ricardo", ""], ["Kang", "Soong M", ""], ["Gretton", "Arthur", ""]]}, {"id": "1908.07204", "submitter": "Gael Martin Prof", "authors": "Patrick Leung, Catherine S. Forbes, Gael M. Martin and Brendan McCabe", "title": "Forecasting observables with particle filters: Any filter will do!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the impact of filter choice on forecast accuracy in state\nspace models. The filters are used both to estimate the posterior distribution\nof the parameters, via a particle marginal Metropolis-Hastings (PMMH)\nalgorithm, and to produce draws from the filtered distribution of the final\nstate. Multiple filters are entertained, including two new data-driven methods.\nSimulation exercises are used to document the performance of each PMMH\nalgorithm, in terms of computation time and the efficiency of the chain. We\nthen produce the forecast distributions for the one-step-ahead value of the\nobserved variable, using a fixed number of particles and Markov chain draws.\nDespite distinct differences in efficiency, the filters yield virtually\nidentical forecasting accuracy, with this result holding under both correct and\nincorrect specification of the model. This invariance of forecast performance\nto the specification of the filter also characterizes an empirical analysis of\nS&P500 daily returns.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 07:51:57 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Leung", "Patrick", ""], ["Forbes", "Catherine S.", ""], ["Martin", "Gael M.", ""], ["McCabe", "Brendan", ""]]}, {"id": "1908.07254", "submitter": "Sylvain Le Corff", "authors": "Pierre Gloaguen (MIA-Paris), Sylvain Le Corff (IP Paris, CITI,\n  TIPIC-SAMOVAR), Jimmy Olsson (KTH Royal Institute of Technology)", "title": "A pseudo-marginal sequential Monte Carlo online smoothing algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online computation of expectations of additive state functionals\nunder general path probability measures proportional to products of\nunnormalised transition densities. These transition densities are assumed to be\nintractable but possible to estimate, with or without bias. Using\npseudo-marginalisation techniques we are able to extend the particle-based,\nrapid incremental smoother (PaRIS) algorithm proposed in [J.Olsson and\nJ.Westerborn. Efficient particle-based online smoothing in general hidden\nMarkov models: The PaRIS algorithm. Bernoulli, 23(3):1951--1996, 2017] to this\nsetting. The resulting algorithm, which has a linear complexity in the number\nof particles and constant memory requirements, applies to a wide range of\nchallenging path-space Monte Carlo problems, including smoothing in partially\nobserved diffusion processes and models with intractable likelihood. The\nalgorithm is furnished with several theoretical results, including a central\nlimit theorem, establishing its convergence and numerical stability. Moreover,\nunder strong mixing assumptions we establish a novel $O(n \\varepsilon)$ bound\non the asymptotic bias of the algorithm, where $n$ is the path length and\n$\\varepsilon$ controls the bias of the density estimators.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 09:53:17 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 08:28:46 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Gloaguen", "Pierre", "", "MIA-Paris"], ["Corff", "Sylvain Le", "", "IP Paris, CITI,\n  TIPIC-SAMOVAR"], ["Olsson", "Jimmy", "", "KTH Royal Institute of Technology"]]}, {"id": "1908.07352", "submitter": "Colin Fogarty", "authors": "Colin B. Fogarty", "title": "Testing weak nulls in matched observational studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop sensitivity analyses for weak nulls in matched observational\nstudies while allowing unit-level treatment effects to vary. The methods may be\napplied to studies using any optimal without-replacement matching algorithm. In\ncontrast to randomized experiments and to paired observational studies, we show\nfor general matched designs that over a large class of test statistics, any\nvalid sensitivity analysis for the entirety of the weak null must be\nunnecessarily conservative if Fisher's sharp null of no treatment effect for\nany individual also holds. We present a sensitivity analysis valid for the weak\nnull, and illustrate why it is generally conservative if the sharp null holds\nthrough new connections to inverse probability weighted estimators. An\nalternative procedure is presented that is asymptotically sharp if treatment\neffects are constant, and that is valid for the weak null under additional\nrestrictions which may be deemed benign by practitioners. Simulations\ndemonstrate that this alternative procedure results in a valid sensitivity\nanalysis for the weak null hypothesis under a host of reasonable\ndata-generating processes. The procedures allow practitioners to assess\nrobustness of estimated sample average treatment effects to hidden bias while\nallowing for unspecified effect heterogeneity in matched observational studies.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 13:57:03 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 21:48:56 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2020 00:07:10 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Fogarty", "Colin B.", ""]]}, {"id": "1908.07390", "submitter": "Rui Portocarrero Sarmento MSc", "authors": "Rui Portocarrero Sarmento and Vera Costa", "title": "An Overview of Statistical Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of statistical software in academia and enterprises has been evolving\nover the last years. More often than not, students, professors, workers, and\nusers, in general, have all had, at some point, exposure to statistical\nsoftware. Sometimes, difficulties are felt when dealing with such type of\nsoftware. Very few persons have theoretical knowledge to clearly understand\nsoftware configurations or settings, and sometimes even the presented results.\nVery often, the users are required by academies or enterprises to present\nreports, without the time to explore or understand the results or tasks\nrequired to do an optimal preparation of data or software settings. In this\nwork, we present a statistical overview of some theoretical concepts, to\nprovide fast access to some concepts.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 09:54:50 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Sarmento", "Rui Portocarrero", ""], ["Costa", "Vera", ""]]}, {"id": "1908.07477", "submitter": "Catherine Trottier", "authors": "Jocelyn Chauvet (IMAG), Catherine Trottier (IMAG, UM3), Xavier Bry\n  (IMAG)", "title": "Regularising Generalised Linear Mixed Models with an autoregressive\n  random effect", "comments": null, "journal-ref": "IWSM 2017, 32nd International Workshop on Statistical Modelling,\n  Jul 2017, Groningen, Netherlands", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address regularised versions of the Expectation-Maximisation (EM)\nalgorithm for Generalised Linear Mixed Models (GLMM) in the context of panel\ndata (measured on several individuals at different time-points). A random\nresponse y is modelled by a GLMM, using a set X of explanatory variables and\ntwo random effects. The first one introduces the dependence within individuals\non which data is repeatedly collected while the second one embodies the\nserially correlated time-specific effect shared by all the individuals.\nVariables in X are assumed many and redundant, so that regression demands\nregularisation. In this context, we first propose a L2-penalised EM algorithm,\nand then a supervised component-based regularised EM algorithm as an\nalternative.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 06:27:43 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Chauvet", "Jocelyn", "", "IMAG"], ["Trottier", "Catherine", "", "IMAG, UM3"], ["Bry", "Xavier", "", "IMAG"]]}, {"id": "1908.07478", "submitter": "Catherine Trottier", "authors": "Jocelyn Chauvet (IMAG), Catherine Trottier (IMAG, UM3), Xavier Bry\n  (IMAG)", "title": "R\\'egularisation dans les Mod\\`eles Lin\\'eaires G\\'en\\'eralis\\'es Mixtes\n  avec effet al\\'eatoire autor\\'egressif", "comments": "in French. JdS 2017, 49{\\`e}mes Journ{\\'e}es de Statistique de la\n  SFdS, May 2017, Avignon, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address regularised versions of the Expectation-Maximisation (EM)\nalgorithm for Generalised Linear Mixed Models (GLMM) in the context of panel\ndata (measured on several individuals at different time points). A random\nresponse y is modelled by a GLMM, using a set X of explanatory variables and\ntwo random effects. The first effect introduces the dependence within\nindividuals on which data is repeatedly collected while the second embodies the\nserially correlated time-specific effect shared by all the individuals.\nVariables in X are assumed many and redundant, so that regression demands\nregularisation. In this context, we first propose a L2-penalised EM algorithm\nfor low-dimensional data, and then a supervised component-based regularised EM\nalgorithm for the high-dimensional case.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 06:22:53 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Chauvet", "Jocelyn", "", "IMAG"], ["Trottier", "Catherine", "", "IMAG, UM3"], ["Bry", "Xavier", "", "IMAG"]]}, {"id": "1908.07639", "submitter": "Jingchen Hu", "authors": "Jingchen Hu, Terrance D. Savitsky, Matthew R. Williams", "title": "Risk-Efficient Bayesian Data Synthesis for Privacy Protection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical agencies utilize models to synthesize respondent-level data for\nrelease to the public for privacy protection. In this work, we efficiently\ninduce privacy protection into any Bayesian synthesis model by employing a\npseudo likelihood that exponentiates each likelihood contribution by an\nobservation record-indexed weight in [0, 1], defined to be inversely\nproportional to the identification risk for that record. We start with the\nmarginal probability of identification risk for a record, which is composed as\nthe probability that the identity of the record may be disclosed. Our\napplication to the Consumer Expenditure Surveys (CE) of the U.S. Bureau of\nLabor Statistics demonstrates that the marginally risk-adjusted synthesizer\nprovides an overall improved privacy protection; however, the identification\nrisks actually increase for some moderate-risk records after risk-adjusted\npseudo posterior estimation synthesis due to increased isolation after\nweighting; a phenomenon we label \"whack-a-mole\". We proceed to construct a\nweight for each record from a collection of pairwise identification risk\nprobabilities with other records, where each pairwise probability measures the\njoint probability of re-identification of the pair of records, which mitigates\nthe whack-a-mole issue and produces a more efficient set of synthetic data with\nlower risk and higher utility for the CE data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 22:42:40 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 21:31:33 GMT"}, {"version": "v3", "created": "Sun, 10 May 2020 16:02:40 GMT"}, {"version": "v4", "created": "Wed, 20 May 2020 18:01:55 GMT"}, {"version": "v5", "created": "Mon, 2 Nov 2020 20:33:23 GMT"}, {"version": "v6", "created": "Mon, 8 Feb 2021 23:03:48 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Hu", "Jingchen", ""], ["Savitsky", "Terrance D.", ""], ["Williams", "Matthew R.", ""]]}, {"id": "1908.07659", "submitter": "Spiridon  Penev", "authors": "Spiridon Penev, Pavel Shevchenko and Wei Wu", "title": "Myopic robust index tracking with Bregman divergence", "comments": "To be published in Quantitative Finance", "journal-ref": null, "doi": "10.1080/14697688.2021.1950918", "report-no": null, "categories": "q-fin.PM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Index tracking is a popular form of asset management. Typically, a quadratic\nfunction is used to define the tracking error of a portfolio and the look back\napproach is applied to solve the index tracking problem. We argue that a\nforward looking approach is more suitable, whereby the tracking error is\nexpressed as expectation of a function of the difference between the returns of\nthe index and of the portfolio. We also assume that there is an uncertainty in\nthe distribution of the assets, hence a robust version of the optimization\nproblem needs to be adopted. We use Bregman divergence in describing the\ndeviation between the nominal and actual distribution of the components of the\nindex. In this scenario, we derive the optimal robust index tracking strategy\nin a semi-analytical form as a solution of a system of nonlinear equations.\nSeveral numerical results are presented that allow us to compare the\nperformance of this robust strategy with the optimal non-robust strategy. We\nshow that, especially during market downturns, the robust strategy can be very\nadvantageous.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 00:35:06 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 13:47:15 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Penev", "Spiridon", ""], ["Shevchenko", "Pavel", ""], ["Wu", "Wei", ""]]}, {"id": "1908.07769", "submitter": "Duncan Wilson", "authors": "Duncan T. Wilson and Rebecca E. A. Walwyn and Richard Hooper and Julia\n  Brown and Amanda J. Farrin", "title": "Efficient and flexible simulation-based sample size determination for\n  clinical trials with multiple design parameters", "comments": null, "journal-ref": null, "doi": "10.1177/0962280220975790", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulation offers a simple and flexible way to estimate the power of a\nclinical trial when analytic formulae are not available. The computational\nburden of using simulation has, however, restricted its application to only the\nsimplest of sample size determination problems, minimising a single parameter\n(the overall sample size) subject to power being above a target level. We\ndescribe a general framework for solving simulation-based sample size\ndetermination problems with several design parameters over which to optimise\nand several conflicting criteria to be minimised. The method is based on an\nestablished global optimisation algorithm widely used in the design and\nanalysis of computer experiments, using a non-parametric regression model as an\napproximation of the true underlying power function. The method is flexible,\ncan be used for almost any problem for which power can be estimated using\nsimulation, and can be implemented using existing statistical software\npackages. We illustrate its application to three increasingly complicated\nsample size determination problems involving complex clustering structures,\nco-primary endpoints, and small sample considerations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 09:43:57 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Wilson", "Duncan T.", ""], ["Walwyn", "Rebecca E. A.", ""], ["Hooper", "Richard", ""], ["Brown", "Julia", ""], ["Farrin", "Amanda J.", ""]]}, {"id": "1908.07798", "submitter": "Tore Selland Kleppe", "authors": "Tore Selland Kleppe, Roman Liesenfeld, Guilherme Valle Moura and Atle\n  Oglend", "title": "Analyzing Commodity Futures Using Factor State-Space Models with Wishart\n  Stochastic Volatility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a factor state-space approach with stochastic volatility to model\nand forecast the term structure of future contracts on commodities. Our\napproach builds upon the dynamic 3-factor Nelson-Siegel model and its 4-factor\nSvensson extension and assumes for the latent level, slope and curvature\nfactors a Gaussian vector autoregression with a multivariate Wishart stochastic\nvolatility process. Exploiting the conjugacy of the Wishart and the Gaussian\ndistribution, we develop a computationally fast and easy to implement MCMC\nalgorithm for the Bayesian posterior analysis. An empirical application to\ndaily prices for contracts on crude oil with stipulated delivery dates ranging\nfrom one to 24 months ahead show that the estimated 4-factor Svensson model\nwith two curvature factors provides a good parsimonious representation of the\nserial correlation in the individual prices and their volatility. It also shows\nthat this model has a good out-of-sample forecast performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 11:15:28 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Kleppe", "Tore Selland", ""], ["Liesenfeld", "Roman", ""], ["Moura", "Guilherme Valle", ""], ["Oglend", "Atle", ""]]}, {"id": "1908.07869", "submitter": "Konstantinos Perrakis", "authors": "Konstantinos Perrakis, Thomas Lartigue, Frank Dondelinger and Sach\n  Mukherjee", "title": "Regularized joint mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized regression models, such as the lasso and variants, are well\nstudied and, under appropriate conditions, offer fast and statistically\ninterpretable results. However, large data in many applications are\nheterogeneous in the sense of harboring distributional differences between\nlatent groups. Then, the assumption that the conditional distribution of\nresponse Y given features X is the same for all samples may not hold (even\napproximately). Furthermore, in scientific applications, the covariance\nstructure of the features may contain important signals and its learning is\nalso affected by latent group structure. We propose a class of regularized\nmixture models for paired data of the form (X,Y) that couples together the\ndistribution of X (modeled using sparse graphical models) and the conditional Y\n| X (modeled using sparse regression). Both the regression and graphical models\nare specific to the latent groups and model parameters are estimated jointly\n(hence we call the approach \"regularized joint mixtures\"). This allows signals\nin either or both of the feature distribution and regression model to inform\nlearning of latent structure and provides automatic control of confounding by\nsuch structure. Estimation is handled via an expectation-maximization\nalgorithm, whose convergence is established theoretically. We illustrate the\nkey ideas via empirical examples.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 13:41:02 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 14:53:10 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 13:04:12 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Perrakis", "Konstantinos", ""], ["Lartigue", "Thomas", ""], ["Dondelinger", "Frank", ""], ["Mukherjee", "Sach", ""]]}, {"id": "1908.07908", "submitter": "Catherine Trottier", "authors": "Jocelyn Chauvet (IMAG), Catherine Trottier (IMAG, UM3), Xavier Bry\n  (IMAG), Frederic Mortier (UPR 105 BSEF)", "title": "Extension to mixed models of the Supervised Component-based Generalised\n  Linear Regression", "comments": "arXiv admin note: text overlap with arXiv:1908.04020", "journal-ref": "COMPSTAT 2016, 22nd International Conference on Computational\n  Statistics, Aug 2016, Oviedo, Spain", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the component-based regularisation of a multivariate Generalized\nLinear Mixed Model (GLMM). A set of random responses Y is modelled by a GLMM,\nusing a set X of explanatory variables, a set T of additional covariates, and\nrandom effects used to introduce the dependence between statistical units.\nVariables in X are assumed many and redundant, so that regression demands\nregularisation. By contrast, variables in T are assumed few and selected so as\nto require no regularisation. Regularisation is performed building an\nappropriate number of orthogonal components that both contribute to model Y and\ncapture relevant structural information in X. To estimate the model, we propose\nto maximise a criterion specific to the Supervised Component-based Generalised\nLinear Regression (SCGLR) within an adaptation of Schall's algorithm. This\nextension of SCGLR is tested on both simulated and real data, and compared to\nRidge- and Lasso-based regularisations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 06:25:24 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Chauvet", "Jocelyn", "", "IMAG"], ["Trottier", "Catherine", "", "IMAG, UM3"], ["Bry", "Xavier", "", "IMAG"], ["Mortier", "Frederic", "", "UPR 105 BSEF"]]}, {"id": "1908.07963", "submitter": "Keefe Murphy", "authors": "Keefe Murphy, Thomas Brendan Murphy, Raffaella Piccarreta, Isobel\n  Claire Gormley", "title": "Clustering Longitudinal Life-Course Sequences Using Mixtures of\n  Exponential-Distance Models", "comments": "Published in Journal of the Royal Statistical Society: Series A\n  (Statistics in Society)", "journal-ref": null, "doi": "10.1111/rssa.12712", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence analysis is an increasingly popular approach for analysing life\ncourses represented by ordered collections of activities experienced by\nsubjects over time. Here, we analyse a survey data set containing information\non the career trajectories of a cohort of Northern Irish youths tracked between\nthe ages of 16 and 22. We propose a novel, model-based clustering approach\nsuited to the analysis of such data from a holistic perspective, with the aims\nof estimating the number of typical career trajectories, identifying the\nrelevant features of these patterns, and assessing the extent to which such\npatterns are shaped by background characteristics.\n  Several criteria exist for measuring pairwise dissimilarities among\ncategorical sequences. Typically, dissimilarity matrices are employed as input\nto heuristic clustering algorithms. The family of methods we develop instead\nclusters sequences directly using mixtures of exponential-distance models.\nBasing the models on weighted variants of the Hamming distance metric permits\nclosed-form expressions for parameter estimation. Simultaneously allowing the\ncomponent membership probabilities to depend on fixed covariates and\naccommodating sampling weights in the clustering process yields new insights on\nthe Northern Irish data. In particular, we find that school examination\nperformance is the single most important predictor of cluster membership.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 16:15:43 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 14:34:10 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 17:19:18 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Murphy", "Keefe", ""], ["Murphy", "Thomas Brendan", ""], ["Piccarreta", "Raffaella", ""], ["Gormley", "Isobel Claire", ""]]}, {"id": "1908.07979", "submitter": "Baolin Wu", "authors": "Yun Bai, Zengri Wang, Theodore Lystig, Baolin Wu", "title": "Efficient and powerful equivalency test on combined mean and variance\n  with application to diagnostic device comparison studies", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical device comparison studies, equivalency test is commonly used to\ndemonstrate two measurement methods agree up to a pre-specified performance\ngoal based on the paired repeated measures. Such equivalency test often\ninvolves controlling the absolute differences that depend on both the mean and\nvariance parameters, and poses some challenges for statistical analysis. For\nexample, for the oximetry comparison study that motivates our research, FDA has\nclear guidelines approving an investigational pulse oximeter in comparison to a\nstandard oximeter via testing the root mean squares (RMS), a composite measure\nof both mean and variance parameters. For the hypothesis testing of this\ncomposite measure, existing methods have been either exploratory or relying on\nthe large-sample normal approximation with conservative and unsatisfactory\nperformance. We develop a novel generalized pivotal test to rigorously and\naccurately test the system equivalency based on RMS. The proposed method has\nwell-controlled type I error and favorable performance in our extensive\nnumerical studies. When analyzing data from an oximetry comparison study,\naiming to demonstrate performance equivalency between an FDA-cleared oximetry\nsystem and an investigational system, our proposed method resulted in a highly\nsignificant test result strongly supporting the system equivalency. We also\nprovide efficient R programs for the proposed method in a publicly available R\npackage. Considering that many practical equivalency studies of diagnostic\ndevices are of small to medium sizes, our proposed method and software timely\nbridge an existing gap in the field.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 16:38:52 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Bai", "Yun", ""], ["Wang", "Zengri", ""], ["Lystig", "Theodore", ""], ["Wu", "Baolin", ""]]}, {"id": "1908.08093", "submitter": "Yongli Han", "authors": "Yongli Han, Paul S. Albert, Christine D. Berg, Nicolas Wentzensen,\n  Hormuzd A. Katki, Danping Liu", "title": "Statistical approaches using longitudinal biomarkers for disease early\n  detection: A comparison of methodologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of clinical outcomes such as cancer may be predicted based on\nlongitudinal biomarker measurements. Tracking longitudinal biomarkers as a way\nto identify early disease onset may help to reduce mortality from diseases like\novarian cancer that are more treatable if detected early. Two general\nframeworks for disease risk prediction, the shared random effects model (SREM)\nand the pattern mixture model (PMM) could be used to assess longitudinal\nbiomarkers on disease early detection. In this paper, we studied the predictive\nperformances of SREM and PMM on disease early detection through an application\nto ovarian cancer, where early detection using the risk of ovarian cancer\nalgorithm (ROCA) has been evaluated. Comparisons of the above three methods\nwere performed via the analyses of the ovarian cancer data from the Prostate,\nLung, Colorectal, and Ovarian (PLCO) Cancer Screening Trial and extensive\nsimulation studies. The time-dependent receiving operating characteristic (ROC)\ncurve and its area (AUC) were used to evaluate the prediction accuracy. The\nout-of-sample predictive performance was calculated using leave-one-out\ncross-validation (LOOCV), aiming to minimize the problem of model over-fitting.\nA careful analysis of the use of the biomarker cancer antigen 125 for ovarian\ncancer early detection showed improved performance of PMM as compared with SREM\nand ROCA. More generally, simulation studies showed that PMM outperforms ROCA\nunless biomarkers are taken at very frequent screening settings.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 19:30:00 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Han", "Yongli", ""], ["Albert", "Paul S.", ""], ["Berg", "Christine D.", ""], ["Wentzensen", "Nicolas", ""], ["Katki", "Hormuzd A.", ""], ["Liu", "Danping", ""]]}, {"id": "1908.08095", "submitter": "Yuting Ye", "authors": "Yuting Ye, Yin Xia, Lexin Li", "title": "Paired Test of Matrix Graphs and Brain Connectivity Analysis", "comments": "30 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring brain connectivity network and quantifying the significance of\ninteractions between brain regions are of paramount importance in neuroscience.\nAlthough there have recently emerged some tests for graph inference based on\nindependent samples, there is no readily available solution to test the change\nof brain network for paired and correlated samples. In this article, we develop\na paired test of matrix graphs to infer brain connectivity network when the\ngroups of samples are correlated. The proposed test statistic is both bias\ncorrected and variance corrected, and achieves a small estimation error rate.\nThe subsequent multiple testing procedure built on this test statistic is\nguaranteed to asymptotically control the false discovery rate at the\npre-specified level. Both the methodology and theory of the new test are\nconsiderably different from the two independent samples framework, owing to the\nstrong correlations of measurements on the same subjects before and after the\nstimulus activity. We illustrate the efficacy of our proposal through\nsimulations and an analysis of an Alzheimer's Disease Neuroimaing Initiative\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 19:37:10 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Ye", "Yuting", ""], ["Xia", "Yin", ""], ["Li", "Lexin", ""]]}, {"id": "1908.08133", "submitter": "Luke Prendergast", "authors": "Dilanka S. Dedduwakumara, Luke A. Prendergast and Robert G. Staudte", "title": "Insights and inference for the proportion below the relative poverty\n  line", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a commonly used relative poverty measure called the headcount\nratio ($H_p$), defined to be the proportion of incomes falling below the\nrelative poverty line, which is defined to be a fraction $p$ of the median\nincome. We do this by considering this concept for theoretical income\npopulations, and its potential for determining actual changes following\ntransfer of incomes from the wealthy to those whose incomes fall below the\nrelative poverty line. In the process we derive and evaluate the performance of\nlarge sample confidence intervals for $H_p$. Finally, we illustrate the\nestimators on real income data sets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 22:37:39 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 06:41:08 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Dedduwakumara", "Dilanka S.", ""], ["Prendergast", "Luke A.", ""], ["Staudte", "Robert G.", ""]]}, {"id": "1908.08243", "submitter": "Bernhard Klar", "authors": "Andreas Eberl, Bernhard Klar", "title": "Expectile based measures of skewness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the literature, quite a few measures have been proposed for quantifying\nthe deviation of a probability distribution from symmetry. The most popular of\nthese skewness measures are based on the third centralized moment and on\nquantiles. However, there are major drawbacks in using these quantities. These\ninclude a strong emphasis on the distributional tails and a poor asymptotic\nbehaviour for the (empirical) moment based measure as well as difficult\nstatistical inference and strange behaviour for discrete distributions for\nquantile based measures.\n  Therefore, in this paper, we introduce skewness measures based on or\nconnected with expectiles. Since expectiles can be seen as smoothed versions of\nquantiles, they preserve the advantages over the moment based measure while not\nexhibiting most of the disadvantages of quantile based measures. We introduce\ncorresponding empirical counterparts and derive asymptotic properties. Finally,\nwe conduct a simulation study, comparing the newly introduced measures with\nestablished ones, and evaluating the performance of the respective estimators.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 08:08:03 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Eberl", "Andreas", ""], ["Klar", "Bernhard", ""]]}, {"id": "1908.08320", "submitter": "Philipp Otto", "authors": "Philipp Otto and Wolfgang Schmid", "title": "Spatial and Spatiotemporal GARCH Models -- A Unified Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In time-series analyses, particularly for finance, generalized autoregressive\nconditional heteroscedasticity (GARCH) models are widely applied statistical\ntools for modelling volatility clusters (i.e., periods of increased or\ndecreased risk). In contrast, it has not been considered to be of critical\nimportance until now to model spatial dependence in the conditional second\nmoments. Only a few models have been proposed for modelling local clusters of\nincreased risks. In this paper, we introduce novel spatial GARCH and\nexponential GARCH processes in a unified spatial and spatiotemporal GARCH-type\nmodel, which also covers all previously proposed spatial ARCH models as well as\ntime-series GARCH models. For this common modelling framework, estimators are\nderived based on nonlinear least squares and on the maximum-likelihood\napproach. In addition to the theoretical contributions of this paper, we\nsuggest a model selection strategy that is verified by a series of Monte Carlo\nsimulation studies. Eventually, the use of the unified model is demonstrated by\nan empirical example that focuses on real estate prices from 1995 to 2014\nacross the ZIP-Code areas of Berlin. A spatial autoregressive model is applied\nto the data to illustrate how locally varying model uncertainties can be\ncaptured by the spatial GARCH-type models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 11:29:03 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 08:33:41 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Otto", "Philipp", ""], ["Schmid", "Wolfgang", ""]]}, {"id": "1908.08444", "submitter": "Asaf Weinstein", "authors": "Daniel Yekutieli and Asaf Weinstein", "title": "Hierarchical Bayes Modeling for Large-Scale Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian modeling is now ubiquitous in problems of large-scale inference even\nwhen frequentist criteria are in mind for evaluating the performance of a\nprocedure. By far most popular in the statistical literature of the past decade\nand a half are empirical Bayes methods, that have shown in practice to improve\nsignificantly over strictly-frequentist competitors in many different problems.\nAs an alternative to empirical Bayes methods, in this paper we propose\nhierarchical Bayes modeling for large-scale problems, and address two separate\npoints that, in our opinion, deserve more attention. The first is nonparametric\n\"deconvolution\" methods that are applicable also outside the sequence model.\nThe second point is the adequacy of Bayesian modeling for situations where the\nparameters are by assumption deterministic. We provide partial answers to both:\nfirst, we demonstrate how our methodology applies in the analysis of a logistic\nregression model. Second, we appeal to Robbins's compound decision theory and\nprovide an extension, to give formal justification for the Bayesian approach in\nthe sequence case.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 15:18:02 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 06:55:38 GMT"}, {"version": "v3", "created": "Sun, 6 Oct 2019 14:38:03 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Yekutieli", "Daniel", ""], ["Weinstein", "Asaf", ""]]}, {"id": "1908.08484", "submitter": "Teemu Roos", "authors": "Peter Gr\\\"unwald and Teemu Roos", "title": "Minimum Description Length Revisited", "comments": "to appear in International Journal of Mathematics for Industry", "journal-ref": null, "doi": "10.1142/S2661335219300018", "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is an up-to-date introduction to and overview of the Minimum Description\nLength (MDL) Principle, a theory of inductive inference that can be applied to\ngeneral problems in statistics, machine learning and pattern recognition. While\nMDL was originally based on data compression ideas, this introduction can be\nread without any knowledge thereof. It takes into account all major\ndevelopments since 2007, the last time an extensive overview was written. These\ninclude new methods for model selection and averaging and hypothesis testing,\nas well as the first completely general definition of {\\em MDL estimators}.\nIncorporating these developments, MDL can be seen as a powerful extension of\nboth penalized likelihood and Bayesian approaches, in which penalization\nfunctions and prior distributions are replaced by more general luckiness\nfunctions, average-case methodology is replaced by a more robust worst-case\napproach, and in which methods classically viewed as highly distinct, such as\nAIC vs BIC and cross-validation vs Bayes can, to a large extent, be viewed from\na unified perspective.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 11:42:56 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 13:54:57 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Gr\u00fcnwald", "Peter", ""], ["Roos", "Teemu", ""]]}, {"id": "1908.08558", "submitter": "Leying Guan", "authors": "Leying Guan", "title": "Conformal prediction with localization", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method called localized conformal prediction, where we can\nperform conformal inference using only a local region around a new test sample\nto construct its confidence interval. Localized conformal inference is a\nnatural extension to conformal inference. It generalizes the method of\nconformal prediction to the case where we can break the data exchangeability,\nso as to give the test sample a special role. To our knowledge, this is the\nfirst work that introduces such a localization to the framework of conformal\nprediction. We prove that our proposal can also have assumption-free and finite\nsample coverage guarantees, and we compare the behaviors of localized conformal\nprediction and conformal prediction in simulations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 18:37:27 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 20:13:13 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 03:54:21 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Guan", "Leying", ""]]}, {"id": "1908.08596", "submitter": "Brian Knaeble", "authors": "Brian Knaeble, Braxton Osting, and Mark Abramson", "title": "Regression Analysis of Unmeasured Confounding", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When studying the causal effect of $x$ on $y$, researchers may conduct\nregression and report a confidence interval for the slope coefficient\n$\\beta_{x}$. This common confidence interval provides an assessment of\nuncertainty from sampling error, but it does not assess uncertainty from\nconfounding. An intervention on $x$ may produce a response in $y$ that is\nunexpected, and our misinterpretation of the slope happens when there are\nconfounding factors $w$. When $w$ are measured we may conduct multiple\nregression, but when $w$ are unmeasured it is common practice to include a\nprecautionary statement when reporting the confidence interval, warning against\nunwarranted causal interpretation. If the goal is robust causal interpretation\nthen we can do something more informative. Uncertainty in the specification of\nthree confounding parameters can be propagated through an equation to produce a\nconfounding interval. Here we develop supporting mathematical theory and\ndescribe an example application. Our proposed methodology applies well to\nstudies of a continuous response or rare outcome. It is a general method for\nquantifying error from model uncertainty. Whereas confidence intervals are used\nto assess uncertainty from unmeasured individuals, confounding intervals can be\nused to assess uncertainty from unmeasured attributes.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 21:04:43 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Knaeble", "Brian", ""], ["Osting", "Braxton", ""], ["Abramson", "Mark", ""]]}, {"id": "1908.08702", "submitter": "Oliver Braganza", "authors": "Oliver Braganza", "title": "A simple model suggesting economically rational sample-size choice\n  drives irreproducibility", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0229615", "report-no": null, "categories": "econ.GN cs.SY eess.SY q-fin.EC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several systematic studies have suggested that a large fraction of published\nresearch is not reproducible. One probable reason for low reproducibility is\ninsufficient sample size, resulting in low power and low positive predictive\nvalue. It has been suggested that insufficient sample-size choice is driven by\na combination of scientific competition and 'positive publication bias'. Here\nwe formalize this intuition in a simple model, in which scientists choose\neconomically rational sample sizes, balancing the cost of experimentation with\nincome from publication. Specifically, assuming that a scientist's income\nderives only from 'positive' findings (positive publication bias) and that\nindividual samples cost a fixed amount, allows to leverage basic statistical\nformulas into an economic optimality prediction. We find that if effects have\ni) low base probability, ii) small effect size or iii) low grant income per\npublication, then the rational (economically optimal) sample size is small.\nFurthermore, for plausible distributions of these parameters we find a robust\nemergence of a bimodal distribution of obtained statistical power and low\noverall reproducibility rates, both matching empirical findings. Finally, we\nexplore conditional equivalence testing as a means to align economic incentives\nwith adequate sample sizes. Overall, the model describes a simple mechanism\nexplaining both the prevalence and the persistence of small sample sizes, and\nis well suited for empirical validation. It proposes economic rationality, or\neconomic pressures, as a principal driver of irreproducibility and suggests\nstrategies to change this.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 07:45:30 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 08:59:23 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 14:27:15 GMT"}, {"version": "v4", "created": "Thu, 13 Feb 2020 09:22:52 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Braganza", "Oliver", ""]]}, {"id": "1908.08741", "submitter": "PierGianLuca Porta Mana", "authors": "PierGianLuca Porta Mana", "title": "A relation between log-likelihood and cross-validation log-scores", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is shown that the log-likelihood of a hypothesis or model given some data\nis equivalent to an average of all leave-one-out cross-validation log-scores\nthat can be calculated from all subsets of the data. This relation can be\ngeneralized to any $k$-fold cross-validation log-scores.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 09:52:40 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Mana", "PierGianLuca Porta", ""]]}, {"id": "1908.08760", "submitter": "Ioannis Kalogridis Mr", "authors": "Ioannis Kalogridis and Stefan Van Aelst", "title": "M-type penalized splines for functional linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis is a fast evolving branch of modern statistics.\nDespite the popularity of the functional linear model in recent years, current\nestimation procedures either suffer from lack of robustness or are\ncomputationally burdensome. To address these shortcomings, we propose a\nflexible family of lower-rank smoothers that combines penalized splines and\nM-estimation. Under suitable conditions on the design, these estimators exhibit\nthe same asymptotic properties as the corresponding least-squares estimators,\nwhile being considerably more reliable in the presence of outliers. The\nproposed methods can easily be generalized to functional models with additional\nfunctional predictors, scalar covariates or nonparametric components, thus\nproviding a wide framework of estimation. Empirical investigation shows that\nthe proposed estimators can combine high efficiency with protection against\noutliers, and produce smooth estimates that compare favourably with existing\napproaches, robust and non-robust alike.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 11:07:25 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 09:27:47 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kalogridis", "Ioannis", ""], ["Van Aelst", "Stefan", ""]]}, {"id": "1908.08764", "submitter": "Rahma Abid", "authors": "Rahma Abid, Celestin C. Kokonendji, Afif Masmoudi", "title": "On Poisson-exponential-Tweedie models for ultra-overdispersed data", "comments": "Preprint submitted to impacted journal for publication. 4 figures and\n  21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of Poisson-exponential-Tweedie (PET) mixture in the\nframework of generalized linear models for ultra-overdispersed count data. The\nmean-variance relationship is of the form $m+m^{2}+\\phi m^{p}$, where $\\phi$\nand $p$ are the dispersion and Tweedie power parameters, respectively. The\nproposed model is equivalent to the exponential-Poisson-Tweedie models arising\nfrom geometric sums of Poisson-Tweedie random variables. In this respect, the\nPET models encompass the geometric versions of Hermite, Neyman Type A,\nP\\'{o}lya-Aeppli, negative binomial and Poisson inverse Gaussian models. The\nalgorithms we shall propose allow us to estimate the real power parameter,\nwhich works as an automatic distribution selection. Instead of the classical\nPoisson, zero-shifted geometric is presented as the reference count\ndistribution. Practical properties are incorporated into the PET of new\nrelative indexes of dispersion and zero-inflation phenomena. Simulation studies\ndemonstrate that the proposed model highlights unbiased and consistent\nestimators for large samples. Illustrative practical applications are analyzed\non count datasets; in particular, PET models for data without covariates and\nPET regression models. The PET models are compared to Poisson-Tweedie models\nshowing that parameters of both models are adopted to data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 11:39:22 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Abid", "Rahma", ""], ["Kokonendji", "Celestin C.", ""], ["Masmoudi", "Afif", ""]]}, {"id": "1908.08845", "submitter": "Luis Vargas Mieles", "authors": "Luis Vargas, Marcelo Pereyra and Konstantinos C. Zygalakis", "title": "Accelerating proximal Markov chain Monte Carlo by using an explicit\n  stabilised method", "comments": "28 pages, 13 figures. Accepted for publication in SIAM Journal on\n  Imaging Sciences (SIIMS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a highly efficient proximal Markov chain Monte Carlo methodology\nto perform Bayesian computation in imaging problems. Similarly to previous\nproximal Monte Carlo approaches, the proposed method is derived from an\napproximation of the Langevin diffusion. However, instead of the conventional\nEuler-Maruyama approximation that underpins existing proximal Monte Carlo\nmethods, here we use a state-of-the-art orthogonal Runge-Kutta-Chebyshev\nstochastic approximation that combines several gradient evaluations to\nsignificantly accelerate its convergence speed, similarly to accelerated\ngradient optimisation methods. The proposed methodology is demonstrated via a\nrange of numerical experiments, including non-blind image deconvolution,\nhyperspectral unmixing, and tomographic reconstruction, with total-variation\nand $\\ell_1$-type priors. Comparisons with Euler-type proximal Monte Carlo\nmethods confirm that the Markov chains generated with our method exhibit\nsignificantly faster convergence speeds, achieve larger effective sample sizes,\nand produce lower mean square estimation errors at equal computational budget.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 14:44:28 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 15:52:49 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 13:55:48 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Vargas", "Luis", ""], ["Pereyra", "Marcelo", ""], ["Zygalakis", "Konstantinos C.", ""]]}, {"id": "1908.08868", "submitter": "Simon Mak", "authors": "Liang Ding, Simon Mak, C. F. Jeff Wu", "title": "BdryGP: a new Gaussian process model for incorporating boundary\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are widely used as surrogate models for emulating\ncomputer code, which simulate complex physical phenomena. In many problems,\nadditional boundary information (i.e., the behavior of the phenomena along\ninput boundaries) is known beforehand, either from governing physics or\nscientific knowledge. While there has been recent work on incorporating\nboundary information within GPs, such models do not provide theoretical\ninsights on improved convergence rates. To this end, we propose a new GP model,\ncalled BdryGP, for incorporating boundary information. We show that BdryGP not\nonly has improved convergence rates over existing GP models (which do not\nincorporate boundaries), but is also more resistant to the\n\"curse-of-dimensionality\" in nonparametric regression. Our proofs make use of a\nnovel connection between GP interpolation and finite-element modeling.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 15:31:18 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Ding", "Liang", ""], ["Mak", "Simon", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "1908.09077", "submitter": "Rachael C Aikens", "authors": "Rachael C. Aikens (1 and 2), Dylan Greaves (2), Michael Baiocchi (3)\n  ((1) Stanford University Department of Biomedical Informatics, (2) Stanford\n  University Department of Statistics, (3) Stanford University Department of\n  Epidemiology and Population Health)", "title": "A Pilot Design for Observational Studies: Using Abundant Data\n  Thoughtfully", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observational studies often benefit from an abundance of observational units.\nThis can lead to studies that -- while challenged by issues of internal\nvalidity -- have inferences derived from sample sizes substantially larger than\nrandomized controlled trials. But is the information provided by an\nobservational unit best used in the analysis phase? We propose the use of\n`pilot design,' in which observations are expended in the design phase of the\nstudy, and the post-treatment information from these observations is used to\nimprove study design. In modern observational studies, which are data rich but\ncontrol poor, pilot designs can be used to gain information about the structure\nof post-treatment variation. This information can then be used to improve\ninstrumental variable designs, propensity score matching, doubly-robust\nestimation, and other observational study designs.\n  We illustrate one version of a pilot design, which aims to reduce within-set\nheterogeneity and improve performance in sensitivity analyses. This version of\na pilot design expends observational units during the design phase to fit a\nprognostic model, avoiding concerns of overfitting. Additionally, it enables\nthe construction of `Assignment-Control (AC) plots,' which visualize the\nrelationship between propensity and prognostic scores. We first show some\nexamples of these plots, then we demonstrate in a simulation setting how this\nalternative use of the observations can lead to gains in terms of both\ntreatment effect estimation and sensitivity analyses of unobserved confounding.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 02:42:18 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 23:39:29 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 00:34:11 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Aikens", "Rachael C.", "", "1 and 2"], ["Greaves", "Dylan", ""], ["Baiocchi", "Michael", ""]]}, {"id": "1908.09112", "submitter": "Daniel Andrade", "authors": "Daniel Andrade and Kenji Fukumizu", "title": "Disjunct Support Spike and Slab Priors for Variable Selection in\n  Regression under Quasi-sparseness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparseness of the regression coefficient vector is often a desirable\nproperty, since, among other benefits, sparseness improves interpretability. In\npractice, many true regression coefficients might be negligibly small, but\nnon-zero, which we refer to as quasi-sparseness. Spike-and-slab priors as\nintroduced in (Chipman et al., 2001) can be tuned to ignore very small\nregression coefficients, and, as a consequence provide a trade-off between\nprediction accuracy and interpretability. However, spike-and-slab priors with\nfull support lead to inconsistent Bayes factors, in the sense that the Bayes\nfactors of any two models are bounded in probability. This is clearly an\nundesirable property for Bayesian hypotheses testing, where we wish that\nincreasing sample sizes lead to increasing Bayes factors favoring the true\nmodel. The moment matching priors as in (Johnson and Rossell, 2012) can resolve\nthis issue, but are unsuitable for the quasi-sparse setting due to their full\nsupport outside the exact value 0. As a remedy, we suggest disjunct support\nspike and slab priors, for which we prove consistent Bayes factors in the\nquasi-sparse setting, and show experimentally fast growing Bayes factors\nfavoring the true model. Several experiments on simulated and real data confirm\nthe usefulness of our proposed method to identify models with high effect size,\nwhile leading to better control over false positives than hard-thresholding.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 09:23:59 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 09:42:59 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Andrade", "Daniel", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1908.09230", "submitter": "Issa Dahabreh", "authors": "Issa J. Dahabreh and Sarah E. Robertson and Lucia C. Petito and Miguel\n  A. Hern\\'an and Jon A. Steingrimsson", "title": "Efficient and robust methods for causally interpretable meta-analysis:\n  transporting inferences from multiple randomized trials to a target\n  population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methods for causally interpretable meta-analyses that combine\ninformation from multiple randomized trials to estimate potential\n(counterfactual) outcome means and average treatment effects in a target\npopulation. We consider identifiability conditions, derive implications of the\nconditions for the law of the observed data, and obtain identification results\nfor transporting causal inferences from a collection of independent randomized\ntrials to a new target population in which experimental data may not be\navailable. We propose an estimator for the potential (counterfactual) outcome\nmean in the target population under each treatment studied in the trials. The\nestimator uses covariate, treatment, and outcome data from the collection of\ntrials, but only covariate data from the target population sample. We show that\nit is doubly robust, in the sense that it is consistent and asymptotically\nnormal when at least one of the models it relies on is correctly specified. We\nstudy the finite sample properties of the estimator in simulation studies and\ndemonstrate its implementation using data from a multi-center randomized trial.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 23:25:44 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 16:39:03 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 16:54:18 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2020 00:53:32 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Dahabreh", "Issa J.", ""], ["Robertson", "Sarah E.", ""], ["Petito", "Lucia C.", ""], ["Hern\u00e1n", "Miguel A.", ""], ["Steingrimsson", "Jon A.", ""]]}, {"id": "1908.09410", "submitter": "Shinichiro Shirota Dr", "authors": "Alan E. Gelfand and Shinichiro Shirota", "title": "Clarifying species dependence under joint species distribution modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint species distribution modeling is attracting increasing attention these\ndays, acknowledging the fact that individual level modeling fails to take into\naccount expected dependence/interaction between species. These models attempt\nto capture species dependence through an associated correlation matrix arising\nfrom a set of latent multivariate normal variables. However, these associations\noffer little insight into dependence behavior between species at sites.\n  We focus on presence/absence data using joint species modeling which\nincorporates spatial dependence between sites. For pairs of species, we\nemphasize the induced odds ratios (along with the joint probabilities of\noccurrence); they provide much clearer understanding of joint presence/absence\nbehavior. In fact, we propose a spatial odds ratio surface over the region of\ninterest to capture how dependence varies over the region.\n  We illustrate with a dataset from the Cape Floristic Region of South Africa\nconsisting of more than 600 species at more than 600 sites. We present the\nspatial distribution of odds ratios for pairs of species that are positively\ncorrelated and pairs that are negatively correlated under the joint species\ndistribution model.\n  The multivariate normal covariance matrix associated with a collection of\nspecies is only a device for creating dependence among species but it lacks\ninterpretation. By considering odds ratios, the quantitative ecologist will be\nable to better appreciate the practical dependence between species that is\nimplicit in these joint species distribution modeling specifications.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 00:07:50 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Gelfand", "Alan E.", ""], ["Shirota", "Shinichiro", ""]]}, {"id": "1908.09425", "submitter": "Raiden Hasegawa", "authors": "Raiden B. Hasegawa and Dylan S. Small", "title": "Estimating Malaria Vaccine Efficacy in the Absence of a Gold Standard\n  Case Definition: Mendelian Factorial Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimates of malaria vaccine efficacy require a reliable definition\nof a malaria case. However, the symptoms of clinical malaria are unspecific,\noverlapping with other childhood illnesses. Additionally, children in endemic\nareas tolerate varying levels of parasitemia without symptoms. Together, this\nmakes finding a gold-standard case definition challenging. We present a method\nto identify and estimate malaria vaccine efficacy that does not require an\nobservable gold-standard case definition. Instead, we leverage genetic traits\nthat are protective against malaria but not against other illnesses, e.g., the\nsickle cell trait, to identify vaccine efficacy in a randomized trial. Inspired\nby Mendelian randomization, we introduce Mendelian factorial design, a method\nthat augments a randomized trial with genetic variation to produce a natural\nfactorial experiment, which identifies vaccine efficacy under realistic\nassumptions. A robust, covariance adjusted estimation procedure is developed\nfor estimating vaccine efficacy on the risk ratio and incidence ratio scales.\nSimulations suggest that our estimator has good performance whereas standard\nmethods are systematically biased. We demonstrate that a combined estimator\nusing both our proposed estimator and the standard approach yields significant\nimprovements when the Mendelian factor is only weakly protective.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 01:18:25 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hasegawa", "Raiden B.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1908.09429", "submitter": "Xin Tong Thomson", "authors": "X. T. Tong and M. Morzfeld and Y. M. Marzouk", "title": "MALA-within-Gibbs samplers for high-dimensional distributions with\n  sparse conditional structure", "comments": "38 ages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) samplers are numerical methods for drawing\nsamples from a given target probability distribution. We discuss one particular\nMCMC sampler, the MALA-within-Gibbs sampler, from the theoretical and practical\nperspectives. We first show that the acceptance ratio and step size of this\nsampler are independent of the overall problem dimension when (i) the target\ndistribution has sparse conditional structure, and (ii) this structure is\nreflected in the partial updating strategy of MALA-within-Gibbs. If, in\naddition, the target density is block-wise log-concave, then the sampler's\nconvergence rate is independent of dimension. From a practical perspective, we\nexpect that MALA-within-Gibbs is useful for solving high-dimensional Bayesian\ninference problems where the posterior exhibits sparse conditional structure at\nleast approximately. In this context, a partitioning of the state that\ncorrectly reflects the sparse conditional structure must be found, and we\nillustrate this process in two numerical examples. We also discuss trade-offs\nbetween the block size used for partial updating and computational requirements\nthat may increase with the number of blocks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 01:39:32 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 08:36:22 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Tong", "X. T.", ""], ["Morzfeld", "M.", ""], ["Marzouk", "Y. M.", ""]]}, {"id": "1908.09482", "submitter": "Nadja Klein Prof. Dr.", "authors": "Nadja Klein, David J. Nott and Michael Stanley Smith", "title": "Marginally-calibrated deep distributional regression", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics (2020)", "doi": "10.1080/10618600.2020.1807996", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) regression models are widely used in applications\nrequiring state-of-the-art predictive accuracy. However, until recently there\nhas been little work on accurate uncertainty quantification for predictions\nfrom such models. We add to this literature by outlining an approach to\nconstructing predictive distributions that are `marginally calibrated'. This is\nwhere the long run average of the predictive distributions of the response\nvariable matches the observed empirical margin. Our approach considers a DNN\nregression with a conditionally Gaussian prior for the final layer weights,\nfrom which an implicit copula process on the feature space is extracted. This\ncopula process is combined with a non-parametrically estimated marginal\ndistribution for the response. The end result is a scalable distributional DNN\nregression method with marginally calibrated predictions, and our work\ncomplements existing methods for probability calibration. The approach is first\nillustrated using two applications of dense layer feed-forward neural networks.\nHowever, our main motivating applications are in likelihood-free inference,\nwhere distributional deep regression is used to estimate marginal posterior\ndistributions. In two complex ecological time series examples we employ the\nimplicit copulas of convolutional networks, and show that marginal calibration\nresults in improved uncertainty quantification. Our approach also avoids the\nneed for manual specification of summary statistics, a requirement that is\nburdensome for users and typical of competing likelihood-free inference\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 05:47:34 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 09:29:21 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 19:33:06 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Klein", "Nadja", ""], ["Nott", "David J.", ""], ["Smith", "Michael Stanley", ""]]}, {"id": "1908.09653", "submitter": "Yipeng Song", "authors": "Yipeng Song", "title": "Fusing heterogeneous data sets", "comments": "PhD thesis, 173 pages, 60 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In systems biology, it is common to measure biochemical entities at different\nlevels of the same biological system. One of the central problems for the data\nfusion of such data sets is the heterogeneity of the data. This thesis\ndiscusses two types of heterogeneity. The first one is the type of data, such\nas metabolomics, proteomics and RNAseq data in genomics. These different omics\ndata reflect the properties of the studied biological system from different\nperspectives. The second one is the type of scale, which indicates the\nmeasurements obtained at different scales, such as binary, ordinal, interval\nand ratio-scaled variables. In this thesis, we developed several statistical\nmethods capable to fuse data sets of these two types of heterogeneity. The\nadvantages of the proposed methods in comparison with other approaches are\nassessed using comprehensive simulations as well as the analysis of real\nbiological data sets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 12:20:04 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Song", "Yipeng", ""]]}, {"id": "1908.09718", "submitter": "Nickalus Redell", "authors": "Nickalus Redell", "title": "Shapley Decomposition of R-Squared in Machine Learning Models", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a metric aimed at helping machine learning\npractitioners quickly summarize and communicate the overall importance of each\nfeature in any black-box machine learning prediction model. Our proposed\nmetric, based on a Shapley-value variance decomposition of the familiar $R^2$\nfrom classical statistics, is a model-agnostic approach for assessing feature\nimportance that fairly allocates the proportion of model-explained variability\nin the data to each model feature. This metric has several desirable properties\nincluding boundedness at 0 and 1 and a feature-level variance decomposition\nsumming to the overall model $R^2$. In contrast to related methods for\ncomputing feature-level $R^2$ variance decompositions with linear models, our\nmethod makes use of pre-computed Shapley values which effectively shifts the\ncomputational burden from iteratively fitting many models to the Shapley values\nthemselves. And with recent advancements in Shapley value calculations for\ngradient boosted decision trees and neural networks, computing our proposed\nmetric after model training can come with minimal computational overhead. Our\nimplementation is available in the R package shapFlex.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 14:56:36 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Redell", "Nickalus", ""]]}, {"id": "1908.09794", "submitter": "Nirian Mart\\'in", "authors": "Ayanendranath Basu, Abhik Ghosh, Nirian Martin, Leandro Pardo", "title": "A Robust Generalization of the Rao Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new families of Rao-type test statistics based on the\nminimum density power divergence estimators which provide robust\ngeneralizations for testing simple and composite null hypotheses. The\nasymptotic null distributions of the proposed tests are obtained and their\nrobustness properties are also theoretically studied. Numerical illustrations\nare provided to substantiate the theory developed. On the whole, the proposed\ntests are seen to be excellent alternatives to the classical Rao test.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 16:51:44 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Basu", "Ayanendranath", ""], ["Ghosh", "Abhik", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1908.09830", "submitter": "Adrian Dobra", "authors": "Zhihang Dong, Yen-Chi Chen and Adrian Dobra", "title": "A statistical framework for measuring the temporal stability of human\n  mobility patterns", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing popularity of human mobility studies that collect GPS\nlocation data, the problem of determining the minimum required length of GPS\nmonitoring has not been addressed in the current statistical literature. In\nthis paper we tackle this problem by laying out a theoretical framework for\nassessing the temporal stability of human mobility based on GPS location data.\nWe define several measures of the temporal dynamics of human spatiotemporal\ntrajectories based on the average velocity process, and on activity\ndistributions in a spatial observation window. We demonstrate the use of our\nmethods with data that comprise the GPS locations of 185 individuals over the\ncourse of 18 months. Our empirical results suggest that GPS monitoring should\nbe performed over periods of time that are significantly longer than what has\nbeen previously suggested. Furthermore, we argue that GPS study designs should\ntake into account demographic groups.\n  KEYWORDS: Density estimation; global positioning systems (GPS); human\nmobility; spatiotemporal trajectories; temporal dynamics\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 18:35:01 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Dong", "Zhihang", ""], ["Chen", "Yen-Chi", ""], ["Dobra", "Adrian", ""]]}, {"id": "1908.09881", "submitter": "Tyler McCormick", "authors": "Emily Breza, Arun G. Chandrasekhar, Tyler H. McCormick, Mengjie Pan", "title": "Consistently estimating graph statistics using Aggregated Relational\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregated Relational Data, known as ARD, capture information about a social\nnetwork by asking about the number of connections between a person and a group\nwith a particular characteristic, rather than asking about connections between\neach pair of individuals directly. Breza et al. (Forthcoming) and McCormick and\nZheng (2015) relate ARD questions, consisting of survey items of the form \"How\nmany people with characteristic X do you know?\" to parametric statistical\nmodels for complete graphs. In this paper, we propose criteria for consistent\nestimation of individual and graph level statistics from ARD data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 19:04:05 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Breza", "Emily", ""], ["Chandrasekhar", "Arun G.", ""], ["McCormick", "Tyler H.", ""], ["Pan", "Mengjie", ""]]}, {"id": "1908.09967", "submitter": "Timothy Coleman", "authors": "Tim Coleman, Kimberly Kaufeld, Mary Frances Dorn, Lucas Mentch", "title": "Locally Optimized Random Forests", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard supervised learning procedures are validated against a test set that\nis assumed to have come from the same distribution as the training data.\nHowever, in many problems, the test data may have come from a different\ndistribution. We consider the case of having many labeled observations from one\ndistribution, $P_1$, and making predictions at unlabeled points that come from\n$P_2$. We combine the high predictive accuracy of random forests (Breiman,\n2001) with an importance sampling scheme, where the splits and predictions of\nthe base-trees are done in a weighted manner, which we call Locally Optimized\nRandom Forests. These weights correspond to a non-parametric estimate of the\nlikelihood ratio between the training and test distributions. To estimate these\nratios with an unlabeled test set, we make the covariate shift assumption,\nwhere the differences in distribution are only a function of the training\ndistributions (Shimodaira, 2000.) This methodology is motivated by the problem\nof forecasting power outages during hurricanes. The extreme nature of the most\ndevastating hurricanes means that typical validation set ups will overly favor\nless extreme storms. Our method provides a data-driven means of adapting a\nmachine learning method to deal with extreme events.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 00:42:56 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Coleman", "Tim", ""], ["Kaufeld", "Kimberly", ""], ["Dorn", "Mary Frances", ""], ["Mentch", "Lucas", ""]]}, {"id": "1908.09968", "submitter": "Anna Booth", "authors": "Anna Tudehope Booth, Jacqui Macdonald, George Youssef", "title": "Contextual Stress and Maternal Sensitivity: A Meta-Analytic Review of\n  Stress Associations with the Maternal Behavior Q-Sort in Observational\n  Studies", "comments": null, "journal-ref": null, "doi": "10.1016/j.dr.2018.02.002", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maternal sensitivity is a modifiable determinant of infant attachment\nsecurity and a precursor to optimal child development. Contextual stressors\nundermine sensitivity, but research was yet to be synthesized. We aimed to\nidentify i) types of stress associations analyzed in studies of maternal\nsensitivity and ii) the strength of effects of various stress factors. A\nsystematic search identified all studies that used the Maternal Behavior Q-Sort\n(MBQS) to code sensitivity in dyadic observations and that reported a\ncoefficient for MBQS associations with contextual stress. Identified stressors\ncohered around three spheres: sociodemography (maternal education, family\nincome, composite SES, maternal age and cohabitation status); parenting stress\n(perceived maternal stress related to parenting); and mental health\n(specifically maternal internalizing symptoms). Seven meta-analyses (combined\nns range 223-1239) of a subset of 30 effects from 20 articles, and a\nmulti-level meta-analysis (N=1324) assessed aggregated correlations with\nsensitivity. Significant mean effects emerged in expected directions, whereby\nall stress indicators were negatively associated with sensitivity. Small\neffects were found for associations with parenting stress (r=-0.13) and mental\nhealth indicators (r=-0.12). Generally moderate effects were found for\nassociations with socio-demographic indicators (range r=-0.12 to r=0.32).\nEmerging findings support the proposition that in various contexts of stress,\nmaternal sensitivity to infant needs can be undermined. Implications and\nresearch directions are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 00:48:52 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Booth", "Anna Tudehope", ""], ["Macdonald", "Jacqui", ""], ["Youssef", "George", ""]]}, {"id": "1908.10158", "submitter": "Xynthia Kavelaars", "authors": "X.M. Kavelaars, J. Mulder, M.C. Kaptein", "title": "Decision-making with multiple correlated binary outcomes in clinical\n  trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trials often evaluate multiple outcome variables to form a\ncomprehensive picture of the effects of a new treatment. The resulting\nmultidimensional insight contributes to clinically relevant and efficient\ndecision-making about treatment superiority. Common statistical procedures to\nmake these superiority decisions with multiple outcomes have two important\nshortcomings however: 1) Outcome variables are often modeled individually, and\nconsequently fail to consider the relation between outcomes; and 2) superiority\nis often defined as a relevant difference on a single, on any, or on all\noutcomes(s); and lacks a compensatory mechanism that allows large positive\neffects on one or multiple outcome(s) to outweigh small negative effects on\nother outcomes. To address these shortcomings, this paper proposes 1) a\nBayesian model for the analysis of correlated binary outcomes based on the\nmultivariate Bernoulli distribution; and 2) a flexible decision criterion with\na compensatory mechanism that captures the relative importance of the outcomes.\nA simulation study demonstrates that efficient and unbiased decisions can be\nmade while Type I error rates are properly controlled. The performance of the\nframework is illustrated for 1) fixed, group sequential, and adaptive designs;\nand 2) non-informative and informative prior distributions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 12:25:52 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 09:56:48 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Kavelaars", "X. M.", ""], ["Mulder", "J.", ""], ["Kaptein", "M. C.", ""]]}, {"id": "1908.10262", "submitter": "Tianyu Zhan", "authors": "Tianyu Zhan, Alan H Hartford, Jian Kang, and Walter W Offen", "title": "Optimizing Graphical Procedures for Multiplicity Control in a\n  Confirmatory Clinical Trial via Deep Learning", "comments": "26 pages, 4 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In confirmatory clinical trials, it has been proposed [Bretz et al., 2009] to\nuse a simple iterative graphical approach to constructing and performing\nintersection hypotheses tests using weighted Bonferroni-type procedures to\ncontrol type I errors in the strong sense. Given Phase II study results or\nprior knowledge, it is usually of main interest to find the optimal graph that\nmaximizes a certain objective function in a future Phase III study. However,\nlack of a closed form expression in the objective function makes the\noptimization challenging. In this manuscript, we propose a general optimization\nframework to obtain the global maximum via feedforward neural networks (FNNs)\nin deep learning. Simulation studies show that our FNN-based approach has a\nbetter balance between robustness and time efficiency than some existing\nderivative-free constrained optimization algorithms. Compared to the\ntraditional window searching approach, our optimizer has moderate multiplicity\nadjusted power gain when the number of hypotheses is relatively large or when\ntheir correlations are high. We further apply it to a case study to illustrate\nhow to optimize a multiple test procedure with respect to a specific study\nobjective.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 15:11:43 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Zhan", "Tianyu", ""], ["Hartford", "Alan H", ""], ["Kang", "Jian", ""], ["Offen", "Walter W", ""]]}, {"id": "1908.10401", "submitter": "Martin Wendler", "authors": "Alfredas Ra\\v{c}kauskas, Martin Wendler", "title": "Convergence of U-Processes in H\\\"older Spaces with Application to Robust\n  Detection of a Changed Segment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To detect a changed segment (so called epidemic changes) in a time series,\nvariants of the CUSUM statistic are frequently used. However, they are\nsensitive to outliers in the data and do not perform well for heavy tailed\ndata, especially when short segments get a high weight in the test statistic.\nWe will present a robust test statistic for epidemic changes based on the\nWilcoxon statistic. To study their asymptotic behavior, we prove functional\nlimit theorems for U-processes in H\\\"older spaces. We also study the finite\nsample behavior via simulations and apply the statistic to a real data example.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 18:36:30 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 09:57:44 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Ra\u010dkauskas", "Alfredas", ""], ["Wendler", "Martin", ""]]}, {"id": "1908.10448", "submitter": "Lin Liu", "authors": "Lin Liu, Zach Shahn, James M. Robins, Andrea Rotnitzky", "title": "Efficient estimation of optimal regimes under a no direct effect\n  assumption", "comments": "In press in the Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive new estimators of an optimal joint testing and treatment regime\nunder the no direct effect (NDE) assumption that a given laboratory,\ndiagnostic, or screening test has no effect on a patient's clinical outcomes\nexcept through the effect of the test results on the choice of treatment. We\nmodel the optimal joint strategy using an optimal regime structural nested mean\nmodel (opt-SNMM). The proposed estimators are more efficient than previous\nestimators of the parameters of an opt-SNMM because they efficiently leverage\nthe `no direct effect (NDE) of testing' assumption. Our methods will be of\nimportance to decision scientists who either perform cost-benefit analyses or\nare tasked with the estimation of the `value of information' supplied by an\nexpensive diagnostic test (such as an MRI to screen for lung cancer).\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 20:09:58 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 09:03:28 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liu", "Lin", ""], ["Shahn", "Zach", ""], ["Robins", "James M.", ""], ["Rotnitzky", "Andrea", ""]]}, {"id": "1908.10488", "submitter": "Paul Parker", "authors": "Paul A. Parker, Ryan Janicki, Scott H. Holan", "title": "Unit Level Modeling of Survey Data for Small Area Estimation Under\n  Informative Sampling: A Comprehensive Overview with Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based small area estimation is frequently used in conjunction with\nsurvey data in order to establish estimates for under-sampled or unsampled\ngeographies. These models can be specified at either the area-level, or the\nunit-level, but unit-level models often offer potential advantages such as more\nprecise estimates and easy spatial aggregation. Nevertheless, relative to\narea-level models, literature on unit-level models is less prevalent. In\nmodeling small areas at the unit level, challenges often arise as a consequence\nof the informative sampling mechanism used to collect the survey data. This\npaper provides a comprehensive methodological review for unit-level models\nunder informative sampling, with an emphasis on Bayesian approaches. To provide\ninsight into the differences between methods, we conduct a simulation study\nthat compares several of the described approaches. In addition, the methods\nused for simulation are further illustrated through an application to the\nAmerican Community Survey. Finally, we present several extensions and areas for\nfuture research.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 22:43:40 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 17:58:25 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Parker", "Paul A.", ""], ["Janicki", "Ryan", ""], ["Holan", "Scott H.", ""]]}, {"id": "1908.10502", "submitter": "Jose Jimenez", "authors": "Jos\\'e L. Jim\\'enez", "title": "Quantifying treatment differences in confirmatory trials under\n  non-proportional hazards", "comments": null, "journal-ref": null, "doi": "10.1080/02664763.2020.1815673", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proportional hazards are a common assumption when designing confirmatory\nclinical trials in oncology. With the emergence of immunotherapy and novel\ntargeted therapies, departure from the proportional hazard assumption is not\nrare in nowadays clinical research. Under non-proportional hazards, the hazard\nratio does not have a straightforward clinical interpretation, and the log-rank\ntest is no longer the most powerful statistical test even though it is still\nvalid. Nevertheless, the log-rank test and the hazard ratio are still the\nprimary analysis tools, and traditional approaches such as sample size increase\nare still proposed to account for the impact of non-proportional hazards. The\nweighed log-rank test and the test based on the restricted mean survival time\n(RMST) are receiving a lot of attention as a potential alternative to the\nlog-rank test. We conduct a simulation study comparing the performance and\noperating characteristics of the log-rank test, the weighted log-rank test and\nthe test based on the RMST, including a treatment effect estimation, under\ndifferent non-proportional hazards patterns. Results show that, under\nnon-proportional hazards, the hazard ratio and weighted hazard ratio have no\nstraightforward clinical interpretation whereas the RMST ratio can be\ninterpreted regardless of the proportional hazards assumption. In terms of\npower, the RMST achieves a similar performance when compared to the log-rank\ntest.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 00:33:38 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 17:38:57 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 16:38:18 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Jim\u00e9nez", "Jos\u00e9 L.", ""]]}, {"id": "1908.10506", "submitter": "Donghui Yan", "authors": "Donghui Yan, Songxiang Gu, Ying Xu and Zhiwei Qin", "title": "Similarity Kernel and Clustering via Random Projection Forests", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity plays a fundamental role in many areas, including data mining,\nmachine learning, statistics and various applied domains. Inspired by the\nsuccess of ensemble methods and the flexibility of trees, we propose to learn a\nsimilarity kernel called rpf-kernel through random projection forests\n(rpForests). Our theoretical analysis reveals a highly desirable property of\nrpf-kernel: far-away (dissimilar) points have a low similarity value while\nnearby (similar) points would have a high similarity}, and the similarities\nhave a native interpretation as the probability of points remaining in the same\nleaf nodes during the growth of rpForests. The learned rpf-kernel leads to an\neffective clustering algorithm--rpfCluster. On a wide variety of real and\nbenchmark datasets, rpfCluster compares favorably to K-means clustering,\nspectral clustering and a state-of-the-art clustering ensemble\nalgorithm--Cluster Forests. Our approach is simple to implement and readily\nadapt to the geometry of the underlying data. Given its desirable theoretical\nproperty and competitive empirical performance when applied to clustering, we\nexpect rpf-kernel to be applicable to many problems of an unsupervised nature\nor as a regularizer in some supervised or weakly supervised settings.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 00:38:53 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Yan", "Donghui", ""], ["Gu", "Songxiang", ""], ["Xu", "Ying", ""], ["Qin", "Zhiwei", ""]]}, {"id": "1908.10572", "submitter": "Toru Imai", "authors": "Toru Imai", "title": "On the overestimation of widely applicable Bayesian information\n  criterion", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely applicable Bayesian information criterion (Watanabe, 2013) is\napplicable for both regular and singular models in the model selection problem.\nThis criterion tends to overestimate the log marginal likelihood. We identify\nan overestimating term of a widely applicable Bayesian information criterion.\nAdjustment of the term gives an asymptotically unbiased estimator of the\nleading two terms of asymptotic expansion of the log marginal likelihood. In\nnumerical experiments on regular and singular models, the adjustment resulted\nin smaller bias than the original criterion.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 07:04:39 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Imai", "Toru", ""]]}, {"id": "1908.10613", "submitter": "Tat-Thang Vo", "authors": "Tat-Thang Vo, Raphael Porcher and Stijn Vansteelandt", "title": "Rethinking meta-analysis: assessing case-mix heterogeneity when\n  combining treatment effects across patient populations", "comments": "27 pages, 6 tables, 2 figures. Currently under peer-review at the\n  journal of Research Synthesis Methods", "journal-ref": null, "doi": "10.1002/jrsm.1382", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Case-mix heterogeneity across studies complicates meta-analyses. As a result\nof this, treatments that are equally effective on patient subgroups may appear\nto have different effectiveness on patient populations with different case mix.\nIt is therefore important that meta-analyses be explicit for what patient\npopulation they describe the treatment effect. To achieve this, we develop\nalternative approaches for meta-analysis of randomized clinical trials, which\nuse individual patient data (IPD) from all trials to infer the treatment effect\nfor the patient population in a given trial, based on direct standardization\nusing either outcome regression (OCR) or inverse probability weighting (IPW).\nAccompanying random-effect meta-analysis models are developed. The new\napproaches enable disentangling heterogeneity due to case-mix inconsistency\nfrom that due to beyond case-mix reasons.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 09:50:16 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 14:46:06 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Vo", "Tat-Thang", ""], ["Porcher", "Raphael", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "1908.10742", "submitter": "Zhengling Qi", "authors": "Zhengling Qi, Ying Cui, Yufeng Liu, Jong-Shi Pang", "title": "Estimation of Individualized Decision Rules Based on an Optimized\n  Covariate-Dependent Equivalent of Random Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent exploration of optimal individualized decision rules (IDRs) for\npatients in precision medicine has attracted a lot of attention due to the\nheterogeneous responses of patients to different treatments. In the existing\nliterature of precision medicine, an optimal IDR is defined as a decision\nfunction mapping from the patients' covariate space into the treatment space\nthat maximizes the expected outcome of each individual. Motivated by the\nconcept of Optimized Certainty Equivalent (OCE) introduced originally in\n\\cite{ben1986expected} that includes the popular conditional-value-of risk\n(CVaR) \\cite{rockafellar2000optimization}, we propose a decision-rule based\noptimized covariates dependent equivalent (CDE) for individualized decision\nmaking problems. Our proposed IDR-CDE broadens the existing expected-mean\noutcome framework in precision medicine and enriches the previous concept of\nthe OCE. Numerical experiments demonstrate that our overall approach\noutperforms existing methods in estimating optimal IDRs under heavy-tail\ndistributions of the data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 14:54:46 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Qi", "Zhengling", ""], ["Cui", "Ying", ""], ["Liu", "Yufeng", ""], ["Pang", "Jong-Shi", ""]]}, {"id": "1908.10925", "submitter": "Yi Zhao", "authors": "Yi Zhao, Lexin Li, Brian S. Caffo", "title": "Multimodal Neuroimaging Data Integration and Pathway Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With fast advancements in technologies, the collection of multiple types of\nmeasurements on a common set of subjects is becoming routine in science. Some\nnotable examples include multimodal neuroimaging studies for the simultaneous\ninvestigation of brain structure and function, and multi-omics studies for\ncombining genetic and genomic information. Integrative analysis of multimodal\ndata allows scientists to interrogate new mechanistic questions. However, the\ndata collection and generation of integrative hypotheses is outpacing available\nmethodology for joint analysis of multimodal measurements. In this article, we\nstudy high-dimensional multimodal data integration in the context of mediation\nanalysis. We aim to understand the roles different data modalities play as\npossible mediators in the pathway between an exposure variable and an outcome.\nWe propose a mediation model framework with two data types serving as separate\nsets of mediators, and develop a penalized optimization approach for parameter\nestimation. We study both the theoretical properties of the estimator through\nan asymptotic analysis, and its finite-sample performance through simulations.\nWe illustrate our method with a multimodal brain pathway analysis having both\nstructural and functional connectivities as mediators in the association\nbetween sex and language processing.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 19:44:20 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Zhao", "Yi", ""], ["Li", "Lexin", ""], ["Caffo", "Brian S.", ""]]}, {"id": "1908.10965", "submitter": "Christine Peterson", "authors": "Christine B. Peterson, Nathan Osborne, Francesco C. Stingo, Pierrick\n  Bourgeat, James D. Doecke, and Marina Vannucci", "title": "Bayesian Modeling of Multiple Structural Connectivity Networks During\n  the Progression of Alzheimer's Disease", "comments": "Accepted to Biometrics January 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease is the most common neurodegenerative disease. The aim of\nthis study is to infer structural changes in brain connectivity resulting from\ndisease progression using cortical thickness measurements from a cohort of\nparticipants who were either healthy control, or with mild cognitive\nimpairment, or Alzheimer's disease patients. For this purpose, we develop a\nnovel approach for inference of multiple networks with related edge values\nacross groups. Specifically, we infer a Gaussian graphical model for each group\nwithin a joint framework, where we rely on Bayesian hierarchical priors to link\nthe precision matrix entries across groups. Our proposal differs from existing\napproaches in that it flexibly learns which groups have the most similar edge\nvalues, and accounts for the strength of connection (rather than only edge\npresence or absence) when sharing information across groups. Our results\nidentify key alterations in structural connectivity which may reflect\ndisruptions to the healthy brain, such as decreased connectivity within the\noccipital lobe with increasing disease severity. We also illustrate the\nproposed method through simulations, where we demonstrate its performance in\nstructure learning and precision matrix estimation with respect to alternative\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 22:09:17 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 15:45:28 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Peterson", "Christine B.", ""], ["Osborne", "Nathan", ""], ["Stingo", "Francesco C.", ""], ["Bourgeat", "Pierrick", ""], ["Doecke", "James D.", ""], ["Vannucci", "Marina", ""]]}, {"id": "1908.11048", "submitter": "Hyowon An", "authors": "Hyowon An, Kai Zhang, Hannu Oja and J. S. Marron", "title": "Variable screening based on Gaussian Centered L-moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important challenge in big data is identification of important variables.\nIn this paper, we propose methods of discovering variables with non-standard\nunivariate marginal distributions. The conventional moments-based summary\nstatistics can be well-adopted for that purpose, but their sensitivity to\noutliers can lead to selection based on a few outliers rather than\ndistributional shape such as bimodality. To address this type of\nnon-robustness, we consider the L-moments. Using these in practice, however,\nhas a limitation because they do not take zero values at the Gaussian\ndistributions to which the shape of a marginal distribution is most naturally\ncompared. As a remedy, we propose Gaussian Centered L-moments which share\nadvantages of the L-moments but have zeros at the Gaussian distributions. The\nstrength of Gaussian Centered L-moments over other conventional moments is\nshown in theoretical and practical aspects such as their performances in\nscreening important genes in cancer genetics data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 04:50:43 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["An", "Hyowon", ""], ["Zhang", "Kai", ""], ["Oja", "Hannu", ""], ["Marron", "J. S.", ""]]}, {"id": "1908.11251", "submitter": "Kevin Vanslette", "authors": "Kevin Vanslette, Tony Tohme, Kamal Youcef-Toumi", "title": "A General Model Validation and Testing Tool", "comments": "A few more examples and figures were added", "journal-ref": "Reliability Engineering & System Safety, February 2019, Volume\n  195, 106684", "doi": "10.1016/j.ress.2019.106684", "report-no": null, "categories": "stat.ME physics.data-an stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We construct and propose the \"Bayesian Validation Metric\" (BVM) as a general\nmodel validation and testing tool. We find the BVM to be capable of\nrepresenting all of the standard validation metrics (square error, reliability,\nprobability of agreement, frequentist, area, probability density comparison,\nstatistical hypothesis testing, and Bayesian model testing) as special cases\nand find that it can be used to improve, generalize, or further quantify their\nuncertainties. Thus, the BVM allows us to assess the similarities and\ndifferences between existing validation metrics in a new light.\n  The BVM has the capacity to allow users to invent and select models according\nto novel validation requirements. We formulate and test a few novel compound\nvalidation metrics that improve upon other validation metrics in the\nliterature. Further, we construct the BVM Ratio for the purpose of quantifying\nmodel selection under user defined definitions of agreement in the presence or\nabsence of uncertainty. This construction generalizes the Bayesian model\ntesting framework.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 14:18:50 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 15:32:39 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Vanslette", "Kevin", ""], ["Tohme", "Tony", ""], ["Youcef-Toumi", "Kamal", ""]]}, {"id": "1908.11305", "submitter": "Roberto Hern\\'andez Santander", "authors": "Roberto Hern\\'andez Santander and Esperanza Camargo Casallas", "title": "Inspection of methods of empirical mode decomposition", "comments": "11 pages, 6 figures. Introduced in the 5th International Conference\n  on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical Mode Decomposition is an adaptive and local tool that extracts\nunderlying analytical components of a non-linear and non-stationary process, in\nturn, is the basis of Hilbert Huang transform, however, there are problems such\nas interfering modes or ensuring the orthogonality of decomposition. Three\nvariants of the algorithm are evaluated, with different experimental parameters\nand on a set of 10 time series obtained from surface electromyography.\nExperimental results show that obtaining low error in reconstruction with the\nanalytical signals obtained from a process is not a valid characteristic to\nensure that the purpose of decomposition has been fulfilled (physical\nsignificance and no interference between modes), in addition, freedom must be\ngenerated in the iterative processes of decomposition so that it has\nconsistency and does not generate biased information. This project was\ndeveloped within the framework of the research group DIGITI of the Universidad\nDistrital Francisco Jos\\'e de Caldas.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 15:42:04 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Santander", "Roberto Hern\u00e1ndez", ""], ["Casallas", "Esperanza Camargo", ""]]}, {"id": "1908.11364", "submitter": "Jean-Philippe Montillet Dr.", "authors": "Machiel S. Bos, Jean-Philippe Montillet, Simon D.P. Williams, Rui M.S.\n  Fernandes", "title": "Introduction to Geodetic Time Series Analysis", "comments": "24 pages, 6 figures; Chapter 2 in the Book Geodetic Time Series\n  Analysis edited by J.P. Montillet and M. S. Bos", "journal-ref": null, "doi": "10.1007/978-3-030-21718-1_2", "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution is the chapter 2 of the book \"geodetic time series\nanalysis\" (10.1007/978-3-030-21718-1). The book is dedicated to the art of\nfitting a trajectory model to those geodetic time series in order to extract\naccurate geophysical information with realistic error bars in geodymanics and\nenvironmental geodesy related studies. In the vast amount of the literature\npublished on this topic in the past 25 years, we are specifically interested in\nparametric algorithms which are estimating both functional and stochastic\nmodels using various Bayesian statistical tools (maximum likelihood, Monte\nCarlo Markov chain, Kalman filter, least squares variance component estimation,\ninformation criteria). This chapter will focus on how the parameters of the\ntrajectory model can be estimated. It is meant to give researchers new to this\ntopic an easy introduction to the theory with references to key books and\narticles where more details can be found. In addition, we hope that it\nrefreshes some of the details for the more experienced readers. We pay special\nattention to the modelling of the noise which has received much attention in\nthe literature in the last years and highlight some of the numerical aspects.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 20:24:50 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Bos", "Machiel S.", ""], ["Montillet", "Jean-Philippe", ""], ["Williams", "Simon D. P.", ""], ["Fernandes", "Rui M. S.", ""]]}, {"id": "1908.11464", "submitter": "Sharmodeep Bhattacharyya", "authors": "Trevor Ruiz, Mahesh Balasubramanian, Kristofer E. Bouchard, Sharmodeep\n  Bhattacharyya", "title": "Sparse, Low-bias, and Scalable Estimation of High Dimensional Vector\n  Autoregressive Models via Union of Intersections", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector autoregressive (VAR) models are widely used for causal discovery and\nforecasting in multivariate time series analyses in fields as diverse as\nneuroscience, environmental science, and econometrics. In the high-dimensional\nsetting, model parameters are typically estimated by L1-regularized maximum\nlikelihood; yet, when applied to VAR models, this technique produces a sizable\ntrade-off between sparsity and bias with the choice of the regularization\nhyperparameter, and thus between causal discovery and prediction. That is,\nlow-bias estimation entails dense parameter selection, and sparse selection\nentails increased bias; the former is useful in forecasting but less likely to\nyield scientific insight leading to discovery of causal influences, and\nconversely for the latter. This paper presents a scalable algorithm for\nsimultaneous low-bias and low-variance estimation (hence good prediction) with\nsparse selection for high-dimensional VAR models. The method leverages the\nrecently developed Union of Intersections (UoI) algorithmic framework for\nflexible, modular, and scalable feature selection and estimation that allows\ncontrol of false discovery and false omission in feature selection while\nmaintaining low bias and low variance. This paper demonstrates the superior\nperformance of the UoI-VAR algorithm compared with other methods in simulation\nstudies, exhibits its application in data analysis, and illustrates its good\nalgorithmic scalability in multi-node distributed memory implementations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 22:07:00 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Ruiz", "Trevor", ""], ["Balasubramanian", "Mahesh", ""], ["Bouchard", "Kristofer E.", ""], ["Bhattacharyya", "Sharmodeep", ""]]}, {"id": "1908.11466", "submitter": "Junmo Song", "authors": "Jiwon Kang and Junmo Song", "title": "A robust approach for testing parameter change in Poisson autoregressive\n  models", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter change test has been an important issue in time series analysis.\nThe problem has also been actively explored in the field of integer-valued time\nseries, but the testing in the presence of outliers has not yet been\nextensively investigated. This study considers the problem of testing for\nparameter change in Poisson autoregressive models particularly when\nobservations are contaminated by outliers. To lessen the impact of outliers on\ntesting procedure, we propose a test based on the density power divergence,\nwhich is introduced by Basu et al. (Biometrika, 1998), and derive its limiting\nnull distribution. Monte Carlo simulation results demonstrate validity and\nstrong robustness of the proposed test.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 22:13:27 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Kang", "Jiwon", ""], ["Song", "Junmo", ""]]}, {"id": "1908.11601", "submitter": "Harjit Hullait", "authors": "Harjit Hullait, David S. Leslie, Nicos G. Pavlidis and Steve King", "title": "Robust Function-on-Function Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional linear regression is a widely used approach to model functional\nresponses with respect to functional inputs. However, classical functional\nlinear regression models can be severely affected by outliers. We therefore\nintroduce a Fisher-consistent robust functional linear regression model that is\nable to effectively fit data in the presence of outliers. The model is built\nusing robust functional principal component and least squares regression\nestimators. The performance of the functional linear regression model depends\non the number of principal components used. We therefore introduce a consistent\nrobust model selection procedure to choose the number of principal components.\nOur robust functional linear regression model can be used alongside an outlier\ndetection procedure to effectively identify abnormal functional responses. A\nsimulation study shows our method is able to effectively capture the regression\nbehaviour in the presence of outliers, and is able to find the outliers with\nhigh accuracy. We demonstrate the usefulness of our method on jet engine sensor\ndata. We identify outliers that would not be found if the functional responses\nwere modelled independently of the functional input, or using non-robust\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 09:04:02 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Hullait", "Harjit", ""], ["Leslie", "David S.", ""], ["Pavlidis", "Nicos G.", ""], ["King", "Steve", ""]]}, {"id": "1908.11611", "submitter": "Jinzhou Li", "authors": "Jinzhou Li and Marloes H. Maathuis", "title": "GGM knockoff filter: False Discovery Rate Control for Gaussian Graphical\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to learn the structure of a Gaussian graphical model\nwith finite sample false discovery rate control. Our method builds on the\nknockoff framework of Barber and Cand\\`{e}s for linear models. We extend their\napproach to the graphical model setting by using a local (node-based) and a\nglobal (graph-based) step: we construct knockoffs and feature statistics for\neach node locally, and then solve a global optimization problem to determine a\nthreshold for each node. We then estimate the neighborhood of each node, by\ncomparing its feature statistics to its threshold, resulting in our graph\nestimate. Our proposed method is very flexible, in the sense that there is\nfreedom in the choice of knockoffs, feature statistics, and the way in which\nthe final graph estimate is obtained. For any given data set, it is not clear a\npriori what choices of these hyperparameters are optimal. We therefore use a\nsample-splitting-recycling procedure that first uses half of the samples to\nselect the hyperparameters, and then learns the graph using all samples, in\nsuch a way that the finite sample FDR control still holds. We compare our\nmethod to several competitors in simulations and on a real data set.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 09:33:26 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 12:26:08 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 10:25:40 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Jinzhou", ""], ["Maathuis", "Marloes H.", ""]]}, {"id": "1908.11736", "submitter": "Jean-Philippe Montillet Dr.", "authors": "J.P. Montillet, X. He, K. Yu", "title": "Application of Levy Processes in Modelling (Geodetic) Time Series With\n  Mixed Spectra", "comments": "24 pages, 3 figures, 4 Tables - Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, various models have been developed, including the fractional\nBrownian motion (fBm), to analyse the stochastic properties of geodetic time\nseries, together with the extraction of geophysical signals. The noise spectrum\nof these time series is generally modeled as a mixed spectrum, with a sum of\nwhite and coloured noise. Here, we are interested in modelling the residual\ntime series, after deterministically subtracting geophysical signals from the\nobservations. This residual time series is then assumed to be a sum of three\nrandom variables (r.v.), with the last r.v. belonging to the family of Levy\nprocesses. This stochastic term models the remaining residual signals and other\ncorrelated processes. Via simulations and real time series, we identify three\nclasses of Levy processes: Gaussian, fractional and stable. In the first case,\nresiduals are predominantly constituted of short-memory processes. Fractional\nLevy process can be an alternative model to the fBm in the presence of\nlong-term correlations and self-similarity property. Stable process is\ncharacterized by a large variance, which can be satisfied in the case of\nheavy-tailed distributions. The application to geodetic time series imply\npotential anxiety in the functional model selection where missing geophysical\ninformation can generate such residual time series.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 14:27:00 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Montillet", "J. P.", ""], ["He", "X.", ""], ["Yu", "K.", ""]]}, {"id": "1908.11812", "submitter": "Giacomo Zanella", "authors": "Samuel Livingstone and Giacomo Zanella", "title": "The Barker proposal: combining robustness and efficiency in\n  gradient-based MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a tension between robustness and efficiency when designing Markov\nchain Monte Carlo (MCMC) sampling algorithms. Here we focus on robustness with\nrespect to tuning parameters, showing that more sophisticated algorithms tend\nto be more sensitive to the choice of step-size parameter and less robust to\nheterogeneity of the distribution of interest. We characterise this phenomenon\nby studying the behaviour of spectral gaps as an increasingly poor step-size is\nchosen for the algorithm. Motivated by these considerations, we propose a novel\nand simple gradient-based MCMC algorithm, inspired by the classical Barker\naccept-reject rule, with improved robustness properties. Extensive theoretical\nresults, dealing with robustness to tuning, geometric ergodicity and scaling\nwith dimension, suggest that the novel scheme combines the robustness of simple\nschemes with the efficiency of gradient-based ones. We show numerically that\nthis type of robustness is particularly beneficial in the context of adaptive\nMCMC, giving examples where our proposed scheme significantly outperforms\nstate-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 16:09:22 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 14:40:25 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Livingstone", "Samuel", ""], ["Zanella", "Giacomo", ""]]}]