[{"id": "1011.0057", "submitter": "Julien Cornebise", "authors": "Luke Bornn, Julien Cornebise, Gareth W. Peters", "title": "Discussion of \"Riemann manifold Langevin and Hamiltonian Monte Carlo\n  methods'' by M. Girolami and B. Calderhead", "comments": "To appear in Journal of the Royal Statitistical Society Series B, in\n  the discussion of the read paper by Calderhead and Girolami 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report is the union of two contributions to the discussion of\nthe Read Paper \"Riemann manifold Langevin and Hamiltonian Monte Carlo methods\"\nby B. Calderhead and M. Girolami, presented in front of the Royal Statistical\nSociety on October 13th 2010 and to appear in the Journal of the Royal\nStatistical Society Series B. The first comment establishes a parallel and\npossible interactions with Adaptive Monte Carlo methods. The second comment\nexposes a detailed study of Riemannian Manifold Hamiltonian Monte Carlo (RMHMC)\nfor a weakly identifiable model presenting a strong ridge in its geometry.\n", "versions": [{"version": "v1", "created": "Sat, 30 Oct 2010 08:38:15 GMT"}], "update_date": "2010-11-02", "authors_parsed": [["Bornn", "Luke", ""], ["Cornebise", "Julien", ""], ["Peters", "Gareth W.", ""]]}, {"id": "1011.0471", "submitter": "Sean Simpson", "authors": "Lloyd J. Edwards", "title": "A Note on an R^2 Measure for Fixed Effects in the Generalized Linear\n  Mixed Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the LRT statistic, a model R^2 is proposed for the generalized linear\nmixed model for assessing the association between the correlated outcomes and\nfixed effects. The R^2 compares the full model to a null model with all fixed\neffects deleted.\n", "versions": [{"version": "v1", "created": "Mon, 1 Nov 2010 23:36:50 GMT"}], "update_date": "2010-11-03", "authors_parsed": [["Edwards", "Lloyd J.", ""]]}, {"id": "1011.0595", "submitter": "Vanessa Didelez", "authors": "Vanessa Didelez, Sha Meng, Nuala A. Sheehan", "title": "Assumptions of IV Methods for Observational Epidemiology", "comments": "Published in at http://dx.doi.org/10.1214/09-STS316 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 1, 22-40", "doi": "10.1214/09-STS316", "report-no": "IMS-STS-STS316", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable (IV) methods are becoming increasingly popular as they\nseem to offer the only viable way to overcome the problem of unobserved\nconfounding in observational studies. However, some attention has to be paid to\nthe details, as not all such methods target the same causal parameters and some\nrely on more restrictive parametric assumptions than others. We therefore\ndiscuss and contrast the most common IV approaches with relevance to typical\napplications in observational epidemiology. Further, we illustrate and compare\nthe asymptotic bias of these IV estimators when underlying assumptions are\nviolated in a numerical study. One of our conclusions is that all IV methods\nencounter problems in the presence of effect modification by unobserved\nconfounders. Since this can never be ruled out for sure, we recommend that\npractical applications of IV estimators be accompanied routinely by a\nsensitivity analysis.\n", "versions": [{"version": "v1", "created": "Tue, 2 Nov 2010 12:23:21 GMT"}], "update_date": "2010-11-03", "authors_parsed": [["Didelez", "Vanessa", ""], ["Meng", "Sha", ""], ["Sheehan", "Nuala A.", ""]]}, {"id": "1011.0662", "submitter": "Nikolai Gagunashvili", "authors": "N.D. Gagunashvili", "title": "Parametric fitting of data obtained from detectors with finite\n  resolution and limited acceptance", "comments": "11 pages, two figures", "journal-ref": "Nuclear Instruments and Methods A 635 (2011) 86-91", "doi": "10.1016/j.nima.2010.12.230", "report-no": null, "categories": "physics.data-an astro-ph.IM hep-ex nucl-ex stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A goodness-of-fit test for the fitting of a parametric model to data obtained\nfrom a detector with finite resolution and limited acceptance is proposed. The\nparameters of the model are found by minimization of a statistic that is used\nfor comparing experimental data and simulated reconstructed data. Numerical\nexamples are presented to illustrate and validate the fitting procedure.\n", "versions": [{"version": "v1", "created": "Tue, 2 Nov 2010 16:16:02 GMT"}, {"version": "v2", "created": "Wed, 5 Jan 2011 16:25:53 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Gagunashvili", "N. D.", ""]]}, {"id": "1011.0723", "submitter": "Ariel Caticha", "authors": "Ariel Caticha", "title": "Entropic Inference", "comments": "Presented at MaxEnt 2010, the 30th International Workshop on Bayesian\n  Inference and Maximum Entropy Methods in Science and Engineering (July 4-9,\n  2010, Chamonix, France)", "journal-ref": null, "doi": "10.1063/1.3573619", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this tutorial we review the essential arguments behing entropic inference.\nWe focus on the epistemological notion of information and its relation to the\nBayesian beliefs of rational agents. The problem of updating from a prior to a\nposterior probability distribution is tackled through an eliminative induction\nprocess that singles out the logarithmic relative entropy as the unique tool\nfor inference. The resulting method of Maximum relative Entropy (ME), includes\nas special cases both MaxEnt and Bayes' rule, and therefore unifies the two\nthemes of these workshops -- the Maximum Entropy and the Bayesian methods --\ninto a single general inference scheme.\n", "versions": [{"version": "v1", "created": "Tue, 2 Nov 2010 19:58:44 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Caticha", "Ariel", ""]]}, {"id": "1011.0810", "submitter": "Nell Sedransk", "authors": "Nell Sedransk, Lawrence H. Cox, Deborah Nolan, Keith Soper, Cliff\n  Spiegelman, Linda J. Young, Katrina L. Kelner, Robert A. Moffitt, Ani Thakar,\n  Jordan Raddick, Edward J. Ungvarsky, Richard W. Carlson, Rolf Apweiler", "title": "Make Research Data Public? -- Not Always so Simple: A Dialogue for\n  Statisticians and Science Editors", "comments": "Published in at http://dx.doi.org/10.1214/10-STS320 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 1, 41-50", "doi": "10.1214/10-STS320", "report-no": "IMS-STS-STS320", "categories": "stat.ME cs.DL physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Putting data into the public domain is not the same thing as making those\ndata accessible for intelligent analysis. A distinguished group of editors and\nexperts who were already engaged in one way or another with the issues inherent\nin making research data public came together with statisticians to initiate a\ndialogue about policies and practicalities of requiring published research to\nbe accompanied by publication of the research data. This dialogue carried\nbeyond the broad issues of the advisability, the intellectual integrity, the\nscientific exigencies to the relevance of these issues to statistics as a\ndiscipline and the relevance of statistics, from inference to modeling to data\nexploration, to science and social science policies on these issues.\n", "versions": [{"version": "v1", "created": "Wed, 3 Nov 2010 07:28:00 GMT"}], "update_date": "2016-11-27", "authors_parsed": [["Sedransk", "Nell", ""], ["Cox", "Lawrence H.", ""], ["Nolan", "Deborah", ""], ["Soper", "Keith", ""], ["Spiegelman", "Cliff", ""], ["Young", "Linda J.", ""], ["Kelner", "Katrina L.", ""], ["Moffitt", "Robert A.", ""], ["Thakar", "Ani", ""], ["Raddick", "Jordan", ""], ["Ungvarsky", "Edward J.", ""], ["Carlson", "Richard W.", ""], ["Apweiler", "Rolf", ""]]}, {"id": "1011.0819", "submitter": "Ryan Martin", "authors": "Ryan Martin, Jianchun Zhang, Chuanhai Liu", "title": "Dempster--Shafer Theory and Statistical Inference with Weak Beliefs", "comments": "Published in at http://dx.doi.org/10.1214/10-STS322 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 1, 72-87", "doi": "10.1214/10-STS322", "report-no": "IMS-STS-STS322", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dempster--Shafer (DS) theory is a powerful tool for probabilistic\nreasoning based on a formal calculus for combining evidence. DS theory has been\nwidely used in computer science and engineering applications, but has yet to\nreach the statistical mainstream, perhaps because the DS belief functions do\nnot satisfy long-run frequency properties. Recently, two of the authors\nproposed an extension of DS, called the weak belief (WB) approach, that can\nincorporate desirable frequency properties into the DS framework by\nsystematically enlarging the focal elements. The present paper reviews and\nextends this WB approach. We present a general description of WB in the context\nof inferential models, its interplay with the DS calculus, and the maximal\nbelief solution. New applications of the WB method in two high-dimensional\nhypothesis testing problems are given. Simulations show that the WB procedures,\nsuitably calibrated, perform well compared to popular classical methods. Most\nimportantly, the WB approach combines the probabilistic reasoning of DS with\nthe desirable frequency properties of classical statistics.\n", "versions": [{"version": "v1", "created": "Wed, 3 Nov 2010 08:32:03 GMT"}], "update_date": "2010-11-04", "authors_parsed": [["Martin", "Ryan", ""], ["Zhang", "Jianchun", ""], ["Liu", "Chuanhai", ""]]}, {"id": "1011.1079", "submitter": "Kosuke Imai", "authors": "Kosuke Imai, Luke Keele, Teppei Yamamoto", "title": "Identification, Inference and Sensitivity Analysis for Causal Mediation\n  Effects", "comments": "Published in at http://dx.doi.org/10.1214/10-STS321 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 1, 51-71", "doi": "10.1214/10-STS321", "report-no": "IMS-STS-STS321", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis is routinely conducted by applied researchers in a\nvariety of disciplines. The goal of such an analysis is to investigate\nalternative causal mechanisms by examining the roles of intermediate variables\nthat lie in the causal paths between the treatment and outcome variables. In\nthis paper we first prove that under a particular version of sequential\nignorability assumption, the average causal mediation effect (ACME) is\nnonparametrically identified. We compare our identification assumption with\nthose proposed in the literature. Some practical implications of our\nidentification result are also discussed. In particular, the popular estimator\nbased on the linear structural equation model (LSEM) can be interpreted as an\nACME estimator once additional parametric assumptions are made. We show that\nthese assumptions can easily be relaxed within and outside of the LSEM\nframework and propose simple nonparametric estimation strategies. Second, and\nperhaps most importantly, we propose a new sensitivity analysis that can be\neasily implemented by applied researchers within the LSEM framework. Like the\nexisting identifying assumptions, the proposed sequential ignorability\nassumption may be too strong in many applied settings. Thus, sensitivity\nanalysis is essential in order to examine the robustness of empirical findings\nto the possible existence of an unmeasured confounder. Finally, we apply the\nproposed methods to a randomized experiment from political psychology. We also\nmake easy-to-use software available to implement the proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Nov 2010 08:32:07 GMT"}], "update_date": "2010-11-05", "authors_parsed": [["Imai", "Kosuke", ""], ["Keele", "Luke", ""], ["Yamamoto", "Teppei", ""]]}, {"id": "1011.1098", "submitter": "Carlos M. Carvalho", "authors": "Carlos M. Carvalho, Michael S. Johannes, Hedibert F. Lopes, Nicholas\n  G. Polson", "title": "Particle Learning and Smoothing", "comments": "Published in at http://dx.doi.org/10.1214/10-STS325 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 1, 88-106", "doi": "10.1214/10-STS325", "report-no": "IMS-STS-STS325", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle learning (PL) provides state filtering, sequential parameter\nlearning and smoothing in a general class of state space models. Our approach\nextends existing particle methods by incorporating the estimation of static\nparameters via a fully-adapted filter that utilizes conditional sufficient\nstatistics for parameters and/or states as particles. State smoothing in the\npresence of parameter uncertainty is also solved as a by-product of PL. In a\nnumber of examples, we show that PL outperforms existing particle filtering\nalternatives and proves to be a competitor to MCMC.\n", "versions": [{"version": "v1", "created": "Thu, 4 Nov 2010 10:17:09 GMT"}], "update_date": "2010-11-05", "authors_parsed": [["Carvalho", "Carlos M.", ""], ["Johannes", "Michael S.", ""], ["Lopes", "Hedibert F.", ""], ["Polson", "Nicholas G.", ""]]}, {"id": "1011.1139", "submitter": "Christopher J. Paciorek", "authors": "Christopher J. Paciorek", "title": "The Importance of Scale for Spatial-Confounding Bias and Precision of\n  Spatial Regression Estimators", "comments": "Published in at http://dx.doi.org/10.1214/10-STS326 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 1, 107-125", "doi": "10.1214/10-STS326", "report-no": "IMS-STS-STS326", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residuals in regression models are often spatially correlated. Prominent\nexamples include studies in environmental epidemiology to understand the\nchronic health effects of pollutants. I consider the effects of residual\nspatial structure on the bias and precision of regression coefficients,\ndeveloping a simple framework in which to understand the key issues and derive\ninformative analytic results. When unmeasured confounding introduces spatial\nstructure into the residuals, regression models with spatial random effects and\nclosely-related models such as kriging and penalized splines are biased, even\nwhen the residual variance components are known. Analytic and simulation\nresults show how the bias depends on the spatial scales of the covariate and\nthe residual: one can reduce bias by fitting a spatial model only when there is\nvariation in the covariate at a scale smaller than the scale of the unmeasured\nconfounding. I also discuss how the scales of the residual and the covariate\naffect efficiency and uncertainty estimation when the residuals are independent\nof the covariate. In an application on the association between black carbon\nparticulate matter air pollution and birth weight, controlling for large-scale\nspatial variation appears to reduce bias from unmeasured confounders, while\nincreasing uncertainty in the estimated pollution effect.\n", "versions": [{"version": "v1", "created": "Thu, 4 Nov 2010 12:37:00 GMT"}], "update_date": "2010-11-05", "authors_parsed": [["Paciorek", "Christopher J.", ""]]}, {"id": "1011.1253", "submitter": "Li Ma", "authors": "Li Ma and Wing H. Wong", "title": "Coupling optional P\\'olya trees and the two sample problem", "comments": "44 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing and characterizing the difference between two data samples is of\nfundamental interest in statistics. Existing methods such as Kolmogorov-Smirnov\nand Cramer-von-Mises tests do not scale well as the dimensionality increases\nand provides no easy way to characterize the difference should it exist. In\nthis work, we propose a theoretical framework for inference that addresses\nthese challenges in the form of a prior for Bayesian nonparametric analysis.\nThe new prior is constructed based on a random-partition-and-assignment\nprocedure similar to the one that defines the standard optional P\\'olya tree\ndistribution, but has the ability to generate multiple random distributions\njointly. These random probability distributions are allowed to \"couple\", that\nis to have the same conditional distribution, on subsets of the sample space.\nWe show that this \"coupling optional P\\'olya tree\" prior provides a convenient\nand effective way for both the testing of two sample difference and the\nlearning of the underlying structure of the difference. In addition, we discuss\nsome practical issues in the computational implementation of this prior and\nprovide several numerical examples to demonstrate its work.\n", "versions": [{"version": "v1", "created": "Thu, 4 Nov 2010 19:54:34 GMT"}, {"version": "v2", "created": "Sun, 6 Feb 2011 00:45:44 GMT"}, {"version": "v3", "created": "Mon, 7 Mar 2011 19:11:32 GMT"}, {"version": "v4", "created": "Tue, 22 Mar 2011 08:38:28 GMT"}], "update_date": "2011-03-23", "authors_parsed": [["Ma", "Li", ""], ["Wong", "Wing H.", ""]]}, {"id": "1011.1373", "submitter": "Minh-Ngoc Tran", "authors": "Minh-Ngoc Tran", "title": "The Loss Rank Criterion for Variable Selection in Linear Regression\n  Analysis", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": "10.1111/j.1467-9469.2011.00732.x", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Lasso and other regularization procedures are attractive methods for variable\nselection, subject to a proper choice of shrinkage parameter. Given a set of\npotential subsets produced by a regularization algorithm, a consistent model\nselection criterion is proposed to select the best one among this preselected\nset. The approach leads to a fast and efficient procedure for variable\nselection, especially in high-dimensional settings. Model selection consistency\nof the suggested criterion is proven when the number of covariates d is fixed.\nSimulation studies suggest that the criterion still enjoys model selection\nconsistency when d is much larger than the sample size. The simulations also\nshow that our approach for variable selection works surprisingly well in\ncomparison with existing competitors. The method is also applied to a real data\nset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Nov 2010 11:54:24 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Tran", "Minh-Ngoc", ""]]}, {"id": "1011.1379", "submitter": "Minh-Ngoc Tran", "authors": "Minh-Ngoc Tran and Marcus Hutter", "title": "Model Selection by Loss Rank for Classification and Unsupervised\n  Learning", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Hutter (2007) recently introduced the loss rank principle (LoRP) as a\ngeneralpurpose principle for model selection. The LoRP enjoys many attractive\nproperties and deserves further investigations. The LoRP has been well-studied\nfor regression framework in Hutter and Tran (2010). In this paper, we study the\nLoRP for classification framework, and develop it further for model selection\nproblems in unsupervised learning where the main interest is to describe the\nassociations between input measurements, like cluster analysis or graphical\nmodelling. Theoretical properties and simulation studies are presented.\n", "versions": [{"version": "v1", "created": "Fri, 5 Nov 2010 12:21:12 GMT"}], "update_date": "2010-11-08", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Hutter", "Marcus", ""]]}, {"id": "1011.1533", "submitter": "Sta\\v{s}a Milojevi\\'c", "authors": "Sta\\v{s}a Milojevi\\'c", "title": "Power-law Distributions in Information Science - Making the Case for\n  Logarithmic Binning", "comments": "Accepted for publication in JASIST", "journal-ref": null, "doi": "10.1002/asi.21426", "report-no": null, "categories": "physics.soc-ph cs.DL stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest partial logarithmic binning as the method of choice for uncovering\nthe nature of many distributions encountered in information science (IS).\nLogarithmic binning retrieves information and trends \"not visible\" in noisy\npower-law tails. We also argue that obtaining the exponent from logarithmically\nbinned data using a simple least square method is in some cases warranted in\naddition to methods such as the maximum likelihood. We also show why often used\ncumulative distributions can make it difficult to distinguish noise from\ngenuine features, and make it difficult to obtain an accurate power-law\nexponent of the underlying distribution. The treatment is non-technical, aimed\nat IS researchers with little or no background in mathematics.\n", "versions": [{"version": "v1", "created": "Sat, 6 Nov 2010 01:47:41 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Milojevi\u0107", "Sta\u0161a", ""]]}, {"id": "1011.1703", "submitter": "Patrick Perry", "authors": "Patrick O. Perry and Patrick J. Wolfe", "title": "Point process modeling for directed interaction networks", "comments": "36 pages, 13 figures; includes supplementary material", "journal-ref": "Journal of the Royal Statistical Society, Series B, 2013", "doi": "10.1111/rssb.12013", "report-no": null, "categories": "stat.ME cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data often take the form of repeated interactions between senders and\nreceivers tabulated over time. A primary question to ask of such data is which\ntraits and behaviors are predictive of interaction. To answer this question, a\nmodel is introduced for treating directed interactions as a multivariate point\nprocess: a Cox multiplicative intensity model using covariates that depend on\nthe history of the process. Consistency and asymptotic normality are proved for\nthe resulting partial-likelihood-based estimators under suitable regularity\nconditions, and an efficient fitting procedure is described. Multicast\ninteractions--those involving a single sender but multiple receivers--are\ntreated explicitly. The resulting inferential framework is then employed to\nmodel message sending behavior in a corporate e-mail network. The analysis\ngives a precise quantification of which static shared traits and dynamic\nnetwork effects are predictive of message recipient selection.\n", "versions": [{"version": "v1", "created": "Mon, 8 Nov 2010 03:20:54 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2011 17:10:47 GMT"}, {"version": "v3", "created": "Tue, 20 Nov 2012 20:35:36 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Perry", "Patrick O.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "1011.1761", "submitter": "Francois Caron", "authors": "Francois Caron (INRIA Bordeaux - Sud-Ouest, IMB), Arnaud Doucet", "title": "Efficient Bayesian Inference for Generalized Bradley-Terry Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bradley-Terry model is a popular approach to describe probabilities of\nthe possible outcomes when elements of a set are repeatedly compared with one\nanother in pairs. It has found many applications including animal behaviour,\nchess ranking and multiclass classification. Numerous extensions of the basic\nmodel have also been proposed in the literature including models with ties,\nmultiple comparisons, group comparisons and random graphs. From a computational\npoint of view, Hunter (2004) has proposed efficient iterative MM\n(minorization-maximization) algorithms to perform maximum likelihood estimation\nfor these generalized Bradley-Terry models whereas Bayesian inference is\ntypically performed using MCMC (Markov chain Monte Carlo) algorithms based on\ntailored Metropolis-Hastings (M-H) proposals. We show here that these MM\\\nalgorithms can be reinterpreted as special instances of\nExpectation-Maximization (EM) algorithms associated to suitable sets of latent\nvariables and propose some original extensions. These latent variables allow us\nto derive simple Gibbs samplers for Bayesian inference. We demonstrate\nexperimentally the efficiency of these algorithms on a variety of applications.\n", "versions": [{"version": "v1", "created": "Mon, 8 Nov 2010 10:40:19 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Caron", "Francois", "", "INRIA Bordeaux - Sud-Ouest, IMB"], ["Doucet", "Arnaud", ""]]}, {"id": "1011.1937", "submitter": "Pavel  Krivitsky", "authors": "Pavel N. Krivitsky (Department of Statistics, Penn State University,\n  University Park) and Mark S. Handcock (Department of Statistics, University\n  of California, Los Angeles)", "title": "A Separable Model for Dynamic Networks", "comments": "28 pages (including a 4-page appendix); a substantial rewrite, with\n  many corrections, changes in terminology, and a different analysis for the\n  example", "journal-ref": "Journal of the Royal Statistical Society, Series B 76(1) (2014)\n  29-46", "doi": "10.1111/rssb.12014", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of dynamic networks --- networks that evolve over time --- have\nmanifold applications. We develop a discrete-time generative model for social\nnetwork evolution that inherits the richness and flexibility of the class of\nexponential-family random graph models. The model --- a Separable Temporal ERGM\n(STERGM) --- facilitates separable modeling of the tie duration distributions\nand the structural dynamics of tie formation. We develop likelihood-based\ninference for the model, and provide computational algorithms for maximum\nlikelihood estimation. We illustrate the interpretability of the model in\nanalyzing a longitudinal network of friendship ties within a school.\n", "versions": [{"version": "v1", "created": "Mon, 8 Nov 2010 22:47:13 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2012 03:23:20 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Krivitsky", "Pavel N.", "", "Department of Statistics, Penn State University,\n  University Park"], ["Handcock", "Mark S.", "", "Department of Statistics, University\n  of California, Los Angeles"]]}, {"id": "1011.2553", "submitter": "Luke Bornn", "authors": "Luke Bornn, Gavin Shaddick, and James V Zidek", "title": "Modeling Non-Stationary Processes Through Dimension Expansion", "comments": null, "journal-ref": "Bornn, L., Shaddick, G., Zidek, J. (2012) Modelling Nonstationary\n  Processes Through Dimension Expansion. Journal of the American Statistical\n  Association Vol. 107, No. 497, 281-289", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach to modeling nonstationary spatial\nfields. The proposed method works by expanding the geographic plane over which\nthese processes evolve into higher dimensional spaces, transforming and\nclarifying complex patterns in the physical plane. By combining aspects of\nmulti-dimensional scaling, group lasso, and latent variables models, a\ndimensionally sparse projection is found in which the originally nonstationary\nfield exhibits stationarity. Following a comparison with existing methods in a\nsimulated environment, dimension expansion is studied on a classic test-bed\ndata set historically used to study nonstationary models. Following this, we\nexplore the use of dimension expansion in modeling air pollution in the United\nKingdom, a process known to be strongly influenced by rural/urban effects,\namongst others, which gives rise to a nonstationary field.\n", "versions": [{"version": "v1", "created": "Thu, 11 Nov 2010 04:41:42 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2011 21:37:56 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Bornn", "Luke", ""], ["Shaddick", "Gavin", ""], ["Zidek", "James V", ""]]}, {"id": "1011.2624", "submitter": "Marcela Svarc", "authors": "Ricardo Fraiman, Badih Ghattas and Marcela Svarc", "title": "Clustering using Unsupervised Binary Trees: CUBT", "comments": "This paper has been withdrawn by the author due to an involuntary\n  double submission to the arxiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We herein introduce a new method of interpretable clustering that uses\nunsupervised binary trees. It is a three-stage procedure, the first stage of\nwhich entails a series of recursive binary splits to reduce the heterogeneity\nof the data within the new subsamples. During the second stage (pruning),\nconsideration is given to whether adjacent nodes can be aggregated. Finally,\nduring the third stage (joining), similar clusters are joined together, even if\nthey do not share the same parent originally. Consistency results are obtained,\nand the procedure is used on simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Nov 2010 12:12:56 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2011 14:00:46 GMT"}], "update_date": "2011-10-28", "authors_parsed": [["Fraiman", "Ricardo", ""], ["Ghattas", "Badih", ""], ["Svarc", "Marcela", ""]]}, {"id": "1011.2649", "submitter": "Andrea Tancredi", "authors": "Andrea Tancredi, Brunero Liseo", "title": "A hierarchical Bayesian approach to record linkage and population size\n  problems", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS447 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2B, 1553-1585", "doi": "10.1214/10-AOAS447", "report-no": "IMS-AOAS-AOAS447", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and illustrate a hierarchical Bayesian approach for matching\nstatistical records observed on different occasions. We show how this model can\nbe profitably adopted both in record linkage problems and in capture--recapture\nsetups, where the size of a finite population is the real object of interest.\nThere are at least two important differences between the proposed model-based\napproach and the current practice in record linkage. First, the statistical\nmodel is built up on the actually observed categorical variables and no\nreduction (to 0--1 comparisons) of the available information takes place.\nSecond, the hierarchical structure of the model allows a two-way propagation of\nthe uncertainty between the parameter estimation step and the matching\nprocedure so that no plug-in estimates are used and the correct uncertainty is\naccounted for both in estimating the population size and in performing the\nrecord linkage. We illustrate and motivate our proposal through a real data\nexample and simulations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Nov 2010 13:33:20 GMT"}, {"version": "v2", "created": "Mon, 13 Dec 2010 14:15:12 GMT"}, {"version": "v3", "created": "Thu, 28 Jul 2011 12:10:46 GMT"}], "update_date": "2011-07-29", "authors_parsed": [["Tancredi", "Andrea", ""], ["Liseo", "Brunero", ""]]}, {"id": "1011.3725", "submitter": "Paul Hahn", "authors": "P. Richard Hahn and Sayan Mukherjee and Carlos Carvalho", "title": "Predictor-dependent shrinkage for linear regression via partial factor\n  modeling", "comments": "16 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In prediction problems with more predictors than observations, it can\nsometimes be helpful to use a joint probability model, $\\pi(Y,X)$, rather than\na purely conditional model, $\\pi(Y \\mid X)$, where $Y$ is a scalar response\nvariable and $X$ is a vector of predictors. This approach is motivated by the\nfact that in many situations the marginal predictor distribution $\\pi(X)$ can\nprovide useful information about the parameter values governing the conditional\nregression. However, under very mild misspecification, this marginal\ndistribution can also lead conditional inferences astray. Here, we explore\nthese ideas in the context of linear factor models, to understand how they play\nout in a familiar setting. The resulting Bayesian model performs well across a\nwide range of covariance structures, on real and simulated data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Nov 2010 15:17:28 GMT"}], "update_date": "2010-11-17", "authors_parsed": [["Hahn", "P. Richard", ""], ["Mukherjee", "Sayan", ""], ["Carvalho", "Carlos", ""]]}, {"id": "1011.4339", "submitter": "Srikanth Jagabathula", "authors": "Vivek F. Farias and Srikanth Jagabathula and Devavrat Shah", "title": "Sparse Choice Models", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choice models, which capture popular preferences over objects of interest,\nplay a key role in making decisions whose eventual outcome is impacted by human\nchoice behavior. In most scenarios, the choice model, which can effectively be\nviewed as a distribution over permutations, must be learned from observed data.\nThe observed data, in turn, may frequently be viewed as (partial, noisy)\ninformation about marginals of this distribution over permutations. As such,\nthe search for an appropriate choice model boils down to learning a\ndistribution over permutations that is (near-)consistent with observed\ninformation about this distribution.\n  In this work, we pursue a non-parametric approach which seeks to learn a\nchoice model (i.e. a distribution over permutations) with {\\em sparsest}\npossible support, and consistent with observed data. We assume that the data\nobserved consists of noisy information pertaining to the marginals of the\nchoice model we seek to learn. We establish that {\\em any} choice model admits\na `very' sparse approximation in the sense that there exists a choice model\nwhose support is small relative to the dimension of the observed data and whose\nmarginals approximately agree with the observed marginal information. We\nfurther show that under, what we dub, `signature' conditions, such a sparse\napproximation can be found in a computationally efficiently fashion relative to\na brute force approach. An empirical study using the American Psychological\nAssociation election data-set suggests that our approach manages to unearth\nuseful structural properties of the underlying choice model using the sparse\napproximation found. Our results further suggest that the signature condition\nis a potential alternative to the recently popularized Restricted Null Space\ncondition for efficient recovery of sparse models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Nov 2010 03:51:01 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2011 07:05:07 GMT"}], "update_date": "2011-09-22", "authors_parsed": [["Farias", "Vivek F.", ""], ["Jagabathula", "Srikanth", ""], ["Shah", "Devavrat", ""]]}, {"id": "1011.4644", "submitter": "Patrick J. Wolfe", "authors": "David S. Choi, Patrick J. Wolfe, Edoardo M. Airoldi", "title": "Stochastic blockmodels with growing number of classes", "comments": "12 pages, 3 figures; revised version", "journal-ref": "Biometrika, 99:273--284, 2012", "doi": "10.1093/biomet/asr053", "report-no": null, "categories": "math.ST cs.SI stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present asymptotic and finite-sample results on the use of stochastic\nblockmodels for the analysis of network data. We show that the fraction of\nmisclassified network nodes converges in probability to zero under maximum\nlikelihood fitting when the number of classes is allowed to grow as the root of\nthe network size and the average network degree grows at least\npoly-logarithmically in this size. We also establish finite-sample confidence\nbounds on maximum-likelihood blockmodel parameter estimates from data\ncomprising independent Bernoulli random variates; these results hold uniformly\nover class assignment. We provide simulations verifying the conditions\nsufficient for our results, and conclude by fitting a logit parameterization of\na stochastic blockmodel with covariates to a network data example comprising a\ncollection of Facebook profiles, resulting in block estimates that reveal\nresidual structure.\n", "versions": [{"version": "v1", "created": "Sun, 21 Nov 2010 07:44:03 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2011 19:29:57 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Choi", "David S.", ""], ["Wolfe", "Patrick J.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1011.4832", "submitter": "Minh-Ngoc Tran", "authors": "David J. Nott, Minh-Ngoc Tran, Chenlei Leng", "title": "Variational approximation for heteroscedastic linear models and matching\n  pursuit algorithms", "comments": "35 pages, 3 figures, 4 tables", "journal-ref": null, "doi": "10.1007/s11222-011-9243-2", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Modern statistical applications involving large data sets have focused\nattention on statistical methodologies which are both efficient computationally\nand able to deal with the screening of large numbers of different candidate\nmodels. Here we consider computationally efficient variational Bayes approaches\nto inference in high-dimensional heteroscedastic linear regression, where both\nthe mean and variance are described in terms of linear functions of the\npredictors and where the number of predictors can be larger than the sample\nsize. We derive a closed form variational lower bound on the log marginal\nlikelihood useful for model selection, and propose a novel fast greedy search\nalgorithm on the model space which makes use of one step optimization updates\nto the variational lower bound in the current model for screening large numbers\nof candidate predictor variables for inclusion/exclusion in a computationally\nthrifty way. We show that the model search strategy we suggest is related to\nwidely used orthogonal matching pursuit algorithms for model search but yields\na framework for potentially extending these algorithms to more complex models.\nThe methodology is applied in simulations and in two real examples involving\nprediction for food constituents using NIR technology and prediction of disease\nprogression in diabetes.\n", "versions": [{"version": "v1", "created": "Mon, 22 Nov 2010 14:55:45 GMT"}, {"version": "v2", "created": "Wed, 24 Nov 2010 05:52:55 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2011 15:46:40 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Nott", "David J.", ""], ["Tran", "Minh-Ngoc", ""], ["Leng", "Chenlei", ""]]}, {"id": "1011.4916", "submitter": "Luo Xiao Luo Xiao", "authors": "Luo Xiao, Yingxing Li and David Ruppert", "title": "Fast Bivariate Penalized Splines: the Sandwich Smoother", "comments": "45 pages, 3 fgiures", "journal-ref": "J. R. Statist. Soc. B (2013), 75, Part 3, pp. 577-599", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast penalized spline method for bivariate smoothing. Univariate\nP-spline smoothers (Eilers and Marx, 1996) are applied simultaneously along\nboth coordinates. The new smoother has a sandwich form which suggested the name\n\"sandwich smoother\" to a referee. The sandwich smoother has a tensor product\nstructure that simplifies an asymptotic analysis and it can be fast computed.\nWe derive a local central limit theorem for the sandwich smoother, with simple\nexpressions for the asymptotic bias and variance, by showing that the sandwich\nsmoother is asymptotically equivalent to a bivariate kernel regression\nestimator with a product kernel. As far as we are aware, this is the first\ncentral limit theorem for a bivariate spline estimator of any type. Our\nsimulation study shows that the sandwich smoother is orders of magnitude faster\nto compute than other bivariate spline smoothers, even when the latter are\ncomputed using a fast GLAM (Generalized Linear Array Model) algorithm, and\ncomparable to them in terms of mean squared integrated errors. We extend the\nsandwich smoother to array data of higher dimensions, where a GLAM algorithm\nimproves the computational speed of the sandwich smoother. One important\napplication of the sandwich smoother is to estimate covariance functions in\nfunctional data analysis. In this application, our numerical results show that\nthe sandwich smoother is orders of magnitude faster than local linear\nregression. The speed of the sandwich formula is important because functional\ndata sets are becoming quite large.\n", "versions": [{"version": "v1", "created": "Mon, 22 Nov 2010 19:34:07 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2012 21:59:49 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Xiao", "Luo", ""], ["Li", "Yingxing", ""], ["Ruppert", "David", ""]]}, {"id": "1011.5226", "submitter": "M. Atakan Gurkan", "authors": "M. Atakan G\\\"urkan (Leiden Observatory)", "title": "Fractal Geometry of Angular Momentum Evolution in Near-Keplerian Systems", "comments": "5 pages, 5 figures. Accepted for publication in MNRAS Letters", "journal-ref": "MNRAS Letters 411 (2011), 56", "doi": "10.1111/j.1745-3933.2010.00994.x", "report-no": null, "categories": "astro-ph.IM astro-ph.GA stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we propose a method to study the nature of resonant relaxation\nin near-Keplerian systems. Our technique is based on measuring the fractal\ndimension of the angular momentum trails and we use it to analyze the outcome\nof N-body simulations. With our method, we can reliably determine the timescale\nfor resonant relaxation, as well as the rate of change of angular momentum in\nthis regime. We find that growth of angular momentum is more rapid than random\nwalk, but slower than linear growth. We also determine the presence of long\nterm correlations, arising from the bounds on angular momentum growth. We\ndevelop a toy model that reproduces all essential properties of angular\nmomentum evolution.\n", "versions": [{"version": "v1", "created": "Tue, 23 Nov 2010 21:00:07 GMT"}], "update_date": "2011-01-31", "authors_parsed": [["G\u00fcrkan", "M. Atakan", "", "Leiden Observatory"]]}, {"id": "1011.5298", "submitter": "Vikram Krishnamurthy", "authors": "Vikram Krishnamurthy", "title": "Bayesian Sequential Detection with Phase-Distributed Change Time and\n  Nonlinear Penalty -- A POMDP Approach", "comments": "accepted for publication in IEEE Transactions Information Theory,\n  2011", "journal-ref": "IEEE Transactions Information Theory, October 2011. Vol.57, No.10", "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the optimal decision policy for several types of Bayesian\nsequential detection problems has a threshold switching curve structure on the\nspace of posterior distributions. This is established by using lattice\nprogramming and stochastic orders in a partially observed Markov decision\nprocess (POMDP) framework. A stochastic gradient algorithm is presented to\nestimate the optimal linear approximation to this threshold curve. We\nillustrate these results by first considering quickest time detection with\nphase-type distributed change time and a variance stopping penalty. Then it is\nproved that the threshold switching curve also arises in several other Bayesian\ndecision problems such as quickest transient detection, exponential delay\n(risk-sensitive) penalties, stopping time problems in social learning, and\nmulti-agent scheduling in a changing world. Using Blackwell dominance, it is\nshown that for dynamic decision making problems, the optimal decision policy is\nlower bounded by a myopic policy. Finally, it is shown how the achievable cost\nof the optimal decision policy varies with change time distribution by imposing\na partial order on transition matrices.\n", "versions": [{"version": "v1", "created": "Wed, 24 Nov 2010 05:17:17 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2011 01:44:02 GMT"}, {"version": "v3", "created": "Sat, 14 May 2011 03:39:05 GMT"}, {"version": "v4", "created": "Mon, 13 Jun 2011 02:17:24 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Krishnamurthy", "Vikram", ""]]}, {"id": "1011.5631", "submitter": "Valderio Reisen VAR", "authors": "Valderio A. Reisen, Wilfredo Palma, Josu Arteche, Bartolomeu Zamprogno", "title": "Seasonal fractional long-memory processes. A semiparametric estimation\n  approach", "comments": "submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores seasonal and long-memory time series properties by using\nthe seasonal fractional ARIMA model when the seasonal data has one and two\nseasonal periods and short-memory counterparts. The stationarity and\ninvertibility parameter conditions are established for the model studied. To\nestimate the memory parameters, the method given in Reisen, Rodrigues and Palma\n(2006 a,b) is generalized here to deal with a time series with two seasonal\nfractional long-memory parameters. The asymptotic properties are established\nand the accuracy of the method is investigated through Monte Carlo experiments.\nThe good performance of the estimator indicates that it can be an alternative\ncompetitive procedure to estimate seasonal long-memory time series data.\nArtificial and PM10 series were considered as examples of applications of the\nproposed estimation method.\n", "versions": [{"version": "v1", "created": "Thu, 25 Nov 2010 14:15:19 GMT"}], "update_date": "2010-11-29", "authors_parsed": [["Reisen", "Valderio A.", ""], ["Palma", "Wilfredo", ""], ["Arteche", "Josu", ""], ["Zamprogno", "Bartolomeu", ""]]}, {"id": "1011.6095", "submitter": "Yang Feng", "authors": "Jianqing Fan, Yang Feng and Xin Tong", "title": "A ROAD to Classification in High Dimensional Space", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For high-dimensional classification, it is well known that naively performing\nthe Fisher discriminant rule leads to poor results due to diverging spectra and\nnoise accumulation. Therefore, researchers proposed independence rules to\ncircumvent the diverse spectra, and sparse independence rules to mitigate the\nissue of noise accumulation. However, in biological applications, there are\noften a group of correlated genes responsible for clinical outcomes, and the\nuse of the covariance information can significantly reduce misclassification\nrates. The extent of such error rate reductions is unveiled by comparing the\nmisclassification rates of the Fisher discriminant rule and the independence\nrule. To materialize the gain based on finite samples, a Regularized Optimal\nAffine Discriminant (ROAD) is proposed based on a covariance penalty. ROAD\nselects an increasing number of features as the penalization relaxes. Further\nbenefits can be achieved when a screening method is employed to narrow the\nfeature pool before hitting the ROAD. An efficient Constrained Coordinate\nDescent algorithm (CCD) is also developed to solve the associated optimization\nproblems. Sampling properties of oracle type are established. Simulation\nstudies and real data analysis support our theoretical results and demonstrate\nthe advantages of the new classification procedure under a variety of\ncorrelation structures. A delicate result on continuous piecewise linear\nsolution path for the ROAD optimization problem at the population level\njustifies the linear interpolation of the CCD algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 28 Nov 2010 22:04:28 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2011 17:31:30 GMT"}], "update_date": "2011-11-10", "authors_parsed": [["Fan", "Jianqing", ""], ["Feng", "Yang", ""], ["Tong", "Xin", ""]]}, {"id": "1011.6217", "submitter": "Chris Sherlock", "authors": "Chris Sherlock, Paul Fearnhead, Gareth O. Roberts", "title": "The Random Walk Metropolis: Linking Theory and Practice Through a Case\n  Study", "comments": "Published in at http://dx.doi.org/10.1214/10-STS327 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 2, 172-190", "doi": "10.1214/10-STS327", "report-no": "IMS-STS-STS327", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random walk Metropolis (RWM) is one of the most common Markov chain Monte\nCarlo algorithms in practical use today. Its theoretical properties have been\nextensively explored for certain classes of target, and a number of results\nwith important practical implications have been derived. This article draws\ntogether a selection of new and existing key results and concepts and describes\ntheir implications. The impact of each new idea on algorithm efficiency is\ndemonstrated for the practical example of the Markov modulated Poisson process\n(MMPP). A reparameterization of the MMPP which leads to a highly efficient\nRWM-within-Gibbs algorithm in certain circumstances is also presented.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 12:49:32 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Sherlock", "Chris", ""], ["Fearnhead", "Paul", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "1011.6233", "submitter": "Francisco Kitaura", "authors": "Francisco-Shu Kitaura, Simona Gallerani and Andrea Ferrara", "title": "Multiscale Inference of Matter Fields and Baryon Acoustic Oscillations\n  from the Ly-alpha Forest", "comments": "14 pages, 7 figures; accepted in MNRAS 2011 October 12, in original\n  form 2010 November 29, Publication Date: 02/2012", "journal-ref": null, "doi": "10.1111/j.1365-2966.2011.19997.x", "report-no": null, "categories": "astro-ph.CO math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Bayesian method for the joint reconstruction of\ncosmological matter density fields, peculiar velocities and power-spectra in\nthe quasi-nonlinear regime. We study its applicability to the Ly-alpha forest\nbased on multiple quasar absorption spectra. Our approach to this problem\nincludes a multiscale, nonlinear, two-step scheme since the statistics\ndescribing the matter distribution depends on scale, being strongly\nnon-Gaussian on small scales (< 0.1 h^{-1} Mpc) and closely lognormal on scales\n>~10 h^{-1} Mpc. The first step consists on performing 1D highly resolved\nmatter density reconstructions along the line-of-sight towards z~2-3 quasars\nbased on an arbitrary non-Gaussian univariate model for matter statistics. The\nsecond step consists on Gibbs-sampling based on conditional PDFs. The matter\ndensity field is sampled in real space with Hamiltonian-sampling using the\nPoisson/Gamma-lognormal model, while redshift distortions are corrected with\nlinear Lagrangian perturbation theory. The power-spectrum of the lognormal\ntransformed variable which is Gaussian distributed (and thus close to the\nlinear regime) can consistently be sampled with the inverse Gamma distribution\nfunction. We test our method through numerical N-body simulations with a\ncomputational volume large enough (> 1 h^{-3} Gpc^3) to show that the linear\npower-spectra are nicely recovered over scales larger than >~20 h^{-1} Mpc,\ni.e. the relevant range where features imprinted by the baryon-acoustics\noscillations (BAOs) appear.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 14:09:22 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2011 14:32:01 GMT"}, {"version": "v3", "created": "Wed, 27 Jun 2012 12:11:03 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Kitaura", "Francisco-Shu", ""], ["Gallerani", "Simona", ""], ["Ferrara", "Andrea", ""]]}, {"id": "1011.6240", "submitter": "Ying Kuen Cheung", "authors": "Ying Kuen Cheung", "title": "Stochastic Approximation and Modern Model-Based Designs for Dose-Finding\n  Clinical Trials", "comments": "Published in at http://dx.doi.org/10.1214/10-STS334 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 2, 191-201", "doi": "10.1214/10-STS334", "report-no": "IMS-STS-STS334", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1951 Robbins and Monro published the seminal article on stochastic\napproximation and made a specific reference to its application to the\n\"estimation of a quantal using response, nonresponse data.\" Since the 1990s,\nstatistical methodology for dose-finding studies has grown into an active area\nof research. The dose-finding problem is at its core a percentile estimation\nproblem and is in line with what the Robbins--Monro method sets out to solve.\nIn this light, it is quite surprising that the dose-finding literature has\ndeveloped rather independently of the older stochastic approximation\nliterature. The fact that stochastic approximation has seldom been used in\nactual clinical studies stands in stark contrast with its constant application\nin engineering and finance. In this article, I explore similarities and\ndifferences between the dose-finding and the stochastic approximation\nliteratures. This review also sheds light on the present and future relevance\nof stochastic approximation to dose-finding clinical trials. Such connections\nwill in turn steer dose-finding methodology on a rigorous course and extend its\nability to handle increasingly complex clinical situations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 14:29:19 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Cheung", "Ying Kuen", ""]]}, {"id": "1011.6251", "submitter": "John O'Quigley", "authors": "John O'Quigley, Mark Conaway", "title": "Continual Reassessment and Related Dose-Finding Designs", "comments": "Published in at http://dx.doi.org/10.1214/10-STS332 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 2, 202-216", "doi": "10.1214/10-STS332", "report-no": "IMS-STS-STS332", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last twenty years there have been considerable methodological\ndevelopments in the design and analysis of Phase 1, Phase 2 and Phase 1/2\ndose-finding studies. Many of these developments are related to the continual\nreassessment method (CRM), first introduced by O'Quigley, Pepe and Fisher\n(\\citeyearQPF1990). CRM models have proven themselves to be of practical use\nand, in this discussion, we investigate the basic approach, some connections to\nother methods, some generalizations, as well as further applications of the\nmodel. We obtain some new results which can provide guidance in practice.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 14:56:50 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["O'Quigley", "John", ""], ["Conaway", "Mark", ""]]}, {"id": "1011.6298", "submitter": "Jie Peng", "authors": "Owen Carmichael, Jun Chen, Debashis Paul and Jie Peng", "title": "Geometric kernel smoothing of tensor fields", "comments": "14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a kernel smoothing approach for denoising a tensor\nfield. Particularly, both simulation studies and theoretical analysis are\nconducted to understand the effects of the noise structure and the structure of\nthe tensor field on the performance of different smoothers arising from using\ndifferent metrics, viz., Euclidean, log-Euclidean and affine invariant metrics.\nWe also study the Rician noise model and compare two regression estimators of\ndiffusion tensors based on raw diffusion weighted imaging data at each voxel.\n", "versions": [{"version": "v1", "created": "Mon, 29 Nov 2010 17:18:34 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Carmichael", "Owen", ""], ["Chen", "Jun", ""], ["Paul", "Debashis", ""], ["Peng", "Jie", ""]]}, {"id": "1011.6458", "submitter": "Javier Rojo", "authors": "J. Rojo and R. C. Ott", "title": "Testing for tail behavior using extreme spacings", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methodologies to test hypotheses about the tail-heaviness of an underlying\ndistribution are introduced based on results of Rojo (1996) using the limiting\nbehavior of the extreme spacings. The tests are consistent and have point-wise\nrobust levels in the sense of Lehmann (2005) and Lehmann and Loh (1990).\nSimulation results based on these new methodologies indicate that the tests\nexhibit good control of the probability of Type I error and have good power\nproperties for finite sample sizes. The tests are compared with a test proposed\nby Bryson (1974) and it is seen that, although Bryson's test is competitive\nwith the tests proposed here, Bryson's test does not have point-wise robust\nlevels. The operating characteristics of the tests are also explored when the\ndata is blocked. It turns out that the power increases substantially by\nblocking. The methodology is illustrated by analyzing various data sets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Nov 2010 05:08:09 GMT"}], "update_date": "2010-12-01", "authors_parsed": [["Rojo", "J.", ""], ["Ott", "R. C.", ""]]}, {"id": "1011.6479", "submitter": "Mourad Tighiouart", "authors": "Mourad Tighiouart, Andr\\'e Rogatko", "title": "Dose Finding with Escalation with Overdose Control (EWOC) in Cancer\n  Clinical Trials", "comments": "Published in at http://dx.doi.org/10.1214/10-STS333 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 2, 217-226", "doi": "10.1214/10-STS333", "report-no": "IMS-STS-STS333", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the major objective in phase I trials is to identify a\nworking-dose for subsequent studies, whereas the major endpoint in phase II and\nIII trials is treatment efficacy. The dose sought is typically referred to as\nthe maximum tolerated dose (MTD). Several statistical methodologies have been\nproposed to select the MTD in cancer phase I trials. In this manuscript, we\nfocus on a Bayesian adaptive design, known as escalation with overdose control\n(EWOC). Several aspects of this design are discussed, including large sample\nproperties of the sequence of doses selected in the trial, choice of prior\ndistributions, and use of covariates. The methodology is exemplified with\nreal-life examples of cancer phase I trials. In particular, we show in the\nrecently completed ABR-217620 (naptumomab estafenatox) trial that omitting an\nimportant predictor of toxicity when dose assignments to cancer patients are\ndetermined results in a high percent of patients experiencing severe side\neffects and a significant proportion treated at sub-optimal doses.\n", "versions": [{"version": "v1", "created": "Tue, 30 Nov 2010 08:31:06 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["Tighiouart", "Mourad", ""], ["Rogatko", "Andr\u00e9", ""]]}, {"id": "1011.6494", "submitter": "Peter F. Thall", "authors": "Peter F. Thall", "title": "Bayesian Models and Decision Algorithms for Complex Early Phase Clinical\n  Trials", "comments": "Published in at http://dx.doi.org/10.1214/09-STS315 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 2, 227-244", "doi": "10.1214/09-STS315", "report-no": "IMS-STS-STS315", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An early phase clinical trial is the first step in evaluating the effects in\nhumans of a potential new anti-disease agent or combination of agents. Usually\ncalled \"phase I\" or \"phase I/II\" trials, these experiments typically have the\nnominal scientific goal of determining an acceptable dose, most often based on\nadverse event probabilities. This arose from a tradition of phase I trials to\nevaluate cytotoxic agents for treating cancer, although some methods may be\napplied in other medical settings, such as treatment of stroke or immunological\ndiseases. Most modern statistical designs for early phase trials include\nmodel-based, outcome-adaptive decision rules that choose doses for successive\npatient cohorts based on data from previous patients in the trial. Such designs\nhave seen limited use in clinical practice, however, due to their complexity,\nthe requirement of intensive, computer-based data monitoring, and the medical\ncommunity's resistance to change. Still, many actual applications of\nmodel-based outcome-adaptive designs have been remarkably successful in terms\nof both patient benefit and scientific outcome. In this paper I will review\nseveral Bayesian early phase trial designs that were tailored to accommodate\nspecific complexities of the treatment regime and patient outcomes in\nparticular clinical settings.\n", "versions": [{"version": "v1", "created": "Tue, 30 Nov 2010 09:25:14 GMT"}], "update_date": "2010-12-01", "authors_parsed": [["Thall", "Peter F.", ""]]}, {"id": "1011.6509", "submitter": "Jay Bartroff", "authors": "Jay Bartroff, Tze Leung Lai", "title": "Approximate Dynamic Programming and Its Applications to the Design of\n  Phase I Cancer Trials", "comments": "Published in at http://dx.doi.org/10.1214/10-STS317 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 2, 245-257", "doi": "10.1214/10-STS317", "report-no": "IMS-STS-STS317", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal design of a Phase I cancer trial can be formulated as a stochastic\noptimization problem. By making use of recent advances in approximate dynamic\nprogramming to tackle the problem, we develop an approximation of the Bayesian\noptimal design. The resulting design is a convex combination of a \"treatment\"\ndesign, such as Babb et al.'s (1998) escalation with overdose control, and a\n\"learning\" design, such as Haines et al.'s (2003) $c$-optimal design, thus\ndirectly addressing the treatment versus experimentation dilemma inherent in\nPhase I trials and providing a simple and intuitive design for clinical use.\nComputational details are given and the proposed design is compared to existing\ndesigns in a simulation study. The design can also be readily modified to\ninclude a first stage that cautiously escalates doses similarly to traditional\nnonparametric step-up/down schemes, while validating the Bayesian parametric\nmodel for the efficient model-based design in the second stage.\n", "versions": [{"version": "v1", "created": "Tue, 30 Nov 2010 10:08:40 GMT"}], "update_date": "2010-12-01", "authors_parsed": [["Bartroff", "Jay", ""], ["Lai", "Tze Leung", ""]]}, {"id": "1011.6649", "submitter": "John Hughes", "authors": "John Hughes and Murali Haran", "title": "Dimension Reduction and Alleviation of Confounding for Spatial\n  Generalized Linear Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-gaussian spatial data are very common in many disciplines. For instance,\ncount data are common in disease mapping, and binary data are common in\necology. When fitting spatial regressions for such data, one needs to account\nfor dependence to ensure reliable inference for the regression coefficients.\nThe spatial generalized linear mixed model (SGLMM) offers a very popular and\nflexible approach to modeling such data, but the SGLMM suffers from three major\nshortcomings: (1) uninterpretability of parameters due to spatial confounding,\n(2) variance inflation due to spatial confounding, and (3) high-dimensional\nspatial random effects that make fully Bayesian inference for such models\ncomputationally challenging. We propose a new parameterization of the SGLMM\nthat alleviates spatial confounding and speeds computation by greatly reducing\nthe dimension of the spatial random effects. We illustrate the application of\nour approach to simulated binary, count, and Gaussian spatial datasets, and to\na large infant mortality dataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Nov 2010 19:45:50 GMT"}], "update_date": "2010-12-01", "authors_parsed": [["Hughes", "John", ""], ["Haran", "Murali", ""]]}]