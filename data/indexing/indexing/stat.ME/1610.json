[{"id": "1610.00033", "submitter": "Anders Huitfeldt", "authors": "Anders Huitfeldt, Mats Julius Stensrud and Etsuji Suzuki", "title": "On the collapsibility of measures of effect in the counterfactual causal\n  framework", "comments": "Minor updates", "journal-ref": "Emerging Themes in Epidemiology 2019 16:1", "doi": "10.1186/s12982-018-0083-9", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A measure of association is said to be collapsible over a set of baseline\ncovariates if the marginal value of the measure of association is equal to a\nweighted average of the stratum-specific measures of association. In this\npaper, we consider two subtly different definitions of collapsibility, and show\nthat by considering causal measures of effect based on counterfactual variables\nit is possible to separate out the component of non-collapsibility which is due\nto the mathematical properties of the effect measure. We provide weights such\nthat the causal risk difference and the causal risk ratio are collapsible over\narbitrary baseline covariates, and demonstrate that such general weights do not\nexist for the odds ratio.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 21:01:25 GMT"}, {"version": "v2", "created": "Sat, 7 Jan 2017 11:34:24 GMT"}, {"version": "v3", "created": "Tue, 24 Oct 2017 12:03:16 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Huitfeldt", "Anders", ""], ["Stensrud", "Mats Julius", ""], ["Suzuki", "Etsuji", ""]]}, {"id": "1610.00048", "submitter": "Joshua EmBree", "authors": "Joshua D. EmBree, Mark S. Handcock", "title": "Spatial Temporal Exponential-Family Point Process Models for the\n  Evolution of Social Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a class of exponential-family point processes based on a latent\nsocial space to model the coevolution of social structure and behavior over\ntime. Temporal dynamics are modeled as a discrete Markov process specified\nthrough individual transition distributions for each actor in the system at a\ngiven time. We prove that these distributions have an analytic closed form\nunder certain conditions and use the result to develop likelihood-based\ninference. We provide a computational framework to enable both simulation and\ninference in practice. Finally, we demonstrate the value of these models by\nanalyzing alcohol and drug use over time in the context of adolescent\nfriendship networks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 22:30:37 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["EmBree", "Joshua D.", ""], ["Handcock", "Mark S.", ""]]}, {"id": "1610.00068", "submitter": "Anders Huitfeldt", "authors": "Anders Huitfeldt, Sonja A. Swanson, Mats Julius Stensrud and Etsuji\n  Suzuki", "title": "Effect heterogeneity and variable selection for standardizing causal\n  effects to a target population", "comments": "Final author manuscript. To appear in European Journal of\n  Epidemiology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The participants in randomized trials and other studies used for causal\ninference are often not representative of the populations seen by clinical\ndecision-makers. To account for differences between populations, researchers\nmay consider standardizing results to a target population. We discuss several\ndifferent types of homogeneity conditions that are relevant for\nstandardization: Homogeneity of effect measures, homogeneity of counterfactual\noutcome state transition parameters, and homogeneity of counterfactual\ndistributions. Each of these conditions can be used to show that a particular\nstandardization procedure will result in unbiased estimates of the effect in\nthe target population, given assumptions about the relevant scientific context.\nWe compare and contrast the homogeneity conditions, in particular their\nimplications for selection of covariates for standardization and their\nimplications for how to compute the standardized causal effect in the target\npopulation. While some of the recently developed counterfactual approaches to\ngeneralizability rely upon homogeneity conditions that avoid many of the\nproblems associated with traditional approaches, they often require adjustment\nfor a large (and possibly unfeasible) set of covariates.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 01:28:00 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 10:43:16 GMT"}, {"version": "v3", "created": "Wed, 15 May 2019 13:45:18 GMT"}, {"version": "v4", "created": "Fri, 16 Aug 2019 08:10:29 GMT"}, {"version": "v5", "created": "Sat, 5 Oct 2019 13:00:58 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Huitfeldt", "Anders", ""], ["Swanson", "Sonja A.", ""], ["Stensrud", "Mats Julius", ""], ["Suzuki", "Etsuji", ""]]}, {"id": "1610.00069", "submitter": "Anders Huitfeldt", "authors": "Anders Huitfeldt, Andrew Goldstein and Sonja A. Swanson", "title": "The choice of effect measure for binary outcomes: Introducing\n  counterfactual outcome state transition parameters", "comments": "Final author manuscript. To appear in Epidemiologic Methods", "journal-ref": "Epidemiologic Methods 2018", "doi": "10.1515/em-2016-0014", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard measures of effect, including the risk ratio, the odds ratio, and\nthe risk difference, are associated with a number of well-described\nshortcomings, and no consensus exists about the conditions under which\ninvestigators should choose one effect measure over another. In this paper, we\nintroduce a new framework for reasoning about choice of effect measure by\nlinking two separate versions of the risk ratio to a counterfactual causal\nmodel. In our approach, effects are defined in terms of \"counterfactual outcome\nstate transition parameters\", that is, the proportion of those individuals who\nwould not have been a case by the end of follow-up if untreated, who would have\nresponded to treatment by becoming a case; and the proportion of those\nindividuals who would have become a case by the end of follow-up if untreated\nwho would have responded to treatment by not becoming a case. Although\ncounterfactual outcome state transition parameters are generally not identified\nfrom the data without strong monotonicity assumptions, we show that when they\nstay constant between populations, there are important implications for model\nspecification, meta-analysis, and research generalization.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 01:36:13 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 07:22:55 GMT"}, {"version": "v3", "created": "Thu, 15 Dec 2016 22:53:38 GMT"}, {"version": "v4", "created": "Mon, 28 Aug 2017 11:02:52 GMT"}, {"version": "v5", "created": "Tue, 24 Oct 2017 11:59:02 GMT"}, {"version": "v6", "created": "Mon, 26 Feb 2018 20:14:55 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Huitfeldt", "Anders", ""], ["Goldstein", "Andrew", ""], ["Swanson", "Sonja A.", ""]]}, {"id": "1610.00147", "submitter": "Maria DeYoreo", "authors": "Tracy Schifeling, Jerome P. Reiter, Maria DeYoreo", "title": "Data Fusion for Correcting Measurement Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often in surveys, key items are subject to measurement errors. Given just the\ndata, it can be difficult to determine the distribution of this error process,\nand hence to obtain accurate inferences that involve the error-prone variables.\nIn some settings, however, analysts have access to a data source on different\nindividuals with high quality measurements of the error-prone survey items. We\npresent a data fusion framework for leveraging this information to improve\ninferences in the error-prone survey. The basic idea is to posit models about\nthe rates at which individuals make errors, coupled with models for the values\nreported when errors are made. This can avoid the unrealistic assumption of\nconditional independence typically used in data fusion. We apply the approach\non the reported values of educational attainments in the American Community\nSurvey, using the National Survey of College Graduates as the high quality data\nsource. In doing so, we account for the informative sampling design used to\nselect the National Survey of College Graduates. We also present a process for\nassessing the sensitivity of various analyses to different choices for the\nmeasurement error models. Supplemental material is available online.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 15:15:40 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Schifeling", "Tracy", ""], ["Reiter", "Jerome P.", ""], ["DeYoreo", "Maria", ""]]}, {"id": "1610.00168", "submitter": "Berk Ustun", "authors": "Berk Ustun, Cynthia Rudin", "title": "Learning Optimized Risk Scores", "comments": null, "journal-ref": "Journal of Machine Learning Research 2019. Volume 20. Issue 150.\n  Pages 1-75", "doi": null, "report-no": null, "categories": "stat.ML math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk scores are simple classification models that let users make quick risk\npredictions by adding and subtracting a few small numbers. These models are\nwidely used in medicine and criminal justice, but are difficult to learn from\ndata because they need to be calibrated, sparse, use small integer\ncoefficients, and obey application-specific operational constraints. In this\npaper, we present a new machine learning approach to learn risk scores. We\nformulate the risk score problem as a mixed integer nonlinear program, and\npresent a cutting plane algorithm for non-convex settings to efficiently\nrecover its optimal solution. We improve our algorithm with specialized\ntechniques to generate feasible solutions, narrow the optimality gap, and\nreduce data-related computation. Our approach can fit risk scores in a way that\nscales linearly in the number of samples, provides a certificate of optimality,\nand obeys real-world constraints without parameter tuning or post-processing.\nWe benchmark the performance benefits of this approach through an extensive set\nof numerical experiments, comparing to risk scores built using heuristic\napproaches. We also discuss its practical benefits through a real-world\napplication where we build a customized risk score for ICU seizure prediction\nin collaboration with the Massachusetts General Hospital.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 18:40:08 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 15:06:56 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2017 02:27:05 GMT"}, {"version": "v4", "created": "Fri, 7 Sep 2018 17:18:35 GMT"}, {"version": "v5", "created": "Tue, 17 Sep 2019 01:58:54 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Ustun", "Berk", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1610.00207", "submitter": "Johannes Lederer", "authors": "Wei Li and Johannes Lederer", "title": "Tuning parameter calibration for $\\ell_1$-regularized logistic\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is a standard approach to understanding and modeling\nhigh-dimensional classification data, but the corresponding statistical methods\nhinge on tuning parameters that are difficult to calibrate. In particular,\nexisting calibration schemes in the logistic regression framework lack any\nfinite sample guarantees. In this paper, we introduce a novel calibration\nscheme for $\\ell_1$-penalized logistic regression. It is based on simple tests\nalong the tuning parameter path and is equipped with optimal guarantees for\nfeature selection. It is also amenable to easy and efficient implementations,\nand it rivals or outmatches existing methods in simulations and real data\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 23:53:04 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 14:27:23 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Li", "Wei", ""], ["Lederer", "Johannes", ""]]}, {"id": "1610.00287", "submitter": "Ashkan Esmaeili", "authors": "Ashkan Esmaeili, Ehsan Asadi, and Farokh Marvasti", "title": "Iterative Null-space Projection Method with Adaptive Thresholding in\n  Sparse Signal Recovery and Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive thresholding methods have proved to yield high SNRs and fast\nconvergence in finding the solution to the Compressed Sensing (CS) problems.\nRecently, it was observed that the robustness of a class of iterative sparse\nrecovery algorithms such as Iterative Method with Adaptive Thresholding (IMAT)\nhas outperformed the well-known LASSO algorithm in terms of reconstruction\nquality, convergence speed, and the sensitivity to the noise. In this paper, we\nintroduce a new method towards solving the CS problem. The logic of this method\nis based on iterative projections of the thresholded signal onto the null-space\nof the sensing matrix. The thresholding is carried out by recovering the\nsupport of the desired signal by projection on thresholding subspaces. The\nsimulations reveal that the proposed method has the capability of yielding\nnoticeable output SNR values with about as many samples as twice the sparsity\nnumber, while other methods fail to recover the signals when approaching the\nalgebraic bound for the number of samples required. The computational\ncomplexity of our method is also comparable to other methods as observed in the\nsimulations. We have also extended our Algorithm to Matrix Completion (MC)\nscenarios and compared its efficiency to other well-reputed approaches for MC\nin the literature.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 15:10:51 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 20:29:41 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Esmaeili", "Ashkan", ""], ["Asadi", "Ehsan", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1610.00345", "submitter": "Daniel W. Meyer", "authors": "Daniel W. Meyer", "title": "Density Estimation with Distribution Element Trees", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-017-9751-9", "report-no": null, "categories": "stat.ME cs.MS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of probability densities based on available data is a central\ntask in many statistical applications. Especially in the case of large\nensembles with many samples or high-dimensional sample spaces, computationally\nefficient methods are needed. We propose a new method that is based on a\ndecomposition of the unknown distribution in terms of so-called distribution\nelements (DEs). These elements enable an adaptive and hierarchical\ndiscretization of the sample space with small or large elements in regions with\nsmoothly or highly variable densities, respectively. The novel refinement\nstrategy that we propose is based on statistical goodness-of-fit and pair-wise\n(as an approximation to mutual) independence tests that evaluate the local\napproximation of the distribution in terms of DEs. The capabilities of our new\nmethod are inspected based on several examples of different dimensionality and\nsuccessfully compared with other state-of-the-art density estimators.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 20:07:28 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 07:08:16 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Meyer", "Daniel W.", ""]]}, {"id": "1610.00951", "submitter": "Anirvan Chakraborty Mr.", "authors": "Anirvan Chakraborty and Victor M. Panaretos", "title": "Hybrid Regularisation of Functional Linear Models", "comments": "34 pages, 1 figure and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the slope function in a functional\nregression with a scalar response and a functional covariate. This central\nproblem of functional data analysis is well known to be ill-posed, thus\nrequiring a regularised estimation procedure. The two most commonly used\napproaches are based on spectral truncation or Tikhonov regularisation of the\nempirical covariance operator. In principle, Tikhonov regularisation is the\nmore canonical choice. Compared to spectral truncation, it is robust to\neigenvalue ties, while it attains the optimal minimax rate of convergence in\nthe mean squared sense, and not just in a concentration probability sense. In\nthis paper, we show that, surprisingly, one can strictly improve upon the\nperformance of the Tikhonov estimator in finite samples by means of a linear\nestimator, while retaining its stability and asymptotic properties by combining\nit with a form of spectral truncation. Specifically, we construct an estimator\nthat additively decomposes the functional covariate by projecting it onto two\northogonal subspaces defined via functional PCA; it then applies Tikhonov\nregularisation to the one component, while leaving the other component\nunregularised. We prove that when the covariate is Gaussian, this hybrid\nestimator uniformly improves upon the MSE of the Tikhonov estimator in a\nnon-asymptotic sense, effectively rendering it inadmissible. This domination is\nshown to also persist under discrete observation of the covariate function. The\nhybrid estimator is linear, straightforward to construct in practice, and with\nno computational overhead relative to the standard regularisation methods. By\nmeans of simulation, it is shown to furnish sizeable gains even for modest\nsample sizes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 12:37:55 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Chakraborty", "Anirvan", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "1610.01198", "submitter": "Zhichao Jiang", "authors": "Zhichao Jiang and Peng Ding", "title": "Using Missing Types to Improve Partial Identification with Application\n  to a Study of HIV Prevalence in Malawi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequently, empirical studies are plagued with missing data. When the data\nare missing not at random, the parameter of interest is not identifiable in\ngeneral. Without additional assumptions, we can derive bounds of the parameters\nof interest, which, unfortunately, are often too wide to be informative.\nTherefore, it is of great importance to sharpen these worst-case bounds by\nexploiting additional information. Traditional missing data analysis uses only\nthe information of the binary missing data indicator, that is, a certain data\npoint is either missing or not. Nevertheless, real data often provide more\ninformation than a binary missing data indicator, and they often record\ndifferent types of missingness. In a motivating HIV status survey, missing data\nmay be due to the units' unwillingness to respond to the survey items or their\nhospitalization during the visit, and may also be due to the units' temporarily\nabsence or relocation. It is apparent that some missing types are more likely\nto be missing not at random, but other missing types are more likely to be\nmissing at random. We show that making full use of the missing types results in\nnarrower bounds of the parameters of interest. In a real-life example, we\ndemonstrate substantial improvement of more than 50% reduction in bound widths\nfor estimating the prevalence of HIV in rural Malawi. As we illustrate using\nthe HIV study, our strategy is also useful for conducting sensitivity analysis\nby gradually increasing or decreasing the set of types that are missing at\nrandom. In addition, we propose an easy-to-implement method to construct\nconfidence intervals for partially identified parameters with bounds expressed\nas the minimums and maximums of finite parameters, which is useful for not only\nour problem but also many other problems involving bounds.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 20:56:08 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 04:32:04 GMT"}, {"version": "v3", "created": "Wed, 12 Oct 2016 15:42:28 GMT"}, {"version": "v4", "created": "Fri, 7 Apr 2017 18:54:11 GMT"}, {"version": "v5", "created": "Tue, 2 May 2017 18:34:16 GMT"}, {"version": "v6", "created": "Tue, 11 Sep 2018 17:28:26 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Jiang", "Zhichao", ""], ["Ding", "Peng", ""]]}, {"id": "1610.01226", "submitter": "Philip Browne", "authors": "PA Browne", "title": "Model error moment estimation via data assimilation", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a dynamical model to make predictions about a system has many sources\nof error. These can include errors in how the model was initialised but also\nerrors in the dynamics of the model itself. For many applications in data\nassimilation, probabilistic forecasting, or model improvement, these model\nerrors need to be known over the timestep of the model, not over a\ntime-averaged period. Using a forecast from a state that combines observational\ninformation as well as prior information we can gain an approximation to the\nstatistics of the model errors on the timescale of the model that is required.\nHere we give bounds on the errors in the estimation of the mean and covariance\nof the errors in the model equations in terms of the errors made in the state\nestimation. This is the first time that such a result has been derived. The\nresult shows to what extent the state estimation must constrain the analysis in\norder to obtain a specified error on the mean or covariance of the model\nerrors. This is particularly useful for experimental design as it indicates the\nnecessary information content required in observations of the dynamical system.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 22:57:55 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Browne", "PA", ""]]}, {"id": "1610.01271", "submitter": "Stefan Wager", "authors": "Susan Athey, Julie Tibshirani and Stefan Wager", "title": "Generalized Random Forests", "comments": "Forthcoming in the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose generalized random forests, a method for non-parametric\nstatistical estimation based on random forests (Breiman, 2001) that can be used\nto fit any quantity of interest identified as the solution to a set of local\nmoment equations. Following the literature on local maximum likelihood\nestimation, our method considers a weighted set of nearby training examples;\nhowever, instead of using classical kernel weighting functions that are prone\nto a strong curse of dimensionality, we use an adaptive weighting function\nderived from a forest designed to express heterogeneity in the specified\nquantity of interest. We propose a flexible, computationally efficient\nalgorithm for growing generalized random forests, develop a large sample theory\nfor our method showing that our estimates are consistent and asymptotically\nGaussian, and provide an estimator for their asymptotic variance that enables\nvalid confidence intervals. We use our approach to develop new methods for\nthree statistical tasks: non-parametric quantile regression, conditional\naverage partial effect estimation, and heterogeneous treatment effect\nestimation via instrumental variables. A software implementation, grf for R and\nC++, is available from CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 04:42:13 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 02:56:28 GMT"}, {"version": "v3", "created": "Thu, 6 Jul 2017 01:30:47 GMT"}, {"version": "v4", "created": "Thu, 5 Apr 2018 17:51:23 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Athey", "Susan", ""], ["Tibshirani", "Julie", ""], ["Wager", "Stefan", ""]]}, {"id": "1610.01353", "submitter": "Jana Jankova", "authors": "Jana Jankov\\'a and Sara van de Geer", "title": "Confidence regions for high-dimensional generalized linear models under\n  sparsity", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study asymptotically normal estimation and confidence regions for\nlow-dimensional parameters in high-dimensional sparse models. Our approach is\nbased on the $\\ell_1$-penalized M-estimator which is used for construction of a\nbias corrected estimator. We show that the proposed estimator is asymptotically\nnormal, under a sparsity assumption on the high-dimensional parameter,\nsmoothness conditions on the expected loss and an entropy condition. This leads\nto uniformly valid confidence regions and hypothesis testing for\nlow-dimensional parameters. The present approach is different in that it allows\nfor treatment of loss functions that we not sufficiently differentiable, such\nas quantile loss, Huber loss or hinge loss functions. We also provide new\nresults for estimation of the inverse Fisher information matrix, which is\nnecessary for the construction of the proposed estimator. We formulate our\nresults for general models under high-level conditions, but investigate these\nconditions in detail for generalized linear models and provide mild sufficient\nconditions. As particular examples, we investigate the case of quantile loss\nand Huber loss in linear regression and demonstrate the performance of the\nestimators in a simulation study and on real datasets from genome-wide\nassociation studies. We further investigate the case of logistic regression and\nillustrate the performance of the estimator on simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 10:44:09 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Jankov\u00e1", "Jana", ""], ["van de Geer", "Sara", ""]]}, {"id": "1610.01424", "submitter": "Erika Helgeson", "authors": "Erika S. Helgeson, Eric Bair", "title": "Non-Parametric Cluster Significance Testing with Reference to a Unimodal\n  Null Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis is an unsupervised learning strategy that can be employed to\nidentify subgroups of observations in data sets of unknown structure. This\nstrategy is particularly useful for analyzing high-dimensional data such as\nmicroarray gene expression data. Many clustering methods are available, but it\nis challenging to determine if the identified clusters represent distinct\nsubgroups. We propose a novel strategy to investigate the significance of\nidentified clusters by comparing the within- cluster sum of squares from the\noriginal data to that produced by clustering an appropriate unimodal null\ndistribution. The null distribution we present for this problem uses kernel\ndensity estimation and thus does not require that the data follow any\nparticular distribution. We find that our method can accurately test for the\npresence of clustering even when the number of features is high.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 14:01:57 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 00:18:17 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Helgeson", "Erika S.", ""], ["Bair", "Eric", ""]]}, {"id": "1610.01526", "submitter": "Peter Craigmile", "authors": "Jeffrey J. Gory and Peter F. Craigmile and Steven N. MacEachern", "title": "Marginally Interpretable Generalized Linear Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two popular approaches for relating correlated measurements of a non-Gaussian\nresponse variable to a set of predictors are to fit a marginal model using\ngeneralized estimating equations and to fit a generalized linear mixed model by\nintroducing latent random variables. The first approach is effective for\nparameter estimation, but leaves one without a formal model for the data with\nwhich to assess quality of fit or make predictions for future observations. The\nsecond approach overcomes the deficiencies of the first, but leads to parameter\nestimates that must be interpreted conditional on the latent variables. Further\ncomplicating matters, obtaining marginal summaries from a generalized linear\nmixed model often requires evaluation of an analytically intractable integral\nor use of attenuation factors that are not exact. We define a class of\nmarginally interpretable generalized linear mixed models that lead to parameter\nestimates with a marginal interpretation while maintaining the desirable\nstatistical properties of a conditionally-specified model. We discuss the form\nof these models under various common link functions and also address\ncomputational issues associated with these models. For logistic mixed effects\nmodels, we introduce an accurate and efficient method for evaluating the\nlogistic-normal integral.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 16:57:31 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 02:04:38 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Gory", "Jeffrey J.", ""], ["Craigmile", "Peter F.", ""], ["MacEachern", "Steven N.", ""]]}, {"id": "1610.01697", "submitter": "Guido Kuersteiner", "authors": "Jinyong Hahn, Guido Kuersteiner, Maurizio Mazzocco", "title": "Central Limit Theory for Combined Cross-Section and Time Series", "comments": "arXiv admin note: substantial text overlap with arXiv:1507.04415", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining cross-section and time series data is a long and well established\npractice in empirical economics. We develop a central limit theory that\nexplicitly accounts for possible dependence between the two data sets. We focus\non common factors as the mechanism behind this dependence. Using our central\nlimit theorem (CLT) we establish the asymptotic properties of parameter\nestimates of a general class of models based on a combination of\ncross-sectional and time series data, recognizing the interdependence between\nthe two data sources in the presence of aggregate shocks. Despite the\ncomplicated nature of the analysis required to formulate the joint CLT, it is\nstraightforward to implement the resulting parameter limiting distributions due\nto a formal similarity of our approximations with the standard Murphy and\nTopel's (1985) formula.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 00:30:57 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 17:45:13 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 14:22:38 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Hahn", "Jinyong", ""], ["Kuersteiner", "Guido", ""], ["Mazzocco", "Maurizio", ""]]}, {"id": "1610.01869", "submitter": "Daniel Commenges", "authors": "Daniel Commenges", "title": "Dealing with death when studying disease or physiological marker: the\n  stochastic system approach to causality", "comments": "25 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic system approach to causality is applied to situations where\nthe risk of death is not negligible. This approach grounds causality on\nphysical laws, distinguishes system and observation and represents the system\nby multivariate stochastic processes. The particular role of death is\nhighlighted, and it is shown that local influences must be defined on the\nrandom horizon of time of death. We particularly study the problem of\nestimating the effect of a factor $V$ on a process of interest $Y$, taking\ndeath into account. We unify the cases where $Y$ is a counting process\n(describing an event) and the case where $Y$ is quantitative; we examine the\ncase of observations in continuous and discrete time and we give a typology of\ncases where the mechanism leading to incomplete data can be ignored. Finally,\nwe give an example of a situation where we are interested in estimating the\neffect of a factor (blood pressure) on cognitive ability in elderly.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 13:39:14 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Commenges", "Daniel", ""]]}, {"id": "1610.01889", "submitter": "Dong Wang", "authors": "Dong Wang, Xialu Liu, Rong Chen", "title": "Factor Models for Matrix-Valued High-Dimensional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In finance, economics and many other fields, observations in a matrix form\nare often observed over time. For example, many economic indicators are\nobtained in different countries over time. Various financial characteristics of\nmany companies are reported over time. Although it is natural to turn a matrix\nobservation into a long vector then use standard vector time series models or\nfactor analysis, it is often the case that the columns and rows of a matrix\nrepresent different sets of information that are closely interrelated in a very\nstructural way. We propose a novel factor model that maintains and utilizes the\nmatrix structure to achieve greater dimensional reduction as well as finding\nclearer and more interpretable factor structures. Estimation procedure and its\ntheoretical properties are investigated and demonstrated with simulated and\nreal examples.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 14:36:31 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 03:03:42 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Wang", "Dong", ""], ["Liu", "Xialu", ""], ["Chen", "Rong", ""]]}, {"id": "1610.02122", "submitter": "Jelena Bradic", "authors": "Yinchu Zhu and Jelena Bradic", "title": "Significance testing in non-sparse high-dimensional linear models", "comments": "43 pages", "journal-ref": null, "doi": "10.1214/18-EJS1443", "report-no": null, "categories": "stat.ME math.SP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional linear models, the sparsity assumption is typically made,\nstating that most of the parameters are equal to zero. Under the sparsity\nassumption, estimation and, recently, inference have been well studied.\nHowever, in practice, sparsity assumption is not checkable and more importantly\nis often violated; a large number of covariates might be expected to be\nassociated with the response, indicating that possibly all, rather than just a\nfew, parameters are non-zero. A natural example is a genome-wide gene\nexpression profiling, where all genes are believed to affect a common disease\nmarker. We show that existing inferential methods are sensitive to the sparsity\nassumption, and may, in turn, result in the severe lack of control of Type-I\nerror. In this article, we propose a new inferential method, named CorrT, which\nis robust to model misspecification such as heteroscedasticity and lack of\nsparsity. CorrT is shown to have Type I error approaching the nominal level for\n\\textit{any} models and Type II error approaching zero for sparse and many\ndense models.\n  In fact, CorrT is also shown to be optimal in a variety of frameworks:\nsparse, non-sparse and hybrid models where sparse and dense signals are mixed.\nNumerical experiments show a favorable performance of the CorrT test compared\nto the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 02:17:00 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 01:35:02 GMT"}, {"version": "v3", "created": "Mon, 28 Aug 2017 06:08:42 GMT"}, {"version": "v4", "created": "Mon, 28 May 2018 01:31:25 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhu", "Yinchu", ""], ["Bradic", "Jelena", ""]]}, {"id": "1610.02351", "submitter": "Lucas Janson", "authors": "Emmanuel Candes, Yingying Fan, Lucas Janson, Jinchi Lv", "title": "Panning for Gold: Model-X Knockoffs for High-dimensional Controlled\n  Variable Selection", "comments": "39 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many contemporary large-scale applications involve building interpretable\nmodels linking a large set of potential covariates to a response in a nonlinear\nfashion, such as when the response is binary. Although this modeling problem\nhas been extensively studied, it remains unclear how to effectively control the\nfraction of false discoveries even in high-dimensional logistic regression, not\nto mention general high-dimensional nonlinear models. To address such a\npractical problem, we propose a new framework of $model$-$X$ knockoffs, which\nreads from a different perspective the knockoff procedure (Barber and Cand\\`es,\n2015) originally designed for controlling the false discovery rate in linear\nmodels. Whereas the knockoffs procedure is constrained to homoscedastic linear\nmodels with $n\\ge p$, the key innovation here is that model-X knockoffs provide\nvalid inference from finite samples in settings in which the conditional\ndistribution of the response is arbitrary and completely unknown. Furthermore,\nthis holds no matter the number of covariates. Correct inference in such a\nbroad setting is achieved by constructing knockoff variables probabilistically\ninstead of geometrically. To do this, our approach requires the covariates be\nrandom (independent and identically distributed rows) with a distribution that\nis known, although we provide preliminary experimental evidence that our\nprocedure is robust to unknown/estimated distributions. To our knowledge, no\nother procedure solves the $controlled$ variable selection problem in such\ngenerality, but in the restricted settings where competitors exist, we\ndemonstrate the superior power of knockoffs through simulations. Finally, we\napply our procedure to data from a case-control study of Crohn's disease in the\nUnited Kingdom, making twice as many discoveries as the original analysis of\nthe same data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 17:18:02 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 18:25:01 GMT"}, {"version": "v3", "created": "Fri, 6 Jan 2017 05:37:10 GMT"}, {"version": "v4", "created": "Tue, 12 Dec 2017 14:57:10 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Candes", "Emmanuel", ""], ["Fan", "Yingying", ""], ["Janson", "Lucas", ""], ["Lv", "Jinchi", ""]]}, {"id": "1610.02372", "submitter": "Adelchi Azzalini", "authors": "Adelchi Azzalini", "title": "Combining local and global smoothing in multivariate density estimation", "comments": "Two figures, one table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric estimation of a multivariate density estimation is tackled via\na method which combines traditional local smoothing with a form of global\nsmoothing but without imposing a rigid structure. Simulation work delivers\nencouraging indications on the effectiveness of the method. An application to\ndensity-based clustering illustrates a possible usage.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 18:54:58 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Azzalini", "Adelchi", ""]]}, {"id": "1610.02427", "submitter": "Pierre Latouche", "authors": "Bouveyron Charles and Latouche Pierre and Zreik Rawya", "title": "The Stochastic Topic Block Model for the Clustering of Vertices in\n  Networks with Textual Edges", "comments": "in Statistics and Computing, 2016", "journal-ref": null, "doi": "10.1007/s11222-016-9713-7", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the significant increase of communications between individuals via\nsocial media (Facebook, Twitter, Linkedin) or electronic formats (email, web,\ne-publication) in the past two decades, network analysis has become a\nunavoidable discipline. Many random graph models have been proposed to extract\ninformation from networks based on person-to-person links only, without taking\ninto account information on the contents. This paper introduces the stochastic\ntopic block model (STBM), a probabilistic model for networks with textual\nedges. We address here the problem of discovering meaningful clusters of\nvertices that are coherent from both the network interactions and the text\ncontents. A classification variational expectation-maximization (C-VEM)\nalgorithm is proposed to perform inference. Simulated data sets are considered\nin order to assess the proposed approach and to highlight its main features.\nFinally, we demonstrate the effectiveness of our methodology on two real-word\ndata sets: a directed communication network and a undirected co-authorship\nnetwork.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 21:21:28 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 09:24:51 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Charles", "Bouveyron", ""], ["Pierre", "Latouche", ""], ["Rawya", "Zreik", ""]]}, {"id": "1610.02436", "submitter": "Kshitij Khare", "authors": "Kshitij Khare, Sang Oh, Syed Rahman and Bala Rajaratnam", "title": "A convex framework for high-dimensional sparse Cholesky based covariance\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance estimation for high-dimensional datasets is a fundamental problem\nin modern day statistics with numerous applications. In these high dimensional\ndatasets, the number of variables p is typically larger than the sample size n.\nA popular way of tackling this challenge is to induce sparsity in the\ncovariance matrix, its inverse or a relevant transformation. In particular,\nmethods inducing sparsity in the Cholesky pa- rameter of the inverse covariance\nmatrix can be useful as they are guaranteed to give a positive definite\nestimate of the covariance matrix. Also, the estimated sparsity pattern\ncorresponds to a Directed Acyclic Graph (DAG) model for Gaussian data. In\nrecent years, two useful penalized likelihood methods for sparse estimation of\nthis Cholesky parameter (with no restrictions on the sparsity pattern) have\nbeen developed. How- ever, these methods either consider a non-convex\noptimization problem which can lead to convergence issues and singular\nestimates of the covariance matrix when p > n, or achieve a convex formulation\nby placing a strict constraint on the conditional variance parameters. In this\npaper, we propose a new penalized likelihood method for sparse estimation of\nthe inverse covariance Cholesky parameter that aims to overcome some of the\nshortcomings of current methods, but retains their respective strengths. We ob-\ntain a jointly convex formulation for our objective function, which leads to\nconvergence guarantees, even when p > n. The approach always leads to a\npositive definite and symmetric estimator of the covariance matrix. We\nestablish high-dimensional estima- tion and graph selection consistency, and\nalso demonstrate finite sample performance on simulated/real data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 21:49:09 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Khare", "Kshitij", ""], ["Oh", "Sang", ""], ["Rahman", "Syed", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1610.02447", "submitter": "Mark Risser", "authors": "Mark D. Risser", "title": "Review: Nonstationary Spatial Modeling, with Emphasis on Process\n  Convolution and Covariate-Driven Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many environmental applications involving spatially-referenced data,\nlimitations on the number and locations of observations motivate the need for\npractical and efficient models for spatial interpolation, or kriging. A key\ncomponent of models for continuously-indexed spatial data is the covariance\nfunction, which is traditionally assumed to belong to a parametric class of\nstationary models. While convenient, the assumption of stationarity is rarely\nrealistic; as a result, there is a rich literature on alternative methodologies\nwhich capture and model the nonstationarity present in most environmental\nprocesses. This review document provides a rigorous and concise description of\nthe existing literature on nonstationary methods, paying particular attention\nto process convolution (also called kernel smoothing or moving average)\napproaches. A summary is also provided of more recent methods which leverage\ncovariate information and yield both interpretational and computational\nbenefits.\n  Note: the article is borrowed from Chapters 1 and 2 of the author's Ph.D.\ndissertation, joint with Catherine A. Calder.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 23:47:41 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Risser", "Mark D.", ""]]}, {"id": "1610.02503", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universit\\'e Paris-Dauphine PSL, CREST Paris, and\n  University of Warwick) and Judith Rousseau (Universit\\'e Paris-Dauphine PSL\n  and CREST Paris)", "title": "Some comments about A Bayesian criterion for singular models by M. Drton\n  and M. Plummer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are written comments about the Read Paper A Bayesian criterion for\nsingular models by M. Drton and M. Plummer, read to the Royal Statistical\nSociety on October 5, 2016. The discussion was delivered by Judith Rousseau.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 09:12:08 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine PSL, CREST Paris, and\n  University of Warwick"], ["Rousseau", "Judith", "", "Universit\u00e9 Paris-Dauphine PSL\n  and CREST Paris"]]}, {"id": "1610.02590", "submitter": "Yiyuan She", "authors": "Yiyuan She, Shao Tang, and Qiaoya Zhang", "title": "Indirect Gaussian Graph Learning beyond Gaussianity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies how to capture dependency graph structures from real data\nwhich may not be Gaussian. Starting from marginal loss functions not\nnecessarily derived from probability distributions, we utilize an additive\nover-parametrization with shrinkage to incorporate variable dependencies into\nthe criterion. An iterative Gaussian graph learning algorithm is proposed with\nease in implementation. Statistical analysis shows that the estimators achieve\nsatisfactory accuracy with the error measured in terms of a proper Bregman\ndivergence. Real-life examples in different settings are given to demonstrate\nthe efficacy of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 22:22:37 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 23:33:56 GMT"}, {"version": "v3", "created": "Sun, 13 Jan 2019 03:17:33 GMT"}, {"version": "v4", "created": "Sat, 30 Nov 2019 22:21:07 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["She", "Yiyuan", ""], ["Tang", "Shao", ""], ["Zhang", "Qiaoya", ""]]}, {"id": "1610.02725", "submitter": "Qiuyi Han", "authors": "Qiuyi Han, Jie Ding, Edoardo Airoldi, and Vahid Tarokh", "title": "SLANTS: Sequential Adaptive Nonlinear Modeling of Vector Time Series", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2716898", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for adaptive nonlinear sequential modeling of vector-time\nseries data. Data is modeled as a nonlinear function of past values corrupted\nby noise, and the underlying non-linear function is assumed to be approximately\nexpandable in a spline basis. We cast the modeling of data as finding a good\nfit representation in the linear span of multi-dimensional spline basis, and\nuse a variant of l1-penalty regularization in order to reduce the\ndimensionality of representation. Using adaptive filtering techniques, we\ndesign our online algorithm to automatically tune the underlying parameters\nbased on the minimization of the regularized sequential prediction error. We\ndemonstrate the generality and flexibility of the proposed approach on both\nsynthetic and real-world datasets. Moreover, we analytically investigate the\nperformance of our algorithm by obtaining both bounds of the prediction errors,\nand consistency results for variable selection.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 21:46:16 GMT"}, {"version": "v2", "created": "Sat, 15 Oct 2016 02:42:27 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Han", "Qiuyi", ""], ["Ding", "Jie", ""], ["Airoldi", "Edoardo", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1610.02738", "submitter": "Le-Yu Chen", "authors": "Le-Yu Chen, Sokbae Lee", "title": "Best Subset Binary Prediction", "comments": "55 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variable selection problem for the prediction of binary\noutcomes. We study the best subset selection procedure by which the covariates\nare chosen by maximizing Manski (1975, 1985)'s maximum score objective function\nsubject to a constraint on the maximal number of selected variables. We show\nthat this procedure can be equivalently reformulated as solving a mixed integer\noptimization problem, which enables computation of the exact or an approximate\nsolution with a definite approximation error bound. In terms of theoretical\nresults, we obtain non-asymptotic upper and lower risk bounds when the\ndimension of potential covariates is possibly much larger than the sample size.\nOur upper and lower risk bounds are minimax rate-optimal when the maximal\nnumber of selected variables is fixed and does not increase with the sample\nsize. We illustrate usefulness of the best subset binary prediction approach\nvia Monte Carlo simulations and an empirical application of the work-trip\ntransportation mode choice.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 23:01:46 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 19:38:15 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 09:30:24 GMT"}, {"version": "v4", "created": "Fri, 9 Jun 2017 16:53:39 GMT"}, {"version": "v5", "created": "Mon, 20 Nov 2017 16:43:49 GMT"}, {"version": "v6", "created": "Thu, 3 May 2018 22:08:39 GMT"}, {"version": "v7", "created": "Wed, 16 May 2018 22:18:09 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Chen", "Le-Yu", ""], ["Lee", "Sokbae", ""]]}, {"id": "1610.02753", "submitter": "Myung Hwan Seo", "authors": "Myung Hwan Seo and Taisuke Otsu", "title": "Local M-estimation with Discontinuous Criterion for Dependent and\n  Limited Observations", "comments": null, "journal-ref": "the Annals of Statistics (2018), 46, 344-369,", "doi": "10.1214/17-AOS1552", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines asymptotic properties of local M-estimators under three\nsets of high-level conditions. These conditions are sufficiently general to\ncover the minimum volume predictive region, conditional maximum score estimator\nfor a panel data discrete choice model, and many other widely used estimators\nin statistics and econometrics. Specifically, they allow for discontinuous\ncriterion functions of weakly dependent observations, which may be localized by\nkernel smoothing and contain nuisance parameters whose dimension may grow to\ninfinity. Furthermore, the localization can occur around parameter values\nrather than around a fixed point and the observation may take limited values,\nwhich leads to set estimators. Our theory produces three different\nnonparametric cube root rates and enables valid inference for the local\nM-estimators, building on novel maximal inequalities for weakly dependent data.\nOur results include the standard cube root asymptotics as a special case. To\nillustrate the usefulness of our results, we verify our conditions for various\nexamples such as the Hough transform estimator with diminishing bandwidth,\nmaximum score-type set estimator, and many others.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 02:28:57 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Seo", "Myung Hwan", ""], ["Otsu", "Taisuke", ""]]}, {"id": "1610.02987", "submitter": "Jelena Bradic", "authors": "Yinchu Zhu and Jelena Bradic", "title": "Linear Hypothesis Testing in Dense High-Dimensional Linear Models", "comments": "42 pages, 8 figures", "journal-ref": "Journal of the American Statistical Association: theory and\n  methods, 2017", "doi": "10.1080/01621459.2017.1356319", "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a methodology for testing linear hypothesis in high-dimensional\nlinear models. The proposed test does not impose any restriction on the size of\nthe model, i.e. model sparsity or the loading vector representing the\nhypothesis. Providing asymptotically valid methods for testing general linear\nfunctions of the regression parameters in high-dimensions is extremely\nchallenging -- especially without making restrictive or unverifiable\nassumptions on the number of non-zero elements. We propose to test the moment\nconditions related to the newly designed restructured regression, where the\ninputs are transformed and augmented features. These new features incorporate\nthe structure of the null hypothesis directly. The test statistics are\nconstructed in such a way that lack of sparsity in the original model parameter\ndoes not present a problem for the theoretical justification of our procedures.\nWe establish asymptotically exact control on Type I error without imposing any\nsparsity assumptions on model parameter or the vector representing the linear\nhypothesis. Our method is also shown to achieve certain optimality in detecting\ndeviations from the null hypothesis. We demonstrate the favorable finite-sample\nperformance of the proposed methods, via a number of numerical and a real data\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 16:30:27 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 01:10:12 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhu", "Yinchu", ""], ["Bradic", "Jelena", ""]]}, {"id": "1610.03177", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Ali Shojaie, and Daniela M. Witten", "title": "Network Reconstruction From High Dimensional Ordinary Differential\n  Equations", "comments": "To appear in JASA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of learning a dynamical system from high-dimensional\ntime-course data. For instance, we might wish to estimate a gene regulatory\nnetwork from gene expression data measured at discrete time points. We model\nthe dynamical system non-parametrically as a system of additive ordinary\ndifferential equations. Most existing methods for parameter estimation in\nordinary differential equations estimate the derivatives from noisy\nobservations. This is known to be challenging and inefficient. We propose a\nnovel approach that does not involve derivative estimation. We show that the\nproposed method can consistently recover the true network structure even in\nhigh dimensions, and we demonstrate empirical improvement over competing\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 04:13:36 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Chen", "Shizhe", ""], ["Shojaie", "Ali", ""], ["Witten", "Daniela M.", ""]]}, {"id": "1610.03287", "submitter": "Max Sommerfeld", "authors": "Max Sommerfeld and Axel Munk", "title": "Inference for Empirical Wasserstein Distances on Finite Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wasserstein distance is an attractive tool for data analysis but\nstatistical inference is hindered by the lack of distributional limits. To\novercome this obstacle, for probability measures supported on finitely many\npoints, we derive the asymptotic distribution of empirical Wasserstein\ndistances as the optimal value of a linear program with random objective\nfunction. This facilitates statistical inference (e.g. confidence intervals for\nsample based Wasserstein distances) in large generality. Our proof is based on\ndirectional Hadamard differentiability. Failure of the classical bootstrap and\nalternatives are discussed. The utility of the distributional results is\nillustrated on two data sets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 11:54:18 GMT"}, {"version": "v2", "created": "Wed, 26 Apr 2017 11:29:55 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Sommerfeld", "Max", ""], ["Munk", "Axel", ""]]}, {"id": "1610.03330", "submitter": "Jingshu Wang", "authors": "Jingshu Wang, Lin Gui, Weijie J. Su, Chiara Sabatti and Art B. Owen", "title": "Detecting Multiple Replicating Signals using Adaptive Filtering\n  Procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replicability is a fundamental quality of scientific discoveries: we are\ninterested in those signals that are detectable in different laboratories,\nstudy populations, across time etc. Unlike meta-analysis which accounts for\nexperimental variability but does not guarantee replicability, testing a\npartial conjunction (PC) null aims specifically to identify the signals that\nare discovered in multiple studies. In many contemporary applications, e.g.,\ncomparing multiple high-throughput genetic experiments, a large number $M$ of\nPC nulls need to be tested simultaneously, calling for a multiple comparisons\ncorrection. However, standard multiple testing adjustments on the $M$ PC\n$p$-values can be severely conservative, especially when $M$ is large and the\nsignals are sparse. We introduce AdaFilter, a new multiple testing procedure\nthat increases power by adaptively filtering out unlikely candidates of PC\nnulls. We prove that AdaFilter can control FWER and FDR as long as data across\nstudies are independent, and has much higher power than other existing methods.\nWe illustrate the application of AdaFilter with three examples: microarray\nstudies of Duchenne muscular dystrophy, single-cell RNA sequencing of T cells\nin lung cancer tumors and GWAS for metabolomics.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 13:41:37 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 01:50:40 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 02:05:17 GMT"}, {"version": "v4", "created": "Wed, 24 Jun 2020 17:02:36 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Wang", "Jingshu", ""], ["Gui", "Lin", ""], ["Su", "Weijie J.", ""], ["Sabatti", "Chiara", ""], ["Owen", "Art B.", ""]]}, {"id": "1610.03408", "submitter": "Daniele Durante", "authors": "Daniele Durante", "title": "A note on the multiplicative gamma process", "comments": null, "journal-ref": "Statistics & Probability Letters (2017). 122, 198-204", "doi": "10.1016/j.spl.2016.11.014", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive dimensionality reduction in high-dimensional problems is a key topic\nin statistics. The multiplicative gamma process takes a relevant step in this\ndirection, but improved studies on its properties are required to ease\nimplementation. This note addresses such aim.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 16:09:02 GMT"}, {"version": "v2", "created": "Sun, 30 Oct 2016 19:38:50 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Durante", "Daniele", ""]]}, {"id": "1610.03458", "submitter": "Philippe Roy Dr.", "authors": "Philippe Roy, Ren\\'e Laprise, Philippe Gachon", "title": "Sampling errors of quantile estimations from finite samples of data", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical relationships are derived for the expected sampling error of\nquantile estimations using Monte Carlo experiments for two frequency\ndistributions frequently encountered in climate sciences. The relationships\nfound are expressed as a scaling factor times the standard error of the mean;\nthese give a quick tool to estimate the uncertainty of quantiles for a given\nfinite sample size.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 18:42:33 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Roy", "Philippe", ""], ["Laprise", "Ren\u00e9", ""], ["Gachon", "Philippe", ""]]}, {"id": "1610.03580", "submitter": "Yongqiang Tang", "authors": "Yongqiang Tang", "title": "An efficient multiple imputation algorithm for control-based and\n  delta-adjusted pattern mixture models using SAS", "comments": "27 pages", "journal-ref": "Statistics in Biopharmaceutical research 2016", "doi": "10.1080/19466315.2016.1225595", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical trials, mixed effects models for repeated measures (MMRM) and\npattern mixture models (PMM) are often used to analyze longitudinal continuous\noutcomes. We describe a simple missing data imputation algorithm for the MMRM\nthat can be easily implemented in standard statistical software packages such\nas SAS PROC MI. We explore the relationship of the missing data distribution in\nthe control-based and delta-adjusted PMMs with that in the MMRM, and suggest an\nefficient imputation algorithm for these PMMs. The unobserved values in PMMs\ncan be imputed by subtracting the mean difference in the posterior predictive\ndistributions of missing data from the imputed values in MMRM. We also suggest\na modification of the copy reference imputation procedure to avoid the\npossibility that after dropout, subjects from the active treatment arm will\nhave better mean response trajectory than subjects who stay on the active\ntreatment. The proposed methods are illustrated by the analysis of an\nantidepressant trial.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 02:14:31 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Tang", "Yongqiang", ""]]}, {"id": "1610.03588", "submitter": "Bill Rea", "authors": "Alethea Rea and William Rea", "title": "How Many Components should be Retained from a Multivariate Time Series\n  PCA?", "comments": "49 Pages, 36 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on the results of two new approaches to considering how many\nprincipal components to retain from an analysis of a multivariate time series.\nThe first is by using a \"heat map\" based approach. A heat map in this context\nrefers to a series of principal component coefficients created by applying a\nsliding window to a multivariate time series. Furthermore the heat maps can\nprovide detailed insights into the evolution of the structure of each principal\ncomponent over time. The second is by examining the change of the angle of the\nprincipal component over time within the high-dimensional data space. We\nprovide evidence that both are useful in studying structure and evolution of a\nmultivariate time series.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 03:32:59 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 00:49:42 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Rea", "Alethea", ""], ["Rea", "William", ""]]}, {"id": "1610.03608", "submitter": "PuXue Qiao", "authors": "Puxue Qiao, Christina M{\\o}lck, Davide Ferrari and Fr\\'ed\\'eric\n  Hollande", "title": "A spatio-temporal model and inference tools for longitudinal count data\n  on multicolor cell growth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multicolor cell spatio-temporal image data have become important to\ninvestigate organ development and regeneration, malignant growth or immune\nresponses by tracking different cell types both in vivo and in vitro.\nStatistical modeling of image data from common longitudinal cell experiments\nposes significant challenges due to the presence of complex spatio-temporal\ninteractions between different cell types and difficulties related to\nmeasurement of single cell trajectories. Current analysis methods focus mainly\non univariate cases, often not considering the spatio-temporal effects\naffecting cell growth between different cell populations. In this paper, we\npropose a conditional spatial autoregressive model to describe multivariate\ncount cell data on the lattice, and develop inference tools. The proposed\nmethodology is computationally tractable and enables researchers to estimate a\ncomplete statistical model of multicolor cell growth. Our methodology is\napplied on real experimental data where we investigate how interactions between\ncells affect their growth. We include two case studies; the first evaluates\ninteractions between cancer cells and fibroblasts, which are normally present\nin the tumor microenvironment, whilst the second evaluates interactions between\ncloned cancer cells when grown as different combinations.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 06:26:02 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 23:33:58 GMT"}, {"version": "v3", "created": "Sun, 17 Sep 2017 09:21:07 GMT"}, {"version": "v4", "created": "Tue, 10 Apr 2018 03:38:41 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Qiao", "Puxue", ""], ["M\u00f8lck", "Christina", ""], ["Ferrari", "Davide", ""], ["Hollande", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1610.03687", "submitter": "Juho Kopra", "authors": "Juho Kopra, Juha Karvanen, Tommi H\\\"ark\\\"anen", "title": "Bayesian models for data missing not at random in health examination\n  surveys", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In epidemiological surveys, data missing not at random (MNAR) due to survey\nnonresponse may potentially lead to a bias in the risk factor estimates. We\npropose an approach based on Bayesian data augmentation and survival modelling\nto reduce the nonresponse bias. The approach requires additional information\nbased on follow-up data. We present a case study of smoking prevalence using\nFINRISK data collected between 1972 and 2007 with a follow-up to the end of\n2012 and compare it to other commonly applied missing at random (MAR)\nimputation approaches. A simulation experiment is carried out to study the\nvalidity of the approaches. Our approach appears to reduce the nonresponse bias\nsubstantially, where as MAR imputation was not successful in bias reduction.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 12:31:20 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 11:56:34 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Kopra", "Juho", ""], ["Karvanen", "Juha", ""], ["H\u00e4rk\u00e4nen", "Tommi", ""]]}, {"id": "1610.03725", "submitter": "Makoto Yamada", "authors": "Makoto Yamada and Yuta Umezu and Kenji Fukumizu and Ichiro Takeuchi", "title": "Post Selection Inference with Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel kernel based post selection inference (PSI) algorithm,\nwhich can not only handle non-linearity in data but also structured output such\nas multi-dimensional and multi-label outputs. Specifically, we develop a PSI\nalgorithm for independence measures, and propose the Hilbert-Schmidt\nIndependence Criterion (HSIC) based PSI algorithm (hsicInf). The novelty of the\nproposed algorithm is that it can handle non-linearity and/or structured data\nthrough kernels. Namely, the proposed algorithm can be used for wider range of\napplications including nonlinear multi-class classification and multi-variate\nregressions, while existing PSI algorithms cannot handle them. Through\nsynthetic experiments, we show that the proposed approach can find a set of\nstatistically significant features for both regression and classification\nproblems. Moreover, we apply the hsicInf algorithm to a real-world data, and\nshow that hsicInf can successfully identify important features.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 14:23:09 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 00:34:11 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Yamada", "Makoto", ""], ["Umezu", "Yuta", ""], ["Fukumizu", "Kenji", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1610.03927", "submitter": "Yen-Chi Chen", "authors": "Yunhua Xiang, Yen-Chi Chen", "title": "Statistical Inference Using Mean Shift Denoising", "comments": "19 page, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study how the mean shift algorithm can be used to denoise a\ndataset. We introduce a new framework to analyze the mean shift algorithm as a\ndenoising approach by viewing the algorithm as an operator on a distribution\nfunction. We investigate how the mean shift algorithm changes the distribution\nand show that data points shifted by the mean shift concentrate around high\ndensity regions of the underlying density function. By using the mean shift as\na denoising method, we enhance the performance of several clustering\ntechniques, improve the power of two-sample tests, and obtain a new method for\nanomaly detection.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 03:36:55 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Xiang", "Yunhua", ""], ["Chen", "Yen-Chi", ""]]}, {"id": "1610.03938", "submitter": "Masanori Kawakita", "authors": "Masanori Kawakita and Jun'ichi Takeuchi", "title": "A Note on Model Selection for Small Sample Regression", "comments": "Sumbitted, Machine Learning, on Jul. 22nd, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risk estimator called \"Direct Eigenvalue Estimator\" (DEE) is studied. DEE\nwas developed for small sample regression. In contrast to many existing model\nselection criteria, derivation of DEE requires neither any asymptotic\nassumption nor any prior knowledge about the noise variance and the noise\ndistribution. It was reported that DEE performed well in small sample cases but\nDEE performed a little worse than the state-of-the-art ADJ. This seems somewhat\ncounter-intuitive because DEE was developed for specifically regression problem\nby exploiting available information exhaustively, while ADJ was developed for\ngeneral setting. In this paper, we point out that the derivation of DEE\nincludes an inappropriate part in spite that the resultant form of DEE is valid\nin a sense. As its result, DEE cannot derive its potential. We introduce a\nclass of valid risk estimators based on the idea of DEE and show that better\nrisk estimators (mDEE) can be found in the class. By numerical experiments, we\nverify that mDEE often performs better than or at least equally the original\nDEE and ADJ.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 04:53:24 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Kawakita", "Masanori", ""], ["Takeuchi", "Jun'ichi", ""]]}, {"id": "1610.03944", "submitter": "Kenneth Hung", "authors": "Kenneth Hung and William Fithian", "title": "Rank Verification for Exponential Families", "comments": null, "journal-ref": "Ann. Statist., Volume 47, Number 2 (2019), 758-782", "doi": "10.1214/17-AOS1634", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical experiments involve comparing multiple population groups.\nFor example, a public opinion poll may ask which of several political\ncandidates commands the most support; a social scientific survey may report the\nmost common of several responses to a question; or, a clinical trial may\ncompare binary patient outcomes under several treatment conditions to determine\nthe most effective treatment. Having observed the \"winner\" (largest observed\nresponse) in a noisy experiment, it is natural to ask whether that candidate,\nsurvey response, or treatment is actually the \"best\" (stochastically largest\nresponse). This article concerns the problem of rank verification --- post hoc\nsignificance tests of whether the orderings discovered in the data reflect the\npopulation ranks. For exponential family models, we show under mild conditions\nthat an unadjusted two-tailed pairwise test comparing the top two observations\n(i.e., comparing the \"winner\" to the \"runner-up\") is a valid test of whether\nthe winner is truly the best. We extend our analysis to provide equally simple\nprocedures to obtain lower confidence bounds on the gap between the winning\npopulation and the others, and to verify ranks beyond the first.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 05:34:30 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 17:37:37 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Hung", "Kenneth", ""], ["Fithian", "William", ""]]}, {"id": "1610.04078", "submitter": "Kefei Liu", "authors": "Kefei Liu, Jieping Ye, Yang Yang, Li Shen, Hui Jiang", "title": "A Unified Model for Differential Expression Analysis of RNA-seq Data via\n  L1-Penalized Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The RNA-sequencing (RNA-seq) is becoming increasingly popular for quantifying\ngene expression levels. Since the RNA-seq measurements are relative in nature,\nbetween-sample normalization of counts is an essential step in differential\nexpression (DE) analysis. The normalization of existing DE detection algorithms\nis ad hoc and performed once for all prior to DE detection, which may be\nsuboptimal since ideally normalization should be based on non-DE genes only and\nthus coupled with DE detection. We propose a unified statistical model for\njoint normalization and DE detection of log-transformed RNA-seq data.\nSample-specific normalization factors are modeled as unknown parameters in the\ngene-wise linear models and jointly estimated with the regression coefficients.\nBy imposing sparsity-inducing L1 penalty (or mixed L1/L2-norm for multiple\ntreatment conditions) on the regression coefficients, we formulate the problem\nas a penalized least-squares regression problem and apply the augmented\nlagrangian method to solve it. Simulation studies show that the proposed model\nand algorithms outperform existing methods in terms of detection power and\nfalse-positive rate when more than half of the genes are differentially\nexpressed and/or when the up- and down-regulated genes among DE genes are\nunbalanced in amount.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 22:44:08 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Liu", "Kefei", ""], ["Ye", "Jieping", ""], ["Yang", "Yang", ""], ["Shen", "Li", ""], ["Jiang", "Hui", ""]]}, {"id": "1610.04397", "submitter": "Robert Pich\\'e", "authors": "Robert Piche", "title": "Automatic numerical differentiation by maximum likelihood estimation of\n  state-space model", "comments": "submitted to Automatica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A linear Gaussian state-space smoothing algorithm is presented for estimation\nof derivatives from a sequence of noisy measurements. The algorithm uses\nnumerically stable square-root formulas, can handle simultaneous independent\nmeasurements and non-equally spaced abscissas, and can compute state estimates\nat points between the data abscissas. The state space model's parameters,\nincluding driving noise intensity, measurement variance, and initial state, are\ndetermined from the given data sequence using maximum likelihood estimation\ncomputed using a expectation maximisation iteration. In tests with synthetic\nbiomechanics data, the algorithm has equivalent or better accuracy compared to\nother automatic numerical differentiation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 10:28:12 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Piche", "Robert", ""]]}, {"id": "1610.04465", "submitter": "Mar Rodr\\'iguez-Girondo", "authors": "Mar Rodr\\'iguez-Girondo, Alexia Kakourou, Perttu Salo, Markus Perola,\n  Wilma E. Mesker, Rob A. E. M. Tollenaar, Jeanine Houwing-Duistermaat, Bart\n  J.A. Mertens", "title": "On the combination of omics data for prediction of binary outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enrichment of predictive models with new biomolecular markers is an important\ntask in high-dimensional omic applications. Increasingly, clinical studies\ninclude several sets of such omics markers available for each patient,\nmeasuring different levels of biological variation. As a result, one of the\nmain challenges in predictive research is the integration of different sources\nof omic biomarkers for the prediction of health traits. We review several\napproaches for the combination of omic markers in the context of binary outcome\nprediction, all based on double cross-validation and regularized regression\nmodels. We evaluate their performance in terms of calibration and\ndiscrimination and we compare their performance with respect to single-omic\nsource predictions. We illustrate the methods through the analysis of two real\ndatasets. On the one hand, we consider the combination of two fractions of\nproteomic mass spectrometry for the calibration of a diagnostic rule for the\ndetection of early-stage breast cancer. On the other hand, we consider\ntranscriptomics and metabolomics as predictors of obesity using data from the\nDietary, Lifestyle, and Genetic determinants of Obesity and Metabolic syndrome\n(DILGOM) study, a population-based cohort, from Finland.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 13:58:25 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Rodr\u00edguez-Girondo", "Mar", ""], ["Kakourou", "Alexia", ""], ["Salo", "Perttu", ""], ["Perola", "Markus", ""], ["Mesker", "Wilma E.", ""], ["Tollenaar", "Rob A. E. M.", ""], ["Houwing-Duistermaat", "Jeanine", ""], ["Mertens", "Bart J. A.", ""]]}, {"id": "1610.04467", "submitter": "Marco Compagnoni", "authors": "Marco Compagnoni, Alessia Pini, Antonio Canclini, Paolo Bestagini,\n  Fabio Antonacci, Stefano Tubaro, Augusto Sarti", "title": "A Geometrical-Statistical approach to outlier removal for TDOA\n  measuments", "comments": "30 pages, 10 figure, 3 tables, in press on IEEE Transactions on\n  Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2701311", "report-no": null, "categories": "cs.IT cs.SD math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The curse of outlier measurements in estimation problems is a well known\nissue in a variety of fields. Therefore, outlier removal procedures, which\nenables the identification of spurious measurements within a set, have been\ndeveloped for many different scenarios and applications. In this paper, we\npropose a statistically motivated outlier removal algorithm for time\ndifferences of arrival (TDOAs), or equivalently range differences (RD),\nacquired at sensor arrays. The method exploits the TDOA-space formalism and\nworks by only knowing relative sensor positions. As the proposed method is\ncompletely independent from the application for which measurements are used, it\ncan be reliably used to identify outliers within a set of TDOA/RD measurements\nin different fields (e.g. acoustic source localization, sensor synchronization,\nradar, remote sensing, etc.). The proposed outlier removal algorithm is\nvalidated by means of synthetic simulations and real experiments.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 14:03:26 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 07:48:05 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Compagnoni", "Marco", ""], ["Pini", "Alessia", ""], ["Canclini", "Antonio", ""], ["Bestagini", "Paolo", ""], ["Antonacci", "Fabio", ""], ["Tubaro", "Stefano", ""], ["Sarti", "Augusto", ""]]}, {"id": "1610.04536", "submitter": "Rapha\\\"el Huser", "authors": "Raphael Huser, Thomas Opitz, Emeric Thibaud", "title": "Bridging Asymptotic Independence and Dependence in Spatial Extremes\n  Using Gaussian Scale Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian scale mixtures are constructed as Gaussian processes with a random\nvariance. They have non-Gaussian marginals and can exhibit asymptotic\ndependence unlike Gaussian processes, which are asymptotically independent\nexcept in the case of perfect dependence. In this paper, we study in detail the\nextremal dependence properties of Gaussian scale mixtures and we unify and\nextend general results on their joint tail decay rates in both asymptotic\ndependence and independence cases. Motivated by the analysis of spatial\nextremes, we propose several flexible yet parsimonious parametric copula models\nthat smoothly interpolate from asymptotic dependence to independence and\ninclude the Gaussian dependence as a special case. We show how these new models\ncan be fitted to high threshold exceedances using a censored likelihood\napproach, and we demonstrate that they provide valuable information about tail\ncharacteristics. Our parametric approach outperforms the widely used\nnonparametric $\\chi$ and $\\bar\\chi$ statistics often used to guide model choice\nat an exploratory stage by borrowing strength across locations for better\nestimation of the asymptotic dependence class. We demonstrate the capacity of\nour methodology by adequately capturing the extremal properties of wind speed\ndata collected in the Pacific Northwest, US.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 17:07:08 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 23:16:28 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Huser", "Raphael", ""], ["Opitz", "Thomas", ""], ["Thibaud", "Emeric", ""]]}, {"id": "1610.04580", "submitter": "Jelena Bradic", "authors": "Yinchu Zhu and Jelena Bradic", "title": "Two-sample testing in non-sparse high-dimensional linear models", "comments": "55 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In analyzing high-dimensional models, sparsity of the model parameter is a\ncommon but often undesirable assumption. In this paper, we study the following\ntwo-sample testing problem: given two samples generated by two high-dimensional\nlinear models, we aim to test whether the regression coefficients of the two\nlinear models are identical. We propose a framework named TIERS (short for\nTestIng Equality of Regression Slopes), which solves the two-sample testing\nproblem without making any assumptions on the sparsity of the regression\nparameters. TIERS builds a new model by convolving the two samples in such a\nway that the original hypothesis translates into a new moment condition. A\nself-normalization construction is then developed to form a moment test. We\nprovide rigorous theory for the developed framework. Under very weak conditions\nof the feature covariance, we show that the accuracy of the proposed test in\ncontrolling Type I errors is robust both to the lack of sparsity in the\nfeatures and to the heavy tails in the error distribution, even when the sample\nsize is much smaller than the feature dimension. Moreover, we discuss minimax\noptimality and efficiency properties of the proposed test. Simulation analysis\ndemonstrates excellent finite-sample performance of our test. In deriving the\ntest, we also develop tools that are of independent interest. The test is built\nupon a novel estimator, called Auto-aDaptive Dantzig Selector (ADDS), which not\nonly automatically chooses an appropriate scale of the error term but also\nincorporates prior information. To effectively approximate the critical value\nof the test statistic, we develop a novel high-dimensional plug-in approach\nthat complements the recent advances in Gaussian approximation theory.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 18:51:34 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Zhu", "Yinchu", ""], ["Bradic", "Jelena", ""]]}, {"id": "1610.04960", "submitter": "Damian Brzyski", "authors": "Damian Brzyski and Alexej Gossmann and Weijie Su and Malgorzata Bogdan", "title": "Group SLOPE - adaptive selection of groups of predictors", "comments": "40 pages, 22 paged in Appendix, 5 figures included. arXiv admin note:\n  text overlap with arXiv:1511.09078", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorted L-One Penalized Estimation (SLOPE) is a relatively new convex\noptimization procedure which allows for adaptive selection of regressors under\nsparse high dimensional designs. Here we extend the idea of SLOPE to deal with\nthe situation when one aims at selecting whole groups of explanatory variables\ninstead of single regressors. Such groups can be formed by clustering strongly\ncorrelated predictors or groups of dummy variables corresponding to different\nlevels of the same qualitative predictor. We formulate the respective convex\noptimization problem, gSLOPE (group SLOPE), and propose an efficient algorithm\nfor its solution. We also define a notion of the group false discovery rate\n(gFDR) and provide a choice of the sequence of tuning parameters for gSLOPE so\nthat gFDR is provably controlled at a prespecified level if the groups of\nvariables are orthogonal to each other. Moreover, we prove that the resulting\nprocedure adapts to unknown sparsity and is asymptotically minimax with respect\nto the estimation of the proportions of variance of the response variable\nexplained by regressors from different groups. We also provide a method for the\nchoice of the regularizing sequence when variables in different groups are not\northogonal but statistically independent and illustrate its good properties\nwith computer simulations. Finally, we illustrate the advantages of gSLOPE in\nthe context of Genome Wide Association Studies. R package grpSLOPE with\nimplementation of our method is available on CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 02:59:12 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Brzyski", "Damian", ""], ["Gossmann", "Alexej", ""], ["Su", "Weijie", ""], ["Bogdan", "Malgorzata", ""]]}, {"id": "1610.05005", "submitter": "Caleb Miles", "authors": "Caleb H. Miles, Joel Schwartz, Eric J. Tchetgen Tchetgen", "title": "A Class of Semiparametric Tests of Treatment Effect Robust to Confounder\n  Classical Measurement Error", "comments": "35 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When assessing the presence of an exposure causal effect on a given outcome,\nit is well known that classical measurement error of the exposure can reduce\nthe power of a test of the null hypothesis in question, although its type I\nerror rate will generally remain at the nominal level. In contrast, classical\nmeasurement error of a confounder can inflate the type I error rate of a test\nof treatment effect. In this paper, we develop a large class of semiparametric\ntest statistics of an exposure causal effect, which are completely robust to\nclassical measurement error of a subset of confounders. A unique and appealing\nfeature of our proposed methods is that they require no external information\nsuch as validation data or replicates of error-prone confounders. We present a\ndoubly-robust form of this test that requires only one of two models to be\ncorrectly specified for the resulting test statistic to have correct type I\nerror rate. We demonstrate validity and power within our class of test\nstatistics through simulation studies. We apply the methods to a\nmulti-U.S.-city, time-series data set to test for an effect of temperature on\nmortality while adjusting for atmospheric particulate matter with diameter of\n2.5 micrometres or less (PM2.5), which is known to be measured with error.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 08:20:06 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Miles", "Caleb H.", ""], ["Schwartz", "Joel", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "1610.05035", "submitter": "H{\\aa}kon Otneim", "authors": "H{\\aa}kon Otneim and Dag Tj{\\o}stheim", "title": "Non-parametric estimation of conditional densities: A new method", "comments": null, "journal-ref": "Otneim, H{\\aa}kon, and Dag Tj{\\o}stheim. \"Conditional density\n  estimation using the local Gaussian correlation.\" Statistics and Computing\n  28.2 (2018): 303-321", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\textbf{X} = (X_1,\\ldots, X_p)$ be a stochastic vector having joint\ndensity function $f_{\\textbf{X}}(x)$ with partitions $\\textbf{X}_1 =\n(X_1,\\ldots, X_k)$ and $\\textbf{X}_2 = (X_{k+1},\\ldots, X_p)$. A new method for\nestimating the conditional density function of $\\textbf{X}_1$ given\n$\\textbf{X}_2$ is presented. It is based on locally Gaussian approximations,\nbut simplified in order to tackle the curse of dimensionality in multivariate\napplications, where both response and explanatory variables can be vectors. We\ncompare our method to some available competitors, and the error of\napproximation is shown to be small in a series of examples using real and\nsimulated data, and the estimator is shown to be particularly robust against\nnoise caused by independent variables. We also present examples of practical\napplications of our conditional density estimator in the analysis of time\nseries. Typical values for $k$ in our examples are 1 and 2, and we include\nsimulation experiments with values of $p$ up to 6. Large sample theory is\nestablished under a strong mixing condition.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 09:58:22 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Otneim", "H\u00e5kon", ""], ["Tj\u00f8stheim", "Dag", ""]]}, {"id": "1610.05045", "submitter": "Luisa Cutillo", "authors": "Annamaria Carissimo and Luisa Cutillo and Italia Defeis", "title": "Validation of community robustness", "comments": "arXiv admin note: text overlap with arXiv:0908.1062,\n  arXiv:cond-mat/0610077 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large amount of work on community detection and its applications leaves\nunaddressed one important question: the statistical validation of the results.\nIn this paper we present a methodology able to clearly detect if the community\nstructure found by some algorithms is statistically significant or is a result\nof chance, merely due to edge positions in the network. Given a community\ndetection method and a network of interest, our proposal examines the stability\nof the partition recovered against random perturbations of the original graph\nstructure. To address this issue, we specify a perturbation strategy and a null\nmodel to build a set of procedures based on a special measure of clustering\ndistance, namely Variation of Information, using tools set up for functional\ndata analysis. The procedures determine whether the obtained clustering departs\nsignificantly from the null model. This strongly supports the robustness\nagainst perturbation of the algorithm used to identify the community structure.\nWe show the results obtained with the proposed technique on simulated and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 11:16:18 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Carissimo", "Annamaria", ""], ["Cutillo", "Luisa", ""], ["Defeis", "Italia", ""]]}, {"id": "1610.05076", "submitter": "Xing He", "authors": "Xing He, Lei Chu, Robert C. Qiu, Qian Ai, Zenan Ling", "title": "A Novel Data-Driven Situation Awareness Approach for Future Grids--Using\n  Large Random Matrices for Big Data Modeling", "comments": "10 pages, 14 figures, 2 tables, Submit to IEEE Access. Personal use\n  of this material is permitted. Permission from IEEE must be obtained for all\n  other uses", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven approaches, when tasked with situation awareness, are suitable\nfor complex grids with massive datasets. It is a challenge, however, to\nefficiently turn these massive datasets into useful big data analytics. To\naddress such a challenge, this paper, based on random matrix theory (RMT),\nproposes a datadriven approach. The approach models massive datasets as large\nrandom matrices; it is model-free and requiring no knowledge about physical\nmodel parameters. In particular, the large data dimension N and the large time\nspan T, from the spatial aspect and the temporal aspect respectively, lead to\nfavorable results. The beautiful thing lies in that these linear eigenvalue\nstatistics (LESs) built from data matrices follow Gaussian distributions for\nvery general conditions, due to the latest breakthroughs in probability on the\ncentral limit theorems of those LESs. Numerous case studies, with both\nsimulated data and field data, are given to validate the proposed new\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 12:31:24 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 20:40:18 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["He", "Xing", ""], ["Chu", "Lei", ""], ["Qiu", "Robert C.", ""], ["Ai", "Qian", ""], ["Ling", "Zenan", ""]]}, {"id": "1610.05131", "submitter": "Patrick Laurie Davies Mr", "authors": "Laurie Davies", "title": "Stepwise Choice of Covariates in High Dimensional Regression", "comments": "This is a revised version of 1610.05131. It contains some results on\n  false postives, an analysis of the birthday data also analysed in \"Bayesian\n  Data Analysis\" (Chapman & Hall/CRC Texts in Statistical Science) and an\n  application to the construction of dependency graphs. 38 pages and one figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data y(n) and p(n)covariates x(n) one problem in linear regression is\nto decide which if any of the covariates to include. There are many articles on\nthis problem but all are based on a stochastic model for the data. This paper\ngives what seems to be a new approach which does not require any form of model.\nIt is conceptually and algorithmically simple and consistency results can be\nproved under appropriate assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 14:16:43 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 17:37:20 GMT"}, {"version": "v3", "created": "Mon, 2 Oct 2017 18:55:01 GMT"}, {"version": "v4", "created": "Thu, 5 Oct 2017 15:21:47 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Davies", "Laurie", ""]]}, {"id": "1610.05246", "submitter": "Kai Zhang", "authors": "Kai Zhang", "title": "BET on Independence", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2018.1537921", "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of nonparametric dependence detection. Many existing\nmethods may suffer severe power loss due to non-uniform consistency, which we\nillustrate with a paradox. To avoid such power loss, we approach the\nnonparametric test of independence through the new framework of binary\nexpansion statistics (BEStat) and binary expansion testing (BET), which examine\ndependence through a novel binary expansion filtration approximation of the\ncopula. Through a Hadamard transform, we find that the symmetry statistics in\nthe filtration are complete sufficient statistics for dependence. These\nstatistics are also uncorrelated under the null. By utilizing symmetry\nstatistics, the BET avoids the problem of non-uniform consistency and improves\nupon a wide class of commonly used methods (a) by achieving the minimax rate in\nsample size requirement for reliable power and (b) by providing clear\ninterpretations of global relationships upon rejection of independence. The\nbinary expansion approach also connects the symmetry statistics with the\ncurrent computing system to facilitate efficient bitwise implementation. We\nillustrate the BET with a study of the distribution of stars in the night sky\nand with an exploratory data analysis of the TCGA breast cancer data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 18:19:49 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 03:26:00 GMT"}, {"version": "v3", "created": "Thu, 26 Jan 2017 07:09:37 GMT"}, {"version": "v4", "created": "Sun, 23 Apr 2017 02:08:08 GMT"}, {"version": "v5", "created": "Mon, 20 Nov 2017 15:57:14 GMT"}, {"version": "v6", "created": "Sun, 13 May 2018 02:25:46 GMT"}, {"version": "v7", "created": "Mon, 15 Apr 2019 20:39:38 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Kai", ""]]}, {"id": "1610.05280", "submitter": "Yair Daon", "authors": "Yair Daon, Georg Stadler", "title": "Mitigating the Influence of the Boundary on PDE-based Covariance\n  Operators", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian random fields over infinite-dimensional Hilbert spaces require the\ndefinition of appropriate covariance operators. The use of elliptic PDE\noperators to construct covariance operators allows to build on fast PDE solvers\nfor manipulations with the resulting covariance and precision operators.\nHowever, PDE operators require a choice of boundary conditions, and this choice\ncan have a strong and usually undesired influence on the Gaussian random field.\nWe propose two techniques that allow to ameliorate these boundary effects for\nlarge-scale problems. The first approach combines the elliptic PDE operator\nwith a Robin boundary condition, where a varying Robin coefficient is computed\nfrom an optimization problem. The second approach normalizes the pointwise\nvariance by rescaling the covariance operator. These approaches can be used\nindividually or can be combined. We study properties of these approaches, and\ndiscuss their computational complexity. The performance of our approaches is\nstudied for random fields defined over simple and complex two- and\nthree-dimensional domains.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 19:27:48 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 18:24:40 GMT"}, {"version": "v3", "created": "Mon, 11 Dec 2017 20:12:51 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Daon", "Yair", ""], ["Stadler", "Georg", ""]]}, {"id": "1610.05440", "submitter": "Eero Siivola", "authors": "Eero Siivola, Juho Piironen, Aki Vehtari", "title": "Automatic monotonicity detection for Gaussian Processes", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for automatically detecting monotonic input-output\nrelationships from data using Gaussian Process (GP) models with virtual\nderivative observations. Our results on synthetic and real datasets show that\nthe proposed method detects monotonic directions from input spaces with high\naccuracy. We expect the method to be useful especially for improving\nexplainability of the models and improving the accuracy of regression and\nclassification tasks, especially near the edges of the data or when\nextrapolating.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 05:35:52 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Siivola", "Eero", ""], ["Piironen", "Juho", ""], ["Vehtari", "Aki", ""]]}, {"id": "1610.05559", "submitter": "Juho Piironen", "authors": "Juho Piironen, Aki Vehtari", "title": "On the Hyperprior Choice for the Global Shrinkage Parameter in the\n  Horseshoe Prior", "comments": "Appearing in AISTATS 2017, added one experiment to the supplementary", "journal-ref": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics, PMLR 54:905-913, 2017", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The horseshoe prior has proven to be a noteworthy alternative for sparse\nBayesian estimation, but as shown in this paper, the results can be sensitive\nto the prior choice for the global shrinkage hyperparameter. We argue that the\nprevious default choices are dubious due to their tendency to favor solutions\nwith more unshrunk coefficients than we typically expect a priori. This can\nlead to bad results if this parameter is not strongly identified by data. We\nderive the relationship between the global parameter and the effective number\nof nonzeros in the coefficient vector, and show an easy and intuitive way of\nsetting up the prior for the global parameter based on our prior beliefs about\nthe number of nonzero coefficients in the model. The results on real world data\nshow that one can benefit greatly -- in terms of improved parameter estimates,\nprediction accuracy, and reduced computation time -- from transforming even a\ncrude guess for the number of nonzero coefficients into the prior for the\nglobal parameter using our framework.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 12:14:11 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 13:10:23 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Piironen", "Juho", ""], ["Vehtari", "Aki", ""]]}, {"id": "1610.05604", "submitter": "Nathan Kallus", "authors": "Nathan Kallus, Madeleine Udell", "title": "Dynamic Assortment Personalization in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of dynamic assortment personalization with large,\nheterogeneous populations and wide arrays of products, and demonstrate the\nimportance of structural priors for effective, efficient large-scale\npersonalization. Assortment personalization is the problem of choosing, for\neach individual (type), a best assortment of products, ads, or other offerings\n(items) so as to maximize revenue. This problem is central to revenue\nmanagement in e-commerce and online advertising where both items and types can\nnumber in the millions.\n  We formulate the dynamic assortment personalization problem as a\ndiscrete-contextual bandit with $m$ contexts (types) and exponentially many\narms (assortments of the $n$ items). We assume that each type's preferences\nfollow a simple parametric model with $n$ parameters. In all, there are $mn$\nparameters, and existing literature suggests that order optimal regret scales\nas $mn$. However, the data required to estimate so many parameters is orders of\nmagnitude larger than the data available in most revenue management\napplications; and the optimal regret under these models is unacceptably high.\n  In this paper, we impose a natural structure on the problem -- a small latent\ndimension, or low rank. In the static setting, we show that this model can be\nefficiently learned from surprisingly few interactions, using a time- and\nmemory-efficient optimization algorithm that converges globally whenever the\nmodel is learnable. In the dynamic setting, we show that structure-aware\ndynamic assortment personalization can have regret that is an order of\nmagnitude smaller than structure-ignorant approaches. We validate our\ntheoretical results empirically.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 13:32:36 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 15:42:42 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 14:46:44 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Kallus", "Nathan", ""], ["Udell", "Madeleine", ""]]}, {"id": "1610.05683", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth and Francisco J. R. Ruiz and Scott W. Linderman\n  and David M. Blei", "title": "Reparameterization Gradients through Acceptance-Rejection Sampling\n  Algorithms", "comments": "An error in the von Mises distribution reparameterization in Table 2\n  has been corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference using the reparameterization trick has enabled\nlarge-scale approximate Bayesian inference in complex probabilistic models,\nleveraging stochastic optimization to sidestep intractable expectations. The\nreparameterization trick is applicable when we can simulate a random variable\nby applying a differentiable deterministic function on an auxiliary random\nvariable whose distribution is fixed. For many distributions of interest (such\nas the gamma or Dirichlet), simulation of random variables relies on\nacceptance-rejection sampling. The discontinuity introduced by the\naccept-reject step means that standard reparameterization tricks are not\napplicable. We propose a new method that lets us leverage reparameterization\ngradients even when variables are outputs of a acceptance-rejection sampling\nalgorithm. Our approach enables reparameterization on a larger class of\nvariational distributions. In several studies of real and synthetic data, we\nshow that the variance of the estimator of the gradient is significantly lower\nthan other state-of-the-art methods. This leads to faster convergence of\nstochastic gradient variational inference.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 15:55:08 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 14:16:52 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2020 15:01:15 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Ruiz", "Francisco J. R.", ""], ["Linderman", "Scott W.", ""], ["Blei", "David M.", ""]]}, {"id": "1610.05695", "submitter": "Pierre Masselot", "authors": "Pierre Masselot, Fateh Chebana, Taha B.M.J. Ouarda", "title": "Fast and direct nonparametric procedures in the L-moment homogeneity\n  test", "comments": "37 pages, 5 figures", "journal-ref": "Masselot P, Chebana F, Ouarda TBMJ. Fast and direct nonparametric\n  procedures in the L-moment homogeneity test. Stochastic Environmental\n  Research and Risk Assessment. 2016:1-14", "doi": "10.1007/s00477-016-1248-0", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regional frequency analysis is an important tool to properly estimate\nhydrological characteristics at ungauged or partially gauged sites in order to\nprevent hydrological disasters. The delineation of homogeneous groups of sites\nis an important first step in order to transfer information and obtain accurate\nquantile estimates at the target site. The Hosking-Wallis homogeneity test is\nusually used to test the homogeneity of the selected sites. Despite its\nusefulness and good power, it presents some drawbacks including the subjective\nchoice of a parametric distribution for the data and a poorly justified\nrejection threshold. The present paper addresses these drawbacks by integrating\nnonparametric procedures in the L-moment homogeneity test. To assess the\nrejection threshold, three resampling methods (permutation, bootstrap and\nP\\'olya resampling) are considered. Results indicate that permutation and\nbootstrap methods perform better than the parametric Hosking-Wallis test in\nterms of power as well as in time and procedure simplicity. A real-world case\nstudy shows that the nonparametric tests agree with the HW test concerning the\nhomogeneity of the volume and the bivariate case while they disagree for the\npeak case, but that the assumptions of the HW test are not well respected.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 16:25:47 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Masselot", "Pierre", ""], ["Chebana", "Fateh", ""], ["Ouarda", "Taha B. M. J.", ""]]}, {"id": "1610.05747", "submitter": "Teague Henry", "authors": "Teague R Henry, Kathleen M Gates, Mitchell J Prinstein, Douglas\n  Steinley", "title": "Modeling Heterogeneous Peer Assortment Effects using Finite Mixture\n  Exponential Random Graph Models", "comments": null, "journal-ref": "Psychometrika 2019 pp 1 27", "doi": "10.1007/s11336-019-09685-2", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a class of models called Sender/Receiver Finite Mixture\nExponential Random Graph Models (SRFM-ERGMs) that enables inference on\nnetworks. This class of models extends the existing Exponential Random Graph\nModeling framework to allow analysts to model unobserved heterogeneity in the\neffects of nodal covariates and network features. An empirical example\nregarding substance use among adolescents is presented. Simulations across a\nvariety of conditions are used to evaluate the performance of this technique.\nWe conclude that that unobserved heterogeneity in effects of nodal covariates\ncan be a major cause of mis-fit in network models, and the SRFM-ERGM approach\ncan alleviate this misfit. Implications for the analysis of social networks in\npsychological science are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 19:21:12 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 20:39:52 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Henry", "Teague R", ""], ["Gates", "Kathleen M", ""], ["Prinstein", "Mitchell J", ""], ["Steinley", "Douglas", ""]]}, {"id": "1610.05795", "submitter": "Rogemar Mamon", "authors": "Fuqi Chen, Rogemar Mamon, Matt Davison", "title": "Estimation of multiple change points under a generalised\n  Ornstein-Uhlenbeck framework", "comments": "15 figures, 7 tables, 46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of an Ornstein-Uhlenbeck (OU) process is ubiquitous in business,\neconomics and finance to capture various price processes and evolution of\neconomic indicators exhibiting mean-reverting properties. When structural\nchanges happen, economic dynamics drastically change and the times at which\nthese occur are of particular interest to policy makers, investors and\nfinancial product providers. This paper addresses the change-point problem\nunder a generalised OU model and investigates the associated statistical\ninference. We propose two estimation methods to locate multiple change points\nand show the asymptotic properties of the estimators. An informational approach\nis employed in detecting the change points, and the consistency of our methods\nis also theoretically demonstrated. Estimation is considered under the setting\nwhere both the number and location of change points are unknown. Three\ncomputing algorithms are further developed for implementation. The practical\napplicability of our methods is illustrated using simulated and observed\nfinancial market data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 20:39:49 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 06:15:10 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Chen", "Fuqi", ""], ["Mamon", "Rogemar", ""], ["Davison", "Matt", ""]]}, {"id": "1610.05809", "submitter": "Jiahua Chen", "authors": "Jiahua Chen, Pengfei Li, Yukun Liu, James V. Zidek", "title": "Monitoring test under nonparametric random effects model", "comments": "38 pages, 1 figure, many table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factors such as climate change, forest fire and plague of insects, lead to\nconcerns on the mechanical strength of plantation materials. To address such\nconcerns, these products must be closely monitored. This leads to the need of\nupdating lumber quality monitoring procedures in American Society for Testing\nand Materials (ASTM) Standard D1990 (adopted in 1991) from time to time. A key\ncomponent of monitoring is an effective method for detecting the change in\nlower percentiles of the solid lumber strength based on multiple samples. In a\nrecent study by Verrill et al.\\ (2015), eight statistical tests proposed by\nwood scientists were examined thoroughly based on real and simulated data sets.\nThese tests are found unsatisfactory in differing aspects such as seriously\ninflated false alarm rate when observations are clustered, suboptimal power\nproperties, or having inconvenient ad hoc rejection regions. A contributing\nfactor behind suboptimal performance is that most of these tests are not\ndeveloped to detect the change in quantiles. In this paper, we use a\nnonparametric random effects model to handle the within cluster correlations,\ncomposite empirical likelihood to avoid explicit modelling of the correlations\nstructure, and a density ratio model to combine the information from multiple\nsamples. In addition, we propose a cluster-based bootstrapping procedure to\nconstruct the monitoring test on quantiles which satisfactorily controls the\ntype I error in the presence of within cluster correlation. The performance of\nthe test is examined through simulation experiments and a real world example.\nThe new method is generally applicable, not confined to the motivating example.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 21:35:15 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Chen", "Jiahua", ""], ["Li", "Pengfei", ""], ["Liu", "Yukun", ""], ["Zidek", "James V.", ""]]}, {"id": "1610.05857", "submitter": "Andrew McDavid", "authors": "Andrew McDavid, Raphael Gottardo, Noah Simon and Mathias Drton", "title": "Graphical Models for Zero-Inflated Single Cell Gene Expression", "comments": "Fixed error in software URL", "journal-ref": "Ann. Appl. Stat., Volume 13, Number 2 (2019), 848-873", "doi": "10.1214/18-AOAS1213", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bulk gene expression experiments relied on aggregations of thousands of cells\nto measure the average expression in an organism. Advances in microfluidic and\ndroplet sequencing now permit expression profiling in single cells. This study\nof cell-to-cell variation reveals that individual cells lack detectable\nexpression of transcripts that appear abundant on a population level, giving\nrise to zero-inflated expression patterns. To infer gene co-regulatory networks\nfrom such data, we propose a multivariate Hurdle model. It is comprised of a\nmixture of singular Gaussian distributions. We employ neighborhood selection\nwith the pseudo-likelihood and a group lasso penalty to select and fit\nundirected graphical models that capture conditional independences between\ngenes. The proposed method is more sensitive than existing approaches in\nsimulations, even under departures from our Hurdle model. The method is applied\nto data for T follicular helper cells, and a high-dimensional profile of mouse\ndendritic cells. It infers network structure not revealed by other methods; or\nin bulk data sets. An R implementation is available at\nhttps://github.com/amcdavid/HurdleNormal .\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 03:53:13 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 19:44:10 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 18:00:22 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["McDavid", "Andrew", ""], ["Gottardo", "Raphael", ""], ["Simon", "Noah", ""], ["Drton", "Mathias", ""]]}, {"id": "1610.05868", "submitter": "Ian Barnett", "authors": "Ian Barnett, Nishant Malik, Marieke L. Kuijjer, Peter J. Mucha,\n  Jukka-Pekka Onnela", "title": "Feature-Based Classification of Networks", "comments": "14 pages including 4 figures and a table. Methods and supplementary\n  material included to the end", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network representations of systems from various scientific and societal\ndomains are neither completely random nor fully regular, but instead appear to\ncontain recurring structural building blocks. These features tend to be shared\nby networks belonging to the same broad class, such as the class of social\nnetworks or the class of biological networks. At a finer scale of\nclassification within each such class, networks describing more similar systems\ntend to have more similar features. This occurs presumably because networks\nrepresenting similar purposes or constructions would be expected to be\ngenerated by a shared set of domain specific mechanisms, and it should\ntherefore be possible to classify these networks into categories based on their\nfeatures at various structural levels. Here we describe and demonstrate a new,\nhybrid approach that combines manual selection of features of potential\ninterest with existing automated classification methods. In particular,\nselecting well-known and well-studied features that have been used throughout\nsocial network analysis and network science and then classifying with methods\nsuch as random forests that are of special utility in the presence of feature\ncollinearity, we find that we achieve higher accuracy, in shorter computation\ntime, with greater interpretability of the network classification results.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 05:19:06 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Barnett", "Ian", ""], ["Malik", "Nishant", ""], ["Kuijjer", "Marieke L.", ""], ["Mucha", "Peter J.", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1610.05996", "submitter": "Shuangge Ma", "authors": "Zhixuan Fu and Shuangge Ma and Haiqun Lin and Chirag R Parikh and\n  Bingqing Zhou", "title": "Penalized Variable Selection for Multi-center Competing Risks Data", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider variable selection in competing risks regression for multi-center\ndata. Our research is motivated by deceased donor kidney transplants, from\nwhich recipients would experience graft failure, death with functioning graft\n(DWFG), or graft survival. The occurrence of DWFG precludes graft failure from\nhappening and therefore is a competing risk. Data within a transplant center\nmay be correlated due to a latent center effect, such as varying patient\npopulations, surgical techniques, and patient management. The proportional\nsubdistribution hazard (PSH) model has been frequently used in the regression\nanalysis of competing risks data. Two of its extensions, the stratified and the\nmarginal PSH models, can be applied to multi-center data to account for the\ncenter effect. In this paper, we propose penalization strategies for the two\nmodels, primarily to select important variables and estimate their effects\nwhereas correlations within centers serve as a nuisance. Simulations\ndemonstrate good performance and computational efficiency for the proposed\nmethods. It is further assessed using an analysis of data from the United\nNetwork of Organ Sharing.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 13:01:57 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Fu", "Zhixuan", ""], ["Ma", "Shuangge", ""], ["Lin", "Haiqun", ""], ["Parikh", "Chirag R", ""], ["Zhou", "Bingqing", ""]]}, {"id": "1610.06145", "submitter": "Fan Zhang", "authors": "Fan Zhang, Chuangqi Wang, Andrew Trapp, Patrick Flaherty", "title": "A global optimization algorithm for sparse mixed membership matrix\n  factorization", "comments": "19 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed membership factorization is a popular approach for analyzing data sets\nthat have within-sample heterogeneity. In recent years, several algorithms have\nbeen developed for mixed membership matrix factorization, but they only\nguarantee estimates from a local optimum. Here, we derive a global optimization\n(GOP) algorithm that provides a guaranteed $\\epsilon$-global optimum for a\nsparse mixed membership matrix factorization problem. We test the algorithm on\nsimulated data and find the algorithm always bounds the global optimum across\nrandom initializations and explores multiple modes efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 18:39:41 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 22:12:17 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Zhang", "Fan", ""], ["Wang", "Chuangqi", ""], ["Trapp", "Andrew", ""], ["Flaherty", "Patrick", ""]]}, {"id": "1610.06157", "submitter": "Rose Baker", "authors": "Rose Baker, Tarak Kharrat", "title": "Event count distributions from renewal processes: fast computation of\n  probabilities", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete distributions derived from renewal processes, ie distributions of\nthe number of events by some time t are beginning to be used in econometrics\nand health sciences. A new fast method is presented for computation of the\nprobabilities for these distributions. We calculate the count probabilities by\nrepeatedly convolving the discretized distribution, and then correct them using\nRichardson extrapolation. When just one probability is required, a second\nalgorithm is described, an adaptation of De Pril's method, in which the\ncomputation time does not depend on the ordinality, so that even high-order\nprobabilities can be rapidly found. Any survival distribution can be used to\nmodel the inter-arrival times, which gives a rich class of models with great\nflexibility for modelling both underdispersed and overdispersed data. This work\ncould pave the way for the routine use of these distributions as an additional\ntool for modelling event count data. An empirical example using fertility data\nillustrates the use of the method and was fully implemented using an R package\nCountr developed by the authors and available from the Comprehensive R Archive\nNetwork.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 19:10:15 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Baker", "Rose", ""], ["Kharrat", "Tarak", ""]]}, {"id": "1610.06279", "submitter": "Nan Zou", "authors": "Nan Zou and Dimitris Politis", "title": "Linear Process Bootstrap Unit Root Test", "comments": null, "journal-ref": null, "doi": "10.1016/j.spl.2018.08.006", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most widely applied unit root test, Phillips-Perron test, enjoys\nin general highpowers, but suffers from size distortions when moving average\nnoise exists. As a remedy, thispaper proposes a nonparametric bootstrap unit\nroot test that specifically targets moving aver-age noise. Via a bootstrap\nfunctional central limit theorem, the consistency of this bootstrapapproach is\nestablished under general assumptions which allows a large family of non-linear\ntimeseries. In simulation, this bootstrap test alleviates the size distortions\nof the Phillips-Perrontest while preserving its high powers.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 03:39:38 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Zou", "Nan", ""], ["Politis", "Dimitris", ""]]}, {"id": "1610.06462", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Michael Gutmann, Aki Vehtari, Pekka Marttinen", "title": "Gaussian process modeling in approximate Bayesian computation to\n  estimate horizontal gene transfer in bacteria", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) can be used for model fitting when the\nlikelihood function is intractable but simulating from the model is feasible.\nHowever, even a single evaluation of a complex model may take several hours,\nlimiting the number of model evaluations available. Modelling the discrepancy\nbetween the simulated and observed data using a Gaussian process (GP) can be\nused to reduce the number of model evaluations required by ABC, but the\nsensitivity of this approach to a specific GP formulation has not yet been\nthoroughly investigated. We begin with a comprehensive empirical evaluation of\nusing GPs in ABC, including various transformations of the discrepancies and\ntwo novel GP formulations. Our results indicate the choice of GP may\nsignificantly affect the accuracy of the estimated posterior distribution.\nSelection of an appropriate GP model is thus important. We formulate expected\nutility to measure the accuracy of classifying discrepancies below or above the\nABC threshold, and show that it can be used to automate the GP model selection\nstep. Finally, based on the understanding gained with toy examples, we fit a\npopulation genetic model for bacteria, providing insight into horizontal gene\ntransfer events within the population and from external origins.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 15:39:15 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 07:57:22 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 12:24:27 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Gutmann", "Michael", ""], ["Vehtari", "Aki", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1610.06506", "submitter": "Sara Shashaani", "authors": "Sara Shashaani, Fatemeh Hashemi, Raghu Pasupathy", "title": "ASTRO-DF: A Class of Adaptive Sampling Trust-Region Algorithms for\n  Derivative-Free Stochastic Optimization", "comments": "27 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider unconstrained optimization problems where only \"stochastic\"\nestimates of the objective function are observable as replicates from a Monte\nCarlo oracle. The Monte Carlo oracle is assumed to provide no direct\nobservations of the function gradient. We present ASTRO-DF --- a class of\nderivative-free trust-region algorithms, where a stochastic local interpolation\nmodel is constructed, optimized, and updated iteratively. Function estimation\nand model construction within ASTRO-DF is adaptive in the sense that the extent\nof Monte Carlo sampling is determined by continuously monitoring and balancing\nmetrics of sampling error (or variance) and structural error (or model bias)\nwithin ASTRO-DF. Such balancing of errors is designed to ensure that Monte\nCarlo effort within ASTRO-DF is sensitive to algorithm trajectory, sampling\nmore whenever an iterate is inferred to be close to a critical point and less\nwhen far away. We demonstrate the almost-sure convergence of ASTRO-DF's\niterates to a first-order critical point when using linear or quadratic\nstochastic interpolation models. The question of using more complicated models,\ne.g., regression or stochastic kriging, in combination with adaptive sampling\nis worth further investigation and will benefit from the methods of proof\npresented here. We speculate that ASTRO-DF's iterates achieve the canonical\nMonte Carlo convergence rate, although a proof remains elusive.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 17:13:18 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Shashaani", "Sara", ""], ["Hashemi", "Fatemeh", ""], ["Pasupathy", "Raghu", ""]]}, {"id": "1610.06511", "submitter": "James Wilson", "authors": "James D. Wilson, John Palowitch, Shankar Bhamidi, Andrew B. Nobel", "title": "Community extraction in multilayer networks with heterogeneous community\n  structure", "comments": "46 pages. Accepted at the Journal of Machine Learning Research\n  (11/17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer networks are a useful way to capture and model multiple, binary or\nweighted relationships among a fixed group of objects. While community\ndetection has proven to be a useful exploratory technique for the analysis of\nsingle-layer networks, the development of community detection methods for\nmultilayer networks is still in its infancy. We propose and investigate a\nprocedure, called Multilayer Extraction, that identifies densely connected\nvertex-layer sets in multilayer networks. Multilayer Extraction makes use of a\nsignificance based score that quantifies the connectivity of an observed\nvertex-layer set through comparison with a fixed degree random graph model.\nMultilayer Extraction directly handles networks with heterogeneous layers where\ncommunity structure may be different from layer to layer. The procedure can\ncapture overlapping communities, as well as background vertex-layer pairs that\ndo not belong to any community. We establish consistency of the vertex-layer\nset optimizer of our proposed multilayer score under the multilayer stochastic\nblock model. We investigate the performance of Multilayer Extraction on three\napplications and a test bed of simulations. Our theoretical and numerical\nevaluations suggest that Multilayer Extraction is an effective exploratory tool\nfor analyzing complex multilayer networks. Publicly available R software for\nMultilayer Extraction is available at\nhttps://github.com/jdwilson4/MultilayerExtraction.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 17:35:42 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 20:04:51 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 22:09:34 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Wilson", "James D.", ""], ["Palowitch", "John", ""], ["Bhamidi", "Shankar", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1610.06640", "submitter": "Andriy Olenko", "authors": "A. Olenko, K. T. Wong, H. Mir, and H. Al-Nashash", "title": "A Generalized Correlation Index for Quantifying Signal Morphological\n  Similarity", "comments": "2 two-column pages, 3 figures", "journal-ref": null, "doi": "10.1049/el.2016.2974", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biomedical applications, the similarity between a signal measured from an\ninjured subject and a reference signal measured from a normal subject can be\nused to quantify the injury severity. This paper proposes a generalization of\nthe adaptive signed correlation index (ASCI) to account for specific signal\nfeatures of interest and extend the trichotomization of conventional ASCI to an\narbitrary number of levels. In the context of spinal cord injury assessment, a\ncomputational example is presented to illustrate the enhanced resolution of the\nproposed measure and its ability to offer a more refined measure of the level\nof injury.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 01:25:00 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Olenko", "A.", ""], ["Wong", "K. T.", ""], ["Mir", "H.", ""], ["Al-Nashash", "H.", ""]]}, {"id": "1610.06683", "submitter": "Ashton Verdery", "authors": "Ashton M. Verdery, Jacob C. Fisher, Nalyn Siripong, Kahina Abdesselam,\n  Shawn Bauldry", "title": "New Survey Questions and Estimators for Network Clustering with\n  Respondent-Driven Sampling Data", "comments": "47 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) is a popular method for sampling\nhard-to-survey populations that leverages social network connections through\npeer recruitment. While RDS is most frequently applied to estimate the\nprevalence of infections and risk behaviors of interest to public health, like\nHIV/AIDS or condom use, it is rarely used to draw inferences about the\nstructural properties of social networks among such populations because it does\nnot typically collect the necessary data. Drawing on recent advances in\ncomputer science, we introduce a set of data collection instruments and RDS\nestimators for network clustering, an important topological property that has\nbeen linked to a network's potential for diffusion of information, disease, and\nhealth behaviors. We use simulations to explore how these estimators,\noriginally developed for random walk samples of computer networks, perform when\napplied to RDS samples with characteristics encountered in realistic field\nsettings that depart from random walks. In particular, we explore the effects\nof multiple seeds, without vs. with replacement, branching chains, imperfect\nresponse rates, preferential recruitment, and misreporting of ties. We find\nthat clustering coefficient estimators retain desirable properties in RDS\nsamples. This paper takes an important step towards calculating network\ncharacteristics using non-traditional sampling methods, and it expands RDS's\npotential to tell researchers more about hidden populations and the social\nfactors driving disease prevalence.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 07:06:17 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Verdery", "Ashton M.", ""], ["Fisher", "Jacob C.", ""], ["Siripong", "Nalyn", ""], ["Abdesselam", "Kahina", ""], ["Bauldry", "Shawn", ""]]}, {"id": "1610.06833", "submitter": "Alfred Galichon", "authors": "Guillaume Carlier, Victor Chernozhukov, Alfred Galichon", "title": "Vector quantile regression beyond correct specification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies vector quantile regression (VQR), which is a way to model\nthe dependence of a random vector of interest with respect to a vector of\nexplanatory variables so to capture the whole conditional distribution, and not\nonly the conditional mean. The problem of vector quantile regression is\nformulated as an optimal transport problem subject to an additional\nmean-independence condition. This paper provides a new set of results on VQR\nbeyond the case with correct specification which had been the focus of previous\nwork. First, we show that even under misspecification, the VQR problem still\nhas a solution which provides a general representation of the conditional\ndependence between random vectors. Second, we provide a detailed comparison\nwith the classical approach of Koenker and Bassett in the case when the\ndependent variable is univariate and we show that in that case, VQR is\nequivalent to classical quantile regression with an additional monotonicity\nconstraint.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 15:55:24 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Carlier", "Guillaume", ""], ["Chernozhukov", "Victor", ""], ["Galichon", "Alfred", ""]]}, {"id": "1610.06835", "submitter": "Nickos Papadatos D", "authors": "Nickos Papadatos", "title": "On sequences of expected maxima and expected ranges", "comments": "23 pages", "journal-ref": "Journal of Applied Probability 54 (2017), 1144-1166", "doi": "10.1017/jpr.2017.57", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate conditions in order to decide whether a given sequence of real\nnumbers represents expected maxima or expected ranges. The main result provides\na novel necessary and sufficient condition, relating an expected maxima\nsequence to a translation of a Bernstein function through its\nL\\'{e}vy-Khintchine representation.\n  Key words and phrases: expected maxima; expected ranges; Bernstein functions,\nL\\'{e}vy-Khintchine representation, order statistics.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 16:07:44 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 01:18:26 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 00:25:51 GMT"}, {"version": "v4", "created": "Thu, 18 May 2017 15:48:36 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Papadatos", "Nickos", ""]]}, {"id": "1610.06860", "submitter": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez", "authors": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez and Mar\\'ia Durb\\'an and Dae-Jin\n  Lee and Paul H. C. Eilers", "title": "Spatio-temporal adaptive penalized splines with application to\n  Neuroscience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysed here derive from experiments conducted to study neurons'\nactivity in the visual cortex of behaving monkeys. We consider a\nspatio-temporal adaptive penalized spline (P-spline) approach for modelling the\nfiring rate of visual neurons. To the best of our knowledge, this is the first\nattempt in the statistical literature for locally adaptive smoothing in three\ndimensions. Estimation is based on the Separation of Overlapping Penalties\n(SOP) algorithm, which provides the stability and speed we look for.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 17:06:47 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 11:41:15 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Rodr\u00edguez-\u00c1lvarez", "Mar\u00eda Xos\u00e9", ""], ["Durb\u00e1n", "Mar\u00eda", ""], ["Lee", "Dae-Jin", ""], ["Eilers", "Paul H. C.", ""]]}, {"id": "1610.06861", "submitter": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez", "authors": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez and Mar\\'ia Durb\\'an and Dae-Jin\n  Lee and Paul H. C. Eilers", "title": "Fast estimation of multidimensional adaptive P-spline models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fast and stable algorithm for estimating multidimensional adaptive P-spline\nmodels is presented. We call it as Separation of Overlapping Penalties (SOP) as\nit is an extension of the \\textit{Separation of Anisotropic Penalties} (SAP)\nalgorithm. SAP was originally derived for the estimation of the smoothing\nparameters of a multidimensional tensor product P-spline model with anisotropic\npenalties.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 17:08:04 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Rodr\u00edguez-\u00c1lvarez", "Mar\u00eda Xos\u00e9", ""], ["Durb\u00e1n", "Mar\u00eda", ""], ["Lee", "Dae-Jin", ""], ["Eilers", "Paul H. C.", ""]]}, {"id": "1610.06943", "submitter": "Trang Nguyen", "authors": "Trang Quynh Nguyen, Cyrus Ebnesajjad, Stephen R. Cole, Elizabeth A.\n  Stuart", "title": "Sensitivity analysis for an unobserved moderator in\n  RCT-to-target-population generalization of treatment effects", "comments": "17 pages, 2 figures, 3 tables", "journal-ref": "Ann. Appl. Stat. 2017;11:225-47", "doi": "10.1214/16-AOAS1001", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of treatment effect heterogeneity, the average treatment\neffect (ATE) in a randomized controlled trial (RCT) may differ from the average\neffect of the same treatment if applied to a target population of interest. If\nall treatment effect moderators are observed in the RCT and in a dataset\nrepresenting the target population, we can obtain an estimate for the target\npopulation ATE by adjusting for the difference in the distribution of the\nmoderators between the two samples. This paper considers sensitivity analyses\nfor two situations: (1) where we cannot adjust for a specific moderator $V$\nobserved in the RCT because we do not observe it in the target population; and\n(2) where we are concerned that the treatment effect may be moderated by\nfactors not observed even in the RCT, which we represent as a composite\nmoderator $U$. In both situations, the outcome is not observed in the target\npopulation. For situation (1), we offer three sensitivity analysis methods\nbased on (i) an outcome model, (ii) full weighting adjustment, and (iii)\npartial weighting combined with an outcome model. For situation (2), we offer\ntwo sensitivity analyses based on (iv) a bias formula and (v) partial weighting\ncombined with a bias formula. We apply methods (i) and (iii) to an example\nwhere the interest is to generalize from a smoking cessation RCT conducted with\nparticipants of alcohol/illicit drug use treatment programs to the target\npopulation of people who seek treatment for alcohol/illicit drug use in the US\nwho are also cigarette smokers. In this case a treatment effect moderator is\nobserved in the RCT but not in the target population dataset.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 20:30:39 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Nguyen", "Trang Quynh", ""], ["Ebnesajjad", "Cyrus", ""], ["Cole", "Stephen R.", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "1610.07123", "submitter": "Deepesh Bhati Mr.", "authors": "Subrata Chakraborty and Deepesh Bhati", "title": "Analysis of Count Data by Transmuted Geometric Distribution", "comments": "21 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transmuted geometric distribution (TGD) was recently introduced and\ninvestigated by Chakraborty and Bhati (2016). This is a flexible extension of\ngeometric distribution having an additional parameter that determines its zero\ninflation as well as the tail length. In the present article we further study\nthis distribution for some of its reliability, stochastic ordering and\nparameter estimation properties. In parameter estimation among others we\ndiscuss an EM algorithm and the performance of estimators is evaluated through\nextensive simulation. For assessing the statistical significance of additional\nparameter, Likelihood ratio test, the Rao's score tests and the Wald's test are\ndeveloped and its empirical power via simulation were compared. We have\ndemonstrate two applications of (TGD) in modeling real life count data.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 05:18:01 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Chakraborty", "Subrata", ""], ["Bhati", "Deepesh", ""]]}, {"id": "1610.07213", "submitter": "Chao Du", "authors": "Chao Du, Wing Hong Wong", "title": "Stochastic Modeling and Statistical Inference of Intrinsic Noise in Gene\n  Regulation System via Chemical Master Equation", "comments": "64 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.MN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic noise, the stochastic cell-to-cell fluctuations in mRNAs and\nproteins, has been observed and proved to play important roles in cellular\nsystems. Due to the recent development in single-cell-level measurement\ntechnology, the studies on intrinsic noise are becoming increasingly popular\namong scholars. The chemical master equation (CME) has been used to model the\nevolutions of complex chemical and biological systems since 1940, and are often\nserved as the standard tool for modeling intrinsic noise in gene regulation\nsystem. A CME-based model can capture the discrete, stochastic, and dynamical\nnature of gene regulation system, and may offer casual and physical explanation\nof the observed data at single-cell level. Nonetheless, the complexity of CME\nalso pose serious challenge for researchers in proposing practical modeling and\ninference frameworks. In this article, we will review the existing works on the\nmodelings and inference of intrinsic noise in gene regulation system within the\nframework of CME model. We will explore the principles in constructing a CME\nmodel for studying gene regulation system and discuss the popular\napproximations of CME. Then we will study the simulation simulation methods as\nwell as the analytical and numerical approaches that can be used to obtain\nsolution to a CME model. Finally we will summary the exiting statistical\nmethods that can be used to infer the unknown parameters or structures in CME\nmodel using single-cell-level gene expression data.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 18:39:42 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 21:26:51 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Du", "Chao", ""], ["Wong", "Wing Hong", ""]]}, {"id": "1610.07217", "submitter": "Gero Walter", "authors": "Gero Walter and Frank P.A. Coolen", "title": "Sets of Priors Reflecting Prior-Data Conflict and Agreement", "comments": "12 pages, 6 figures, In: Paulo Joao Carvalho et al. (eds.), IPMU\n  2016: Proceedings of the 16th International Conference on Information\n  Processing and Management of Uncertainty in Knowledge-Based Systems,\n  Eindhoven, The Netherlands", "journal-ref": null, "doi": "10.1007/978-3-319-40596-4_14", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian statistics, the choice of prior distribution is often debatable,\nespecially if prior knowledge is limited or data are scarce. In imprecise\nprobability, sets of priors are used to accurately model and reflect prior\nknowledge. This has the advantage that prior-data conflict sensitivity can be\nmodelled: Ranges of posterior inferences should be larger when prior and data\nare in conflict. We propose a new method for generating prior sets which, in\naddition to prior-data conflict sensitivity, allows to reflect strong\nprior-data agreement by decreased posterior imprecision.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 18:52:36 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Walter", "Gero", ""], ["Coolen", "Frank P. A.", ""]]}, {"id": "1610.07222", "submitter": "Gero Walter", "authors": "Gero Walter and Frank P.A. Coolen", "title": "Robust Bayesian Reliability for Complex Systems under Prior-Data\n  Conflict", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reliability engineering, data about failure events is often scarce. To\narrive at meaningful estimates for the reliability of a system, it is therefore\noften necessary to also include expert information in the analysis, which is\nstraightforward in the Bayesian approach by using an informative prior\ndistribution. A problem called prior-data conflict then can arise: observed\ndata seem very surprising from the viewpoint of the prior, i.e., information\nfrom data is in conflict with prior assumptions. Models based on conjugate\npriors can be insensitive to prior-data conflict, in the sense that the spread\nof the posterior distribution does not increase in case of such a conflict,\nthus conveying a false sense of certainty. An approach to mitigate this issue\nis presented, by considering sets of prior distributions to model limited\nknowledge on Weibull distributed component lifetimes, treating systems with\narbitrary layout using the survival signature. This approach can be seen as a\nrobust Bayesian procedure or imprecise probability method that reflects\nsurprisingly early or late component failures by wider system reliability\nbounds.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 19:24:59 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Walter", "Gero", ""], ["Coolen", "Frank P. A.", ""]]}, {"id": "1610.07271", "submitter": "Xu Gao", "authors": "Xu Gao, Weining Shen, Babak Shahbaba, Norbert Fortin, Hernando Ombao", "title": "Evolutionary State-Space Model and Its Application to Time-Frequency\n  Analysis of Local Field Potentials", "comments": "Statistica Sinica (2018+)", "journal-ref": null, "doi": "10.5705/ss.202017.0420", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an evolutionary state space model (E-SSM) for analyzing high\ndimensional brain signals whose statistical properties evolve over the course\nof a non-spatial memory experiment. Under E-SSM, brain signals are modeled as\nmixtures of components (e.g., AR(2) process) with oscillatory activity at\npre-defined frequency bands. To account for the potential non-stationarity of\nthese components (since the brain responses could vary throughout the entire\nexperiment), the parameters are allowed to vary over epochs. Compared with\nclassical approaches such as independent component analysis and filtering, the\nproposed method accounts for the entire temporal correlation of the components\nand accommodates non-stationarity. For inference purpose, we propose a novel\ncomputational algorithm based upon using Kalman smoother, maximum likelihood\nand blocked resampling. The E-SSM model is applied to simulation studies and an\napplication to a multi-epoch local field potentials (LFP) signal data collected\nfrom a non-spatial (olfactory) sequence memory task study. The results confirm\nthat our method captures the evolution of the power for different components\nacross different phases in the experiment and identifies clusters of electrodes\nthat behave similarly with respect to the decomposition of different sources.\nThese findings suggest that the activity of different electrodes does change\nover the course of an experiment in practice, treating these epoch recordings\nas realizations of an identical process could lead to misleading results. In\nsummary, the proposed method underscores the importance of capturing the\nevolution in brain responses over the study period.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 03:36:16 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 04:25:32 GMT"}, {"version": "v3", "created": "Sun, 21 Oct 2018 22:39:06 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Gao", "Xu", ""], ["Shen", "Weining", ""], ["Shahbaba", "Babak", ""], ["Fortin", "Norbert", ""], ["Ombao", "Hernando", ""]]}, {"id": "1610.07306", "submitter": "Anand Bhaskar", "authors": "Anand Bhaskar, Adel Javanmard, Thomas A. Courtade, David Tse", "title": "Novel probabilistic models of spatial genetic ancestry with applications\n  to stratification correction in genome-wide association studies", "comments": "Supplementary information included to the main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic variation in human populations is influenced by geographic ancestry\ndue to spatial locality in historical mating and migration patterns. Spatial\npopulation structure in genetic datasets has been traditionally analyzed using\neither model-free algorithms, such as principal components analysis (PCA) and\nmultidimensional scaling, or using explicit spatial probabilistic models of\nallele frequency evolution. We develop a general probabilistic model and an\nassociated inference algorithm that unify the model-based and data-driven\napproaches to visualizing and inferring population structure. Our algorithm,\nGeographic Ancestry Positioning (GAP), relates local genetic distances between\nsamples to their spatial distances, and can be used for visually discerning\npopulation structure as well as accurately inferring the spatial origin of\nindividuals on a two-dimensional continuum. On both simulated and several real\ndatasets from diverse human populations, GAP exhibits substantially lower error\nin reconstructing spatial ancestry coordinates compared to PCA.\n  Our spatial inference algorithm can also be effectively applied to the\nproblem of population stratification in genome-wide association studies (GWAS),\nwhere hidden population structure can create fictitious associations when\npopulation ancestry is correlated with both the genotype and the trait. We\ndevelop an association test that uses the ancestry coordinates inferred by GAP\nto accurately account for ancestry-induced correlations in GWAS. Based on\nsimulations and analysis of a dataset of 10 metabolic traits measured in a\nNorthern Finland cohort, which is known to exhibit significant population\nstructure, we find that our method has superior power to current approaches.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 07:22:43 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 04:55:27 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Bhaskar", "Anand", ""], ["Javanmard", "Adel", ""], ["Courtade", "Thomas A.", ""], ["Tse", "David", ""]]}, {"id": "1610.07453", "submitter": "Yao Zheng", "authors": "Yao Zheng, Qianqian Zhu, Guodong Li and Zhijie Xiao", "title": "Hybrid Quantile Regression Estimation for Time Series Models with\n  Conditional Heteroscedasticity", "comments": "53 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating conditional quantiles of financial time series is essential for\nrisk management and many other applications in finance. It is well-known that\nfinancial time series display conditional heteroscedasticity. Among the large\nnumber of conditional heteroscedastic models, the generalized autoregressive\nconditional heteroscedastic (GARCH) process is the most popular and influential\none. So far, feasible quantile regression methods for this task have been\nconfined to a variant of the GARCH model, the linear GARCH model, owing to its\ntractable conditional quantile structure. This paper considers the widely used\nGARCH model. An easy-to-implement hybrid conditional quantile estimation\nprocedure is developed based on a simple albeit nontrivial transformation.\nAsymptotic properties of the proposed estimator and statistics are derived,\nwhich facilitate corresponding inferences. To approximate the asymptotic\ndistribution of the quantile regression estimator, we introduce a mixed\nbootstrapping procedure, where a time-consuming optimization is replaced by a\nsample averaging. Moreover, diagnostic tools based on the residual quantile\nautocorrelation function are constructed to check the adequacy of the fitted\nconditional quantiles. Simulation experiments are carried out to assess the\nfinite-sample performance of the proposed approach. The favorable performance\nof the conditional quantile estimator and the usefulness of the inference tools\nare further illustrated by an empirical application.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 15:10:38 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Zheng", "Yao", ""], ["Zhu", "Qianqian", ""], ["Li", "Guodong", ""], ["Xiao", "Zhijie", ""]]}, {"id": "1610.07540", "submitter": "Subhabrata Majumdar", "authors": "Subhabrata Majumdar, Snigdhansu Chatterjee", "title": "Nonconvex penalized multitask regression using data depth-based\n  penalties", "comments": null, "journal-ref": "Stat 7 (2018) e174", "doi": "10.1002/sta4.174", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new class of nonconvex penalty functions, based on data depth\nfunctions, for multitask sparse penalized regression. These penalties quantify\nthe relative position of rows of the coefficient matrix from a fixed\ndistribution centered at the origin. We derive the theoretical properties of an\napproximate one-step sparse estimator of the coefficient matrix using local\nlinear approximation of the penalty function, and provide algorithm for its\ncomputation. For orthogonal design and independent responses, the resulting\nthresholding rule enjoys near-minimax optimal risk performance, similar to the\nadaptive lasso (Zou, 2006). A simulation study and real data analysis\ndemonstrate its effectiveness compared to some of the present methods that\nprovide sparse solutions in multivariate regression.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 18:47:59 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 18:19:27 GMT"}, {"version": "v3", "created": "Fri, 23 Jun 2017 02:21:28 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Majumdar", "Subhabrata", ""], ["Chatterjee", "Snigdhansu", ""]]}, {"id": "1610.07550", "submitter": "Vladimir Minin", "authors": "Jason Xu, Samson Koelle, Peter Guttorp, Chuanfeng Wu, Cynthia E.\n  Dunbar, Janis L. Abkowitz, Vladimir N. Minin", "title": "Statistical inference in partially observed branching processes with\n  application to cell lineage tracking of in vivo hematopoiesis", "comments": "36 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-cell lineage tracking strategies enabled by recent experimental\ntechnologies have produced significant insights into cell fate decisions, but\nlack the quantitative framework necessary for rigorous statistical analysis of\nmechanistic models describing cell division and differentiation. In this paper,\nwe develop such a framework with corresponding moment-based parameter\nestimation techniques for continuous-time, multi-type branching processes. Such\nprocesses provide a probabilistic model of how cells divide and differentiate,\nand we apply our method to study hematopoiesis, the mechanism of blood cell\nproduction. We derive closed-form expressions for higher moments in a general\nclass of such models. These analytical results allow us to efficiently estimate\nparameters of much richer statistical models of hematopoiesis than those used\nin previous statistical studies. After validating the methodology in simulation\nstudies, we apply our estimator to hematopoietic lineage tracking data from\nrhesus macaques. Our analysis provides a more complete understanding of cell\nfate decisions during hematopoiesis in non-human primates, which may be more\nrelevant to human biology and clinical strategies than previous findings from\nmurine studies. For example, in addition to previously estimated hematopoietic\nstem cell self-renewal rate, we are able to estimate fate decision\nprobabilities and to compare structurally distinct models of hematopoiesis\nusing cross validation. These estimates of fate decision probabilities and our\nmodel selection results should help biologists compare competing hypotheses\nabout how progenitor cells differentiate. The methodology is transferrable to a\nlarge class of stochastic compartmental models and multi-type branching models,\ncommonly used in studies of cancer progression, epidemiology, and many other\nfields.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 19:05:46 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 16:35:20 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Xu", "Jason", ""], ["Koelle", "Samson", ""], ["Guttorp", "Peter", ""], ["Wu", "Chuanfeng", ""], ["Dunbar", "Cynthia E.", ""], ["Abkowitz", "Janis L.", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1610.07583", "submitter": "Georgia Papadogeorgou", "authors": "Georgia Papadogeorgou, Christine Choirat, Corwin Zigler", "title": "Adjusting for Unmeasured Spatial Confounding with Distance Adjusted\n  Propensity Score Matching", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score matching is a common tool for adjusting for observed\nconfounding in observational studies, but is known to have limitations in the\npresence of unmeasured confounding. In many settings, researchers are\nconfronted with spatially-indexed data where the relative locations of the\nobservational units may serve as a useful proxy for unmeasured confounding that\nvaries according to a spatial pattern. We develop a new method, termed Distance\nAdjusted Propensity Score Matching (DAPSm) that incorporates information on\nunits' spatial proximity into a propensity score matching procedure. We show\nthat DAPSm can adjust for both observed and some forms of unobserved\nconfounding and evaluate its performance relative to several other reasonable\nalternatives for incorporating spatial information into propensity score\nadjustment. The method is motivated by and applied to a comparative\neffectiveness investigation of power plant emission reduction technologies\ndesigned to reduce population exposure to ambient ozone pollution. Ultimately,\nDAPSm provides a framework for augmenting a \"standard\" propensity score\nanalysis with information on spatial proximity and provides a transparent and\nprincipled way to assess the relative trade offs of prioritizing observed\nconfounding adjustment versus spatial proximity adjustment.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 19:53:26 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 19:01:07 GMT"}, {"version": "v3", "created": "Wed, 6 Dec 2017 19:31:14 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Papadogeorgou", "Georgia", ""], ["Choirat", "Christine", ""], ["Zigler", "Corwin", ""]]}, {"id": "1610.07683", "submitter": "Ming Yuan", "authors": "Shulei Wang and Ming Yuan", "title": "Combined Hypothesis Testing on Graphs with Applications to Gene Set\n  Enrichment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by gene set enrichment analysis, we investigate the problem of\ncombined hypothesis testing on a graph. We introduce a general framework to\neffectively use the structural information of the underlying graph when testing\nmultivariate means. A new testing procedure is proposed within this framework.\nWe show that the test is optimal in that it can consistently detect departure\nfrom the collective null at a rate that no other test could improve, for almost\nall graphs. We also provide general performance bounds for the proposed test\nunder any specific graph, and illustrate their utility through several common\ntypes of graphs. Numerical experiments are presented to further demonstrate the\nmerits of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 23:51:07 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Wang", "Shulei", ""], ["Yuan", "Ming", ""]]}, {"id": "1610.07697", "submitter": "Quefeng Li", "authors": "Quefeng Li, Guang Cheng, Jianqing Fan and Yuyan Wang", "title": "Embracing the Blessing of Dimensionality in Factor Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor modeling is an essential tool for exploring intrinsic dependence\nstructures among high-dimensional random variables. Much progress has been made\nfor estimating the covariance matrix from a high-dimensional factor model.\nHowever, the blessing of dimensionality has not yet been fully embraced in the\nliterature: much of the available data is often ignored in constructing\ncovariance matrix estimates. If our goal is to accurately estimate a covariance\nmatrix of a set of targeted variables, shall we employ additional data, which\nare beyond the variables of interest, in the estimation? In this paper, we\nprovide sufficient conditions for an affirmative answer, and further quantify\nits gain in terms of Fisher information and convergence rate. In fact, even an\noracle-like result (as if all the factors were known) can be achieved when a\nsufficiently large number of variables is used. The idea of utilizing data as\nmuch as possible brings computational challenges. A divide-and-conquer\nalgorithm is thus proposed to alleviate the computational burden, and also\nshown not to sacrifice any statistical accuracy in comparison with a pooled\nanalysis. Simulation studies further confirm our advocacy for the use of full\ndata, and demonstrate the effectiveness of the above algorithm. Our proposal is\napplied to a microarray data example that shows empirical benefits of using\nmore data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 01:07:43 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Li", "Quefeng", ""], ["Cheng", "Guang", ""], ["Fan", "Jianqing", ""], ["Wang", "Yuyan", ""]]}, {"id": "1610.07714", "submitter": "Ivan Fernandez-Val", "authors": "Mario Cruz-Gonzalez, Ivan Fernandez-Val, and Martin Weidner", "title": "probitfe and logitfe: Bias corrections for probit and logit models with\n  two-way fixed effects", "comments": "29 pages, 3 tables, ado and help files for the commands available at\n  http://econpapers.repec.org/software/bocbocode/s458279.htm and\n  http://econpapers.repec.org/software/bocbocode/s458278.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Stata commands probitfe and logitfe, which estimate probit and\nlogit panel data models with individual and/or time unobserved effects. Fixed\neffect panel data methods that estimate the unobserved effects can be severely\nbiased because of the incidental parameter problem (Neyman and Scott, 1948). We\ntackle this problem by using the analytical and jackknife bias corrections\nderived in Fernandez-Val and Weidner (2016) for panels where the two dimensions\n($N$ and $T$) are moderately large. We illustrate the commands with an\nempirical application to international trade and a Monte Carlo simulation\ncalibrated to this application.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 03:04:35 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 20:51:44 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Cruz-Gonzalez", "Mario", ""], ["Fernandez-Val", "Ivan", ""], ["Weidner", "Martin", ""]]}, {"id": "1610.07925", "submitter": "Hailin Sang", "authors": "Xin Dang, Hailin Sang and Lauren Weatherall", "title": "Gini Covariance Matrix and its Affine Equivariant Version", "comments": "28 pages, 1 figure, 2 tables. Accepted by Statistical Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new covariance matrix called Gini covariance matrix (GCM), which\nis a natural generalization of univariate Gini mean difference (GMD) to the\nmultivariate case. The extension is based on the covariance representation of\nGMD by applying the multivariate spatial rank function. We study properties of\nGCM, especially in the elliptical distribution family. In order to gain the\naffine equivariance property for GCM, we utilize the\ntransformation-retransformation (TR) technique and obtain an affine equivariant\nversion GCM that turns out to be a symmetrized M-functional. The influence\nfunction of those two GCM's are obtained and their estimation has been\npresented. Asymptotic results of estimators have been established. A closely\nrelated scatter Kotz functional and its estimator are also explored. Finally,\nasymptotical efficiency and finite sample efficiency of the TR version GCM are\ncompared with those of sample covariance matrix, Tyler-M estimator and other\nscatter estimators under different distributions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 15:39:26 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Dang", "Xin", ""], ["Sang", "Hailin", ""], ["Weatherall", "Lauren", ""]]}, {"id": "1610.07949", "submitter": "Suman Majumder", "authors": "Suman Majumder, Adhidev Biswas, Tania Roy, Subir Kumar Bhandari,\n  Ayanendranath Basu", "title": "Statistical Inference Based on a New Weighted Likelihood Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a new weighted likelihood method for parametric estimation. The\nmethod is motivated by the need for generating a simple estimation strategy\nwhich provides a robust solution that is simultaneously fully efficient when\nthe model is correctly specified. This is achieved by appropriately weighting\nthe score function at each observation in the maximum likelihood score\nequation. The weight function determines the compatibility of each observation\nwith the model in relation to the remaining observations and applies a\ndownweighting only if it is necessary, rather than automatically downweighting\na proportion of the observations all the time. This allows the estimators to\nretain full asymptotic efficiency at the model. We establish all the\ntheoretical properties of the proposed estimators and substantiate the theory\ndeveloped through simulation and real data examples. Our approach provides an\nalternative to the weighted likelihood method of Markatou et al. (1997, 1998).\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 16:18:46 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 14:35:15 GMT"}, {"version": "v3", "created": "Sun, 25 Aug 2019 05:57:36 GMT"}, {"version": "v4", "created": "Sun, 25 Aug 2019 16:00:39 GMT"}, {"version": "v5", "created": "Tue, 27 Aug 2019 19:22:34 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Majumder", "Suman", ""], ["Biswas", "Adhidev", ""], ["Roy", "Tania", ""], ["Bhandari", "Subir Kumar", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1610.08013", "submitter": "Tingyang Xu", "authors": "Tingyang Xu, Jiangwen Sun, and Jinbo Bi", "title": "Longitudinal LASSO: Jointly Learning Features and Temporal Contingency\n  for Outcome Prediction", "comments": "Proceedings of the 21th ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining. ACM, 2015", "journal-ref": null, "doi": "10.1145/2783258.2783403", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal analysis is important in many disciplines, such as the study of\nbehavioral transitions in social science. Only very recently, feature selection\nhas drawn adequate attention in the context of longitudinal modeling. Standard\ntechniques, such as generalized estimating equations, have been modified to\nselect features by imposing sparsity-inducing regularizers. However, they do\nnot explicitly model how a dependent variable relies on features measured at\nproximal time points. Recent graphical Granger modeling can select features in\nlagged time points but ignores the temporal correlations within an individual's\nrepeated measurements. We propose an approach to automatically and\nsimultaneously determine both the relevant features and the relevant temporal\npoints that impact the current outcome of the dependent variable. Meanwhile,\nthe proposed model takes into account the non-{\\em i.i.d} nature of the data by\nestimating the within-individual correlations. This approach decomposes model\nparameters into a summation of two components and imposes separate block-wise\nLASSO penalties to each component when building a linear model in terms of the\npast $\\tau$ measurements of features. One component is used to select features\nwhereas the other is used to select temporal contingent points. An accelerated\ngradient descent algorithm is developed to efficiently solve the related\noptimization problem with detailed convergence analysis and asymptotic\nanalysis. Computational results on both synthetic and real world problems\ndemonstrate the superior performance of the proposed approach over existing\ntechniques.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 20:34:10 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Xu", "Tingyang", ""], ["Sun", "Jiangwen", ""], ["Bi", "Jinbo", ""]]}, {"id": "1610.08088", "submitter": "Art Owen", "authors": "K. Gao and A. B. Owen", "title": "Estimation and Inference for Very Large Linear Mixed Effects Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed models with large imbalanced crossed random effects structures\npose severe computational problems for maximum likelihood estimation and for\nBayesian analysis. The costs can grow as fast as $N^{3/2}$ when there are N\nobservations. Such problems arise in any setting where the underlying factors\nsatisfy a many to many relationship (instead of a nested one) and in electronic\ncommerce applications, the N can be quite large. Methods that do not account\nfor the correlation structure can greatly underestimate uncertainty. We propose\na method of moments approach that takes account of the correlation structure\nand that can be computed at O(N) cost. The method of moments is very amenable\nto parallel computation and it does not require parametric distributional\nassumptions, tuning parameters or convergence diagnostics. For the regression\ncoefficients, we give conditions for consistency and asymptotic normality as\nwell as a consistent variance estimate. For the variance components, we give\nconditions for consistency and we use consistent estimates of a mildly\nconservative variance estimate. All of these computations can be done in O(N)\nwork. We illustrate the algorithm with some data from Stitch Fix where the\ncrossed random effects correspond to clients and items.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 20:38:24 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 23:28:55 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Gao", "K.", ""], ["Owen", "A. B.", ""]]}, {"id": "1610.08203", "submitter": "Robin Genuer", "authors": "Robin Genuer (SISTM), Jean-Michel Poggi (LMO, UPD5)", "title": "Arbres CART et For\\^ets al\\'eatoires, Importance et s\\'election de\n  variables", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two algorithms proposed by Leo Breiman : CART trees (Classification And\nRegression Trees for) introduced in the first half of the 80s and random\nforests emerged, meanwhile, in the early 2000s, are the subject of this\narticle. The goal is to provide each of the topics, a presentation, a\ntheoretical guarantee, an example and some variants and extensions. After a\npreamble, introduction recalls objectives of classification and regression\nproblems before retracing some predecessors of the Random Forests. Then, a\nsection is devoted to CART trees then random forests are presented. Then, a\nvariable selection procedure based on permutation variable importance is\nproposed. Finally the adaptation of random forests to the Big Data context is\nsketched.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 06:56:14 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 14:05:06 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Genuer", "Robin", "", "SISTM"], ["Poggi", "Jean-Michel", "", "LMO, UPD5"]]}, {"id": "1610.08244", "submitter": "Suneel Babu Chatla", "authors": "Suneel Babu Chatla, Galit Shmueli", "title": "Efficient Estimation of COM-Poisson Regression and Generalized Additive\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Conway-Maxwell-Poisson (CMP) or COM-Poison regression is a popular model\nfor count data due to its ability to capture both under dispersion and over\ndispersion. However, CMP regression is limited when dealing with complex\nnonlinear relationships. With today's wide availability of count data,\nespecially due to the growing collection of data on human and social behavior,\nthere is need for count data models that can capture complex nonlinear\nrelationships. One useful approach is additive models; but, there has been no\nadditive model implementation for the CMP distribution. To fill this void, we\nfirst propose a flexible estimation framework for CMP regression based on\niterative reweighed least squares (IRLS) and then extend this model to allow\nfor additive components using a penalized splines approach. Because the CMP\ndistribution belongs to the exponential family, convergence of IRLS is\nguaranteed under some regularity conditions. Further, it is also known that\nIRLS provides smaller standard errors compared to gradient-based methods. We\nillustrate the usefulness of this approach through extensive simulation studies\nand using real data from a bike sharing system in Washington, DC.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 09:21:50 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 06:53:19 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 15:34:16 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Chatla", "Suneel Babu", ""], ["Shmueli", "Galit", ""]]}, {"id": "1610.08278", "submitter": "Koby Todros", "authors": "Koby Todros", "title": "Measure Transformed Quasi Score Test with Application to Location\n  Mismatch Detection", "comments": "arXiv admin note: text overlap with arXiv:1609.07958,\n  arXiv:1511.00237", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a generalization of the Gaussian quasi score test\n(GQST) for composite binary hypothesis testing. The proposed test, called\nmeasure transformed GQST (MT-GQST), is based on the score-function of the\nmeasure transformed Gaussian quasi maximum likelihood estimator (MT-GQMLE) that\noperates by empirically fitting a Gaussian model to a transformed probability\nmeasure of the data. By judicious choice of the transform we show that, unlike\nthe GQST, the proposed MT-GQST involves higher-order statistical moments and\ncan gain resilience to outliers, leading to significant mitigation of the model\nmismatch effect on the decision performance. A data-driven procedure for\noptimal selection of the measure transformation parameters is developed that\nminimizes the spectral norm of the empirical asymptotic error-covariance of the\nMT-GQMLE. This amounts to maximization of an empirical worst-case asymptotic\nlocal power at a fixed asymptotic size. The MT-GQST is applied to location\nmismatch detection of a near-field point source in a simulation example that\nillustrates its robustness to outliers.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 10:54:14 GMT"}, {"version": "v2", "created": "Sun, 13 Nov 2016 11:30:41 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Todros", "Koby", ""]]}, {"id": "1610.08360", "submitter": "Justin Chown", "authors": "Justin Chown and Ursula U. M\\\"uller", "title": "Efficiently estimating the error distribution in nonparametric\n  regression with responses missing at random", "comments": "Preprint is 13 pages in length and includes 1 figure", "journal-ref": "Chown, J. and M\\\"uller, U.U. (2013). Efficiently estimating the\n  error distribution in nonparametric regression with responses missing at\n  random. J. Nonparametr. Stat. 25, 665-677", "doi": "10.1080/10485252.2013.795222", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers nonparametric regression models with multivariate\ncovariates and with responses missing at random. We estimate the regression\nfunction with a local polynomial smoother. The residual-based empirical\ndistribution function that only uses complete cases, i.e. residuals that can\nactually be constructed from the data, is shown to be efficient in the sense of\nH\\'ajek and Le Cam. In the proofs we derive, more generally, the efficient\ninfluence function for estimating an arbitrary linear functional of the error\ndistribution; this covers the distribution function as a special case. We also\nshow that the complete case residual-based empirical distribution function\nadmits a functional central limit theorem. The article concludes with a small\nsimulation study investigating the performance of the complete case\nresidual-based empirical distribution function.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 14:49:30 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Chown", "Justin", ""], ["M\u00fcller", "Ursula U.", ""]]}, {"id": "1610.08363", "submitter": "Jon Cockayne", "authors": "Jon Cockayne", "title": "Comments on \"Bayesian Solution Uncertainty Quantification for\n  Differential Equations\" by Chkrebtii, Campbell, Calderhead & Girolami", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I would like to thank the authors for their interesting and very clearly\npresented paper discussing probabilistic solvers for ODEs and PDEs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 11:48:39 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Cockayne", "Jon", ""]]}, {"id": "1610.08367", "submitter": "Sourabh Bhattacharya", "authors": "Satyaki Mazumder and Sourabh Bhattacharya", "title": "Nonparametric Dynamic State Space Modeling of Observed Circular Time\n  Series with Circular Latent States: A Bayesian Perspective", "comments": "This significantly updated version will appear in Journal of\n  Statistical Theory and Practice", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Circular time series has received relatively little attention in statistics\nand modeling complex circular time series using the state space approach is\nnon-existent in the literature. In this article we introduce a flexible\nBayesian nonparametric approach to state space modeling of observed circular\ntime series where even the latent states are circular random variables.\nCrucially, we assume that the forms of both observational and evolutionary\nfunctions, both of which are circular in nature, are unknown and time-varying.\nWe model these unknown circular functions by appropriate wrapped Gaussian\nprocesses having desirable properties.\n  We develop an effective Markov chain Monte Carlo strategy for implementing\nour Bayesian model, by judiciously combining Gibbs sampling and\nMetropolis-Hastings methods. Validation of our ideas with a simulation study\nand two real bivariate circular time series data sets, where we assume one of\nthe variables to be unobserved, revealed very encouraging performance of our\nmodel and methods.\n  We finally analyse a data consisting of directions of whale migration,\nconsidering the unobserved ocean current direction as the latent circular\nprocess of interest. The results that we obtain are encouraging, and the\nposterior predictive distribution of the observed process correctly predicts\nthe observed whale movement.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 15:06:36 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 06:38:56 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Mazumder", "Satyaki", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1610.08580", "submitter": "Kirk Bansak", "authors": "Kirk Bansak", "title": "A Generalized Approach to Power Analysis for Local Average Treatment\n  Effects", "comments": "Forthcoming, Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study introduces a new approach to power analysis in the context of\nestimating a local average treatment effect (LATE), where the study subjects\nexhibit noncompliance with treatment assignment. As a result of distributional\ncomplications in the LATE context, compared to the simple ATE context, there is\ncurrently no standard method of power analysis for the LATE. Moreover, existing\nmethods and commonly used substitutes - which include instrumental variable\n(IV), intent-to-treat (ITT), and scaled ATE power analyses - require specifying\ngenerally unknown variance terms and/or rely upon strong and unrealistic\nassumptions, thus providing unreliable guidance on the power of tests of the\nLATE. This study develops a new approach that uses standardized effect sizes to\nplace bounds on the power for the most commonly used estimator of the LATE, the\nWald IV estimator, whereby variance terms and distributional parameters need\nnot be specified nor assumed. Instead, in addition to the effect size, sample\nsize, and error tolerance parameters, the only other parameter that must be\nspecified by the researcher is the compliance rate. Additional conditions can\nalso be introduced to further narrow the bounds on the power calculation. The\nresult is a generalized approach to power analysis in the LATE context that is\nsimple to implement.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 00:11:22 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 19:24:48 GMT"}, {"version": "v3", "created": "Mon, 18 Sep 2017 18:42:32 GMT"}, {"version": "v4", "created": "Wed, 8 Apr 2020 18:43:40 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Bansak", "Kirk", ""]]}, {"id": "1610.08621", "submitter": "Qing Zhou", "authors": "Qing Zhou and Seunghyun Min", "title": "Estimator Augmentation with Applications in High-Dimensional Group\n  Inference", "comments": "35 pages, 4 figures", "journal-ref": "Electronic Journal of Statistics, Vol. 11 (2017): 3039-3080", "doi": "10.1214/17-EJS1309", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make inference about a group of parameters on high-dimensional data, we\ndevelop the method of estimator augmentation for the block Lasso, which is\ndefined via the block norm. By augmenting a block Lasso estimator $\\hat{\\beta}$\nwith the subgradient $S$ of the block norm evaluated at $\\hat{\\beta}$, we\nderive a closed-form density for the joint distribution of $(\\hat{\\beta},S)$\nunder a high-dimensional setting. This allows us to draw from an estimated\nsampling distribution of $\\hat{\\beta}$, or more generally any function of\n$(\\hat{\\beta},S)$, by Monte Carlo algorithms. We demonstrate the application of\nestimator augmentation in group inference with the group Lasso and a de-biased\ngroup Lasso constructed as a function of $(\\hat{\\beta},S)$. Our numerical\nresults show that importance sampling via estimator augmentation can be orders\nof magnitude more efficient than parametric bootstrap in estimating tail\nprobabilities for significance tests. This work also brings new insights into\nthe geometry of the sample space and the solution uniqueness of the block\nLasso.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 05:31:10 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 20:34:58 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Zhou", "Qing", ""], ["Min", "Seunghyun", ""]]}, {"id": "1610.08663", "submitter": "Justin Chown", "authors": "Nicolai Bissantz, Justin Chown and Holger Dette", "title": "Regularization parameter selection in indirect regression by residual\n  based bootstrap", "comments": "Keywords: bandwidth selection, indirect regression estimator, inverse\n  problems, regularization, residual-based empirical distribution function,\n  smooth bootstrap", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual-based analysis is generally considered a cornerstone of statistical\nmethodology. For a special case of indirect regression, we investigate the\nresidual-based empirical distribution function and provide a uniform expansion\nof this estimator, which is also shown to be asymptotically most precise. This\ninvestigation naturally leads to a completely data-driven technique for\nselecting a regularization parameter used in our indirect regression function\nestimator. The resulting methodology is based on a smooth bootstrap of the\nmodel residuals. A simulation study demonstrates the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 08:50:01 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 12:12:19 GMT"}, {"version": "v3", "created": "Wed, 4 Oct 2017 12:35:33 GMT"}, {"version": "v4", "created": "Wed, 28 Feb 2018 16:07:46 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Bissantz", "Nicolai", ""], ["Chown", "Justin", ""], ["Dette", "Holger", ""]]}, {"id": "1610.08718", "submitter": "Manuel Oviedo de la Fuente", "authors": "Manuel Oviedo de la Fuente, Manuel Febrero Bande, Mar\\'ia Pilar\n  Mu\\~noz and \\`Angela Dom\\'inguez", "title": "Predicting seasonal influenza transmission using Regression Models with\n  Temporal Dependence", "comments": "25 pages, 2 figures, 10 tables, R code in fda.usc package", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we use meteorological information in Galicia (Spain) to\npropose a novel approach to predict the incidence of influenza. Our approach\nextends the GLS methods in the multivariate framework to functional regression\nmodels with dependent errors. A simulation study shows that the GLS estimators\nrender better estimations of the parameters associated with the regression\nmodel and obtain extremely good results from the predictive point of view. Thus\nthey improve the classical linear approach. It proposes an iterative version of\nthe GLS estimator (called iGLS) that can help to model complicated dependence\nstructures, uses the distance correlation measure $\\mathcal{R}$ to select\nrelevant information to predict influenza rate and applies the GLS procedure to\nthe prediction of the influenza rate using readily available functional\nvariables. These kinds of models are extremely useful to health managers in\nallocating resources in advance for an epidemic outbreak.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 11:41:29 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["de la Fuente", "Manuel Oviedo", ""], ["Bande", "Manuel Febrero", ""], ["Mu\u00f1oz", "Mar\u00eda Pilar", ""], ["Dom\u00ednguez", "\u00c0ngela", ""]]}, {"id": "1610.08749", "submitter": "Antti Honkela", "authors": "Joonas J\\\"alk\\\"o and Onur Dikmen and Antti Honkela", "title": "Differentially Private Variational Inference for Non-conjugate Models", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning applications are based on data collected from people,\nsuch as their tastes and behaviour as well as biological traits and genetic\ndata. Regardless of how important the application might be, one has to make\nsure individuals' identities or the privacy of the data are not compromised in\nthe analysis. Differential privacy constitutes a powerful framework that\nprevents breaching of data subject privacy from the output of a computation.\nDifferentially private versions of many important Bayesian inference methods\nhave been proposed, but there is a lack of an efficient unified approach\napplicable to arbitrary models. In this contribution, we propose a\ndifferentially private variational inference method with a very wide\napplicability. It is built on top of doubly stochastic variational inference, a\nrecent advance which provides a variational solution to a large class of\nmodels. We add differential privacy into doubly stochastic variational\ninference by clipping and perturbing the gradients. The algorithm is made more\nefficient through privacy amplification from subsampling. We demonstrate the\nmethod can reach an accuracy close to non-private level under reasonably strong\nprivacy guarantees, clearly improving over previous sampling-based alternatives\nespecially in the strong privacy regime.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 12:34:36 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 12:59:55 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["J\u00e4lk\u00f6", "Joonas", ""], ["Dikmen", "Onur", ""], ["Honkela", "Antti", ""]]}, {"id": "1610.08768", "submitter": "Justin Chown", "authors": "Justin Chown", "title": "Efficient estimation of the error distribution function in\n  heteroskedastic nonparametric regression with missing data", "comments": "Preprint is 20 pages in length", "journal-ref": "Statist. Probab. Lett. 117, 31-39 (2016)", "doi": "10.1016/j.spl.2016.04.009", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A residual-based empirical distribution function is proposed to estimate the\ndistribution function of the errors of a heteroskedastic nonparametric\nregression with responses missing at random based on completely observed data,\nand this estimator is shown to be asymptotically most precise.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 13:28:25 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Chown", "Justin", ""]]}, {"id": "1610.08779", "submitter": "Toby Kenney", "authors": "Toby Kenney and Hao He and Hong Gu", "title": "Prior Distributions for Ranking Problems", "comments": "42 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ranking problem is to order a collection of units by some unobserved\nparameter, based on observations from the associated distribution. This problem\narises naturally in a number of contexts, such as business, where we may want\nto rank potential projects by profitability; or science, where we may want to\nrank variables potentially associated with some trait by the strength of the\nassociation. Most approaches to this problem are empirical Bayesian, where we\nuse the data to estimate the hyperparameters of the prior distribution, then\nuse that distribution to estimate the unobserved parameter values. There are a\nnumber of different approaches to this problem, based on different loss\nfunctions for mis-ranking units. However, little has been done on the choice of\nprior distribution. Typical approaches involve choosing a conjugate prior for\nconvenience, and estimating the hyperparameters by MLE from the whole dataset.\nIn this paper, we look in more detail at the effect of choice of prior\ndistribution on Bayesian ranking. We focus on the use of posterior mean for\nranking, but many of our conclusions should apply to other ranking criteria,\nand it is not too difficult to adapt our methods to other choices of prior\ndistributions.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 13:52:28 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Kenney", "Toby", ""], ["He", "Hao", ""], ["Gu", "Hong", ""]]}, {"id": "1610.08795", "submitter": "Matthias Killiches", "authors": "Matthias Killiches, Daniel Kraus, Claudia Czado", "title": "Using model distances to investigate the simplifying assumption, model\n  selection and truncation levels for vine copulas", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vine copulas are a useful statistical tool to describe the dependence\nstructure between several random variables, especially when the number of\nvariables is very large. When modeling data with vine copulas, one often is\nconfronted with a set of candidate models out of which the best one is supposed\nto be selected. For example, this may arise in the context of non-simplified\nvine copulas, truncations of vines and other simplifications regarding\npair-copula families or the vine structure. With the help of distance measures\nwe develop a parametric bootstrap based testing procedure to decide between\ncopulas from nested model classes. In addition we use distance measures to\nselect among different candidate models. All commonly used distance measures,\ne.g. the Kullback-Leibler distance, suffer from the curse of dimensionality due\nto high-dimensional integrals. As a remedy for this problem, Killiches, Kraus\nand Czado (2017) propose several modifications of the Kullback-Leibler\ndistance. We apply these distance measures to the above mentioned model\nselection problems and substantiate their usefulness.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 14:27:40 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 17:13:55 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 09:13:31 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Killiches", "Matthias", ""], ["Kraus", "Daniel", ""], ["Czado", "Claudia", ""]]}, {"id": "1610.08805", "submitter": "Khanh To Duc", "authors": "Khanh To Duc, Monica Chiogna and Gianfranco Adimari", "title": "Estimation of the Volume Under the ROC Surface in Presence of\n  Nonignorable Verification Bias", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volume under the receiver operating characteristic surface (VUS) is\nuseful for measuring the overall accuracy of a diagnostic test when the\npossible disease status belongs to one of three ordered categories. In medical\nstudies, the VUS of a new test is typically estimated through a sample of\nmeasurements obtained by some suitable sample of patients. However, in many\ncases, only a subset of such patients has the true disease status assessed by a\ngold standard test. In this paper, for a continuous-scale diagnostic test, we\npropose four estimators of the VUS which accommodate for nonignorable\nmissingness of the disease status. The estimators are based on a parametric\nmodel which jointly describes both the disease and the verification process.\nIdentifiability of the model is discussed. Consistency and asymptotic normality\nof the proposed estimators are shown, and variance estimation is discussed. The\nfinite-sample behavior is investigated by means of simulation experiments. An\nillustration is provided.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 14:38:47 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 10:48:55 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Duc", "Khanh To", ""], ["Chiogna", "Monica", ""], ["Adimari", "Gianfranco", ""]]}, {"id": "1610.08860", "submitter": "Haiming Zhou", "authors": "Haiming Zhou and Xianzheng Huang", "title": "Nonparametric modal regression in the presence of measurement error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of regressing a response $Y$ on a predictor $X$, we consider\nestimating the local modes of the distribution of $Y$ given $X=x$ when $X$ is\nprone to measurement error. We propose two nonparametric estimation methods,\nwith one based on estimating the joint density of $(X, Y)$ in the presence of\nmeasurement error, and the other built upon estimating the conditional density\nof $Y$ given $X=x$ using error-prone data. We study the asymptotic properties\nof each proposed mode estimator, and provide implementation details including\nthe mean-shift algorithm for mode seeking and bandwidth selection. Numerical\nstudies are presented to compare the proposed methods with an existing mode\nestimation method developed for error-free data naively applied to error-prone\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 16:04:06 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Zhou", "Haiming", ""], ["Huang", "Xianzheng", ""]]}, {"id": "1610.08899", "submitter": "Haiqing Xu", "authors": "Qian Feng and Quang Vuong and Haiqing Xu", "title": "Estimation of heterogeneous individual treatment effects with endogenous\n  treatments", "comments": "32 pages, 12 figures, 6 tables, Texas Econometrics Camp 2015 and 2016\n  CEME conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper estimates individual treatment effects in a triangular model with\nbinary--valued endogenous treatments. Following the identification strategy\nestablished in Vuong and Xu (2014), we propose a two--stage estimation\napproach. First, we estimate the counterfactual outcome and hence the\nindividual treatment effect (ITE) for every observational unit in the sample.\nSecond, we estimate the density of individual treatment effects in the\npopulation. Our estimation method does not suffer from the ill--posed inverse\nproblem associated with inverting a non--linear functional. Asymptotic\nproperties of the proposed method are established. We study its finite sample\nproperties in Monte Carlo experiments. We also illustrate our approach with an\nempirical application assessing the effects of 401(k) retirement programs on\npersonal savings. Our results show that there exists a small but statistically\nsignificant proportion of individuals who experience negative effects, although\nthe majority of ITEs is positive.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 17:44:32 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Feng", "Qian", ""], ["Vuong", "Quang", ""], ["Xu", "Haiqing", ""]]}, {"id": "1610.08903", "submitter": "Haiqing Xu", "authors": "Haiqing Xu", "title": "Social Interactions in Large Networks: A Game Theoretic Approach", "comments": "A previous version of this paper was circulated under the title\n  \"Social interactions: a game theoretic approach\" (2010). This paper was\n  presented at the 2011 Cornell--PSU Macro Seminar, the 2011 North American\n  Econometric Society Summer Meeting, and 2012 Texas Econometrics Camp", "journal-ref": null, "doi": "10.1111/iere.12269", "report-no": null, "categories": "stat.ME cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies social interactions in a game theoretic model with players\nin a large social network. We consider observations from one single equilibrium\nof a large network game with asymmetric information, in which each player\nchooses an action from a finite set and is subject to interactions with her\nfriends. Simple assumptions about the structure are made to establish the\nexistence and uniqueness of equilibrium. In particular, we show that the\nequilibrium strategies satisfy a network decaying dependence (NDD) condition\nrequiring that dependence between any two players' decisions decays with their\nnetwork distance. The formulation of such an NDD property is novel and serves\nas the basis for statistical inference. Further, we establish the\nidentification of the structural model and introduce a computationally feasible\nand efficient estimation method. We illustrate the estimation method with an\nactual application to college attendance, as well as in Monte Carlo\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 17:50:50 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Xu", "Haiqing", ""]]}, {"id": "1610.08909", "submitter": "Haiqing Xu", "authors": "Nianqing Liu and Quang Vuong and Haiqing Xu", "title": "Rationalization and Identification of Binary Games with Correlated Types", "comments": "This paper was presented at Texas Metrics Camp 2013, the 2013 North\n  American Summer Meeting of the Econometric Society at University of Southern\n  California, and the 2013 Asian Meeting of the Econometric Society at National\n  University of Singapore", "journal-ref": null, "doi": "10.1016/j.jeconom.2017.08.007", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the rationalization and identification of binary games\nwhere players have correlated private types. Allowing for correlation is\ncrucial in global games and in models with social interactions as it represents\ncorrelated information and homophily, respectively. Our approach is fully\nnonparametric in the joint distribution of types and the strategic effects in\nthe payoffs. First, under monotone pure Bayesian Nash Equilibrium strategy, we\ncharacterize all the restrictions if any on the distribution of players'\nchoices imposed by the game-theoretic model as well as restrictions associated\nwith two assumptions frequently made in the empirical analysis of discrete\ngames. Namely, we consider exogeneity of payoff shifters relative to private\ninformation, and mutual independence of private information given payoff\nshifters. Second, we study the nonparametric identification of the payoff\nfunctions and types distribution. We show that the model with exogenous payoff\nshifters is fully identified up to a single location--scale normalization under\nsome exclusion restrictions and rank conditions. Third, we discuss partial\nidentification under weaker conditions and multiple equilibria. Lastly, we\nbriefly point out the implications of our results for model testing and\nestimation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 18:07:27 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Liu", "Nianqing", ""], ["Vuong", "Quang", ""], ["Xu", "Haiqing", ""]]}, {"id": "1610.09026", "submitter": "Y. Samuel Wang", "authors": "Y. Samuel Wang, Elena A. Erosheva", "title": "On the relationship between set-based and network-based measures of\n  gender homophily in scholarly publications", "comments": "University of Washington; Center for Statistics and Social Sciences;\n  WP 157", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increased interest in the scientific community in the problem of\nmeasuring gender homophily in co-authorship on scholarly publications (Eisen,\n2016). For a given set of publications and co-authorships, we assume that\nauthor identities have not been disambiguated in that we do not know when one\nperson is an author on more than one paper. In this case, one way to think\nabout measuring gender homophily is to consider all observed co-authorship\npairs and obtain a set-based gender homophily coefficient (e.g., Bergstrom et\nal., 2016). Another way is to consider papers as observed disjoint networks of\nco-authors and use a network-based assortativity coefficient (e.g., Newman,\n2003). In this note, we review both metrics and show that the gender homophily\nset-based index is equivalent to the gender assortativity network-based\ncoefficient with properly weighted edges.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 22:27:38 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 18:09:47 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Wang", "Y. Samuel", ""], ["Erosheva", "Elena A.", ""]]}, {"id": "1610.09033", "submitter": "Jaan Altosaar", "authors": "Rajesh Ranganath, Jaan Altosaar, Dustin Tran, David M. Blei", "title": "Operator Variational Inference", "comments": "Appears in Neural Information Processing Systems, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is an umbrella term for algorithms which cast Bayesian\ninference as optimization. Classically, variational inference uses the\nKullback-Leibler divergence to define the optimization. Though this divergence\nhas been widely used, the resultant posterior approximation can suffer from\nundesirable statistical properties. To address this, we reexamine variational\ninference from its roots as an optimization problem. We use operators, or\nfunctions of functions, to design variational objectives. As one example, we\ndesign a variational objective with a Langevin-Stein operator. We develop a\nblack box algorithm, operator variational inference (OPVI), for optimizing any\noperator objective. Importantly, operators enable us to make explicit the\nstatistical and computational tradeoffs for variational inference. We can\ncharacterize different properties of variational objectives, such as objectives\nthat admit data subsampling---allowing inference to scale to massive data---as\nwell as objectives that admit variational programs---a rich class of posterior\napproximations that does not require a tractable density. We illustrate the\nbenefits of OPVI on a mixture model and a generative model of images.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 23:32:25 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 23:58:43 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 01:08:06 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Altosaar", "Jaan", ""], ["Tran", "Dustin", ""], ["Blei", "David M.", ""]]}, {"id": "1610.09036", "submitter": "Yichen Zhou", "authors": "Yichen Zhou and Giles Hooker", "title": "Interpreting Models via Single Tree Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a procedure to build a decision tree which approximates the\nperformance of complex machine learning models. This single approximation tree\ncan be used to interpret and simplify the predicting pattern of random forests\n(RFs) and other models. The use of a tree structure is particularly relevant in\nmedical questionnaires where it enables an adaptive shortening of the\nquestionnaire, reducing response burden. We study the asymptotic behavior of\nsplits and introduce an improved splitting method designed to stabilize tree\nstructure. Empirical studies on both simulation and real data sets illustrate\nthat our method can simultaneously achieve high approximation power and\nstability.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 23:46:19 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Zhou", "Yichen", ""], ["Hooker", "Giles", ""]]}, {"id": "1610.09037", "submitter": "Dustin Tran", "authors": "Dustin Tran, Francisco J.R. Ruiz, Susan Athey, David M.Blei", "title": "Model Criticism for Bayesian Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of causal inference is to understand the outcome of alternative\ncourses of action. However, all causal inference requires assumptions. Such\nassumptions can be more influential than in typical tasks for probabilistic\nmodeling, and testing those assumptions is important to assess the validity of\ncausal inference. We develop model criticism for Bayesian causal inference,\nbuilding on the idea of posterior predictive checks to assess model fit. Our\napproach involves decomposing the problem, separately criticizing the model of\ntreatment assignments and the model of outcomes. Conditioned on the assumption\nof unconfoundedness---that the treatments are assigned independently of the\npotential outcomes---we show how to check any additional modeling assumption.\nOur approach provides a foundation for diagnosing model-based causal\ninferences.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 23:53:59 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Tran", "Dustin", ""], ["Ruiz", "Francisco J. R.", ""], ["Athey", "Susan", ""], ["Blei", "David M.", ""]]}, {"id": "1610.09139", "submitter": "Justin Chown", "authors": "Justin Chown and Ursula U. M\\\"uller", "title": "Detecting heteroskedasticity in nonparametric regression using weighted\n  empirical processes", "comments": "Preprint is 28 pages in length. This manuscript is currently under\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heteroskedastic errors can lead to inaccurate statistical conclusions if they\nare not properly handled. We introduce a test for heteroskedasticity for the\nnonparametric regression model with multiple covariates. It is based on a\nsuitable residual-based empirical distribution function. The residuals are\nconstructed using local polynomial smoothing. Our test statistic involves a\ndetection function that can verify heteroskedasticity by exploiting just the\nindependence-dependence structure between the detection function and model\nerrors, i.e. we do not require a specific model of the variance function. The\nprocedure is asymptotically distribution free: inferences made from it do not\ndepend on unknown parameters. It is consistent at the parametric (root-n) rate\nof convergence. Our results are extended to the case of missing responses and\nillustrated with simulations.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 09:25:28 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 12:20:50 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 11:06:41 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Chown", "Justin", ""], ["M\u00fcller", "Ursula U.", ""]]}, {"id": "1610.09363", "submitter": "Christoph Rothe", "authors": "Christoph Rothe, Dominik Wied", "title": "Estimating Derivatives of Function-Valued Parameters in a Class of\n  Moment Condition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general approach to estimating the derivative of a\nfunction-valued parameter $\\theta_o(u)$ that is identified for every value of\n$u$ as the solution to a moment condition. This setup in particular covers many\ninteresting models for conditional distributions, such as quantile regression\nor distribution regression. Exploiting that $\\theta_o(u)$ solves a moment\ncondition, we obtain an explicit expression for its derivative from the\nImplicit Function Theorem, and estimate the components of this expression by\nsuitable sample analogues, which requires the use of (local linear) smoothing.\nOur estimator can then be used for a variety of purposes, including the\nestimation of conditional density functions, quantile partial effects, and\nstructural auction models in economics.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 19:57:47 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Rothe", "Christoph", ""], ["Wied", "Dominik", ""]]}, {"id": "1610.09698", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo, Pape Djiby Mergane, Tchilabalo Abozou Kpanzou", "title": "On the joint distribution of variations of the Gini index and Welfare\n  indices", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to establish the asymptotic behavior of the mutual\ninfluence of the Gini index and the poverty measures by using the Gaussian\nfields described in Mergane and Lo(2013). The results are given as\nrepresentation theorems using the Gaussian fields of the unidimensional or the\nbidimensional functional Brownian bridges. Such representations, when combined\nwith those already available, lead to joint asymptotic distributions with other\nstatistics of interest like growth, welfare and inequality indices and then,\nunveil interesting results related to the mutual influence between them. The\nresults are also appropriate for studying whether a growth is fair or not,\ndepending on the variation of the inequality measure. Datadriven applications\nare also available. Although the variances may seem complicated at a first\nsight, their computations which are needed to get confidence intervals of the\nindices, are possible with the help of R codes we provide. Beyond the current\nresults, the provided representations are useful in connection with different\nones of other statistics.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 19:34:57 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 22:20:33 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Lo", "Gane Samb", ""], ["Mergane", "Pape Djiby", ""], ["Kpanzou", "Tchilabalo Abozou", ""]]}, {"id": "1610.09724", "submitter": "Sandipan Roy", "authors": "Sandipan Roy, Yves Atchad\\'e and George Michailidis", "title": "Likelihood Inference for Large Scale Stochastic Blockmodels with\n  Covariates based on a Divide-and-Conquer Parallelizable Algorithm with\n  Communication", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stochastic blockmodel equipped with node covariate information,\nthat is helpful in analyzing social network data. The key objective is to\nobtain maximum likelihood estimates of the model parameters. For this task, we\ndevise a fast, scalable Monte Carlo EM type algorithm based on case-control\napproximation of the log-likelihood coupled with a subsampling approach. A key\nfeature of the proposed algorithm is its parallelizability, by processing\nportions of the data on several cores, while leveraging communication of key\nstatistics across the cores during each iteration of the algorithm. The\nperformance of the algorithm is evaluated on synthetic data sets and compared\nwith competing methods for blockmodel parameter estimation. We also illustrate\nthe model on data from a Facebook derived social network enhanced with node\ncovariate information.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 22:57:43 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 14:28:06 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 16:01:01 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Roy", "Sandipan", ""], ["Atchad\u00e9", "Yves", ""], ["Michailidis", "George", ""]]}, {"id": "1610.09735", "submitter": "Haolei Weng", "authors": "Haolei Weng and Yang Feng", "title": "Community detection with nodal information", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is one of the fundamental problems in the study of\nnetwork data. Most existing community detection approaches only consider edge\ninformation as inputs, and the output could be suboptimal when nodal\ninformation is available. In such cases, it is desirable to leverage nodal\ninformation for the improvement of community detection accuracy. Towards this\ngoal, we propose a flexible network model incorporating nodal information, and\ndevelop likelihood-based inference methods. For the proposed methods, we\nestablish favorable asymptotic properties as well as efficient algorithms for\ncomputation. Numerical experiments show the effectiveness of our methods in\nutilizing nodal information across a variety of simulated and real network data\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 00:00:49 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 19:27:25 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Weng", "Haolei", ""], ["Feng", "Yang", ""]]}, {"id": "1610.09780", "submitter": "Rebecca Steorts", "authors": "Giacomo Zanella, Brenda Betancourt, Hanna Wallach, Jeffrey Miller,\n  Abbas Zaidi, and Rebecca C. Steorts", "title": "Flexible Models for Microclustering with Application to Entity\n  Resolution", "comments": "15 pages, 3 figures, 1 table, to appear NIPS 2016. arXiv admin note:\n  text overlap with arXiv:1512.00792", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most generative models for clustering implicitly assume that the number of\ndata points in each cluster grows linearly with the total number of data\npoints. Finite mixture models, Dirichlet process mixture models, and\nPitman--Yor process mixture models make this assumption, as do all other\ninfinitely exchangeable clustering models. However, for some applications, this\nassumption is inappropriate. For example, when performing entity resolution,\nthe size of each cluster should be unrelated to the size of the data set, and\neach cluster should contain a negligible fraction of the total number of data\npoints. These applications require models that yield clusters whose sizes grow\nsublinearly with the size of the data set. We address this requirement by\ndefining the microclustering property and introducing a new class of models\nthat can exhibit this property. We compare models within this class to two\ncommonly used clustering models using four entity-resolution data sets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 04:00:11 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Zanella", "Giacomo", ""], ["Betancourt", "Brenda", ""], ["Wallach", "Hanna", ""], ["Miller", "Jeffrey", ""], ["Zaidi", "Abbas", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1610.09788", "submitter": "Alexandre Thiery", "authors": "Chris Sherlock, Alexandre Thiery and Anthony Lee", "title": "Pseudo-marginal Metropolis--Hastings using averages of unbiased\n  estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a pseudo-marginal Metropolis--Hastings kernel $P_m$ that is\nconstructed using an average of $m$ exchangeable random variables, as well as\nan analogous kernel $P_s$ that averages $s<m$ of these same random variables.\nUsing an embedding technique to facilitate comparisons, we show that the\nasymptotic variances of ergodic averages associated with $P_m$ are lower\nbounded in terms of those associated with $P_s$. We show that the bound\nprovided is tight and disprove a conjecture that when the random variables to\nbe averaged are independent, the asymptotic variance under $P_m$ is never less\nthan $s/m$ times the variance under $P_s$. The conjecture does, however, hold\nwhen considering continuous-time Markov chains. These results imply that if the\ncomputational cost of the algorithm is proportional to $m$, it is often better\nto set $m=1$. We provide intuition as to why these findings differ so markedly\nfrom recent results for pseudo-marginal kernels employing particle filter\napproximations. Our results are exemplified through two simulation studies; in\nthe first the computational cost is effectively proportional to $m$ and in the\nsecond there is a considerable start-up cost at each iteration.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 05:01:09 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Sherlock", "Chris", ""], ["Thiery", "Alexandre", ""], ["Lee", "Anthony", ""]]}, {"id": "1610.09802", "submitter": "Paul Kabaila", "authors": "Paul Kabaila and Christeen Wijethunga", "title": "Confidence intervals centered on bootstrap smoothed estimators", "comments": "This is an updated version", "journal-ref": "Australian & New Zealand Journal of Statistics, 61, 19-38 (2019)", "doi": "10.1111/anzs.12252", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrap smoothed (bagged) parameter estimators have been proposed as an\nimprovement on estimators found after preliminary data-based model selection.\nThe key result of Efron (2014) is a very convenient and widely applicable\nformula for a delta method approximation to the standard deviation of the\nbootstrap smoothed estimator. This approximation provides an easily computed\nguide to the accuracy of this estimator. In addition, Efron (2014) proposed a\nconfidence interval centered on the bootstrap smoothed estimator, with width\nproportional to the estimate of this approximation to the standard deviation.\nWe evaluate this confidence interval in the scenario of two nested linear\nregression models, the full model and a simpler model, and a preliminary test\nof the null hypothesis that the simpler model is correct. We derive\ncomputationally convenient expressions for the ideal bootstrap smoothed\nestimator and the coverage probability and expected length of this confidence\ninterval. In terms of coverage probability, this confidence interval\noutperforms the post-model-selection confidence interval with the same nominal\ncoverage and based on the same preliminary test. We also compare the\nperformance of confidence interval centered on the bootstrap smoothed\nestimator, in terms of expected length, to the usual confidence interval, with\nthe same minimum coverage probablility, based on the full model.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 06:17:15 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 00:48:24 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 01:25:22 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Kabaila", "Paul", ""], ["Wijethunga", "Christeen", ""]]}, {"id": "1610.10028", "submitter": "Art Owen", "authors": "Art B. Owen", "title": "Refiltering hypothesis tests to control sign error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common, though not recommended statistical practice is to report confidence\nintervals if and only if they exclude a null value of 0. The resulting filtered\nconfidence intervals generally do not have their nominal confidence level. More\nworryingly, in low power settings their center points will be much farther from\nzero than the true parameter is and they will frequently lie on the wrong side\nof zero. Many confidence intervals are constructed using an asymptotically\nGaussian parameter estimate accompanied by a weakly consistent estimate of its\nvariance. In these cases, we can subject the given confidence interval(s) to a\nsecond filtering step such that the probability of a sign error is controled.\nThis refiltering step retains only those confidence intervals that are\nsufficiently well separated from the origin. It requires no assumptions on the\ndependencies among the test statistics.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 17:30:33 GMT"}, {"version": "v2", "created": "Sat, 24 Aug 2019 00:20:29 GMT"}, {"version": "v3", "created": "Wed, 13 Nov 2019 22:46:53 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Owen", "Art B.", ""]]}, {"id": "1610.10040", "submitter": "Shahin Tavakoli", "authors": "Shahin Tavakoli and Davide Pigoli and John A. D. Aston and John S.\n  Coleman", "title": "A Spatial Modeling Approach for Linguistic Object Data: Analysing\n  dialect sound variations across Great Britain", "comments": "18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialect variation is of considerable interest in linguistics and other social\nsciences. However, traditionally it has been studied using proxies\n(transcriptions) rather than acoustic recordings directly. We introduce novel\nstatistical techniques to analyse geolocalised speech recordings and to explore\nthe spatial variation of pronunciations continuously over the region of\ninterest, as opposed to traditional isoglosses, which provide a discrete\npartition of the region. Data of this type require an explicit modeling of the\nvariation in the mean and the covariance. Usual Euclidean metrics are not\nappropriate, and we therefore introduce the concept of $d$-covariance, which\nallows consistent estimation both in space and at individual locations. We then\npropose spatial smoothing for these objects which accounts for the possibly non\nconvex geometry of the domain of interest. We apply the proposed method to data\nfrom the spoken part of the British National Corpus, deposited at the British\nLibrary, London, and we produce maps of the dialect variation over Great\nBritain. In addition, the methods allow for acoustic reconstruction across the\ndomain of interest, allowing researchers to listen to the statistical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 18:05:47 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 16:11:07 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 11:34:28 GMT"}, {"version": "v4", "created": "Thu, 28 Jun 2018 20:29:12 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Tavakoli", "Shahin", ""], ["Pigoli", "Davide", ""], ["Aston", "John A. D.", ""], ["Coleman", "John S.", ""]]}]