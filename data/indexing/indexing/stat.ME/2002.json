[{"id": "2002.00010", "submitter": "Louise Kimpton", "authors": "Louise Kimpton, Peter Challenor, Daniel Williamson", "title": "Classification of Computer Models with Labelled Outputs", "comments": "arXiv admin note: substantial text overlap with arXiv:1901.07413", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is a vital tool that is important for modelling many complex\nnumerical models. A model or system may be such that, for certain areas of\ninput space, the output either does not exist, or is not in a quantifiable\nform. Here, we present a new method for classification where the model outputs\nare given distinct classifying labels, which we model using a latent Gaussian\nprocess (GP). The latent variable is estimated using MCMC sampling, a unique\nlikelihood and distinct prior specifications. Our classifier is then verified\nby calculating a misclassification rate across the input space.\n  Comparisons are made with other existing classification methods including\nlogistic regression, which models the probability of being classified into one\nof two regions. To make classification predictions we draw from an independent\nBernoulli distribution, meaning that distance correlation is lost from the\nindependent draws and so can result in many misclassifications. By modelling\nthe labels using a latent GP, this problem does not occur in our method. We\napply our novel method to a range of examples including a motivating example\nwhich models the hormones associated with the reproductive system in mammals,\nwhere the two labelled outputs are high and low rates of reproduction.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 16:33:05 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Kimpton", "Louise", ""], ["Challenor", "Peter", ""], ["Williamson", "Daniel", ""]]}, {"id": "2002.00033", "submitter": "Leah F. South", "authors": "Leah F. South, Toni Karvonen, Chris Nemeth, Mark Girolami and Chris.\n  J. Oates", "title": "Semi-Exact Control Functionals From Sard's Method", "comments": "There are 17 pages of main text. This revision provides an extended\n  version of Theorem 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The numerical approximation of posterior expected quantities of interest is\nconsidered. A novel control variate technique is proposed for post-processing\nof Markov chain Monte Carlo output, based both on Stein's method and an\napproach to numerical integration due to Sard. The resulting estimators are\nproven to be polynomially exact in the Gaussian context, while empirical\nresults suggest the estimators approximate a Gaussian cubature method near the\nBernstein-von-Mises limit. The main theoretical result establishes a\nbias-correction property in settings where the Markov chain does not leave the\nposterior invariant. Empirical results are presented across a selection of\nBayesian inference tasks. All methods used in this paper are available in the R\npackage ZVCV.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 19:27:04 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 17:24:46 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 06:11:17 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 06:37:40 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["South", "Leah F.", ""], ["Karvonen", "Toni", ""], ["Nemeth", "Chris", ""], ["Girolami", "Mark", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2002.00089", "submitter": "Neal Marquez", "authors": "Neal Marquez, Jon Wakefield", "title": "Harmonizing Child Mortality Data at Disparate Geographic Levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing focus on reducing inequalities in health outcomes in\ndeveloping countries. Subnational variation is of particular interest, with\ngeographic data used to understand the spatial risk of detrimental outcomes and\nto identify who is at greatest risk. While some health surveys provide\nobservations with associated geographic coordinates, many others provide data\nthat have their locations masked and instead only report the strata within\nwhich the data resides. How to harmonize these data sources for spatial\nanalysis has seen previously considered though no method has been agreed upon\nand comparison of the validity of methods are lacking. In this paper, we\npresent a new method for analyzing masked survey data alongside traditional\ngeolocated data, using a method that is consistent with the data generating\nprocess. In addition, we critique two proposed approaches to analyzing masked\ndata and illustrate that they are fundamentally flawed methodologically. To\nvalidate our method, we compare our approach with previously formulated\nsolutions in several realistic simulation environments in which the underlying\nstructure of the risk field is known. We simulate samples from spatial fields\nin a way that mimics the sampling frame implemented in the most common health\nsurveys in low and middle income countries, the DHS and MICS. In simulations,\nthe newly proposed approach outperforms previously proposed approaches in terms\nof minimizing error while increasing the precision of estimates. The approaches\nare subsequently compared using child mortality data from the Dominican\nRepublic where our findings are reinforced. Accurately increasing precision of\nchild mortality estimates, and health estimates in general, by leveraging\nvarious types of data improves our ability to implement precision public health\ninitiatives and better understand the landscape of geographic health\ninequalities.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 22:57:45 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 18:30:49 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Marquez", "Neal", ""], ["Wakefield", "Jon", ""]]}, {"id": "2002.00130", "submitter": "Tao Jiang", "authors": "Tao Jiang, Stephanie J. London, Mi Kyeong Lee, Josyf C. Mychaleckyj,\n  Alison A. Motsinger-Reif", "title": "Higher Criticism Tuned Regression For Weak And Sparse Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we propose a novel searching scheme for a tuning parameter in\nhigh-dimensional penalized regression methods to address variable selection and\nmodeling when sample sizes are limited compared to the data dimensions. Our\nmethod is motivated by high-throughput biological data such as genome-wide\nassociation studies (GWAS) and epigenome-wide association studies (EWAS). We\npropose a new estimate of the regularization parameter $\\lambda$ in penalized\nregression methods based on an estimated lower bound of the proportion of false\nnull hypotheses with confidence $(1 - \\alpha)$. The bound is estimated by\napplying the empirical null distribution of the higher criticism statistic, a\nsecond-level significance test constructed by dependent $p$-values using a\nmulti-split regression and aggregation method. A tuning parameter estimate in\npenalized regression, $\\lambda$, corresponds with the lower bound of the\nproportion of false null hypotheses. Different penalized regression methods\nwith varied signal sparsity and strength are compared in the multi-split method\nsetting. We demonstrate the performance of our method using both simulation\nexperiments and the applications of real data on (1) lipid-trait genetics from\nthe Action to Control Cardiovascular Risk in Diabetes (ACCORD) clinical trial\nand (2) epigenetic analysis evaluating smoking's influence in differential\nmethylation in the Agricultural Lung Health Study. The proposed algorithm is\nincluded in the HCTR package, available at\nhttps://cran.r-project.org/web/packages/HCTR/index.html.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 02:55:50 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 16:19:59 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2020 18:55:51 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Jiang", "Tao", ""], ["London", "Stephanie J.", ""], ["Lee", "Mi Kyeong", ""], ["Mychaleckyj", "Josyf C.", ""], ["Motsinger-Reif", "Alison A.", ""]]}, {"id": "2002.00158", "submitter": "Li Yin", "authors": "Xiaoqin Wang and Li Yin", "title": "Hypothesis Testing of Blip Effects in Sequential Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study the hypothesis testing of the blip / net effects of\ntreatments in a treatment sequence. We illustrate that the likelihood ratio\ntest and the score test may suffer from the curse of dimensionality, the null\nparadox and the high-dimensional constraint on standard parameters under the\nnull hypothesis. On the other hand, we construct the Wald test via a small\nnumber of point effects of treatments in single-point causal inference. We show\nthat the Wald test can avoid these problems under the same assumptions as the\nWald test for testing the point effect of treatment. The simulation study\nillustrates that the Wald test achieves the nominal level of type I error and a\nlow level of type II error. A real medical example illustrates how to conduct\nthe Wald test in practice.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 07:14:34 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Wang", "Xiaoqin", ""], ["Yin", "Li", ""]]}, {"id": "2002.00202", "submitter": "Rocio Titiunik", "authors": "Rocio Titiunik", "title": "Natural Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term natural experiment is used inconsistently. In one interpretation, it\nrefers to an experiment where a treatment is randomly assigned by someone other\nthan the researcher. In another interpretation, it refers to a study in which\nthere is no controlled random assignment, but treatment is assigned by some\nexternal factor in a way that loosely resembles a randomized experiment---often\ndescribed as an \"as if random\" assignment. In yet another interpretation, it\nrefers to any non-randomized study that compares a treatment to a control\ngroup, without any specific requirements on how the treatment is assigned. I\nintroduce an alternative definition that seeks to clarify the integral features\nof natural experiments and at the same time distinguish them from randomized\ncontrolled experiments. I define a natural experiment as a research study where\nthe treatment assignment mechanism (i) is neither designed nor implemented by\nthe researcher, (ii) is unknown to the researcher, and (iii) is probabilistic\nby virtue of depending on an external factor. The main message of this\ndefinition is that the difference between a randomized controlled experiment\nand a natural experiment is not a matter of degree, but of essence, and thus\nconceptualizing a natural experiment as a research design akin to a randomized\nexperiment is neither rigorous nor a useful guide to empirical analysis. Using\nmy alternative definition, I discuss how a natural experiment differs from a\ntraditional observational study, and offer practical recommendations for\nresearchers who wish to use natural experiments to study causal effects.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 12:52:51 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Titiunik", "Rocio", ""]]}, {"id": "2002.00208", "submitter": "Chainarong Amornbunchornvej", "authors": "Chainarong Amornbunchornvej, Elena Zheleva, and Tanya Berger-Wolf", "title": "Variable-lag Granger Causality and Transfer Entropy for Time Series\n  Analysis", "comments": "This preprint is the extension of the work [arXiv:1912.10829]\n  entitled \"Variable-lag Granger Causality for Time Series Analysis\" by the\n  same authors. The revision was made based on reviewers' suggestions. The R\n  package is available at https://github.com/DarkEyes/VLTimeSeriesCausality", "journal-ref": "ACM Transactions on Knowledge Discovery from Data (TKDD), 15(4),\n  67 (2021)", "doi": "10.1145/3441452", "report-no": null, "categories": "cs.LG econ.EM physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger causality is a fundamental technique for causal inference in time\nseries data, commonly used in the social and biological sciences. Typical\noperationalizations of Granger causality make a strong assumption that every\ntime point of the effect time series is influenced by a combination of other\ntime series with a fixed time delay. The assumption of fixed time delay also\nexists in Transfer Entropy, which is considered to be a non-linear version of\nGranger causality. However, the assumption of the fixed time delay does not\nhold in many applications, such as collective behavior, financial markets, and\nmany natural phenomena. To address this issue, we develop Variable-lag Granger\ncausality and Variable-lag Transfer Entropy, generalizations of both Granger\ncausality and Transfer Entropy that relax the assumption of the fixed time\ndelay and allow causes to influence effects with arbitrary time delays. In\naddition, we propose methods for inferring both variable-lag Granger causality\nand Transfer Entropy relations. In our approaches, we utilize an optimal\nwarping path of Dynamic Time Warping (DTW) to infer variable-lag causal\nrelations. We demonstrate our approaches on an application for studying\ncoordinated collective behavior and other real-world casual-inference datasets\nand show that our proposed approaches perform better than several existing\nmethods in both simulated and real-world datasets. Our approaches can be\napplied in any domain of time series analysis. The software of this work is\navailable in the R-CRAN package: VLTimeCausality.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 14:03:01 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 03:26:47 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 09:24:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Amornbunchornvej", "Chainarong", ""], ["Zheleva", "Elena", ""], ["Berger-Wolf", "Tanya", ""]]}, {"id": "2002.00245", "submitter": "Mandev Gill", "authors": "Mandev S. Gill, Philippe Lemey, Marc A. Suchard, Andrew Rambaut, Guy\n  Baele", "title": "Online Bayesian phylodynamic inference in BEAST with application to\n  epidemic reconstruction", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing pathogen dynamics from genetic data as they become available\nduring an outbreak or epidemic represents an important statistical scenario in\nwhich observations arrive sequentially in time and one is interested in\nperforming inference in an 'online' fashion. Widely-used Bayesian phylogenetic\ninference packages are not set up for this purpose, generally requiring one to\nrecompute trees and evolutionary model parameters de novo when new data arrive.\nTo accommodate increasing data flow in a Bayesian phylogenetic framework, we\nintroduce a methodology to efficiently update the posterior distribution with\nnewly available genetic data. Our procedure is implemented in the BEAST 1.10\nsoftware package, and relies on a distance-based measure to insert new taxa\ninto the current estimate of the phylogeny and imputes plausible values for new\nmodel parameters to accommodate growing dimensionality. This augmentation\ncreates informed starting values and re-uses optimally tuned transition kernels\nfor posterior exploration of growing data sets, reducing the time necessary to\nconverge to target posterior distributions. We apply our framework to data from\nthe recent West African Ebola virus epidemic and demonstrate a considerable\nreduction in time required to obtain posterior estimates at different time\npoints of the outbreak. Beyond epidemic monitoring, this framework easily finds\nother applications within the phylogenetics community, where changes in the\ndata -- in terms of alignment changes, sequence addition or removal -- present\ncommon scenarios that can benefit from online inference.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 17:30:59 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Gill", "Mandev S.", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""], ["Rambaut", "Andrew", ""], ["Baele", "Guy", ""]]}, {"id": "2002.00288", "submitter": "Yu Wang", "authors": "Yu Wang, Byoungwook Jang, Alfred Hero", "title": "The Sylvester Graphical Lasso (SyGlasso)", "comments": "Accepted in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces the Sylvester graphical lasso (SyGlasso) that captures\nmultiway dependencies present in tensor-valued data. The model is based on the\nSylvester equation that defines a generative model. The proposed model\ncomplements the tensor graphical lasso (Greenewald et al., 2019) that imposes a\nKronecker sum model for the inverse covariance matrix by providing an\nalternative Kronecker sum model that is generative and interpretable. A\nnodewise regression approach is adopted for estimating the conditional\nindependence relationships among variables. The statistical convergence of the\nmethod is established, and empirical studies are provided to demonstrate the\nrecovery of meaningful conditional dependency graphs. We apply the SyGlasso to\nan electroencephalography (EEG) study to compare the brain connectivity of\nalcoholic and nonalcoholic subjects. We demonstrate that our model can\nsimultaneously estimate both the brain connectivity and its temporal\ndependencies.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 22:57:45 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Wang", "Yu", ""], ["Jang", "Byoungwook", ""], ["Hero", "Alfred", ""]]}, {"id": "2002.00357", "submitter": "Anna Klimova", "authors": "Anna Klimova, Tam\\'as Rudas", "title": "Hierarchical Aitchison-Silvey models for incomplete binary sample spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate sample spaces may be incomplete Cartesian products, when certain\ncombinations of the categories of the variables are not possible. Traditional\nlog-linear models, which generalize independence and conditional independence,\ndo not apply in such cases, as they may associate positive probabilities with\nthe non-existing cells. To describe the association structure in incomplete\nsample spaces, this paper develops a class of hierarchical multiplicative\nmodels which are defined by setting certain non-homogeneous generalized odds\nratios equal to one and are named after Aitchison and Silvey who were among the\nfirst to consider such ratios. These models are curved exponential families\nthat do not contain an overall effect and, from an algebraic perspective, are\nnon-homogeneous toric ideals. The relationship of this model class with\nlog-linear models and quasi log-linear models is studied in detail in terms of\nboth statistics and algebraic geometry. The existence of maximum likelihood\nestimates and their properties, as well as the relevant algorithms are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 10:07:04 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Klimova", "Anna", ""], ["Rudas", "Tam\u00e1s", ""]]}, {"id": "2002.00499", "submitter": "Cole Sodja", "authors": "Cole Sodja", "title": "Detecting Anomalous Time Series by GAMLSS-Akaike-Weights-Scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An extensible statistical framework for detecting anomalous time series\nincluding those with heavy-tailed distributions and non-stationarity in\nhigher-order moments is introduced based on penalized likelihood distributional\nregression. Specifically, generalized additive models for location, scale, and\nshape are used to infer sample path representations defined by a parametric\ndistribution with parameters comprised of basis functions. Akaike weights are\nthen applied to each model and time series, yielding a probability measure that\ncan be effectively used to classify and rank anomalous time series. A\nmathematical exposition is also given to justify the proposed Akaike weight\nscoring under a suitable model embedding as a way to asymptotically identify\nanomalous time series. Studies evaluating the methodology on both multiple\nsimulations and real-world datasets also confirm that high accuracy can be\nobtained detecting many different and complex types of shape anomalies. Both\ncode implementing GAWS for running on a local machine and the datasets\nreferenced in this paper are available online.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 22:01:54 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Sodja", "Cole", ""]]}, {"id": "2002.00839", "submitter": "Xiao Guo", "authors": "Hai Zhang and Xiao Guo and Xiangyu Chang", "title": "Randomized Spectral Clustering in Large-Scale Stochastic Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering has been one of the widely used methods for community\ndetection in networks. However, large-scale networks bring computational\nchallenge to it. In this paper, we study spectral clustering using randomized\nsketching algorithms from a statistical perspective, where we typically assume\nthe network data are generated from a stochastic block model. To do this, we\nfirst use the recent developed sketching algorithms to derive two randomized\nspectral clustering algorithms, namely, the random projection-based and the\nrandom sampling-based spectral clustering. Then we study the theoretical bounds\nof the resulting algorithms in terms of the approximation error for the\npopulation adjacency matrix, the misclustering error, and the estimation error\nfor the link probability matrix. It turns out that, under mild conditions, the\nrandomized spectral clustering algorithms perform similarly to the original\none. We also conduct numerical experiments to support the theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 04:15:25 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 09:59:32 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zhang", "Hai", ""], ["Guo", "Xiao", ""], ["Chang", "Xiangyu", ""]]}, {"id": "2002.00849", "submitter": "Alex Stivala", "authors": "Alex D. Stivala, H. Colin Gallagher, David A. Rolls, Peng Wang, Garry\n  L. Robins", "title": "Using Sampled Network Data With The Autologistic Actor Attribute Model", "comments": "Change author affiliation, add citation for network science\n  collaboration network data, and other minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social science research increasingly benefits from statistical methods for\nunderstanding the structured nature of social life, including for social\nnetwork data. However, the application of statistical network models within\nlarge-scale community research is hindered by too little understanding of the\nvalidity of their inferences under realistic data collection conditions,\nincluding sampled or missing network data. The autologistic actor attribute\nmodel (ALAAM) is a statistical model based on the well-established exponential\nrandom graph model (ERGM) for social networks. ALAAMs can be regarded as a\nsocial influence model, predicting an individual-level outcome based on the\nactor's network ties, concurrent outcomes of his/her network partners, and\nattributes of the actor and his/her network partners. In particular, an ALAAM\ncan be used to measure contagion effects, that is, the propensity of two actors\nconnected by a social network tie to both have the same value of an attribute.\nWe investigate the effect of using simple random samples and snowball samples\nof network data on ALAAM parameter inference, and find that parameter inference\ncan still work well even with a nontrivial fraction of missing nodes. However\nit is safer to take a snowball sample of the network and estimate conditional\non the snowball sampling structure.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 05:04:32 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 03:34:46 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Stivala", "Alex D.", ""], ["Gallagher", "H. Colin", ""], ["Rolls", "David A.", ""], ["Wang", "Peng", ""], ["Robins", "Garry L.", ""]]}, {"id": "2002.00922", "submitter": "Yafei Han Ms", "authors": "Yafei Han, Christopher Zegras, Francisco Camara Pereira, Moshe\n  Ben-Akiva", "title": "A Neural-embedded Choice Model: TasteNet-MNL Modeling Taste\n  Heterogeneity with Flexibility and Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete choice models (DCMs) and neural networks (NNs) can complement each\nother. We propose a neural network embedded choice model - TasteNet-MNL, to\nimprove the flexibility in modeling taste heterogeneity while keeping model\ninterpretability. The hybrid model consists of a TasteNet module: a\nfeed-forward neural network that learns taste parameters as flexible functions\nof individual characteristics; and a choice module: a multinomial logit model\n(MNL) with manually specified utility. TasteNet and MNL are fully integrated\nand jointly estimated. By embedding a neural network into a DCM, we exploit a\nneural network's function approximation capacity to reduce specification bias.\nThrough special structure and parameter constraints, we incorporate expert\nknowledge to regularize the neural network and maintain interpretability. On\nsynthetic data, we show that TasteNet-MNL can recover the underlying non-linear\nutility function, and provide predictions and interpretations as accurate as\nthe true model; while examples of logit or random coefficient logit models with\nmisspecified utility functions result in large parameter bias and low\npredictability. In the case study of Swissmetro mode choice, TasteNet-MNL\noutperforms benchmarking MNLs' predictability; and discovers a wider spectrum\nof taste variations within the population, and higher values of time on\naverage. This study takes an initial step towards developing a framework to\ncombine theory-based and data-driven approaches for discrete choice modeling.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 18:03:54 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Han", "Yafei", ""], ["Zegras", "Christopher", ""], ["Pereira", "Francisco Camara", ""], ["Ben-Akiva", "Moshe", ""]]}, {"id": "2002.00993", "submitter": "Martina Vittorietti", "authors": "Martina Vittorietti, Javier Hidalgo, Jilt Sietsma, Wei Li, Geurt\n  Jongbloed", "title": "Isotonic regression for metallic microstructure data: estimation and\n  testing under order restrictions", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigating the main determinants of the mechanical performance of metals\nis not a simple task. Already known physical inspired qualitative relations\nbetween 2D microstructure characteristics and 3D mechanical properties can act\nas the starting point of the investigation. Isotonic regression allows to take\ninto account ordering relations and leads to more efficient and accurate\nresults when the underlying assumptions actually hold. The main goal in this\npaper is to test order relations in a model inspired by a materials science\napplication. The statistical estimation procedure is described considering\nthree different scenarios according to the knowledge of the variances: known\nvariance ratio, completely unknown variances, variances under order\nrestrictions. New likelihood ratio tests are developed in the last two cases.\nBoth parametric and non-parametric bootstrap approaches are developed for\nfinding the distribution of the test statistics under the null hypothesis.\nFinally an application on the relation between Geometrically Necessary\nDislocations and number of observed microstructure precipitations is shown.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 19:42:04 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Vittorietti", "Martina", ""], ["Hidalgo", "Javier", ""], ["Sietsma", "Jilt", ""], ["Li", "Wei", ""], ["Jongbloed", "Geurt", ""]]}, {"id": "2002.01003", "submitter": "Daniel Eck", "authors": "Daniel J. Eck", "title": "General model-free weighted envelope estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Envelope methodology is succinctly pitched as a class of procedures for\nincreasing efficiency in multivariate analyses without altering traditional\nobjectives \\citep[first sentence of page 1]{cook2018introduction}. This\ndescription is true with the additional caveat that the efficiency gains\nobtained by envelope methodology are mitigated by model selection volatility to\nan unknown degree. The bulk of the current envelope methodology literature does\nnot account for this added variance that arises from the uncertainty in model\nselection. Recent strides to account for model selection volatility have been\nmade on two fronts: 1) development of a weighted envelope estimator, in the\ncontext of multivariate regression, to account for this variability directly;\n2) development of a model selection criterion that facilitates consistent\nestimation of the correct envelope model for more general settings. In this\npaper, we unify these two directions and provide weighted envelope estimators\nthat directly account for the variability associated with model selection and\nare appropriate for general multivariate estimation settings for vector valued\nparameters. Our weighted estimation technique provides practitioners with\nrobust and useful variance reduction in finite samples. Theoretical\njustification is given for our estimators and validity of a nonparametric\nbootstrap procedure for estimating their asymptotic variance are established.\nSimulation studies and a real data analysis support our claims and demonstrate\nthe advantage of our weighted envelope estimator when model selection\nvariability is present.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 20:31:46 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Eck", "Daniel J.", ""]]}, {"id": "2002.01018", "submitter": "Kresten Lindorff-Larsen", "authors": "Kresten Lindorff-Larsen", "title": "Dissecting the statistical properties of the Linear Extrapolation Method\n  of determining protein stability", "comments": "12 pages, 8 figures (version updated with revised text)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.bio-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When protein stability is measured by denaturant induced unfolding the linear\nextrapolation method is usually used to analyse the data. This method is based\non the observation that the change in Gibbs free energy associated with\nunfolding, $\\Delta_rG$, is often found to be a linear function of the\ndenaturant concentration, $D$. The free energy change of unfolding in the\nabsence of denaturant, $\\Delta_rG_0$, is estimated by extrapolation from this\nlinear relationship. Data analysis is generally done by nonlinear least-squares\nregression to obtain estimates of the parameters as well as confidence\nintervals. We have compared different methods for calculating confidence\nintervals of the parameters and found that a simple method based on linear\ntheory gives as good, if not better, results than more advanced methods. We\nhave also compared three different parameterizations of the linear\nextrapolation method and show that one of the forms, $\\Delta_rG(D) =\n\\Delta_rG_0 - mD$, is problematic since the value of $\\Delta_rG_0$ and that of\nthe $m$-value are correlated in the nonlinear least-squares analysis. Parameter\ncorrelation can in some cases cause problems in the estimation of\nconfidence-intervals and -regions and should be avoided when possible. Two\nalternative parameterizations, $\\Delta_rG(D) = -m(D-D_{50})$ and $\\Delta_rG(D)\n= \\Delta_rG_0(1-D/D_{50})$, where $D_{50}$ is the midpoint of the transition\nregion show much less correlation between parameters.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 21:15:52 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 20:36:27 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 00:48:33 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Lindorff-Larsen", "Kresten", ""]]}, {"id": "2002.01040", "submitter": "Fernanda Lang Schumacher", "authors": "Fernanda L. Schumacher, Victor H. Lachos and Larissa A. Matos", "title": "Scale mixture of skew-normal linear mixed models with within-subject\n  serial dependence", "comments": null, "journal-ref": null, "doi": "10.1002/sim.8870", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In longitudinal studies, repeated measures are collected over time and hence\nthey tend to be serially correlated. In this paper we consider an extension of\nskew-normal/independent linear mixed models introduced by Lachos et al. (2010),\nwhere the error term has a dependence structure, such as damped exponential\ncorrelation or autoregressive correlation of order p. The proposed model\nprovides flexibility in capturing the effects of skewness and heavy tails\nsimultaneously when continuous repeated measures are serially correlated. For\nthis robust model, we present an efficient EM-type algorithm for computation of\nmaximum likelihood estimation of parameters and the observed information matrix\nis derived analytically to account for standard errors. The methodology is\nillustrated through an application to schizophrenia data and several simulation\nstudies. The proposed algorithm and methods are implemented in the new R\npackage skewlmm.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 22:42:13 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 20:46:49 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 03:11:02 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Schumacher", "Fernanda L.", ""], ["Lachos", "Victor H.", ""], ["Matos", "Larissa A.", ""]]}, {"id": "2002.01052", "submitter": "Ryan Martin", "authors": "Indrabati Bhattacharya and Ryan Martin", "title": "Gibbs posterior inference on multivariate quantiles", "comments": "24 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian and other likelihood-based methods require specification of a\nstatistical model and may not be fully satisfactory for inference on\nquantities, such as quantiles, that are not naturally defined as model\nparameters. In this paper, we construct a direct and model-free Gibbs posterior\ndistribution for multivariate quantiles. Being model-free means that inferences\ndrawn from the Gibbs posterior are not subject to model misspecification bias,\nand being direct means that no priors for or marginalization over nuisance\nparameters are required. We show here that the Gibbs posterior enjoys a\nroot-$n$ convergence rate and a Bernstein--von Mises property, i.e., for large\nn, the Gibbs posterior distribution can be approximated by a Gaussian.\nMoreover, we present numerical results showing the validity and efficiency of\ncredible sets derived from a suitably scaled Gibbs posterior.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 23:40:36 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 11:56:23 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 09:56:12 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Bhattacharya", "Indrabati", ""], ["Martin", "Ryan", ""]]}, {"id": "2002.01095", "submitter": "Qiong Zhang", "authors": "Qiong Zhang, Amin Khademi, Yongjia Song", "title": "Robust Optimal Design of Two-Armed Trials with Side Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant evidence has become available that emphasizes the importance of\npersonalization in medicine. In fact, it has become a common belief that\npersonalized medicine is the future of medicine. The core of personalized\nmedicine is the ability to design clinical trials that investigate the role of\npatient covariates on treatment effects. In this work, we study the optimal\ndesign of two-armed clinical trials to maximize accuracy of statistical models\nwhere the interaction between patient covariates and treatment effect are\nincorporated to enable precision medication. Such a modeling extension leads to\nsignificant complexities for the produced optimization problems because they\ninclude optimization over design and covariates concurrently. We take a robust\noptimization approach and minimize (over design) the maximum (over population)\nvariance of interaction effect between treatment and patient covariates. This\nresults in a min-max bi-level mixed integer nonlinear programming problem,\nwhich is notably challenging to solve. To address this challenge, we introduce\na surrogate model by approximating the objective function for which we propose\ntwo solution approaches. The first approach provides an exact solution based on\nreformulation and decomposition techniques. In the second approach, we provide\na lower bound for the inner optimization problem and solve the outer\noptimization problem over the lower bound. We test our proposed algorithms with\nsynthetic and real-world data sets and compare it with standard\n(re-)randomization methods. Our numerical analysis suggests that the lower\nbounding approach provides high-quality solutions across a variety of settings.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 02:46:35 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 15:47:40 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Zhang", "Qiong", ""], ["Khademi", "Amin", ""], ["Song", "Yongjia", ""]]}, {"id": "2002.01124", "submitter": "Douglas Nychka", "authors": "Ashton Wiens, Douglas Nychka, and William Kleibe", "title": "Modeling spatial data using local likelihood estimation and a Mat\\'ern\n  to SAR translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling data with non-stationary covariance structure is important to\nrepresent heterogeneity in geophysical and other environmental spatial\nprocesses. In this work, we investigate a multistage approach to modeling\nnon-stationary covariances that is efficient for large data sets. First, we use\nlikelihood estimation in local, moving windows to infer spatially varying\ncovariance parameters. These surfaces of covariance parameters can then be\nencoded into a global covariance model specifying the second-order structure\nfor the complete spatial domain. The resulting global model allows for\nefficient simulation and prediction. We investigate the non-stationary spatial\nautoregressive (SAR) model related to Gaussian Markov random field (GMRF)\nmethods, which is amenable to plug in local estimates and practical for large\ndata sets. In addition we use a simulation study to establish the accuracy of\nlocal Mat\\'ern parameter estimation as a reliable technique when replicate\nfields are available and small local windows are exploited to reduce\ncomputation. This multistage modeling approach is implemented on a\nnon-stationary climate model output data set with the goal of emulating the\nvariation in the numerical model ensemble using a Gaussian process.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 04:43:16 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Wiens", "Ashton", ""], ["Nychka", "Douglas", ""], ["Kleibe", "William", ""]]}, {"id": "2002.01146", "submitter": "Nicole Pashley", "authors": "Peter Z. Schochet, Nicole E. Pashley, Luke W. Miratrix, and Tim Kautz", "title": "Design-Based Ratio Estimators and Central Limit Theorems for Clustered,\n  Blocked RCTs", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2021.1906685", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops design-based ratio estimators for clustered, blocked\nrandomized controlled trials (RCTs), with an application to a federally funded,\nschool-based RCT testing the effects of behavioral health interventions. We\nconsider finite population weighted least squares estimators for average\ntreatment effects (ATEs), allowing for general weighting schemes and\ncovariates. We consider models with block-by-treatment status interactions as\nwell as restricted models with block indicators only. We prove new finite\npopulation central limit theorems for each block specification. We also discuss\nsimple variance estimators that share features with commonly used\ncluster-robust standard error estimators. Simulations show that the\ndesign-based ATE estimator yields nominal rejection rates with standard errors\nnear true ones, even with few clusters.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 06:31:14 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 16:55:38 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 18:40:08 GMT"}, {"version": "v4", "created": "Fri, 11 Dec 2020 19:43:51 GMT"}, {"version": "v5", "created": "Thu, 25 Feb 2021 21:40:08 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Schochet", "Peter Z.", ""], ["Pashley", "Nicole E.", ""], ["Miratrix", "Luke W.", ""], ["Kautz", "Tim", ""]]}, {"id": "2002.01290", "submitter": "Nora L\\\"uthen", "authors": "Nora L\\\"uthen, Stefano Marelli and Bruno Sudret", "title": "Sparse Polynomial Chaos Expansions: Literature Survey and Benchmark", "comments": null, "journal-ref": "SIAM/ASA J. Uncertainty Quantification (2021), 9(2), 593-649", "doi": "10.1137/20M1315774", "report-no": "RSUQ-2020-002C", "categories": "math.NA cs.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse polynomial chaos expansions (PCE) are a popular surrogate modelling\nmethod that takes advantage of the properties of PCE, the sparsity-of-effects\nprinciple, and powerful sparse regression solvers to approximate computer\nmodels with many input parameters, relying on only few model evaluations.\nWithin the last decade, a large number of algorithms for the computation of\nsparse PCE have been published in the applied math and engineering literature.\nWe present an extensive review of the existing methods and develop a framework\nfor classifying the algorithms. Furthermore, we conduct a unique benchmark on a\nselection of methods to identify which approaches work best in practical\napplications. Comparing their accuracy on several benchmark models of varying\ndimensionality and complexity, we find that the choice of sparse regression\nsolver and sampling scheme for the computation of a sparse PCE surrogate can\nmake a significant difference, of up to several orders of magnitude in the\nresulting mean-squared error. Different methods seem to be superior in\ndifferent regimes of model dimensionality and experimental design size.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 14:14:16 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 14:38:47 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 14:57:44 GMT"}, {"version": "v4", "created": "Wed, 19 May 2021 13:00:19 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["L\u00fcthen", "Nora", ""], ["Marelli", "Stefano", ""], ["Sudret", "Bruno", ""]]}, {"id": "2002.01305", "submitter": "Elynn Chen", "authors": "Elynn Y. Chen, Xin Yun, Rong Chen, Qiwei Yao", "title": "Modeling Multivariate Spatial-Temporal Data with Latent Low-Dimensional\n  Dynamics", "comments": "arXiv admin note: text overlap with arXiv:1710.06351", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional multivariate spatial-temporal data arise frequently in a\nwide range of applications; however, there are relatively few statistical\nmethods that can simultaneously deal with spatial, temporal and variable-wise\ndependencies in large data sets. In this paper, we propose a new approach to\nutilize the correlations in variable, space and time to achieve dimension\nreduction and to facilitate spatial/temporal predictions in the\nhigh-dimensional settings. The multivariate spatial-temporal process is\nrepresented as a linear transformation of a lower-dimensional latent factor\nprocess. The spatial dependence structure of the factor process is further\nrepresented non-parametrically in terms of latent empirical orthogonal\nfunctions. The low-dimensional structure is completely unknown in our setting\nand is learned entirely from data collected irregularly over space but\nregularly over time. We propose innovative estimation and prediction methods\nbased on the latent low-rank structures. Asymptotic properties of the\nestimators and predictors are established. Extensive experiments on synthetic\nand real data sets show that, while the dimensions are reduced significantly,\nthe spatial, temporal and variable-wise covariance structures are largely\npreserved. The efficacy of our method is further confirmed by the prediction\nperformances on both synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 01:45:53 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Chen", "Elynn Y.", ""], ["Yun", "Xin", ""], ["Chen", "Rong", ""], ["Yao", "Qiwei", ""]]}, {"id": "2002.01321", "submitter": "Evan Baker", "authors": "Evan Baker, Pierre Barbillon, Arindam Fadikar, Robert B. Gramacy, Radu\n  Herbei, David Higdon, Jiangeng Huang, Leah R. Johnson, Pulong Ma, Anirban\n  Mondal, Bianica Pires, Jerome Sacks, Vadim Sokolov", "title": "Analyzing Stochastic Computer Models: A Review with Opportunities", "comments": "48 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern science, computer models are often used to understand complex\nphenomena, and a thriving statistical community has grown around analyzing\nthem. This review aims to bring a spotlight to the growing prevalence of\nstochastic computer models -- providing a catalogue of statistical methods for\npractitioners, an introductory view for statisticians (whether familiar with\ndeterministic computer models or not), and an emphasis on open questions of\nrelevance to practitioners and statisticians. Gaussian process surrogate models\ntake center stage in this review, and these, along with several extensions\nneeded for stochastic settings, are explained. The basic issues of designing a\nstochastic computer experiment and calibrating a stochastic computer model are\nprominent in the discussion. Instructive examples, with data and code, are used\nto describe the implementation of, and results from, various methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 14:36:24 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 09:06:00 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 08:16:09 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Baker", "Evan", ""], ["Barbillon", "Pierre", ""], ["Fadikar", "Arindam", ""], ["Gramacy", "Robert B.", ""], ["Herbei", "Radu", ""], ["Higdon", "David", ""], ["Huang", "Jiangeng", ""], ["Johnson", "Leah R.", ""], ["Ma", "Pulong", ""], ["Mondal", "Anirban", ""], ["Pires", "Bianica", ""], ["Sacks", "Jerome", ""], ["Sokolov", "Vadim", ""]]}, {"id": "2002.01339", "submitter": "Dalia Chakrabarty Dr.", "authors": "Kangrui Wang and Dalia Chakrabarty", "title": "Soft Random Graphs in Probabilistic Metric Spaces & Inter-graph Distance", "comments": "arXiv admin note: substantial text overlap with arXiv:1710.11292", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for learning Soft Random Geometric Graphs (SRGGs),\ndrawn in probabilistic metric spaces, with the connection function of the graph\ndefined as the marginal posterior probability of an edge random variable, given\nthe correlation between the nodes connected by that edge. In fact, this\ninter-node correlation matrix is itself a random variable in our learning\nstrategy, and we learn this by identifying each node as a random variable,\nmeasurements of which comprise a column of a given multivariate dataset. We\nundertake inference with Metropolis with a 2-block update scheme. The SRGG is\nshown to be generated by a non-homogeneous Poisson point process, the intensity\nof which is location-dependent. Given the multivariate dataset, likelihood of\nthe inter-column correlation matrix is attained following achievement of a\nclosed-form marginalisation over all inter-row correlation matrices. Distance\nbetween a pair of graphical models learnt given the respective datasets, offers\nthe absolute correlation between the given datasets; such inter-graph distance\ncomputation is our ultimate objective, and is achieved using a newly introduced\nmetric that resembles an uncertainty-normalised Hellinger distance between\nposterior probabilities of the two learnt SRGGs, given the respective datasets.\nTwo sets of empirical illustrations on real data are undertaken, and\napplication to simulated data is included to exemplify the effect of\nincorporating measurement noise in the learning of a graphical model.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 15:09:45 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Wang", "Kangrui", ""], ["Chakrabarty", "Dalia", ""]]}, {"id": "2002.01350", "submitter": "Steven Thompson", "authors": "Steve Thompson", "title": "New estimates for network sampling", "comments": "arXiv admin note: substantial text overlap with arXiv:1909.05018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network sampling is used around the world for surveys of vulnerable,\nhard-to-reach populations including people at risk for HIV, opioid misuse, and\nemerging epidemics. The sampling methods include tracing social links to add\nnew people to the sample. Current estimates from these surveys are inaccurate,\nwith large biases and mean squared errors and unreliable confidence intervals.\nNew estimators are introduced here which eliminate almost all of the bias, have\nmuch lower mean squared error, and enable confidence intervals with good\nproperties. The improvement is attained by avoiding unrealistic assumptions\nabout the population network and the design, instead using the topology of the\nsample network data together with the sampling design actually used. In\nsimulations using the real network of an at-risk population, the new estimates\neliminate almost all the bias and have mean squared-errors that are 2 to 92\ntimes lower than those of current estimators. The new estimators are effective\nwith a wide variety of network designs including those with strongly restricted\nbranching such as Respondent-Driven Sampling and freely branching designs such\nas Snowball Sampling.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 19:16:17 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Thompson", "Steve", ""]]}, {"id": "2002.01369", "submitter": "Marta Bofill Roig", "authors": "Marta Bofill Roig and Guadalupe G\\'omez Melis", "title": "A class of two-sample nonparametric statistics for binary and\n  time-to-event outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of two-sample statistics for testing the equality of\nproportions and the equality of survival functions. We build our proposal on a\nweighted combination of a score test for the difference in proportions and a\nWeighted Kaplan-Meier statistic-based test for the difference of survival\nfunctions. The proposed statistics are fully non-parametric and do not rely on\nthe proportional hazards assumption for the survival outcome. We present the\nasymptotic distribution of these statistics, propose a variance estimator and\nshow their asymptotic properties under fixed and local alternatives. We discuss\ndifferent choices of weights including those that control the relative\nrelevance of each outcome and emphasize the type of difference to be detected\nin the survival outcome. We evaluate the performance of these statistics with a\nsimulation study, and illustrate their use with a randomized phase III cancer\nvaccine trial. We have implemented the proposed statistics in the R package\nSurvBin, available on GitHub (https://github.com/MartaBofillRoig/SurvBin).\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 15:35:51 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 15:57:41 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Roig", "Marta Bofill", ""], ["Melis", "Guadalupe G\u00f3mez", ""]]}, {"id": "2002.01645", "submitter": "Jes\\'us Daniel Arroyo Reli\\'on", "authors": "Jes\\'us Arroyo, Elizaveta Levina", "title": "Simultaneous prediction and community detection for networks with\n  application to neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community structure in networks is observed in many different domains, and\nunsupervised community detection has received a lot of attention in the\nliterature. Increasingly the focus of network analysis is shifting towards\nusing network information in some other prediction or inference task rather\nthan just analyzing the network itself. In particular, in neuroimaging\napplications brain networks are available for multiple subjects and the goal is\noften to predict a phenotype of interest. Community structure is well known to\nbe a feature of brain networks, typically corresponding to different regions of\nthe brain responsible for different functions. There are standard parcellations\nof the brain into such regions, usually obtained by applying clustering methods\nto brain connectomes of healthy subjects. However, when the goal is predicting\na phenotype or distinguishing between different conditions, these static\ncommunities from an unrelated set of healthy subjects may not be the most\nuseful for prediction. Here we present a method for supervised community\ndetection, aiming to find a partition of the network into communities that is\nmost useful for predicting a particular response. We use a block-structured\nregularization penalty combined with a prediction loss function, and compute\nthe solution with a combination of a spectral method and an ADMM optimization\nalgorithm. We show that the spectral clustering method recovers the correct\ncommunities under a weighted stochastic block model. The method performs well\non both simulated and real brain networks, providing support for the idea of\ntask-dependent brain regions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 05:15:55 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 04:46:27 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Arroyo", "Jes\u00fas", ""], ["Levina", "Elizaveta", ""]]}, {"id": "2002.01648", "submitter": "Jes\\'us Daniel Arroyo Reli\\'on", "authors": "Jes\\'us Arroyo, Carey E. Priebe, Vince Lyzinski", "title": "Graph matching between bipartite and unipartite networks: to collapse,\n  or not to collapse, that is the question", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching consists of aligning the vertices of two unlabeled graphs in\norder to maximize the shared structure across networks; when the graphs are\nunipartite, this is commonly formulated as minimizing their edge disagreements.\nIn this paper, we address the common setting in which one of the graphs to\nmatch is a bipartite network and one is unipartite. Commonly, the bipartite\nnetworks are collapsed or projected into a unipartite graph, and graph matching\nproceeds as in the classical setting. This potentially leads to noisy edge\nestimates and loss of information. We formulate the graph matching problem\nbetween a bipartite and a unipartite graph using an undirected graphical model,\nand introduce methods to find the alignment with this model without collapsing.\nWe theoretically demonstrate that our methodology is consistent, and provide\nnon-asymptotic conditions that ensure exact recovery of the matching solution.\nIn simulations and real data examples, we show how our methods can result in a\nmore accurate matching than the naive approach of transforming the bipartite\nnetworks into unipartite, and we demonstrate the performance gains achieved by\nour method in simulated and real data networks, including a\nco-authorship-citation network pair, and brain structural and functional data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 05:24:54 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 21:30:45 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 17:35:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Arroyo", "Jes\u00fas", ""], ["Priebe", "Carey E.", ""], ["Lyzinski", "Vince", ""]]}, {"id": "2002.01822", "submitter": "Christian Hennig", "authors": "Serhat Emre Akhanli, Christian Hennig", "title": "Comparing clusterings and numbers of clusters by aggregation of\n  calibrated clustering validity indexes", "comments": "42 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key issue in cluster analysis is the choice of an appropriate clustering\nmethod and the determination of the best number of clusters. Different\nclusterings are optimal on the same data set according to different criteria,\nand the choice of such criteria depends on the context and aim of clustering.\nTherefore, researchers need to consider what data analytic characteristics the\nclusters they are aiming at are supposed to have, among others within-cluster\nhomogeneity, between-clusters separation, and stability. Here, a set of\ninternal clustering validity indexes measuring different aspects of clustering\nquality is proposed, including some indexes from the literature. Users can\nchoose the indexes that are relevant in the application at hand. In order to\nmeasure the overall quality of a clustering (for comparing clusterings from\ndifferent methods and/or different numbers of clusters), the index values are\ncalibrated for aggregation. Calibration is relative to a set of random\nclusterings on the same data. Two specific aggregated indexes are proposed and\ncompared with existing indexes on simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 15:08:19 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 15:31:35 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 16:34:30 GMT"}, {"version": "v4", "created": "Tue, 23 Jun 2020 16:02:38 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Akhanli", "Serhat Emre", ""], ["Hennig", "Christian", ""]]}, {"id": "2002.01973", "submitter": "Raul Rojas Prof.", "authors": "Raul Rojas", "title": "Exploring Maximum Entropy Distributions with Evolutionary Algorithms", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG cs.NE math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how to evolve numerically the maximum entropy probability\ndistributions for a given set of constraints, which is a variational calculus\nproblem. An evolutionary algorithm can obtain approximations to some well-known\nanalytical results, but is even more flexible and can find distributions for\nwhich a closed formula cannot be readily stated. The numerical approach handles\ndistributions over finite intervals. We show that there are two ways of\nconducting the procedure: by direct optimization of the Lagrangian of the\nconstrained problem, or by optimizing the entropy among the subset of\ndistributions which fulfill the constraints. An incremental evolutionary\nstrategy easily obtains the uniform, the exponential, the Gaussian, the\nlog-normal, the Laplace, among other distributions, once the constrained\nproblem is solved with any of the two methods. Solutions for mixed (\"chimera\")\ndistributions can be also found. We explain why many of the distributions are\nsymmetrical and continuous, but some are not.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 19:52:05 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Rojas", "Raul", ""]]}, {"id": "2002.02001", "submitter": "Marie Auger-M\\'eth\\'e", "authors": "Marie Auger-M\\'eth\\'e, Ken Newman, Diana Cole, Fanny Empacher, Rowenna\n  Gryba, Aaron A. King, Vianney Leos-Barajas, Joanna Mills Flemming, Anders\n  Nielsen, Giovanni Petris, Len Thomas", "title": "A guide to state-space modeling of ecological time series", "comments": "71 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models (SSMs) are an important modeling framework for analyzing\necological time series. These hierarchical models are commonly used to model\npopulation dynamics, animal movement, and capture-recapture data, and are now\nincreasingly being used to model other ecological processes. SSMs are popular\nbecause they are flexible and they model the natural variation in ecological\nprocesses separately from observation error. Their flexibility allows\necologists to model continuous, count, binary, and categorical data with linear\nor nonlinear processes that evolve in discrete or continuous time. Modeling the\ntwo sources of stochasticity separately allows researchers to differentiate\nbetween biological variation (e.g., in birth processes) and imprecision in the\nsampling methodology, and generally provides better estimates of the ecological\nquantities of interest than if only one source of stochasticity is directly\nmodeled. Since the introduction of SSMs, a broad range of fitting procedures\nhave been proposed. However, the variety and complexity of these procedures can\nlimit the ability of ecologists to formulate and fit their own SSMs. We provide\nthe knowledge for ecologists to create SSMs that are robust to common, and\noften hidden, estimation problems, and the model selection and validation tools\nthat can help them assess how well their models fit their data. In this paper,\nwe present a review of SSMs that will provide a strong foundation to ecologists\ninterested in learning about SSMs, introduce new tools to veteran SSM users,\nand highlight promising research directions for statisticians interested in\necological applications. The review is accompanied by an in-depth tutorial that\ndemonstrates how SSMs models can be fitted and validated in R. Together, the\nreview and tutorial present an introduction to SSMs that will help ecologists\nto formulate, fit, and validate their models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 21:43:46 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 22:29:11 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 02:52:32 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Auger-M\u00e9th\u00e9", "Marie", ""], ["Newman", "Ken", ""], ["Cole", "Diana", ""], ["Empacher", "Fanny", ""], ["Gryba", "Rowenna", ""], ["King", "Aaron A.", ""], ["Leos-Barajas", "Vianney", ""], ["Flemming", "Joanna Mills", ""], ["Nielsen", "Anders", ""], ["Petris", "Giovanni", ""], ["Thomas", "Len", ""]]}, {"id": "2002.02024", "submitter": "Michelle Carey", "authors": "Michelle Carey and James O. Ramsay", "title": "Fast Stable Parameter Estimation for Linear Dynamical Systems", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2020.107124", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical systems describe the changes in processes that arise naturally from\ntheir underlying physical principles, such as the laws of motion or the\nconservation of mass, energy or momentum. These models facilitate a causal\nexplanation for the drivers and impediments of the processes. But do they\ndescribe the behaviour of the observed data? And how can we quantify the\nmodels' parameters that cannot be measured directly? This paper addresses these\ntwo questions by providing a methodology for estimating the solution; and the\nparameters of linear dynamical systems from incomplete and noisy observations\nof the processes.\n  The proposed procedure builds on the parameter cascading approach, where a\nlinear combination of basis functions approximates the implicitly defined\nsolution of the dynamical system. The systems' parameters are then estimated so\nthat this approximating solution adheres to the data. By taking advantage of\nthe linearity of the system, we have simplified the parameter cascading\nestimation procedure, and by developing a new iterative scheme, we achieve fast\nand stable computation.\n  We illustrate our approach by obtaining a linear differential equation that\nrepresents real data from biomechanics. Comparing our approach with popular\nmethods for estimating the parameters of linear dynamical systems, namely, the\nnon-linear least-squares approach, simulated annealing, parameter cascading and\nsmooth functional tempering reveals a considerable reduction in computation and\nan improved bias and sampling variance.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 22:37:25 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Carey", "Michelle", ""], ["Ramsay", "James O.", ""]]}, {"id": "2002.02054", "submitter": "Xiaomeng Ju", "authors": "Xiaomeng Ju, Mat\\'ias Salibi\\'an-Barrera", "title": "Robust Boosting for Regression Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting algorithms construct a regression predictor using a linear\ncombination of ``base learners''. Boosting also offers an approach to obtaining\nrobust non-parametric regression estimators that are scalable to applications\nwith many explanatory variables. The robust boosting algorithm is based on a\ntwo-stage approach, similar to what is done for robust linear regression: it\nfirst minimizes a robust residual scale estimator, and then improves it by\noptimizing a bounded loss function. Unlike previous robust boosting proposals\nthis approach does not require computing an ad-hoc residual scale estimator in\neach boosting iteration. Since the loss functions involved in this robust\nboosting algorithm are typically non-convex, a reliable initialization step is\nrequired, such as an L1 regression tree, which is also fast to compute. A\nrobust variable importance measure can also be calculated via a permutation\nprocedure. Thorough simulation studies and several data analyses show that,\nwhen no atypical observations are present, the robust boosting approach works\nas well as the standard gradient boosting with a squared loss. Furthermore,\nwhen the data contain outliers, the robust boosting estimator outperforms the\nalternatives in terms of prediction error and variable selection accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 01:12:45 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 21:09:17 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Ju", "Xiaomeng", ""], ["Salibi\u00e1n-Barrera", "Mat\u00edas", ""]]}, {"id": "2002.02097", "submitter": "Michael Leung", "authors": "Michael P. Leung", "title": "Dependence-Robust Inference Using Resampled Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop inference procedures robust to general forms of weak dependence.\nThe procedures use test statistics constructed by resampling data in a manner\nthat does not depend on the unknown correlation structure of the data. We prove\nthat the statistics are asymptotically normal under the weak requirement that\nthe target parameter can be consistently estimated at the parametric rate. This\nholds for regular estimators under many well-known forms of weak dependence and\njustifies the claim of dependence-robustness. We consider applications to\nsettings with unknown or complicated forms of dependence, with various forms of\nnetwork dependence as leading examples. We develop tests for both moment\nequalities and inequalities.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 04:49:51 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 20:48:42 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 21:30:56 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Leung", "Michael P.", ""]]}, {"id": "2002.02272", "submitter": "Brice Ozenne", "authors": "Brice Ozenne, Patrick M. Fisher, Esben Budtz-J{\\o}rgensen", "title": "Small sample corrections for Wald tests in Latent Variable Models", "comments": null, "journal-ref": "The Journal of the Royal Statistical Society, Series C (Applied\n  Statistics) 2020", "doi": "10.1111/rssc.12414", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models (LVMs) are commonly used in psychology and\nincreasingly used for analyzing brain imaging data. Such studies typically\ninvolve a small number of participants (n<100), where standard asymptotic\nresults often fail to appropriately control the type 1 error. This paper\npresents two corrections improving the control of the type 1 error of Wald\ntests in LVMs estimated using maximum likelihood (ML). First, we derive a\ncorrection for the bias of the ML estimator of the variance parameters. This\nenables us to estimate corrected standard errors for model parameters and\ncorrected Wald statistics. Second, we use a Student's t-distribution instead of\na Gaussian distribution to account for the variability of the variance\nestimator. The degrees of freedom of the Student's t-distributions are\nestimated using a Satterthwaite approximation. A simulation study based on data\nfrom two published brain imaging studies demonstrates that combining these two\ncorrections provides superior control of the type 1 error rate compared to the\nuncorrected Wald test, despite being conservative for some parameters. The\nproposed methods are implemented in the R package lavaSearch2 available at\nhttps://cran.r-project.org/web/packages/lavaSearch2.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 14:25:33 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Ozenne", "Brice", ""], ["Fisher", "Patrick M.", ""], ["Budtz-J\u00f8rgensen", "Esben", ""]]}, {"id": "2002.02477", "submitter": "Jeremie Fish", "authors": "Jeremie Fish and Daqing Hou and Jie Sun and Erik Bollt", "title": "Inference of Polygenic Factors Associated with Breast Cancer Gene\n  Interaction Networks from Discrete Data Utilizing Poisson Multivariate Mutual\n  Information", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a new methodology to infer from gene expression\ndata the complex interactions associated with polygenetic diseases that remain\na major frontier in understanding factors in human health. In many cases\ndisease may be related to the covariance of several genes, rather than simply\nthe variance of a single gene, making network inference crucial to the\ndevelopment of potential treatments. Specifically we investigate the network of\nfactors and associations involved in developing breast cancer from gene\nexpression data. Our approach is information theoretic, but a major obstacle\nhas been the discrete nature of such data that is well described as a\nmulti-variate Poisson process. In fact despite that mutual information is\ngenerally a well regarded approach for developing networks of association in\ndata science of complex systems across many disciplines, until now a good\nmethod to accurately and efficiently compute entropies from such processes as\nbeen lacking. Nonparameteric methods such as the popular k-nearest neighbors\n(KNN) methods are slow converging and thus require unrealistic amounts of data.\nWe will use the causation entropy (CSE) principle, together with the associated\ngreedy search algorithm optimal CSE (oCSE) as a network inference method to\ndeduce the actual structure, with our multi-variate Poisson estimator developed\nhere as the core computational engine. We show that the Poisson version of oCSE\noutperforms both the Kraskov-St\\\"ogbauer-Grassberger (KSG) oCSE method (which\nis a KNN method for estimating the entropy) and the Gaussian oCSE method on\nsynthetic data. We present the results for a breast cancer gene expression data\nset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 19:20:14 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Fish", "Jeremie", ""], ["Hou", "Daqing", ""], ["Sun", "Jie", ""], ["Bollt", "Erik", ""]]}, {"id": "2002.02564", "submitter": "F. Richard Guo", "authors": "F. Richard Guo, James McQueen and Thomas S. Richardson", "title": "Empirical Bayes for Large-scale Randomized Experiments: a Spectral\n  Approach", "comments": "Corrections and notational changes to Sec 4.4; added acknowledgments;\n  some contents of Sec 2.3 are moved to the Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale randomized experiments, sometimes called A/B tests, are\nincreasingly prevalent in many industries. Though such experiments are often\nanalyzed via frequentist $t$-tests, arguably such analyses are deficient:\n$p$-values are hard to interpret and not easily incorporated into\ndecision-making. As an alternative, we propose an empirical Bayes approach,\nwhich assumes that the treatment effects are realized from a \"true prior\". This\nrequires inferring the prior from previous experiments. Following Robbins, we\nestimate a family of marginal densities of empirical effects, indexed by the\nnoise scale. We show that this family is characterized by the heat equation. We\ndevelop a spectral maximum likelihood estimate based on a Fourier series\nrepresentation, which can be efficiently computed via convex optimization. In\norder to select hyperparameters and compare models, we describe two model\nselection criteria. We demonstrate our method on simulated and real data, and\ncompare posterior inference to that under a Gaussian mixture model of the\nprior.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 00:25:07 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 23:25:39 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 22:59:43 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Guo", "F. Richard", ""], ["McQueen", "James", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "2002.02579", "submitter": "Bo Zhang", "authors": "Hongming Pu and Bo Zhang", "title": "Estimating Optimal Treatment Rules with an Instrumental Variable: A\n  Partial Identification Learning Approach", "comments": "J R Stat Soc Series B. (2021)", "journal-ref": null, "doi": "10.1111/rssb.12413", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individualized treatment rules (ITRs) are considered a promising recipe to\ndeliver better policy interventions. One key ingredient in optimal ITR\nestimation problems is to estimate the average treatment effect conditional on\na subject's covariate information, which is often challenging in observational\nstudies due to the universal concern of unmeasured confounding. Instrumental\nvariables (IVs) are widely-used tools to infer the treatment effect when there\nis unmeasured confounding between the treatment and outcome. In this work, we\npropose a general framework of approaching the optimal ITR estimation problem\nwhen a valid IV is allowed to only partially identify the treatment effect. We\nintroduce a novel notion of optimality called \"IV-optimality\". A treatment rule\nis said to be IV-optimal if it minimizes the maximum risk with respect to the\nputative IV and the set of IV identification assumptions. We derive a bound on\nthe risk of an IV-optimal rule that illuminates when an IV-optimal rule has\nfavorable generalization performance. We propose a classification-based\nstatistical learning method that estimates such an IV-optimal rule, design\ncomputationally-efficient algorithms, and prove theoretical guarantees. We\ncontrast our proposed method to the popular outcome weighted learning (OWL)\napproach via extensive simulations, and apply our method to study which mothers\nwould benefit from traveling to deliver their premature babies at hospitals\nwith high level neonatal intensive care units.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 01:27:36 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 05:15:56 GMT"}, {"version": "v3", "created": "Sun, 8 Mar 2020 23:48:44 GMT"}, {"version": "v4", "created": "Wed, 5 Aug 2020 19:18:54 GMT"}, {"version": "v5", "created": "Tue, 12 Jan 2021 02:10:39 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Pu", "Hongming", ""], ["Zhang", "Bo", ""]]}, {"id": "2002.02592", "submitter": "Nicholas James", "authors": "Nick James, Max Menzies", "title": "Equivalence relations and $L^p$ distances between time series", "comments": "Equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for defining equivalence and measuring\ndistances between time series, and a first concrete method for doing so. We\nprove the existence of equivalence relations on the space of time series, such\nthat the quotient spaces can be equipped with a metrizable topology. We\nillustrate algorithmically how to calculate such distances among a collection\nof time series, and perform clustering analysis based on these distances. We\napply these insights to analyse the recent bushfires in NSW, Australia. There,\nwe introduce a new method to analyse time series in a cross-contextual setting.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 02:32:33 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["James", "Nick", ""], ["Menzies", "Max", ""]]}, {"id": "2002.02594", "submitter": "Estate Khmaladze", "authors": "Estate V. Khmaladze", "title": "Distribution free testing for linear regression. Extension to general\n  parametric regression", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a distribution free approach for testing parametric hypotheses based\non unitary transformations has been suggested in \\cite{Khm13, Khm16, Khm17} and\nfurther studied in \\cite{Ngu17} and \\cite{Rob19}. In this note we show that the\ntransformation takes extremely simple form in distribution free testing of\nlinear regression. Then we extend it to general parametric regression with\nvector-valued covariates.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 02:39:37 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Khmaladze", "Estate V.", ""]]}, {"id": "2002.02601", "submitter": "Eric Lock", "authors": "Eric F. Lock, Jun Young Park, and Katherine A. Hoadley", "title": "Bidimensional linked matrix factorization for pan-omics pan-cancer\n  analysis", "comments": "46 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several modern applications require the integration of multiple large data\nmatrices that have shared rows and/or columns. For example, cancer studies that\nintegrate multiple omics platforms across multiple types of cancer, pan-omics\npan-cancer analysis, have extended our knowledge of molecular heterogenity\nbeyond what was observed in single tumor and single platform studies. However,\nthese studies have been limited by available statistical methodology. We\npropose a flexible approach to the simultaneous factorization and decomposition\nof variation across such bidimensionally linked matrices, BIDIFAC+. This\ndecomposes variation into a series of low-rank components that may be shared\nacross any number of row sets (e.g., omics platforms) or column sets (e.g.,\ncancer types). This builds on a growing literature for the factorization and\ndecomposition of linked matrices, which has primarily focused on multiple\nmatrices that are linked in one dimension (rows or columns) only. Our objective\nfunction extends nuclear norm penalization, is motivated by random matrix\ntheory, gives an identifiable decomposition under relatively mild conditions,\nand can be shown to give the mode of a Bayesian posterior distribution. We\napply BIDIFAC+ to pan-omics pan-cancer data from TCGA, identifying shared and\nspecific modes of variability across 4 different omics platforms and 29\ndifferent cancer types.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 03:11:44 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Lock", "Eric F.", ""], ["Park", "Jun Young", ""], ["Hoadley", "Katherine A.", ""]]}, {"id": "2002.02629", "submitter": "Tun Lee Ng", "authors": "Tun Lee Ng and Michael A. Newton", "title": "Random weighting in LASSO regression", "comments": "Major revision with extensions, consolidated theorems and numerical\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish statistical properties of random-weighting methods in LASSO\nregression under different regularization parameters $\\lambda_n$ and suitable\nregularity conditions. The random-weighting methods in view concern repeated\noptimization of a randomized objective function, motivated by the need for\ncomputational approximations to Bayesian posterior sampling. In the context of\nLASSO regression, we repeatedly assign analyst-drawn random weights to terms in\nthe objective function (including the penalty terms), and optimize to obtain a\nsample of random-weighting estimators. We show that existing approaches have\nconditional model selection consistency and conditional asymptotic normality at\ndifferent growth rates of $\\lambda_n$ as $n \\to \\infty$. We propose an\nextension to the available random-weighting methods and establish that the\nresulting samples attain conditional sparse normality and conditional\nconsistency in a growing-dimension setting. We find that random-weighting has\nboth approximate-Bayesian and sampling-theory interpretations. Finally, we\nillustrate the proposed methodology via extensive simulation studies and a\nbenchmark data example.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 05:37:04 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 16:28:26 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 00:58:28 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Ng", "Tun Lee", ""], ["Newton", "Michael A.", ""]]}, {"id": "2002.02711", "submitter": "Rapha\\\"el de Fondeville", "authors": "Rapha\\\"el de Fondeville and Anthony C. Davison", "title": "Functional Peaks-over-threshold Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peaks-over-threshold analysis using the generalized Pareto distribution is\nwidely applied in modelling tails of univariate random variables, but much\ninformation may be lost when complex extreme events are studied using\nunivariate results. In this paper, we extend peaks-over-threshold analysis to\nextremes of functional data. Threshold exceedances defined using a functional\n$r$ are modelled by the generalized $r$-Pareto process, a functional\ngeneralization of the generalized Pareto distribution that covers the three\nclassical regimes for the decay of tail probabilities, and that is the only\npossible continuous limit for $r$-exceedances of a properly rescaled process.\nWe give construction rules, simulation algorithms and inference procedures for\ngeneralized $r$-Pareto processes, discuss model validation, and use the new\nmethodology to study extreme European windstorms and heavy spatial rainfall.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 10:55:29 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 16:55:25 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["de Fondeville", "Rapha\u00ebl", ""], ["Davison", "Anthony C.", ""]]}, {"id": "2002.02770", "submitter": "Yaliang Li", "authors": "Liuyi Yao, Zhixuan Chu, Sheng Li, Yaliang Li, Jing Gao, Aidong Zhang", "title": "A Survey on Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference is a critical research topic across many domains, such as\nstatistics, computer science, education, public policy and economics, for\ndecades. Nowadays, estimating causal effect from observational data has become\nan appealing research direction owing to the large amount of available data and\nlow budget requirement, compared with randomized controlled trials. Embraced\nwith the rapidly developed machine learning area, various causal effect\nestimation methods for observational data have sprung up. In this survey, we\nprovide a comprehensive review of causal inference methods under the potential\noutcome framework, one of the well known causal inference framework. The\nmethods are divided into two categories depending on whether they require all\nthree assumptions of the potential outcome framework or not. For each category,\nboth the traditional statistical methods and the recent machine learning\nenhanced methods are discussed and compared. The plausible applications of\nthese methods are also presented, including the applications in advertising,\nrecommendation, medicine and so on. Moreover, the commonly used benchmark\ndatasets as well as the open-source codes are also summarized, which facilitate\nresearchers and practitioners to explore, evaluate and apply the causal\ninference methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 21:35:29 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Yao", "Liuyi", ""], ["Chu", "Zhixuan", ""], ["Li", "Sheng", ""], ["Li", "Yaliang", ""], ["Gao", "Jing", ""], ["Zhang", "Aidong", ""]]}, {"id": "2002.02933", "submitter": "Francesco Morandin", "authors": "Silvia Giulia Galfre' and Francesco Morandin", "title": "A mathematical framework for raw counts of single-cell RNA-seq data\n  analysis", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-cell RNA-seq data are challenging because of the sparseness of the\nread counts, the tiny expression of many relevant genes, and the variability in\nthe efficiency of RNA extraction for different cells. We consider a simple\nprobabilistic model for read counts, based on a negative binomial distribution\nfor each gene, modified by a cell-dependent coefficient interpreted as an\nextraction efficiency. We provide two alternative fast methods to estimate the\nmodel parameters, together with the probability that a cell results in zero\nread counts for a gene. This allows to measure genes co-expression and\ndifferential expression in a novel way.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 18:10:50 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Galfre'", "Silvia Giulia", ""], ["Morandin", "Francesco", ""]]}, {"id": "2002.03223", "submitter": "Michelle Ngo", "authors": "Michelle N. Ngo, Dustin S. Pluta, Alexander N. Ngo, Babak Shahbaba", "title": "Conjoined Dirichlet Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biclustering is a class of techniques that simultaneously clusters the rows\nand columns of a matrix to sort heterogeneous data into homogeneous blocks.\nAlthough many algorithms have been proposed to find biclusters, existing\nmethods suffer from the pre-specification of the number of biclusters or place\nconstraints on the model structure. To address these issues, we develop a\nnovel, non-parametric probabilistic biclustering method based on Dirichlet\nprocesses to identify biclusters with strong co-occurrence in both rows and\ncolumns. The proposed method utilizes dual Dirichlet process mixture models to\nlearn row and column clusters, with the number of resulting clusters determined\nby the data rather than pre-specified. Probabilistic biclusters are identified\nby modeling the mutual dependence between the row and column clusters. We apply\nour method to two different applications, text mining and gene expression\nanalysis, and demonstrate that our method improves bicluster extraction in many\nsettings compared to existing approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 19:41:23 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ngo", "Michelle N.", ""], ["Pluta", "Dustin S.", ""], ["Ngo", "Alexander N.", ""], ["Shahbaba", "Babak", ""]]}, {"id": "2002.03277", "submitter": "Miao Yu", "authors": "Miao Yu, Wenbin Lu, Rui Song", "title": "A New Framework for Online Testing of Heterogeneous Treatment Effect", "comments": "8 pages, no figures. To be published on AAAI 2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for online testing of heterogeneous treatment\neffects. The proposed test, named sequential score test (SST), is able to\ncontrol type I error under continuous monitoring and detect multi-dimensional\nheterogeneous treatment effects. We provide an online p-value calculation for\nSST, making it convenient for continuous monitoring, and extend our tests to\nonline multiple testing settings by controlling the false discovery rate. We\nexamine the empirical performance of the proposed tests and compare them with a\nstate-of-art online test, named mSPRT using simulations and a real data. The\nresults show that our proposed test controls type I error at any time, has\nhigher detection power and allows quick inference on online A/B testing.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 04:02:11 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Yu", "Miao", ""], ["Lu", "Wenbin", ""], ["Song", "Rui", ""]]}, {"id": "2002.03318", "submitter": "Yuling Jiao", "authors": "Xingdong Feng, Jian Huang, Yuling Jiao, Shuang Zhang", "title": "$\\ell_0$-Regularized High-dimensional Accelerated Failure Time Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a constructive approach for $\\ell_0$-penalized estimation in the\nsparse accelerated failure time (AFT) model with high-dimensional covariates.\nOur proposed method is based on Stute's weighted least squares criterion\ncombined with $\\ell_0$-penalization. This method is a computational algorithm\nthat generates a sequence of solutions iteratively, based on active sets\nderived from primal and dual information and root finding according to the KKT\nconditions. We refer to the proposed method as AFT-SDAR (for support detection\nand root finding). An important aspect of our theoretical results is that we\ndirectly concern the sequence of solutions generated based on the AFT-SDAR\nalgorithm. We prove that the estimation errors of the solution sequence decay\nexponentially to the optimal error bound with high probability, as long as the\ncovariate matrix satisfies a mild regularity condition which is necessary and\nsufficient for model identification even in the setting of high-dimensional\nlinear regression. We also proposed an adaptive version of AFT-SDAR, or\nAFT-ASDAR, which determines the support size of the estimated coefficient in a\ndata-driven fashion. We conduct simulation studies to demonstrate the superior\nperformance of the proposed method over the lasso and MCP in terms of accuracy\nand speed. We also apply the proposed method to a real data set to illustrate\nits application.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 08:50:36 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Feng", "Xingdong", ""], ["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Zhang", "Shuang", ""]]}, {"id": "2002.03355", "submitter": "Yusha Liu", "authors": "Yusha Liu, Meng Li, Jeffrey S. Morris", "title": "On Function-on-Scalar Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing work on functional response regression has focused predominantly on\nmean regression. However, in many applications, predictors may not strongly\ninfluence the conditional mean of functional responses, but other\ncharacteristics of their conditional distribution. In this paper, we study\nfunction-on-scalar quantile regression, or functional quantile regression\n(FQR), which can provide a comprehensive understanding of how scalar predictors\ninfluence the conditional distribution of functional responses. We introduce a\nscalable, distributed strategy to perform FQR that can account for\nintrafunctional dependence structures in the functional responses. This general\ndistributed strategy first performs separate quantile regression to compute\n$M$-estimators at each sampling location, and then carries out estimation and\ninference for the entire coefficient functions by properly exploiting the\nuncertainty quantifications and dependence structures of $M$-estimators. We\nderive a uniform Bahadur representation and a strong Gaussian approximation\nresult for the $M$-estimators on the discrete sampling grid, which are of\nindependent interest and provide theoretical justification for this distributed\nstrategy. Some large sample properties of the proposed coefficient function\nestimators are described. Interestingly, our rate calculations show a phase\ntransition phenomenon that has been previously observed in functional mean\nregression. We conduct simulations to assess the finite sample performance of\nthe proposed methods, and present an application to a mass spectrometry\nproteomics dataset, in which the use of FQR to delineate the relationship\nbetween functional responses and predictors is strongly warranted.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 12:44:49 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Liu", "Yusha", ""], ["Li", "Meng", ""], ["Morris", "Jeffrey S.", ""]]}, {"id": "2002.03375", "submitter": "Jingyu He", "authors": "Jingyu He, P. Richard Hahn", "title": "Stochastic tree ensembles for regularized nonlinear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops a novel stochastic tree ensemble method for nonlinear\nregression, which we refer to as XBART, short for Accelerated Bayesian Additive\nRegression Trees. By combining regularization and stochastic search strategies\nfrom Bayesian modeling with computationally efficient techniques from recursive\npartitioning approaches, the new method attains state-of-the-art performance:\nin many settings it is both faster and more accurate than the widely-used\nXGBoost algorithm. Via careful simulation studies, we demonstrate that our new\napproach provides accurate point-wise estimates of the mean function and does\nso faster than popular alternatives, such as BART, XGBoost and neural networks\n(using Keras). We also prove a number of basic theoretical results about the\nnew algorithm, including consistency of the single tree version of the model\nand stationarity of the Markov chain produced by the ensemble version.\nFurthermore, we demonstrate that initializing standard Bayesian additive\nregression trees Markov chain Monte Carlo (MCMC) at XBART-fitted trees\nconsiderably improves credible interval coverage and reduces total run-time.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 14:37:02 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 03:55:01 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 16:08:50 GMT"}, {"version": "v4", "created": "Thu, 3 Jun 2021 14:44:02 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["He", "Jingyu", ""], ["Hahn", "P. Richard", ""]]}, {"id": "2002.03382", "submitter": "Zhaoxing Gao", "authors": "Zhaoxing Gao", "title": "Segmenting High-dimensional Matrix-valued Time Series via Sequential\n  Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling matrix-valued time series is an interesting and important research\ntopic. In this paper, we extend the method of Chang et al. (2017) to\nmatrix-valued time series. For any given $p\\times q$ matrix-valued time series,\nwe look for linear transformations to segment the matrix into many small\nsub-matrices for which each of them are uncorrelated with the others both\ncontemporaneously and serially, thus they can be analyzed separately, which\nwill greatly reduce the number of parameters to be estimated in terms of\nmodeling. To overcome the identification issue, we propose a two-step and more\nstructured procedure to segment the rows and columns separately. When\n$\\max(p,q)$ is large in relation to the sample size $n$, we assume the\ntransformation matrices are sparse and use threshold estimators for the\n(auto)covariance matrices. We also propose a block-wisely thresholding method\nto separate the columns (or rows) of the transformed matrix-valued data. The\nasymptotic properties are established for both fixed and diverging $\\max(p,q)$.\nUnlike principal component analysis (PCA) for independent data, we cannot\nguarantee that the required linear transformation exists. When it does not, the\nproposed method provides an approximate segmentation, which may be useful for\nforecasting. The proposed method is illustrated with both simulated and real\ndata examples. We also propose a sequential transformation algorithm to segment\nhigher-order tensor-valued time series.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 15:32:23 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Gao", "Zhaoxing", ""]]}, {"id": "2002.03467", "submitter": "Takuma Usuzaki", "authors": "T. Usuzaki, M. Shimoyama S. Chiba, S. Hotta", "title": "Random family method: Confirming inter-generational relations by\n  restricted re-sampling", "comments": "2 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomness is one of the important key concepts of statistics. In\nepidemiology or medical science, we investigate our hypotheses and interpret\nresults through this statistical randomness. We hypothesized by imposing some\nconditions to this randomness, interpretation of our result may be changed. In\nthis article, we introduced the restricted re-sampling method to confirm\ninter-generational relations and presented an example.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 23:12:36 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Usuzaki", "T.", ""], ["Chiba", "M. Shimoyama S.", ""], ["Hotta", "S.", ""]]}, {"id": "2002.03490", "submitter": "Luai Al-Labadi Dr.", "authors": "Luai Al-Labadi, Forough Fazeli Asl, and Zahra Saberi", "title": "A Test for Independence Via Bayesian Nonparametric Estimation of Mutual\n  Information", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual information is a well-known tool to measure the mutual dependence\nbetween variables. In this paper, a Bayesian nonparametric estimation of mutual\ninformation is established by means of the Dirichlet process and the\n$k$-nearest neighbor distance. As a direct outcome of the estimation, an\neasy-to-implement test of independence is introduced through the relative\nbelief ratio. Several theoretical properties of the approach are presented. The\nprocedure is investigated through various examples where the results are\ncompared to its frequentist counterpart and demonstrate a good performance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 01:29:08 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Al-Labadi", "Luai", ""], ["Asl", "Forough Fazeli", ""], ["Saberi", "Zahra", ""]]}, {"id": "2002.03600", "submitter": "Luca Scrucca", "authors": "Luca Scrucca", "title": "A fast and efficient Modal EM algorithm for Gaussian mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the modal approach to clustering, clusters are defined as the local maxima\nof the underlying probability density function, where the latter can be\nestimated either non-parametrically or using finite mixture models. Thus,\nclusters are closely related to certain regions around the density modes, and\nevery cluster corresponds to a bump of the density. The Modal EM algorithm is\nan iterative procedure that can identify the local maxima of any density\nfunction. In this contribution, we propose a fast and efficient Modal EM\nalgorithm to be used when the density function is estimated through a finite\nmixture of Gaussian distributions with parsimonious component-covariance\nstructures. After describing the procedure, we apply the proposed Modal EM\nalgorithm on both simulated and real data examples, showing its high\nflexibility in several contexts.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 08:34:16 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 14:31:30 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Scrucca", "Luca", ""]]}, {"id": "2002.03674", "submitter": "Scott Ward", "authors": "Scott Ward, Edward A. K. Cohen, Niall Adams", "title": "Testing for complete spatial randomness on three dimensional bounded\n  convex shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is currently a gap in theory for point patterns that lie on the surface\nof objects, with researchers focusing on patterns that lie in a Euclidean\nspace, typically planar and spatial data. Methodology for planar and spatial\ndata thus relies on Euclidean geometry and is therefore inappropriate for\nanalysis of point patterns observed in non-Euclidean spaces. Recently, there\nhas been extensions to the analysis of point patterns on a sphere, however,\nmany other shapes are left unexplored. This is in part due to the challenge of\ndefining the notion of stationarity for a point process existing on such a\nspace due to the lack of rotational and translational isometries. Here, we\nconstruct functional summary statistics for Poisson processes defined on convex\nshapes in three dimensions. Using the Mapping Theorem, a Poisson process can be\ntransformed from any convex shape to a Poisson process on the unit sphere which\nhas rotational symmetries that allow for functional summary statistics to be\nconstructed. We present the first and second order properties of such summary\nstatistics and demonstrate how they can be used to test whether an observed\npattern exhibits complete spatial randomness or spatial preference on the\noriginal convex space. A study of the Type I and II errors of our test\nstatistics are explored through simulations on ellipsoids of varying\ndimensions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 11:57:38 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ward", "Scott", ""], ["Cohen", "Edward A. K.", ""], ["Adams", "Niall", ""]]}, {"id": "2002.03678", "submitter": "Ruth Keogh", "authors": "Ruth H. Keogh, Shaun R. Seaman, Jon Michael Gran, Stijn Vansteelandt", "title": "Simulating longitudinal data from marginal structural models using the\n  additive hazard model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Observational longitudinal data on treatments and covariates are increasingly\nused to investigate treatment effects, but are often subject to time-dependent\nconfounding. Marginal structural models (MSMs), estimated using inverse\nprobability of treatment weighting or the g-formula, are popular for handling\nthis problem. With increasing development of advanced causal inference methods,\nit is important to be able to assess their performance in different scenarios\nto guide their application. Simulation studies are a key tool for this, but\ntheir use to evaluate causal inference methods has been limited. This paper\nfocuses on the use of simulations for evaluations involving MSMs in studies\nwith a time-to-event outcome. In a simulation, it is important to be able to\ngenerate the data in such a way that the correct form of any models to be\nfitted to those data is known. However, this is not straightforward in the\nlongitudinal setting because it is natural for data to be generated in a\nsequential conditional manner, whereas MSMs involve fitting marginal rather\nthan conditional hazard models. We provide general results that enable the form\nof the correctly-specified MSM to be derived based on a conditional data\ngenerating procedure, and show how the results can be applied when the\nconditional hazard model is an Aalen additive hazard or Cox model. Using\nconditional additive hazard models is advantageous because they imply additive\nMSMs that can be fitted using standard software. We describe and illustrate a\nsimulation algorithm. Our results will help researchers to effectively evaluate\ncausal inference methods via simulation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 12:18:17 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Keogh", "Ruth H.", ""], ["Seaman", "Shaun R.", ""], ["Gran", "Jon Michael", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2002.03817", "submitter": "Xianzheng Huang", "authors": "Xianzheng Huang and Hongmei Zhang", "title": "Corrected score methods for estimating Bayesian networks with\n  error-prone nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by inferring cellular signaling networks using noisy flow cytometry\ndata, we develop procedures to draw inference for Bayesian networks based on\nerror-prone data. Two methods for inferring causal relationships between nodes\nin a network are proposed based on penalized estimation methods that account\nfor measurement error and encourage sparsity. We discuss consistency of the\nproposed network estimators and develop an approach for selecting the tuning\nparameter in the penalized estimation methods. Empirical studies are carried\nout to compare the proposed methods and a naive method that ignores measurement\nerror with applications to synthetic data and to single cell flow cytometry\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 14:45:31 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Huang", "Xianzheng", ""], ["Zhang", "Hongmei", ""]]}, {"id": "2002.03828", "submitter": "Jun Li", "authors": "Jun Li", "title": "A Robust Stochastic Method of Estimating the Transmission Potential of\n  2019-nCoV", "comments": "Short paper, 4 page text, total 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent outbreak of a novel coronavirus (2019-nCoV) has quickly evolved\ninto a global health crisis. The transmission potential of 2019-nCoV has been\nmodelled and studied in several recent research works. The key factors such as\nthe basic reproductive number, $R_{0}$, of the virus have been identified by\nfitting contagious disease spreading models to aggregated data. The data\ninclude the reported cases both within China and in closely connected cities\nover the world. In this paper, we study the transmission potential of 2019-nCoV\nfrom the perspective of the robustness of the statistical estimation, in light\nof varying data quality and timeliness in the initial stage of the outbreak.\nSample consensus algorithm has been adopted to improve model fitting when\noutliers are present. The robust estimation enables us to identify two clusters\nof transmission models, both are of substantial concern, one with\n$R_0:8\\sim14$, comparable to that of measles and the other dictates a large\ninitial infected group.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 16:01:28 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Li", "Jun", ""]]}, {"id": "2002.03840", "submitter": "Chiranjit Maji Mr.", "authors": "Chiranjit Maji, Pratyay Sengupta, Anandi Batabyal, and Hirok Chaudhuri", "title": "Nonlinear and statistical analysis of ECG signals from Arrhythmia\n  affected cardiac system through the EMD process", "comments": "The manuscript of 24 pages including the reference list. It also\n  contains 5 tables along with 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human heart is a complex system exhibiting stochastic nature, as\nreflected in electrocardiogram (ECG) signals. ECG signal is a weak,\nnon-stationary, and nonlinear signal, which indicates the health of a heart in\nterms of temporal variations of electromagnetic pulses from the heart. Abnormal\nfluctuations in ECG signal invokes the possibility of various cardiovascular\ndisorders, which is diagnosed through intuitive analysis of the ECG reports by\nthe medical practitioners. This could be made fast, accurate, and simple by\nimposing advanced nonlinear tools on the recorded ECG signals. In this paper, a\nwell-known nonlinear technique, i.e., Empirical Mode Decomposition (EMD) method\nis adopted to extract the hidden information in the recorded ECG signal. Here,\nwe try to explore the human heart as a dynamic model and perform EMD on ECG\nreports distinguishing arrhythmia from normal data obtained from the widely\nused MIT-BIH database. EMD essentially involves the decomposition of the signal\ninto a finite number of Intrinsic Mode Functions (IMFs), keeping its original\nproperties unaltered. For analysis, we use the powerful Savitzky-Golay (SG)\nfilter for removing non-stationary noises from the ECG signals. The popular\nnonlinear parameter Hurst Exponent (H) is estimated for every IMF by R/S\ntechnique. We identified a distinct margin of the H of 1st IMFs in between the\nnormal and the arrhythmia affected patients. Our model confirms with 94.92%\ncertainty the chances of occurrence of arrhythmia disease in patients by\ndiagnosing ECG signals without performing other expensive and time-consuming\ntechniques such as Holter test, echocardiogram, and stress test.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 15:05:28 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Maji", "Chiranjit", ""], ["Sengupta", "Pratyay", ""], ["Batabyal", "Anandi", ""], ["Chaudhuri", "Hirok", ""]]}, {"id": "2002.03890", "submitter": "Lauri Jetsu", "authors": "Lauri Jetsu", "title": "Discrete Chi-square Method for Detecting Many Signals", "comments": "20 pages, 14 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unambiguous detection of signals superimposed on unknown trends is difficult\nfor unevenly spaced data. Here, we formulate the Discrete Chi-square Method\n(DCM) that can determine the best model for many signals superimposed on\narbitrary polynomial trends. DCM minimizes the Chi-square for the data in the\nmulti-dimensional tested frequency space. The required number of tested\nfrequency combinations remains manageable, because the method test statistic is\nsymmetric in this tested frequency space. With our known tested constant\nfrequency grid values, the non-linear DCM model becomes linear, and all results\nbecome unambiguous. We test DCM with simulated data containing different\nmixtures of signals and trends. DCM gives unambiguous results, if the signal\nfrequencies are not too close to each other, and none of the signals is too\nweak. It relies on brute computational force, because all possible free\nparameter combinations for all reasonable linear models are tested. DCM works\nlike winning a lottery by buying all lottery tickets. Anyone can reproduce all\nour results with the DCM computer code. All files, variables and other program\ncode related items are printed in magenta colour. Our Appendix gives detailed\ninstructions for using dcm.py. We also present one preliminary real use case,\nwhere DCM is applied to the observed (O) minus the computed (C) eclipse epochs\nof a binary star, XZ And. This DCM analysis reveals evidence for the possible\npresence of a third and a fourth body in this system. One recent study of a\nvery large sample of binary stars indicated that the probability for detecting\na fourth body from the O-C data of eclipsing binaries is only about 0.00005.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 15:59:56 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 09:20:08 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Jetsu", "Lauri", ""]]}, {"id": "2002.03899", "submitter": "Adrien Saumard", "authors": "Camille Brunet-Saumard, Edouard Genetay, Adrien Saumard", "title": "K-bMOM: a robust Lloyd-type clustering algorithm based on bootstrap\n  Median-of-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new clustering algorithm that is robust to the presence of\noutliers in the dataset. We perform Lloyd-type iterations with robust estimates\nof the centroids. More precisely, we build on the idea of median-of-means\nstatistics to estimate the centroids, but allow for replacement while\nconstructing the blocks. We call this methodology the bootstrap median-of-means\n(bMOM) and prove that if enough blocks are generated through the bootstrap\nsampling, then it has a better breakdown point for mean estimation than the\nclassical median-of-means (MOM), where the blocks form a partition of the\ndataset. From a clustering perspective, bMOM enables to take many blocks of a\ndesired size, thus avoiding possible disappearance of clusters in some blocks,\na pitfall that can occur for the partition-based generation of blocks of the\nclassical median-of-means. Experiments on simulated datasets show that the\nproposed approach, called K-bMOM, performs better than existing robust K-means\nbased methods. Guidelines are provided for tuning the hyper-parameters K-bMOM\nin practice. It is also recommended to the practitionner to use such a robust\napproach to initialize their clustering algorithm. Finally, considering a\nsimplified and theoretical version of our estimator, we prove its robustness to\nadversarial contamination by deriving robust rates of convergence for the\nK-means distorsion. To our knowledge, it is the first result of this kind for\nthe K-means distorsion.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 16:08:08 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 16:56:21 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Brunet-Saumard", "Camille", ""], ["Genetay", "Edouard", ""], ["Saumard", "Adrien", ""]]}, {"id": "2002.04081", "submitter": "Andrea Arf\\'e", "authors": "Andrea Arf\\`e, Pietro Muliere", "title": "A general Bayesian bootstrap for censored data based on the beta-Stacy\n  process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel procedure to perform Bayesian non-parametric inference\nwith right-censored data, the \\emph{beta-Stacy bootstrap}. This approximates\nthe posterior law of summaries of the survival distribution (e.g. the mean\nsurvival time), which is often difficult in the non-parametric case. More\nprecisely, our procedure approximates the joint posterior law of functionals of\nthe beta-Stacy process, a non-parametric process prior widely used in survival\nanalysis. It also represents the missing link that unifies other common\nBayesian bootstraps for complete or censored data based on non-parametric\npriors. It is defined by an exact sampling algorithm that does not require\ntuning of Markov Chain Monte Carlo steps. We illustrate the beta-Stacy\nbootstrap by analyzing survival data from a real clinical trial.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 20:41:35 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Arf\u00e8", "Andrea", ""], ["Muliere", "Pietro", ""]]}, {"id": "2002.04101", "submitter": "Shanglin Lu", "authors": "Lajos Horv\\'ath (1), Zhenya Liu (2 and 3) and Shanglin Lu (2) ((1)\n  Department of Mathematics, University of Utah, (2) School of Finance, Renmin\n  University of China, (3) CERGAM, Aix--Marseille University)", "title": "Sequential Monitoring of Changes in Housing Prices", "comments": "47 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sequential monitoring scheme to find structural breaks in real\nestate markets. The changes in the real estate prices are modeled by a\ncombination of linear and autoregressive terms. The monitoring scheme is based\non a detector and a suitably chosen boundary function. If the detector crosses\nthe boundary function, a structural break is detected. We provide the\nasymptotics for the procedure under the stability null hypothesis and the\nstopping time under the change point alternative. Monte Carlo simulation is\nused to show the size and the power of our method under several conditions. We\nstudy the real estate markets in Boston, Los Angeles and at the national U.S.\nlevel. We find structural breaks in the markets, and we segment the data into\nstationary segments. It is observed that the autoregressive parameter is\nincreasing but stays below 1.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 21:49:19 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Horv\u00e1th", "Lajos", "", "2 and 3"], ["Liu", "Zhenya", "", "2 and 3"], ["Lu", "Shanglin", ""]]}, {"id": "2002.04115", "submitter": "Runmin Wang", "authors": "Runmin Wang and Xiaofeng Shao", "title": "Dating the Break in High-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with estimation and inference for the location of a\nchange point in the mean of independent high-dimensional data. Our change point\nlocation estimator maximizes a new U-statistic based objective function, and\nits convergence rate and asymptotic distribution after suitable centering and\nnormalization are obtained under mild assumptions. Our estimator turns out to\nhave better efficiency as compared to the least squares based counterpart in\nthe literature. Based on the asymptotic theory, we construct a confidence\ninterval by plugging in consistent estimates of several quantities in the\nnormalization. We also provide a bootstrap-based confidence interval and state\nits asymptotic validity under suitable conditions. Through simulation studies,\nwe demonstrate favorable finite sample performance of the new change point\nlocation estimator as compared to its least squares based counterpart, and our\nbootstrap-based confidence intervals, as compared to several existing\ncompetitors. The asymptotic theory based on high-dimensional U-statistic is\nsubstantially different from those developed in the literature and is of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 22:19:29 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Wang", "Runmin", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "2002.04211", "submitter": "Tiejun Tong", "authors": "Enxuan Lin and Tiejun Tong and Yong Chen and Yuedong Wang", "title": "Fixed-effects model: the most convincing model for meta-analysis with\n  few studies", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to Davey et al. (2011) with a total of 22,453 meta-analyses from\nthe January 2008 Issue of the Cochrane Database of Systematic Reviews, the\nmedian number of studies included in each meta-analysis is only three. In other\nwords, about a half or more of meta-analyses conducted in the literature\ninclude only two or three studies. While the common-effect model (also referred\nto as the fixed-effect model) may lead to misleading results when the\nheterogeneity among studies is large, the conclusions based on the\nrandom-effects model may also be unreliable when the number of studies is\nsmall. Alternatively, the fixed-effects model avoids the restrictive assumption\nin the common-effect model and the need to estimate the between-study variance\nin the random-effects model. We note, however, that the fixed-effects model is\nunder appreciated and rarely used in practice until recently. In this paper, we\ncompare all three models and demonstrate the usefulness of the fixed-effects\nmodel when the number of studies is small. In addition, we propose a new\nestimator for the unweighted average effect in the fixed-effects model.\nSimulations and real examples are also used to illustrate the benefits of the\nfixed-effects model and the new estimator.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 05:25:40 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Lin", "Enxuan", ""], ["Tong", "Tiejun", ""], ["Chen", "Yong", ""], ["Wang", "Yuedong", ""]]}, {"id": "2002.04328", "submitter": "Giuseppe Brandi", "authors": "Giuseppe Brandi and T. Di Matteo", "title": "Predicting Multidimensional Data via Tensor Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of multidimensional data is becoming a more and more relevant\ntopic in statistical and machine learning research. Given their complexity,\nsuch data objects are usually reshaped into matrices or vectors and then\nanalysed. However, this methodology presents several drawbacks. First of all,\nit destroys the intrinsic interconnections among datapoints in the\nmultidimensional space and, secondly, the number of parameters to be estimated\nin a model increases exponentially. We develop a model that overcomes such\ndrawbacks. In particular, in this paper, we propose a parsimonious tensor\nregression model that retains the intrinsic multidimensional structure of the\ndataset. Tucker structure is employed to achieve parsimony and a shrinkage\npenalization is introduced to deal with over-fitting and collinearity. To\nestimate the model parameters, an Alternating Least Squares algorithm is\ndeveloped. In order to validate the model performance and robustness, a\nsimulation exercise is produced. Moreover, we perform an empirical analysis\nthat highlight the forecasting power of the model with respect to benchmark\nmodels. This is achieved by implementing an autoregressive specification on the\nFoursquares spatio-temporal dataset together with a macroeconomic panel\ndataset. Overall, the proposed model is able to outperform benchmark models\npresent in the forecasting literature.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 11:57:07 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:37:28 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 18:13:53 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Brandi", "Giuseppe", ""], ["Di Matteo", "T.", ""]]}, {"id": "2002.04362", "submitter": "Emma Simpson", "authors": "Emma S. Simpson and Jennifer L. Wadsworth", "title": "Conditional Modelling of Spatio-Temporal Extremes for Red Sea Surface\n  Temperatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent extreme value theory literature has seen significant emphasis on the\nmodelling of spatial extremes, with comparatively little consideration of\nspatio-temporal extensions. This neglects an important feature of extreme\nevents: their evolution over time. Many existing models for the spatial case\nare limited by the number of locations they can handle; this impedes extension\nto space-time settings, where models for higher dimensions are required.\nMoreover, the spatio-temporal models that do exist are restrictive in terms of\nthe range of extremal dependence types they can capture. Recently, conditional\napproaches for studying multivariate and spatial extremes have been proposed,\nwhich enjoy benefits in terms of computational efficiency and an ability to\ncapture both asymptotic dependence and asymptotic independence. We extend this\nclass of models to a spatio-temporal setting, conditioning on the occurrence of\nan extreme value at a single space-time location. We adopt a composite\nlikelihood approach for inference, which combines information from full\nlikelihoods across multiple space-time conditioning locations. We apply our\nmodel to Red Sea surface temperatures, show that it fits well using a range of\ndiagnostic plots, and demonstrate how it can be used to assess the risk of\ncoral bleaching attributed to high water temperatures over consecutive days.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 13:10:28 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 10:15:30 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Simpson", "Emma S.", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "2002.04457", "submitter": "Dong Xia", "authors": "Bing-Yi Jing and Ting Li and Zhongyuan Lyu and Dong Xia", "title": "Community Detection on Mixture Multi-layer Networks via Regularized\n  Tensor Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IT cs.LG math.IT math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of community detection in multi-layer networks, where\npairs of nodes can be related in multiple modalities. We introduce a general\nframework, i.e., mixture multi-layer stochastic block model (MMSBM), which\nincludes many earlier models as special cases. We propose a tensor-based\nalgorithm (TWIST) to reveal both global/local memberships of nodes, and\nmemberships of layers. We show that the TWIST procedure can accurately detect\nthe communities with small misclassification error as the number of nodes\nand/or the number of layers increases. Numerical studies confirm our\ntheoretical findings. To our best knowledge, this is the first systematic study\non the mixture multi-layer networks using tensor decomposition. The method is\napplied to two real datasets: worldwide trading networks and malaria parasite\ngenes networks, yielding new and interesting findings.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 06:19:50 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Jing", "Bing-Yi", ""], ["Li", "Ting", ""], ["Lyu", "Zhongyuan", ""], ["Xia", "Dong", ""]]}, {"id": "2002.04470", "submitter": "Giulia Carallo", "authors": "Giulia Carallo, Roberto Casarin, Christian P. Robert", "title": "Generalized Poisson Difference Autoregressive Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new stochastic process with values in the set Z of\nintegers with sign. The increments of process are Poisson differences and the\ndynamics has an autoregressive structure. We study the properties of the\nprocess and exploit the thinning representation to derive stationarity\nconditions and the stationary distribution of the process. We provide a\nBayesian inference method and an efficient posterior approximation procedure\nbased on Monte Carlo. Numerical illustrations on both simulated and real data\nshow the effectiveness of the proposed inference.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 15:21:24 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Carallo", "Giulia", ""], ["Casarin", "Roberto", ""], ["Robert", "Christian P.", ""]]}, {"id": "2002.04578", "submitter": "Johannes Martini", "authors": "J W R Martini", "title": "A sufficient condition for penalized polynomial regression to be\n  invariant to translations of the predictor variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas translating the coding of predictor variables does not change the fit\nof a polynomial least squares regression, penalized polynomial regressions are\npotentially affected. A result on which terms can be penalized to maintain the\ninvariance to translations of the coding has earlier been published. A\ngeneralization of a corresponding proposition, which requires a more precise\nmathematical framework, is presented in this short note.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 18:08:37 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Martini", "J W R", ""]]}, {"id": "2002.04592", "submitter": "Min Zhou", "authors": "Yang Feng, Min Zhou, Xin Tong", "title": "Imbalanced classification: a paradigm-based review", "comments": "34 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common issue for classification in scientific research and industry is the\nexistence of imbalanced classes. When sample sizes of different classes are\nimbalanced in training data, naively implementing a classification method often\nleads to unsatisfactory prediction results on test data. Multiple resampling\ntechniques have been proposed to address the class imbalance issues. Yet, there\nis no general guidance on when to use each technique. In this article, we\nprovide a paradigm-based review of the common resampling techniques for binary\nclassification under imbalanced class sizes. The paradigms we consider include\nthe classical paradigm that minimizes the overall classification error, the\ncost-sensitive learning paradigm that minimizes a cost-adjusted weighted type I\nand type II errors, and the Neyman-Pearson paradigm that minimizes the type II\nerror subject to a type I error constraint. Under each paradigm, we investigate\nthe combination of the resampling techniques and a few state-of-the-art\nclassification methods. For each pair of resampling techniques and\nclassification methods, we use simulation studies and a real data set on credit\ncard fraud to study the performance under different evaluation metrics. From\nthese extensive numerical experiments, we demonstrate under each classification\nparadigm, the complex dynamics among resampling techniques, base classification\nmethods, evaluation metrics, and imbalance ratios. We also summarize a few\ntakeaway messages regarding the choices of resampling techniques and base\nclassification methods, which could be helpful for practitioners.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 18:34:48 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 02:08:35 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Feng", "Yang", ""], ["Zhou", "Min", ""], ["Tong", "Xin", ""]]}, {"id": "2002.04691", "submitter": "Michail Tsagris", "authors": "M. Tsagris, A. Alenazi, and S. Fafalios", "title": "Computationally efficient univariate filtering for massive data", "comments": "The paper has been submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast availability of large scale, massive and big data has increased the\ncomputational cost of data analysis. One such case is the computational cost of\nthe univariate filtering which typically involves fitting many univariate\nregression models and is essential for numerous variable selection algorithms\nto reduce the number of predictor variables. The paper manifests how to\ndramatically reduce that computational cost by employing the score test or the\nsimple Pearson correlation (or the t-test for binary responses). Extensive\nMonte Carlo simulation studies will demonstrate their advantages and\ndisadvantages compared to the likelihood ratio test and examples with real data\nwill illustrate the performance of the score test and the log-likelihood ratio\ntest under realistic scenarios. Depending on the regression model used, the\nscore test is 30 - 60,000 times faster than the log-likelihood ratio test and\nproduces nearly the same results. Hence this paper strongly recommends to\nsubstitute the log-likelihood ratio test with the score test when coping with\nlarge scale data, massive data, big data, or even with data whose sample size\nis in the order of a few tens of thousands or higher.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 21:20:24 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Tsagris", "M.", ""], ["Alenazi", "A.", ""], ["Fafalios", "S.", ""]]}, {"id": "2002.04697", "submitter": "Filippo Pellegrino", "authors": "Filippo Pellegrino", "title": "Selecting time-series hyperparameters with the artificial jackknife", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a generalisation of the delete-$d$ jackknife to solve\nhyperparameter selection problems for time series. This novel technique is\ncompatible with dependent data since it substitutes the jackknife removal step\nwith a fictitious deletion, wherein observed datapoints are replaced with\nartificial missing values. In order to emphasise this point, I called this\nmethodology artificial delete-$d$ jackknife. As an illustration, it is used to\nregulate vector autoregressions with an elastic-net penalty on the\ncoefficients. A software implementation, ElasticNetVAR.jl, is available on\nGitHub.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 21:38:51 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 19:37:17 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Pellegrino", "Filippo", ""]]}, {"id": "2002.04706", "submitter": "Arman Oganisian", "authors": "Arman Oganisian, Nandita Mitra, Jason Roy", "title": "Bayesian Nonparametric Cost-Effectiveness Analyses: Causal Estimation\n  and Adaptive Subgroup Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cost-effectiveness analyses (CEAs) are at the center of health economic\ndecision making. While these analyses help policy analysts and economists\ndetermine coverage, inform policy, and guide resource allocation, they are\nstatistically challenging for several reasons. Cost and effectiveness are\ncorrelated and follow complex joint distributions which are difficult to\ncapture parametrically. Effectiveness (often measured as increased survival\ntime) and accumulated cost tends to be right-censored in many applications.\nMoreover, CEAs are often conducted using observational data with non-random\ntreatment assignment. Policy-relevant causal estimation therefore requires\nrobust confounding control. Finally, current CEA methods do not address\ncost-effectiveness heterogeneity in a principled way - often presenting\npopulation-averaged estimates even though significant effect heterogeneity may\nexist. Motivated by these challenges, we develop a nonparametric Bayesian model\nfor joint cost-survival distributions in the presence of censoring. Our\napproach utilizes a joint Enriched Dirichlet Process prior on the covariate\neffects of cost and survival time, while using a Gamma Process prior on the\nbaseline survival time hazard. Causal CEA estimands, with policy-relevant\ninterpretations, are identified and estimated via a Bayesian nonparametric\ng-computation procedure. Finally, we outline how the induced clustering of the\nEnriched Dirichlet Process can be used to adaptively detect presence of\nsubgroups with different cost-effectiveness profiles. We outline an MCMC\nprocedure for full posterior inference and evaluate frequentist properties via\nsimulations. We use our model to assess the cost-efficacy of chemotherapy\nversus radiation adjuvant therapy for treating endometrial cancer in the\nSEER-Medicare database.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 21:51:58 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 22:28:22 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Oganisian", "Arman", ""], ["Mitra", "Nandita", ""], ["Roy", "Jason", ""]]}, {"id": "2002.04721", "submitter": "Takuma Usuzaki", "authors": "Takuma Usuzaki, Minoru Shimoyama, Shuji Chiba, Naoko Mori and Shunji\n  Mugikura", "title": "A Method Expanding 2 by 2 Contingency Table by Obtaining Tendencies of\n  Boolean Operators: Boolean Monte Carlo Method", "comments": "9 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy of medical test and diagnosis are often discussed by 2 by 2\ncontingency tables. However, it is difficult to apply a 2 by 2 contingency\ntable to multivariate cases because the number of possible categories increases\nexponentially. The aims of this study is introducing a method by which we can\nobtain trends of boolean operators and construct a 2 by 2 contingency tables to\nmultivariate cases. In this method, we randomly assigned boolean operators\nbetween binary variables and focused on frequencies of boolean operators which\ncould explain outcomes correctly. We determined trends of boolean operators by\nchi-square test. As an application, we performed this method for a dataset\nwhich included patient's age, body mass index (BMI), mean arterial pressure\n(MAP) and the quantitative measure of diabetes progression ($Y$). We set cutoff\nto each variable and considered these as binary variables. Interactions of age,\nBMI and MAP were determined as age $and$ BMI $and$ MAP ($p<0.0001$ for both\noperators) in estimating $Y$ and sensitivity, specificity, positive and\nnegative predicting value were 0.50, 0.85. 0.59, 0.80 respectively. We may be\nable to detemine trends of boolean operators and construct a 2 by 2 contingency\ntable in multivariate situation by this method.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 22:47:56 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 01:11:15 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Usuzaki", "Takuma", ""], ["Shimoyama", "Minoru", ""], ["Chiba", "Shuji", ""], ["Mori", "Naoko", ""], ["Mugikura", "Shunji", ""]]}, {"id": "2002.04775", "submitter": "Chuan Hong", "authors": "Chuan Hong, Jing Zhang, Yang Li, Elena Elia, Richard Riley, and Yong\n  Chen", "title": "A regression-based method for detecting publication bias in multivariate\n  meta-analysis", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publication bias occurs when the publication of research results depends not\nonly on the quality of the research but also on its nature and direction. The\nconsequence is that published studies may not be truly representative of all\nvalid studies undertaken, and this bias may threaten the validity of systematic\nreviews and meta-analyses - on which evidence-based medicine increasingly\nrelies. Multivariate meta-analysis has recently received increasing attention\nfor its ability reducing potential bias and improving statistical efficiency by\nborrowing information across outcomes. However, detecting and accounting for\npublication bias are more challenging in multivariate meta-analysis setting\nbecause some studies may be completely unpublished whereas some studies may\nselectively report part of multiple outcomes. In this paper, we propose a score\ntest for jointly testing publication bias for multiple outcomes, which is novel\nto the multivariate setting. The proposed test is a natural multivariate\nextension of the univariate Egger's test, and can handle the above mentioned\nscenarios simultaneously, It accounts for correlations among multivariate\noutcomes, while allowing different types of outcomes, and can borrow\ninformation across outcomes. The proposed test is shown to be more powerful\nthan the Egger's test, Begg's test and Trim and Fill method through simulation\nstudies. Two data analyses are given to illustrate the performance of the\nproposed test in practice.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 03:22:26 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Hong", "Chuan", ""], ["Zhang", "Jing", ""], ["Li", "Yang", ""], ["Elia", "Elena", ""], ["Riley", "Richard", ""], ["Chen", "Yong", ""]]}, {"id": "2002.04996", "submitter": "Esa Ollila", "authors": "Esa Ollila, Daniel P. Palomar and Frederic Pascal", "title": "M-estimators of scatter with eigenvalue shrinkage", "comments": "To appear in Proc. IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP 2020), May 4 - 8, Barcelona, Spain, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular regularized (shrinkage) covariance estimator is the shrinkage\nsample covariance matrix (SCM) which shares the same set of eigenvectors as the\nSCM but shrinks its eigenvalues toward its grand mean. In this paper, a more\ngeneral approach is considered in which the SCM is replaced by an M-estimator\nof scatter matrix and a fully automatic data adaptive method to compute the\noptimal shrinkage parameter with minimum mean squared error is proposed. Our\napproach permits the use of any weight function such as Gaussian, Huber's, or\n$t$ weight functions, all of which are commonly used in M-estimation framework.\nOur simulation examples illustrate that shrinkage M-estimators based on the\nproposed optimal tuning combined with robust weight function do not loose in\nperformance to shrinkage SCM estimator when the data is Gaussian, but provide\nsignificantly improved performance when the data is sampled from a heavy-tailed\ndistribution.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 13:47:58 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Ollila", "Esa", ""], ["Palomar", "Daniel P.", ""], ["Pascal", "Frederic", ""]]}, {"id": "2002.05048", "submitter": "Jakub Pecanka", "authors": "Marianne A. Jonker and Jakub Pecanka", "title": "A powerful MAF-neutral allele-based test for case-control association\n  studies", "comments": "24 pages, 5 figures, includes Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a case-control study aimed at locating autosomal disease variants for a\ndisease of interest, association between markers and the disease status is\noften tested by comparing the marker minor allele frequencies (MAFs) between\ncases and controls. For most common allele-based tests the statistical power is\nhighly dependent on the actual values of these MAFs, where associated markers\nwith low MAFs have less power to be detected compared to associated markers\nwith high MAFs. Therefore, the popular strategy of selecting markers for\nfollow-up studies based primarily on their p-values is likely to preferentially\nselect markers with high MAFs. We propose a new test which does not favor\nmarkers with high MAFs and improves the power for markers with low to moderate\nMAFs without sacrificing performance for markers with high MAFs and is\ntherefore superior to most existing tests in this regard. An explicit formula\nfor the asymptotic power function of the proposed test is derived\ntheoretically, which allows for fast and easy computation of the corresponding\np-values. The performance of the proposed test is compared with several\nexisting tests both in the asymptotic and the finite sample size settings.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 15:31:32 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Jonker", "Marianne A.", ""], ["Pecanka", "Jakub", ""]]}, {"id": "2002.05137", "submitter": "Michail Tsagris", "authors": "Michail Tsagris, Abdulaziz Alenazi and Connie Stewart", "title": "Non-parametric regression models for compositional data", "comments": "This is a preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional data arise in many real-life applications and versatile methods\nfor properly analyzing this type of data in the regression context are needed.\nThis paper, through use of the $\\alpha$-transformation, extends the classical\n$k$-$NN$ regression to what is termed $\\alpha$-$k$-$NN$ regression, yielding a\nhighly flexible non-parametric regression model for compositional data. The\n$\\alpha$-$k$-$NN$ is further extended to the $\\alpha$-kernel regression by\nadopting the Nadaray-Watson estimator. Unlike many of the recommended\nregression models for compositional data, zeros values (which commonly occur in\npractice) are not problematic and they can be incorporated into the proposed\nmodels without modification. Extensive simulation studies and real-life data\nanalyses highlight the advantage of using these non-parametric regressions for\ncomplex relationships between the compositional response data and Euclidean\npredictor variables. Both suggest that $\\alpha$-$k$-$NN$ and $\\alpha$-kernel\nregressions can lead to more accurate predictions compared to current\nregression models which assume a, sometimes restrictive, parametric\nrelationship with the predictor variables. In addition, the $\\alpha$-$k$-$NN$\nregression, in contrast to $\\alpha$-kernel regression, enjoys a high\ncomputational efficiency rendering it highly attractive for use with large\nscale, massive, or big data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 18:29:33 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 22:04:34 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 15:23:01 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Tsagris", "Michail", ""], ["Alenazi", "Abdulaziz", ""], ["Stewart", "Connie", ""]]}, {"id": "2002.05211", "submitter": "Kidus Asfaw", "authors": "Edward L. Ionides, Kidus Asfaw, Joonha Park, Aaron A. King", "title": "Bagged filters for partially observed interacting systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bagging (i.e., bootstrap aggregating) involves combining an ensemble of\nbootstrap estimators. We consider bagging for inference from noisy or\nincomplete measurements on a collection of interacting stochastic dynamic\nsystems. Each system is called a unit, and each unit is associated with a\nspatial location. A motivating example arises in epidemiology, where each unit\nis a city: the majority of transmission occurs within a city, with smaller yet\nepidemiologically important interactions arising from disease transmission\nbetween cities. Monte Carlo filtering methods used for inference on nonlinear\nnon-Gaussian systems can suffer from a curse of dimensionality as the number of\nunits increases. We introduce bagged filter (BF) methodology which combines an\nensemble of Monte Carlo filters, using spatiotemporally localized weights to\nselect successful filters at each unit and time. We obtain conditions under\nwhich likelihood evaluation using a BF algorithm can beat a curse of\ndimensionality, and we demonstrate applicability even when these conditions do\nnot hold. BF can out-perform an ensemble Kalman filter on a coupled population\ndynamics model describing infectious disease transmission. A block particle\nfilter also performs well on this task, though the bagged filter respects\nsmoothness and conservation laws that a block particle filter can violate.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 20:09:18 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 16:13:32 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 22:23:37 GMT"}, {"version": "v4", "created": "Mon, 28 Jun 2021 22:40:12 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ionides", "Edward L.", ""], ["Asfaw", "Kidus", ""], ["Park", "Joonha", ""], ["King", "Aaron A.", ""]]}, {"id": "2002.05226", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Conditional Path Analysis in Singly-Connected Path Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the classical path analysis by showing that, for a singly-connected\npath diagram, the partial covariance of two random variables factorizes over\nthe nodes and edges in the path between the variables. This result allows us to\nshow that Simpson's paradox cannot occur in singly-connected path diagrams.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 20:31:02 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 15:12:41 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 21:08:06 GMT"}, {"version": "v4", "created": "Fri, 2 Oct 2020 10:50:18 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "2002.05249", "submitter": "Yun-Hee Choi", "authors": "Yun-Hee Choi, Hae Jung, Saundra Buys, Mary Daly, Esther John, John\n  Hopper, Irene Andrulis, Mary-Beth Terry, Laurent Briollais", "title": "A Competing Risks Model with Binary Time Varying Covariates for\n  Estimation of Breast Cancer Risks in BRCA1 Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammographic screening and prophylactic surgery such as risk-reducing\nsalpingo oophorectomy (RRSO) can potentially reduce breast cancer risks among\nmutation carriers of BRCA families. The evaluation of these interventions is\nusually complicated by the fact that their effects on breast cancer may change\nover time and by the presence of competing risks. We introduce a correlated\ncompeting risks model to model breast and ovarian cancer risks within BRCA1\nfamilies that accounts for time-varying covariates (TVCs). Different parametric\nforms for these TVCs are proposed for more flexibility and a correlated gamma\nfrailty model is specified to account for the correlated competing events. We\nalso introduced a new ascertainment correction approach that accounts for the\nselection of families through probands affected with either breast or ovarian\ncancer, or unaffected. Our simulation studies demonstrate the good performances\nof our proposed approach in terms of bias and precision of the estimators of\nmodel parameters and cause-specific penetrances over different levels of\nfamilial correlations. We apply our new approach to 498 BRCA1 mutation carrier\nfamilies recruited through the Breast Cancer Family Registry. Our results\ndemonstrate the importance of the functional form of the TVC when assessing the\nrole of RRSO on breast cancer. In particular, under the best fitting TVC model,\nthe overall effect of RRSO on breast cancer risk was statistically significant\nin women with BRCA1.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 21:45:27 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 11:02:18 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Choi", "Yun-Hee", ""], ["Jung", "Hae", ""], ["Buys", "Saundra", ""], ["Daly", "Mary", ""], ["John", "Esther", ""], ["Hopper", "John", ""], ["Andrulis", "Irene", ""], ["Terry", "Mary-Beth", ""], ["Briollais", "Laurent", ""]]}, {"id": "2002.05253", "submitter": "Martin Huber", "authors": "Martin Huber and Luk\\'a\\v{s} Laff\\'ers", "title": "Bounds on direct and indirect effects under treatment/mediator\n  endogeneity and outcome attrition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis aims at disentangling a treatment effect into an\nindirect mechanism operating through an intermediate outcome or mediator, as\nwell as the direct effect of the treatment on the outcome of interest. However,\nthe evaluation of direct and indirect effects is frequently complicated by\nnon-ignorable selection into the treatment and/or mediator, even after\ncontrolling for observables, as well as sample selection/outcome attrition. We\npropose a method for bounding direct and indirect effects in the presence of\nsuch complications using a method that is based on a sequence of linear\nprogramming problems. Considering inverse probability weighting by propensity\nscores, we compute the weights that would yield identification in the absence\nof complications and perturb them by an entropy parameter reflecting a specific\namount of propensity score misspecification to set-identify the effects of\ninterest. We apply our method to data from the National Longitudinal Survey of\nYouth 1979 to derive bounds on the explained and unexplained components of a\ngender wage gap decomposition that is likely prone to non-ignorable mediator\nselection and outcome attrition.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 21:51:00 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 07:28:06 GMT"}, {"version": "v3", "created": "Sun, 3 May 2020 07:05:42 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Huber", "Martin", ""], ["Laff\u00e9rs", "Luk\u00e1\u0161", ""]]}, {"id": "2002.05264", "submitter": "Abdul-Nasah Soale", "authors": "Abdul-Nasah Soale and Yuexiao Dong", "title": "On sufficient dimension reduction via principal asymmetric least squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce principal asymmetric least squares (PALS) as a\nunified framework for linear and nonlinear sufficient dimension reduction.\nClassical methods such as sliced inverse regression (Li, 1991) and principal\nsupport vector machines (Li, Artemiou and Li, 2011) may not perform well in the\npresence of heteroscedasticity, while our proposal addresses this limitation by\nsynthesizing different expectile levels. Through extensive numerical studies,\nwe demonstrate the superior performance of PALS in terms of both computation\ntime and estimation accuracy. For the asymptotic analysis of PALS for linear\nsufficient dimension reduction, we develop new tools to compute the derivative\nof an expectation of a non-Lipschitz function.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 22:23:28 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Soale", "Abdul-Nasah", ""], ["Dong", "Yuexiao", ""]]}, {"id": "2002.05297", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen", "title": "Solution manifold and Its Statistical Applications", "comments": "36 page, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CG stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A solution manifold is the collection of points in a $d$-dimensional space\nsatisfying a system of $s$ equations with $s<d$. Solution manifolds occur in\nseveral statistical problems including hypothesis testing, curved-exponential\nfamilies, constrained mixture models, partial identifications, and\nnonparametric set estimation. We analyze solution manifolds both theoretically\nand algorithmically. In terms of theory, we derive five useful results: the\nsmoothness theorem, the stability theorem (which implies the consistency of a\nplug-in estimator), the convergence of a gradient flow, the local center\nmanifold theorem and the convergence of the gradient descent algorithm. To\nnumerically approximate a solution manifold, we propose a Monte Carlo gradient\ndescent algorithm. In the case of likelihood inference, we design a manifold\nconstraint maximization procedure to find the maximum likelihood estimator on\nthe manifold. We also develop a method to approximate a posterior distribution\ndefined on a solution manifold.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 01:00:30 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Chen", "Yen-Chi", ""]]}, {"id": "2002.05335", "submitter": "Jay Bartroff", "authors": "Maria Allayioti, Jay Bartroff, Larry Goldstein, Susan Luczak, Gary\n  Rosen", "title": "$M$-estimation in a diffusion model with application to biosensor\n  transdermal blood alcohol monitoring", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the goal of well-founded statistical inference on an individual's blood\nalcohol level based on noisy measurements of their skin alcohol content, we\ndevelop $M$-estimation methodology in a general setting. We then apply it to a\ndiffusion equation-based model for the blood/skin alcohol relationship thereby\nestablishing existence, consistency, and asymptotic normality of the nonlinear\nleast squares estimator of the diffusion model's parameter. Simulation studies\nshow agreement between the estimator's performance and its asymptotic\ndistribution, and it is applied to a real skin alcohol data set collected via\nbiosensor.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 04:20:23 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Allayioti", "Maria", ""], ["Bartroff", "Jay", ""], ["Goldstein", "Larry", ""], ["Luczak", "Susan", ""], ["Rosen", "Gary", ""]]}, {"id": "2002.05423", "submitter": "Frederic Lavancier", "authors": "Fr\\'ed\\'eric Lavancier, Ronan Le Gu\\'evel (IRMAR)", "title": "Spatial birth-death-move processes : basic properties and estimation of\n  their intensity functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many spatio-temporal data record the time of birth and death of individuals,\nalong with their spatial trajectories during their lifetime, whether through\ncontinuous-time observations or discrete-time observations. Natural\napplications include epidemiology, individual-based modelling in ecology,\nspatio-temporal dynamics observed in bio-imaging, and computer vision. The aim\nof this article is to estimate in this context the birth and death intensity\nfunctions, that depend in full generality on the current spatial configuration\nof all alive individuals. While the temporal evolution of the population size\nis a simple birth-death process, observing the lifetime and trajectories of all\nindividuals calls for a new paradigm. To formalise this framework, we introduce\nspatial birth-death-move processes, where the birth and death dynamics depends\non the current spatial configuration of the population and where individuals\ncan move during their lifetime according to a continuous Markov process with\npossible interactions.We consider non-parametric kernel estimators of their\nbirth and death intensity functions. The setting is original because each\nobservation in time belongs to a non-vectorial, infinite dimensional space and\nthe dependence between observations is barely tractable. We prove the\nconsistency of the estimators in presence of continuous-time and discrete-time\nobservations, under fairly simple conditions. We moreover discuss how we can\ntake advantage in practice of structural assumptions made on the intensity\nfunctions and we explain how data-driven bandwidth selection can be conducted,\ndespite the unknown (and sometimes undefined) second order moments of the\nestimators. We finally apply our statistical method to the analysis of the\nspatio-temporal dynamics of proteins involved in exocytosis in cells, providing\nnew insights on this complex mechanism.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 10:29:44 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 08:32:51 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Lavancier", "Fr\u00e9d\u00e9ric", "", "IRMAR"], ["Gu\u00e9vel", "Ronan Le", "", "IRMAR"]]}, {"id": "2002.05438", "submitter": "Sylvain Le Corff", "authors": "Alice Martin (TIPIC-SAMOVAR), Marie-Pierre Etienne (IRMAR), Pierre\n  Gloaguen (MIA-Paris), Sylvain Le Corff (TIPIC-SAMOVAR), Jimmy Olsson (KTH)", "title": "Backward importance sampling for online estimation of state space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new Sequential Monte Carlo algorithm to perform online\nestimation in the context of state space models when either the transition\ndensity of the latent state or the conditional likelihood of an observation\ngiven a state is intractable. In this setting, obtaining low variance\nestimators of expectations under the posterior distributions of the unobserved\nstates given the observations is a challenging task. Following recent\ntheoretical results for pseudo-marginal sequential Monte Carlo smoothers, a\npseudo-marginal backward importance sampling step is introduced to estimate\nsuch expectations. This new step allows to reduce very significantly the\ncomputational time of the existing numerical solutions based on an\nacceptance-rejection procedure for similar performance, and to broaden the\nclass of eligible models for such methods. For instance, in the context of\nmultivariate stochastic differential equations, the proposed algorithm makes\nuse of unbiased estimates of the unknown transition densities under much weaker\nassumptions than standard alternatives. The performance of this estimator is\nassessed for high-dimensional discrete-time latent data models, for recursive\nmaximum likelihood estimation in the context of partially observed diffusion\nprocess, and in the case of a bidimensional partially observed stochastic\nLotka-Volterra model.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 10:46:02 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 12:20:20 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Martin", "Alice", "", "TIPIC-SAMOVAR"], ["Etienne", "Marie-Pierre", "", "IRMAR"], ["Gloaguen", "Pierre", "", "MIA-Paris"], ["Corff", "Sylvain Le", "", "TIPIC-SAMOVAR"], ["Olsson", "Jimmy", "", "KTH"]]}, {"id": "2002.05489", "submitter": "Andrea Meil\\'an-Vila", "authors": "Andrea Meil\\'an-Vila, Rub\\'en Fern\\'andez-Casal, Rosa M. Crujeiras\n  Mario Francisco-Fern\\'andez", "title": "A computational validation for nonparametric assessment of spatial\n  trends", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of continuously spatially varying processes usually considers\ntwo sources of variation, namely, the large-scale variation collected by the\ntrend of the process, and the small-scale variation. Parametric trend models on\nlatitude and longitude are easy to fit and to interpret. However, the use of\nsimple parametric models for characterizing spatially varying processes may\nlead to misspecification problems if the model is not appropriate. Recently,\nMeil\\'an-Vila et al. (2019) proposed a goodness-of-fit test based on an\nL2-distance for assessing a parametric trend model with correlated errors,\nunder random design, comparing a parametric and a nonparametric trend\nestimators. The present work aims to provide a detailed computational analysis\nof the behavior of this approach using different bootstrap algorithms for\ncalibration, under a fixed-design geostatistical framework. Asymptotic results\nfor the test are provided and an extensive simulation study, considering\ncomplexities that usually arise in geostatistics, is carried out to illustrate\nthe performance of the proposal.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 13:27:32 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Meil\u00e1n-Vila", "Andrea", ""], ["Fern\u00e1ndez-Casal", "Rub\u00e9n", ""], ["Francisco-Fern\u00e1ndez", "Rosa M. Crujeiras Mario", ""]]}, {"id": "2002.05550", "submitter": "Dino Sejdinovic", "authors": "Qinyi Zhang, Sarah Filippi, Seth Flaxman, Dino Sejdinovic", "title": "Bayesian Kernel Two-Sample Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern data analysis, nonparametric measures of discrepancies between\nrandom variables are particularly important. The subject is well-studied in the\nfrequentist literature, while the development in the Bayesian setting is\nlimited where applications are often restricted to univariate cases. Here, we\npropose a Bayesian kernel two-sample testing procedure based on modelling the\ndifference between kernel mean embeddings in the reproducing kernel Hilbert\nspace utilising the framework established by Flaxman et al (2016). The use of\nkernel methods enables its application to random variables in generic domains\nbeyond the multivariate Euclidean spaces. The proposed procedure results in a\nposterior inference scheme that allows an automatic selection of the kernel\nparameters relevant to the problem at hand. In a series of synthetic\nexperiments and two real data experiments (i.e. testing network heterogeneity\nfrom high-dimensional data and six-membered monocyclic ring conformation\ncomparison), we illustrate the advantages of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 15:01:10 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Zhang", "Qinyi", ""], ["Filippi", "Sarah", ""], ["Flaxman", "Seth", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "2002.05592", "submitter": "Manuel Lladser", "authors": "Antony Pearson and Manuel E. Lladser", "title": "On Contamination of Symbolic Datasets", "comments": "18 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST q-bio.GN stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data taking values on discrete sample spaces are the embodiment of modern\nbiological research. \"Omics\" experiments produce millions of symbolic outcomes\nin the form of reads (i.e., DNA sequences of a few dozens to a few hundred\nnucleotides). Unfortunately, these intrinsically non-numerical datasets are\noften highly contaminated, and the possible sources of contamination are\nusually poorly characterized. This contrasts with numerical datasets where\nGaussian-type noise is often well-justified. To overcome this hurdle, we\nintroduce the notion of latent weight, which measures the largest expected\nfraction of samples from a contaminated probabilistic source that conform to a\nmodel in a well-structured class of desired models. We examine various\nproperties of latent weights, which we specialize to the class of exchangeable\nprobability distributions. As proof of concept, we analyze DNA methylation data\nfrom the 22 human autosome pairs. Contrary to what it is usually assumed, we\nprovide strong evidence that highly specific methylation patterns are\noverrepresented at some genomic locations when contamination is taken into\naccount.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 16:13:09 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Pearson", "Antony", ""], ["Lladser", "Manuel E.", ""]]}, {"id": "2002.05670", "submitter": "Hannah Li", "authors": "Ramesh Johari, Hannah Li, Inessa Liskovich, Gabriel Weintraub", "title": "Experimental Design in Two-Sided Platforms: An Analysis of Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an analytical framework to study experimental design in two-sided\nmarketplaces. Many of these experiments exhibit interference, where an\nintervention applied to one market participant influences the behavior of\nanother participant. This interference leads to biased estimates of the\ntreatment effect of the intervention. We develop a stochastic market model and\nassociated mean field limit to capture dynamics in such experiments, and use\nour model to investigate how the performance of different designs and\nestimators is affected by marketplace interference effects. Platforms typically\nuse two common experimental designs: demand-side (\"customer\") randomization\n(CR) and supply-side (\"listing\") randomization (LR), along with their\nassociated estimators. We show that good experimental design depends on market\nbalance: in highly demand-constrained markets, CR is unbiased, while LR is\nbiased; conversely, in highly supply-constrained markets, LR is unbiased, while\nCR is biased. We also introduce and study a novel experimental design based on\ntwo-sided randomization (TSR) where both customers and listings are randomized\nto treatment and control. We show that appropriate choices of TSR designs can\nbe unbiased in both extremes of market balance, while yielding relatively low\nbias in intermediate regimes of market balance.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 17:49:42 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 18:57:18 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 21:54:08 GMT"}, {"version": "v4", "created": "Mon, 18 Jan 2021 06:26:55 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Johari", "Ramesh", ""], ["Li", "Hannah", ""], ["Liskovich", "Inessa", ""], ["Weintraub", "Gabriel", ""]]}, {"id": "2002.05676", "submitter": "Renato Rodrigues Silva", "authors": "Renato Rodrigues Silva", "title": "Generalized Autoregressive Neural Network Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A time series is a sequence of observations taken sequentially in time. The\nautoregressive integrated moving average is a class of the model more used for\ntimes series data. However, this class of model has two critical limitations.\nIt fits well onlyGaussian data with the linear structure of correlation. Here,\nI present a new model named as generalized autoregressive neural networks,\nGARNN. The GARNN is an extension of the generalized linear model where the mean\nmarginal depends on the lagged values via the inclusion of the neural network\nin the link function. A practical application of the model is shown using a\nwell-known poliomyelitis case number, originated analyzed by Zeger and Qaqish\n(1988),\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 17:56:42 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Silva", "Renato Rodrigues", ""]]}, {"id": "2002.05746", "submitter": "Luke Miratrix", "authors": "Luke Miratrix", "title": "Using Simulation to Analyze Interrupted Time Series Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are sometimes forced to use the Interrupted Time Series (ITS) design as an\nidentification strategy for potential policy change, such as when we only have\na single treated unit and no comparable controls. For example, with recent\ncounty- and state-wide criminal justice reform efforts, where judicial bodies\nhave changed bail setting practices for everyone in their jurisdiction in order\nto reduce rates of pre-trial detention while maintaining court order and public\nsafety, we have no natural comparison group other than the past. In these\ncontexts, it is imperative to model pre-policy trends with a light touch,\nallowing for structures such as autoregressive departures from any pre-existing\ntrend, in order to accurately and realistically assess the statistical\nuncertainty of our projections (beyond the stringent assumptions necessary for\nthe subsequent causal inferences). To tackle this problem we provide a\nmethodological approach rooted in commonly understood and used modeling\napproaches that better captures uncertainty. We quantify uncertainty with\nsimulation, generating a distribution of plausible counterfactual trajectories\nto compare to the observed; this approach naturally allows for incorporating\nseasonality and other time varying covariates, and provides confidence\nintervals along with point estimates for the potential impacts of policy\nchange. We find simulation provides a natural framework to capture and show\nuncertainty in the ITS designs. It also allows for easy extensions such as\nnonparametric smoothing in order to handle multiple post-policy time points or\nmore structural models to account for seasonality.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 19:10:23 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Miratrix", "Luke", ""]]}, {"id": "2002.05763", "submitter": "Wenrui Li", "authors": "Wenrui Li, Daniel L. Sussman, Eric D. Kolaczyk", "title": "Estimation of the Epidemic Branching Factor in Noisy Contact Networks", "comments": "44 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many fundamental concepts in network-based epidemic modeling depend on the\nbranching factor, which captures a sense of dispersion in the network\nconnectivity and quantifies the rate of spreading across the network. Moreover,\ncontact network information generally is available only up to some level of\nerror. We study the propagation of such errors to the estimation of the\nbranching factor. Specifically, we characterize the impact of network noise on\nthe bias and variance of the observed branching factor for arbitrary true\nnetworks, with examples in sparse, dense, homogeneous and inhomogeneous\nnetworks. In addition, we propose a method-of-moments estimator for the true\nbranching factor. We illustrate the practical performance of our estimator\nthrough simulation studies and with contact networks observed in British\nsecondary schools and a French hospital.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 20:09:42 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 21:34:11 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Li", "Wenrui", ""], ["Sussman", "Daniel L.", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "2002.05777", "submitter": "David R\\\"ugamer", "authors": "David R\\\"ugamer, Chris Kolb, Nadja Klein", "title": "Semi-Structured Deep Distributional Regression: Combining Structured\n  Additive Models and Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining additive models and neural networks allows to broaden the scope of\nstatistical regression and extends deep learning-based approaches by\ninterpretable structured additive predictors at the same time. Existing\napproaches uniting the two modeling approaches are, however, limited to very\nspecific combinations and, more importantly, involve an identifiability issue.\nAs a consequence, interpretability and stable estimation is typically lost. We\npropose a general framework to combine structured regression models and deep\nneural networks into a unifying network architecture. To overcome the inherent\nidentifiability issues between different model parts, we construct an\northogonalization cell that projects the deep neural network into the\northogonal complement of the statistical model predictor. This enables proper\nestimation of structured model parts and thereby interpretability. We\ndemonstrate the framework's efficacy in numerical experiments and illustrate\nits special merits in benchmarks and real-world applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 21:01:26 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 16:06:39 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 13:32:23 GMT"}, {"version": "v4", "created": "Mon, 8 Feb 2021 14:44:21 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["R\u00fcgamer", "David", ""], ["Kolb", "Chris", ""], ["Klein", "Nadja", ""]]}, {"id": "2002.05782", "submitter": "Ioannis Ntzoufras", "authors": "Dimitris Fouskakis and Ioannis Ntzoufras", "title": "Power-Expected-Posterior Priors as Mixtures of g-Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main approaches used to construct prior distributions for\nobjective Bayes methods is the concept of random imaginary observations. Under\nthis setup, the expected-posterior prior (EPP) offers several advantages, among\nwhich it has a nice and simple interpretation and provides an effective way to\nestablish compatibility of priors among models. In this paper, we study the\npower-expected posterior prior as a generalization to the EPP in objective\nBayesian model selection under normal linear models. We prove that it can be\nrepresented as a mixture of $g$-prior, like a wide range of prior distributions\nunder normal linear models, and thus posterior distributions and Bayes factors\nare derived in closed form, keeping therefore computational tractability.\nComparisons with other mixtures of $g$-prior are made and emphasis is given in\nthe posterior distribution of g and its effect on Bayesian model selection and\nmodel averaging.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 21:26:44 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 11:36:45 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Fouskakis", "Dimitris", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "2002.06032", "submitter": "Irene Kyomuhangi", "authors": "Irene Kyomuhangi, Tarekegn A. Abeku, Matthew J. Kirby, Gezahegn\n  Tesfaye and Emanuele Giorgi", "title": "Understanding the effects of dichotomization of continuous outcomes on\n  geostatistical inference", "comments": "18 pages, 5 figures, to be published in the journal of Spatial\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosis is often based on the exceedance or not of continuous health\nindicators of a predefined cut-off value, so as to classify patients into\npositives and negatives for the disease under investigation. In this paper, we\ninvestigate the effects of dichotomization of spatially-referenced continuous\noutcome variables on geostatistical inference. Although this issue has been\nextensively studied in other fields, dichotomization is still a common practice\nin epidemiological studies. Furthermore, the effects of this practice in the\ncontext of prevalence mapping have not been fully understood. Here, we\ndemonstrate how spatial correlation affects the loss of information due to\ndichotomization, how linear geostatistical models can be used to map disease\nprevalence and thus avoid dichotomization, and finally, how dichotomization\naffects our predictive inference on prevalence. To pursue these objectives, we\ndevelop a metric, based on the composite likelihood, which can be used to\nquantify the potential loss of information after dichotomization without\nrequiring the fitting of Binomial geostatistical models. Through a simulation\nstudy and two applications on disease mapping in Africa, we show that, as\nthresholds used for dichotomization move further away from the mean of the\nunderlying process, the performance of binomial geostatistical models\ndeteriorates substantially. We also find that dichotomization can lead to the\nloss of fine scale features of disease prevalence and increased uncertainty in\nthe parameter estimates, especially in the presence of a large noise to signal\nratio. These findings strongly support the conclusions from previous studies\nthat dichotomization should be always avoided whenever feasible.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 13:32:14 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Kyomuhangi", "Irene", ""], ["Abeku", "Tarekegn A.", ""], ["Kirby", "Matthew J.", ""], ["Tesfaye", "Gezahegn", ""], ["Giorgi", "Emanuele", ""]]}, {"id": "2002.06041", "submitter": "Iavor Bojinov", "authors": "Guillaume Basse and Iavor Bojinov", "title": "A general theory of identification", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does it mean to say that a quantity is identifiable from the data?\nStatisticians seem to agree on a definition in the context of parametric\nstatistical models --- roughly, a parameter $\\theta$ in a model $\\mathcal{P} =\n\\{P_\\theta: \\theta \\in \\Theta\\}$ is identifiable if the mapping $\\theta \\mapsto\nP_\\theta$ is injective. This definition raises important questions: Are\nparameters the only quantities that can be identified? Is the concept of\nidentification meaningful outside of parametric statistics? Does it even\nrequire the notion of a statistical model? Partial and idiosyncratic answers to\nthese questions have been discussed in econometrics, biological modeling, and\nin some subfields of statistics like causal inference. This paper proposes a\nunifying theory of identification that incorporates existing definitions for\nparametric and nonparametric models and formalizes the process of\nidentification analysis. The applicability of this framework is illustrated\nthrough a series of examples and two extended case studies.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 14:03:42 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Basse", "Guillaume", ""], ["Bojinov", "Iavor", ""]]}, {"id": "2002.06060", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald and Jonas Peters", "title": "Causality in cognitive neuroscience: concepts, challenges, and\n  distributional robustness", "comments": null, "journal-ref": "Journal of Cognitive Neuroscience, 33(2):226-247, 2021", "doi": "10.1162/jocn_a_01623", "report-no": null, "categories": "q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While probabilistic models describe the dependence structure between observed\nvariables, causal models go one step further: they predict, for example, how\ncognitive functions are affected by external interventions that perturb\nneuronal activity. In this review and perspective article, we introduce the\nconcept of causality in the context of cognitive neuroscience and review\nexisting methods for inferring causal relationships from data. Causal inference\nis an ambitious task that is particularly challenging in cognitive\nneuroscience. We discuss two difficulties in more detail: the scarcity of\ninterventional data and the challenge of finding the right variables. We argue\nfor distributional robustness as a guiding principle to tackle these problems.\nRobustness (or invariance) is a fundamental principle underlying causal\nmethodology. A causal model of a target variable generalises across\nenvironments or subjects as long as these environments leave the causal\nmechanisms intact. Consequently, if a candidate model does not generalise, then\neither it does not consist of the target variable's causes or the underlying\nvariables do not represent the correct granularity of the problem. In this\nsense, assessing generalisability may be useful when defining relevant\nvariables and can be used to partially compensate for the lack of\ninterventional data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 14:49:34 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 07:39:52 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Peters", "Jonas", ""]]}, {"id": "2002.06143", "submitter": "Axel B\\\"ucher", "authors": "Axel B\\\"ucher, Holger Dette, Florian Heinrichs", "title": "Are deviations in a gradually varying mean relevant? A testing approach\n  based on sup-norm estimators", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical change point analysis aims at (1) detecting abrupt changes in the\nmean of a possibly non-stationary time series and at (2) identifying regions\nwhere the mean exhibits a piecewise constant behavior. In many applications\nhowever, it is more reasonable to assume that the mean changes gradually in a\nsmooth way. Those gradual changes may either be non-relevant (i.e., small), or\nrelevant for a specific problem at hand, and the present paper presents\nstatistical methodology to detect the latter. More precisely, we consider the\ncommon nonparametric regression model $X_{i} = \\mu (i/n) + \\varepsilon_{i}$\nwith possibly non-stationary errors and propose a test for the null hypothesis\nthat the maximum absolute deviation of the regression function $\\mu$ from a\nfunctional $g (\\mu )$ (such as the value $\\mu (0)$ or the integral\n$\\int_{0}^{1} \\mu (t) dt$) is smaller than a given threshold on a given\ninterval $[x_{0},x_{1}] \\subseteq [0,1]$. A test for this type of hypotheses is\ndeveloped using an appropriate estimator, say $\\hat d_{\\infty, n}$, for the\nmaximum deviation $ d_{\\infty}= \\sup_{t \\in [x_{0},x_{1}]} |\\mu (t) - g( \\mu)\n|$. We derive the limiting distribution of an appropriately standardized\nversion of $\\hat d_{\\infty,n}$, where the standardization depends on the\nLebesgue measure of the set of extremal points of the function\n$\\mu(\\cdot)-g(\\mu)$. A refined procedure based on an estimate of this set is\ndeveloped and its consistency is proved. The results are illustrated by means\nof a simulation study and a data example.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 17:53:58 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Dette", "Holger", ""], ["Heinrichs", "Florian", ""]]}, {"id": "2002.06159", "submitter": "Mohsen Ebadi", "authors": "Mohsen Ebadi, Shoja'eddin Chenouri, Dennis K. J. Lin, Stefan H.\n  Steiner", "title": "Statistical Monitoring of the Covariance Matrix in Multivariate\n  Processes: A Literature Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring several correlated quality characteristics of a process is common\nin modern manufacturing and service industries. Although a lot of attention has\nbeen paid to monitoring the multivariate process mean, not many control charts\nare available for monitoring the covariance matrix. This paper presents a\ncomprehensive overview of the literature on control charts for monitoring the\ncovariance matrix in a multivariate statistical process monitoring (MSPM)\nframework. It classifies the research that has previously appeared in the\nliterature. We highlight the challenging areas for research and provide some\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:18:42 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 03:15:40 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Ebadi", "Mohsen", ""], ["Chenouri", "Shoja'eddin", ""], ["Lin", "Dennis K. J.", ""], ["Steiner", "Stefan H.", ""]]}, {"id": "2002.06204", "submitter": "Burak K\\\"ursad G\\\"unhan", "authors": "Burak K\\\"ursad G\\\"unhan, Sebastian Weber, Tim Friede", "title": "A Bayesian time-to-event pharmacokinetic model for phase I\n  dose-escalation trials with multiple schedules", "comments": "arXiv admin note: text overlap with arXiv:1811.09433", "journal-ref": "Statistics in Medicine, 2020", "doi": "10.1002/sim.8703", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase I dose-escalation trials must be guided by a safety model in order to\navoid exposing patients to unacceptably high risk of toxicities. Traditionally,\nthese trials are based on one type of schedule. In more recent practice,\nhowever, there is often a need to consider more than one schedule, which means\nthat in addition to the dose itself, the schedule needs to be varied in the\ntrial. Hence, the aim is finding an acceptable dose-schedule combination.\nHowever, most established methods for dose-escalation trials are designed to\nescalate the dose only and ad-hoc choices must be made to adapt these to the\nmore complicated setting of finding an acceptable dose-schedule combination. In\nthis paper, we introduce a Bayesian time-to-event model which takes explicitly\nthe dose amount and schedule into account through the use of pharmacokinetic\nprinciples. The model uses a time-varying exposure measure to account for the\nrisk of a dose-limiting toxicity over time. The dose-schedule decisions are\ninformed by an escalation with overdose control criterion. The model is\nformulated using interpretable parameters which facilitates the specification\nof priors. In a simulation study, we compared the proposed method with an\nexisting method. The simulation study demonstrates that the proposed method\nyields similar or better results compared to an existing method in terms of\nrecommending acceptable dose-schedule combinations, yet reduces the number of\npatients enrolled in most of scenarios. The \\texttt{R} and \\texttt{Stan} code\nto implement the proposed method is publicly available from Github\n(\\url{https://github.com/gunhanb/TITEPK_code}).\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 16:18:02 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["G\u00fcnhan", "Burak K\u00fcrsad", ""], ["Weber", "Sebastian", ""], ["Friede", "Tim", ""]]}, {"id": "2002.06439", "submitter": "Samyajoy Pal", "authors": "Samyajoy Pal, Prof. Dr. Christian Heumann, Dr. M. Subbiah", "title": "Further Inference on Categorical Data -- A Bayesian Approach", "comments": "Further improvement is going on", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three different inferential problems related to a two dimensional categorical\ndata from a Bayesian perspective have been discussed in this article. Conjugate\nprior distribution with symmetric and asymmetric hyper parameters are\nconsidered. Newly conceived asymmetric prior is based on perceived preferences\nof categories. An extension of test of independence by introducing a notion of\nmeasuring association between the parameters has been shown using correlation\nmatrix. Probabilities of different parametric combinations have been estimated\nfrom the posterior distribution using closed form integration, Monte-Carlo\nintegration and MCMC methods to draw further inference from categorical data.\nBayesian computation is done using R programming language and illustrated with\nappropriate data sets. Study has highlighted the application of Bayesian\ninference exploiting the distributional form of underlying parameters.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 20:01:17 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 22:42:17 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Pal", "Samyajoy", ""], ["Heumann", "Prof. Dr. Christian", ""], ["Subbiah", "Dr. M.", ""]]}, {"id": "2002.06467", "submitter": "Yuling Yao", "authors": "Andrew Gelman and Yuling Yao", "title": "Holes in Bayesian Statistics", "comments": "To appear in Journal of Physics G", "journal-ref": null, "doi": "10.1088/1361-6471/abc3a5", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every philosophy has holes, and it is the responsibility of proponents of a\nphilosophy to point out these problems. Here are a few holes in Bayesian data\nanalysis: (1) the usual rules of conditional probability fail in the quantum\nrealm, (2) flat or weak priors lead to terrible inferences about things we care\nabout, (3) subjective priors are incoherent, (4) Bayesian decision picks the\nwrong model, (5) Bayes factors fail in the presence of flat or weak priors, (6)\nfor Cantorian reasons we need to check our models, but this destroys the\ncoherence of Bayesian inference. Some of the problems of Bayesian statistics\narise from people trying to do things they shouldn't be trying to do, but other\nholes are not so easily patched. In particular, it may be a good idea to avoid\nflat, weak, or conventional priors, but such advice, if followed, would go\nagainst the vast majority of Bayesian practice and requires us to confront the\nfundamental incoherence of Bayesian inference. This does not mean that we think\nBayesian inference is a bad idea, but it does mean that there is a tension\nbetween Bayesian logic and Bayesian workflow which we believe can only be\nresolved by considering Bayesian logic as a tool, a way of revealing inevitable\nmisfits and incoherences in our model assumptions, rather than as an end in\nitself.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 23:02:46 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 23:20:21 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Gelman", "Andrew", ""], ["Yao", "Yuling", ""]]}, {"id": "2002.06471", "submitter": "Yanjun Han", "authors": "Zijun Gao and Yanjun Han", "title": "Minimax Optimal Nonparametric Estimation of Heterogeneous Treatment\n  Effects", "comments": "To appear at NeurIPS 2020 as a spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central goal of causal inference is to detect and estimate the treatment\neffects of a given treatment or intervention on an outcome variable of\ninterest, where a member known as the heterogeneous treatment effect (HTE) is\nof growing popularity in recent practical applications such as the personalized\nmedicine. In this paper, we model the HTE as a smooth nonparametric difference\nbetween two less smooth baseline functions, and determine the tight statistical\nlimits of the nonparametric HTE estimation as a function of the covariate\ngeometry. In particular, a two-stage nearest-neighbor-based estimator throwing\naway observations with poor matching quality is near minimax optimal. We also\nestablish the tight dependence on the density ratio without the usual\nassumption that the covariate densities are bounded away from zero, where a key\nstep is to employ a novel maximal inequality which could be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 23:42:48 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 11:33:24 GMT"}, {"version": "v3", "created": "Sat, 24 Oct 2020 22:14:24 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Gao", "Zijun", ""], ["Han", "Yanjun", ""]]}, {"id": "2002.06516", "submitter": "Amadou Diadie Ba", "authors": "Ba Amadou Diadie and Lo Gane Samb", "title": "Conditional Shannon, R\\'eyni, and Tsallis entropies estimation and\n  asymptotic limits: discrete case", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method of estimating the joint probability mass function of a pair of\ndiscrete random variables is described. This estimator is used to construct the\nconditional Shannon-R\\'eyni-Tsallis entropies estimates. From there almost sure\nrates of convergence and asymptotic normality are established. The theorical\nresults are validated by simulations.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 06:15:48 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Diadie", "Ba Amadou", ""], ["Samb", "Lo Gane", ""]]}, {"id": "2002.06519", "submitter": "Janet van Niekerk Dr", "authors": "Janet van Niekerk, Haakon Bakka and Haavard Rue", "title": "A principled distance-based prior for the shape of the Weibull model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of flat or weakly informative priors is popular due to the objective\na priori belief in the absence of strong prior information. In the case of the\nWeibull model the improper uniform, equal parameter gamma and joint Jeffrey's\npriors for the shape parameter are popular choices. The effects and behaviors\nof these priors have yet to be established from a modeling viewpoint,\nespecially their ability to reduce to the simpler exponential model. In this\nwork we propose a new principled prior for the shape parameter of the Weibull\nmodel, originating from a prior on the distance function, and advocate this new\nprior as a principled choice in the absence of strong prior information. This\nnew prior can then be used in models with a Weibull modeling component, like\ncompeting risks, joint and spatial models, to mention a few. This prior is\navailable in the R-INLA for use, and is applied in a joint\nlongitudinal-survival model framework using the INLA method.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 06:26:32 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["van Niekerk", "Janet", ""], ["Bakka", "Haakon", ""], ["Rue", "Haavard", ""]]}, {"id": "2002.06521", "submitter": "Lan Liu", "authors": "Lan Liu and Eric Tchetgen Tchetgen", "title": "Regression-based Negative Control of Homophily in Dyadic Peer Effect\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prominent threat to causal inference about peer effects over social\nnetworks is the presence of homophily bias, that is, social influence between\nfriends and families is entangled with common characteristics or underlying\nsimilarities that form close connections. Analysis of social network data has\nsuggested that certain health conditions such as obesity and psychological\nstates including happiness and loneliness can spread over a network. However,\nsuch analyses of peer effects or contagion effects have come under criticism\nbecause homophily bias may compromise the causal statement. We develop a\nregression-based approach which leverages a negative control exposure for\nidentification and estimation of contagion effects on additive or\nmultiplicative scales, in the presence of homophily bias. We apply our methods\nto evaluate the peer effect of obesity in Framingham Offspring Study.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 06:43:19 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Liu", "Lan", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "2002.06524", "submitter": "Chanwoo Lee", "authors": "Chanwoo Lee, Miaoyan Wang", "title": "Tensor denoising and completion based on ordinal observations", "comments": "35 pages, 6 figures", "journal-ref": "Proceedings of the 37th International Conference on Machine\n  Learning(ICML), 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order tensors arise frequently in applications such as neuroimaging,\nrecommendation system, social network analysis, and psychological studies. We\nconsider the problem of low-rank tensor estimation from possibly incomplete,\nordinal-valued observations. Two related problems are studied, one on tensor\ndenoising and the other on tensor completion. We propose a multi-linear\ncumulative link model, develop a rank-constrained M-estimator, and obtain\ntheoretical accuracy guarantees. Our mean squared error bound enjoys a faster\nconvergence rate than previous results, and we show that the proposed estimator\nis minimax optimal under the class of low-rank models. Furthermore, the\nprocedure developed serves as an efficient completion method which guarantees\nconsistent recovery of an order-$K$ $(d,\\ldots,d)$-dimensional low-rank tensor\nusing only $\\tilde{\\mathcal{O}}(Kd)$ noisy, quantized observations. We\ndemonstrate the outperformance of our approach over previous methods on the\ntasks of clustering and collaborative filtering.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 07:09:56 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 19:01:19 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2020 00:04:56 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Lee", "Chanwoo", ""], ["Wang", "Miaoyan", ""]]}, {"id": "2002.06633", "submitter": "Solt Kov\\'acs", "authors": "Solt Kov\\'acs, Housen Li, Peter B\\\"uhlmann, Axel Munk", "title": "Seeded Binary Segmentation: A general methodology for fast and optimal\n  change point detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been an increasing demand on efficient algorithms\nfor large scale change point detection problems. To this end, we propose seeded\nbinary segmentation, an approach relying on a deterministic construction of\nbackground intervals, called seeded intervals, in which single change points\nare searched. The final selection of change points based on the candidates from\nseeded intervals can be done in various ways, adapted to the problem at hand.\nThus, seeded binary segmentation is easy to adapt to a wide range of change\npoint detection problems, let that be univariate, multivariate or even\nhigh-dimensional.\n  We consider the univariate Gaussian change in mean setup in detail. For this\nspecific case we show that seeded binary segmentation leads to a near-linear\ntime approach (i.e. linear up to a logarithmic factor) independent of the\nunderlying number of change points. Furthermore, using appropriate selection\nmethods, the methodology is shown to be asymptotically minimax optimal. While\ncomputationally more efficient, the finite sample estimation performance\nremains competitive compared to state of the art procedures. Moreover, we\nillustrate the methodology for high-dimensional settings with an inverse\ncovariance change point detection problem where our proposal leads to massive\ncomputational gains while still exhibiting good statistical performance.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 18:08:10 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Kov\u00e1cs", "Solt", ""], ["Li", "Housen", ""], ["B\u00fchlmann", "Peter", ""], ["Munk", "Axel", ""]]}, {"id": "2002.06654", "submitter": "Peter Cohen", "authors": "Peter L. Cohen, Colin B. Fogarty", "title": "Gaussian Prepivoting for Finite Population Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In finite population causal inference exact randomization tests can be\nconstructed for sharp null hypotheses, i.e. hypotheses which fully impute the\nmissing potential outcomes. Oftentimes inference is instead desired for the\nweak null that the sample average of the treatment effects takes on a\nparticular value while leaving the subject-specific treatment effects\nunspecified. Without proper care, tests valid for sharp null hypotheses may be\nanti-conservative should only the weak null hold, creating the risk of\nmisinterpretation when randomization tests are deployed in practice. We develop\na general framework for unifying modes of inference for sharp and weak nulls,\nwherein a single procedure simultaneously delivers exact inference for sharp\nnulls and asymptotically valid inference for weak nulls. To do this, we employ\nrandomization tests based upon prepivoted test statistics, wherein a test\nstatistic is first transformed by a suitably constructed cumulative\ndistribution function and its randomization distribution assuming the sharp\nnull is then enumerated. For a large class of commonly employed test\nstatistics, we show that prepivoting may be accomplished by employing the\npush-forward of a sample-based Gaussian measure based upon a suitably\nconstructed covariance estimator. In essence, the approach enumerates the\nrandomization distribution (assuming the sharp null) of a P-value for a\nlarge-sample test known to be valid under the weak null, and uses the resulting\nrandomization distribution to perform inference. The versatility of the method\nis demonstrated through a host of examples, including rerandomized designs and\nregression-adjusted estimators in completely randomized designs.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 19:31:14 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 22:02:26 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 23:31:29 GMT"}, {"version": "v4", "created": "Sun, 13 Jun 2021 13:36:11 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Cohen", "Peter L.", ""], ["Fogarty", "Colin B.", ""]]}, {"id": "2002.06663", "submitter": "Yishu Xue", "authors": "Guanyu Hu, Junxian Geng, Yishu Xue, Huiyan Sang", "title": "Bayesian Spatial Homogeneity Pursuit of Functional Data: an Application\n  to the U.S. Income Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An income distribution describes how an entity's total wealth is distributed\namongst its population. A problem of interest to regional economics researchers\nis to understand the spatial homogeneity of income distributions among\ndifferent regions. In economics, the Lorenz curve is a well-known functional\nrepresentation of income distribution. In this article, we propose a mixture of\nfinite mixtures (MFM) model as well as a Markov random field constrained\nmixture of finite mixtures (MRFC-MFM) model in the context of spatial\nfunctional data analysis to capture spatial homogeneity of Lorenz curves. We\ndesign efficient Markov chain Monte Carlo (MCMC) algorithms to simultaneously\ninfer the posterior distributions of the number of clusters and the clustering\nconfiguration of spatial functional data. Extensive simulation studies are\ncarried out to show the effectiveness of the proposed methods compared with\nexisting methods. We apply the proposed spatial functional clustering method to\nstate level income Lorenz curves from the American Community Survey Public Use\nMicrodata Sample (PUMS) data. The results reveal a number of important\nclustering patterns of state-level income distributions across US.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 20:02:15 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 14:54:36 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 01:23:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hu", "Guanyu", ""], ["Geng", "Junxian", ""], ["Xue", "Yishu", ""], ["Sang", "Huiyan", ""]]}, {"id": "2002.06678", "submitter": "Peng Zhao", "authors": "Peng Zhao, Hou-Cheng Yang, Dipak K. Dey, Guanyu Hu", "title": "Bayesian Spatial Homogeneity Pursuit Regression for Count Value Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial regression models are ubiquitous in many different areas such as\nenvironmental science, geoscience, and public health. Exploring relationships\nbetween response variables and covariates with complex spatial patterns is a\nvery important work. In this paper, we propose a novel spatially clustered\ncoefficients regression model for count value data based on nonparametric\nBayesian methods. Our proposed method detects the spatial homogeneity of the\nPoisson regression coefficients. A Markov random field constraint mixture of\nfinite mixtures prior provides a consistent estimator of the number of the\nclusters of regression coefficients with the geographically neighborhood\ninformation. The theoretical properties of our proposed method are established.\nAn efficient Markov chain Monte Carlo algorithm is developed by using\nmultivariate log gamma distribution as a base distribution. Extensive\nsimulation studies are carried out to examine empirical performance of the\nproposed method. Additionally, we analyze Georgia premature deaths data as an\nillustration of the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 21:05:45 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zhao", "Peng", ""], ["Yang", "Hou-Cheng", ""], ["Dey", "Dipak K.", ""], ["Hu", "Guanyu", ""]]}, {"id": "2002.06708", "submitter": "Evan Rosenman", "authors": "Evan Rosenman, Guillaume Basse, Art Owen, Michael Baiocchi", "title": "Combining Observational and Experimental Datasets Using Shrinkage\n  Estimators", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of combining data from observational and experimental\nsources to make causal conclusions. This problem is increasingly relevant, as\nthe modern era has yielded passive collection of massive observational datasets\nin areas such as e-commerce and electronic health. These data may be used to\nsupplement experimental data, which is frequently expensive to obtain. In\nRosenman et al. (2018), we considered this problem under the assumption that\nall confounders were measured. Here, we relax the assumption of\nunconfoundedness. To derive combined estimators with desirable properties, we\nmake use of results from the Stein Shrinkage literature. Our contributions are\nthreefold. First, we propose a generic procedure for deriving shrinkage\nestimators in this setting, making use of a generalized unbiased risk estimate.\nSecond, we develop two new estimators, prove finite sample conditions under\nwhich they have lower risk than an estimator using only experimental data, and\nshow that each achieves a notion of asymptotic optimality. Third, we draw\nconnections between our approach and results in sensitivity analysis, including\nproposing a method for evaluating the feasibility of our estimators.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 23:30:42 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 04:52:22 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Rosenman", "Evan", ""], ["Basse", "Guillaume", ""], ["Owen", "Art", ""], ["Baiocchi", "Michael", ""]]}, {"id": "2002.06843", "submitter": "Wenkai Xu", "authors": "Wenkai Xu, Takeru Matsuda", "title": "A Stein Goodness-of-fit Test for Directional Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields, data appears in the form of direction (unit vector) and usual\nstatistical procedures are not applicable to such directional data. In this\nstudy, we propose non-parametric goodness-of-fit testing procedures for general\ndirectional distributions based on kernel Stein discrepancy. Our method is\nbased on Stein's operator on spheres, which is derived by using Stokes'\ntheorem. Notably, the proposed method is applicable to distributions with an\nintractable normalization constant, which commonly appear in directional\nstatistics. Experimental results demonstrate that the proposed methods control\ntype-I error well and have larger power than existing tests, including the test\nbased on the maximum mean discrepancy.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 08:58:10 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Xu", "Wenkai", ""], ["Matsuda", "Takeru", ""]]}, {"id": "2002.07094", "submitter": "Ya Su", "authors": "Ya Su", "title": "A Divide and Conquer Algorithm of Bayesian Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sets for statistical analysis become extremely large even with some\ndifficulty of being stored on one single machine. Even when the data can be\nstored in one machine, the computational cost would still be intimidating. We\npropose a divide and conquer solution to density estimation using Bayesian\nmixture modeling including the infinite mixture case. The methodology can be\ngeneralized to other application problems where a Bayesian mixture model is\nadopted. The proposed prior on each machine or subsample modifies the original\nprior on both mixing probabilities as well as on the rest of parameters in the\ndistributions being mixed. The ultimate estimator is obtained by taking the\naverage of the posterior samples corresponding to the proposed prior on each\nsubset. Despite the tremendous reduction in time thanks to data splitting, the\nposterior contraction rate of the proposed estimator stays the same (up to a\nlog factor) as that of the original prior when the data is analyzed as a whole.\nSimulation studies also justify the competency of the proposed method compared\nto the established WASP estimator in the finite dimension case. In addition,\none of our simulations is performed in a shape constrained deconvolution\ncontext and reveals promising results. The application to a GWAS data set\nreveals the advantage over a naive method that uses the original prior.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 17:31:47 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Su", "Ya", ""]]}, {"id": "2002.07122", "submitter": "Min Jin Ha", "authors": "Min Jin Ha, Francesco Stingo, Veerabhadran Baladandayuthapani", "title": "Bayesian Structure Learning in Multi-layered Genomic Networks", "comments": "39 pages with 8 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrative network modeling of data arising from multiple genomic platforms\nprovides insight into the holistic picture of the interactive system, as well\nas the flow of information across many disease domains including cancer. The\nbasic data structure consists of a sequence of hierarchically ordered datasets\nfor each individual subject, which facilitates integration of diverse inputs,\nsuch as genomic, transcriptomic, and proteomic data. A primary analytical task\nin such contexts is to model the layered architecture of networks where the\nvertices can be naturally partitioned into ordered layers, dictated by multiple\nplatforms, and exhibit both undirected and directed relationships. We propose a\nmulti-layered Gaussian graphical model (mlGGM) to investigate conditional\nindependence structures in such multi-level genomic networks in human cancers.\nWe implement a Bayesian node-wise selection (BANS) approach based on variable\nselection techniques that coherently accounts for the multiple types of\ndependencies in mlGGM; this flexible strategy exploits edge-specific prior\nknowledge and selects sparse and interpretable models. Through simulated data\ngenerated under various scenarios, we demonstrate that BANS outperforms other\nexisting multivariate regression-based methodologies. Our integrative genomic\nnetwork analysis for key signaling pathways across multiple cancer types\nhighlights commonalities and differences of p53 integrative networks and\nepigenetic effects of BRCA2 on p53 and its interaction with T68 phosphorylated\nCHK2, that may have translational utilities of finding biomarkers and\ntherapeutic targets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 18:40:21 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Ha", "Min Jin", ""], ["Stingo", "Francesco", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "2002.07242", "submitter": "Afonso Silva", "authors": "Kelly C. M. Gon\\c{c}alves and Afonso C. B. Silva", "title": "Bayesian Quantile Factor Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Factor analysis is a flexible technique for assessment of multivariate\ndependence and codependence. Besides being an exploratory tool used to reduce\nthe dimensionality of multivariate data, it allows estimation of common factors\nthat often have an interesting theoretical interpretation in real problems.\nHowever, in some specific cases the interest involves the effects of latent\nfactors not only in the mean, but in the entire response distribution,\nrepresented by a quantile. This paper introduces a new class of models, named\nquantile factor models, which combines factor model theory with\ndistribution-free quantile regression producing a robust statistical method.\nBayesian estimation for the proposed model is performed using an efficient\nMarkov chain Monte Carlo algorithm. The proposed model is evaluated using\nsynthetic datasets in different settings, in order to evaluate its robustness\nand performance under different quantiles compared to more usual methods. The\nmodel is also applied to a financial sector dataset and a heart disease\nexperiment.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 20:48:01 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Gon\u00e7alves", "Kelly C. M.", ""], ["Silva", "Afonso C. B.", ""]]}, {"id": "2002.07255", "submitter": "Ya Su", "authors": "Ya Su, Anirban Bhattacharya, Yan Zhang, Nilanjan Chatterjee and\n  Raymond J. Carroll", "title": "Nonparametric Bayesian Deconvolution of a Symmetric Unimodal Density", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric measurement error density deconvolution subject to\nheteroscedastic measurement errors as well as symmetry about zero and shape\nconstraints, in particular unimodality. The problem is motivated by\napplications where the observed data are estimated effect sizes from\nregressions on multiple factors, where the target is the distribution of the\ntrue effect sizes. We exploit the fact that any symmetric and unimodal density\ncan be expressed as a mixture of symmetric uniform densities, and model the\nmixing density in a new way using a Dirichlet process location-mixture of Gamma\ndistributions. We do the computations within a Bayesian context, describe a\nsimple scalable implementation that is linear in the sample size, and show that\nthe estimate of the unknown target density is consistent. Within our\napplication context of regression effect sizes, the target density is likely to\nhave a large probability near zero (the near null effects) coupled with a\nheavy-tailed distribution (the actual effects). Simulations show that unlike\nstandard deconvolution methods, our Constrained Bayesian Deconvolution method\ndoes a much better job of reconstruction of the target density. Applications to\na genome-wise association study (GWAS) and microarray data reveal similar\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 21:18:51 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Su", "Ya", ""], ["Bhattacharya", "Anirban", ""], ["Zhang", "Yan", ""], ["Chatterjee", "Nilanjan", ""], ["Carroll", "Raymond J.", ""]]}, {"id": "2002.07270", "submitter": "Oliver Y. Ch\\'en", "authors": "Oliver Y. Ch\\'en, Ra\\'ul G. Saraiva, Guy Nagels, Huy Phan, Tom\n  Schwantje, Hengyi Cao, Jiangtao Gou, Jenna M. Reinen, Bin Xiong, and Maarten\n  de Vos", "title": "Thou Shalt Not Reject the P-value", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.18014.59206", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since its debut in the 18th century, the P-value has been an integral part of\nhypothesis testing based scientific discoveries. As the statistical engine\nages, questions are beginning to be raised, asking to what extent scientific\ndiscoveries based on a P-value (e.g., the practice of drawing scientific\nconclusions relying on the fact that the P-value is smaller than an\nartificially determined threshold, for example, that of 0.05) are reliable and\nreproducible, and the voice calling for adjusting the significance level of\n0.05 or banning the P-value has been increasingly heard. Inspired by these\nquestions, we inquire into the useful roles and misuses of the P-value in\nscientific studies. We attempt to unravel the associations between the P-value,\nsample size, significance level, and statistical power. For common misuses and\nmisinterpretations of the P-value, we provide modest recommendations for\npractitioners. Additionally, we review, with a comparison, Bayesian\nalternatives to the P-value, and discuss the advantages of meta-analysis in\ncombining information, reducing bias, and delivering reproducible evidence.\nTaken together, we argue that the P-value underpins a useful probabilistic\ndecision-making system, provides evidence at a continuous scale, and allows for\nintegrating results from multiple studies and data sets. But the interpretation\nmust be contextual, taking into account the scientific question, experimental\ndesign (including the sample size and significance level), statistical power,\neffect size, and reproducibility.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 21:52:26 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 21:27:31 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2020 13:44:56 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Ch\u00e9n", "Oliver Y.", ""], ["Saraiva", "Ra\u00fal G.", ""], ["Nagels", "Guy", ""], ["Phan", "Huy", ""], ["Schwantje", "Tom", ""], ["Cao", "Hengyi", ""], ["Gou", "Jiangtao", ""], ["Reinen", "Jenna M.", ""], ["Xiong", "Bin", ""], ["de Vos", "Maarten", ""]]}, {"id": "2002.07370", "submitter": "Ran Dai", "authors": "Ran Dai and Mladen Kolar", "title": "Post-selection inference on high-dimensional varying-coefficient\n  quantile regression model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression has been successfully used to study heterogeneous and\nheavy-tailed data. In this work, we study high-dimensional varying-coefficient\nquantile regression model that allows us to capture non-stationary effects of\nthe input variables across time. We develop new tools for statistical inference\nthat allow us to construct valid confidence intervals and honest tests for\nnonparametric coefficient at fixed time and quantile. Our focus is on inference\nin a high-dimensional setting where the number of input variables exceeds the\nsample size. Performing statistical inference in this regime is challenging due\nto the usage of model selection techniques in estimation. Never the less, we\nare able to develop valid inferential tools that are applicable to a wide range\nof data generating processes and do not suffer from biases introduced by model\nselection. The statistical framework allows us to construct a confidence\ninterval at a fixed point in time and a fixed quantile based on a Normal\napproximation. We performed numerical simulations to demonstrate the finite\nsample performance of our method and we also illustrated the application with a\nreal data example.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 04:43:38 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 16:49:17 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Dai", "Ran", ""], ["Kolar", "Mladen", ""]]}, {"id": "2002.07389", "submitter": "Janusz Milek", "authors": "Janusz Milek", "title": "Quantum Implementation of Risk Analysis-relevant Copulas", "comments": "15 pages, 32+10 figures, 4+1 tables. Changes in v.2: updated\n  references p. 2 and 14-15, typo correction p. 6, quantum circuit diagrams\n  improved for gray print", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.CP q-fin.RM quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern quantitative risk management relies on an adequate modeling of the\ntail dependence and a possibly accurate quantification of risk measures, like\nValue at Risk (VaR), at high confidence levels like 1 in 100 or even 1 in 2000.\nQuantum computing makes such a quantification quadratically more efficient than\nthe Monte Carlo method; see (Woerner and Egger, 2018) and, for a broader\nperspective, (Or\\'us et al., 2018). An important element of the risk analysis\ntoolbox is copula, see (Jouanin et al., 2004) regarding financial applications.\nHowever, to the best knowledge of the author, no quantum computing\nimplementation for sampling from a risk modeling-relevant copula in explicit\nform has been published so far. Our focus here is implementation of simple yet\npowerful copula models, capable of a satisfactory capturing the joint tail\nbehaviour of the modelled risk factors. This paper deals with a few simple\ncopula families, including Multivariate B11 (MB11) copula family, presented in\n(Milek, 2014). We will show that this copula family is suitable for the risk\naggregation as it is exceptionally able to reproduce tail dependence\nstructures; see (Embrechts et al., 2016) for a relevant benchmark as well as\nnecessary and sufficient conditions regarding the ultimate feasible bivariate\ntail dependence structures. It turns out that such a discretized copula can be\nexpressed using simple constructs present in the quantum computing: binary\nfraction expansion format, comonotone/independent random variables, controlled\ngates, and convex combinations, and is therefore suitable for a quantum\ncomputer implementation. This paper presents design behind the quantum\nimplementation circuits, numerical and symbolic simulation results, and\nexperimental validation on IBM quantum computer. The paper proposes also a\ngeneric method for quantum implementation of any discretized copula.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 06:05:43 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 06:08:37 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Milek", "Janusz", ""]]}, {"id": "2002.07467", "submitter": "Per Sid\\'en", "authors": "Per Sid\\'en and Fredrik Lindsten", "title": "Deep Gaussian Markov Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Markov random fields (GMRFs) are probabilistic graphical models\nwidely used in spatial statistics and related fields to model dependencies over\nspatial structures. We establish a formal connection between GMRFs and\nconvolutional neural networks (CNNs). Common GMRFs are special cases of a\ngenerative model where the inverse mapping from data to latent variables is\ngiven by a 1-layer linear CNN. This connection allows us to generalize GMRFs to\nmulti-layer CNN architectures, effectively increasing the order of the\ncorresponding GMRF in a way which has favorable computational scaling. We\ndescribe how well-established tools, such as autodiff and variational\ninference, can be used for simple and efficient inference and learning of the\ndeep GMRF. We demonstrate the flexibility of the proposed model and show that\nit outperforms the state-of-the-art on a dataset of satellite temperatures, in\nterms of prediction and predictive uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 10:06:39 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 15:19:04 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Sid\u00e9n", "Per", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "2002.07598", "submitter": "Tim Friede", "authors": "M. Henmi, S. Hattori and T. Friede", "title": "A confidence interval robust to publication bias for random-effects\n  meta-analysis of few studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic reviews aim to summarize all the available evidence relevant to a\nparticular research question. If appropriate, the data from identified studies\nare quantitatively combined in a meta-analysis. Often only few studies\nregarding a particular research question exist. In these settings the\nestimation of the between-study heterogeneity is challenging. Furthermore, the\nassessment of publication bias is difficult as standard methods such as visual\ninspection or formal hypothesis tests in funnel plots do not provide adequate\nguidance. Previously, Henmi and Copas (Statistics in Medicine 2010, 29:\n2969--2983) proposed a confidence interval for the overall effect in\nrandom-effects meta-analysis that is robust to publication bias to some extent.\nAs is evident from their simulations, the confidence intervals have improved\ncoverage compared with standard methods. To our knowledge, the properties of\ntheir method has never been assessed for meta-analyses including fewer than\nfive studies. In this manuscript, we propose a variation of the method by Henmi\nand Copas employing an improved estimator of the between-study heterogeneity,\nin particular when dealing with few studies only. In a simulation study, the\nproposed method is compared to several competitors. Overall, we found that our\nmethod outperforms the others in terms of coverage probabilities. In\nparticular, an improvement compared with the proposal by Henmi and Copas is\ndemonstrated. The work is motivated and illustrated by a systematic review and\nmeta-analysis in paediatric immunosuppression following liver transplantations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 14:48:38 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 16:50:15 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Henmi", "M.", ""], ["Hattori", "S.", ""], ["Friede", "T.", ""]]}, {"id": "2002.07808", "submitter": "Kirstin Strokorb", "authors": "Kirstin Strokorb", "title": "Extremal independence old and new", "comments": "2 pages. Comment on arXiv:1812.01734", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On 12 February 2020 the Royal Statistical Society hosted a meeting to discuss\nthe forthcoming paper ``Graphical models for extremes'' by Sebastian Engelke\nand Adrien Hitz [arXiv:1812.01734]. This short note is a supplement to my\ndiscussion contribution. It contains the proofs. It is shown that the\ntraditional notion of extremal independence agrees with the newly introduced\nnotion of extremal independence, which subsequently allows for a meaningful\ninterpretation of disconnected graphs in the context of the discussion paper.\nThe notation and references used in this note are adopted from the discussion\npaper.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 20:56:24 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Strokorb", "Kirstin", ""]]}, {"id": "2002.07815", "submitter": "Gr\\'egoire Aufort", "authors": "G. Aufort, L. Ciesla, P. Pudlo and V. Buat", "title": "Constraining the recent star formation history of galaxies : an\n  Approximate Bayesian Computation approach", "comments": null, "journal-ref": "A&A 635, A136 (2020)", "doi": "10.1051/0004-6361/201936788", "report-no": null, "categories": "astro-ph.GA stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  [Abridged] Although galaxies are found to follow a tight relation between\ntheir star formation rate and stellar mass, they are expected to exhibit\ncomplex star formation histories (SFH), with short-term fluctuations. The goal\nof this pilot study is to present a method that will identify galaxies that are\nundergoing a strong variation of star formation activity in the last tens to\nhundreds Myr. In other words, the proposed method will determine whether a\nvariation in the last few hundreds of Myr of the SFH is needed to properly\nmodel the SED rather than a smooth normal SFH. To do so, we analyze a sample of\nCOSMOS galaxies using high signal-to-noise ratio broad band photometry. We\napply Approximate Bayesian Computation, a state-of-the-art statistical method\nto perform model choice, associated to machine learning algorithms to provide\nthe probability that a flexible SFH is preferred based on the observed flux\ndensity ratios of galaxies. We present the method and test it on a sample of\nsimulated SEDs. The input information fed to the algorithm is a set of\nbroadband UV to NIR (rest-frame) flux ratios for each galaxy. The method has an\nerror rate of 21% in recovering the right SFH and is sensitive to SFR\nvariations larger than 1 dex. A more traditional SED fitting method using\nCIGALE is tested to achieve the same goal, based on fits comparisons through\nBayesian Information Criterion but the best error rate obtained is higher, 28%.\nWe apply our new method to the COSMOS galaxies sample. The stellar mass\ndistribution of galaxies with a strong to decisive evidence against the smooth\ndelayed-$\\tau$ SFH peaks at lower M* compared to galaxies where the smooth\ndelayed-$\\tau$ SFH is preferred. We discuss the fact that this result does not\ncome from any bias due to our training. Finally, we argue that flexible SFHs\nare needed to be able to cover that largest SFR-M* parameter space possible.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 19:00:01 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Aufort", "G.", ""], ["Ciesla", "L.", ""], ["Pudlo", "P.", ""], ["Buat", "V.", ""]]}, {"id": "2002.07899", "submitter": "Kevin Josey", "authors": "Kevin P. Josey, Seth A. Berkowitz, Debashis Ghosh and Sridharan\n  Raghavan", "title": "Transporting Experimental Results with Entropy Balancing", "comments": null, "journal-ref": null, "doi": "10.1002/sim.9031", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how entropy balancing can be used for transporting experimental\ntreatment effects from a trial population onto a target population. This method\nis doubly-robust in the sense that if either the outcome model or the\nprobability of trial participation is correctly specified, then the estimate of\nthe target population average treatment effect is consistent. Furthermore, we\nonly require the sample moments of the effect modifiers drawn from the target\npopulation to consistently estimate the target population average treatment\neffect. We compared the finite-sample performance of entropy balancing with\nseveral alternative methods for transporting treatment effects between\npopulations. Entropy balancing techniques are efficient and robust to\nviolations of model misspecification. We also examine the results of our\nproposed method in an applied analysis of the Action to Control Cardiovascular\nRisk in Diabetes Blood Pressure (ACCORD-BP) trial transported to a sample of US\nadults with diabetes taken from the National Health and Nutrition Examination\nSurvey (NHANES) cohort.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 22:05:25 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 20:07:25 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 15:52:03 GMT"}, {"version": "v4", "created": "Wed, 28 Apr 2021 03:15:20 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Josey", "Kevin P.", ""], ["Berkowitz", "Seth A.", ""], ["Ghosh", "Debashis", ""], ["Raghavan", "Sridharan", ""]]}, {"id": "2002.07966", "submitter": "Russell Bowater", "authors": "Russell J. Bowater", "title": "Integrated organic inference (IOI): A reconciliation of statistical\n  paradigms", "comments": "Final version with corrections. arXiv admin note: text overlap with\n  arXiv:1901.08589", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is recognised that the Bayesian approach to inference can not adequately\ncope with all the types of pre-data beliefs about population quantities of\ninterest that are commonly held in practice. In particular, it generally\nencounters difficulty when there is a lack of such beliefs over some or all the\nparameters of a model, or within certain partitions of the parameter space\nconcerned. To address this issue, a fairly comprehensive theory of inference is\nput forward called integrated organic inference that is based on a fusion of\nFisherian and Bayesian reasoning. Depending on the pre-data knowledge that is\nheld about any given model parameter, inferences are made about the parameter\nconditional on all other parameters using one of three methods of inference,\nnamely organic fiducial inference, bispatial inference and Bayesian inference.\nThe full conditional post-data densities that result from doing this are then\ncombined using a framework that allows a joint post-data density for all the\nparameters to be sensibly formed without requiring these full conditional\ndensities to be compatible. Various examples of the application of this theory\nare presented. Finally, the theory is defended against possible criticisms\npartially in terms of what was previously defined as generalised subjective\nprobability.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 02:37:29 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 16:48:26 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 16:50:25 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Bowater", "Russell J.", ""]]}, {"id": "2002.08009", "submitter": "Yeng Xiong", "authors": "Yeng Xiong and Michael J. Higgins", "title": "The Benefits of Probability-Proportional-to-Size Sampling in\n  Cluster-Randomized Experiments", "comments": "44 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a cluster-randomized experiment, treatment is assigned to clusters of\nindividual units of interest--households, classrooms, villages, etc.--instead\nof the units themselves. The number of clusters sampled and the number of units\nsampled within each cluster is typically restricted by a budget constraint.\nPrevious analysis of cluster randomized experiments under the Neyman-Rubin\npotential outcomes model of response have assumed a simple random sample of\nclusters. Estimators of the population average treatment effect (PATE) under\nthis assumption are often either biased or not invariant to location shifts of\npotential outcomes. We demonstrate that, by sampling clusters with probability\nproportional to the number of units within a cluster, the Horvitz-Thompson\nestimator (HT) is invariant to location shifts and unbiasedly estimates PATE.\nWe derive standard errors of HT and discuss how to estimate these standard\nerrors. We also show that results hold for stratified random samples when\nsamples are drawn proportionally to cluster size within each stratum. We\ndemonstrate the efficacy of this sampling scheme using a simulation based on\ndata from an experiment measuring the efficacy of the National Solidarity\nProgramme in Afghanistan.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 05:30:02 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Xiong", "Yeng", ""], ["Higgins", "Michael J.", ""]]}, {"id": "2002.08094", "submitter": "Thomas Opitz", "authors": "Thomas Opitz, Denis Allard, Gr\\'egoire Mari\\'ethoz", "title": "Semi-parametric resampling with extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric resampling methods such as Direct Sampling are powerful tools\nto simulate new datasets preserving important data features such as spatial\npatterns from observed datasets while using only minimal assumptions. However,\nsuch methods cannot generate extreme events beyond the observed range of data\nvalues. We here propose using tools from extreme value theory for stochastic\nprocesses to extrapolate observed data towards yet unobserved high quantiles.\nOriginal data are first enriched with new values in the tail region, and then\nclassical resampling algorithms are applied to enriched data. In a first\napproach to enrichment that we label \"naive resampling\", we generate an\nindependent sample of the marginal distribution while keeping the rank order of\nthe observed data. We point out inaccuracies of this approach around the most\nextreme values, and therefore develop a second approach that works for datasets\nwith many replicates. It is based on the asymptotic representation of extreme\nevents through two stochastically independent components: a magnitude variable,\nand a profile field describing spatial variation. To generate enriched data, we\nfix a target range of return levels of the magnitude variable, and we resample\nmagnitudes constrained to this range. We then use the second approach to\ngenerate heatwave scenarios of yet unobserved magnitude over France, based on\ndaily temperature reanalysis training data for the years 2010 to 2016.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 10:07:43 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 08:06:47 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Opitz", "Thomas", ""], ["Allard", "Denis", ""], ["Mari\u00e9thoz", "Gr\u00e9goire", ""]]}, {"id": "2002.08113", "submitter": "Vladimir Panov", "authors": "Anatoly N. Varaksin and Vladimir G. Panov", "title": "Linear Regression Models in Epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper proposes to analyze epidemiological data using regression models\nwhich enable subject-matter (epidemiological) interpretation of such data\nwhether with uncorrelated or correlated predictors. To this end, response\nfunctions should include not only terms linear in predictors but also higher\norder ones (e.g. quadratic and cross terms). For epidemiological interpretation\nof a regression model, the suggestion is to construct conditional functions\nderived from the general regression function with the values of all predictor\nvariables held fixed excepting one predictor. Unlike the conventional\ntechniques based on linear-predictor models in which the coefficient at any\nvariable is interpreted, our approach proposes to interpret this conditional\nfunction, which is multivariate for any predictor being dependent on the values\nof all the other predictors. It is such functions that can describe\nrelationships between Y and a predictor that have different forms in different\npredictor domains. The paper discusses differences in the interpretation of the\nproposed conditional functions between cases involving correlated and\nuncorrelated predictor variables. The construction and analysis of regression\nmodels for epidemiological and environmental data are illustrated with\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 11:34:09 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Varaksin", "Anatoly N.", ""], ["Panov", "Vladimir G.", ""]]}, {"id": "2002.08129", "submitter": "Steven Kleinegesse", "authors": "Steven Kleinegesse and Michael U. Gutmann", "title": "Bayesian Experimental Design for Implicit Models by Mutual Information\n  Neural Estimation", "comments": "Accepted at the thirty-seventh International Conference on Machine\n  Learning (ICML) 2020. Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit stochastic models, where the data-generation distribution is\nintractable but sampling is possible, are ubiquitous in the natural sciences.\nThe models typically have free parameters that need to be inferred from data\ncollected in scientific experiments. A fundamental question is how to design\nthe experiments so that the collected data are most useful. The field of\nBayesian experimental design advocates that, ideally, we should choose designs\nthat maximise the mutual information (MI) between the data and the parameters.\nFor implicit models, however, this approach is severely hampered by the high\ncomputational cost of computing posteriors and maximising MI, in particular\nwhen we have more than a handful of design variables to optimise. In this\npaper, we propose a new approach to Bayesian experimental design for implicit\nmodels that leverages recent advances in neural MI estimation to deal with\nthese issues. We show that training a neural network to maximise a lower bound\non MI allows us to jointly determine the optimal design and the posterior.\nSimulation studies illustrate that this gracefully extends Bayesian\nexperimental design for implicit models to higher design dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 12:09:42 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 17:28:45 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 15:04:46 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Kleinegesse", "Steven", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "2002.08338", "submitter": "Haw-Minn Lu", "authors": "Haw-minn Lu (1), Giancarlo Perrone (1), Jos\\'e Unpingco (1) ((1) Gary\n  and Mary West Health Institute)", "title": "Multiple Imputation with Denoising Autoencoder using Metamorphic Truth\n  and Imputation Feedback", "comments": "Machine Learning and Data Mining in Pattern Recognition, 16th\n  International Conference on Machine Learning and Data Mining, MLDM 2020,\n  Amsterdam, The Netherlands, July 20-21, 2020, Proceedings, pp. 197-208", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although data may be abundant, complete data is less so, due to missing\ncolumns or rows. This missingness undermines the performance of downstream data\nproducts that either omit incomplete cases or create derived completed data for\nsubsequent processing. Appropriately managing missing data is required in order\nto fully exploit and correctly use data. We propose a Multiple Imputation model\nusing Denoising Autoencoders to learn the internal representation of data.\nFurthermore, we use the novel mechanisms of Metamorphic Truth and Imputation\nFeedback to maintain statistical integrity of attributes and eliminate bias in\nthe learning process. Our approach explores the effects of imputation on\nvarious missingness mechanisms and patterns of missing data, outperforming\nother methods in many standard test cases.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 18:26:59 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 02:20:17 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Lu", "Haw-minn", ""], ["Perrone", "Giancarlo", ""], ["Unpingco", "Jos\u00e9", ""]]}, {"id": "2002.08341", "submitter": "Jaime Roquero Gimenez", "authors": "Jaime Roquero Gimenez and James Zou", "title": "Identifying Invariant Factors Across Multiple Environments with KL\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many datasets are collected from multiple environments (e.g. different labs,\nperturbations, etc.), and it is often advantageous to learn models and\nrelations that are invariant across environments. Invariance can improve\nrobustness to unknown confounders and improve generalization to new domains. We\ndevelop a novel framework -- KL regression -- to reliably estimate regression\ncoefficients in a challenging multi-environment setting, where latent\nconfounders affect the data from each environment. KL regression is based on a\nnew objective of simultaneously minimizing the KL- divergence between a\nparametric model and the observed data from each environment. We prove that KL\nregression recovers the true invariant factors under a flexible confounding\nsetup. Moreover, it is computationally efficient as we derive an analytic\nsolution for its global optimum. In systematic experiments, we validate the\nimproved performance of KL regression compared to commonly used approaches.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 18:35:38 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 03:38:58 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Gimenez", "Jaime Roquero", ""], ["Zou", "James", ""]]}, {"id": "2002.08476", "submitter": "Harlan Campbell", "authors": "Harlan Campbell", "title": "A non-inferiority test for R-squared with random regressors", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the lack of association between an outcome variable and a number\nof different explanatory variables is frequently necessary in order to\ndisregard a proposed model. This paper proposes a non-inferiority test for the\ncoefficient of determination (or squared multiple correlation coefficient),\nR-squared, in a linear regression analysis with random predictors. The test is\nderived from inverting a one-sided confidence interval based on a scaled\ncentral F distribution.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 22:19:55 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 04:16:36 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Campbell", "Harlan", ""]]}, {"id": "2002.08506", "submitter": "Yunpu Ma", "authors": "Yunpu Ma and Volker Tresp", "title": "Causal Inference under Networked Interference and Intervention Policy\n  Enhancement", "comments": "Published on AISTATS 2021", "journal-ref": "Proceedings of The 24th International Conference on Artificial\n  Intelligence and Statistics, PMLR 130:3700-3708, 2021", "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating individual treatment effects from data of randomized experiments\nis a critical task in causal inference. The Stable Unit Treatment Value\nAssumption (SUTVA) is usually made in causal inference. However, interference\ncan introduce bias when the assigned treatment on one unit affects the\npotential outcomes of the neighboring units. This interference phenomenon is\nknown as spillover effect in economics or peer effect in social science.\nUsually, in randomized experiments or observational studies with interconnected\nunits, one can only observe treatment responses under interference. Hence, how\nto estimate the superimposed causal effect and recover the individual treatment\neffect in the presence of interference becomes a challenging task in causal\ninference. In this work, we study causal effect estimation under general\nnetwork interference using GNNs, which are powerful tools for capturing the\ndependency in the graph. After deriving causal effect estimators, we further\nstudy intervention policy improvement on the graph under capacity constraint.\nWe give policy regret bounds under network interference and treatment capacity\nconstraint. Furthermore, a heuristic graph structure-dependent error bound for\nGNN-based causal estimators is provided.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 00:35:50 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 10:58:12 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ma", "Yunpu", ""], ["Tresp", "Volker", ""]]}, {"id": "2002.08521", "submitter": "Guanhua Fang", "authors": "Haochen Xu and Guanhua Fang and Xuening Zhu", "title": "Network Group Hawkes Process Model", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the event occurrences of user activities on online\nsocial network platforms. To characterize the social activity interactions\namong network users, we propose a network group Hawkes (NGH) process model.\nParticularly, the observed network structure information is employed to model\nthe users' dynamic posting behaviors. Furthermore, the users are clustered into\nlatent groups according to their dynamic behavior patterns. To estimate the\nmodel, a constraint maximum likelihood approach is proposed. Theoretically, we\nestablish the consistency and asymptotic normality of the estimators. In\naddition, we show that the group memberships can be identified consistently. To\nconduct estimation, a branching representation structure is firstly introduced,\nand a stochastic EM (StEM) algorithm is developed to tackle the computational\nproblem. Lastly, we apply the proposed method to a social network data\ncollected from Sina Weibo, and identify the infuential network users as an\ninteresting application.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 01:30:42 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Xu", "Haochen", ""], ["Fang", "Guanhua", ""], ["Zhu", "Xuening", ""]]}, {"id": "2002.08536", "submitter": "Kohei Yata", "authors": "Yusuke Narita, Shota Yasui, Kohei Yata", "title": "Off-policy Bandit and Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for predicting the performance of reinforcement learning\nand bandit algorithms, given historical data that may have been generated by a\ndifferent algorithm. Our estimator has the property that its prediction\nconverges in probability to the true performance of a counterfactual algorithm\nat the fast $\\sqrt{N}$ rate, as the sample size $N$ increases. We also show a\ncorrect way to estimate the variance of our prediction, thus allowing the\nanalyst to quantify the uncertainty in the prediction. These properties hold\neven when the analyst does not know which among a large number of potentially\nimportant state variables are really important. These theoretical guarantees\nmake our estimator safe to use. We finally apply it to improve advertisement\ndesign by a major advertisement company. We find that our method produces\nsmaller mean squared errors than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 02:30:02 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 22:44:37 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Narita", "Yusuke", ""], ["Yasui", "Shota", ""], ["Yata", "Kohei", ""]]}, {"id": "2002.08542", "submitter": "Chenguang Dai", "authors": "Chenguang Dai, Buyu Lin, Xin Xing, Jun S. Liu", "title": "False Discovery Rate Control via Data Splitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting relevant features associated with a given response variable is an\nimportant issue in many scientific fields. Quantifying quality and uncertainty\nof a selection result via false discovery rate (FDR) control has been of recent\ninterest. This paper introduces a way of using data-splitting strategies to\nasymptotically control the FDR while maintaining a high power. For each\nfeature, the method constructs a test statistic by estimating two independent\nregression coefficients via data splitting. FDR control is achieved by taking\nadvantage of the statistic's property that, for any null feature, its sampling\ndistribution is symmetric about zero. Furthermore, we propose Multiple Data\nSplitting (MDS) to stabilize the selection result and boost the power.\nInterestingly and surprisingly, with the FDR still under control, MDS not only\nhelps overcome the power loss caused by sample splitting, but also results in a\nlower variance of the false discovery proportion (FDP) compared with all other\nmethods in consideration. We prove that the proposed data-splitting methods can\nasymptotically control the FDR at any designated level for linear and Gaussian\ngraphical models in both low and high dimensions. Through intensive simulation\nstudies and a real-data application, we show that the proposed methods are\nrobust to the unknown distribution of features, easy to implement and\ncomputationally efficient, and are often the most powerful ones amongst\ncompetitors especially when the signals are weak and the correlations or\npartial correlations are high among features.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 02:43:05 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 03:05:17 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Dai", "Chenguang", ""], ["Lin", "Buyu", ""], ["Xing", "Xin", ""], ["Liu", "Jun S.", ""]]}, {"id": "2002.08545", "submitter": "Boyan Duan", "authors": "Boyan Duan, Aaditya Ramdas, Larry Wasserman", "title": "Familywise Error Rate Control by Interactive Unmasking", "comments": "29 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for multiple hypothesis testing with familywise error\nrate (FWER) control, called the i-FWER test. Most testing methods are\npredefined algorithms that do not allow modifications after observing the data.\nHowever, in practice, analysts tend to choose a promising algorithm after\nobserving the data; unfortunately, this violates the validity of the\nconclusion. The i-FWER test allows much flexibility: a human (or a computer\nprogram acting on the human's behalf) may adaptively guide the algorithm in a\ndata-dependent manner. We prove that our test controls FWER if the analysts\nadhere to a particular protocol of \"masking\" and \"unmasking\". We demonstrate\nvia numerical experiments the power of our test under structured non-nulls, and\nthen explore new forms of masking.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 03:13:50 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 18:55:58 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 15:25:04 GMT"}, {"version": "v4", "created": "Mon, 19 Apr 2021 13:18:42 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Duan", "Boyan", ""], ["Ramdas", "Aaditya", ""], ["Wasserman", "Larry", ""]]}, {"id": "2002.08560", "submitter": "Douglas Simpson", "authors": "Yeonjoo Park, Xiaohui Chen and Douglas G. Simpson", "title": "Robust Inference for Partially Observed Functional Response Data", "comments": "52 pages, 8 figures, Statistica Sinica (2021), Published online", "journal-ref": null, "doi": "10.5705/ss.202020.0358", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irregular functional data in which densely sampled curves are observed over\ndifferent ranges pose a challenge for modeling and inference, and sensitivity\nto outlier curves is a concern in applications. Motivated by applications in\nquantitative ultrasound signal analysis, this paper investigates a class of\nrobust M-estimators for partially observed functional data including functional\nlocation and quantile estimators. Consistency of the estimators is established\nunder general conditions on the partial observation process. Under smoothness\nconditions on the class of M-estimators, asymptotic Gaussian process\napproximations are established and used for large sample inference. The large\nsample approximations justify a bootstrap approximation for robust inferences\nabout the functional response process. The performance is demonstrated in\nsimulations and in the analysis of irregular functional data from quantitative\nultrasound analysis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 04:13:13 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 18:43:29 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 14:28:59 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Park", "Yeonjoo", ""], ["Chen", "Xiaohui", ""], ["Simpson", "Douglas G.", ""]]}, {"id": "2002.08597", "submitter": "Kostas Loumponias", "authors": "Kostas Loumponias and George Tsaklidis", "title": "Kalman Filtering With Censored Measurements", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns Kalman filtering when the measurements of the process are\ncensored. The censored measurements are addressed by the Tobit model of Type I\nand are one-dimensional with two censoring limits, while the (hidden) state\nvectors are multidimensional. For this model, Bayesian estimates for the state\nvectors are provided through a recursive algorithm of Kalman filtering type.\nExperiments are presented to illustrate the effectiveness and applicability of\nthe algorithm. The experiments show that the proposed method outperforms other\nfiltering methodologies in minimizing the computational cost as well as the\noverall Root Mean Square Error (RMSE) for synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 07:24:59 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Loumponias", "Kostas", ""], ["Tsaklidis", "George", ""]]}, {"id": "2002.08731", "submitter": "Kristiaan Pelckmans", "authors": "Kristiaan Pelckmans and Liu Yang", "title": "APTER: Aggregated Prognosis Through Exponential Reweighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the task of learning how to make a prognosis of a\npatient based on his/her micro-array expression levels. The method is an\napplication of the aggregation method as recently proposed in the literature on\ntheoretical machine learning, and excels in its computational convenience and\ncapability to deal with high-dimensional data. A formal analysis of the method\nis given, yielding rates of convergence similar to what traditional techniques\nobtain, while it is shown to cope well with an exponentially large set of\nfeatures. Those results are supported by numerical simulations on a range of\npublicly available survival-micro-array datasets. It is empirically found that\nthe proposed technique combined with a recently proposed preprocessing\ntechnique gives excellent performances.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 13:53:05 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Pelckmans", "Kristiaan", ""], ["Yang", "Liu", ""]]}, {"id": "2002.08757", "submitter": "Mucyo Karemera", "authors": "St\\'ephane Guerrier, Mucyo Karemera, Samuel Orso, Maria-Pia\n  Victoria-Feser", "title": "Asymptotically Optimal Bias Reduction for Parametric Models", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.11541", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important challenge in statistical analysis concerns the control of the\nfinite sample bias of estimators. This problem is magnified in high-dimensional\nsettings where the number of variables $p$ diverges with the sample size $n$,\nas well as for nonlinear models and/or models with discrete data. For these\ncomplex settings, we propose to use a general simulation-based approach and\nshow that the resulting estimator has a bias of order $\\mathcal{O}(0)$, hence\nproviding an asymptotically optimal bias reduction. It is based on an initial\nestimator that can be slightly asymptotically biased, making the approach very\ngenerally applicable. This is particularly relevant when classical estimators,\nsuch as the maximum likelihood estimator, can only be (numerically)\napproximated. We show that the iterative bootstrap of Kuk (1995) provides a\ncomputationally efficient approach to compute this bias reduced estimator. We\nillustrate our theoretical results in simulation studies for which we develop\nnew bias reduced estimators for the logistic regression, with and without\nrandom effects. These estimators enjoy additional properties such as robustness\nto data contamination and to the problem of separability.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 16:11:08 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Karemera", "Mucyo", ""], ["Orso", "Samuel", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "2002.09025", "submitter": "Byol Kim", "authors": "Byol Kim, Chen Xu, Rina Foygel Barber", "title": "Predictive Inference Is Free with the Jackknife+-after-Bootstrap", "comments": "31 pages, 12 figures, 4 tables. To appear in the 34th Conference on\n  Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning is widely used in applications to make predictions in\ncomplex decision problems---for example, averaging models fitted to a sequence\nof samples bootstrapped from the available training data. While such methods\noffer more accurate, stable, and robust predictions and model estimates, much\nless is known about how to perform valid, assumption-lean inference on the\noutput of these types of procedures. In this paper, we propose the\njackknife+-after-bootstrap (J+aB), a procedure for constructing a predictive\ninterval, which uses only the available bootstrapped samples and their\ncorresponding fitted models, and is therefore \"free\" in terms of the cost of\nmodel fitting. The J+aB offers a predictive coverage guarantee that holds with\nno assumptions on the distribution of the data, the nature of the fitted model,\nor the way in which the ensemble of models are aggregated---at worst, the\nfailure rate of the predictive interval is inflated by a factor of 2. Our\nnumerical experiments verify the coverage and accuracy of the resulting\npredictive intervals on real data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 21:22:31 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 17:13:17 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 22:15:46 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kim", "Byol", ""], ["Xu", "Chen", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "2002.09032", "submitter": "Tao Jiang", "authors": "Tao Jiang, Yuanyuan Li, and Alison A. Motsinger-Reif", "title": "Knockoff Boosted Tree for Model-Free Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a novel strategy for conducting variable\nselection without prior model topology knowledge using the knockoff method with\nboosted tree models. Our method is inspired by the original knockoff method,\nwhere the differences between original and knockoff variables are used for\nvariable selection with false discovery rate control. The original method uses\nLasso for regression models and assumes there are more samples than variables.\nWe extend this method to both model-free and high-dimensional variable\nselection. We propose two new sampling methods for generating knockoffs, namely\nthe sparse covariance and principal component knockoff methods. We test these\nmethods and compare them with the original knockoff method in terms of their\nability to control type I errors and power. The boosted tree model is a complex\nsystem and has more hyperparameters than models with simpler assumptions. In\nour framework, these hyperparameters are either tuned through Bayesian\noptimization or fixed at multiple levels for trend detection. In simulation\ntests, we also compare the properties and performance of importance test\nstatistics of tree models. The results include combinations of different\nknockoffs and importance test statistics. We also consider scenarios that\ninclude main-effect, interaction, exponential, and second-order models while\nassuming the true model structures are unknown. We apply our algorithm for\ntumor purity estimation and tumor classification using the Cancer Genome Atlas\n(TCGA) gene expression data. The proposed algorithm is included in the KOBT\npackage, available at\n\\url{https://cran.r-project.org/web/packages/KOBT/index.html}.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 22:02:52 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Jiang", "Tao", ""], ["Li", "Yuanyuan", ""], ["Motsinger-Reif", "Alison A.", ""]]}, {"id": "2002.09116", "submitter": "Danica J. Sutherland", "authors": "Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, Arthur Gretton, Danica\n  J. Sutherland", "title": "Learning Deep Kernels for Non-Parametric Two-Sample Tests", "comments": null, "journal-ref": "Proceedings of the 37th International Conference on Machine\n  Learning (ICML 2020), PMLR 119:6316-6326", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of kernel-based two-sample tests, which aim to determine\nwhether two sets of samples are drawn from the same distribution. Our tests are\nconstructed from kernels parameterized by deep neural nets, trained to maximize\ntest power. These tests adapt to variations in distribution smoothness and\nshape over space, and are especially suited to high dimensions and complex\ndata. By contrast, the simpler kernels used in prior kernel testing work are\nspatially homogeneous, and adaptive only in lengthscale. We explain how this\nscheme includes popular classifier-based two-sample tests as a special case,\nbut improves on them in general. We provide the first proof of consistency for\nthe proposed adaptation method, which applies both to kernels on deep features\nand to simpler radial basis kernels or multiple kernel learning. In\nexperiments, we establish the superior performance of our deep kernels in\nhypothesis testing on benchmark and real-world data. The code of our\ndeep-kernel-based two sample tests is available at\nhttps://github.com/fengliu90/DK-for-TST.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 03:54:23 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 18:23:31 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 05:29:18 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Liu", "Feng", ""], ["Xu", "Wenkai", ""], ["Lu", "Jie", ""], ["Zhang", "Guangquan", ""], ["Gretton", "Arthur", ""], ["Sutherland", "Danica J.", ""]]}, {"id": "2002.09119", "submitter": "Sharmistha Guha", "authors": "Sharmistha Guha, Jerome P. Reiter and Andrea Mercatanti", "title": "Bayesian Causal Inference with Bipartite Record Linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scenarios, the observational data needed for causal inferences are\nspread over two data files. In particular, we consider scenarios where one file\nincludes covariates and the treatment measured on one set of individuals, and a\nsecond file includes responses measured on another, partially overlapping set\nof individuals. In the absence of error free direct identifiers like social\nsecurity numbers, straightforward merging of separate files is not feasible, so\nthat records must be linked using error-prone variables such as names, birth\ndates, and demographic characteristics. Typical practice in such situations\ngenerally follows a two-stage procedure: first link the two files using a\nprobabilistic linkage technique, then make causal inferences with the linked\ndataset. This does not propagate uncertainty due to imperfect linkages to the\ncausal inference, nor does it leverage relationships among the study variables\nto improve the quality of the linkages. We propose a hierarchical model for\nsimultaneous Bayesian inference on probabilistic linkage and causal effects\nthat addresses these deficiencies. Using simulation studies and theoretical\narguments, we show the hierarchical model can improve the accuracy of estimated\ntreatment effects, as well as the record linkages, compared to the two-stage\nmodeling option. We illustrate the hierarchical model using a causal study of\nthe effects of debit card possession on household spending.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 04:04:29 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 20:58:02 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Guha", "Sharmistha", ""], ["Reiter", "Jerome P.", ""], ["Mercatanti", "Andrea", ""]]}, {"id": "2002.09132", "submitter": "Duy Vo Nguyen Le", "authors": "Vo Nguyen Le Duy, Hiroki Toda, Ryota Sugiyama, Ichiro Takeuchi", "title": "Computing Valid p-value for Optimal Changepoint by Selective Inference\n  using Dynamic Programming", "comments": "Spotlight Presentation at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a vast body of literature related to methods for detecting\nchangepoints (CP). However, less attention has been paid to assessing the\nstatistical reliability of the detected CPs. In this paper, we introduce a\nnovel method to perform statistical inference on the significance of the CPs,\nestimated by a Dynamic Programming (DP)-based optimal CP detection algorithm.\nBased on the selective inference (SI) framework, we propose an exact\n(non-asymptotic) approach to compute valid p-values for testing the\nsignificance of the CPs. Although it is well-known that SI has low statistical\npower because of over-conditioning, we address this disadvantage by introducing\nparametric programming techniques. Then, we propose an efficient method to\nconduct SI with the minimum amount of conditioning, leading to high statistical\npower. We conduct experiments on both synthetic and real-world datasets,\nthrough which we offer evidence that our proposed method is more powerful than\nexisting methods, has decent performance in terms of computational efficiency,\nand provides good results in many practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 05:07:22 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 13:06:56 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Duy", "Vo Nguyen Le", ""], ["Toda", "Hiroki", ""], ["Sugiyama", "Ryota", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "2002.09188", "submitter": "Shuichi Kawano", "authors": "Shuichi Kawano", "title": "Sparse principal component regression via singular value decomposition\n  approach", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component regression (PCR) is a two-stage procedure: the first\nstage performs principal component analysis (PCA) and the second stage\nconstructs a regression model whose explanatory variables are replaced by\nprincipal components obtained by the first stage. Since PCA is performed by\nusing only explanatory variables, the principal components have no information\nabout the response variable. To address the problem, we propose a one-stage\nprocedure for PCR in terms of singular value decomposition approach. Our\napproach is based upon two loss functions, a regression loss and a PCA loss,\nwith sparse regularization. The proposed method enables us to obtain principal\ncomponent loadings that possess information about both explanatory variables\nand a response variable. An estimation algorithm is developed by using\nalternating direction method of multipliers. We conduct numerical studies to\nshow the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 09:03:05 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Kawano", "Shuichi", ""]]}, {"id": "2002.09227", "submitter": "Jacinto Carrasco", "authors": "J. Carrasco, S. Garc\\'ia, M.M. Rueda, S. Das and F. Herrera", "title": "Recent Trends in the Use of Statistical Tests for Comparing Swarm and\n  Evolutionary Computing Algorithms: Practical Guidelines and a Critical Review", "comments": "52 pages, 10 figures, 19 tables", "journal-ref": "SWEVO, Volume 54, May 2020, 100665", "doi": "10.1016/j.swevo.2020.100665", "report-no": null, "categories": "cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key aspect of the design of evolutionary and swarm intelligence algorithms\nis studying their performance. Statistical comparisons are also a crucial part\nwhich allows for reliable conclusions to be drawn. In the present paper we\ngather and examine the approaches taken from different perspectives to\nsummarise the assumptions made by these statistical tests, the conclusions\nreached and the steps followed to perform them correctly. In this paper, we\nconduct a survey on the current trends of the proposals of statistical analyses\nfor the comparison of algorithms of computational intelligence and include a\ndescription of the statistical background of these tests. We illustrate the use\nof the most common tests in the context of the Competition on single-objective\nreal parameter optimisation of the IEEE Congress on Evolutionary Computation\n(CEC) 2017 and describe the main advantages and drawbacks of the use of each\nkind of test and put forward some recommendations concerning their use.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 11:06:47 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Carrasco", "J.", ""], ["Garc\u00eda", "S.", ""], ["Rueda", "M. M.", ""], ["Das", "S.", ""], ["Herrera", "F.", ""]]}, {"id": "2002.09269", "submitter": "Binh T. Nguyen", "authors": "Tuan-Binh Nguyen, J\\'er\\^ome-Alexis Chevalier, Bertrand Thirion,\n  Sylvain Arlot", "title": "Aggregation of Multiple Knockoffs", "comments": "Accepted to ICML 2020 (Thirty-seventh International Conference on\n  Machine Learning). This version includes both the main text of the conference\n  paper and supplementary materials (as appendices). 35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an extension of the Knockoff Inference procedure, introduced by\nBarber and Candes (2015). This new method, called Aggregation of Multiple\nKnockoffs (AKO), addresses the instability inherent to the random nature of\nKnockoff-based inference. Specifically, AKO improves both the stability and\npower compared with the original Knockoff algorithm while still maintaining\nguarantees for False Discovery Rate control. We provide a new inference\nprocedure, prove its core properties, and demonstrate its benefits in a set of\nexperiments on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 13:28:40 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 14:26:21 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Nguyen", "Tuan-Binh", ""], ["Chevalier", "J\u00e9r\u00f4me-Alexis", ""], ["Thirion", "Bertrand", ""], ["Arlot", "Sylvain", ""]]}, {"id": "2002.09273", "submitter": "Edgar Brunner", "authors": "Edgar Brunner", "title": "Success-Odds: An improved Win-Ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple and combined endpoints involving also non-normal outcomes appear in\nmany clinical trials in various areas in medicine. In some cases, the outcome\ncan be observed only on an ordinal or dichotomous scale. Then the success of\ntwo therapies is assessed by comparing the outcome of two randomly selected\npatients from the two therapy groups by 'better', 'equal' or 'worse'. These\noutcomes can be described by the probabilities $p^-=P(X<Y)$, $p_0=P(X=Y)$, and\n$p^+ =P(X~>~Y)$. For a clinician, however, these quantities are less intuitive.\nTherefore, Noether (1987) introduced the quantity $\\lambda=p^+ / p^-$ assuming\ncontinuous distributions. The same quantity was used by Pocock et al. (2012)\nand by Wang and Pocock (2016) also for general non-normal outcomes and has been\ncalled 'win-ratio' $\\lambda_{WR}$. Unlike Noether (1987), Wang and Pocock\n(2016) explicitly allowed for ties in the data. It is the aim of this\nmanuscript to investigate the properties of $\\lambda_{WR}$ in case of ties. It\nturns out that it has the strange property of becoming larger if the data are\nobserved less accurately, i.e. include more ties. Thus, in case of ties, the\nwin-ratio looses its appealing property to describe and quantify an intuitive\nand well interpretable treatment effect. Therefore, a slight modification of\n$\\lambda_{WR} = \\theta / (1-\\theta)$ is suggested, namely the so-called\n'success-odds' where $\\theta=p^+ + \\frac12 p_0$ is called a success of a\ntherapy if $\\theta>\\frac12$. In the case of no ties, $\\lambda_{SO}$ is\nidentical to $\\lambda_{WR}$. A test for the hypothesis $\\lambda_{SO}=1$ and\nrange preserving confidence intervals for $\\lambda_{SO}$ are derived. By two\ncounterexamples it is demonstrated that generalizations of both the win-ratio\nand the success-odds to more than two treatments or to stratified designs are\nnot straightforward and need more detailed considerations.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 13:31:35 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Brunner", "Edgar", ""]]}, {"id": "2002.09287", "submitter": "Kai Zhou", "authors": "Kai Zhou and Jiong Tang", "title": "Uncertainty Quantification of Mode Shape Variation Utilizing Multi-Level\n  Multi-Response Gaussian Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mode shape information play the essential role in deciding the spatial\npattern of vibratory response of a structure. The uncertainty quantification of\nmode shape, i.e., predicting mode shape variation when the structure is\nsubjected to uncertainty, can provide guidance for robust design and control.\nNevertheless, computational efficiency is a challenging issue. Direct Monte\nCarlo simulation is unlikely to be feasible especially for a complex structure\nwith large number of degrees of freedom. In this research, we develop a new\nprobabilistic framework built upon Gaussian process meta-modeling architecture\nto analyze mode shape variation. To expedite the generation of input dataset\nfor meta-model establishment, a multi-level strategy is adopted which can blend\na large amount of low-fidelity data acquired from order-reduced analysis with a\nsmall amount of high-fidelity data produced by high-dimensional full finite\nelement analysis. To take advantage of the intrinsic relation of spatial\ndistribution of mode shape, a multi-response strategy is incorporated to\npredict mode shape variation at different locations simultaneously. These yield\na multi-level, multi-response Gaussian process that can efficiently and\naccurately quantify the effect of structural uncertainty to mode shape\nvariation. Comprehensive case studies are carried out for demonstration and\nvalidation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 15:08:53 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Zhou", "Kai", ""], ["Tang", "Jiong", ""]]}, {"id": "2002.09301", "submitter": "Hans Kersting", "authors": "Hans Kersting, Nicholas Kr\\\"amer, Martin Schiegg, Christian Daniel,\n  Michael Tiemann, Philipp Hennig", "title": "Differentiable Likelihoods for Fast Inversion of 'Likelihood-Free'\n  Dynamical Systems", "comments": "11 pages (+ 5 pages appendix), 6 figures", "journal-ref": null, "doi": null, "report-no": "Published at ICML 2020", "categories": "stat.ML cs.LG cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-free (a.k.a. simulation-based) inference problems are inverse\nproblems with expensive, or intractable, forward models. ODE inverse problems\nare commonly treated as likelihood-free, as their forward map has to be\nnumerically approximated by an ODE solver. This, however, is not a fundamental\nconstraint but just a lack of functionality in classic ODE solvers, which do\nnot return a likelihood but a point estimate. To address this shortcoming, we\nemploy Gaussian ODE filtering (a probabilistic numerical method for ODEs) to\nconstruct a local Gaussian approximation to the likelihood. This approximation\nyields tractable estimators for the gradient and Hessian of the\n(log-)likelihood. Insertion of these estimators into existing gradient-based\noptimization and sampling methods engenders new solvers for ODE inverse\nproblems. We demonstrate that these methods outperform standard likelihood-free\napproaches on three benchmark-systems.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 14:00:15 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 18:06:37 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Kersting", "Hans", ""], ["Kr\u00e4mer", "Nicholas", ""], ["Schiegg", "Martin", ""], ["Daniel", "Christian", ""], ["Tiemann", "Michael", ""], ["Hennig", "Philipp", ""]]}, {"id": "2002.09316", "submitter": "Holger Dette", "authors": "Kathrin M\\\"ollenhoff, Florence Loingeville, Julie Bertrand, Thu Thuy\n  Nguyen, Satish Sharan, Guoying Sun, Stella Grosser, Liang Zhao, Lanyan Fang,\n  France Mentr\\'e, Holger Dette", "title": "Efficient model-based Bioequivalence Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical approach to analyze pharmacokinetic (PK) data in bioequivalence\nstudies aiming to compare two different formulations is to perform\nnoncompartmental analysis (NCA) followed by two one-sided tests (TOST). In this\nregard the PK parameters $AUC$ and $C_{max}$ are obtained for both treatment\ngroups and their geometric mean ratios are considered. According to current\nguidelines by the U.S. Food and Drug Administration and the European Medicines\nAgency the formulations are declared to be sufficiently similar if the $90\\%$-\nconfidence interval for these ratios falls between $0.8$ and $1.25$. As NCA is\nnot a reliable approach in case of sparse designs, a model-based alternative\nhas already been proposed for the estimation of $AUC$ and $C_{max}$ using\nnon-linear mixed effects models. Here we propose another, more powerful test\nthan the TOST and demonstrate its superiority through a simulation study both\nfor NCA and model-based approaches. For products with high variability on PK\nparameters, this method appears to have closer type I errors to the\nconventionally accepted significance level of $0.05$, suggesting its potential\nuse in situations where conventional bioequivalence analysis is not applicable.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 14:11:41 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["M\u00f6llenhoff", "Kathrin", ""], ["Loingeville", "Florence", ""], ["Bertrand", "Julie", ""], ["Nguyen", "Thu Thuy", ""], ["Sharan", "Satish", ""], ["Sun", "Guoying", ""], ["Grosser", "Stella", ""], ["Zhao", "Liang", ""], ["Fang", "Lanyan", ""], ["Mentr\u00e9", "France", ""], ["Dette", "Holger", ""]]}, {"id": "2002.09358", "submitter": "Achraf Bennis", "authors": "Achraf Bennis (IRIT), Sandrine Mouysset (IRIT), Mathieu Serrurier\n  (IRIT)", "title": "Estimation of conditional mixture Weibull distribution with\n  right-censored data using neural network for time-to-event analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider survival analysis with right-censored data which\nis a common situation in predictive maintenance and health field. We propose a\nmodel based on the estimation of two-parameter Weibull distribution\nconditionally to the features. To achieve this result, we describe a neural\nnetwork architecture and the associated loss functions that takes into account\nthe right-censored data. We extend the approach to a finite mixture of\ntwo-parameter Weibull distributions. We first validate that our model is able\nto precisely estimate the right parameters of the conditional Weibull\ndistribution on synthetic datasets. In numerical experiments on two real-word\ndatasets (METABRIC and SEER), our model outperforms the state-of-the-art\nmethods. We also demonstrate that our approach can consider any survival time\nhorizon.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 15:32:06 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Bennis", "Achraf", "", "IRIT"], ["Mouysset", "Sandrine", "", "IRIT"], ["Serrurier", "Mathieu", "", "IRIT"]]}, {"id": "2002.09377", "submitter": "Owen Thomas", "authors": "Owen Thomas, Henri Pesonen, Raquel S\\'a-Le\\~ao, Herm\\'inia de\n  Lencastre, Samuel Kaski, Jukka Corander", "title": "Split-BOLFI for for misspecification-robust likelihood free inference in\n  high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-free inference for simulator-based statistical models has recently\ngrown rapidly from its infancy to a useful tool for practitioners. However,\nmodels with more than a very small number of parameters as the target of\ninference have remained an enigma, in particular for the approximate Bayesian\ncomputation (ABC) community. To advance the possibilities for performing\nlikelihood-free inference in high-dimensional parameter spaces, here we\nintroduce an extension of the popular Bayesian optimisation based approach to\napproximate discrepancy functions in a probabilistic manner which lends itself\nto an efficient exploration of the parameter space. Our method achieves\ncomputational scalability by using separate acquisition procedures for the\ndiscrepancies defined for different parameters. These efficient\nhigh-dimensional simulation acquisitions are combined with exponentiated\nloss-likelihoods to provide a misspecification-robust characterisation of the\nmarginal posterior distribution for all model parameters. The method\nsuccessfully performs computationally efficient inference in a 100-dimensional\nspace on canonical examples and compares favourably to existing Copula-ABC\nmethods. We further illustrate the potential of this approach by fitting a\nbacterial transmission dynamics model to daycare centre data, which provides\nbiologically coherent results on the strain competition in a 30-dimensional\nparameter space.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 16:06:11 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Thomas", "Owen", ""], ["Pesonen", "Henri", ""], ["S\u00e1-Le\u00e3o", "Raquel", ""], ["de Lencastre", "Herm\u00ednia", ""], ["Kaski", "Samuel", ""], ["Corander", "Jukka", ""]]}, {"id": "2002.09414", "submitter": "Marco Piccininni", "authors": "Marco Piccininni, Stefan Konigorski, Jessica L Rohmann, Tobias Kurth", "title": "Directed Acyclic Graphs and causal thinking in clinical risk prediction\n  modeling", "comments": "22 pages, 3 figures, 1 table", "journal-ref": "BMC Med Res Methodol 20, 179 (2020)", "doi": "10.1186/s12874-020-01058-z", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: In epidemiology, causal inference and prediction modeling\nmethodologies have been historically distinct. Directed Acyclic Graphs (DAGs)\nare used to model a priori causal assumptions and inform variable selection\nstrategies for causal questions. Although tools originally designed for\nprediction are finding applications in causal inference, the counterpart has\nremained largely unexplored. The aim of this theoretical and simulation-based\nstudy is to assess the potential benefit of using DAGs in clinical risk\nprediction modeling.\n  Methods and Findings: We explore how incorporating knowledge about the\nunderlying causal structure can provide insights about the transportability of\ndiagnostic clinical risk prediction models to different settings. A\nsingle-predictor model in the causal direction is likely to have better\ntransportability than one in the anticausal direction. We further probe whether\ncausal knowledge can be used to improve predictor selection. We empirically\nshow that the Markov Blanket, the set of variables including the parents,\nchildren, and parents of the children of the outcome node in a DAG, is the\noptimal set of predictors for that outcome.\n  Conclusions: Our findings challenge the generally accepted notion that a\nchange in the distribution of the predictors does not affect diagnostic\nclinical risk prediction model calibration if the predictors are properly\nincluded in the model. Furthermore, using DAGs to identify Markov Blanket\nvariables may be a useful, efficient strategy to select predictors in clinical\nrisk prediction models if strong knowledge of the underlying causal structure\nexists or can be learned.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 17:02:38 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Piccininni", "Marco", ""], ["Konigorski", "Stefan", ""], ["Rohmann", "Jessica L", ""], ["Kurth", "Tobias", ""]]}, {"id": "2002.09578", "submitter": "Xiaochun Meng", "authors": "Xiaochun Meng, James W. Taylor, Souhaib Ben Taieb, Siran Li", "title": "Scoring Functions for Multivariate Distributions and Level Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in predicting multivariate probability distributions is growing due\nto the increasing availability of rich datasets and computational developments.\nScoring functions enable the comparison of forecast accuracy, and can\npotentially be used for estimation. A scoring function for multivariate\ndistributions that has gained some popularity is the energy score. This is a\ngeneralization of the continuous ranked probability score (CRPS), which is\nwidely used for univariate distributions. A little-known, alternative\ngeneralization is the multivariate CRPS (MCRPS). We propose a theoretical\nframework for scoring functions for multivariate distributions, which\nencompasses the energy score and MCRPS, as well as the quadratic score, which\nhas also received little attention. We demonstrate how this framework can be\nused to generate new scores. For univariate distributions, it is\nwell-established that the CRPS can be expressed as the integral over a quantile\nscore. We show that, in a similar way, scoring functions for multivariate\ndistributions can be \"disintegrated\" to obtain scoring functions for level\nsets. Using this, we present scoring functions for different types of level\nset, including those for densities and cumulative distributions. To compute the\nscoring functions, we propose a simple numerical algorithm. We illustrate our\nproposals using simulated and stock returns data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 23:56:14 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 23:46:22 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 00:43:56 GMT"}, {"version": "v4", "created": "Mon, 8 Jun 2020 15:06:46 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Meng", "Xiaochun", ""], ["Taylor", "James W.", ""], ["Taieb", "Souhaib Ben", ""], ["Li", "Siran", ""]]}, {"id": "2002.09606", "submitter": "Maoran Xu", "authors": "Maoran Xu, Leo L. Duan", "title": "Bayesian Multi-scale Modeling of Factor Matrix without using Partition\n  Tree", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-scale factor models are particularly appealing for analyzing\nmatrix- or tensor-valued data, due to their adaptiveness to local geometry and\nintuitive interpretation. However, the reliance on the binary tree for\nrecursive partitioning creates high complexity in the parameter space, making\nit extremely challenging to quantify its uncertainty. In this article, we\ndiscover an alternative way to generate multi-scale matrix using simple matrix\noperation: starting from a random matrix with each column having two unique\nvalues, its Cholesky whitening transform obeys a recursive partitioning\nstructure. This allows us to consider a generative distribution with large\nprior support on common multi-scale factor models, and efficient posterior\ncomputation via Hamiltonian Monte Carlo. We demonstrate its potential in a\nmulti-scale factor model to find broader regions of interest for human brain\nconnectivity.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 02:51:28 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 18:53:15 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Xu", "Maoran", ""], ["Duan", "Leo L.", ""]]}, {"id": "2002.09633", "submitter": "Sam Brilleman", "authors": "Samuel L. Brilleman (1), Eren M. Elci (2), Jacqueline Buros Novik (3),\n  Rory Wolfe (1) ((1) Monash University, Melbourne, Australia, (2) Bayer AG,\n  Berlin, Germany, (3) Generable Inc, New York, USA)", "title": "Bayesian Survival Analysis Using the rstanarm R Package", "comments": "50 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival data is encountered in a range of disciplines, most notably health\nand medical research. Although Bayesian approaches to the analysis of survival\ndata can provide a number of benefits, they are less widely used than classical\n(e.g. likelihood-based) approaches. This may be in part due to a relative\nabsence of user-friendly implementations of Bayesian survival models. In this\narticle we describe how the rstanarm R package can be used to fit a wide range\nof Bayesian survival models. The rstanarm package facilitates Bayesian\nregression modelling by providing a user-friendly interface (users specify\ntheir model using customary R formula syntax and data frames) and using the\nStan software (a C++ library for Bayesian inference) for the back-end\nestimation. The suite of models that can be estimated using rstanarm is broad\nand includes generalised linear models (GLMs), generalised linear mixed models\n(GLMMs), generalised additive models (GAMs) and more. In this article we focus\nonly on the survival modelling functionality. This includes standard parametric\n(exponential, Weibull, Gompertz) and flexible parametric (spline-based) hazard\nmodels, as well as standard parametric accelerated failure time (AFT) models.\nAll types of censoring (left, right, interval) are allowed, as is delayed entry\n(left truncation), time-varying covariates, time-varying effects, and frailty\neffects. We demonstrate the functionality through worked examples. We\nanticipate these implementations will increase the uptake of Bayesian survival\nanalysis in applied research.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 05:39:32 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Brilleman", "Samuel L.", ""], ["Elci", "Eren M.", ""], ["Novik", "Jacqueline Buros", ""], ["Wolfe", "Rory", ""]]}, {"id": "2002.09644", "submitter": "Stephen Bates", "authors": "Stephen Bates, Matteo Sesia, Chiara Sabatti, Emmanuel Candes", "title": "Causal Inference in Genetic Trio Studies", "comments": null, "journal-ref": "Proc. Natl. Acad. Sci. U.S.A. 177 (2020) 24117-24126", "doi": "10.1073/pnas.2007743117", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to rigorously draw causal inferences---inferences\nimmune to all possible confounding---from genetic data that include parents and\noffspring. Causal conclusions are possible with these data because the natural\nrandomness in meiosis can be viewed as a high-dimensional randomized\nexperiment. We make this observation actionable by developing a novel\nconditional independence test that identifies regions of the genome containing\ndistinct causal variants. The proposed Digital Twin Test compares an observed\noffspring to carefully constructed synthetic offspring from the same parents in\norder to determine statistical significance, and it can leverage any black-box\nmultivariate model and additional non-trio genetic data in order to increase\npower. Crucially, our inferences are based only on a well-established\nmathematical description of the rearrangement of genetic material during\nmeiosis and make no assumptions about the relationship between the genotypes\nand phenotypes.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 06:40:05 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bates", "Stephen", ""], ["Sesia", "Matteo", ""], ["Sabatti", "Chiara", ""], ["Candes", "Emmanuel", ""]]}, {"id": "2002.09736", "submitter": "Mehdi Dagdoug", "authors": "Mehdi Dagdoug, Camelia Goga and David Haziza", "title": "Model-assisted estimation through random forests in finite population\n  sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In surveys, the interest lies in estimating finite population parameters such\nas population totals and means. In most surveys, some auxiliary information is\navailable at the estimation stage. This information may be incorporated in the\nestimation procedures to increase their precision. In this article, we use\nrandom forests to estimate the functional relationship between the survey\nvariable and the auxiliary variables. In recent years, random forests have\nbecome attractive as National Statistical Offices have now access to a variety\nof data sources, potentially exhibiting a large number of observations on a\nlarge number of variables. We establish the theoretical properties of\nmodel-assisted procedures based on random forests and derive corresponding\nvariance estimators. A model-calibration procedure for handling multiple survey\nvariables is also discussed. The results of a simulation study suggest that the\nproposed point and estimation procedures perform well in term of bias,\nefficiency, and coverage of normal-based confidence intervals, in a wide\nvariety of settings. Finally, we apply the proposed methods using data on radio\naudiences collected by M\\'ediam\\'etrie, a French audience company.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 17:20:13 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 10:44:00 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 12:38:14 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Dagdoug", "Mehdi", ""], ["Goga", "Camelia", ""], ["Haziza", "David", ""]]}, {"id": "2002.09868", "submitter": "Marcelo Hartmann", "authors": "Marcelo Hartmann, Georgi Agiashvili, Paul B\\\"urkner and Arto Klami", "title": "Flexible Prior Elicitation via the Prior Predictive Distribution", "comments": "24 pages, 3 figures, conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The prior distribution for the unknown model parameters plays a crucial role\nin the process of statistical inference based on Bayesian methods. However,\nspecifying suitable priors is often difficult even when detailed prior\nknowledge is available in principle. The challenge is to express quantitative\ninformation in the form of a probability distribution. Prior elicitation\naddresses this question by extracting subjective information from an expert and\ntransforming it into a valid prior. Most existing methods, however, require\ninformation to be provided on the unobservable parameters, whose effect on the\ndata generating process is often complicated and hard to understand. We propose\nan alternative approach that only requires knowledge about the observable\noutcomes - knowledge which is often much easier for experts to provide.\nBuilding upon a principled statistical framework, our approach utilizes the\nprior predictive distribution implied by the model to automatically transform\nexperts judgements about plausible outcome values to suitable priors on the\nparameters. We also provide computational strategies to perform inference and\nguidelines to facilitate practical use.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 09:09:27 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 07:33:43 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 18:38:40 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Hartmann", "Marcelo", ""], ["Agiashvili", "Georgi", ""], ["B\u00fcrkner", "Paul", ""], ["Klami", "Arto", ""]]}, {"id": "2002.09910", "submitter": "Shirin Golchi", "authors": "Shirin Golchi", "title": "Use of Historical Individual Patient Data in Analysis of Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical data from previous clinical trials, observational studies and\nhealth records may be utilized in analysis of clinical trials data to\nstrengthen inference. Under the Bayesian framework incorporation of information\nobtained from any source other than the current data is facilitated through\nconstruction of an informative prior. The existing methodology for defining an\ninformative prior based on historical data relies on measuring similarity to\nthe current data at the study level and does not take advantage of individual\npatient data (IPD). This paper proposes a family of priors that utilize IPD to\nstrengthen statistical inference. It is demonstrated that the proposed prior\nconstruction approach outperforms the existing methods where the historical\ndata are partially exchangeable with the present data. The proposed method is\napplied to IPD from a set of trials in non-small cell lung cancer.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 14:18:36 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 13:30:48 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Golchi", "Shirin", ""]]}, {"id": "2002.09968", "submitter": "Simone Giannerini", "authors": "Kung-Sik Chan, Simone Giannerini, Greta Goracci, Howell Tong", "title": "Unit-root test within a threshold ARMA framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new unit-root test based on Lagrange Multipliers, where we\nextend the null hypothesis to an integrated moving-average process (IMA(1,1))\nand the alternative to a first-order threshold autoregressive moving-average\nprocess (TARMA(1,1)). This new theoretical framework provides tests with good\nsize without pre-modelling steps. Moreover, leveraging on the versatile\ncapability of the TARMA(1,1), our test has power against a wide range of linear\nand nonlinear alternatives. We prove the consistency and asymptotic similarity\nof the test. The proof of tightness of the test is of independent and general\ntheoretical interest. Moreover, we propose a wild bootstrap version of the\nstatistic. Our proposals outperform most existing tests in many contexts. We\nsupport the view that rejection does not necessarily imply nonlinearity so that\nunit-root tests should not be used uncritically to select a model. Finally, we\npresent an application to real exchange rates.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 18:54:51 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Chan", "Kung-Sik", ""], ["Giannerini", "Simone", ""], ["Goracci", "Greta", ""], ["Tong", "Howell", ""]]}, {"id": "2002.09976", "submitter": "Donniell Fishkind", "authors": "Donniell E. Fishkind, Avanti Athreya, Lingyao Meng, Vince Lyzinski,\n  Carey E. Priebe", "title": "On a complete and sufficient statistic for the correlated Bernoulli\n  random graph model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Inference on vertex-aligned graphs is of wide theoretical and practical\nimportance.There are, however, few flexible and tractable statistical models\nfor correlated graphs, and even fewer comprehensive approaches to parametric\ninference on data arising from such graphs. In this paper, we consider the\ncorrelated Bernoulli random graph model (allowing different Bernoulli\ncoefficients and edge correlations for different pairs of vertices), and we\nintroduce a new variance-reducing technique -- called \\emph{balancing} -- that\ncan refine estimators for model parameters. Specifically, we construct a\ndisagreement statistic and show that it is complete and sufficient; balancing\ncan be interpreted as Rao-Blackwellization with this disagreement statistic. We\nshow that for unbiased estimators of functions of model parameters, balancing\ngenerates uniformly minimum variance unbiased estimators (UMVUEs). However,\neven when unbiased estimators for model parameters do {\\em not} exist -- which,\nas we prove, is the case with both the heterogeneity correlation and the total\ncorrelation parameters -- balancing is still useful, and lowers mean squared\nerror. In particular, we demonstrate how balancing can improve the efficiency\nof the alignment strength estimator for the total correlation, a parameter that\nplays a critical role in graph matchability and graph matching runtime\ncomplexity.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 19:37:22 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 18:48:16 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Fishkind", "Donniell E.", ""], ["Athreya", "Avanti", ""], ["Meng", "Lingyao", ""], ["Lyzinski", "Vince", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2002.09983", "submitter": "Jonathan Bradley", "authors": "Jonathan R. Bradley", "title": "Joint spatio-temporal analysis of multiple response types using the\n  hierarchical generalized transformation model with application to coronavirus\n  disease 2019 and social distancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social distancing can be described as an effort to maintain a physical\ndistance between individuals and has become a necessary public health measure\nto combat cornoavirus disease 2019 (COVID-19). Social distancing is known to\nweaken incidences and deaths due to COVID-19, however, there are detrimental\neconomic and psychological effects. This motivates us to analyze incidences\n(and deaths) of COVID-19 along with a measure of the health of the US economy\n(i.e., the adjusted closing price of the Dow Jones Industrial), and a measure\nof the public interest in COVID-19 through Google Trends data. The model we\nimplement is developed to be easily adapted to a data scientist's preferred\nmethod for continuous data, which is done to aid future analyses of this\nimportant dataset. This dataset consists of multiple response types (e.g.,\ncontinuous-valued, count-valued, binomial counts). Thus, we introduce a\nreasonable easy-to-implement all-purpose method that \"converts\" a statistical\nmodel for continuous responses (the preferred model) into a Bayesian model for\nmulti-response data sets. To do this, we transform the data such that the\ncontinuous-valued transformed data can be reasonably modeled using the\npreferred model and the transformation itself is treated as unknown. The\nimplementation of our approach involves two steps. The first step produces\nposterior replicates of the transformed data using a latent conjugate\nmultivariate (LCM) model. The second step involves generating values from the\nposterior distribution implied by the preferred model. We refer to our model as\nthe hierarchical generalized transformation (HGT) model. In a simulation, we\ndemonstrate the flexibility of the HGT model by incorporating two different\npreferred models: Bayesian additive regression trees (BART) and the spatial\nmixed effects (spatio-temporal mixed effects) models.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 20:43:58 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 15:00:35 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 00:53:57 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Bradley", "Jonathan R.", ""]]}, {"id": "2002.09998", "submitter": "Ayman Boustati", "authors": "Ayman Boustati, \\\"Omer Deniz Akyildiz, Theodoros Damoulas, Adam M.\n  Johansen", "title": "Generalized Bayesian Filtering via Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for inference in general state-space hidden Markov\nmodels (HMMs) under likelihood misspecification. In particular, we leverage the\nloss-theoretic perspective of Generalized Bayesian Inference (GBI) to define\ngeneralised filtering recursions in HMMs, that can tackle the problem of\ninference under model misspecification. In doing so, we arrive at principled\nprocedures for robust inference against observation contamination by utilising\nthe $\\beta$-divergence. Operationalising the proposed framework is made\npossible via sequential Monte Carlo methods (SMC), where most standard particle\nmethods, and their associated convergence results, are readily adapted to the\nnew setting. We apply our approach to object tracking and Gaussian process\nregression problems, and observe improved performance over both standard\nfiltering algorithms and other robust filters.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 22:15:52 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 15:05:58 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Boustati", "Ayman", ""], ["Akyildiz", "\u00d6mer Deniz", ""], ["Damoulas", "Theodoros", ""], ["Johansen", "Adam M.", ""]]}, {"id": "2002.10038", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Bayesian bandwidth estimation and semi-metric selection for a functional\n  partial linear model with unknown error density", "comments": "27 pages, 10 figures, to appear in Journal of Applied Statistics", "journal-ref": "Journal of Applied Statistics (2020)", "doi": "10.1080/02664763.2020.1736527", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study examines the optimal selections of bandwidth and semi-metric for a\nfunctional partial linear model. Our proposed method begins by estimating the\nunknown error density using a kernel density estimator of residuals, where the\nregression function, consisting of parametric and nonparametric components, can\nbe estimated by functional principal component and functional Nadayara-Watson\nestimators. The estimation accuracy of the regression function and error\ndensity crucially depends on the optimal estimations of bandwidth and\nsemi-metric. A Bayesian method is utilized to simultaneously estimate the\nbandwidths in the regression function and kernel error density by minimizing\nthe Kullback-Leibler divergence. For estimating the regression function and\nerror density, a series of simulation studies demonstrate that the functional\npartial linear model gives improved estimation and forecast accuracies compared\nwith the functional principal component regression and functional nonparametric\nregression. Using a spectroscopy dataset, the functional partial linear model\nyields better forecast accuracy than some commonly used functional regression\nmodels. As a by-product of the Bayesian method, a pointwise prediction interval\ncan be obtained, and marginal likelihood can be used to select the optimal\nsemi-metric.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 02:15:34 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "2002.10046", "submitter": "Anderson Winkler", "authors": "Anderson M. Winkler, Olivier Renaud, Stephen M. Smith, Thomas E.\n  Nichols", "title": "Permutation Inference for Canonical Correlation Analysis", "comments": "49 pages, 2 figures, 10 tables, 3 algorithms, 119 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) has become a key tool for population\nneuroimaging, allowing investigation of associations between many imaging and\nnon-imaging measurements. As other variables are often a source of variability\nnot of direct interest, previous work has used CCA on residuals from a model\nthat removes these effects, then proceeded directly to permutation inference.\nWe show that such a simple permutation test leads to inflated error rates. The\nreason is that residualisation introduces dependencies among the observations\nthat violate the exchangeability assumption. Even in the absence of nuisance\nvariables, however, a simple permutation test for CCA also leads to excess\nerror rates for all canonical correlations other than the first. The reason is\nthat a simple permutation scheme does not ignore the variability already\nexplained by previous canonical variables. Here we propose solutions for both\nproblems: in the case of nuisance variables, we show that transforming the\nresiduals to a lower dimensional basis where exchangeability holds results in a\nvalid permutation test; for more general cases, with or without nuisance\nvariables, we propose estimating the canonical correlations in a stepwise\nmanner, removing at each iteration the variance already explained, while\ndealing with different number of variables in both sides. We also discuss how\nto address the multiplicity of tests, proposing an admissible test that is not\nconservative, and provide a complete algorithm for permutation inference for\nCCA.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 02:47:01 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 22:45:59 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 18:23:36 GMT"}, {"version": "v4", "created": "Thu, 18 Jun 2020 01:15:58 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Winkler", "Anderson M.", ""], ["Renaud", "Olivier", ""], ["Smith", "Stephen M.", ""], ["Nichols", "Thomas E.", ""]]}, {"id": "2002.10091", "submitter": "Debo Cheng", "authors": "Debo Cheng (1), Jiuyong Li (1), Lin Liu (1), Kui Yu (2), Thuc Duy Lee\n  (1), Jixue Liu (1) ((1) School of Information Technology and Mathematical\n  Sciences, University of South Australia (2) School of Computer Science and\n  Information Engineering, Hefei University of Technology)", "title": "Towards unique and unbiased causal effect estimation from data with\n  hidden variables", "comments": "12 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal effect estimation from observational data is a crucial but challenging\ntask. Currently, only a limited number of data-driven causal effect estimation\nmethods are available. These methods either provide only a bound estimation of\nthe causal effect of a treatment on the outcome, or generate a unique\nestimation of the causal effect, but making strong assumptions on data and\nhaving low efficiency. In this paper, we identify a practical problem setting\nand propose an approach to achieving unique and unbiased estimation of causal\neffects from data with hidden variables. For the approach, we have developed\nthe theorems to support the discovery of the proper covariate sets for\nconfounding adjustment (adjustment sets). Based on the theorems, two algorithms\nare proposed for finding the proper adjustment sets from data with hidden\nvariables to obtain unbiased and unique causal effect estimation. Experiments\nwith synthetic datasets generated using five benchmark Bayesian networks and\nfour real-world datasets have demonstrated the efficiency and effectiveness of\nthe proposed algorithms, indicating the practicability of the identified\nproblem setting and the potential of the proposed approach in real-world\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 06:42:32 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 23:28:15 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Cheng", "Debo", ""], ["Li", "Jiuyong", ""], ["Liu", "Lin", ""], ["Yu", "Kui", ""], ["Lee", "Thuc Duy", ""], ["Liu", "Jixue", ""]]}, {"id": "2002.10135", "submitter": "Alexander McNeil", "authors": "Alexander J. McNeil", "title": "Modelling volatile time series with v-transforms and copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An approach to the modelling of volatile time series using a class of\nuniformity-preserving transforms for uniform random variables is proposed.\nV-transforms describe the relationship between quantiles of the stationary\ndistribution of the time series and quantiles of the distribution of a\npredictable volatility proxy variable. They can be represented as copulas and\npermit the formulation and estimation of models that combine arbitrary marginal\ndistributions with copula processes for the dynamics of the volatility proxy.\nThe idea is illustrated using a Gaussian ARMA copula process and the resulting\nmodel is shown to replicate many of the stylized facts of financial return\nseries and to facilitate the calculation of marginal and conditional\ncharacteristics of the model including quantile measures of risk. Estimation is\ncarried out by adapting the exact maximum likelihood approach to the estimation\nof ARMA processes and the model is shown to be competitive with standard GARCH\nin an empirical application to Bitcoin return data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 10:00:38 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 16:51:52 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 14:28:00 GMT"}, {"version": "v4", "created": "Tue, 24 Nov 2020 17:08:22 GMT"}, {"version": "v5", "created": "Tue, 12 Jan 2021 18:10:08 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["McNeil", "Alexander J.", ""]]}, {"id": "2002.10251", "submitter": "Jinqiao Duan", "authors": "Jian Ren and Jinqiao Duan", "title": "Identifying stochastic governing equations from data of the most\n  probable transition trajectories", "comments": "23 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA physics.comp-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting governing stochastic differential equation models from elusive\ndata is crucial to understand and forecast dynamics for complex systems. We\ndevise a method to extract the drift term and estimate the diffusion\ncoefficient of a governing stochastic dynamical system, from its time-series\ndata of the most probable transition trajectory. By the Onsager-Machlup theory,\nthe most probable transition trajectory satisfies the corresponding\nEuler-Lagrange equation, which is a second order deterministic ordinary\ndifferential equation involving the drift term and diffusion coefficient. We\nfirst estimate the coefficients of the Euler-Lagrange equation based on the\ndata of the most probable trajectory, and then we calculate the drift and\ndiffusion coefficients of the governing stochastic dynamical system. These two\nsteps involve sparse regression and optimization. Finally, we illustrate our\nmethod with an example and some discussions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 14:51:22 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 01:24:14 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Ren", "Jian", ""], ["Duan", "Jinqiao", ""]]}, {"id": "2002.10270", "submitter": "Marc Schneble", "authors": "Marc Schneble and G\\\"oran Kauermann", "title": "Intensity Estimation on Geometric Networks with Penalized Splines", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades, the growing amount of network data has lead to many\nnovel statistical models. In this paper we consider so called geometric\nnetworks. Typical examples are road networks or other infrastructure networks.\nBut also the neurons or the blood vessels in a human body can be interpreted as\na geometric network embedded in a three-dimensional space. In all these\napplications a network specific metric rather than the Euclidean metric is\nusually used, which makes the analyses on network data challenging. We consider\nnetwork based point processes and our task is to estimate the intensity (or\ndensity) of the process which allows to detect high- and low- intensity regions\nof the underlying stochastic processes. Available routines that tackle this\nproblem are commonly based on kernel smoothing methods. However, kernel based\nestimation in general exhibits some drawbacks such as suffering from boundary\neffects and the locality of the smoother. In an Euclidean space, the\ndisadvantages of kernel methods can be overcome by using penalized spline\nsmoothing. We here extend penalized spline smoothing towards smooth intensity\nestimation on geometric networks and apply the approach to both, simulated and\nreal world data. The results show that penalized spline based intensity\nestimation is numerically efficient and outperforms kernel based methods.\nFurthermore, our approach easily allows to incorporate covariates, which allows\nto respect the network geometry in a regression model framework.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 14:04:14 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Schneble", "Marc", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "2002.10281", "submitter": "Thorsten Dickhaus", "authors": "Nico Steffen and Thorsten Dickhaus", "title": "Optimizing effective numbers of tests by vine copula modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the multiple testing context, we utilize vine copulae for optimizing the\neffective number of tests. It is well known that for the calibration of\nmultiple tests (for control of the family-wise error rate) the dependencies\nbetween the marginal tests are of utmost importance. It has been shown in\nprevious work, that positive dependencies between the marginal tests can be\nexploited in order to derive a relaxed Sidak-type multiplicity correction. This\ncorrection can conveniently be expressed by calculating the corresponding\n\"effective number of tests\" for a given (global) significance level. This\nmethodology can also be applied to blocks of test statistics so that the\neffective number of tests can be calculated by the sum of the effective numbers\nof tests for each block. In the present work, we demonstrate how the power of\nthe multiple test can be optimized by taking blocks with high inner-block\ndependencies. The determination of those blocks will be performed by means of\nan estimated vine copula model. An algorithm is presented which uses the\ninformation of the estimated vine copula to make a data-driven choice of\nappropriate blocks in terms of (estimated) dependencies. Numerical experiments\ndemonstrate the usefulness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 14:34:08 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Steffen", "Nico", ""], ["Dickhaus", "Thorsten", ""]]}, {"id": "2002.10335", "submitter": "Fabio Rapallo", "authors": "Giovanni Pistone, Fabio Rapallo, Maria Piera Rogantin", "title": "Finite space Kantorovich problem with an MCMC of table moves", "comments": "25 pages; a proof has been added and some notational issues have been\n  fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Optimal Transport (OT) on a finite metric space, one defines a distance on\nthe probability simplex that extends the distance on the ground space. The\ndistance is the value of a Linear Programming (LP) problem on the set of\nnon-negative-valued 2-way tables with assigned probability functions as\nmargins. We apply to this case the methodology of moves from Algebraic\nStatistics (AS) and use it to derive a Monte Carlo Markov Chain (MCMC) solution\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 16:05:39 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 16:20:37 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 14:18:06 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Pistone", "Giovanni", ""], ["Rapallo", "Fabio", ""], ["Rogantin", "Maria Piera", ""]]}, {"id": "2002.10399", "submitter": "Niccol\\`o Dalmasso", "authors": "Niccol\\`o Dalmasso and Rafael Izbicki and Ann B. Lee", "title": "Confidence Sets and Hypothesis Testing in a Likelihood-Free Inference\n  Setting", "comments": "20 pages, 8 figures, 6 tables, 4 algorithm boxes", "journal-ref": "Proceedings of the 37th International Conference on Machine\n  Learning, PMLR 119:2323-2334, 2020", "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation, statistical tests and confidence sets are the\ncornerstones of classical statistics that allow scientists to make inferences\nabout the underlying process that generated the observed data. A key question\nis whether one can still construct hypothesis tests and confidence sets with\nproper coverage and high power in a so-called likelihood-free inference (LFI)\nsetting; that is, a setting where the likelihood is not explicitly known but\none can forward-simulate observable data according to a stochastic model. In\nthis paper, we present $\\texttt{ACORE}$ (Approximate Computation via Odds Ratio\nEstimation), a frequentist approach to LFI that first formulates the classical\nlikelihood ratio test (LRT) as a parametrized classification problem, and then\nuses the equivalence of tests and confidence sets to build confidence regions\nfor parameters of interest. We also present a goodness-of-fit procedure for\nchecking whether the constructed tests and confidence regions are valid.\n$\\texttt{ACORE}$ is based on the key observation that the LRT statistic, the\nrejection probability of the test, and the coverage of the confidence set are\nconditional distribution functions which often vary smoothly as a function of\nthe parameters of interest. Hence, instead of relying solely on samples\nsimulated at fixed parameter settings (as is the convention in standard Monte\nCarlo solutions), one can leverage machine learning tools and data simulated in\nthe neighborhood of a parameter to improve estimates of quantities of interest.\nWe demonstrate the efficacy of $\\texttt{ACORE}$ with both theoretical and\nempirical results. Our implementation is available on Github.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 17:34:49 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 02:56:38 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Dalmasso", "Niccol\u00f2", ""], ["Izbicki", "Rafael", ""], ["Lee", "Ann B.", ""]]}, {"id": "2002.10436", "submitter": "Qingyuan Zhao", "authors": "Bo Zhang, Jordan Weiss, Dylan S Small, Qingyuan Zhao", "title": "Selecting and ranking individualized treatment rules with unmeasured\n  confounding", "comments": "33 pages, accepted manuscript (by Journal of the American Statistical\n  Association)", "journal-ref": null, "doi": "10.1080/01621459.2020.1736083", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to compare individualized treatment rules based on the value\nfunction, which is the expected potential outcome under the treatment rule.\nAlthough the value function is not point-identified when there is unmeasured\nconfounding, it still defines a partial order among the treatment rules under\nRosenbaum's sensitivity analysis model. We first consider how to compare two\ntreatment rules with unmeasured confounding in the single-decision setting and\nthen use this pairwise test to rank multiple treatment rules. We consider how\nto, among many treatment rules, select the best rules and select the rules that\nare better than a control rule. The proposed methods are illustrated using two\nreal examples, one about the benefit of malaria prevention programs to\ndifferent age groups and another about the effect of late retirement on senior\nhealth in different gender and occupation groups.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 18:33:03 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Zhang", "Bo", ""], ["Weiss", "Jordan", ""], ["Small", "Dylan S", ""], ["Zhao", "Qingyuan", ""]]}, {"id": "2002.10519", "submitter": "Erin Gabriel", "authors": "Erin E. Gabriel and Michael C. Sachs and Arvid Sj\\\"olander", "title": "Causal bounds for outcome-dependent sampling in observational studies", "comments": "36 pages, 3 figures. Update to include revisions after peer review.\n  In Press at the Journal of the American Statistical Association, Theory and\n  methods", "journal-ref": null, "doi": "10.1080/01621459.2020.1832502", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outcome-dependent sampling designs are common in many different scientific\nfields including epidemiology, ecology, and economics. As with all\nobservational studies, such designs often suffer from unmeasured confounding,\nwhich generally precludes the nonparametric identification of causal effects.\nNonparametric bounds can provide a way to narrow the range of possible values\nfor a nonidentifiable causal effect without making additional untestable\nassumptions. The nonparametric bounds literature has almost exclusively focused\non settings with random sampling, and the bounds have often been derived with a\nparticular linear programming method. We derive novel bounds for the causal\nrisk difference, often referred to as the average treatment effect, in six\nsettings with outcome-dependent sampling and unmeasured confounding for a\nbinary outcome and exposure. Our derivations of the bounds illustrate two\napproaches that may be applicable in other settings where the bounding problem\ncannot be directly stated as a system of linear constraints. We illustrate our\nderived bounds in a real data example involving the effect of vitamin D\nconcentration on mortality.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 20:21:28 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 10:38:11 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Gabriel", "Erin E.", ""], ["Sachs", "Michael C.", ""], ["Sj\u00f6lander", "Arvid", ""]]}, {"id": "2002.10645", "submitter": "Marius Hofert", "authors": "Marius Hofert, Avinash Prasad, Mu Zhu", "title": "Multivariate time-series modeling with generative neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative moment matching networks (GMMNs) are introduced as dependence\nmodels for the joint innovation distribution of multivariate time series (MTS).\nFollowing the popular copula-GARCH approach for modeling dependent MTS data, a\nframework based on a GMMN-GARCH approach is presented. First, ARMA-GARCH models\nare utilized to capture the serial dependence within each univariate marginal\ntime series. Second, if the number of marginal time series is large, principal\ncomponent analysis (PCA) is used as a dimension-reduction step. Last, the\nremaining cross-sectional dependence is modeled via a GMMN, the main\ncontribution of this work. GMMNs are highly flexible and easy to simulate from,\nwhich is a major advantage over the copula-GARCH approach. Applications\ninvolving yield curve modeling and the analysis of foreign exchange-rate\nreturns demonstrate the utility of the GMMN-GARCH approach, especially in terms\nof producing better empirical predictive distributions and making better\nprobabilistic forecasts.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 03:26:52 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 14:54:36 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 05:21:52 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hofert", "Marius", ""], ["Prasad", "Avinash", ""], ["Zhu", "Mu", ""]]}, {"id": "2002.10709", "submitter": "Arkopal Choudhury", "authors": "Arkopal Choudhury and Michael R. Kosorok", "title": "Missing Data Imputation for Classification Problems", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imputation of missing data is a common application in various classification\nproblems where the feature training matrix has missingness. A widely used\nsolution to this imputation problem is based on the lazy learning technique,\n$k$-nearest neighbor (kNN) approach. However, most of the previous work on\nmissing data does not take into account the presence of the class label in the\nclassification problem. Also, existing kNN imputation methods use variants of\nMinkowski distance as a measure of distance, which does not work well with\nheterogeneous data. In this paper, we propose a novel iterative kNN imputation\ntechnique based on class weighted grey distance between the missing datum and\nall the training data. Grey distance works well in heterogeneous data with\nmissing instances. The distance is weighted by Mutual Information (MI) which is\na measure of feature relevance between the features and the class label. This\nensures that the imputation of the training data is directed towards improving\nclassification performance. This class weighted grey kNN imputation algorithm\ndemonstrates improved performance when compared to other kNN imputation\nalgorithms, as well as standard imputation algorithms such as MICE and\nmissForest, in imputation and classification problems. These problems are based\non simulated scenarios and UCI datasets with various rates of missingness.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 07:48:45 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Choudhury", "Arkopal", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "2002.10837", "submitter": "Imke Mayer", "authors": "Imke Mayer, Julie Josse, F\\'elix Raimundo, Jean-Philippe Vert", "title": "MissDeepCausal: Causal Inference from Incomplete Data Using Deep Latent\n  Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring causal effects of a treatment, intervention or policy from\nobservational data is central to many applications. However, state-of-the-art\nmethods for causal inference seldom consider the possibility that covariates\nhave missing values, which is ubiquitous in many real-world analyses. Missing\ndata greatly complicate causal inference procedures as they require an adapted\nunconfoundedness hypothesis which can be difficult to justify in practice. We\ncircumvent this issue by considering latent confounders whose distribution is\nlearned through variational autoencoders adapted to missing values. They can be\nused either as a pre-processing step prior to causal inference but we also\nsuggest to embed them in a multiple imputation strategy to take into account\nthe variability due to missing values. Numerical experiments demonstrate the\neffectiveness of the proposed methodology especially for non-linear models\ncompared to competitors.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 12:58:07 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Mayer", "Imke", ""], ["Josse", "Julie", ""], ["Raimundo", "F\u00e9lix", ""], ["Vert", "Jean-Philippe", ""]]}, {"id": "2002.10883", "submitter": "Guosheng Yin", "authors": "Guosheng Yin and Haolun Shi", "title": "Demystify Lindley's Paradox by Interpreting P-value as Posterior\n  Probability", "comments": "arXiv admin note: text overlap with arXiv:1809.08503", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the hypothesis testing framework, p-value is often computed to determine\nrejection of the null hypothesis or not. On the other hand, Bayesian approaches\ntypically compute the posterior probability of the null hypothesis to evaluate\nits plausibility. We revisit Lindley's paradox (Lindley, 1957) and demystify\nthe conflicting results between Bayesian and frequentist hypothesis testing\nprocedures by casting a two-sided hypothesis as a combination of two one-sided\nhypotheses along the opposite directions. This can naturally circumvent the\nambiguities of assigning a point mass to the null and choices of using local or\nnon-local prior distributions. As p-value solely depends on the observed data\nwithout incorporating any prior information, we consider non-informative prior\ndistributions for fair comparisons with p-value. The equivalence of p-value and\nthe Bayesian posterior probability of the null hypothesis can be established to\nreconcile Lindley's paradox. Extensive simulation studies are conducted with\nmultivariate normal data and random effects models to examine the relationship\nbetween the p-value and posterior probability.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 05:18:03 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Yin", "Guosheng", ""], ["Shi", "Haolun", ""]]}, {"id": "2002.10902", "submitter": "Owen Thomas", "authors": "Owen Thomas, Henri Pesonen, Jukka Corander", "title": "Probabilistic elicitation of expert knowledge through assessment of\n  computer simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for probabilistic elicitation of expert knowledge\nusing binary responses of human experts assessing simulated data from a\nstatistical model, where the parameters are subject to uncertainty. The binary\nresponses describe either the absolute realism of individual simulations or the\nrelative realism of a pair of simulations in the two alternative versions of\nout approach. Each version provides a nonparametric representation of the\nexpert belief distribution over the values of a model parameter, without\ndemanding the assertion of any opinion on the parameter values themselves. Our\nframework also integrates the use of active learning to efficiently query the\nexperts, with the possibility to additionally provide a useful misspecification\ndiagnostic. We validate both methods on an automatic expert judging a binomial\ndistribution, and on human experts judging the distribution of voters across\npolitical parties in the United States and Norway. Both methods provide\nflexible and meaningful representations of the human experts' beliefs,\ncorrectly identifying the higher dispersion of voters between parties in\nNorway.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 14:42:48 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 13:28:23 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 14:18:20 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Thomas", "Owen", ""], ["Pesonen", "Henri", ""], ["Corander", "Jukka", ""]]}, {"id": "2002.10962", "submitter": "Matteo Quartagno", "authors": "Matteo Quartagno, James R. Carpenter, A. Sarah Walker, Michelle\n  Clements, Mahesh K.B. Parmar", "title": "The DURATIONS randomised trial design: estimation targets, analysis\n  methods and operating characteristics", "comments": "4 figures, 1 table + additional material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. Designing trials to reduce treatment duration is important in\nseveral therapeutic areas, including TB and antibiotics. We recently proposed a\nnew randomised trial design to overcome some of the limitations of standard\ntwo-arm non-inferiority trials. This DURATIONS design involves randomising\npatients to a number of duration arms, and modelling the so-called\nduration-response curve. This article investigates the operating\ncharacteristics (type-1 and type-2 errors) of different statistical methods of\ndrawing inference from the estimated curve. Methods. Our first estimation\ntarget is the shortest duration non-inferior to the control (maximum) duration\nwithin a specific risk difference margin. We compare different methods of\nestimating this quantity, including using model confidence bands, the delta\nmethod and bootstrap. We then explore the generalisability of results to\nestimation targets which focus on absolute event rates, risk ratio and gradient\nof the curve. Results. We show through simulations that, in most scenarios and\nfor most of the estimation targets, using the bootstrap to estimate variability\naround the target duration leads to good results for DURATIONS\ndesign-appropriate quantities analogous to power and type-1 error. Using model\nconfidence bands is not recommended, while the delta method leads to inflated\ntype-1 error in some scenarios, particularly when the optimal duration is very\nclose to one of the randomised durations. Conclusions. Using the bootstrap to\nestimate the optimal duration in a DURATIONS design has good operating\ncharacteristics in a wide range of scenarios, and can be used with confidence\nby researchers wishing to design a DURATIONS trial to reduce treatment\nduration. Uncertainty around several different targets can be estimated with\nthis bootstrap approach.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 15:28:23 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Quartagno", "Matteo", ""], ["Carpenter", "James R.", ""], ["Walker", "A. Sarah", ""], ["Clements", "Michelle", ""], ["Parmar", "Mahesh K. B.", ""]]}, {"id": "2002.10975", "submitter": "Dimas Abreu Archanjo Dutra", "authors": "Dimas Abreu Archanjo Dutra", "title": "Uncertainty estimation in equality-constrained MAP and maximum\n  likelihood estimation with applications to system identification and state\n  estimation", "comments": null, "journal-ref": "Automatica 116 (June 2020), 108935", "doi": "10.1016/j.automatica.2020.108935", "report-no": null, "categories": "stat.ME math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unconstrained maximum a posteriori (MAP) and maximum likelihood\nestimation, the inverse of minus the merit-function Hessian matrix is an\napproximation of the estimate covariance matrix. In the Bayesian context of MAP\nestimation, it is the covariance of a normal approximation of the posterior\naround the mode; while in maximum likelihood estimation, it an approximation of\nthe inverse Fisher information matrix, to which the covariance of efficient\nestimators converge. These measures are routinely used in system identification\nto evaluate the estimate uncertainties and diagnose problems such as\noverparametrization, improper excitation and unidentifiability. A wide variety\nof estimation problems in systems and control, however, can be formulated as\nequality-constrained optimizations with additional decision variables to\nexploit parallelism in computer hardware, simplify implementation and increase\nthe convergence basin and efficiency of the nonlinear program solver. The\nintroduction of the extra variables, however, dissociates the inverse Hessian\nfrom the covariance matrix. Instead, submatrices of the inverse Hessian of the\nconstrained-problem's Lagrangian must be used. In this paper, we derive these\nrelationships, showing how the estimates' covariance can be estimated directly\nfrom the augmented problem. Application examples are shown in system\nidentification with the output-error method and joint state-path and parameter\nestimation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 15:39:33 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 17:18:24 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Dutra", "Dimas Abreu Archanjo", ""]]}, {"id": "2002.10997", "submitter": "Sina Mews", "authors": "Sina Mews, Roland Langrock, Ruth King, Nicola Quick", "title": "Continuous-time multi-state capture-recapture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-state capture-recapture data comprise individual-specific sighting\nhistories together with information on individuals' states related, for\nexample, to breeding status, infection level, or geographical location. Such\ndata are often analysed using the Arnason-Schwarz model, where transitions\nbetween states are modelled using a discrete-time Markov chain, making the\nmodel most easily applicable to regular time series. When time intervals\nbetween capture occasions are not of equal length, more complex time-dependent\nconstructions may be required, increasing the number of parameters to estimate,\ndecreasing interpretability, and potentially leading to reduced precision. Here\nwe develop a novel continuous-time multi-state model that can be regarded as an\nanalogue of the Arnason-Schwarz model for irregularly sampled data. Statistical\ninference is carried out by regarding the capture-recapture data as\nrealisations from a continuous-time hidden Markov model, which allows the\nassociated efficient algorithms to be used for maximum likelihood estimation\nand state decoding. To illustrate the feasibility of the modelling framework,\nwe use a long-term survey of bottlenose dolphins where capture occasion are not\nregularly spaced through time. Here we are particularly interested in seasonal\neffects on the movement rates of the dolphins along the Scottish east coast.\nThe results reveal seasonal movement patterns between two core areas of their\nrange, providing information that will inform conservation management.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 16:55:26 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Mews", "Sina", ""], ["Langrock", "Roland", ""], ["King", "Ruth", ""], ["Quick", "Nicola", ""]]}, {"id": "2002.11204", "submitter": "Maqsood Ali", "authors": "Maqsood Ali, Abdul Haq, Muhammad Aslam", "title": "Classical and Bayesian Analyses of a Mixture of Exponential and Lomax\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential and the Lomax distributions are widely used in life testing\nexperiments in mixture models. A mixture model of exponential distribution and\nLomax distribution is proposed. Parameters of the proposed model are estimated\nusing classical and Bayesian procedures under type-I right censoring.\nExpressions for Bayes estimators are derived assuming noninformative (uniform\nand Jeffreys) priors under symmetric and asymmetric loss functions. Posterior\npredictive distributions of a future observation are derived and predictive\nestimates are obtained. Extensive Monte Carlo simulations are carried out to\ninvestigate performance of the estimators in terms of sample sizes, censoring\ntimes and mixing proportions. The analysis of mixture model is carried out\nusing a data set of lifetime of transmitter receivers. Interesting properties\nof estimators are observed and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 22:25:02 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Ali", "Maqsood", ""], ["Haq", "Abdul", ""], ["Aslam", "Muhammad", ""]]}, {"id": "2002.11236", "submitter": "Maqsood Ali", "authors": "Maqsood Ali, Muhammad Aslam", "title": "Paired Comparisons Modeling using t-Distribution with Bayesian Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A paired comparison analysis is the simplest way to make comparative\njudgments between objects where objects may be goods, services or skills. For a\nset of problems, this technique helps to choose the most important problem to\nsolve first and/or provides the solution that will be the most effective. This\npaper presents the theory of paired comparisons method and contributes to the\npaired comparisons models by developing a new model based on t-distribution.\nThe developed model is illustrated using a data set of citations among four\nfamous journals of Statistics. Using Bayesian analysis, the journals are ranked\nas JRSS-B --> Biometrika --> JASA --> Comm. in Stats.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 00:45:12 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Ali", "Maqsood", ""], ["Aslam", "Muhammad", ""]]}, {"id": "2002.11255", "submitter": "Rungang Han", "authors": "Rungang Han, Rebecca Willett and Anru R. Zhang", "title": "An Optimal Statistical and Computational Framework for Generalized\n  Tensor Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a flexible framework for generalized low-rank tensor\nestimation problems that includes many important instances arising from\napplications in computational imaging, genomics, and network analysis. The\nproposed estimator consists of finding a low-rank tensor fit to the data under\ngeneralized parametric models. To overcome the difficulty of non-convexity in\nthese problems, we introduce a unified approach of projected gradient descent\nthat adapts to the underlying low-rank structure. Under mild conditions on the\nloss function, we establish both an upper bound on statistical error and the\nlinear rate of computational convergence through a general deterministic\nanalysis. Then we further consider a suite of generalized tensor estimation\nproblems, including sub-Gaussian tensor PCA, tensor regression, and Poisson and\nbinomial tensor PCA. We prove that the proposed algorithm achieves the minimax\noptimal rate of convergence in estimation error. Finally, we demonstrate the\nsuperiority of the proposed framework via extensive experiments on both\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 01:54:35 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 21:55:11 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Han", "Rungang", ""], ["Willett", "Rebecca", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2002.11276", "submitter": "Guillaume Martinet", "authors": "Guillaume Martinet", "title": "A Balancing Weight Framework for Estimating the Causal Effect of General\n  Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, weighting methods that directly optimize the\nbalance between treatment and covariates have received much attention lately;\nhowever these have mainly focused on binary treatments. Inspired by domain\nadaptation, we show that such methods can be actually reformulated as specific\nimplementations of a discrepancy minimization problem aimed at tackling a shift\nof distribution from observational to interventional data. More precisely, we\nintroduce a new framework, Covariate Balance via Discrepancy Minimization\n(CBDM), that provably encompasses most of the existing balancing weight methods\nand formally extends them to treatments of arbitrary types (e.g., continuous or\nmultivariate). We establish theoretical guarantees for our framework that both\noffer generalizations of properties known when the treatment is binary, and\ngive a better grasp on what hyperparameters to choose in non-binary settings.\nBased on such insights, we propose a particular implementation of CBDM for\nestimating dose-response curves and demonstrate through experiments its\ncompetitive performance relative to other existing approaches for continuous\ntreatments.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 03:24:20 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Martinet", "Guillaume", ""]]}, {"id": "2002.11610", "submitter": "Bruce Hoadley", "authors": "Bruce Hoadley", "title": "Liquid Scorecards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional credit scorecards are generalized additive models (GAMs) with\nstep functions as the component functions. The shapes of the step functions may\nbe constrained in order to satisfy the PILE (Palatability, Interpretability,\nLegal, Explain-ability) constraints. Before 2003, FICO used Linear Programming\nto find the traditional scorecard that approximately maximizes divergence\nsubject to the PILE constraints. In this paper, I introduce the Liquid\nScorecard, that allows the component functions to be, at least partially,\nsmooth curves. I use Quadratic Programming and B-Spline theory to find the\nLiquid Scorecard that exactly maximizes divergence subject to the PILE\nconstraints. FICO uses aspects of this technology to develop the famous FICO\nCredit Score.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 18:19:02 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Hoadley", "Bruce", ""]]}, {"id": "2002.11618", "submitter": "Till Koebe", "authors": "Till Koebe", "title": "Better coverage, better outcomes? Mapping mobile network data to\n  official statistics using satellite imagery and radio propagation modelling", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0241981", "report-no": null, "categories": "cs.CY stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile sensing data has become a popular data source for geo-spatial\nanalysis, however, mapping it accurately to other sources of information such\nas statistical data remains a challenge. Popular mapping approaches such as\npoint allocation or voronoi tessellation provide only crude approximations of\nthe mobile network coverage as they do not consider holes, overlaps and\nwithin-cell heterogeneity. More elaborate mapping schemes often require\nadditional proprietary data operators are highly reluctant to share. In this\npaper, I use human settlement information extracted from publicly available\nsatellite imagery in combination with stochastic radio propagation modelling\ntechniques to account for that. I investigate in a simulation study and a\nreal-world application on unemployment estimates in Senegal whether better\ncoverage approximations lead to better outcome predictions. The good news is:\nit does not have to be complicated.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 14:19:19 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Koebe", "Till", ""]]}, {"id": "2002.11846", "submitter": "Aaron Sarvet", "authors": "Aaron L. Sarvet, Kerollos N. Wanis, Jessica Young, Roberto\n  Hernandez-Alejandro, Miguel A. Hern\\'an, Mats J. Stensrud", "title": "Causal inference with limited resources: proportionally-representative\n  interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigators often evaluate treatment effects by considering settings in\nwhich all individuals are assigned a treatment of interest, assuming that an\nunlimited number of treatment units are available. However, many real-life\ntreatments are of limited supply and cannot be provided to all individuals in\nthe population. For example, patients on the liver transplant waiting list\ncannot be assigned a liver transplant immediately at the time they reach\nhighest priority because a suitable organ is not likely to be immediately\navailable. In these cases, investigators may still be interested in the effects\nof treatment strategies in which a finite number of organs are available at a\ngiven time, that is, treatment regimes that satisfy resource constraints. Here,\nwe describe an estimand that can be used to define causal effects of treatment\nstrategies that satisfy resource constraints: proportionally-representative\ninterventions for limited resources. We derive a simple class of inverse\nprobability weighted estimators, and apply one such estimator to evaluate the\neffect of restricting or expanding utilization of \"increased risk\" liver organs\nto treat patients with end-stage liver disease. Our method is designed to\nevaluate policy-relevant interventions in the setting of finite treatment\nresources.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 00:03:20 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 15:53:00 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Sarvet", "Aaron L.", ""], ["Wanis", "Kerollos N.", ""], ["Young", "Jessica", ""], ["Hernandez-Alejandro", "Roberto", ""], ["Hern\u00e1n", "Miguel A.", ""], ["Stensrud", "Mats J.", ""]]}, {"id": "2002.11916", "submitter": "Shih-Ting Huang", "authors": "Shih-Ting Huang, Fang Xie, and Johannes Lederer", "title": "Tuning-free ridge estimators for high-dimensional generalized linear\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge estimators regularize the squared Euclidean lengths of parameters. Such\nestimators are mathematically and computationally attractive but involve tuning\nparameters that can be difficult to calibrate. In this paper, we show that\nridge estimators can be modified such that tuning parameters can be avoided\naltogether. We also show that these modified versions can improve on the\nempirical prediction accuracies of standard ridge estimators combined with\ncross-validation, and we provide first theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 05:01:42 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Huang", "Shih-Ting", ""], ["Xie", "Fang", ""], ["Lederer", "Johannes", ""]]}, {"id": "2002.11989", "submitter": "Alessandra Mattei", "authors": "Alessandra Mattei and Fabrizia Mealli and Peng Ding", "title": "Assessing causal effects in the presence of treatment switching through\n  principal stratification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trials focusing on survival outcomes often allow patients in the\ncontrol arm to switch to the treatment arm if their physical conditions are\nworse than certain tolerance levels. The Intention-To-Treat analysis provides\nvalid causal estimates of the effect of assignment, but it does not measure the\neffect of the actual receipt of the treatment and ignores the information of\ntreatment switching. Other existing methods propose to reconstruct the outcome\na unit would have had if s/he had not switched under strong assumptions. We\npropose to re-define the problem of treatment switching using principal\nstratification focusing on principal causal effects for patients belonging to\nsubpopulations defined by the switching behavior under control. We use a\nBayesian approach to inference taking into account that (i) switching happens\nin continuous time generating infinitely many principal strata; (ii) switching\ntime is not defined for units who never switch in a particular experiment; and\n(iii) survival time and switching time are subject to censoring. We illustrate\nour framework using a synthetic dataset based on the Concorde study, a\nrandomized controlled trial aimed to assess causal effects on time-to-disease\nprogression or death of immediate versus deferred treatment with zidovudine\namong patients with asymptomatic HIV infection.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 09:25:29 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Mattei", "Alessandra", ""], ["Mealli", "Fabrizia", ""], ["Ding", "Peng", ""]]}, {"id": "2002.11992", "submitter": "Lilun Du", "authors": "Lilun Du, Xu Guo, Wenguang Sun, and Changliang Zou", "title": "False Discovery Rate Control Under General Dependence By Symmetrized\n  Data Aggregation", "comments": "33 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new class of distribution--free multiple testing rules for false\ndiscovery rate (FDR) control under general dependence. A key element in our\nproposal is a symmetrized data aggregation (SDA) approach to incorporating the\ndependence structure via sample splitting, data screening and information\npooling. The proposed SDA filter first constructs a sequence of ranking\nstatistics that fulfill global symmetry properties, and then chooses a\ndata--driven threshold along the ranking to control the FDR. The SDA filter\nsubstantially outperforms the knockoff method in power under moderate to strong\ndependence, and is more robust than existing methods based on asymptotic\n$p$-values. We first develop finite--sample theory to provide an upper bound\nfor the actual FDR under general dependence, and then establish the asymptotic\nvalidity of SDA for both the FDR and false discovery proportion (FDP) control\nunder mild regularity conditions. The procedure is implemented in the R package\n\\texttt{SDA}. Numerical results confirm the effectiveness and robustness of SDA\nin FDR control and show that it achieves substantial power gain over existing\nmethods in many settings.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 09:27:57 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 15:33:36 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Du", "Lilun", ""], ["Guo", "Xu", ""], ["Sun", "Wenguang", ""], ["Zou", "Changliang", ""]]}, {"id": "2002.12024", "submitter": "Elmar Plischke", "authors": "Elmar Plischke, Giovanni Rabitti, Emanuele Borgonovo", "title": "Computing Shapley Effects for Sensitivity Analysis", "comments": "16 pages, 5 figures, 3 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shapley effects are attracting increasing attention as sensitivity measures.\nWhen the value function is the conditional variance, they account for the\nindividual and higher order effects of a model input. They are also well\ndefined under model input dependence. However, one of the issues associated\nwith their use is computational cost. We present a new algorithm that offers\nmajor improvements for the computation of Shapley effects, reducing\ncomputational burden by several orders of magnitude (from $k!\\cdot k$ to $2^k$,\nwhere $k$ is the number of inputs) with respect to currently available\nimplementations. The algorithm works in the presence of input dependencies. The\nalgorithm also makes it possible to estimate all generalized (Shapley-Owen)\neffects for interactions.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 10:34:26 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Plischke", "Elmar", ""], ["Rabitti", "Giovanni", ""], ["Borgonovo", "Emanuele", ""]]}, {"id": "2002.12105", "submitter": "Evelien Schat", "authors": "Evelien Schat, Rens van de Schoot, Wouter M. Kouw, Duco Veen,\n  Adri\\\"enne M. Mendrik", "title": "The Data Representativeness Criterion: Predicting the Performance of\n  Supervised Classification Based on Data Set Similarity", "comments": "12 pages, 6 figures", "journal-ref": "PLoS ONE 15(8): e0237009, 2020, pp. 1-16", "doi": "10.1371/journal.pone.0237009", "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a broad range of fields it may be desirable to reuse a supervised\nclassification algorithm and apply it to a new data set. However,\ngeneralization of such an algorithm and thus achieving a similar classification\nperformance is only possible when the training data used to build the algorithm\nis similar to new unseen data one wishes to apply it to. It is often unknown in\nadvance how an algorithm will perform on new unseen data, being a crucial\nreason for not deploying an algorithm at all. Therefore, tools are needed to\nmeasure the similarity of data sets. In this paper, we propose the Data\nRepresentativeness Criterion (DRC) to determine how representative a training\ndata set is of a new unseen data set. We present a proof of principle, to see\nwhether the DRC can quantify the similarity of data sets and whether the DRC\nrelates to the performance of a supervised classification algorithm. We\ncompared a number of magnetic resonance imaging (MRI) data sets, ranging from\nsubtle to severe difference is acquisition parameters. Results indicate that,\nbased on the similarity of data sets, the DRC is able to give an indication as\nto when the performance of a supervised classifier decreases. The strictness of\nthe DRC can be set by the user, depending on what one considers to be an\nacceptable underperformance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 15:08:13 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Schat", "Evelien", ""], ["van de Schoot", "Rens", ""], ["Kouw", "Wouter M.", ""], ["Veen", "Duco", ""], ["Mendrik", "Adri\u00ebnne M.", ""]]}, {"id": "2002.12173", "submitter": "Eric Adjakossa", "authors": "Eric Adjakossa (LPSM), Yannig Goude (EDF R&D), Olivier Wintenberger\n  (LPSM UMR)", "title": "Kalman Recursions Aggregated Online", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we aim at improving the prediction of expert aggregation by\nusing the underlying properties of the models that provide expert predictions.\nWe restrict ourselves to the case where expert predictions come from Kalman\nrecursions, fitting state-space models. By using exponential weights, we\nconstruct different algorithms of Kalman recursions Aggregated Online (KAO)\nthat compete with the best expert or the best convex combination of experts in\na more or less adaptive way. We improve the existing results on expert\naggregation literature when the experts are Kalman recursions by taking\nadvantage of the second-order properties of the Kalman recursions. We apply our\napproach to Kalman recursions and extend it to the general adversarial expert\nsetting by state-space modeling the errors of the experts. We apply these new\nalgorithms to a real dataset of electricity consumption and show how it can\nimprove forecast performances comparing to other exponentially weighted average\nprocedures.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 14:53:53 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Adjakossa", "Eric", "", "LPSM"], ["Goude", "Yannig", "", "EDF R&D"], ["Wintenberger", "Olivier", "", "LPSM UMR"]]}, {"id": "2002.12318", "submitter": "Thomas Opitz", "authors": "Thomas Opitz, Florent Bonneu, Edith Gabriel", "title": "Point-process based Bayesian modeling of space-time structures of forest\n  fire occurrences in Mediterranean France", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to climate change and human activity, wildfires are expected to become\nmore frequent and extreme worldwide, causing economic and ecological disasters.\nThe deployment of preventive measures and operational forecasts can be aided by\nstochastic modeling that helps to understand and quantify the mechanisms\ngoverning the occurrence intensity. We here develop a point process framework\nfor wildfire ignition points observed in the French Mediterranean basin since\n1995, and we fit a spatio-temporal log-Gaussian Cox process with monthly\ntemporal resolution in a Bayesian framework using the integrated nested Laplace\napproximation (INLA). Human activity is the main direct cause of wildfires and\nis indirectly measured through a number of appropriately defined proxies\nrelated to land-use covariates (urbanization, road network) in our approach,\nand we further integrate covariates of climatic and environmental conditions to\nexplain wildfire occurrences. We include spatial random effects with Mat\\'ern\ncovariance and temporal autoregression at yearly resolution. Two major\nmethodological challenges are tackled: first, handling and unifying multi-scale\nstructures in data is achieved through computer-intensive preprocessing steps\nwith GIS software and kriging techniques; second, INLA-based estimation with\nhigh-dimensional response vectors and latent models is facilitated through\nintra-year subsampling, taking into account the occurrence structure of\nwildfires.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 10:41:40 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Opitz", "Thomas", ""], ["Bonneu", "Florent", ""], ["Gabriel", "Edith", ""]]}, {"id": "2002.12321", "submitter": "Wanrong Zhang", "authors": "Wanrong Zhang, Gautam Kamath, Rachel Cummings", "title": "PAPRIKA: Private Online False Discovery Rate Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DS cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hypothesis testing, a false discovery occurs when a hypothesis is\nincorrectly rejected due to noise in the sample. When adaptively testing\nmultiple hypotheses, the probability of a false discovery increases as more\ntests are performed. Thus the problem of False Discovery Rate (FDR) control is\nto find a procedure for testing multiple hypotheses that accounts for this\neffect in determining the set of hypotheses to reject. The goal is to minimize\nthe number (or fraction) of false discoveries, while maintaining a high true\npositive rate (i.e., correct discoveries).\n  In this work, we study False Discovery Rate (FDR) control in multiple\nhypothesis testing under the constraint of differential privacy for the sample.\nUnlike previous work in this direction, we focus on the online setting, meaning\nthat a decision about each hypothesis must be made immediately after the test\nis performed, rather than waiting for the output of all tests as in the offline\nsetting. We provide new private algorithms based on state-of-the-art results in\nnon-private online FDR control. Our algorithms have strong provable guarantees\nfor privacy and statistical performance as measured by FDR and power. We also\nprovide experimental results to demonstrate the efficacy of our algorithms in a\nvariety of data environments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 18:42:23 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 03:06:54 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Zhang", "Wanrong", ""], ["Kamath", "Gautam", ""], ["Cummings", "Rachel", ""]]}, {"id": "2002.12548", "submitter": "Haojie Ren", "authors": "Changliang Zou, Haojie Ren, Xu Guo and Runze Li", "title": "A New Procedure for Controlling False Discovery Rate in Large-Scale\n  t-tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with false discovery rate (FDR) control in\nlarge-scale multiple testing problems. We first propose a new data-driven\ntesting procedure for controlling the FDR in large-scale t-tests for one-sample\nmean problem. The proposed procedure achieves exact FDR control in finite\nsample settings when the populations are symmetric no matter the number of\ntests or sample sizes. Comparing with the existing bootstrap method for FDR\ncontrol, the proposed procedure is computationally efficient. We show that the\nproposed method can control the FDR asymptotically for asymmetric populations\neven when the test statistics are not independent. We further show that the\nproposed procedure with a simple correction is as accurate as the bootstrap\nmethod to the second-order degree, and could be much more effective than the\nexisting normal calibration. We extend the proposed procedure to two-sample\nmean problem. Empirical results show that the proposed procedures have better\nFDR control than existing ones when the proportion of true alternative\nhypotheses is not too low, while maintaining reasonably good detection ability.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 05:13:40 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Zou", "Changliang", ""], ["Ren", "Haojie", ""], ["Guo", "Xu", ""], ["Li", "Runze", ""]]}, {"id": "2002.12586", "submitter": "Luella Fu", "authors": "Luella J. Fu, Gareth M. James, Wenguang Sun", "title": "Nonparametric Empirical Bayes Estimation on Heterogeneous Data", "comments": "55 pages including 33 pages of main text (5 of which are mostly\n  bibliography) and 25 pages of supplementary text; 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simultaneous estimation of many parameters $\\eta_i$, based on a\ncorresponding set of observations $x_i$, for $i=1,\\ldots, n$, is a key research\nproblem that has received renewed attention in the high-dimensional setting.\n%The classic example involves estimating a vector of normal means $\\mu_i$\nsubject to a fixed variance term $\\sigma^2$. However, Many practical situations\ninvolve heterogeneous data $(x_i, \\theta_i)$ where $\\theta_i$ is a known\nnuisance parameter. Effectively pooling information across samples while\ncorrectly accounting for heterogeneity presents a significant challenge in\nlarge-scale estimation problems. We address this issue by introducing the\n\"Nonparametric Empirical Bayes Smoothing Tweedie\" (NEST) estimator, which\nefficiently estimates $\\eta_i$ and properly adjusts for heterogeneity %by\napproximating the marginal density of the data $f_{\\theta_i}(x_i)$ and applying\nthis density to via a generalized version of Tweedie's formula. NEST is capable\nof handling a wider range of settings than previously proposed heterogeneous\napproaches as it does not make any parametric assumptions on the prior\ndistribution of $\\eta_i$. The estimation framework is simple but general enough\nto accommodate any member of the exponential family of distributions. %; a\nthorough study of the normal means problem subject to heterogeneous variances\nis presented to illustrate the proposed framework. Our theoretical results show\nthat NEST is asymptotically optimal, while simulation studies show that it\noutperforms competing methods, with substantial efficiency gains in many\nsettings. The method is demonstrated on a data set measuring the performance\ngap in math scores between socioeconomically advantaged and disadvantaged\nstudents in K-12 schools.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 07:48:39 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 04:02:41 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2020 00:53:51 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Fu", "Luella J.", ""], ["James", "Gareth M.", ""], ["Sun", "Wenguang", ""]]}, {"id": "2002.12606", "submitter": "Benjamin Stokell", "authors": "Benjamin G. Stokell, Rajen D. Shah, Ryan J. Tibshirani", "title": "Modelling High-Dimensional Categorical Data Using Nonconvex Fusion\n  Penalties", "comments": "52 pages, 10 figures; to appear in JRSSB", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for estimation in high-dimensional linear models with\nnominal categorical data. Our estimator, called SCOPE, fuses levels together by\nmaking their corresponding coefficients exactly equal. This is achieved using\nthe minimax concave penalty on differences between the order statistics of the\ncoefficients for a categorical variable, thereby clustering the coefficients.\nWe provide an algorithm for exact and efficient computation of the global\nminimum of the resulting nonconvex objective in the case with a single variable\nwith potentially many levels, and use this within a block coordinate descent\nprocedure in the multivariate case. We show that an oracle least squares\nsolution that exploits the unknown level fusions is a limit point of the\ncoordinate descent with high probability, provided the true levels have a\ncertain minimum separation; these conditions are known to be minimal in the\nunivariate case. We demonstrate the favourable performance of SCOPE across a\nrange of real and simulated datasets. An R package CatReg implementing SCOPE\nfor linear models and also a version for logistic regression is available on\nCRAN.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 09:20:41 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 18:52:13 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 10:45:06 GMT"}, {"version": "v4", "created": "Mon, 28 Jun 2021 14:48:14 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Stokell", "Benjamin G.", ""], ["Shah", "Rajen D.", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "2002.12703", "submitter": "R\\'emy Mari\\'etan", "authors": "R\\'emy Mari\\'etan and Stephan Morgenthaler", "title": "Statistical applications of Random matrix theory: comparison of two\n  populations II", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a statistical procedure for testing the equality of\ntwo independent estimated covariance matrices when the number of potentially\ndependent data vectors is large and proportional to the size of the vectors,\nthat is, the number of variables. Inspired by the spike models used in random\nmatrix theory, we concentrate on the largest eigenvalues of the matrices in\norder to determine significance. To avoid false rejections we must guard\nagainst residual spikes and need a sufficiently precise description of the\nbehaviour of the largest eigenvalues under the null hypothesis.\n  In this paper we propose some \"invariant\" theorems that allows us to extend\nthe test of arXiv:2002.12741 for perturbation of order $1$ to some general\ntests for order $k$. The statistics introduced in this paper allow the user to\ntest the equality of two populations based on high-dimensional multivariate\ndata.\n  Simulations show that these tests have more power of detection than standard\nmultivariate approaches.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 13:28:52 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 10:08:38 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 12:15:34 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Mari\u00e9tan", "R\u00e9my", ""], ["Morgenthaler", "Stephan", ""]]}, {"id": "2002.12741", "submitter": "R\\'emy Mari\\'etan", "authors": "R\\'emy Mari\\'etan and Stephan Morgenthaler", "title": "Statistical applications of random matrix theory: comparison of two\n  populations I", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a statistical procedure for testing the equality of\ntwo independent estimated covariance matrices when the number of potentially\ndependent data vectors is large and proportional to the size of the vectors,\nthat is, the number of variables. Inspired by the spike models used in random\nmatrix theory, we concentrate on the largest eigenvalues of the matrices in\norder to determine significance. To avoid false rejections we must guard\nagainst residual spikes and need a sufficiently precise description of the\nbehaviour of the largest eigenvalues under the null hypothesis. In this paper,\nwe lay a foundation by treating alternatives based on perturbations of order\n$1$, that is, a single large eigenvalue. Our statistic allows the user to test\nthe equality of two populations. Future work will extend the result to\nperturbations of order $k$ and demonstrate conservativeness of the procedure\nfor more general matrices.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 14:31:31 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 09:55:48 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Mari\u00e9tan", "R\u00e9my", ""], ["Morgenthaler", "Stephan", ""]]}]