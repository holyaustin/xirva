[{"id": "0904.0584", "submitter": "Ryota Tomioka", "authors": "Ryota Tomioka and Masashi Sugiyama", "title": "Dual Augmented Lagrangian Method for Efficient Sparse Reconstruction", "comments": "10 pages, 3 figures", "journal-ref": "IEEE Signal Processing Letters, volume 16, issue 12, pages 1067 -\n  1070, 2009", "doi": "10.1109/LSP.2009.2030111", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient algorithm for sparse signal reconstruction problems.\nThe proposed algorithm is an augmented Lagrangian method based on the dual\nsparse reconstruction problem. It is efficient when the number of unknown\nvariables is much larger than the number of observations because of the dual\nformulation. Moreover, the primal variable is explicitly updated and the\nsparsity in the solution is exploited. Numerical comparison with the\nstate-of-the-art algorithms shows that the proposed algorithm is favorable when\nthe design matrix is poorly conditioned or dense and very large.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2009 14:32:47 GMT"}], "update_date": "2010-10-06", "authors_parsed": [["Tomioka", "Ryota", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "0904.0645", "submitter": "Alexander Blocker", "authors": "Alexander W. Blocker, Pavlos Protopapas, Charles R. Alcock", "title": "A Bayesian approach to the analysis of time symmetry in light curves:\n  Reconsidering Scorpius X-1 occultations", "comments": "24 pages, 18 figures. Preprint typeset using LaTeX style emulateapj\n  v. 04/20/08", "journal-ref": "Astrophys.J.701:1742-1752,2009", "doi": "10.1088/0004-637X/701/2/1742", "report-no": null, "categories": "astro-ph.IM astro-ph.EP stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to the analysis of time symmetry in light curves,\nsuch as those in the x-ray at the center of the Scorpius X-1 occultation\ndebate. Our method uses a new parameterization for such events (the bilogistic\nevent profile) and provides a clear, physically relevant characterization of\neach event's key features. We also demonstrate a Markov Chain Monte Carlo\nalgorithm to carry out this analysis, including a novel independence chain\nconfiguration for the estimation of each event's location in the light curve.\nThese tools are applied to the Scorpius X-1 light curves presented in Chang et\nal. (2007), providing additional evidence based on the time series that the\nevents detected thus far are most likely not occultations by TNOs.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2009 15:23:20 GMT"}], "update_date": "2011-02-11", "authors_parsed": [["Blocker", "Alexander W.", ""], ["Protopapas", "Pavlos", ""], ["Alcock", "Charles R.", ""]]}, {"id": "0904.0687", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Smooth Optimization Approach for Sparse Covariance Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we first study a smooth optimization approach for solving a\nclass of nonsmooth strictly concave maximization problems whose objective\nfunctions admit smooth convex minimization reformulations. In particular, we\napply Nesterov's smooth optimization technique [Y.E. Nesterov, Dokl. Akad. Nauk\nSSSR, 269 (1983), pp. 543--547; Y. E. Nesterov, Math. Programming, 103 (2005),\npp. 127--152] to their dual counterparts that are smooth convex problems. It is\nshown that the resulting approach has ${\\cal O}(1/{\\sqrt{\\epsilon}})$ iteration\ncomplexity for finding an $\\epsilon$-optimal solution to both primal and dual\nproblems. We then discuss the application of this approach to sparse covariance\nselection that is approximately solved as an $l_1$-norm penalized maximum\nlikelihood estimation problem, and also propose a variant of this approach\nwhich has substantially outperformed the latter one in our computational\nexperiments. We finally compare the performance of these approaches with other\nfirst-order methods, namely, Nesterov's ${\\cal O}(1/\\epsilon)$ smooth\napproximation scheme and block-coordinate descent method studied in [A.\nd'Aspremont, O. Banerjee, and L. El Ghaoui, SIAM J. Matrix Anal. Appl., 30\n(2008), pp. 56--66; J. Friedman, T. Hastie, and R. Tibshirani, Biostatistics, 9\n(2008), pp. 432--441] for sparse covariance selection on a set of randomly\ngenerated instances. It shows that our smooth optimization approach\nsubstantially outperforms the first method above, and moreover, its variant\nsubstantially outperforms both methods above.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2009 06:11:13 GMT"}], "update_date": "2009-04-07", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "0904.0688", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Adaptive First-Order Methods for General Sparse Inverse Covariance\n  Selection", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider estimating sparse inverse covariance of a Gaussian\ngraphical model whose conditional independence is assumed to be partially\nknown. Similarly as in [5], we formulate it as an $l_1$-norm penalized maximum\nlikelihood estimation problem. Further, we propose an algorithm framework, and\ndevelop two first-order methods, that is, the adaptive spectral projected\ngradient (ASPG) method and the adaptive Nesterov's smooth (ANS) method, for\nsolving this estimation problem. Finally, we compare the performance of these\ntwo methods on a set of randomly generated instances. Our computational results\ndemonstrate that both methods are able to solve problems of size at least a\nthousand and number of constraints of nearly a half million within a reasonable\namount of time, and the ASPG method generally outperforms the ANS method.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2009 06:24:01 GMT"}], "update_date": "2009-04-07", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "0904.0691", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu, Renato D. C. Monteiro, Ming Yuan", "title": "Convex Optimization Methods for Dimension Reduction and Coefficient\n  Estimation in Multivariate Linear Regression", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study convex optimization methods for computing the trace\nnorm regularized least squares estimate in multivariate linear regression. The\nso-called factor estimation and selection (FES) method, recently proposed by\nYuan et al. [22], conducts parameter estimation and factor selection\nsimultaneously and have been shown to enjoy nice properties in both large and\nfinite samples. To compute the estimates, however, can be very challenging in\npractice because of the high dimensionality and the trace norm constraint. In\nthis paper, we explore a variant of Nesterov's smooth method [20] and interior\npoint methods for computing the penalized least squares estimate. The\nperformance of these methods is then compared using a set of randomly generated\ninstances. We show that the variant of Nesterov's smooth method [20] generally\noutperforms the interior point method implemented in SDPT3 version 4.0 (beta)\n[19] substantially . Moreover, the former method is much more memory efficient.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2009 06:34:20 GMT"}], "update_date": "2009-04-07", "authors_parsed": [["Lu", "Zhaosong", ""], ["Monteiro", "Renato D. C.", ""], ["Yuan", "Ming", ""]]}, {"id": "0904.0843", "submitter": "Heng Lian", "authors": "Heng Lian", "title": "Empirical Likelihood Confidence Intervals for Nonparametric Functional\n  Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing confidence intervals for\nnonparametric functional data analysis using empirical likelihood. In this\ndoubly infinite-dimensional context, we demonstrate the Wilks's phenomenon and\npropose a bias-corrected construction that requires neither undersmoothing nor\ndirect bias estimation. We also extend our results to partially linear\nregression involving functional data. Our numerical results demonstrated the\nimproved performance of empirical likelihood over approximation based on\nasymptotic normality.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2009 04:34:24 GMT"}], "update_date": "2009-04-07", "authors_parsed": [["Lian", "Heng", ""]]}, {"id": "0904.0951", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Ivan Fernandez-Val, Blaise Melly", "title": "Inference on Counterfactual Distributions", "comments": "55 pages, 1 table, 3 figures, supplementary appendix with additional\n  results available from the authors' web sites", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual distributions are important ingredients for policy analysis\nand decomposition analysis in empirical economics. In this article we develop\nmodeling and inference tools for counterfactual distributions based on\nregression methods. The counterfactual scenarios that we consider consist of\nceteris paribus changes in either the distribution of covariates related to the\noutcome of interest or the conditional distribution of the outcome given\ncovariates. For either of these scenarios we derive joint functional central\nlimit theorems and bootstrap validity results for regression-based estimators\nof the status quo and counterfactual outcome distributions. These results allow\nus to construct simultaneous confidence sets for function-valued effects of the\ncounterfactual changes, including the effects on the entire distribution and\nquantile functions of the outcome as well as on related functionals. These\nconfidence sets can be used to test functional hypotheses such as no-effect,\npositive effect, or stochastic dominance. Our theory applies to general\ncounterfactual changes and covers the main regression methods including\nclassical, quantile, duration, and distribution regressions. We illustrate the\nresults with an empirical application to wage decompositions using data for the\nUnited States.\n  As a part of developing the main results, we introduce distribution\nregression as a comprehensive and flexible tool for modeling and estimating the\n\\textit{entire} conditional distribution. We show that distribution regression\nencompasses the Cox duration regression and represents a useful alternative to\nquantile regression. We establish functional central limit theorems and\nbootstrap validity results for the empirical distribution regression process\nand various related functionals.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2009 15:17:37 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2012 02:26:47 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2012 15:03:46 GMT"}, {"version": "v4", "created": "Sat, 15 Dec 2012 21:25:12 GMT"}, {"version": "v5", "created": "Thu, 9 May 2013 13:20:20 GMT"}, {"version": "v6", "created": "Wed, 18 Sep 2013 15:03:34 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fernandez-Val", "Ivan", ""], ["Melly", "Blaise", ""]]}, {"id": "0904.0977", "submitter": "Guy Freeman", "authors": "Guy Freeman and Jim Q. Smith", "title": "Bayesian MAP Model Selection of Chain Event Graphs", "comments": "19 pages, 6 figures, 1 table Submitted to Journal of Multivariate\n  Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The class of chain event graph models is a generalisation of the class of\ndiscrete Bayesian networks, retaining most of the structural advantages of the\nBayesian network for model interrogation, propagation and learning, while more\nnaturally encoding asymmetric state spaces and the order in which events\nhappen. In this paper we demonstrate how with complete sampling, conjugate\nclosed form model selection based on product Dirichlet priors is possible, and\nprove that suitable homogeneity assumptions characterise the product Dirichlet\nprior on this class of models. We demonstrate our techniques using two\neducational examples.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2009 17:51:33 GMT"}], "update_date": "2009-04-07", "authors_parsed": [["Freeman", "Guy", ""], ["Smith", "Jim Q.", ""]]}, {"id": "0904.1510", "submitter": "Corinne Dahinden", "authors": "Corinne Dahinden, Markus Kalisch, Peter B\\\"uhlmann", "title": "Decomposition and Model Selection for Large Contingency Tables", "comments": "Changed the structure of the paper during review process; content is\n  almost unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large contingency tables summarizing categorical variables arise in many\nareas. For example in biology when a large number of biomarkers are\ncross-tabulated according to their discrete expression level. Interactions of\nthe variables are generally studied with log-linear models and the structure of\na log-linear model can be visually represented by a graph from which the\nconditional independence structure can then be read off. However, since the\nnumber of parameters in a saturated model grows exponentially in the number of\nvariables, this generally comes with a heavy burden as far as computational\npower is concerned. If we restrict ourselves to models of lower order\ninteractions or other sparse structures we face similar problems as the number\nof cells remains unchanged. We therefore present a divide-and-conquer approach,\nwhere we first divide the problem into several lower-dimensional problems and\nthen combine these to form a global solution. Our methodology is\ncomputationally feasible for log-linear interaction modeling with many\ncategorical variables each or some of them having many categories. We\ndemonstrate the proposed method on simulated data and apply it to a bio-medical\nproblem in cancer research.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2009 12:11:18 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2009 13:57:23 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2009 07:14:21 GMT"}], "update_date": "2009-11-17", "authors_parsed": [["Dahinden", "Corinne", ""], ["Kalisch", "Markus", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "0904.1990", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Ivan Fernandez-Val, Jinyong Hahn, Whitney Newey", "title": "Average and Quantile Effects in Nonseparable Panel Models", "comments": "83 pages, 3 tables, 6 figures; includes supplementary appendix with\n  proofs and additional results", "journal-ref": "Econometrica (2013) 81 (2): 535-580", "doi": "10.3982/ECTA8405", "report-no": null, "categories": "stat.ME econ.EM math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonseparable panel models are important in a variety of economic settings,\nincluding discrete choice. This paper gives identification and estimation\nresults for nonseparable models under time homogeneity conditions that are like\n\"time is randomly assigned\" or \"time is an instrument.\" Partial identification\nresults for average and quantile effects are given for discrete regressors,\nunder static or dynamic conditions, in fully nonparametric and in\nsemiparametric models, with time effects. It is shown that the usual, linear,\nfixed-effects estimator is not a consistent estimator of the identified average\neffect, and a consistent estimator is given. A simple estimator of identified\nquantile treatment effects is given, providing a solution to the important\nproblem of estimating quantile treatment effects from panel data. Bounds for\noverall effects in static and dynamic models are given. The dynamic bounds\nprovide a partial identification solution to the important problem of\nestimating the effect of state dependence in the presence of unobserved\nheterogeneity. The impact of $T$, the number of time periods, is shown by\nderiving shrinkage rates for the identified set as $T$ grows. We also consider\nsemiparametric, discrete-choice models and find that semiparametric panel\nbounds can be much tighter than nonparametric bounds.\nComputationally-convenient methods for semiparametric models are presented. We\npropose a novel inference method that applies in panel data and other settings\nand show that it produces uniformly valid confidence regions in large samples.\nWe give empirical illustrations.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2009 19:13:57 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2011 08:14:01 GMT"}, {"version": "v3", "created": "Tue, 14 Feb 2012 01:56:21 GMT"}, {"version": "v4", "created": "Tue, 26 Mar 2013 15:56:19 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fernandez-Val", "Ivan", ""], ["Hahn", "Jinyong", ""], ["Newey", "Whitney", ""]]}, {"id": "0904.2052", "submitter": "Kaspar Rufibach", "authors": "Fadoua Balabdaoui, Kaspar Rufibach, Filippo Santambrogio", "title": "Least Squares estimation of two ordered monotone regression curves", "comments": "23 pages, 2 figures. Second revised version according to reviewer\n  comments", "journal-ref": "J. Nonparametr. Stat. (2011), 22(8), 1019-1037", "doi": "10.1080/10485250903548729", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of finding the Least Squares\nestimators of two isotonic regression curves $g^\\circ_1$ and $g^\\circ_2$ under\nthe additional constraint that they are ordered; e.g., $g^\\circ_1 \\le\ng^\\circ_2$. Given two sets of $n$ data points $y_1, ..., y_n$ and $z_1,\n>...,z_n$ observed at (the same) design points, the estimates of the true\ncurves are obtained by minimizing the weighted Least Squares criterion $L_2(a,\nb) = \\sum_{j=1}^n (y_j - a_j)^2 w_{1,j}+ \\sum_{j=1}^n (z_j - b_j)^2 w_{2,j}$\nover the class of pairs of vectors $(a, b) \\in \\mathbb{R}^n \\times \\mathbb{R}^n\n$ such that $a_1 \\le a_2 \\le ...\\le a_n $, $b_1 \\le b_2 \\le ...\\le b_n $, and\n$a_i \\le b_i, i=1, ...,n$. The characterization of the estimators is\nestablished. To compute these estimators, we use an iterative projected\nsubgradient algorithm, where the projection is performed with a \"generalized\"\npool-adjacent-violaters algorithm (PAVA), a byproduct of this work. Then, we\napply the estimation method to real data from mechanical engineering.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2009 06:03:36 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2009 09:36:13 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2009 11:14:12 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Balabdaoui", "Fadoua", ""], ["Rufibach", "Kaspar", ""], ["Santambrogio", "Filippo", ""]]}, {"id": "0904.2144", "submitter": "Randal Douc", "authors": "Randal Douc, Christian P. Robert", "title": "A vanilla Rao--Blackwellization of Metropolis--Hastings algorithms", "comments": "Published in at http://dx.doi.org/10.1214/10-AOS838 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 1, 261-277", "doi": "10.1214/10-AOS838", "report-no": "IMS-AOS-AOS838", "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Casella and Robert [Biometrika 83 (1996) 81--94] presented a general\nRao--Blackwellization principle for accept-reject and Metropolis--Hastings\nschemes that leads to significant decreases in the variance of the resulting\nestimators, but at a high cost in computation and storage. Adopting a\ncompletely different perspective, we introduce instead a universal scheme that\nguarantees variance reductions in all Metropolis--Hastings-based estimators\nwhile keeping the computation cost under control. We establish a central limit\ntheorem for the improved estimators and illustrate their performances on toy\nexamples and on a probit model estimation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2009 15:38:16 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2009 19:23:49 GMT"}, {"version": "v3", "created": "Sat, 24 Oct 2009 03:37:54 GMT"}, {"version": "v4", "created": "Mon, 31 May 2010 05:49:42 GMT"}, {"version": "v5", "created": "Tue, 1 Jun 2010 05:53:36 GMT"}, {"version": "v6", "created": "Tue, 8 Mar 2011 11:14:13 GMT"}], "update_date": "2011-03-09", "authors_parsed": [["Douc", "Randal", ""], ["Robert", "Christian P.", ""]]}, {"id": "0904.2207", "submitter": "Miquel Trias", "authors": "Miquel Trias, Alberto Vecchio and John Veitch", "title": "Delayed rejection schemes for efficient Markov-Chain Monte-Carlo\n  sampling of multimodal distributions", "comments": "32 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME gr-qc physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of problems in a variety of fields are characterised by target\ndistributions with a multimodal structure in which the presence of several\nisolated local maxima dramatically reduces the efficiency of Markov Chain Monte\nCarlo sampling algorithms. Several solutions, such as simulated tempering or\nthe use of parallel chains, have been proposed to facilitate the exploration of\nthe relevant parameter space. They provide effective strategies in the cases in\nwhich the dimension of the parameter space is small and/or the computational\ncosts are not a limiting factor. These approaches fail however in the case of\nhigh-dimensional spaces where the multimodal structure is induced by\ndegeneracies between regions of the parameter space. In this paper we present a\nfully Markovian way to efficiently sample this kind of distribution based on\nthe general Delayed Rejection scheme with an arbitrary number of steps, and\nprovide details for an efficient numerical implementation of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2009 20:58:46 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2009 17:21:56 GMT"}], "update_date": "2009-07-31", "authors_parsed": [["Trias", "Miquel", ""], ["Vecchio", "Alberto", ""], ["Veitch", "John", ""]]}, {"id": "0904.2229", "submitter": "Summer Han", "authors": "Summer S. Han, Elena L. Grigorenko and Joseph T. Chang", "title": "Uncovering shared common genetic risk factors for various aspects of\n  complex disorders captured in multiple traits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying shared genetic risk factors for multiple measured traits has been\nof great interest in studying complex disorders. Marlow's (2003) method for\ndetecting shared gene effects on complex traits has been highly influential in\nthe literature of neurodevelopmental disorders as well as other disorders\nincluding obesity and asthma. Although its method has been widely applied and\nhas been recommended as potentially powerful, the validity and power of this\nmethod have not been examined either theoretically or by simulation. This paper\nestablishes the validity and quantifies and explains the power of the method.\nWe show the method has correct type 1 error rates regardless of the number of\ntraits in the model, and confirm power increases compared to standard\nunivariate methods across different genetic models. We discover the main source\nof these power gains is correlations among traits induced by a common major\ngene effect component. We compare the use of the complete pleiotropy model, as\nassumed by Marlow, to the use of a more general model allowing additional\ncorrelation parameters, and find that even when the true model includes those\nparameters, the complete pleiotropy model is more powerful as long as traits\nare moderately correlated by a major gene component. We implement this method\nand a power calculator in software that can assist in designing studies by\nusing pilot data to calculate required sample sizes and choose traits for\nfurther linkage studies. We apply the software to data on reading disability in\nthe Russian language.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2009 01:49:53 GMT"}], "update_date": "2009-04-16", "authors_parsed": [["Han", "Summer S.", ""], ["Grigorenko", "Elena L.", ""], ["Chang", "Joseph T.", ""]]}, {"id": "0904.2906", "submitter": "Heng Lian", "authors": "Heng Lian", "title": "Sparse Bayesian Hierarchical Modeling of High-dimensional Clustering\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is one of the most widely used procedures in the analysis of\nmicroarray data, for example with the goal of discovering cancer subtypes based\non observed heterogeneity of genetic marks between different tissues. It is\nwell-known that in such high-dimensional settings, the existence of many noise\nvariables can overwhelm the few signals embedded in the high-dimensional space.\nWe propose a novel Bayesian approach based on Dirichlet process with a sparsity\nprior that simultaneous performs variable selection and clustering, and also\ndiscover variables that only distinguish a subset of the cluster components.\nUnlike previous Bayesian formulations, we use Dirichlet process (DP) for both\nclustering of samples as well as for regularizing the high-dimensional\nmean/variance structure. To solve the computational challenge brought by this\ndouble usage of DP, we propose to make use of a sequential sampling scheme\nembedded within Markov chain Monte Carlo (MCMC) updates to improve the naive\nimplementation of existing algorithms for DP mixture models. Our method is\ndemonstrated on a simulation study and illustrated with the leukemia gene\nexpression dataset.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2009 11:33:01 GMT"}], "update_date": "2009-04-21", "authors_parsed": [["Lian", "Heng", ""]]}, {"id": "0904.2931", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni and Victor Chernozhukov", "title": "L1-Penalized Quantile Regression in High-Dimensional Sparse Models", "comments": null, "journal-ref": "Annals of Statistics, Volume 39, Number 1, 2011, 82-130", "doi": null, "report-no": null, "categories": "math.ST econ.EM math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider median regression and, more generally, a possibly infinite\ncollection of quantile regressions in high-dimensional sparse models. In these\nmodels the overall number of regressors $p$ is very large, possibly larger than\nthe sample size $n$, but only $s$ of these regressors have non-zero impact on\nthe conditional quantile of the response variable, where $s$ grows slower than\n$n$. We consider quantile regression penalized by the $\\ell_1$-norm of\ncoefficients ($\\ell_1$-QR). First, we show that $\\ell_1$-QR is consistent at\nthe rate $\\sqrt{s/n} \\sqrt{\\log p}$. The overall number of regressors $p$\naffects the rate only through the $\\log p$ factor, thus allowing nearly\nexponential growth in the number of zero-impact regressors. The rate result\nholds under relatively weak conditions, requiring that $s/n$ converges to zero\nat a super-logarithmic speed and that regularization parameter satisfies\ncertain theoretical constraints. Second, we propose a pivotal, data-driven\nchoice of the regularization parameter and show that it satisfies these\ntheoretical constraints. Third, we show that $\\ell_1$-QR correctly selects the\ntrue minimal model as a valid submodel, when the non-zero coefficients of the\ntrue model are well separated from zero. We also show that the number of\nnon-zero coefficients in $\\ell_1$-QR is of same stochastic order as $s$.\nFourth, we analyze the rate of convergence of a two-step estimator that applies\nordinary quantile regression to the selected model. Fifth, we evaluate the\nperformance of $\\ell_1$-QR in a Monte-Carlo experiment, and illustrate its use\non an international economic growth application.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2009 20:15:07 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2009 14:23:01 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2009 21:00:15 GMT"}, {"version": "v4", "created": "Tue, 1 Nov 2011 18:11:34 GMT"}, {"version": "v5", "created": "Thu, 26 Sep 2019 14:27:31 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""]]}, {"id": "0904.3132", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni and Victor Chernozhukov", "title": "Posterior Inference in Curved Exponential Families under Increasing\n  Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the large sample properties of the posterior-based\ninference in the curved exponential family under increasing dimension. The\ncurved structure arises from the imposition of various restrictions on the\nmodel, such as moment restrictions, and plays a fundamental role in\neconometrics and others branches of data analysis. We establish conditions\nunder which the posterior distribution is approximately normal, which in turn\nimplies various good properties of estimation and inference procedures based on\nthe posterior. In the process we also revisit and improve upon previous results\nfor the exponential family under increasing dimension by making use of\nconcentration of measure. We also discuss a variety of applications to\nhigh-dimensional versions of the classical econometric models including the\nmultinomial model with moment restrictions, seemingly unrelated regression\nequations, and single structural equation models. In our analysis, both the\nparameter dimension and the number of moments are increasing with the sample\nsize.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2009 22:41:12 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2011 18:03:07 GMT"}, {"version": "v3", "created": "Sun, 1 Jul 2012 17:01:33 GMT"}, {"version": "v4", "created": "Wed, 23 Apr 2014 01:24:41 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""]]}, {"id": "0904.3915", "submitter": "Emilia Nascimento", "authors": "B. de B. Pereira, E.M. Nascimento, F. Felix, G. F. M. Rezende", "title": "Using survival curves for comparison of ordinal qualitative data in\n  clinical studies", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective: The survival-agreement plot was proposed and\nimproved to assess the reliability of a quantitative measure. We propose the\nuse of survival analysis as an alternative non-parametric approach for\ncomparison of ordinal qualitative data.\n  Study Design and Setting: Two case studies were presented. The first one is\nrelated to a randomized, double blind, placebo-controlled clinical trial to\ninvestigate the safety and efficacy of silymarin/metionin for chronic hepatitis\nC. The second one is a prospective study to identify gustatory alterations due\nto chorda tympani nerve involvement in patients with chronic otitis media\nwithout prior surgery.\n  Results: No significant difference was detected between the two treatments\nrelated to the chronic hepatitis C (p > 0.5). On the other hand, a significant\nassociation was observed between the healthy side and the affected side of the\nface of patients with chronic otitis media related to gustatory alterations (p\n< 0.05).\n  Conclusion: The proposed method can serve as an alternative procedure to\nstatistical test for comparison of samples from ordinal qualitative variables.\nThis approach has the advantage of being more familiar to clinical researchers.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2009 18:34:06 GMT"}], "update_date": "2009-04-27", "authors_parsed": [["Pereira", "B. de B.", ""], ["Nascimento", "E. M.", ""], ["Felix", "F.", ""], ["Rezende", "G. F. M.", ""]]}, {"id": "0904.4139", "submitter": "Artur Lemonte", "authors": "Artur J. Lemonte", "title": "Influence diagnostics in Birnbaum-Saunders nonlinear regression models", "comments": "10 pages, submitted for publication", "journal-ref": null, "doi": "10.1080/02664761003692357", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the issue of assessing influence of observations in the class of\nBirnbaum-Saunders nonlinear regression models, which is useful in lifetime data\nanalysis. Our results generalize those in Galea et al. [2004, Influence\ndiagnostics in log-Birnbaum-Saunders regression models. Journal of Applied\nStatistics, 31, 1049-1064] which are confined to Birnbaum-Saunders linear\nregression models. Some influence methods, such as the local influence, total\nlocal influence of an individual and generalized leverage are discussed.\nAdditionally, the normal curvatures of local influence are derived under\nvarious perturbation schemes.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2009 11:58:48 GMT"}], "update_date": "2011-11-22", "authors_parsed": [["Lemonte", "Artur J.", ""]]}, {"id": "0904.4416", "submitter": "Nicole Kraemer", "authors": "Nicole Kraemer", "title": "On the Peaking Phenomenon of the Lasso in Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I briefly report on some unexpected results that I obtained when optimizing\nthe model parameters of the Lasso. In simulations with varying\nobservations-to-variables ratio n=p, I typically observe a strong peak in the\ntest error curve at the transition point n/p = 1. This peaking phenomenon is\nwell-documented in scenarios that involve the inversion of the sample\ncovariance matrix, and as I illustrate in this note, it is also the source of\nthe peak for the Lasso. The key problem is the parametrization of the Lasso\npenalty (as e.g. in the current R package lars) and I present a solution in\nterms of a normalized Lasso parameter.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2009 14:49:12 GMT"}], "update_date": "2009-04-29", "authors_parsed": [["Kraemer", "Nicole", ""]]}, {"id": "0904.4891", "submitter": "Robert B. Gramacy", "authors": "Tamara Broderick and Robert B. Gramacy", "title": "Classification and categorical inputs with treed Gaussian process models", "comments": "24 pages (now single spaced), 8 figures, 2 tables, presented at IFCS\n  2009 and accepted at JoC", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing the successes of treed Gaussian process (TGP) models as an\ninterpretable and thrifty model for nonparametric regression, we seek to extend\nthe model to classification. Both treed models and Gaussian processes (GPs)\nhave, separately, enjoyed great success in application to classification\nproblems. An example of the former is Bayesian CART. In the latter, real-valued\nGP output may be utilized for classification via latent variables, which\nprovide classification rules by means of a softmax function. We formulate a\nBayesian model averaging scheme to combine these two models and describe a\nMonte Carlo method for sampling from the full posterior distribution with joint\nproposals for the tree topology and the GP parameters corresponding to latent\nvariables at the leaves. We concentrate on efficient sampling of the latent\nvariables, which is important to obtain good mixing in the expanded parameter\nspace. The tree structure is particularly helpful for this task and also for\ndeveloping an efficient scheme for handling categorical predictors, which\ncommonly arise in classification problems. Our proposed classification TGP\n(CTGP) methodology is illustrated on a collection of synthetic and real data\nsets. We assess performance relative to existing methods and thereby show how\nCTGP is highly flexible, offers tractable inference, produces rules that are\neasy to interpret, and performs well out of sample.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2009 17:20:08 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2010 18:01:28 GMT"}, {"version": "v3", "created": "Sun, 26 Sep 2010 17:42:04 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Broderick", "Tamara", ""], ["Gramacy", "Robert B.", ""]]}]