[{"id": "1912.00037", "submitter": "Ryan Martin", "authors": "Joyce Cahoon and Ryan Martin", "title": "Generalized inferential models for censored data", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferential challenges that arise when data are censored have been\nextensively studied under the classical frameworks. In this paper, we provide\nan alternative generalized inferential model approach whose output is a\ndata-dependent plausibility function. This construction is driven by an\nassociation between the distribution of the relative likelihood function at the\ninterest parameter and an unobserved auxiliary variable. The plausibility\nfunction emerges from the distribution of a suitably calibrated random set\ndesigned to predict that unobserved auxiliary variable. The evaluation of this\nplausibility function requires a novel use of the classical Kaplan--Meier\nestimator to estimate the censoring rather than the event distribution. We\nprove that the proposed method provides valid inference, at least\napproximately, and our real- and simulated-data examples demonstrate its\nsuperior performance compared to existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 19:01:29 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Cahoon", "Joyce", ""], ["Martin", "Ryan", ""]]}, {"id": "1912.00039", "submitter": "Andrew Spieker", "authors": "Andrew J. Spieker, Nicholas Illenberger, Jason A. Roy, and Nandita\n  Mitra", "title": "Net benefit separation and the determination curve: a probabilistic\n  framework for cost-effectiveness estimation", "comments": "10 pages; 5 figures; 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considerations regarding clinical effectiveness and cost are essential in\ncomparing the overall value of two treatments. There has been growing interest\nin methodology to integrate cost and effectiveness measures in order to inform\npolicy and promote adequate resource allocation. The net monetary benefit\naggregates information on differences in mean cost and clinical outcomes; the\ncost-effectiveness acceptability curve was then developed to characterize the\nextent to which the strength of evidence regarding net monetary benefit changes\nwith fluctuations in the willingness-to-pay threshold. Methods to derive\ninsights from characteristics of the cost/clinical outcomes besides mean\ndifferences remain undeveloped but may also be informative. We propose a novel\nprobabilistic measure of cost-effectiveness based on the stochastic ordering of\nthe individual net benefit distribution under each treatment. Our approach is\nable to accommodate features frequently encountered in observational data\nincluding confounding and censoring, and complements the net monetary benefit\nin the insights it provides. We conduct a range of simulations to evaluate\nfinite-sample performance and illustrate our proposed approach using simulated\ndata based on a study of endometrial cancer patients.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 19:06:24 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 04:03:30 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Spieker", "Andrew J.", ""], ["Illenberger", "Nicholas", ""], ["Roy", "Jason A.", ""], ["Mitra", "Nandita", ""]]}, {"id": "1912.00111", "submitter": "Cecilia Balocchi", "authors": "Cecilia Balocchi, Sameer K. Deshpande, Edward I. George and Shane T.\n  Jensen", "title": "Crime in Philadelphia: Bayesian Clustering with Particle Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation of the change in crime over time is a critical first step\ntowards better understanding of public safety in large urban environments.\nBayesian hierarchical modeling is a natural way to study spatial variation in\nurban crime dynamics at the neighborhood level, since it facilitates principled\n\"sharing of information\" between spatially adjacent neighborhoods. Typically,\nhowever, cities contain many physical and social boundaries that may manifest\nas spatial discontinuities in crime patterns. In this situation, standard prior\nchoices often yield overly-smooth parameter estimates, which can ultimately\nproduce miscalibrated forecasts. To prevent potential over-smoothing, we\nintroduce a prior that partitions the set of neighborhoods into several\nclusters and encourages spatial smoothness within each cluster. In terms of\nmodel implementation, conventional stochastic search techniques are\ncomputationally prohibitive, as they must traverse a combinatorially vast space\nof partitions. We introduce an ensemble optimization procedure that\nsimultaneously identifies several high probability partitions by solving one\noptimization problem using a new local search strategy. We then use the\nidentified partitions to estimate crime trends in Philadelphia between 2006 and\n2017. On simulated and real data, our proposed method demonstrates good\nestimation and partition selection performance. Supplementary materials for\nthis article are available online.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 01:45:18 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 18:37:16 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Balocchi", "Cecilia", ""], ["Deshpande", "Sameer K.", ""], ["George", "Edward I.", ""], ["Jensen", "Shane T.", ""]]}, {"id": "1912.00277", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb, Pasquale Cirillo", "title": "Branching epistemic uncertainty and thickness of tails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is an epistemological approach to errors in both inference and risk\nmanagement, leading to necessary structural properties for the probability\ndistribution.\n  Many mechanisms have been used to show the emergence of fat tails. Here we\nfollow an alternative route, the epistemological one, using counterfactual\nanalysis, and show how nested uncertainty, that is, errors on the error in\nestimation of parameters, lead to fattailedness of the distribution.\n  The results have relevant implications for forecasting, dealing with model\nrisk and generally all statistical analyses. The more interesting results are\nas follows: + The forecasting paradox: The future is fatter tailed than the\npast. Further, out of sample results should be fatter tailed than in-sample\nones. + Errors on errors can be explosive or implosive with different\nconsequences. Infinite recursions can be easily dealt with, pending on the\nstructure of the errors.\n  We also present a method to perform counterfactual analysis without the\nexplosion of branching counterfactuals.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 22:47:09 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 10:05:13 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Taleb", "Nassim Nicholas", ""], ["Cirillo", "Pasquale", ""]]}, {"id": "1912.00295", "submitter": "Samiran Sinha", "authors": "Tong Wang, Kejun He, Wei Ma, Dipankar Bandyopadhyay and Samiran Sinha", "title": "Efficient Estimation of Mixture Cure Frailty Model for Clustered Current\n  Status Data", "comments": "Unstable EM algorithm due to limited information in current status\n  data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current status data abounds in the field of epidemiology and public health,\nwhere the only observable data for a subject is the random inspection time and\nthe event status at inspection. Motivated by such a current status data from a\nperiodontal study where data are inherently clustered, we propose a unified\nmethodology to analyze such complex data. We allow the time-to-event to follow\nthe semiparametric GOR model with a cure fraction, and develop a unified\nestimation scheme powered by the EM algorithm. The within-subject correlation\nis accounted for by a random (frailty) effect, and the non-parametric component\nof the GOR model is approximated via penalized splines, with a set of knot\npoints that increases with the sample size. Proposed methodology is accompanied\nby a rigorous asymptotic theory, and the related semiparametric efficiency. The\nfinite sample performance of our model parameters are assessed via simulation\nstudies. Furthermore, the proposed methodology is illustrated via application\nto the oral health data, accompanied by diagnostic checks to identify\ninfluential observations. An easy to use R package CRFCSD is also available for\nimplementation.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 00:41:22 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 15:07:41 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 15:32:29 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Wang", "Tong", ""], ["He", "Kejun", ""], ["Ma", "Wei", ""], ["Bandyopadhyay", "Dipankar", ""], ["Sinha", "Samiran", ""]]}, {"id": "1912.00360", "submitter": "Meng Xu", "authors": "Meng Xu and Philip T. Reiss", "title": "Distribution-Free Pointwise Adjusted P-Values for Functional Hypotheses", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-47756-1_32", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical tests assess whether a function of interest departs from an\nenvelope of functions generated under a simulated null distribution. This\napproach originated in spatial statistics, but has recently gained some\npopularity in functional data analysis. Whereas such envelope tests examine\ndeviation from a functional null distribution in an omnibus sense, in some\napplications we wish to do more: to obtain p-values at each point in the\nfunction domain, adjusted to control the familywise error rate. Here we derive\npointwise adjusted p-values based on envelope tests, and relate these to\nprevious approaches for functional data under distributional assumptions. We\nthen present two alternative distribution-free p-value adjustments that offer\ngreater power. The methods are illustrated with an analysis of age-varying sex\neffects on cortical thickness in the human brain.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 08:58:31 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Xu", "Meng", ""], ["Reiss", "Philip T.", ""]]}, {"id": "1912.00478", "submitter": "Rida Benhaddou", "authors": "Rida Benhaddou", "title": "Anisotropic Functional Deconvolution for the irregular design with\n  dependent long-memory errors", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anisotropic functional deconvolution model is investigated in the bivariate\ncase under long-memory errors when the design points $t_i$, $i=1, 2, \\cdots,\nN$, and $x_l$, $l=1, 2, \\cdots, M$, are irregular and follow known densities\n$h_1$, $h_2$, respectively. In particular, we focus on the case when the\ndensities $h_1$ and $h_2$ have singularities, but $1/h_1$ and $1/h_2$ are still\nintegrable on $[0, 1]$. Under both Gaussian and sub-Gaussian errors, we\nconstruct an adaptive wavelet estimator that attains asymptotically\nnear-optimal convergence rates that deteriorate as long-memory strengthens. The\nconvergence rates are completely new and depend on a balance between the\nsmoothness and the spatial homogeneity of the unknown function $f$, the degree\nof ill-posed-ness of the convolution operator, the long-memory parameter in\naddition to the degrees of spatial irregularity associated with $h_1$ and\n$h_2$. Nevertheless, the spatial irregularity affects convergence rates only\nwhen $f$ is spatially inhomogeneous in either direction.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 19:03:19 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 00:15:20 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Benhaddou", "Rida", ""]]}, {"id": "1912.00487", "submitter": "Giorgos Bakoyannis", "authors": "Giorgos Bakoyannis", "title": "Nonparametric analysis of nonhomogeneous multi-state processes based on\n  clustered observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequently, clinical trials and observational studies involve complex event\nhistory data with multiple events. When the observations are independent, the\nanalysis of such studies can be based on standard methods for multi-state\nmodels. However, the independence assumption is often violated, such as in\nmulticenter studies, which makes the use of standard methods improper. In this\nwork we address the issue of nonparametric estimation and two-sample testing\nfor the population-averaged transition and state occupation probabilities under\ngeneral multi-state models based on right-censored, left-truncated, and\nclustered observations. The proposed methods do not impose assumptions\nregarding the within-cluster dependence, allow for informative cluster size,\nand are applicable to both Markov and non-Markov processes. Using empirical\nprocess theory, the estimators are shown to be uniformly consistent and to\nconverge weakly to tight Gaussian processes. Closed-form variance estimators\nare derived, rigorous methodology for the calculation of simultaneous\nconfidence bands is proposed, and the asymptotic properties of the\nnonparametric tests are established. Furthermore, we provide theoretical\narguments for the validity of the nonparametric cluster bootstrap, which can be\nreadily implemented in practice regardless of how complex the underlying\nmulti-state model is. Simulation studies show that the performance of the\nproposed methods is good, and that methods that ignore the within-cluster\ndependence can lead to invalid inferences. Finally, the methods are applied to\ndata from a multicenter randomized controlled trial.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 19:35:35 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 21:03:05 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 18:46:43 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Bakoyannis", "Giorgos", ""]]}, {"id": "1912.00540", "submitter": "Alan Pearse", "authors": "Alan R. Pearse, James M. McGree, Nicholas A. Som, Catherine Leigh, Jay\n  M. Ver Hoef, Paul Maxwell and Erin E. Peterson", "title": "SSNdesign -- an R package for pseudo-Bayesian optimal and adaptive\n  sampling designs on stream networks", "comments": "Main document: 18 pages, 7 figures Supp Info A: 11 pages, 0 figures\n  Supp Info B: 24 pages, 6 figures Supp Info C: 3 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streams and rivers are biodiverse and provide valuable ecosystem services.\nMaintaining these ecosystems is an important task, so organisations often\nmonitor the status and trends in stream condition and biodiversity using field\nsampling and, more recently, autonomous in-situ sensors. However, data\ncollection is often costly and so effective and efficient survey designs are\ncrucial to maximise information while minimising costs. Geostatistics and\noptimal and adaptive design theory can be used to optimise the placement of\nsampling sites in freshwater studies and aquatic monitoring programs.\nGeostatistical modelling and experimental design on stream networks pose\nstatistical challenges due to the branching structure of the network, flow\nconnectivity and directionality, and differences in flow volume. Thus, unique\nchallenges of geostatistics and experimental design on stream networks\nnecessitates the development of new open-source software for implementing the\ntheory. We present SSNdesign, an R package for solving optimal and adaptive\ndesign problems on stream networks that integrates with existing open-source\nsoftware. We demonstrate the mathematical foundations of our approach, and\nillustrate the functionality of SSNdesign using two case studies involving real\ndata from Queensland, Australia. In both case studies we demonstrate that the\noptimal or adaptive designs outperform random and spatially balanced survey\ndesigns. The SSNdesign package has the potential to boost the efficiency of\nfreshwater monitoring efforts and provide much-needed information for\nfreshwater conservation and management.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 01:48:09 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Pearse", "Alan R.", ""], ["McGree", "James M.", ""], ["Som", "Nicholas A.", ""], ["Leigh", "Catherine", ""], ["Hoef", "Jay M. Ver", ""], ["Maxwell", "Paul", ""], ["Peterson", "Erin E.", ""]]}, {"id": "1912.00568", "submitter": "Ziyan Zhang", "authors": "Ziyan Zhang", "title": "Inference for Synthetic Control Methods with Multiple Treated Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the Synthetic Control Method (SCM) is now widely applied, its most\ncommonly-used inference method, placebo test, is often problematic, especially\nwhen the treatment is not uniquely assigned. This paper discuss the problems\nwith the placebo test under multivariate treatment case. And, to improve the\npower of inferences, I further propose an Andrews-type procedure as it\npotentially solve some drawbacks of placebo test. Simulations are conducted to\nshow the Andrews' test is often valid and powerful, compared with the placebo\ntest.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 03:13:08 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Zhang", "Ziyan", ""]]}, {"id": "1912.00687", "submitter": "Valeria Vitelli", "authors": "Valeria Vitelli", "title": "A novel framework for joint sparse clustering and alignment of\n  functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for sparse functional clustering that also\nembeds an alignment step. Sparse functional clustering means finding a grouping\nstructure while jointly detecting the parts of the curves' domains where their\ngrouping structure shows the most. Misalignment is a well-known issue in\nfunctional data analysis, that can heavily affect functional clustering results\nif not properly handled. Therefore, we develop a sparse functional clustering\nprocedure that accounts for the possible curve misalignment: the coherence of\nthe functional measure used in the clustering step to the class where the\nwarping functions are chosen is ensured, and the well-posedness of the sparse\nclustering problem is proved. A possible implementing algorithm is also\nproposed, that jointly performs all these tasks: clustering, alignment, and\ndomain selection. The method is tested on simulated data in various realistic\nsituations, and its application to the Berkeley Growth Study data and to the\nAneuRisk65 data set is discussed.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 11:13:26 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Vitelli", "Valeria", ""]]}, {"id": "1912.00762", "submitter": "Iain Johnston", "authors": "Sam F. Greenbury and Mauricio Barahona and Iain G. Johnston", "title": "HyperTraPS: Inferring probabilistic patterns of trait acquisition in\n  evolutionary and disease progression pathways", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion of data throughout the biomedical sciences provides\nunprecedented opportunities to learn about the dynamics of evolution and\ndisease progression, but harnessing these large and diverse datasets remains\nchallenging. Here, we describe a highly generalisable statistical platform to\ninfer the dynamic pathways by which many, potentially interacting, discrete\ntraits are acquired or lost over time in biomedical systems. The platform uses\nHyperTraPS (hypercubic transition path sampling) to learn progression pathways\nfrom cross-sectional, longitudinal, or phylogenetically-linked data with\nunprecedented efficiency, readily distinguishing multiple competing pathways,\nand identifying the most parsimonious mechanisms underlying given observations.\nIts Bayesian structure quantifies uncertainty in pathway structure and allows\ninterpretable predictions of behaviours, such as which symptom a patient will\nacquire next. We exploit the model's topology to provide visualisation tools\nfor intuitive assessment of multiple, variable pathways. We apply the method to\novarian cancer progression and the evolution of multidrug resistance in\ntuberculosis, demonstrating its power to reveal previously undetected dynamic\npathways.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 09:26:39 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Greenbury", "Sam F.", ""], ["Barahona", "Mauricio", ""], ["Johnston", "Iain G.", ""]]}, {"id": "1912.00905", "submitter": "Roberta Falcone", "authors": "Roberta Falcone, Angela Montanari, Laura Anderlucci", "title": "Matrix sketching for supervised classification with imbalanced classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix sketching is a recently developed data compression technique. An input\nmatrix A is efficiently approximated with a smaller matrix B, so that B\npreserves most of the properties of A up to some guaranteed approximation\nratio. In so doing numerical operations on big data sets become faster.\nSketching algorithms generally use random projections to compress the original\ndataset and this stochastic generation process makes them amenable to\nstatistical analysis. The statistical properties of sketching algorithms have\nbeen widely studied in the context of multiple linear regression. In this paper\nwe propose matrix sketching as a tool for rebalancing class sizes in supervised\nclassification with imbalanced classes. It is well-known in fact that class\nimbalance may lead to poor classification performances especially as far as the\nminority class is concerned.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 16:33:15 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Falcone", "Roberta", ""], ["Montanari", "Angela", ""], ["Anderlucci", "Laura", ""]]}, {"id": "1912.01089", "submitter": "Zhengze Zhou", "authors": "Zhengze Zhou, Lucas Mentch, Giles Hooker", "title": "$V$-statistics and Variance Estimation", "comments": "This version supersedes the previous technical report titled\n  \"Asymptotic Normality and Variance Estimation For Supervised Ensembles\".\n  Extensive simulations are added and we also provide a more detailed\n  discussion on the bias phenomenon in variance estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a general framework for analyzing asymptotics of\n$V$-statistics. Previous literature on limiting distribution mainly focuses on\nthe cases when $n \\to \\infty$ with fixed kernel size $k$. Under some regularity\nconditions, we demonstrate asymptotic normality when $k$ grows with $n$ by\nutilizing existing results for $U$-statistics. The key in our approach lies in\na mathematical reduction to $U$-statistics by designing an equivalent kernel\nfor $V$-statistics. We also provide a unified treatment on variance estimation\nfor both $U$- and $V$-statistics by observing connections to existing methods\nand proposing an empirically more accurate estimator. Ensemble methods such as\nrandom forests, where multiple base learners are trained and aggregated for\nprediction purposes, serve as a running example throughout the paper because\nthey are a natural and flexible application of $V$-statistics.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 21:42:19 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 02:08:01 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zhou", "Zhengze", ""], ["Mentch", "Lucas", ""], ["Hooker", "Giles", ""]]}, {"id": "1912.01157", "submitter": "Xu Han", "authors": "Xu Han", "title": "Nonparametric Screening under Conditional Strictly Convex Loss for\n  Ultrahigh Dimensional Sparse Data", "comments": "Supplementary materials including the technical proofs are available\n  online at Annals of Statistics", "journal-ref": "Annals of Statistics, 2019, Vol 47, No 4, 1995-2022", "doi": "10.1214/18-AOS1738", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sure screening technique has been considered as a powerful tool to handle the\nultrahigh dimensional variable selection problems, where the dimensionality p\nand the sample size n can satisfy the NP dimensionality log p=O(n^a) for some\na>0 (Fan & Lv 2008). The current paper aims to simultaneously tackle the\n\"universality\" and \"effectiveness\" of sure screening procedures. For the\n\"universality\", we develop a general and unified framework for nonparametric\nscreening methods from a loss function perspective. Consider a loss function to\nmeasure the divergence of the response variable and the underlying\nnonparametric function of covariates. We newly propose a class of loss\nfunctions called conditional strictly convex loss, which contains, but is not\nlimited to, negative log likelihood loss from one-parameter exponential\nfamilies, exponential loss for binary classification and quantile regression\nloss. The sure screening property and model selection size control will be\nestablished within this class of loss functions. For the ``effectiveness\", we\nfocus on a goodness of fit nonparametric screening (Goffins) method under\nconditional strictly convex loss. Interestingly, we can achieve a better\nconvergence probability of containing the true model compared with related\nliterature. The superior performance of our proposed method has been further\ndemonstrated by extensive simulation studies and some real scientific data\nexample.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 02:26:48 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Han", "Xu", ""]]}, {"id": "1912.01194", "submitter": "Kenichiro McAlinn", "authors": "Kenichiro McAlinn and Kosaku Takanashi", "title": "Mean-shift least squares model averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new estimator for selecting weights to average over\nleast squares estimates obtained from a set of models. Our proposed estimator\nbuilds on the Mallows model average (MMA) estimator of Hansen (2007), but,\nunlike MMA, simultaneously controls for location bias and regression error\nthrough a common constant. We show that our proposed estimator-- the mean-shift\nMallows model average (MSA) estimator-- is asymptotically optimal to the\noriginal MMA estimator in terms of mean squared error. A simulation study is\npresented, where we show that our proposed estimator uniformly outperforms the\nMMA estimator.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:19:17 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["McAlinn", "Kenichiro", ""], ["Takanashi", "Kosaku", ""]]}, {"id": "1912.01200", "submitter": "Murthy Mittinty N", "authors": "Murthy N Mittinty and Stijn Vansteelandt", "title": "Longitudinal Mediation Analysis Using Natural Effect Models", "comments": "30 pages, 5 figures, 3 Tables, 3 Appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis is concerned with the decomposition of the total effect of\nan exposure on an outcome into the indirect effect through a given mediator,\nand the remaining direct effect. This is ideally done using longitudinal\nmeasurements of the mediator, as these capture the mediator process more\nfinely. However, longitudinal measurements pose challenges for mediation\nanalysis. This is because the mediators and outcomes measured at a given\ntime-point can act as confounders for the association between mediators and\noutcomes at a later time-point; these confounders are themselves affected by\nthe prior exposure and outcome. Such post-treatment confounding cannot be dealt\nwith using standard methods (e.g. generalized estimating equations). Analysis\nis further complicated by the need for so-called cross-world counterfactuals to\ndecompose the total effect. This article addresses these challenges. In\nparticular, we introduce so-called natural effect models, which parameterize\nthe direct and indirect effect of a baseline exposure w.r.t. a longitudinal\nmediator and outcome. These can be viewed as a generalization of marginal\nstructural models to enable effect decomposition. We introduce inverse\nprobability weighting techniques for fitting these models, adjusting for\n(measured) time-varying confounding of the mediator-outcome association.\nApplication of this methodology uses data from the Millennium Cohort Study, UK.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 05:45:55 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Mittinty", "Murthy N", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "1912.01376", "submitter": "Haziq Jamil", "authors": "Haziq Jamil, Wicher Bergsma", "title": "iprior: An R Package for Regression Modelling using I-priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is an overview of the R package iprior, which implements a unified\nmethodology for fitting parametric and nonparametric regression models,\nincluding additive models, multilevel models, and models with one or more\nfunctional covariates. Based on the principle of maximum entropy, an I-prior is\nan objective Gaussian process prior for the regression function with covariance\nkernel equal to its Fisher information. The regression function is estimated by\nits posterior mean under the I-prior, and hyperparameters are estimated via\nmaximum marginal likelihood. Estimation of I-prior models is simple and\ninference straightforward, while small and large sample predictive performances\nare comparative, and often better, to similar leading state-of-the-art models.\nWe illustrate the use of the iprior package by analysing a simulated toy data\nset as well as three real-data examples, in particular, a multilevel data set,\na longitudinal data set, and a dataset involving a functional covariate.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 06:11:57 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Jamil", "Haziq", ""], ["Bergsma", "Wicher", ""]]}, {"id": "1912.01388", "submitter": "Bj\\\"orn B\\\"ottcher", "authors": "Bj\\\"orn B\\\"ottcher", "title": "Copula versions of distance multivariance and dHSIC via the\n  distributional transform -- a general approach to construct invariant\n  dependence measures", "comments": "improved notation", "journal-ref": "Statistics (2020)", "doi": "10.1080/02331888.2020.1748029", "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate Hilbert-Schmidt-Independence-Criterion (dHSIC) and distance\nmultivariance allow to measure and test independence of an arbitrary number of\nrandom vectors with arbitrary dimensions. Here we define versions which only\ndepend on an underlying copula. The approach is based on the distributional\ntransform, yielding dependence measures which always feature a natural\ninvariance with respect to scalings and translations. Moreover, it requires no\ndistributional assumptions, i.e., the distributions can be of pure type or any\nmixture of discrete and continuous distributions and (in our setting) no\nexistence of moments is required.\n  Empirical estimators and tests, which are consistent against all\nalternatives, are provided based on a Monte Carlo distributional transform. In\nparticular, it is shown that the new estimators inherit the exact limiting\ndistributional properties of the original estimators. Examples illustrate that\ntests based on the new measures can be more powerful than tests based on other\ncopula dependence measures.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:20:56 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 10:25:02 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["B\u00f6ttcher", "Bj\u00f6rn", ""]]}, {"id": "1912.01422", "submitter": "Anthony Constantinou", "authors": "Norman Fenton, Martin Neil and Anthony Constantinou", "title": "Simpson's Paradox and the implications for medical trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Simpson's paradox, and explains its serious implications\nfor randomised control trials. In particular, we show that for any number of\nvariables we can simulate the result of a controlled trial which uniformly\npoints to one conclusion (such as 'drug is effective') for every possible\ncombination of the variable states, but when a previously unobserved\nconfounding variable is included every possible combination of the variables\nstate points to the opposite conclusion ('drug is not effective'). In other\nwords no matter how many variables are considered, and no matter how\n'conclusive' the result, one cannot conclude the result is truly 'valid' since\nthere is theoretically an unobserved confounding variable that could completely\nreverse the result.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:47:16 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Fenton", "Norman", ""], ["Neil", "Martin", ""], ["Constantinou", "Anthony", ""]]}, {"id": "1912.01505", "submitter": "Shu-Chuan Chen", "authors": "Shu-Chuan Chen, Lung-An Li, and Jiping He", "title": "An integrated heterogeneous Poisson model for neuron functions in hand\n  movement during reaching and grasp", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand potential encoding mechanism of motor cortical neurons for\ncontrol commands during reach-to-grasp movements, experiments to record\nneuronal activities from primary motor cortical regions have been conducted in\nmany research laboratories (for example, (7), (17)). The most popular approach\nin neuroscience community is to fit the Analysis of Variance (ANOVA) model\nusing the firing rates of individual neurons. In addition to consider neural\nfiring counts but also temporal intervals, (5) proposed to apply Analysis of\nCovariance (ANCOVA) model. Due to the nature of the data, in this paper we\npropose to apply an integrated method, called heterogeneous Poisson regression\nmodel, to categorize different neural activities. Three scenarios are discussed\nto show that the proposed heterogeneous Poisson regression model can overcome\nsome disadvantages of the traditional Poisson regression model.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 07:30:55 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Chen", "Shu-Chuan", ""], ["Li", "Lung-An", ""], ["He", "Jiping", ""]]}, {"id": "1912.01578", "submitter": "Eric Nyarko Mr.", "authors": "Eric Nyarko", "title": "Optimal designs for third-order interactions in paired comparison\n  experiments", "comments": "21 pages, 3 tables, higher order interactions for paired comparison\n  experiments with full or partial profiles. arXiv admin note: text overlap\n  with arXiv:1908.06090 and arXiv:1908.06092", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown how by not losing information on higher order interactions,\noptimal paired comparison designs involving alternatives of either full or\npartial profiles to reduce information overload as frequently encountered in\napplications can be constructed which enable identification of main effects up\nto third-order interactions when all attributes have general common number of\nlevels.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 08:40:29 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Nyarko", "Eric", ""]]}, {"id": "1912.01808", "submitter": "Jingyi Kenneth Tay", "authors": "J. Kenneth Tay, Robert Tibshirani", "title": "Reluctant generalized additive modeling", "comments": "Change of method name, R package now available", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse generalized additive models (GAMs) are an extension of sparse\ngeneralized linear models which allow a model's prediction to vary non-linearly\nwith an input variable. This enables the data analyst build more accurate\nmodels, especially when the linearity assumption is known to be a poor\napproximation of reality. Motivated by reluctant interaction modeling (Yu et\nal. 2019), we propose a multi-stage algorithm, called $\\textit{reluctant\ngeneralized additive modeling (RGAM)}$, that can fit sparse generalized\nadditive models at scale. It is guided by the principle that, if all else is\nequal, one should prefer a linear feature over a non-linear feature. Unlike\nexisting methods for sparse GAMs, RGAM can be extended easily to binary, count\nand survival data. We demonstrate the method's effectiveness on real and\nsimulated examples.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 05:52:09 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 22:28:46 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Tay", "J. Kenneth", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1912.01832", "submitter": "Youngrae Kim", "authors": "Youngrae Kim, Sangkyun Kim, Johan Lim, Sungim Lee, Won Son and Heejin\n  Hwang", "title": "Covariate-dependent control limits for the detection of abnormal price\n  changes in scanner data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, large-scale sales data for consumer goods, called scanner data,\nare obtained by scanning the bar codes of individual products at the points of\nsale of retail outlets. Many national statistical offices use scanner data to\nbuild consumer price statistics. In this process, as in other statistical\nprocedures, the detection of abnormal transactions in sales prices is an\nimportant step in the analysis. Popular methods for conducting such outlier\ndetection are the quartile method, the Hidiroglou-Berthelot method, the\nresistant fences method, and the Tukey algorithm. These methods are based\nsolely on information about price changes and not on any of the other\ncovariates (e.g., sales volume or types of retail shops) that are also\navailable from scanner data. In this paper, we propose a new method to detect\nabnormal price changes that takes into account an additional covariate, namely,\nsales volume. We assume that the variance of the log of the price change is a\nsmooth function of the sales volume and estimate the function from previously\nobserved data. We numerically show the advantages of the new method over\nexisting methods. We also apply the methods to real scanner data collected at\nweekly intervals by the Korean Chamber of Commerce and Industry between 2013\nand 2014 and compare their performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 07:27:37 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 09:16:03 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Kim", "Youngrae", ""], ["Kim", "Sangkyun", ""], ["Lim", "Johan", ""], ["Lee", "Sungim", ""], ["Son", "Won", ""], ["Hwang", "Heejin", ""]]}, {"id": "1912.01833", "submitter": "Kyoungjae Lee", "authors": "Kyoungjae Lee and Xuan Cao", "title": "Bayesian Group Selection in Logistic Regression with Application to MRI\n  Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian logistic regression models with group-structured\ncovariates. In high-dimensional settings, it is often assumed that only small\nportion of groups are significant, thus consistent group selection is of\nsignificant importance. While consistent frequentist group selection methods\nhave been proposed, theoretical properties of Bayesian group selection methods\nfor logistic regression models have not been investigated yet. In this paper,\nwe consider a hierarchical group spike and slab prior for logistic regression\nmodels in high-dimensional settings. Under mild conditions, we establish strong\ngroup selection consistency of the induced posterior, which is the first\ntheoretical result in the Bayesian literature. Through simulation studies, we\ndemonstrate that the performance of the proposed method outperforms existing\nstate-of-the-art methods in various settings. We further apply our method to an\nMRI data set for predicting Parkinson's disease and show its benefits over\nother contenders.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 07:35:56 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Lee", "Kyoungjae", ""], ["Cao", "Xuan", ""]]}, {"id": "1912.01956", "submitter": "Eva-Maria Walz", "authors": "Tilmann Gneiting and Eva-Maria Walz", "title": "Receiver operating characteristic (ROC) movies, universal ROC (UROC)\n  curves, and coefficient of predictive ability (CPA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout science and technology, receiver operating characteristic (ROC)\ncurves and associated area under the curve (AUC) measures constitute powerful\ntools for assessing the predictive abilities of features, markers and tests in\nbinary classification problems. Despite its immense popularity, ROC analysis\nhas been subject to a fundamental restriction, in that it applies to\ndichotomous (yes or no) outcomes only. Here we introduce ROC movies and\nuniversal ROC (UROC) curves that apply to just any linearly ordered outcome,\nalong with an associated coefficient of predictive ability (CPA) measure. CPA\nequals the area under the UROC curve, and admits appealing interpretations in\nterms of probabilities and rank based covariances. For binary outcomes CPA\nequals AUC, and for pairwise distinct outcomes CPA relates linearly to\nSpearman's coefficient, in the same way that the C index relates linearly to\nKendall's coefficient. ROC movies, UROC curves, and CPA nest and generalize the\ntools of classical ROC analysis, and are bound to supersede them in a wealth of\napplications. Their usage is illustrated in data examples from biomedicine and\nmeteorology, where rank based measures yield new insights in the WeatherBench\ncomparison of the predictive performance of convolutional neural networks and\nphysical-numerical models for weather prediction.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 21:16:47 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 13:43:45 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 14:46:15 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Gneiting", "Tilmann", ""], ["Walz", "Eva-Maria", ""]]}, {"id": "1912.02026", "submitter": "Denis Allard", "authors": "Denis Allard, Xavier Emery, C\\'eline Lacaux, Christian Lantu\\'ejoul", "title": "Simulating space-time random fields with nonseparable Gneiting-type\n  covariance functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two algorithms are proposed to simulate space-time Gaussian random fields\nwith a covariance function belonging to an extended Gneiting class, the\ndefinition of which depends on a completely monotone function associated with\nthe spatial structure and a conditionally negative definite function associated\nwith the temporal structure. In both cases, the simulated random field is\nconstructed as a weighted sum of cosine waves, with a Gaussian spatial\nfrequency vector and a uniform phase. The difference lies in the way to handle\nthe temporal component. The first algorithm relies on a spectral decomposition\nin order to simulate a temporal frequency conditional upon the spatial one,\nwhile in the second algorithm the temporal frequency is replaced by an\nintrinsic random field whose variogram is proportional to the conditionally\nnegative definite function associated with the temporal structure. Both\nalgorithms are scalable as their computational cost is proportional to the\nnumber of space-time locations, which may be unevenly spaced in space and/or in\ntime. They are illustrated and validated through synthetic examples.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 14:45:41 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Allard", "Denis", ""], ["Emery", "Xavier", ""], ["Lacaux", "C\u00e9line", ""], ["Lantu\u00e9joul", "Christian", ""]]}, {"id": "1912.02151", "submitter": "Mingli Chen", "authors": "Alexandre Belloni, Mingli Chen, Oscar Hernan Madrid Padilla, Zixuan\n  (Kevin) Wang", "title": "High Dimensional Latent Panel Quantile Regression with an Application to\n  Asset Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalization of the linear panel quantile regression model to\naccommodate both \\textit{sparse} and \\textit{dense} parts: sparse means while\nthe number of covariates available is large, potentially only a much smaller\nnumber of them have a nonzero impact on each conditional quantile of the\nresponse variable; while the dense part is represent by a low-rank matrix that\ncan be approximated by latent factors and their loadings. Such a structure\nposes problems for traditional sparse estimators, such as the\n$\\ell_1$-penalised Quantile Regression, and for traditional latent factor\nestimator, such as PCA. We propose a new estimation procedure, based on the\nADMM algorithm, consists of combining the quantile loss function with $\\ell_1$\n\\textit{and} nuclear norm regularization. We show, under general conditions,\nthat our estimator can consistently estimate both the nonzero coefficients of\nthe covariates and the latent low-rank matrix.\n  Our proposed model has a \"Characteristics + Latent Factors\" Asset Pricing\nModel interpretation: we apply our model and estimator with a large-dimensional\npanel of financial data and find that (i) characteristics have sparser\npredictive power once latent factors were controlled (ii) the factors and\ncoefficients at upper and lower quantiles are different from the median.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 18:03:53 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Belloni", "Alexandre", "", "Kevin"], ["Chen", "Mingli", "", "Kevin"], ["Padilla", "Oscar Hernan Madrid", "", "Kevin"], ["Zixuan", "", "", "Kevin"], ["Wang", "", ""]]}, {"id": "1912.02392", "submitter": "Chencheng Cai", "authors": "Chencheng Cai and Rong Chen and Han Xiao", "title": "KoPA: Automated Kronecker Product Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of matrix approximation and denoising induced by the\nKronecker product decomposition. Specifically, we propose to approximate a\ngiven matrix by the sum of a few Kronecker products of matrices, which we refer\nto as the Kronecker product approximation (KoPA). Because the Kronecker product\nis an extension of the outer product from vectors to matrices, KoPA extends the\nlow rank matrix approximation, and includes it as a special case. Comparing\nwith the latter, KoPA also offers a greater flexibility, since it allows the\nuser to choose the configuration, which are the dimensions of the two smaller\nmatrices forming the Kronecker product. On the other hand, the configuration to\nbe used is usually unknown, and needs to be determined from the data in order\nto achieve the optimal balance between accuracy and parsimony. We propose to\nuse extended information criteria to select the configuration. Under the\nparadigm of high dimensional analysis, we show that the proposed procedure is\nable to select the true configuration with probability tending to one, under\nsuitable conditions on the signal-to-noise ratio. We demonstrate the\nsuperiority of KoPA over the low rank approximations through numerical studies,\nand several benchmark image examples.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:27:01 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 18:52:55 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 20:30:09 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Cai", "Chencheng", ""], ["Chen", "Rong", ""], ["Xiao", "Han", ""]]}, {"id": "1912.02452", "submitter": "Fabian Guignard", "authors": "Fabian Guignard, Mohamed Laib, Federico Amato, Mikhail Kanevski", "title": "Advanced analysis of temporal data using Fisher-Shannon information:\n  theoretical development and application in geosciences", "comments": "18 pages, 5 figures", "journal-ref": "Front. Earth Sci. 8:255 (2020)", "doi": "10.3389/feart.2020.00255", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex non-linear time series are ubiquitous in geosciences. Quantifying\ncomplexity and non-stationarity of these data is a challenging task, and\nadvanced complexity-based exploratory tool are required for understanding and\nvisualizing such data. This paper discusses the Fisher-Shannon method, from\nwhich one can obtain a complexity measure and detect non-stationarity, as an\nefficient data exploration tool. The state-of-the-art studies related to the\nFisher-Shannon measures are collected, and new analytical formulas for positive\nunimodal skewed distributions are proposed. Case studies on both synthetic and\nreal data illustrate the usefulness of the Fisher-Shannon method, which can\nfind application in different domains including time series discrimination and\ngeneration of times series features for clustering, modeling and forecasting.\nThe paper is accompanied with Python and R libraries for the non-parametric\nestimation of the proposed measures.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 09:23:57 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 07:58:27 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Guignard", "Fabian", ""], ["Laib", "Mohamed", ""], ["Amato", "Federico", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1912.02595", "submitter": "Shrijita Bhattacharya", "authors": "Shrijita Bhattacharya and Jan Beirlant", "title": "Outlier detection and a tail-adjusted boxplot based on extreme value\n  theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether an extreme observation is an outlier or not, depends strongly on the\ncorresponding tail behaviour of the underlying distribution. We develop an\nautomatic, data-driven method to identify extreme tail behaviour that deviates\nfrom the intermediate and central characteristics. This allows for detecting\nextreme outliers or sets of extreme data that show less spread than the bulk of\nthe data. To this end we extend a testing method proposed in Bhattacharya et al\n2019 for the specific case of heavy tailed models, to all max-domains of\nattraction. Consequently we propose a tail-adjusted boxplot which yields a more\naccurate representation of possible outliers. Several examples and simulation\nresults illustrate the finite sample behaviour of this approach.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 14:33:40 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Bhattacharya", "Shrijita", ""], ["Beirlant", "Jan", ""]]}, {"id": "1912.02633", "submitter": "Jesse Hemerik", "authors": "Jesse Hemerik, Jelle J. Goeman", "title": "Another look at the Lady Tasting Tea and differences between permutation\n  tests and randomization tests", "comments": "International Statistical Review. Early view version (2020)", "journal-ref": null, "doi": "10.1111/insr.12431", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical literature is known to be inconsistent in the use of the\nterms \"permutation test\" and \"randomization test\". Several authors succesfully\nargue that these terms should be used to refer to two distinct classes of tests\nand that there are major conceptual differences between these classes. The\npresent paper explains an important difference in mathematical reasoning\nbetween these classes: a permutation test fundamentally requires that the set\nof permutations has a group structure, in the algebraic sense; the reasoning\nbehind a randomization test is not based on such a group structure and it is\npossible to use an experimental design that does not correspond to a group. In\nparticular, we can use a randomization scheme where the number of possible\ntreatment patterns is larger than in standard experimental designs. This leads\nto exact \\emph{p}-values of improved resolution, providing increased power for\nvery small significance levels, at the cost of decreased power for larger\nsignificance levels. We discuss applications in randomized trials and\nelsewhere. Further, we explain that Fisher's famous Lady Tasting Tea\nexperiment, which is commonly referred to as the first permutation test, is in\nfact a randomization test. This distinction is important to avoid confusion and\ninvalid tests.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 15:09:50 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 10:02:51 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Hemerik", "Jesse", ""], ["Goeman", "Jelle J.", ""]]}, {"id": "1912.02774", "submitter": "Giorgio Paulon", "authors": "Giorgio Paulon, Fernando Llanos, Bharath Chandrasekaran, Abhra Sarkar", "title": "Bayesian Semiparametric Longitudinal Drift-Diffusion Mixed Models for\n  Tone Learning in Adults", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how adult humans learn non-native speech categories such as\ntone information has shed novel insights into the mechanisms underlying\nexperience-dependent brain plasticity. Scientists have traditionally examined\nthese questions using longitudinal learning experiments under a multi-category\ndecision making paradigm. Drift-diffusion processes are popular in such\ncontexts for their ability to mimic underlying neural mechanisms. Motivated by\nthese problems, we develop a novel Bayesian semiparametric inverse Gaussian\ndrift-diffusion mixed model for multi-alternative decision making in\nlongitudinal settings. We design a Markov chain Monte Carlo algorithm for\nposterior computation. We evaluate the method's empirical performances through\nsynthetic experiments. Applied to our motivating longitudinal tone learning\nstudy, the method provides novel insights into how the biologically\ninterpretable model parameters evolve with learning, differ between\ninput-response tone combinations, and differ between well and poorly performing\nadults.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:13:38 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 23:47:27 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 03:55:46 GMT"}, {"version": "v4", "created": "Mon, 27 Apr 2020 15:23:33 GMT"}, {"version": "v5", "created": "Mon, 15 Jun 2020 15:38:35 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Paulon", "Giorgio", ""], ["Llanos", "Fernando", ""], ["Chandrasekaran", "Bharath", ""], ["Sarkar", "Abhra", ""]]}, {"id": "1912.02793", "submitter": "Matteo Bonvini", "authors": "Matteo Bonvini and Edward H Kennedy", "title": "Sensitivity analysis via the proportion of unmeasured confounding", "comments": "41 pages, 5 figures", "journal-ref": null, "doi": "10.1080/01621459.2020.1864382", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, identification of ATEs is generally achieved by\nassuming that the correct set of confounders has been measured and properly\nincluded in the relevant models. Because this assumption is both strong and\nuntestable, a sensitivity analysis should be performed. Common approaches\ninclude modeling the bias directly or varying the propensity scores to probe\nthe effects of a potential unmeasured confounder. In this paper, we take a\nnovel approach whereby the sensitivity parameter is the \"proportion of\nunmeasured confounding:\" the proportion of units for whom the treatment is not\nas good as randomized even after conditioning on the observed covariates. We\nconsider different assumptions on the probability of a unit being unconfounded.\nIn each case, we derive sharp bounds on the average treatment effect as a\nfunction of the sensitivity parameter and propose nonparametric estimators that\nallow flexible covariate adjustment. We also introduce a one-number summary of\na study's robustness to the number of confounded units. Finally, we explore\nfinite-sample properties via simulation, and apply the methods to an\nobservational database used to assess the effects of right heart\ncatheterization.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 18:37:28 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 12:53:06 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Bonvini", "Matteo", ""], ["Kennedy", "Edward H", ""]]}, {"id": "1912.02872", "submitter": "Linjun Zhang", "authors": "T. Tony Cai and Linjun Zhang", "title": "A Convex Optimization Approach to High-Dimensional Sparse Quadratic\n  Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study high-dimensional sparse Quadratic Discriminant\nAnalysis (QDA) and aim to establish the optimal convergence rates for the\nclassification error. Minimax lower bounds are established to demonstrate the\nnecessity of structural assumptions such as sparsity conditions on the\ndiscriminating direction and differential graph for the possible construction\nof consistent high-dimensional QDA rules. We then propose a classification\nalgorithm called SDAR using constrained convex optimization under the sparsity\nassumptions. Both minimax upper and lower bounds are obtained and this\nclassification rule is shown to be simultaneously rate optimal over a\ncollection of parameter spaces, up to a logarithmic factor. Simulation studies\ndemonstrate that SDAR performs well numerically. The algorithm is also\nillustrated through an analysis of prostate cancer data and colon tissue data.\nThe methodology and theory developed for high-dimensional QDA for two groups in\nthe Gaussian setting are also extended to multi-group classification and to\nclassification under the Gaussian copula model.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 21:03:46 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Linjun", ""]]}, {"id": "1912.02955", "submitter": "Chencheng Cai", "authors": "Chencheng Cai and Rong Chen and Han Xiao", "title": "Hybrid Kronecker Product Decomposition and Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering the underlying low dimensional structure of high dimensional data\nhas attracted a significant amount of researches recently and has shown to have\na wide range of applications. As an effective dimension reduction tool,\nsingular value decomposition is often used to analyze high dimensional\nmatrices, which are traditionally assumed to have a low rank matrix\napproximation. In this paper, we propose a new approach. We assume a high\ndimensional matrix can be approximated by a sum of a small number of Kronecker\nproducts of matrices with potentially different configurations, named as a\nhybird Kronecker outer Product Approximation (hKoPA). It provides an extremely\nflexible way of dimension reduction compared to the low-rank matrix\napproximation. Challenges arise in estimating a hKoPA when the configurations\nof component Kronecker products are different or unknown. We propose an\nestimation procedure when the set of configurations are given and a joint\nconfiguration determination and component estimation procedure when the\nconfigurations are unknown. Specifically, a least squares backfitting algorithm\nis used when the configuration is given. When the configuration is unknown, an\niterative greedy algorithm is used. Both simulation and real image examples\nshow that the proposed algorithms have promising performances. The hybrid\nKronecker product approximation may have potentially wider applications in low\ndimensional representation of high dimensional data\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 02:57:31 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Cai", "Chencheng", ""], ["Chen", "Rong", ""], ["Xiao", "Han", ""]]}, {"id": "1912.02982", "submitter": "Matthew Graham", "authors": "Matthew M. Graham and Alexandre H. Thiery and Alexandros Beskos", "title": "Manifold Markov chain Monte Carlo methods for Bayesian inference in a\n  wide class of diffusion models", "comments": "Updated with additional numerical experiments and improvements to\n  methodology. 50 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for nonlinear diffusions, observed at discrete times, is a\nchallenging task that has prompted the development of a number of algorithms,\nmainly within the computational statistics community. We propose a new\ndirection, and accompanying methodology, borrowing ideas from statistical\nphysics and computational chemistry, for inferring the posterior distribution\nof latent diffusion paths and model parameters, given observations of the\nprocess. Joint configurations of the underlying process noise and of\nparameters, mapping onto diffusion paths consistent with observations, form an\nimplicitly defined manifold. Then, by making use of a constrained Hamiltonian\nMonte Carlo algorithm on the embedded manifold, we are able to perform\ncomputationally efficient inference for an extensive class of discretely\nobserved diffusion models. Critically, in contrast with other approaches\nproposed in the literature, our methodology is highly automated, requiring\nminimal user intervention and applying alike in a range of settings, including:\nelliptic or hypo-elliptic systems; observations with or without noise; linear\nor non-linear observation operators. Exploiting Markovianity, we propose a\nvariant of the method with complexity that scales linearly in the resolution of\npath discretisation and the number of observation times.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 05:55:16 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 12:02:09 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Graham", "Matthew M.", ""], ["Thiery", "Alexandre H.", ""], ["Beskos", "Alexandros", ""]]}, {"id": "1912.03100", "submitter": "Peter Knaus", "authors": "Annalisa Cadonna, Sylvia Fr\\\"uhwirth-Schnatter, Peter Knaus", "title": "Triple the gamma -- A unifying shrinkage prior for variance and variable\n  selection in sparse state space and TVP models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-varying parameter (TVP) models are very flexible in capturing gradual\nchanges in the effect of a predictor on the outcome variable. However, in\nparticular when the number of predictors is large, there is a known risk of\noverfitting and poor predictive performance, since the effect of some\npredictors is constant over time. We propose a prior for variance shrinkage in\nTVP models, called triple gamma. The triple gamma prior encompasses a number of\npriors that have been suggested previously, such as the Bayesian lasso, the\ndouble gamma prior and the Horseshoe prior. We present the desirable properties\nof such a prior and its relationship to Bayesian Model Averaging for variance\nselection. The features of the triple gamma prior are then illustrated in the\ncontext of time varying parameter vector autoregressive models, both for\nsimulated datasets and for a series of macroeconomics variables in the Euro\nArea.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 13:26:01 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Cadonna", "Annalisa", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""], ["Knaus", "Peter", ""]]}, {"id": "1912.03290", "submitter": "Eli Ben-Michael", "authors": "Eli Ben-Michael, Avi Feller, and Jesse Rothstein", "title": "Synthetic Controls with Staggered Adoption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Staggered adoption of policies by different units at different times creates\npromising opportunities for observational causal inference. Estimation remains\nchallenging, however, and common regression methods can give misleading\nresults. A promising alternative is the synthetic control method (SCM), which\nfinds a weighted average of control units that closely balances the treated\nunit's pre-treatment outcomes. In this paper, we generalize SCM, originally\ndesigned to study a single treated unit, to the staggered adoption setting. We\nfirst bound the error for the average effect and show that it depends on both\nthe imbalance for each treated unit separately and the imbalance for the\naverage of the treated units. We then propose \"partially pooled\" SCM weights to\nminimize a weighted combination of these measures; approaches that focus only\non balancing one of the two components can lead to bias. We extend this\napproach to incorporate unit-level intercept shifts and auxiliary covariates.\nWe assess the performance of the proposed method via extensive simulations and\napply our results to the question of whether teacher collective bargaining\nleads to higher school spending, finding minimal impacts. We implement the\nproposed method in the augsynth R package.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 18:45:24 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 21:52:45 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ben-Michael", "Eli", ""], ["Feller", "Avi", ""], ["Rothstein", "Jesse", ""]]}, {"id": "1912.03337", "submitter": "Thomas Jemielita", "authors": "Thomas O. Jemielita, Devan V. Mehrotra", "title": "PRISM: Patient Response Identifiers for Stratified Medicine", "comments": "24 pages; 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pharmaceutical companies continue to seek innovative ways to explore whether\na drug under development is likely to be suitable for all or only an\nidentifiable stratum of patients in the target population. The sooner this can\nbe done during the clinical development process, the better it is for the\ncompany, and downstream for prescribers, payers, and most importantly, for\npatients. To help enable this vision of stratified medicine, we describe a\npowerful statistical framework, Patient Response Identifiers for Stratified\nMedicine (PRISM), for the discovery of potential predictors of drug response\nand associated subgroups using machine learning tools. PRISM is highly flexible\nand can have many \"configurations\", allowing the incorporation of complementary\nmodels or tools for a variety of outcomes and settings. One promising PRISM\nconfiguration is to use the observed outcomes for subgroup identification,\nwhile using counterfactual within-patient predicted treatment differences for\nsubgroup-specific treatment estimates and associated interpretation. This\nseparates the \"subgroup-identification\" from the \"decision-making\" and, to\nfacilitate clinical design planning, is a simple way to obtain unbiased\ntreatment effect sizes in the discovered subgroups. Simulation results, along\nwith data from a real clinical trial are used to illustrate the utility of the\nproposed PRISM framework.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 20:30:59 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Jemielita", "Thomas O.", ""], ["Mehrotra", "Devan V.", ""]]}, {"id": "1912.03358", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir, Julio Isidro Sanchez", "title": "Adventures in Multi-Omics I: Combining heterogeneous data sets via\n  relationships matrices", "comments": "This project was supported by WheatSustain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST q-bio.GN stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a covariance based method for combining partial\ndata sets in the genotype to phenotype spectrum. In particular, an\nexpectation-maximization algorithm that can be used to combine partially\noverlapping relationship/covariance matrices is introduced. Combining data this\nway, based on relationship matrices, can be contrasted with a feature\nimputation based approach. We used several public genomic data sets to explore\nthe accuracy of combining genomic relationship matrices. We have also used the\nheterogeneous genotype/phenotype data sets in the https://triticeaetoolbox.org/\nto illustrate how this new method can be used in genomic prediction, phenomics,\nand graphical modeling.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 17:39:58 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 15:55:39 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Akdemir", "Deniz", ""], ["Sanchez", "Julio Isidro", ""]]}, {"id": "1912.03387", "submitter": "Octavio Mesner", "authors": "Octavio C\\'esar Mesner and Cosma Rohilla Shalizi", "title": "Conditional Mutual Information Estimation for Mixed Discrete and\n  Continuous Variables with Nearest Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fields like public health, public policy, and social science often want to\nquantify the degree of dependence between variables whose relationships take on\nunknown functional forms. Typically, in fact, researchers in these fields are\nattempting to evaluate causal theories, and so want to quantify dependence\nafter conditioning on other variables that might explain, mediate or confound\ncausal relations. One reason conditional mutual information is not more widely\nused for these tasks is the lack of estimators which can handle combinations of\ncontinuous and discrete random variables, common in applications. This paper\ndevelops a new method for estimating mutual and conditional mutual information\nfor data samples containing a mix of discrete and continuous variables. We\nprove that this estimator is consistent and show, via simulation, that it is\nmore accurate than similar estimators.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 23:28:32 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Mesner", "Octavio C\u00e9sar", ""], ["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1912.03407", "submitter": "Sven Serneels", "authors": "Peter Filzmoser, Sebastiaan H\\\"oppner, Irene Ortner, Sven Serneels and\n  Tim Verdonck", "title": "Cellwise Robust M Regression", "comments": null, "journal-ref": "Computational Statistics and Data Analysis, 147 (2020), 106944", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cellwise robust M regression estimator is introduced as the first\nestimator of its kind that intrinsically yields both a map of cellwise outliers\nconsistent with the linear model, and a vector of regression coefficients that\nis robust against vertical outliers and leverage points. As a by-product, the\nmethod yields a weighted and imputed data set that contains estimates of what\nthe values in cellwise outliers would need to amount to if they had fit the\nmodel. The method is illustrated to be equally robust as its casewise\ncounterpart, MM regression. The cellwise regression method discards less\ninformation than any casewise robust estimator. Therefore, predictive power can\nbe expected to be at least as good as casewise alternatives. These results are\ncorroborated in a simulation study. Moreover, while the simulations show that\npredictive performance is at least on par with casewise methods if not better,\nan application to a data set consisting of compositions of Swiss nutrients,\nshows that in individual cases, CRM can achieve a significantly higher\npredictive accuracy compared to MM regression.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 01:17:26 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 12:50:15 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Filzmoser", "Peter", ""], ["H\u00f6ppner", "Sebastiaan", ""], ["Ortner", "Irene", ""], ["Serneels", "Sven", ""], ["Verdonck", "Tim", ""]]}, {"id": "1912.03423", "submitter": "Chandanie Navaratna", "authors": "Richard A. Lockhart and Chandanie W. Navaratna", "title": "A goodness of fit test for two component two parameter Weibull mixtures", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fitting mixture distributions is needed in applications where data belongs to\ninhomogeneous populations comprising homogeneous sub-populations. The mixing\nproportions of the sub populations are in general unknown and need to be\nestimated as well. A goodness of fit test based on the empirical distribution\nfunction is proposed for assessing the goodness of fit in model fits comprising\ntwo components, each distributed as two parameter Weibull. The applicability of\nthe proposed test procedure was empirically established using a Monte Carlo\nsimulation study. The proposed test procedure can be easily altered to handle\ntwo component mixtures with different component distributions.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 03:09:19 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Lockhart", "Richard A.", ""], ["Navaratna", "Chandanie W.", ""]]}, {"id": "1912.03447", "submitter": "Matthias Wagener", "authors": "Matthias Wagener, Mohammad Arashi", "title": "Mastering the body and tail shape of a distribution", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normal distribution and its perturbation has left an immense mark on the\nstatistical literature. Hence, several generalized forms were developed to\nmodel different skewness, kurtosis, and body shapes. However, it is not easy to\ndistinguish between changes in the relative body and tail shapes when using\nthese generalizations. What we propose is a neat integration approach\ngeneralization which enables the visualization and control of the body and the\ntail shape separately. This provides a flexible modeling opportunity with an\nemphasis on parameter inference and interpretation. Two related models, the\ntwo-piece body-tail generalized normal and the two-piece tail adjusted normal\nare swiftly introduced to demonstrate this inferential potential. The\nmethodology is then demonstrated on heavy and light-tailed data.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 05:56:09 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Wagener", "Matthias", ""], ["Arashi", "Mohammad", ""]]}, {"id": "1912.03549", "submitter": "Juho Timonen", "authors": "Juho Timonen, Henrik Mannerstr\\\"om, Aki Vehtari and Harri\n  L\\\"ahdesm\\\"aki", "title": "lgpr: An interpretable nonparametric method for inferring covariate\n  effects from longitudinal data", "comments": "Contains main manuscript and supplementary material. Tables S1-S3 are\n  in ancillary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal study designs are indispensable for studying disease\nprogression. Inferring covariate effects from longitudinal data, however,\nrequires interpretable methods that can model complicated covariance structures\nand detect nonlinear effects of both categorical and continuous covariates, as\nwell as their interactions. Detecting disease effects is hindered by the fact\nthat they often occur rapidly near the disease initiation time, and this time\npoint cannot be exactly observed. An additional challenge is that the effect\nmagnitude can be heterogeneous over the subjects. We present lgpr, a widely\napplicable and interpretable method for nonparametric analysis of longitudinal\ndata using additive Gaussian processes. We demonstrate that it outperforms\nprevious approaches in identifying the relevant categorical and continuous\ncovariates in various settings. Furthermore, it implements important novel\nfeatures, including the ability to account for the heterogeneity of covariate\neffects, their temporal uncertainty, and appropriate observation models for\ndifferent types of biomedical data. The lgpr tool is implemented as a\ncomprehensive and user-friendly R-package. lgpr is available at\njtimonen.github.io/lgpr-usage with documentation, tutorials, test data, and\ncode for reproducing the experiments of this paper.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 19:36:33 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 10:36:10 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Timonen", "Juho", ""], ["Mannerstr\u00f6m", "Henrik", ""], ["Vehtari", "Aki", ""], ["L\u00e4hdesm\u00e4ki", "Harri", ""]]}, {"id": "1912.03662", "submitter": "Duyeol Lee", "authors": "Duyeol Lee, Kai Zhang, and Michael R. Kosorok", "title": "The Binary Expansion Randomized Ensemble Test (BERET)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the binary expansion testing framework was introduced to test the\nindependence of two continuous random variables by utilizing symmetry\nstatistics that are complete sufficient statistics for dependence. We develop a\nnew test based on an ensemble approach that uses the sum of squared symmetry\nstatistics and distance correlation. Simulation studies suggest that this\nmethod improves the power while preserving the clear interpretation of the\nbinary expansion testing. We extend this method to tests of independence of\nrandom vectors in arbitrary dimension. Through random projections, the proposed\nbinary expansion randomized ensemble test transforms the multivariate\nindependence testing problem into a univariate problem. Simulation studies and\ndata example analyses show that the proposed method provides relatively robust\nperformance compared with existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 11:54:57 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 10:25:20 GMT"}, {"version": "v3", "created": "Sun, 17 May 2020 23:37:36 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2021 20:08:37 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Lee", "Duyeol", ""], ["Zhang", "Kai", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1912.03718", "submitter": "Amartansh Dubey", "authors": "Samruddhi Deshmukh, Amartansh Dubey", "title": "Improved Covariance Matrix Estimator using Shrinkage Transformation and\n  Random Matrix Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges in multivariate analysis is the estimation of\npopulation covariance matrix from sample covariance matrix (SCM). Most recent\ncovariance matrix estimators use either shrinkage transformations or asymptotic\nresults from Random Matrix Theory (RMT). Shrinkage techniques help in pulling\nextreme correlation values towards certain target values whereas tools from RMT\nhelp in removing noisy eigenvalues of SCM. Both of these techniques use\ndifferent approaches to achieve a similar goal which is to remove noisy\ncorrelations and add structure to SCM to overcome the bias-variance trade-off.\nIn this paper, we first critically evaluate the pros and cons of these two\ntechniques and then propose an improved estimator which exploits the advantages\nof both by taking an optimally weighted convex combination of covariance\nmatrices estimated by an improved shrinkage transformation and a RMT based\nfilter. It is a generalized estimator which can adapt to changing sampling\nnoise conditions in various datasets by performing hyperparameter optimization.\nWe show the effectiveness of this estimator on the problem of designing a\nfinancial portfolio with minimum risk. We have chosen this problem because the\ncomplex properties of stock market data provide extreme conditions to test the\nrobustness of a covariance estimator. Using data from four of the world's\nlargest stock exchanges, we show that our proposed estimator outperforms\nexisting estimators in minimizing the out-of-sample risk of the portfolio and\nhence predicts population statistics more precisely. Since covariance analysis\nis a crucial statistical tool, this estimator can be used in a wide range of\nmachine learning, signal processing and high dimensional pattern recognition\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 17:15:58 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Deshmukh", "Samruddhi", ""], ["Dubey", "Amartansh", ""]]}, {"id": "1912.03725", "submitter": "Sneha Jadhav", "authors": "Sneha Jadhav and Shuangge Ma", "title": "Kendall's Tau for Functional Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We treat the problem of testing for association between a functional variable\nbelonging to Hilbert space and a scalar variable. Particularly, we propose a\ndistribution-free test statistic based on Kendall's Tau which is one of the\nmost popular methods to determine the association between two random variables.\nThe distribution of the test statistic under the null hypothesis of\nindependence is established using the theory of U-statistics taking values in a\nHilbert space. We also consider the case when the functional data is sparsely\nobserved, a situation that arises in many applications. Simulations show that\nthe proposed method outperforms the alternatives under several conditions\ndemonstrating the robustness of our approach. We provide data applications that\nfurther consolidate the utility of our method.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 17:47:25 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Jadhav", "Sneha", ""], ["Ma", "Shuangge", ""]]}, {"id": "1912.03756", "submitter": "Taeho Kim", "authors": "Taeho Kim and Edsel A. Pena", "title": "Improved Multiple Confidence Intervals via Thresholding Informed by\n  Prior Information", "comments": "34 pages and 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a statistical problem where a set of parameters are of interest to a\nresearcher. Then multiple confidence intervals can be constructed to infer the\nset of parameters simultaneously. The constructed multiple confidence intervals\nare the realization of a multiple interval estimator (MIE), the main focus of\nthis study. In particular, a thresholding approach is introduced to improve the\nperformance of the MIE. The developed thresholds require additional\ninformation, so a prior distribution is assumed for this purpose. The MIE\nprocedure is then evaluated by two performance measures: a global coverage\nprobability and a global expected content, which are averages with respect to\nthe prior distribution. The procedure defined by the performance measures will\nbe called a Bayes MIE with thresholding (BMIE Thres). In this study, a\nnormal-normal model is utilized to build up the BMIE Thres for a set of\nlocation parameters. Then, the behaviors of BMIE Thres are investigated in\nterms of the performance measures, which approach those of the corresponding\nz-based MIE as the thresholding parameter, C, goes to infinity. In addition, an\noptimization procedure is introduced to achieve the best thresholding parameter\nC. For illustrations, in-season baseball batting average data and leukemia gene\nexpression data are used to demonstrate the procedure for the known and unknown\nstandard deviations situations, respectively. In the ensuing simulations, the\ntarget parameters are generated from different true generating distributions to\nconsider the misspecified prior situation. The simulation also involves Bayes\ncredible MIEs, and the effectiveness among the different MIEs are compared with\nrespect to the performance measures. In general, the thresholding procedure\nhelps to achieve a meaningful reduction in the global expected content while\nmaintaining a nominal level of the global coverage probability.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 20:36:43 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Kim", "Taeho", ""], ["Pena", "Edsel A.", ""]]}, {"id": "1912.03781", "submitter": "Pierfrancesco Alaimo Di Loro", "authors": "Giovanna Tagliaferri, Daria Scacciatelli, Pierfrancesco Alaimo Di Loro", "title": "VAT tax gap prediction: a 2-steps Gradient Boosting approach", "comments": "27 pages, 4 figures, 8 tables Presented at NTTS 2019 conference Under\n  review at another peer-reviewed journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tax evasion is the illegal evasion of taxes by individuals, corporations, and\ntrusts. The revenue loss from tax avoidance can undermine the effectiveness and\nequity of the government policies. A standard measure of tax evasion is the tax\ngap, that can be estimated as the difference between the total amounts of tax\ntheoretically collectable and the total amounts of tax actually collected in a\ngiven period. This paper presents an original contribution to bottom-up\napproach, based on results from fiscal audits, through the use of Machine\nLearning. The major disadvantage of bottom-up approaches is represented by\nselection bias when audited taxpayers are not randomly selected, as in the case\nof audits performed by the Italian Revenue Agency. Our proposal, based on a\n2-steps Gradient Boosting model, produces a robust tax gap estimate and, embeds\na solution to correct for the selection bias which do not require any\nassumptions on the underlying data distribution. The 2-steps Gradient Boosting\napproach is used to estimate the Italian Value-added tax (VAT) gap on\nindividual firms on the basis of fiscal and administrative data income tax\nreturns gathered from Tax Administration Data Base, for the fiscal year 2011.\nThe proposed method significantly boost the performance in predicting with\nrespect to the classical parametric approaches.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 23:16:29 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 21:29:07 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 23:06:27 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Tagliaferri", "Giovanna", ""], ["Scacciatelli", "Daria", ""], ["Di Loro", "Pierfrancesco Alaimo", ""]]}, {"id": "1912.03784", "submitter": "Tamara Fernandez", "authors": "Tamara Fernandez, Arthur Gretton, David Rindt, Dino Sejdinovic", "title": "A kernel log-rank test of independence for right-censored data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general non-parametric independence test between\nright-censored survival times and covariates, which may be multivariate. Our\ntest statistic has a dual interpretation, first in terms of the supremum of a\npotentially infinite collection of weight-indexed log-rank tests, with weight\nfunctions belonging to a reproducing kernel Hilbert space (RKHS) of functions;\nand second, as the norm of the difference of embeddings of certain finite\nmeasures into the RKHS, similar to the Hilbert-Schmidt Independence Criterion\n(HSIC) test-statistic. We study the asymptotic properties of the test, finding\nsufficient conditions to ensure our test correctly rejects the null hypothesis\nunder any alternative. The test statistic can be computed straightforwardly,\nand the rejection threshold is obtained via an asymptotically consistent Wild\nBootstrap procedure. Extensive simulations demonstrate that our testing\nprocedure generally performs better than competing approaches in detecting\ncomplex non-linear dependence.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 23:29:40 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 16:23:57 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Fernandez", "Tamara", ""], ["Gretton", "Arthur", ""], ["Rindt", "David", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1912.03805", "submitter": "Scott Sisson", "authors": "Tom Whitaker and Boris Beranger and Scott A. Sisson", "title": "Logistic regression models for aggregated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression models are a popular and effective method to predict the\nprobability of categorical response data. However inference for these models\ncan become computationally prohibitive for large datasets. Here we adapt ideas\nfrom symbolic data analysis to summarise the collection of predictor variables\ninto histogram form, and perform inference on this summary dataset. We develop\nideas based on composite likelihoods to derive an efficient one-versus-rest\napproximate composite likelihood model for histogram-based random variables,\nconstructed from low-dimensional marginal histograms obtained from the full\nhistogram. We demonstrate that this procedure can achieve comparable\nclassification rates compared to the standard full data multinomial analysis\nand against state-of-the-art subsampling algorithms for logistic regression,\nbut at a substantially lower computational cost. Performance is explored\nthrough simulated examples, and analyses of large supersymmetry and satellite\ncrop classification datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 01:15:49 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 07:46:45 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Whitaker", "Tom", ""], ["Beranger", "Boris", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1912.03807", "submitter": "Ryan Martin", "authors": "Chang Liu and Ryan Martin", "title": "An empirical $G$-Wishart prior for sparse high-dimensional Gaussian\n  graphical models", "comments": "36 pages, 4 tables, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Gaussian graphical models, the zero entries in the precision matrix\ndetermine the dependence structure, so estimating that sparse precision matrix\nand, thereby, learning this underlying structure, is an important and\nchallenging problem. We propose an empirical version of the $G$-Wishart prior\nfor sparse precision matrices, where the prior mode is informed by the data in\na suitable way. Paired with a prior on the graph structure, a marginal\nposterior distribution for the same is obtained that takes the form of a ratio\nof two $G$-Wishart normalizing constants. We show that this ratio can be easily\nand accurately computed using a Laplace approximation, which leads to fast and\nefficient posterior sampling even in high-dimensions. Numerical results\ndemonstrate the proposed method's superior performance, in terms of speed and\naccuracy, across a variety of settings, and theoretical support is provided in\nthe form of a posterior concentration rate theorem.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 01:22:09 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Liu", "Chang", ""], ["Martin", "Ryan", ""]]}, {"id": "1912.03968", "submitter": "Mario Krali", "authors": "Claudia Kl\\\"uppelberg, Mario Krali", "title": "Estimating an Extreme Bayesian Network via Scalings", "comments": "36 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive max-linear vectors model causal dependence between its components\nby expressing each node variable as a max-linear function of its parental nodes\nin a directed acyclic graph and some exogenous innovation. Motivated by extreme\nvalue theory, innovations are assumed to have regularly varying distribution\ntails. We propose a scaling technique in order to determine a causal order of\nthe node variables. All dependence parameters are then estimated from the\nestimated scalings. Furthermore, we prove asymptotic normality of the estimated\nscalings and dependence parameters based on asymptotic normality of the\nempirical spectral measure. Finally, we apply our structure learning and\nestimation algorithm to financial data and food dietary interview data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 11:12:06 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Kl\u00fcppelberg", "Claudia", ""], ["Krali", "Mario", ""]]}, {"id": "1912.04006", "submitter": "Yuchen Shi", "authors": "Yuchen Shi and Nan Chen", "title": "Conditional Kernel Density Estimation Considering Autocorrelation for\n  Renewable Energy Probabilistic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Renewable energy is essential for energy security and global warming\nmitigation. However, power generation from renewable energy sources is\nuncertain due to volatile weather conditions and complex equipment operations.\nTo improve equipment's operation efficiency, it is important to understand and\ncharacterize the uncertainty in renewable power generation. In this paper, we\nproposed a conditional kernel density estimation method to model the\ndistribution of equipment's power output given any weather conditions. It\nexplicitly accounts for the temporal dependence in the data stream and uses an\niterative procedure to reduce the bias in kernel density estimation. Compared\nwith existing literature, our approach is especially useful for the purposes of\nequipment condition monitoring or short-term renewable energy forecasting,\nwhere the data dependence plays a more significant role. We demonstrate our\nmethod and compare it with alternatives through real applications.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:50:02 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 12:28:57 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Shi", "Yuchen", ""], ["Chen", "Nan", ""]]}, {"id": "1912.04011", "submitter": "Niels Olsen", "authors": "Niels Lundtorp Olsen", "title": "A new inequality for maximum likelihood estimation in statistical models\n  with latent variables", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum-likelihood estimation (MLE) is arguably the most important tool for\nstatisticians, and many methods have been developed to find the MLE. We present\na new inequality involving posterior distributions of a latent variable that\nholds under very general conditions. It is related to the EM algorithm and has\na clear potential for being used in a similar fashion.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 15:46:13 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Olsen", "Niels Lundtorp", ""]]}, {"id": "1912.04123", "submitter": "Jiahe Lin", "authors": "Jiahe Lin, George Michailidis", "title": "Approximate Factor Models with Strongly Correlated Idiosyncratic Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of approximate factor models for time series data,\nwhere strong serial and cross-sectional correlations amongst the idiosyncratic\ncomponent are present. This setting comes up naturally in many applications,\nbut existing approaches in the literature rely on the assumption that such\ncorrelations are weak, leading to mis-specification of the number of factors\nselected and consequently inaccurate inference. In this paper, we explicitly\nincorporate the dependent structure present in the idiosyncratic component\nthrough lagged values of the observed multivariate time series. We formulate a\nconstrained optimization problem to estimate the factor space and the\ntransition matrices of the lagged values {\\em simultaneously}, wherein the\nconstraints reflect the low rank nature of the common factors and the sparsity\nof the transition matrices. We establish theoretical properties of the obtained\nestimates, and introduce an easy-to-implement computational procedure for\nempirical work. The performance of the model and the implementation procedure\nis evaluated on synthetic data and compared with competing approaches, and\nfurther illustrated on a data set involving weekly log-returns of 75 US large\nfinancial institutions for the 2001-2016 period.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 15:33:33 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Lin", "Jiahe", ""], ["Michailidis", "George", ""]]}, {"id": "1912.04146", "submitter": "Jiahe Lin", "authors": "Jiahe Lin, George Michailidis", "title": "Regularized Estimation of High-dimensional Factor-Augmented Vector\n  Autoregressive (FAVAR) Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A factor-augmented vector autoregressive (FAVAR) model is defined by a VAR\nequation that captures lead-lag correlations amongst a set of observed\nvariables $X$ and latent factors $F$, and a calibration equation that relates\nanother set of observed variables $Y$ with $F$ and $X$. The latter equation is\nused to estimate the factors that are subsequently used in estimating the\nparameters of the VAR system. The FAVAR model has become popular in applied\neconomic research, since it can summarize a large number of variables of\ninterest as a few factors through the calibration equation and subsequently\nexamine their influence on core variables of primary interest through the VAR\nequation. However, there is increasing need for examining lead-lag\nrelationships between a large number of time series, while incorporating\ninformation from another high-dimensional set of variables. Hence, in this\npaper we investigate the FAVAR model under high-dimensional scaling. We\nintroduce an appropriate identification constraint for the model parameters,\nwhich when incorporated into the formulated optimization problem yields\nestimates with good statistical properties. Further, we address a number of\ntechnical challenges introduced by the fact that estimates of the VAR system\nmodel parameters are based on estimated rather than directly observed\nquantities. The performance of the proposed estimators is evaluated on\nsynthetic data. Further, the model is applied to commodity prices and reveals\ninteresting and interpretable relationships between the prices and the factors\nextracted from a set of global macroeconomic indicators.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 16:11:31 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 15:12:33 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Lin", "Jiahe", ""], ["Michailidis", "George", ""]]}, {"id": "1912.04160", "submitter": "Marcos Matabuena", "authors": "Marcos Matabuena and Oscar Hernan Madrid Padilla", "title": "Energy distance and kernel mean embeddings for two-sample survival\n  testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the comparison problem of distribution equality between two random\nsamples under a right censoring scheme. To address this problem, we design a\nseries of tests based on energy distance and kernel mean embeddings. We\ncalibrate our tests using permutation methods and prove that they are\nconsistent against all fixed continuous alternatives. To evaluate our proposed\ntests, we simulate survival curves from previous clinical trials. Additionally,\nwe provide practitioners with a set of recommendations on how to select\nparameters/distances for the delay effect problem. Based on the method for\nparameter tunning that we propose, we show that our tests demonstrate a\nconsiderable gain of statistical power against classical survival tests.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 16:38:42 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Matabuena", "Marcos", ""], ["Padilla", "Oscar Hernan Madrid", ""]]}, {"id": "1912.04223", "submitter": "Kushani De Silva", "authors": "Kushani De Silva, Carlo Cafaro, Adom Giffin", "title": "Gradient Profile Estimation Using Exponential Cubic Spline Smoothing in\n  a Bayesian Framework", "comments": "24 pages, 8 figures, 4 tables", "journal-ref": "Entropy 23, 674 (2021)", "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attaining reliable profile gradients is of utmost relevance for many physical\nsystems. In most situations, the estimation of gradient can be inaccurate due\nto noise. It is common practice to first estimate the underlying system and\nthen compute the profile gradient by taking the subsequent analytic derivative.\nThe underlying system is often estimated by fitting or smoothing the data using\nother techniques. Taking the subsequent analytic derivative of an estimated\nfunction can be ill-posed. The ill-posedness gets worse as the noise in the\nsystem increases. As a result, the uncertainty generated in the gradient\nestimate increases. In this paper, a theoretical framework for a method to\nestimate the profile gradient of discrete noisy data is presented. The method\nis developed within a Bayesian framework. Comprehensive numerical experiments\nare conducted on synthetic data at different levels of random noise. The\naccuracy of the proposed method is quantified. Our findings suggest that the\nproposed gradient profile estimation method outperforms the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 18:00:01 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["De Silva", "Kushani", ""], ["Cafaro", "Carlo", ""], ["Giffin", "Adom", ""]]}, {"id": "1912.04432", "submitter": "Megha Mehrotra", "authors": "Megha L. Mehrotra, M. Maria Glymour, Elvin Geng, Daniel Westreich,\n  David V. Glidden", "title": "Variable selection for transportability", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportability provides a principled framework to address the problem of\napplying study results to new populations. Here, we consider the problem of\nselecting variables to include in transport estimators. We provide a brief\noverview of the transportability framework and illustrate that while selection\ndiagrams are a vital first step in variable selection, these graphs alone\nidentify a sufficient but not strictly necessary set of variables for\ngenerating an unbiased transport estimate. Next, we conduct a simulation\nexperiment assessing the impact of including unnecessary variables on the\nperformance of the parametric g-computation transport estimator. Our results\nhighlight that the types of variables included can affect the bias, variance,\nand mean squared error of the estimates. We find that addition of variables\nthat are not causes of the outcome but whose distributions differ between the\nsource and target populations can increase the variance and mean squared error\nof the transported estimates. On the other hand, inclusion of variables that\nare causes of the outcome (regardless of whether they modify the causal\ncontrast of interest or differ in distribution between the populations) reduces\nthe variance of the estimates without increasing the bias. Finally, exclusion\nof variables that cause the outcome but do not modify the causal contrast of\ninterest does not increase bias. These findings suggest that variable selection\napproaches for transport should prioritize identifying and including all causes\nof the outcome in the study population rather than focusing on variables whose\ndistribution may differ between the study sample and target population.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 00:46:35 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Mehrotra", "Megha L.", ""], ["Glymour", "M. Maria", ""], ["Geng", "Elvin", ""], ["Westreich", "Daniel", ""], ["Glidden", "David V.", ""]]}, {"id": "1912.04542", "submitter": "Jonathan Bradley", "authors": "Jonathan R. Bradley", "title": "What is the best predictor that you can compute in five minutes using a\n  given Bayesian hierarchical model?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to provide a way for statisticians to answer the\nquestion posed in the title of this article using any Bayesian hierarchical\nmodel of their choosing and without imposing additional restrictive model\nassumptions. We are motivated by the fact that the rise of ``big data'' has\ncreated difficulties for statisticians to directly apply their methods to big\ndatasets. We introduce a ``data subset model'' to the popular ``data model,\nprocess model, and parameter model'' framework used to summarize Bayesian\nhierarchical models. The hyperparameters of the data subset model are specified\nconstructively in that they are chosen such that the implied size of the subset\nsatisfies pre-defined computational constraints. Thus, these hyperparameters\neffectively calibrates the statistical model to the computer itself to obtain\npredictions/estimations in a pre-specified amount of time. Several properties\nof the data subset model are provided including: propriety, partial\nsufficiency, and semi-parametric properties. Furthermore, we show that subsets\nof normally distributed data are asymptotically partially sufficient under\nreasonable constraints. Results from a simulated dataset will be presented\nacross different computers, to show the effect of the computer on the\nstatistical analysis. Additionally, we provide a joint spatial analysis of two\ndifferent environmental datasets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 07:12:21 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Bradley", "Jonathan R.", ""]]}, {"id": "1912.04571", "submitter": "Raphael Huser", "authors": "Rishikesh Yadav, Rapha\\\"el Huser and Thomas Opitz", "title": "Spatial hierarchical modeling of threshold exceedances using rate\n  mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new flexible univariate models for light-tailed and heavy-tailed\ndata, which extend a hierarchical representation of the generalized Pareto (GP)\nlimit for threshold exceedances. These models can accommodate departure from\nasymptotic threshold stability in finite samples while keeping the asymptotic\nGP distribution as a special (or boundary) case and can capture the tails and\nthe bulk jointly without losing much flexibility. Spatial dependence is modeled\nthrough a latent process, while the data are assumed to be conditionally\nindependent. Focusing on a gamma-gamma model construction, we design penalized\ncomplexity priors for crucial model parameters, shrinking our proposed spatial\nBayesian hierarchical model toward a simpler reference whose marginal\ndistributions are GP with moderately heavy tails. Our model can be fitted in\nfairly high dimensions using Markov chain Monte Carlo by exploiting the\nMetropolis-adjusted Langevin algorithm (MALA), which guarantees fast\nconvergence of Markov chains with efficient block proposals for the latent\nvariables. We also develop an adaptive scheme to calibrate the MALA tuning\nparameters. Moreover, our model avoids the expensive numerical evaluations of\nmultifold integrals in censored likelihood expressions. We demonstrate our new\nmethodology by simulation and application to a dataset of extreme rainfall\nevents that occurred in Germany. Our fitted gamma-gamma model provides a\nsatisfactory performance and can be successfully used to predict rainfall\nextremes at unobserved locations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 08:27:28 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 18:44:13 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Yadav", "Rishikesh", ""], ["Huser", "Rapha\u00ebl", ""], ["Opitz", "Thomas", ""]]}, {"id": "1912.04607", "submitter": "Etienne Roquain", "authors": "Sebastian D\\\"ohler and Etienne Roquain", "title": "Controlling false discovery exceedance for heterogeneous tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several classical methods exist for controlling the false discovery\nexceedance (FDX) for large scale multiple testing problems, among them the\nLehmann-Romano procedure ([LR] below) and the Guo-Romano procedure ([GR]\nbelow). While these two procedures are the most prominent, they were originally\ndesigned for homogeneous test statistics, that is, when the null distribution\nfunctions of the $p$-values $F_i$, $1\\leq i\\leq m$, are all equal. In many\napplications, however, the data are heterogeneous which leads to heterogeneous\nnull distribution functions. Ignoring this heterogeneity usually induces a\nconservativeness for the aforementioned procedures. In this paper, we develop\nthree new procedures that incorporate the $F_i$'s, while ensuring the FDX\ncontrol. The heterogeneous version of [LR], denoted [HLR], is based on the\narithmetic average of the $F_i$'s, while the heterogeneous version of [GR],\ndenoted [HGR], is based on the geometric average of the $F_i$'s. We also\nintroduce a procedure [PB], that is based on the Poisson-binomial distribution\nand that uniformly improves [HLR] and [HGR], at the price of a higher\ncomputational complexity. Perhaps surprisingly, this shows that, contrary to\nthe known theory of false discovery rate (FDR) control under heterogeneity, the\nway to incorporate the $F_i$'s can be particularly simple in the case of FDX\ncontrol, and does not require any further correction term. The performances of\nthe new proposed procedures are illustrated by real and simulated data in two\nimportant heterogeneous settings: first, when the test statistics are\ncontinuous but the $p$-values are weighted by some known independent weight\nvector, e.g., coming from co-data sets; second, when the test statistics are\ndiscretely distributed, as is the case for data representing frequencies or\ncounts.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 10:06:42 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["D\u00f6hler", "Sebastian", ""], ["Roquain", "Etienne", ""]]}, {"id": "1912.04629", "submitter": "Thomas Berrett", "authors": "Thomas Berrett and Cristina Butucea", "title": "Classification under local differential privacy", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the binary classification problem in a setup that preserves the\nprivacy of the original sample. We provide a privacy mechanism that is locally\ndifferentially private and then construct a classifier based on the private\nsample that is universally consistent in Euclidean spaces. Under stronger\nassumptions, we establish the minimax rates of convergence of the excess risk\nand see that they are slower than in the case when the original sample is\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 10:37:21 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Berrett", "Thomas", ""], ["Butucea", "Cristina", ""]]}, {"id": "1912.04661", "submitter": "Alisa Yusupova", "authors": "Alisa Yusupova, Nicos G. Pavlidis, Efthymios G. Pavlidis", "title": "Adaptive Dynamic Model Averaging with an Application to House Price\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic model averaging (DMA) combines the forecasts of a large number of\ndynamic linear models (DLMs) to predict the future value of a time series. The\nperformance of DMA critically depends on the appropriate choice of two\nforgetting factors. The first of these controls the speed of adaptation of the\ncoefficient vector of each DLM, while the second enables time variation in the\nmodel averaging stage. In this paper we develop a novel, adaptive dynamic model\naveraging (ADMA) methodology. The proposed methodology employs a stochastic\noptimisation algorithm that sequentially updates the forgetting factor of each\nDLM, and uses a state-of-the-art non-parametric model combination algorithm\nfrom the prediction with expert advice literature, which offers finite-time\nperformance guarantees. An empirical application to quarterly UK house price\ndata suggests that ADMA produces more accurate forecasts than the benchmark\nautoregressive model, as well as competing DMA specifications.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 12:03:58 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Yusupova", "Alisa", ""], ["Pavlidis", "Nicos G.", ""], ["Pavlidis", "Efthymios G.", ""]]}, {"id": "1912.04758", "submitter": "Matthew Nunes", "authors": "Marina Knight, Kathryn Leeming, Guy Nason, Matthew Nunes", "title": "Generalised Network Autoregressive Processes and the GNAR package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces the GNAR package, which fits, predicts, and simulates\nfrom a powerful new class of generalised network autoregressive processes. Such\nprocesses consist of a multivariate time series along with a real, or inferred,\nnetwork that provides information about inter-variable relationships. The GNAR\nmodel relates values of a time series for a given variable and time to earlier\nvalues of the same variable and of neighbouring variables, with inclusion\ncontrolled by the network structure. The GNAR package is designed to fit this\nnew model, while working with standard ts objects and the igraph package for\nease of use.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:21:47 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Knight", "Marina", ""], ["Leeming", "Kathryn", ""], ["Nason", "Guy", ""], ["Nunes", "Matthew", ""]]}, {"id": "1912.04902", "submitter": "Lubna Amro", "authors": "Lubna Amro, Markus Pauly, Burim Ramosaj", "title": "Asymptotic based bootstrap approach for matched pairs with missingness\n  in a single-arm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of missing values is an arising difficulty when dealing with paired\ndata. Several test procedures are developed in the literature to tackle this\nproblem. Some of them are even robust under deviations and control type-I error\nquite accurately. However, most these methods are not applicable when missing\nvalues are present only in a single arm. For this case, we provide asymptotic\ncorrect resampling tests that are robust under heteroscedasticity and skewed\ndistributions. The tests are based on a clever restructuring of all observed\ninformation in a quadratic form-type test statistic. An extensive simulation\nstudy is conducted exemplifying the tests for finite sample sizes under\ndifferent missingness mechanisms. In addition, an illustrative data example\nbased on a breast cancer gene study is analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:20:01 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Amro", "Lubna", ""], ["Pauly", "Markus", ""], ["Ramosaj", "Burim", ""]]}, {"id": "1912.04924", "submitter": "Sven Buitendag Mr", "authors": "Jan Beirlant, Sven Buitendag, Eustasio del Bario, Marc Hallin", "title": "Center-outward quantiles and the measurement of multivariate risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All multivariate extensions of the univariate theory of risk measurement run\ninto the same fundamental problem of the absence, in dimension d > 1, of a\ncanonical ordering of Rd. Based on measure transportation ideas, several\nattempts have been made recently in the statistical literature to overcome that\nconceptual difficulty. In Hallin (2017), the concepts of center-outward\ndistribution and quantile functions are developed as generalisations of the\nclassical univariate concepts of distribution and quantile functions, along\nwith their empirical versions. We propose a class of smooth approximations as\nan alternative to the interpolation developed in del Barrio et al. (2018). This\napproximation allows for the computation of some new empirical risk measures,\nbased either on the convex potential associated with the proposed transports,\nor on the volumes of the resulting empirical quantile regions. We also discuss\nthe role of such transports in the evaluation of the risk associated with\nmultivariate regularly varying distributions. Some simulations and applications\nto case studies illustrate the value of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 19:02:05 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Beirlant", "Jan", ""], ["Buitendag", "Sven", ""], ["del Bario", "Eustasio", ""], ["Hallin", "Marc", ""]]}, {"id": "1912.05084", "submitter": "Abhra Sarkar", "authors": "Abhra Sarkar, Debdeep Pati, Bani K. Mallick, Raymond J. Carroll", "title": "Bayesian Copula Density Deconvolution for Zero-Inflated Data in\n  Nutritional Epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the marginal and joint densities of the long-term average intakes\nof different dietary components is an important problem in nutritional\nepidemiology. Since these variables cannot be directly measured, data are\nusually collected in the form of 24-hour recalls of the intakes, which show\nmarked patterns of conditional heteroscedasticity. Significantly compounding\nthe challenges, the recalls for episodically consumed dietary components also\ninclude exact zeros. The problem of estimating the density of the latent\nlong-time intakes from their observed measurement error contaminated proxies is\nthen a problem of deconvolution of densities with zero-inflated data. We\npropose a Bayesian semiparametric solution to the problem, building on a novel\nhierarchical latent variable framework that translates the problem to one\ninvolving continuous surrogates only. Crucial to accommodating important\naspects of the problem, we then design a copula-based approach to model the\ninvolved joint distributions, adopting different modeling strategies for the\nmarginals of the different dietary components. We design efficient Markov chain\nMonte Carlo algorithms for posterior inference and illustrate the efficacy of\nthe proposed method through simulation experiments. Applied to our motivating\nnutritional epidemiology problems, compared to other approaches, our method\nprovides more realistic estimates of the consumption patterns of episodically\nconsumed dietary components.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 02:08:05 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Sarkar", "Abhra", ""], ["Pati", "Debdeep", ""], ["Mallick", "Bani K.", ""], ["Carroll", "Raymond J.", ""]]}, {"id": "1912.05125", "submitter": "Sebastian Kurtek", "authors": "James Matuk, Karthik Bharath, Oksana Chkrebtii, Sebastian Kurtek", "title": "Bayesian Framework for Simultaneous Registration and Estimation of\n  Noisy, Sparse and Fragmented Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, smooth processes generate data that is recorded under a\nvariety of observation regimes, such as dense, sparse or fragmented\nobservations that are often contaminated with error. The statistical goal of\nregistering and estimating the individual underlying functions from discrete\nobservations has thus far been mainly approached sequentially without formal\nuncertainty propagation, or in an application-specific manner. We propose a\nunified Bayesian framework for simultaneous registration and estimation, which\nis flexible enough to accommodate inference on individual functions under\ngeneral observation regimes. Our ability to do this relies on the specification\nof strongly informative prior models over the amplitude component of function\nvariability. We provide two strategies for this critical choice: a data-driven\napproach that defines an empirical basis for the amplitude subspace based on\ntraining data, and a shape-restricted approach when the relative location and\nnumber of local extrema is well-understood. The proposed methods build on\nelastic functional data analysis, which separately models amplitude and phase\nvariability inherent in functional data. We emphasize the importance of\nuncertainty quantification and visualization of these two components as they\nprovide complementary information about the estimated functions. We validate\nthe framework using simulations and real applications to medical imaging and\nbiometrics.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 05:48:00 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Matuk", "James", ""], ["Bharath", "Karthik", ""], ["Chkrebtii", "Oksana", ""], ["Kurtek", "Sebastian", ""]]}, {"id": "1912.05258", "submitter": "Martina McMenamin", "authors": "Martina McMenamin, Jessica K. Barrett, Anna Berglind, James M.S. Wason", "title": "Sample Size Estimation using a Latent Variable Model for Mixed Outcome\n  Co-Primary, Multiple Primary and Composite Endpoints", "comments": "36 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed outcome endpoints that combine multiple continuous and discrete\ncomponents to form co-primary, multiple primary or composite endpoints are\noften employed as primary outcome measures in clinical trials. There are many\nadvantages to joint modelling the individual outcomes using a latent variable\nframework, however in order to make use of the model in practice we require\ntechniques for sample size estimation. In this paper we show how the latent\nvariable model can be applied to the three types of joint endpoints and propose\nappropriate hypotheses, power and sample size estimation methods for each. We\nillustrate the techniques using a numerical example based on the four\ndimensional endpoint in the MUSE trial and find that the sample size required\nfor the co-primary endpoint is larger than that required for the individual\nendpoint with the smallest effect size. Conversely, the sample size required\nfor the multiple primary endpoint is reduced from that required for the\nindividual outcome with the largest effect size. We show that the analytical\ntechnique agrees with the empirical power from simulation studies. We further\nillustrate the reduction in required sample size that may be achieved in trials\nof mixed outcome composite endpoints through a simulation study and find that\nthe sample size primarily depends on the components driving response and the\ncorrelation structure and much less so on the treatment effect structure in the\nindividual endpoints.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 12:24:25 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["McMenamin", "Martina", ""], ["Barrett", "Jessica K.", ""], ["Berglind", "Anna", ""], ["Wason", "James M. S.", ""]]}, {"id": "1912.05449", "submitter": "Minjie Wang", "authors": "Minjie Wang, Genevera I. Allen", "title": "Integrative Generalized Convex Clustering Optimization and Feature\n  Selection for Mixed Multi-View Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mixed multi-view data, multiple sets of diverse features are measured on\nthe same set of samples. By integrating all available data sources, we seek to\ndiscover common group structure among the samples that may be hidden in\nindividualistic cluster analyses of a single data-view. While several\ntechniques for such integrative clustering have been explored, we propose and\ndevelop a convex formalization that will inherit the strong statistical,\nmathematical and empirical properties of increasingly popular convex clustering\nmethods. Specifically, our Integrative Generalized Convex Clustering\nOptimization (iGecco) method employs different convex distances, losses, or\ndivergences for each of the different data views with a joint convex fusion\npenalty that leads to common groups. Additionally, integrating mixed multi-view\ndata is often challenging when each data source is high-dimensional. To perform\nfeature selection in such scenarios, we develop an adaptive shifted group-lasso\npenalty that selects features by shrinking them towards their loss-specific\ncenters. Our so-called iGecco+ approach selects features from each data-view\nthat are best for determining the groups, often leading to improved integrative\nclustering. To fit our model, we develop a new type of generalized multi-block\nADMM algorithm using sub-problem approximations that more efficiently fits our\nmodel for big data sets. Through a series of numerical experiments and real\ndata examples on text mining and genomics, we show that iGecco+ achieves\nsuperior empirical performance for high-dimensional mixed multi-view data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 16:51:25 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Wang", "Minjie", ""], ["Allen", "Genevera I.", ""]]}, {"id": "1912.05503", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay and Emanuel Parzen", "title": "Nonparametric Universal Copula Modeling", "comments": "A perspective on \"60 years of Copula\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To handle the ubiquitous problem of \"dependence learning,\" copulas are\nquickly becoming a pervasive tool across a wide range of data-driven\ndisciplines encompassing neuroscience, finance, econometrics, genomics, social\nscience, machine learning, healthcare and many more. Copula (or connection)\nfunctions were invented in 1959 by Abe Sklar in response to a query of Maurice\nFrechet. After 60 years, where do we stand now? This article provides a history\nof the key developments and offers a unified perspective.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:07:54 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""], ["Parzen", "Emanuel", ""]]}, {"id": "1912.05516", "submitter": "Lorenzo Masoero", "authors": "Lorenzo Masoero, Federico Camerlenghi, Stefano Favaro, Tamara\n  Broderick", "title": "More for less: Predicting and maximizing genetic variant discovery via\n  Bayesian nonparametrics", "comments": null, "journal-ref": null, "doi": "10.1093/biomet/asab012", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the cost of sequencing genomes has decreased dramatically in recent\nyears, this expense often remains non-trivial. Under a fixed budget, then,\nscientists face a natural trade-off between quantity and quality; they can\nspend resources to sequence a greater number of genomes (quantity) or spend\nresources to sequence genomes with increased accuracy (quality). Our goal is to\nfind the optimal allocation of resources between quantity and quality.\nOptimizing resource allocation promises to reveal as many new variations in the\ngenome as possible, and thus as many new scientific insights as possible. In\nthis paper, we consider the common setting where scientists have already\nconducted a pilot study to reveal variants in a genome and are contemplating a\nfollow-up study. We introduce a Bayesian nonparametric methodology to predict\nthe number of new variants in the follow-up study based on the pilot study.\nWhen experimental conditions are kept constant between the pilot and follow-up,\nwe demonstrate on real data from the gnomAD project that our prediction is more\naccurate than three recent proposals, and competitive with a more classic\nproposal. Unlike existing methods, though, our method allows practitioners to\nchange experimental conditions between the pilot and the follow-up. We\ndemonstrate how this distinction allows our method to be used for (i) more\nrealistic predictions and (ii) optimal allocation of a fixed budget between\nquality and quantity.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:30:28 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 19:06:14 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 20:39:07 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Masoero", "Lorenzo", ""], ["Camerlenghi", "Federico", ""], ["Favaro", "Stefano", ""], ["Broderick", "Tamara", ""]]}, {"id": "1912.05588", "submitter": "Haiming Zhou", "authors": "Haiming Zhou and Xianzheng Huang", "title": "Parametric mode regression for bounded responses", "comments": "To appear in Biometrical Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new parametric frameworks of regression analysis with the\nconditional mode of a bounded response as the focal point of interest.\nCovariate effects estimation and prediction based on the maximum likelihood\nmethod under two new classes of regression models are demonstrated. We also\ndevelop graphical and numerical diagnostic tools to detect various sources of\nmodel misspecification. Predictions based on different central tendency\nmeasures inferred using various regression models are compared using synthetic\ndata in simulations. Finally, we conduct regression analysis for data from the\nAlzheimer's Disease Neuroimaging Initiative to demonstrate practical\nimplementation of the proposed methods. Supplementary materials that contain\ntechnical details, and additional simulation and data analysis results are\navailable online.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:25:26 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 16:18:33 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Zhou", "Haiming", ""], ["Huang", "Xianzheng", ""]]}, {"id": "1912.05642", "submitter": "Jonas Wallin", "authors": "David Bolin and Jonas Wallin", "title": "Local scale invariance and robustness of proper scoring rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Averages of proper scoring rules are often used to rank probabilistic\nforecasts. In many cases, the variance of the individual observations and their\npredictive distributions vary in these averages. We show that some of the most\npopular proper scoring rules, such as the continuous ranked probability score\n(CRPS) which is the go-to score for continuous observation ensemble forecasts,\ngive more importance to observations with large uncertainty which can lead to\nunintuitive rankings. To describe this issue, we define the concept of local\nscale invariance for scoring rules. A new class of generalized proper kernel\nscoring rules is derived and as a member of this class we propose the scaled\nCRPS (SCRPS). This new proper scoring rule is locally scale invariant and\ntherefore works in the case of varying uncertainty. Like CRPS it is\ncomputationally available for output from ensemble forecasts, and does not\nrequire ability to evaluate the density of the forecast.\n  We further define robustness of scoring rules, show why this also is an\nimportant concept for average scores, and derive new proper scoring rules that\nare robust against outliers. The theoretical findings are illustrated in three\ndifferent applications from spatial statistics, stochastic volatility models,\nand regression for count data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:28:58 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 16:00:11 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 11:42:01 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Bolin", "David", ""], ["Wallin", "Jonas", ""]]}, {"id": "1912.05657", "submitter": "Raphael Huser", "authors": "Arnab Hazra and Rapha\\\"el Huser", "title": "Estimating high-resolution Red Sea surface temperature hotspots, using a\n  low-rank semiparametric spatial model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we estimate extreme sea surface temperature (SST) hotspots,\ni.e., high threshold exceedance regions, for the Red Sea, a vital region of\nhigh biodiversity. We analyze high-resolution satellite-derived SST data\ncomprising daily measurements at 16703 grid cells across the Red Sea over the\nperiod 1985-2015. We propose a semiparametric Bayesian spatial mixed-effects\nlinear model with a flexible mean structure to capture spatially-varying trend\nand seasonality, while the residual spatial variability is modeled through a\nDirichlet process mixture (DPM) of low-rank spatial Student-$t$ processes\n(LTPs). By specifying cluster-specific parameters for each LTP mixture\ncomponent, the bulk of the SST residuals influence tail inference and hotspot\nestimation only moderately. Our proposed model has a nonstationary mean,\ncovariance and tail dependence, and posterior inference can be drawn\nefficiently through Gibbs sampling. In our application, we show that the\nproposed method outperforms some natural parametric and semiparametric\nalternatives. Moreover, we show how hotspots can be identified and we estimate\nextreme SST hotspots for the whole Red Sea, projected until the year 2100,\nbased on the Representative Concentration Pathways 4.5 and 8.5. The estimated\n95\\% credible region for joint high threshold exceedances include large areas\ncovering major endangered coral reefs in the southern Red Sea.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 21:53:44 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 08:43:00 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 13:53:29 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Hazra", "Arnab", ""], ["Huser", "Rapha\u00ebl", ""]]}, {"id": "1912.05737", "submitter": "Pierre Alquier", "authors": "Badr-Eddine Ch\\'erief-Abdellatif, Pierre Alquier", "title": "Finite sample properties of parametric MMD estimation: robustness to\n  misspecification and dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many works in statistics aim at designing a universal estimation procedure,\nthat is, an estimator that would converge to the best approximation of the\n(unknown) data generating distribution in a model, without any assumption on\nthis distribution. This question is of major interest, in particular because\nthe universality property leads to the robustness of the estimator. In this\npaper, we tackle the problem of universal estimation using a minimum distance\nestimator presented in Briol et al. (2019) based on the Maximum Mean\nDiscrepancy. We show that the estimator is robust to both dependence and to the\npresence of outliers in the dataset. Finally, we provide a theoretical study of\nthe stochastic gradient descent algorithm used to compute the estimator, and we\nsupport our findings with numerical simulations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 02:28:13 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 20:52:14 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 02:11:15 GMT"}, {"version": "v4", "created": "Fri, 31 Jul 2020 02:00:55 GMT"}, {"version": "v5", "created": "Thu, 4 Mar 2021 08:52:02 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""], ["Alquier", "Pierre", ""]]}, {"id": "1912.05738", "submitter": "Sheng Jiang", "authors": "Sheng Jiang, Surya T. Tokdar", "title": "Variable Selection Consistency of Gaussian Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian nonparametric regression under a rescaled Gaussian process prior\noffers smoothness-adaptive function estimation with near minimax-optimal error\nrates. Hierarchical extensions of this approach, equipped with stochastic\nvariable selection, are known to also adapt to the unknown intrinsic dimension\nof a sparse true regression function. But it remains unclear if such extensions\noffer variable selection consistency, i.e., if the true subset of important\nvariables could be consistently learned from the data. It is shown here that\nvariable consistency may indeed be achieved with such models at least when the\ntrue regression function has finite smoothness to induce a polynomially larger\npenalty on inclusion of false positive predictors. Our result covers the high\ndimensional asymptotic setting where the predictor dimension is allowed to grow\nwith the sample size. The proof utilizes Schwartz theory to establish that the\nposterior probability of wrong selection vanishes asymptotically. A necessary\nand challenging technical development involves providing sharp upper and lower\nbounds to small ball probabilities at all rescaling levels of the Gaussian\nprocess prior, a result that could be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 02:29:14 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 02:43:18 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Jiang", "Sheng", ""], ["Tokdar", "Surya T.", ""]]}, {"id": "1912.05769", "submitter": "Or Zuk", "authors": "Yaniv Tenzer, Micha Mandel and Or Zuk", "title": "Testing Independence under Biased Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing for association or dependence between pairs of random variables is a\nfundamental problem in statistics. In some applications, data are subject to\nselection bias that causes dependence between observations even when it is\nabsent from the population. An important example is truncation models, in which\nobserved pairs are restricted to a specific subset of the X-Y plane. Standard\ntests for independence are not suitable in such cases, and alternative tests\nthat take the selection bias into account are required. To deal with this\nissue, we generalize the notion of quasi-independence with respect to the\nsampling mechanism, and study the problem of detecting any deviations from it.\nWe develop two test statistics motivated by the classic Hoeffding's statistic,\nand use two approaches to compute their distribution under the null: (i) a\nbootstrap-based approach, and (ii) a permutation-test with non-uniform\nprobability of permutations, sampled using either MCMC or importance sampling\nwith various proposal distributions. We show that our tests can tackle cases\nwhere the biased sampling mechanism is estimated from the data, with an\nimportant application to the case of censoring with truncation. We prove the\nvalidity of the tests, and show, using simulations, that they perform well for\nimportant special cases of the problem and improve power compared to competing\nmethods. The tests are applied to four datasets, two that are subject to\ntruncation, with and without censoring, and two to positive bias mechanisms\nrelated to length bias.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 04:22:44 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 14:33:30 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Tenzer", "Yaniv", ""], ["Mandel", "Micha", ""], ["Zuk", "Or", ""]]}, {"id": "1912.05800", "submitter": "Linda Nab", "authors": "Linda Nab, Rolf H.H. Groenwold, Maarten van Smeden, Ruth H. Keogh", "title": "Sensitivity analysis for bias due to a misclassfied confounding variable\n  in marginal structural models", "comments": "25 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In observational research treatment effects, the average treatment effect\n(ATE) estimator may be biased if a confounding variable is misclassified. We\ndiscuss the impact of classification error in a dichotomous confounding\nvariable in analyses using marginal structural models estimated using inverse\nprobability weighting (MSMs-IPW) and compare this with its impact in\nconditional regression models, focusing on a point-treatment study with a\ncontinuous outcome. Expressions were derived for the bias in the ATE estimator\nfrom a MSM-IPW and conditional model by using the potential outcome framework.\nBased on these expressions, we propose a sensitivity analysis to investigate\nand quantify the bias due to classification error in a confounding variable in\nMSMs-IPW. Compared to bias in the ATE estimator from a conditional model, the\nbias in MSM-IPW can be dissimilar in magnitude but the bias will always be\nequal in sign. A simulation study was conducted to study the finite sample\nperformance of MSMs-IPW and conditional models if a confounding variable is\nmisclassified. Simulation results showed that confidence intervals of the\ntreatment effect obtained from MSM-IPW are generally wider and coverage of the\ntrue treatment effect is higher compared to a conditional model, ranging from\nover coverage if there is no classification error to smaller under coverage\nwhen there is classification error. The use of the bias expressions to inform a\nsensitivity analysis was demonstrated in a study of blood pressure lowering\ntherapy. It is important to consider the potential impact of classification\nerror in a confounding variable in studies of treatment effects and a\nsensitivity analysis provides an opportunity to quantify the impact of such\nerrors on causal conclusions. An online tool for sensitivity analyses was\ndeveloped: https://lindanab.shinyapps.io/SensitivityAnalysis.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 07:03:22 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Nab", "Linda", ""], ["Groenwold", "Rolf H. H.", ""], ["van Smeden", "Maarten", ""], ["Keogh", "Ruth H.", ""]]}, {"id": "1912.05810", "submitter": "Owen Thomas", "authors": "Owen Thomas, Jukka Corander", "title": "Diagnosing model misspecification and performing generalized Bayes'\n  updates via probabilistic classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model misspecification is a long-standing enigma of the Bayesian inference\nframework as posteriors tend to get overly concentrated on ill-informed\nparameter values towards the large sample limit. Tempering of the likelihood\nhas been established as a safer way to do updates from prior to posterior in\nthe presence of model misspecification. At one extreme tempering can ignore the\ndata altogether and at the other extreme it provides the standard Bayes' update\nwhen no misspecification is assumed to be present. However, it is an open issue\nhow to best recognize misspecification and choose a suitable level of tempering\nwithout access to the true generating model. Here we show how probabilistic\nclassifiers can be employed to resolve this issue. By training a probabilistic\nclassifier to discriminate between simulated and observed data provides an\nestimate of the ratio between the model likelihood and the likelihood of the\ndata under the unobserved true generative process, within the discriminatory\nabilities of the classifier. The expectation of the logarithm of a ratio with\nrespect to the data generating process gives an estimation of the negative\nKullback-Leibler divergence between the statistical generative model and the\ntrue generative distribution. Using a set of canonical examples we show that\nthis divergence provides a useful misspecification diagnostic, a model\ncomparison tool, and a method to inform a generalised Bayesian update in the\npresence of misspecification for likelihood-based models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 07:45:21 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Thomas", "Owen", ""], ["Corander", "Jukka", ""]]}, {"id": "1912.05974", "submitter": "Nicola Rennie", "authors": "Nicola Rennie, Catherine Cleophas, Adam M. Sykulski, Florian Dost", "title": "Identifying and Responding to Outlier Demand in Revenue Management", "comments": null, "journal-ref": null, "doi": "10.1016/j.ejor.2021.01.002", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Revenue management strongly relies on accurate forecasts. Thus, when\nextraordinary events cause outlier demand, revenue management systems need to\nrecognise this and adapt both forecast and controls. Many passenger transport\nservice providers, such as railways and airlines, control the sale of tickets\nthrough revenue management. State-of-the-art systems in these industries rely\non analyst expertise to identify outlier demand both online (within the booking\nhorizon) and offline (in hindsight). So far, little research focuses on\nautomating and evaluating the detection of outlier demand in this context. To\nremedy this, we propose a novel approach, which detects outliers using\nfunctional data analysis in combination with time series extrapolation. We\nevaluate the approach in a simulation framework, which generates outliers by\nvarying the demand model. The results show that functional outlier detection\nyields better detection rates than alternative approaches for both online and\noffline analyses. Depending on the category of outliers, extrapolation further\nincreases online detection performance. We also apply the procedure to a set of\nempirical data to demonstrate its practical implications. By evaluating the\nfull feedback-driven system of forecast and optimisation, we generate insight\non the asymmetric effects of positive and negative demand outliers. We show\nthat identifying instances of outlier demand and adjusting the forecast in a\ntimely fashion substantially increases revenue compared to what is earned when\nignoring outliers.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 14:20:10 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 15:14:14 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 14:48:36 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Rennie", "Nicola", ""], ["Cleophas", "Catherine", ""], ["Sykulski", "Adam M.", ""], ["Dost", "Florian", ""]]}, {"id": "1912.06073", "submitter": "He Jia", "authors": "He Jia, Uro\\v{s} Seljak", "title": "Normalizing Constant Estimation with Gaussianized Bridge Sampling", "comments": "Accepted by AABI 2019 Proceedings", "journal-ref": "Proceedings of The 2nd Symposium on Advances in Approximate\n  Bayesian Inference, PMLR 118:1-14, 2020", "doi": null, "report-no": null, "categories": "stat.ML astro-ph.CO cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing constant (also called partition function, Bayesian evidence, or\nmarginal likelihood) is one of the central goals of Bayesian inference, yet\nmost of the existing methods are both expensive and inaccurate. Here we develop\na new approach, starting from posterior samples obtained with a standard Markov\nChain Monte Carlo (MCMC). We apply a novel Normalizing Flow (NF) approach to\nobtain an analytic density estimator from these samples, followed by Optimal\nBridge Sampling (OBS) to obtain the normalizing constant. We compare our method\nwhich we call Gaussianized Bridge Sampling (GBS) to existing methods such as\nNested Sampling (NS) and Annealed Importance Sampling (AIS) on several\nexamples, showing our method is both significantly faster and substantially\nmore accurate than these methods, and comes with a reliable error estimation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 16:50:03 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Jia", "He", ""], ["Seljak", "Uro\u0161", ""]]}, {"id": "1912.06292", "submitter": "Aur\\'elien Bibaut", "authors": "Aur\\'elien F. Bibaut, Ivana Malenica, Nikos Vlassis, Mark J. van der\n  Laan", "title": "More Efficient Off-Policy Evaluation through Regularized Targeted\n  Learning", "comments": "We are uploading the full paper with the appendix as of 12/12/2019,\n  as we noticed that, unlike the main text, the appendix has not been made\n  available on PMLR's website. The version of the appendix in this document is\n  the same that we have been sending by email since June 2019 to readers who\n  solicited it", "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning, PMLR 97:654-663, 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy evaluation (OPE) in Reinforcement Learning\n(RL), where the aim is to estimate the performance of a new policy given\nhistorical data that may have been generated by a different policy, or\npolicies. In particular, we introduce a novel doubly-robust estimator for the\nOPE problem in RL, based on the Targeted Maximum Likelihood Estimation\nprinciple from the statistical causal inference literature. We also introduce\nseveral variance reduction techniques that lead to impressive performance gains\nin off-policy evaluation. We show empirically that our estimator uniformly wins\nover existing off-policy evaluation methods across multiple RL environments and\nvarious levels of model misspecification. Finally, we further the existing\ntheoretical analysis of estimators for the RL off-policy estimation problem by\nshowing their $O_P(1/\\sqrt{n})$ rate of convergence and characterizing their\nasymptotic distribution.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 02:04:22 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Bibaut", "Aur\u00e9lien F.", ""], ["Malenica", "Ivana", ""], ["Vlassis", "Nikos", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1912.06307", "submitter": "Andrii Babii", "authors": "Andrii Babii and Eric Ghysels and Jonas Striaukas", "title": "High-Dimensional Granger Causality Tests with an Application to VIX and\n  News", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Granger causality testing for high-dimensional time series using\nregularized regressions. To perform proper inference, we rely on\nheteroskedasticity and autocorrelation consistent (HAC) estimation of the\nasymptotic variance and develop the inferential theory in the high-dimensional\nsetting. To recognize the time series data structures we focus on the\nsparse-group LASSO estimator, which includes the LASSO and the group LASSO as\nspecial cases. We establish the debiased central limit theorem for low\ndimensional groups of regression coefficients and study the HAC estimator of\nthe long-run variance based on the sparse-group LASSO residuals. This leads to\nvalid time series inference for individual regression coefficients as well as\ngroups, including Granger causality tests. The treatment relies on a new\nFuk-Nagaev inequality for a class of $\\tau$-mixing processes with heavier than\nGaussian tails, which is of independent interest. In an empirical application,\nwe study the Granger causal relationship between the VIX and financial news.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 03:20:51 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 20:11:21 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 00:51:44 GMT"}, {"version": "v4", "created": "Mon, 1 Feb 2021 15:48:41 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Babii", "Andrii", ""], ["Ghysels", "Eric", ""], ["Striaukas", "Jonas", ""]]}, {"id": "1912.06313", "submitter": "Ashwini Venkatasubramaniam", "authors": "Ashwini Venkatasubramaniam, Brandon Koch, Lauren Erickson, Simone\n  French, David Vock and Julian Wolfson", "title": "Assessing effect heterogeneity of a randomized treatment using\n  conditional inference trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treatment effect heterogeneity occurs when individual characteristics\ninfluence the effect of a treatment. We propose a novel approach that combines\nprognostic score matching and conditional inference trees to characterize\neffect heterogeneity of a randomized binary treatment. One key feature that\ndistinguishes our method from alternative approaches is that it controls the\nType I error rate, i.e., the probability of identifying effect heterogeneity if\nnone exists and retains the underlying subgroups. This feature makes our\ntechnique particularly appealing in the context of clinical trials, where there\nmay be significant costs associated with erroneously declaring that effects\ndiffer across population subgroups. TEHTrees are able to identify heterogeneous\nsubgroups, characterize the relevant subgroups and estimate the associated\ntreatment effects. We demonstrate the efficacy of the proposed method using a\ncomprehensive simulation study and illustrate our method using a nutrition\ntrial dataset to evaluate effect heterogeneity within a patient population.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 03:48:54 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 17:29:20 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Venkatasubramaniam", "Ashwini", ""], ["Koch", "Brandon", ""], ["Erickson", "Lauren", ""], ["French", "Simone", ""], ["Vock", "David", ""], ["Wolfson", "Julian", ""]]}, {"id": "1912.06346", "submitter": "Bryan Graham", "authors": "Bryan S. Graham", "title": "Network Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many economic activities are embedded in networks: sets of agents and the\n(often) rivalrous relationships connecting them to one another. Input sourcing\nby firms, interbank lending, scientific research, and job search are four\nexamples, among many, of networked economic activities. Motivated by the\npremise that networks' structures are consequential, this chapter describes\neconometric methods for analyzing them. I emphasize (i) dyadic regression\nanalysis incorporating unobserved agent-specific heterogeneity and supporting\ncausal inference, (ii) techniques for estimating, and conducting inference on,\nsummary network parameters (e.g., the degree distribution or transitivity\nindex); and (iii) empirical models of strategic network formation admitting\ninterdependencies in preferences. Current research challenges and open\nquestions are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 07:41:57 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Graham", "Bryan S.", ""]]}, {"id": "1912.06357", "submitter": "Zeng Li", "authors": "Zeng Li, Qinwen Wang, Runze Li", "title": "Central Limit Theorem for Linear Spectral Statistics of Large\n  Dimensional Kendall's Rank Correlation Matrices and its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the limiting spectral behaviors of large\ndimensional Kendall's rank correlation matrices generated by samples with\nindependent and continuous components. We do not require the components to be\nidentically distributed, and do not need any moment conditions, which is very\ndifferent from the assumptions imposed in the literature of random matrix\ntheory. The statistical setting in this paper covers a wide range of highly\nskewed and heavy-tailed distributions. We establish the central limit theorem\n(CLT) for the linear spectral statistics of the Kendall's rank correlation\nmatrices under the Marchenko-Pastur asymptotic regime, in which the dimension\ndiverges to infinity proportionally with the sample size. We further propose\nthree nonparametric procedures for high dimensional independent test and their\nlimiting null distributions are derived by implementing this CLT. Our numerical\ncomparisons demonstrate the robustness and superiority of our proposed test\nstatistics under various mixed and heavy-tailed cases.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 08:29:54 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Li", "Zeng", ""], ["Wang", "Qinwen", ""], ["Li", "Runze", ""]]}, {"id": "1912.06382", "submitter": "Colin Griesbach", "authors": "Colin Griesbach, Andreas Groll, Elisabeth Waldmann", "title": "Addressing cluster-constant covariates in mixed effects models via\n  likelihood-based boosting techniques", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting techniques from the field of statistical learning have grown to be a\npopular tool for estimating and selecting predictor effects in various\nregression models and can roughly be separated in two general approaches,\nnamely gradient boosting and likelihood-based boosting. An extensive framework\nhas been proposed in order to fit generalised mixed models based on boosting,\nhowever for the case of cluster-constant covariates likelihood-based boosting\napproaches tend to mischoose variables in the selection step leading to wrong\nestimates. We propose an improved boosting algorithm for linear mixed models\nwhere the random effects are properly weighted, disentangled from the fixed\neffects updating scheme and corrected for correlations with cluster-constant\ncovariates in order to improve quality of estimates and in addition reduce the\ncomputational effort. The method outperforms current state-of-the-art\napproaches from boosting and maximum likelihood inference which is shown via\nsimulations and various data examples.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 10:03:25 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Griesbach", "Colin", ""], ["Groll", "Andreas", ""], ["Waldmann", "Elisabeth", ""]]}, {"id": "1912.06398", "submitter": "Zhuozhao Zhan", "authors": "Zhuozhao Zhan, Vasan S. Ramachandran, Edwin R. van den Heuvel", "title": "Joint modeling with time-dependent treatment and heteroskedasticity:\n  Bayesian analysis with application to the Framingham Heart Study", "comments": "34 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Medical studies for chronic disease are often interested in the relation\nbetween longitudinal risk factor profiles and individuals' later life disease\noutcomes. These profiles may typically be subject to intermediate structural\nchanges due to treatment or environmental influences. Analysis of such studies\nmay be handled by the joint model framework. However, current joint modeling\ndoes not consider structural changes in the residual variability of the risk\nprofile nor consider the influence of subject-specific residual variability on\nthe time-to-event outcome. In the present paper, we extend the joint model\nframework to address these two heterogeneous intra-individual variabilities. A\nBayesian approach is used to estimate the unknown parameters and simulation\nstudies are conducted to investigate the performance of the method. The\nproposed joint model is applied to the Framingham Heart Study to investigate\nthe influence of anti-hypertensive medication on the systolic blood pressure\nvariability together with its effect on the risk of developing cardiovascular\ndisease. We show that anti-hypertensive medication is associated with elevated\nsystolic blood pressure variability and increased variability elevates risk of\ndeveloping cardiovascular disease.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 10:36:29 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 15:10:13 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Zhan", "Zhuozhao", ""], ["Ramachandran", "Vasan S.", ""], ["Heuvel", "Edwin R. van den", ""]]}, {"id": "1912.06407", "submitter": "Pedro Delicado", "authors": "Pedro Delicado and Daniel Pe\\~na", "title": "Understanding complex predictive models with Ghost Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a procedure for assigning a relevance measure to each explanatory\nvariable in a complex predictive model. We assume that we have a training set\nto fit the model and a test set to check the out of sample performance. First,\nthe individual relevance of each variable is computed by comparing the\npredictions in the test set, given by the model that includes all the variables\nwith those of another model in which the variable of interest is substituted by\nits ghost variable, defined as the prediction of this variable by using the\nrest of explanatory variables. Second, we check the joint effects among the\nvariables by using the eigenvalues of a relevance matrix that is the covariance\nmatrix of the vectors of individual effects. It is shown that in simple models,\nas linear or additive models, the proposed measures are related to standard\nmeasures of significance of the variables and in neural networks models (and in\nother algorithmic prediction models) the procedure provides information about\nthe joint and individual effects of the variables that is not usually available\nby other methods. The procedure is illustrated with simulated examples and the\nanalysis of a large real data set.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 11:06:12 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 17:25:06 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Delicado", "Pedro", ""], ["Pe\u00f1a", "Daniel", ""]]}, {"id": "1912.06560", "submitter": "Jennifer Wadsworth", "authors": "Jennifer L. Wadsworth, Jonathan Tawn", "title": "Higher-dimensional spatial extremes via single-site conditioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently available models for spatial extremes suffer either from\ninflexibility in the dependence structures that they can capture, lack of\nscalability to high dimensions, or in most cases, both of these. We present an\napproach to spatial extreme value theory based on the conditional multivariate\nextreme value model, whereby the limit theory is formed through conditioning\nupon the value at a particular site being extreme. The ensuing methodology\nallows for a flexible class of dependence structures, as well as models that\ncan be fitted in high dimensions. To overcome issues of conditioning on a\nsingle site, we suggest a joint inference scheme based on all observation\nlocations, and implement an importance sampling algorithm to provide spatial\nrealizations and estimates of quantities conditioning upon the process being\nextreme at any of one of an arbitrary set of locations. The modelling approach\nis applied to Australian summer temperature extremes, permitting assessment the\nspatial extent of high temperature events over the continent.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 15:34:10 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Wadsworth", "Jennifer L.", ""], ["Tawn", "Jonathan", ""]]}, {"id": "1912.06590", "submitter": "Weiyu Li", "authors": "Canhong Wen, Weiyu Li, Junxian Zhu, Xueqin Wang", "title": "Best Subset Selection in Reduced Rank Regression", "comments": "This paper has been withdrawn by the authors due to a crucial error\n  in the design of the algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a new algorithm on the best subset selection model in reduced rank\nregression.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 16:34:01 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 16:39:12 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Wen", "Canhong", ""], ["Li", "Weiyu", ""], ["Zhu", "Junxian", ""], ["Wang", "Xueqin", ""]]}, {"id": "1912.06667", "submitter": "Naim Rashid", "authors": "Naim U. Rashid, Daniel J. Luckett, Jingxiang Chen, Michael T. Lawson,\n  Longshaokan Wang, Yunshu Zhang, Eric B. Laber, Yufeng Liu, Jen Jen Yeh,\n  Donglin Zeng, Michael R. Kosorok", "title": "High dimensional precision medicine from patient-derived xenografts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of human cancer often results in significant heterogeneity in\nresponse to treatment. Precision medicine offers potential to improve patient\noutcomes by leveraging this heterogeneity. Individualized treatment rules\n(ITRs) formalize precision medicine as maps from the patient covariate space\ninto the space of allowable treatments. The optimal ITR is that which maximizes\nthe mean of a clinical outcome in a population of interest. Patient-derived\nxenograft (PDX) studies permit the evaluation of multiple treatments within a\nsingle tumor and thus are ideally suited for estimating optimal ITRs. PDX data\nare characterized by correlated outcomes, a high-dimensional feature space, and\na large number of treatments. Existing methods for estimating optimal ITRs do\nnot take advantage of the unique structure of PDX data or handle the associated\nchallenges well. In this paper, we explore machine learning methods for\nestimating optimal ITRs from PDX data. We analyze data from a large PDX study\nto identify biomarkers that are informative for developing personalized\ntreatment recommendations in multiple cancers. We estimate optimal ITRs using\nregression-based approaches such as Q-learning and direct search methods such\nas outcome weighted learning. Finally, we implement a superlearner approach to\ncombine a set of estimated ITRs and show that the resulting ITR performs better\nthan any of the input ITRs, mitigating uncertainty regarding user choice of any\nparticular ITR estimation methodology. Our results indicate that PDX data are a\nvaluable resource for developing individualized treatment strategies in\noncology.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 19:17:27 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Rashid", "Naim U.", ""], ["Luckett", "Daniel J.", ""], ["Chen", "Jingxiang", ""], ["Lawson", "Michael T.", ""], ["Wang", "Longshaokan", ""], ["Zhang", "Yunshu", ""], ["Laber", "Eric B.", ""], ["Liu", "Yufeng", ""], ["Yeh", "Jen Jen", ""], ["Zeng", "Donglin", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1912.06709", "submitter": "Jan Posp\\'i\\v{s}il", "authors": "Jan Posp\\'i\\v{s}il, Tom\\'a\\v{s} Sobotka and Philipp Ziegler", "title": "Robustness and sensitivity analyses for stochastic volatility models\n  under uncertain data structure", "comments": null, "journal-ref": "Empir. Econ. 57(6), 1935-1958, 2019", "doi": "10.1007/s00181-018-1535-3", "report-no": null, "categories": "q-fin.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we perform robustness and sensitivity analysis of several\ncontinuous-time stochastic volatility (SV) models with respect to the process\nof market calibration. The analyses should validate the hypothesis on\nimportance of the jump part in the underlying model dynamics. Also an impact of\nthe long memory parameter is measured for the approximative fractional SV\nmodel. For the first time, the robustness of calibrated models is measured\nusing bootstrapping methods on market data and Monte-Carlo filtering\ntechniques. In contrast to several other sensitivity analysis approaches for SV\nmodels, the newly proposed methodology does not require independence of\ncalibrated parameters - an assumption that is typically not satisfied in\npractice. Empirical study is performed on data sets of Apple Inc. equity\noptions traded in April and May 2015.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 21:04:55 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Posp\u00ed\u0161il", "Jan", ""], ["Sobotka", "Tom\u00e1\u0161", ""], ["Ziegler", "Philipp", ""]]}, {"id": "1912.06739", "submitter": "Amanda Kowalski", "authors": "Amanda Kowalski", "title": "Counting Defiers: Examples from Health Care", "comments": "this article draws heavily from arXiv:1908.05811, arXiv:1908.05810", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a finite sample inference procedure that uses a likelihood function\nderived from the randomization process within an experiment to conduct\ninference on various quantities that capture heterogeneous intervention\neffects. One such quantity is the number of defiers---individuals whose\ntreatment runs counter to the intervention. Results from the literature make\ninformative inference on this quantity seem impossible, but they rely on\ndifferent assumptions and data. I only require data on the cross-tabulations of\na binary intervention and a binary treatment. Replacing the treatment variable\nwith a more general outcome variable, I can perform inference on important\nquantities analogous to the number of defiers. I apply the procedure to test\nsafety and efficacy in hypothetical drug trials for which the point estimate of\nthe average intervention effect implies that at least 40 out of 100 individuals\nwould be saved. In one trial, I infer with 95% confidence that at least 3\nindividuals would be killed, which could stop the drug from being approved.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 23:05:01 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 13:55:18 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 21:50:06 GMT"}, {"version": "v4", "created": "Tue, 1 Sep 2020 20:51:25 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Kowalski", "Amanda", ""]]}, {"id": "1912.06926", "submitter": "Stephen Berg", "authors": "Stephen Berg, Jun Zhu, Murray K. Clayton", "title": "Control variates and Rao-Blackwellization for deterministic sweep Markov\n  chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study control variate methods for Markov chain Monte Carlo (MCMC) in the\nsetting of deterministic sweep sampling using $K\\geq 2$ transition kernels. New\nvariance reduction results are provided for MCMC averages based on sweeps over\ngeneral transition kernels, leading to a particularly simple control variate\nestimator in the setting of deterministic sweep Gibbs sampling. Theoretical\ncomparisons of our proposed control variate estimators with existing literature\nare made, and a simulation study is performed to examine the amount of variance\nreduction in some example cases. We also relate control variate approaches to\napproaches based on conditioning (or Rao-Blackwellization), and show that the\nlatter can be viewed as an approximation of the former. Our theoretical results\nhold for Markov chains under standard geometric drift assumptions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 20:57:19 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Berg", "Stephen", ""], ["Zhu", "Jun", ""], ["Clayton", "Murray K.", ""]]}, {"id": "1912.06928", "submitter": "Modou Ngom", "authors": "Gane Samb Lo, Modou Ngom, Moumouni Diallo", "title": "Extremes, extremal index estimation, records, moment problem for the\n  Pseudo-Lindley distribution and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pseudo-Lindley distribution which was introduced in Zeghdoudi and Nedjar\n(2016) is studied with regards to its upper tail. In that regard, and when the\nunderlying distribution function follows the Pseudo-Lindley law, we investigate\nthe behavior of its values, the asymptotic normality of the Hill estimator and\nthe double-indexed generalized Hill statistic process (Ngom and Lo), the\nasymptotic normality of the records values and the moment problem.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 21:09:02 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Lo", "Gane Samb", ""], ["Ngom", "Modou", ""], ["Diallo", "Moumouni", ""]]}, {"id": "1912.06977", "submitter": "Steve Yadlowsky", "authors": "Steve Yadlowsky, Fabio Pellegrini, Federica Lionetto, Stefan Braune,\n  and Lu Tian", "title": "Estimation and Validation of Ratio-based Conditional Average Treatment\n  Effects Using Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While sample sizes in randomized clinical trials are large enough to estimate\nthe average treatment effect well, they are often insufficient for estimation\nof treatment-covariate interactions critical to studying data-driven precision\nmedicine. Observational data from real world practice may play an important\nrole in alleviating this problem. One common approach in trials is to predict\nthe outcome of interest with separate regression models in each treatment arm,\nand estimate the treatment effect based on the contrast of the predictions.\nUnfortunately, this simple approach may induce spurious treatment-covariate\ninteraction in observational studies when the regression model is misspecified.\nMotivated by the need of modeling the number of relapses in multiple sclerosis\npatients, where the ratio of relapse rates is a natural choice of the treatment\neffect, we propose to estimate the conditional average treatment effect (CATE)\nas the ratio of expected potential outcomes, and derive a doubly robust\nestimator of this CATE in a semiparametric model of treatment-covariate\ninteractions. We also provide a validation procedure to check the quality of\nthe estimator on an independent sample. We conduct simulations to demonstrate\nthe finite sample performance of the proposed methods, and illustrate their\nadvantages on real data by examining the treatment effect of dimethyl fumarate\ncompared to teriflunomide in multiple sclerosis patients.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 05:13:56 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 00:48:55 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Yadlowsky", "Steve", ""], ["Pellegrini", "Fabio", ""], ["Lionetto", "Federica", ""], ["Braune", "Stefan", ""], ["Tian", "Lu", ""]]}, {"id": "1912.06982", "submitter": "Thorsten Dickhaus", "authors": "Anh-Tuan Hoang and Thorsten Dickhaus", "title": "Randomized p-values for multiple testing and their application in\n  replicability analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with testing replicability hypotheses for many endpoints\nsimultaneously. This constitutes a multiple test problem with composite null\nhypotheses. Traditional $p$-values, which are computed under least favourable\nparameter configurations, are over-conservative in the case of composite null\nhypotheses. As demonstrated in prior work, this poses severe challenges in the\nmultiple testing context, especially when one goal of the statistical analysis\nis to estimate the proportion $\\pi_0$ of true null hypotheses. Randomized\n$p$-values have been proposed to remedy this issue. In the present work, we\ndiscuss the application of randomized $p$-values in replicability analysis. In\nparticular, we introduce a general class of statistical models for which valid,\nrandomized $p$-values can be calculated easily. By means of computer\nsimulations, we demonstrate that their usage typically leads to a much more\naccurate estimation of $\\pi_0$. Finally, we apply our proposed methodology to a\nreal data example from genomics.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 05:41:17 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 09:40:02 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Hoang", "Anh-Tuan", ""], ["Dickhaus", "Thorsten", ""]]}, {"id": "1912.06995", "submitter": "Han Lin Shang", "authors": "Ufuk Beyaztas and Han Lin Shang", "title": "On function-on-function regression: Partial least squares approach", "comments": "24 pages, 7 figures", "journal-ref": "Environmental and Ecological Statistics, 2020, 27, 95-114", "doi": "10.1007/s10651-019-00436-1", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis tools, such as function-on-function regression\nmodels, have received considerable attention in various scientific fields\nbecause of their observed high-dimensional and complex data structures. Several\nstatistical procedures, including least squares, maximum likelihood, and\nmaximum penalized likelihood, have been proposed to estimate such\nfunction-on-function regression models. However, these estimation techniques\nproduce unstable estimates in the case of degenerate functional data or are\ncomputationally intensive. To overcome these issues, we proposed a partial\nleast squares approach to estimate the model parameters in the\nfunction-on-function regression model. In the proposed method, the B-spline\nbasis functions are utilized to convert discretely observed data into their\nfunctional forms. Generalized cross-validation is used to control the degrees\nof roughness. The finite-sample performance of the proposed method was\nevaluated using several Monte-Carlo simulations and an empirical data analysis.\nThe results reveal that the proposed method competes favorably with existing\nestimation techniques and some other available function-on-function regression\nmodels, with significantly shorter computational time.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 07:21:11 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Beyaztas", "Ufuk", ""], ["Shang", "Han Lin", ""]]}, {"id": "1912.07012", "submitter": "Michal Balcerek PhD", "authors": "Michal Balcerek and Krzysztof Burnecki", "title": "Testing of fractional Brownian motion in a noisy environment", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": "10.1016/j.chaos.2020.110097", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractional Brownian motion (FBM) is the only Gaussian self-similar process\nwith stationary increments. Its increment process, called fractional Gaussian\nnoise, is ergodic and exhibits a property of power-like decaying\nautocorrelation function (ACF) which leads to the notion of long memory. These\nproperties have made FBM important in modelling real-world data recorded in\ndifferent experiments ranging from biology to telecommunication. These\nexperiments are often disturbed by a noise which source can be just the\ninstrument error. In this paper we propose a rigorous statistical test based on\nthe ACF for FBM with added white Gaussian noise. To this end we derive a\ndistribution of the test statistic which is given explicitly by the generalized\nchi-squared distribution. This allows us to find critical regions for the test\nwith a given significance level. We check the quality of the introduced test by\nstudying its power and comparing with other tests existing in the literature.\nWe also note that the introduced test procedure can be applied to an arbitrary\nGaussian process.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 09:48:28 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 20:47:30 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Balcerek", "Michal", ""], ["Burnecki", "Krzysztof", ""]]}, {"id": "1912.07041", "submitter": "Natsuki Kariya", "authors": "Natsuki Kariya, Sumio Watanabe", "title": "Testing Homogeneity for Normal Mixture Models: Variational Bayes\n  Approach", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": "10.1587/transfun.2019EAP1172", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The test of homogeneity for normal mixtures has been conducted in diverse\nresearch areas, but constructing a theory of the test of homogeneity is\nchallenging because the parameter set for the null hypothesis corresponds to\nsingular points in the parameter space. In this paper, we examine this problem\nfrom a new perspective and offer a theory of hypothesis testing for homogeneity\nbased on a variational Bayes framework. In the conventional theory, the\nconstant order term of the free energy has remained unknown, however, we\nclarify its asymptotic behavior because it is necessary for constructing a\nhypothesis test. Numerical experiments shows the validity of our theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 13:41:48 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Kariya", "Natsuki", ""], ["Watanabe", "Sumio", ""]]}, {"id": "1912.07099", "submitter": "Ian Laga", "authors": "Ian Laga, Xiaoyue Niu, Le Bao", "title": "Modeling the Marked Presence-only Data: A Case Study of Estimating the\n  Female Sex Worker Size in Malawi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certain subpopulations like female sex workers (FSW), men who have sex with\nmen (MSM), and people who inject drugs (PWID) often have higher prevalence of\nHIV/AIDS and are difficult to map directly due to stigma, discrimination, and\ncriminalization. Fine-scale mapping of those populations contributes to the\nprogress towards reducing the inequalities and ending the AIDS epidemic. In\n2016 and 2017, the PLACE surveys were conducted at 3,290 venues in 20 out of\nthe total 28 districts in Malawi to estimate the FSW sizes. These venues\nrepresent a presence-only data set where, instead of knowing both where people\nlive and do not live (presence-absence data), only information about visited\nlocations is available. In this study, we develop a Bayesian model for\npresence-only data and utilize the PLACE data to estimate the FSW size and\nuncertainty interval at a $1.5 \\times 1.5$-km resolution for all of Malawi. The\nestimates can also be aggregated to any desirable level (city/district/region)\nfor implementing targeted HIV prevention and treatment programs in FSW\ncommunities, which have been successful in lowering the incidence of HIV and\nother sexually transmitted infections.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 19:54:00 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 21:19:59 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 15:37:17 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Laga", "Ian", ""], ["Niu", "Xiaoyue", ""], ["Bao", "Le", ""]]}, {"id": "1912.07104", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins and Jeffrey W. Miller", "title": "Robust Inference and Model Criticism Using Bagged Posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Bayesian inference is known to be sensitive to model\nmisspecification, leading to unreliable uncertainty quantification and poor\npredictive performance. However, finding generally applicable and\ncomputationally feasible methods for robust Bayesian inference under\nmisspecification has proven to be a difficult challenge. An intriguing,\neasy-to-use, and widely applicable approach is to use bagging on the Bayesian\nposterior (\"BayesBag\"); that is, to use the average of posterior distributions\nconditioned on bootstrapped datasets. In this paper, we develop the asymptotic\ntheory of BayesBag, propose a model-data mismatch index for model criticism\nusing BayesBag, and empirically validate our theory and methodology on\nsynthetic and real-world data in linear regression, sparse logistic regression,\nand a hierarchical mixed effects model. We find that in the presence of\nsignificant misspecification, BayesBag yields more reproducible inferences and\nhas better predictive accuracy than the standard Bayesian posterior; on the\nother hand, when the model is correctly specified, BayesBag produces superior\nor equally good results. Overall, our results demonstrate that BayesBag\ncombines the attractive modeling features of standard Bayesian inference with\nthe distributional robustness properties of frequentist methods, providing\nbenefits over both Bayes alone and the bootstrap alone.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 20:29:12 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 20:11:35 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 00:48:35 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Miller", "Jeffrey W.", ""]]}, {"id": "1912.07120", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo and Yingjie Feng and Rocio Titiunik", "title": "Prediction Intervals for Synthetic Control Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification is a fundamental problem in the analysis and\ninterpretation of synthetic control (SC) methods. We develop conditional\nprediction intervals in the SC framework, and provide conditions under which\nthese intervals offer finite-sample probability guarantees. Our method allows\nfor covariate adjustment and non-stationary data, among other practically\nrelevant features. The construction begins by noting that the statistical\nuncertainty of the SC prediction is governed by two distinct sources of\nrandomness: one coming from the construction of the (likely misspecified) SC\nweights in the pre-treatment period, and the other coming from the unobservable\nstochastic error in the post-treatment period when the treatment effect is\nanalyzed. Accordingly, our proposed prediction intervals are constructed taking\ninto account both sources of randomness. For implementation, we propose a\nsimulation-based approach along with finite-sample-based probability bound\narguments, naturally leading to principled sensitivity analysis methods. We\nillustrate the numerical performance of our methods using empirical\napplications and a small simulation study.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 22:03:16 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 16:08:31 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Feng", "Yingjie", ""], ["Titiunik", "Rocio", ""]]}, {"id": "1912.07137", "submitter": "Meng Xu", "authors": "Meng Xu, Philip T. Reiss, Ivor Cribben", "title": "Generalized reliability based on distances", "comments": null, "journal-ref": null, "doi": "10.1111/biom.13287", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intraclass correlation coefficient (ICC) is a classical index of\nmeasurement reliability. With the advent of new and complex types of data for\nwhich the ICC is not defined, there is a need for new ways to assess\nreliability. To meet this need, we propose a new distance-based intraclass\ncorrelation coefficient (dbICC), defined in terms of arbitrary distances among\nobservations. We introduce a bias correction to improve the coverage of\nbootstrap confidence intervals for the dbICC, and demonstrate its efficacy via\nsimulation. We illustrate the proposed method by analyzing the test-retest\nreliability of brain connectivity matrices derived from a set of repeated\nfunctional magnetic resonance imaging scans. The Spearman-Brown formula, which\nshows how more intensive measurement increases reliability, is extended to\nencompass the dbICC.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 23:48:14 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 19:49:13 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Xu", "Meng", ""], ["Reiss", "Philip T.", ""], ["Cribben", "Ivor", ""]]}, {"id": "1912.07287", "submitter": "Oluwasegun Ojo", "authors": "Oluwasegun Taiwo Ojo, Antonio Fern\\'andez Anta, Rosa E. Lillo, Carlo\n  Sguera", "title": "Detecting and Classifying Outliers in Big Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose two new outlier detection methods, for identifying and classifying\ndifferent types of outliers in (big) functional data sets. The proposed methods\nare based on an existing method called Massive Unsupervised Outlier Detection\n(MUOD). MUOD detects and classifies outliers by computing for each curve, three\nindices, all based on the concept of linear regression and correlation, which\nmeasure outlyingness in terms of shape, magnitude and amplitude, relative to\nthe other curves in the data. 'Semifast-MUOD', the first method, uses a sample\nof the observations in computing the indices, while 'Fast-MUOD', the second\nmethod, uses the point-wise or L1 median in computing the indices. The\nclassical boxplot is used to separate the indices of the outliers from those of\nthe typical observations. Performance evaluation of the proposed methods using\nsimulated data show significant improvements compared to MUOD, both in outlier\ndetection and computational time. We show that Fast-MUOD is especially well\nsuited to handling big and dense functional datasets with very small\ncomputational time compared to other methods. Further comparisons with some\nrecent outlier detection methods for functional data also show superior or\ncomparable outlier detection accuracy of the proposed methods. We apply the\nproposed methods on weather, population growth, and video data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 10:52:27 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 18:26:55 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Ojo", "Oluwasegun Taiwo", ""], ["Anta", "Antonio Fern\u00e1ndez", ""], ["Lillo", "Rosa E.", ""], ["Sguera", "Carlo", ""]]}, {"id": "1912.07359", "submitter": "Michele Zemplenyi", "authors": "Michele Zemplenyi (1), Mark J. Meyer (2), Andres Cardenas (3),\n  Marie-France Hivert (4 and 9), Sheryl L. Rifas-Shiman (4), Heike Gibson (5),\n  Itai Kloog (6), Joel Schwartz (5 and 8), Emily Oken (4), Dawn L. DeMeo (7),\n  Diane R. Gold (5 and 8) and Brent A. Coull ((1) Department of Biostatistics,\n  Harvard T.H. Chan School of Public Health, (2) Department of Mathematics and\n  Statistics, Georgetown University, (3) Division of Environmental Health\n  Sciences, School of Public Health, University of California, Berkeley, (4)\n  Division of Chronic Disease Research Across the Lifecourse, Department of\n  Population Medicine, Harvard Medical School and Harvard Pilgrim Health Care\n  Institute, (5) Department of Environmental Health, Harvard T.H. Chan School\n  of Public Health, (6) Department of Geography and Environmental Development,\n  Faculty of Humanities and Social Sciences, Ben-Gurion University, (7) Center\n  for Chest Diseases, Brigham and Women's Hospital, (8) Channing Division of\n  Network Medicine, Department of Medicine, Brigham and Women's Hospital,\n  Harvard Medical School, (9) Diabetes Unit, Massachusetts General Hospital)", "title": "Function-on-Function Regression for the Identification of Epigenetic\n  Regions Exhibiting Windows of Susceptibility to Environmental Exposures", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to identify time periods when individuals are most susceptible to\nexposures, as well as the biological mechanisms through which these exposures\nact, is of great public health interest. Growing evidence supports an\nassociation between prenatal exposure to air pollution and epigenetic marks,\nsuch as DNA methylation, but the timing and gene-specific effects of these\nepigenetic changes are not well understood. Here, we present the first study\nthat aims to identify prenatal windows of susceptibility to air pollution\nexposures in cord blood DNA methylation. In particular, we propose a\nfunction-on-function regression model that leverages data from nearby DNA\nmethylation probes to identify epigenetic regions that exhibit windows of\nsusceptibility to ambient particulate matter less than 2.5 microns\n(PM$_{2.5}$). By incorporating the covariance structure among both the\nmultivariate DNA methylation outcome and the time-varying exposure under study,\nthis framework yields greater power to detect windows of susceptibility and\ngreater control of false discoveries than methods that model probes\nindependently. We compare our method to a distributed lag model approach that\nmodels DNA methylation in a probe-by-probe manner, both in simulation and by\napplication to motivating data from the Project Viva birth cohort. In two\nepigenetic regions selected based on prior studies of air pollution effects on\nepigenome-wide methylation, we identify windows of susceptibility to PM$_{2.5}$\nexposure near the beginning and middle of the third trimester of pregnancy.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 17:00:17 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zemplenyi", "Michele", "", "4 and 9"], ["Meyer", "Mark J.", "", "4 and 9"], ["Cardenas", "Andres", "", "4 and 9"], ["Hivert", "Marie-France", "", "4 and 9"], ["Rifas-Shiman", "Sheryl L.", "", "5 and 8"], ["Gibson", "Heike", "", "5 and 8"], ["Kloog", "Itai", "", "5 and 8"], ["Schwartz", "Joel", "", "5 and 8"], ["Oken", "Emily", "", "5 and 8"], ["DeMeo", "Dawn L.", "", "5 and 8"], ["Gold", "Diane R.", "", "5 and 8"], ["Coull", "Brent A.", ""]]}, {"id": "1912.07364", "submitter": "Claus Ekstr{\\o}m", "authors": "Claus Thorn Ekstr{\\o}m and Hans Van Eetvelde and Christophe Ley and\n  Ulf Brefeld", "title": "Evaluating one-shot tournament predictions", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Tournament Rank Probability Score (TRPS) as a measure to\nevaluate and compare pre-tournament predictions, where predictions of the full\ntournament results are required to be available before the tournament begins.\nThe TRPS handles partial ranking of teams, gives credit to predictions that are\nonly slightly wrong, and can be modified with weights to stress the importance\nof particular features of the tournament prediction. Thus, the Tournament Rank\nPrediction Score is more flexible than the commonly preferred log loss score\nfor such tasks. In addition, we show how predictions from historic tournaments\ncan be optimally combined into ensemble predictions in order to maximize the\nTRPS for a new tournament.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 21:38:40 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Ekstr\u00f8m", "Claus Thorn", ""], ["Van Eetvelde", "Hans", ""], ["Ley", "Christophe", ""], ["Brefeld", "Ulf", ""]]}, {"id": "1912.07366", "submitter": "Piyush Pandita", "authors": "Piyush Pandita, Nimish Awalgaonkar, Ilias Bilionis and Jitesh Panchal", "title": "Learning Arbitrary Quantities of Interest from Expensive Black-Box\n  Functions through Bayesian Sequential Optimal Design", "comments": "58 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating arbitrary quantities of interest (QoIs) that are non-linear\noperators of complex, expensive-to-evaluate, black-box functions is a\nchallenging problem due to missing domain knowledge and finite budgets.\nBayesian optimal design of experiments (BODE) is a family of methods that\nidentify an optimal design of experiments (DOE) under different contexts, using\nonly in a limited number of function evaluations. Under BODE methods,\nsequential design of experiments (SDOE) accomplishes this task by selecting an\noptimal sequence of experiments while using data-driven probabilistic surrogate\nmodels instead of the expensive black-box function. Probabilistic predictions\nfrom the surrogate model are used to define an information acquisition function\n(IAF) which quantifies the marginal value contributed or the expected\ninformation gained by a hypothetical experiment. The next experiment is\nselected by maximizing the IAF. A generally applicable IAF is the expected\ninformation gain (EIG) about a QoI as captured by the expectation of the\nKullback-Leibler divergence between the predictive distribution of the QoI\nafter doing a hypothetical experiment and the current predictive distribution\nabout the same QoI. We model the underlying information source as a\nfully-Bayesian, non-stationary Gaussian process (FBNSGP), and derive an\napproximation of the information gain of a hypothetical experiment about an\narbitrary QoI conditional on the hyper-parameters The EIG about the same QoI is\nestimated by sample averages to integrate over the posterior of the\nhyper-parameters and the potential experimental outcomes. We demonstrate the\nperformance of our method in four numerical examples and a practical\nengineering problem of steel wire manufacturing. The method is compared to two\nclassic SDOE methods: random sampling and uncertainty sampling.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 13:55:07 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Pandita", "Piyush", ""], ["Awalgaonkar", "Nimish", ""], ["Bilionis", "Ilias", ""], ["Panchal", "Jitesh", ""]]}, {"id": "1912.07433", "submitter": "Tianyu Zhan", "authors": "Tianyu Zhan and Jian Kang", "title": "Targeting the Uniformly Most Powerful Unbiased Test in Sample Size\n  Reassessment Adaptive Clinical Trials with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent pharmaceutical drug development, adaptive clinical trials become\nmore and more appealing due to ethical considerations, and the ability to\naccommodate uncertainty while conducting the trial. Several methods have been\nproposed to optimize a certain study design within a class of candidates, but\nfinding an optimal hypothesis testing strategy for a given design remains\nchallenging, mainly due to the complex likelihood function involved. This\nproblem is of great interest from both patient and sponsor perspectives,\nbecause the smallest sample size is required for the optimal hypothesis testing\nmethod to achieve a desired level of power. To address these issues, we propose\na novel application of the deep neural network to construct the test statistics\nand the critical value with a controlled type I error rate in a computationally\nefficient manner. We apply the proposed method to a sample size reassessment\nconfirmatory adaptive study MUSEC (MUltiple Sclerosis and Extract of Cannabis),\ndemonstrating the proposed method outperforms the existing alternatives.\nSimulation studies are also performed to demonstrate that our proposed method\nessentially establishes the underlying uniformly most powerful (UMP) unbiased\ntest in several non-adaptive designs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 15:08:24 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zhan", "Tianyu", ""], ["Kang", "Jian", ""]]}, {"id": "1912.07466", "submitter": "Karl Schurter", "authors": "Joris Pinkse and Karl Schurter", "title": "Estimation of Auction Models with Shape Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce several new estimation methods that leverage shape constraints\nin auction models to estimate various objects of interest, including the\ndistribution of a bidder's valuations, the bidder's ex ante expected surplus,\nand the seller's counterfactual revenue. The basic approach applies broadly in\nthat (unlike most of the literature) it works for a wide range of auction\nformats and allows for asymmetric bidders. Though our approach is not\nrestrictive, we focus our analysis on first--price, sealed--bid auctions with\nindependent private valuations. We highlight two nonparametric estimation\nstrategies, one based on a least squares criterion and the other on a maximum\nlikelihood criterion. We also provide the first direct estimator of the\nstrategy function. We establish several theoretical properties of our methods\nto guide empirical analysis and inference. In addition to providing the\nasymptotic distributions of our estimators, we identify ways in which\nmethodological choices should be tailored to the objects of their interest. For\nobjects like the bidders' ex ante surplus and the seller's counterfactual\nexpected revenue with an additional symmetric bidder, we show that our\ninput--parameter--free estimators achieve the semiparametric efficiency bound.\nFor objects like the bidders' inverse strategy function, we provide an easily\nimplementable boundary--corrected kernel smoothing and transformation method in\norder to ensure the squared error is integrable over the entire support of the\nvaluations. An extensive simulation study illustrates our analytical results\nand demonstrates the respective advantages of our least--squares and maximum\nlikelihood estimators in finite samples. Compared to estimation strategies\nbased on kernel density estimation, the simulations indicate that the smoothed\nversions of our estimators enjoy a large degree of robustness to the choice of\nan input parameter.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 16:01:16 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Pinkse", "Joris", ""], ["Schurter", "Karl", ""]]}, {"id": "1912.07578", "submitter": "Ali Shojaie", "authors": "Lina Lin, Mathias Drton and Ali Shojaie", "title": "Statistical significance in high-dimensional linear mixed models", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the development of an inferential framework for\nhigh-dimensional linear mixed effect models. These are suitable models, for\ninstance, when we have $n$ repeated measurements for $M$ subjects. We consider\na scenario where the number of fixed effects $p$ is large (and may be larger\nthan $M$), but the number of random effects $q$ is small. Our framework is\ninspired by a recent line of work that proposes de-biasing penalized estimators\nto perform inference for high-dimensional linear models with fixed effects\nonly. In particular, we demonstrate how to correct a `naive' ridge estimator in\nextension of work by B\\\"uhlmann (2013) to build asymptotically valid confidence\nintervals for mixed effect models. We validate our theoretical results with\nnumerical experiments, in which we show our method outperforms those that fail\nto account for correlation induced by the random effects. For a practical\ndemonstration we consider a riboflavin production dataset that exhibits group\nstructure, and show that conclusions drawn using our method are consistent with\nthose obtained on a similar dataset without group structure.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 18:47:09 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Lin", "Lina", ""], ["Drton", "Mathias", ""], ["Shojaie", "Ali", ""]]}, {"id": "1912.07602", "submitter": "Elvis Cui", "authors": "Elvis Cui, Heather Zhou", "title": "Projection pursuit with applications to scRNA sequencing data", "comments": "10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the limitations of PCA as a dimension reduction\ntechnique and study its extension, projection pursuit (PP), which is a broad\nclass of linear dimension reduction methods. We first discuss the relevant\nconcepts and theorems and then apply PCA and PP (with negative standardized\nShannon's entropy as the projection index) on single cell RNA sequencing data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 16:57:03 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Cui", "Elvis", ""], ["Zhou", "Heather", ""]]}, {"id": "1912.07699", "submitter": "Guangxing Wang", "authors": "Guangxing Wang and Wolfgang Polonik", "title": "High Order Adjusted Block-wise Empirical Likelihood For Weakly Dependent\n  Data", "comments": "Typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The upper limit on the coverage probability of the empirical likelihood ratio\nconfidence region severely hampers its application in statistical inferences.\nThe root cause of this upper limit is the convex hull of the estimating\nfunctions that is used in the construction of the profile empirical likelihood.\nFor i.i.d data, various methods have been proposed to solve this issue by\nmodifying the convex hull, but it is not clear how well these methods perform\nwhen the data is no longer independent. In this paper, we consider weakly\ndependent multivariate data, and we combine the block-wise empirical likelihood\nwith the adjusted empirical likelihood to tackle data dependency and the convex\nhull constraint simultaneously. We show that our method not only preserves the\nmuch celebrated asymptotic $\\chi^2-$distribution, but also improves the\ncoverage probability by removing the upper limit. Further, we show that our\nmethod is also Bartlett correctable, thus is able to achieve high order\nasymptotic coverage accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 20:50:21 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 05:22:17 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Wang", "Guangxing", ""], ["Polonik", "Wolfgang", ""]]}, {"id": "1912.07761", "submitter": "David Degras", "authors": "David Degras", "title": "Sparse Group Fused Lasso for Model Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces the sparse group fused lasso (SGFL) as a statistical\nframework for segmenting sparse regression models with multivariate time\nseries. To compute solutions of the SGFL, a nonsmooth and nonseparable convex\nprogram, we develop a hybrid optimization method that is fast, requires no\ntuning parameter selection, and is guaranteed to converge to a global\nminimizer. In numerical experiments, the hybrid method compares favorably to\nstate-of-the-art techniques with respect to computation time and numerical\naccuracy; benefits are particularly substantial in high dimension. The method's\nstatistical performance is satisfactory in recovering nonzero regression\ncoefficients and excellent in change point detection. An application to air\nquality data is presented. The hybrid method is implemented in the R package\nsparseGFL available on the author's Github page.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 23:52:34 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 17:21:37 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Degras", "David", ""]]}, {"id": "1912.07775", "submitter": "Lijing Ma", "authors": "Lijing Ma, Andrew Grant, Georgy Sofronov", "title": "Multiple Change Point Detection and Validation in Autoregressive Time\n  Series Data", "comments": "Changepoint detection, Autoregressive time series, Likelihood ratio\n  scan statistics, Multiple testing problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is quite common that the structure of a time series changes abruptly.\nIdentifying these change points and describing the model structure in the\nsegments between these change points is of interest. In this paper, time series\ndata is modelled assuming each segment is an autoregressive time series with\npossibly different autoregressive parameters. This is achieved using two main\nsteps. The first step is to use a likelihood ratio scan based estimation\ntechnique to identify these potential change points to segment the time series.\nOnce these potential change points are identified, modified parametric spectral\ndiscrimination tests are used to validate the proposed segments. A numerical\nstudy is conducted to demonstrate the performance of the proposed method across\nvarious scenarios and compared against other contemporary techniques.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 01:16:48 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Ma", "Lijing", ""], ["Grant", "Andrew", ""], ["Sofronov", "Georgy", ""]]}, {"id": "1912.07967", "submitter": "Mahdi Doostparast", "authors": "M. Doostparast, M. Hashempour, E. Velayati Moghaddam 1", "title": "Weibull analysis with sequential order statistics under a power trend\n  model for hazard rates", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In engineering systems, it is usually assumed that lifetimes of components\nare independent and identically distributed (iid). But, the failure of a\ncomponent results in a higher load on the remaining components and hence causes\nthe distribution of the surviving components change. For modeling this kind of\nsystems, the theory of sequential order statistics (SOS) can be used. Assuming\nWeibull distribution for lifetimes of components and conditionally proportional\nhazard rates model as a special case of the SOS theory, the maximum likelihood\nestimates of the unknown parameters are obtained in different cases. A new\nmodel, denoted by PTCPHM, as a generalization of the iid case is proposed, and\nthen statistical inferential methods including point and interval estimation as\nwell as hypothesis tests under PTCPHM are then developed. Finally, a real data\non failure times of aircraft components, due to Mann and Fertig (1973), is\nanalyzed to illustrate the model and inferential methods developed here.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 12:29:23 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Doostparast", "M.", ""], ["Hashempour", "M.", ""], ["1", "E. Velayati Moghaddam", ""]]}, {"id": "1912.08050", "submitter": "Xiangyu Luo", "authors": "Qiuyu Wu, Xiangyu Luo", "title": "Nonparametric Bayesian Two-Level Clustering for Subject-Level\n  Single-Cell Expression Data", "comments": null, "journal-ref": null, "doi": "10.5705/ss.202020.0337", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of single-cell sequencing opens new avenues for personalized\ntreatment. In this paper, we address a two-level clustering problem of\nsimultaneous subject subgroup discovery (subject level) and cell type detection\n(cell level) for single-cell expression data from multiple subjects. However,\ncurrent statistical approaches either cluster cells without considering the\nsubject heterogeneity or group subjects without using the single-cell\ninformation. To bridge the gap between cell clustering and subject grouping, we\ndevelop a nonparametric Bayesian model, Subject and Cell clustering for\nSingle-Cell expression data (SCSC) model, to achieve subject and cell grouping\nsimultaneously. SCSC does not need to prespecify the subject subgroup number or\nthe cell type number. It automatically induces subject subgroup structures and\nmatches cell types across subjects. Moreover, it directly models the\nsingle-cell raw count data by deliberately considering the data's dropouts,\nlibrary sizes, and over-dispersion. A blocked Gibbs sampler is proposed for the\nposterior inference. Simulation studies and the application to a multi-subject\niPSC scRNA-seq dataset validate the ability of SCSC to simultaneously cluster\nsubjects and cells.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 14:45:31 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 04:03:21 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wu", "Qiuyu", ""], ["Luo", "Xiangyu", ""]]}, {"id": "1912.08087", "submitter": "Peter Cameron", "authors": "R. A. Bailey, Peter J. Cameron, L. H. Soicher, E. R. Williams", "title": "Substitutes for the non-existent square lattice designs for 36 varieties", "comments": "Paper submitted to JABES special issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Square lattice designs are often used in trials of new varieties of various\nagricultural crops. However, there are no square lattice designs for 36\nvarieties in blocks of size six for four or more replicates. Here we use three\ndifferent approaches to construct designs for up to eight replicates. All the\ndesigns perform well in terms of giving a low average variance of variety\ncontrasts.\n  Supplementary materials are available online.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 15:40:22 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Bailey", "R. A.", ""], ["Cameron", "Peter J.", ""], ["Soicher", "L. H.", ""], ["Williams", "E. R.", ""]]}, {"id": "1912.08149", "submitter": "Xuze Zhang", "authors": "Xuze Zhang, Saumyadipta Pyne and Benjamin Kedem", "title": "Estimation of Residential Radon Concentration in Pennsylvania Counties\n  by Data Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data fusion method for the estimation of residential radon level\ndistribution in any Pennsylvania county is proposed. The method is based on a\nmulti-sample density ratio model with variable tilts and is applied to combined\nradon data from a reference county of interest and its neighboring counties.\nBeaver county and its four immediate neighbors are taken as a case in point.\nThe distribution of radon concentration is estimated in each of six periods,\nand then the analysis is repeated combining the data from all the periods to\nobtain estimates of Beaver threshold probabilities and the corresponding\nconfidence intervals.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 17:33:43 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Zhang", "Xuze", ""], ["Pyne", "Saumyadipta", ""], ["Kedem", "Benjamin", ""]]}, {"id": "1912.08162", "submitter": "Adam Lane", "authors": "Adam Lane", "title": "Efficiency of Observed Information Adaptive Designs", "comments": "25 pages, 3 figures, 1 Table, 1 supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work the primary objective is to maximize the precision of the\nmaximum likelihood estimate in a linear regression model through the efficient\ndesign of the experiment. One common measure of precision is the unconditional\nmean square error. Unconditional mean square error has been a primary motivator\nfor optimal designs; commonly, defined as the design that maximizes a concave\nfunction of the expected Fisher information. The inverse of expected Fisher\ninformation is asymptotically equal to the mean square error of the maximum\nlikelihood estimate. There is a substantial amount of existing literature that\nargues the mean square error conditioned on an appropriate ancillary statistic\nbetter represents the precision of the maximum likelihood estimate. Despite\nevidence in favor of conditioning, limited effort has been made to find designs\nthat are optimal with respect to conditional mean square error. The inverse of\nobserved Fisher information is a higher order approximation of the conditional\nmean square error than the inverse of expected Fisher information [Efron and\nHinkley (1978)]. In light of this, a more relevant objective is to find designs\nthat optimize observed Fisher information. Unlike expected Fisher information,\nobserved Fisher information depends on the observed data and cannot be used to\ndesign an experiment completely in advance of data collection. In a sequential\nexperiment the observed Fisher information from past observations is available\nto inform the design of the next observation. In this work an adaptive design\nthat incorporates observed Fisher information is proposed for linear regression\nmodels. It is shown that the proposed design is more efficient, at the limit,\nthan any fixed design, including the optimal design, with respect to\nconditional mean square error.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:00:36 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 13:29:11 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Lane", "Adam", ""]]}, {"id": "1912.08233", "submitter": "Dennis Dobler", "authors": "Dennis Dobler", "title": "Randomization empirical processes", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article creates a link between two well-established fields in\nmathematical statistics: empirical processes and inference based on\nrandomization via algebraic groups. To this end, a broadly applicable\nconditional weak convergence theorem is developed for empirical processes that\nare based on randomized observations. Random elements of an algebraic group are\napplied to the data vectors from which the randomized version of a statistic is\nderived. Combining a variant of the functional delta-method with a suitable\nstudentization of the statistic, asymptotically exact hypothesis tests can be\ndeduced, while the finite sample exactness property under group-invariant\nsub-hypotheses is preserved. The methodology is exemplified with three\nexamples: the Pearson correlation coefficient, a Mann-Whitney effect based on\nright-censored paired data, and a competing risks analysis. The practical\nusefulness of the approaches is assessed through simulation studies and an\napplication to data from patients suffering from diabetic retinopathy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 19:09:24 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 07:19:11 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Dobler", "Dennis", ""]]}, {"id": "1912.08337", "submitter": "Enrique del Castillo", "authors": "Enrique del Castillo and Rainer Goeb", "title": "A Bivariate Dead Band Process Adjustment Policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bivariate extension to Box and Jenkins (1963) feedback adjustment problem\nis presented in this paper. The model balances the fixed cost of making an\nadjustment, which is assumed independent of the magnitude of the adjustments,\nwith the cost of running the process off-target, which is assumed quadratic. It\nis also assumed that two controllable factors are available to compensate for\nthe deviations from target of two responses in the presence of a bivariate\nIMA(1,1) disturbance. The optimal policy has the form of a \"dead band\", in\nwhich adjustments are justified only when the predicted process responses\nexceed some boundary in $\\mathbb{R}^2$. This boundary indicates when the\nresponses are predicted to be far enough from their targets that an additional\nadjustment or intervention in the process is justified. Although originally\ndeveloped to control a machine tool, dead band control policies have\napplication in other areas. For example, they could be used to control a\ndisease through the application of a drug to a patient depending on the level\nof a substance in the body (e.g., diabetes control). This paper presents\nanalytical formulae for the computation of the loss function that combines\noff-target and adjustment costs per time unit. Expressions are derived for the\naverage adjustment interval and for the scaled mean square deviations from\ntarget. The minimization of the loss function and the practical use of the\nresulting dead band adjustment strategy is illustrated with an application to a\nsemiconductor manufacturing process.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 01:46:21 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["del Castillo", "Enrique", ""], ["Goeb", "Rainer", ""]]}, {"id": "1912.08348", "submitter": "Farzana Nasrin", "authors": "Farzana Nasrin, Christopher Oballe, David L. Boothe, and Vasileios\n  Maroulas", "title": "Bayesian Topological Learning for Brain State Classification", "comments": "7 pages and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of human brain states through electroencephalograph (EEG)\nsignals is a crucial step in human-machine communications. However, classifying\nand analyzing EEG signals are challenging due to their noisy, nonlinear and\nnonstationary nature. Current methodologies for analyzing these signals often\nfall short because they have several regularity assumptions baked in. This work\nprovides an effective, flexible and noise-resilient scheme to analyze EEG by\nextracting pertinent information while abiding by the 3N (noisy, nonlinear and\nnonstationary) nature of data. We implement a topological tool, namely\npersistent homology, that tracks the evolution of topological features over\ntime intervals and incorporates individual's expectations as prior knowledge by\nmeans of a Bayesian framework to compute posterior distributions. Relying on\nthese posterior distributions, we apply Bayes factor classification to noisy\nEEG measurements. The performance of this Bayesian classification scheme is\nthen compared with other existing methods for EEG signals.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 02:36:03 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Nasrin", "Farzana", ""], ["Oballe", "Christopher", ""], ["Boothe", "David L.", ""], ["Maroulas", "Vasileios", ""]]}, {"id": "1912.08370", "submitter": "Yaqing Xu", "authors": "Yaqing Xu, Mengyun Wu, Shuangge Ma", "title": "Multidimensional molecular changes-environment interaction analysis for\n  disease outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the outcomes and phenotypes of complex diseases, multiple types of\nmolecular (genetic, genomic, epigenetic, etc.) changes, environmental risk\nfactors, and their interactions have been found to have important\ncontributions. In each of the existing studies, only the interactions between\none type of molecular changes and environmental risk factors have been\nanalyzed. In recent biomedical studies, multidimensional profiling, under which\ndata on multiple types of molecular changes is collected on the same subjects,\nis becoming popular. A myriad of recent studies have shown that collectively\nanalyzing multiple types of molecular changes is not only biologically sensible\nbut also leads to improved estimation and prediction. In this study, we conduct\nM-E interaction analysis, with M standing for multidimensional molecular\nchanges and E standing for environmental risk factors, which can accommodate\nmultiple types of molecular measurements and sufficiently account for their\noverlapping information (attributable to regulations) as well as independent\ninformation. The proposed approach is based on the penalization technique, has\na solid statistical ground, and can be effectively realized. Extensive\nsimulation shows that it outperforms multiple closely relevant alternatives. In\nthe analysis of TCGA (The Cancer Genome Atlas) data on lung adenocarcinoma and\ncutaneous melanoma, sensible findings with superior stability and prediction\nare made.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 03:54:47 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Xu", "Yaqing", ""], ["Wu", "Mengyun", ""], ["Ma", "Shuangge", ""]]}, {"id": "1912.08612", "submitter": "Robert O'Shea", "authors": "Robert O'Shea", "title": "Interpreting Missing Data Patterns in the ICU", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PURPOSE: Clinical examinations are performed on the basis of necessity.\nHowever, our decisions to investigate and document are influenced by various\nother factors, such as workload and preconceptions. Data missingness patterns\nmay contain insights into conscious and unconscious norms of clinical practice.\nMETHODS: We examine data from the SPOTLIGHT study, a multi-centre cohort study\nof the effect of prompt ICU admission on mortality. We identify missing values\nand generate an auxiliary dataset indicating the missing entries. We deploy\nsparse Gaussian Graphical modelling techniques to identify conditional\ndependencies between the observed data and missingness patterns. We quantify\nthese associations with sparse partial correlation, correcting for multiple\ncollinearity. RESULTS: We identify 35 variables which significantly influence\ndata missingness patterns (alpha = 0.01). We identify reduced recording of\nessential monitoring such as temperature (partial corr. = -0.0542, p =\n6.65e-10), respiratory rate (partial corr. = -0.0437, p = 5.15e-07) and urea\n(partial corr. = -0.0263, p = 0.001611) in patients with reduced consciousness.\nWe demonstrate reduction of temperature (partial corr. = -0.04, p = 8.5e-06),\nurine output (partial corr. = -0.05, p = 7.5e-09), lactate (partial corr. =\n-0.03, p = 0.00032) and bilirubin (partial corr. = -0.03, p = 0.00137)\nmonitoring due to winter pressures. We provide statistical evidence of Missing\nNot at Random patterns in FiO2 and SF ratio recording. CONCLUSIONS: Graphical\nmissingness analysis offers valuable insights into critical care delivery,\nidentifying specific areas for quality improvement.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 10:24:59 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["O'Shea", "Robert", ""]]}, {"id": "1912.08869", "submitter": "Neil Dhir", "authors": "Mathias Edman and Neil Dhir", "title": "Boltzmann Exploration Expectation-Maximisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general method for fitting finite mixture models (FMM). Learning\nin a mixture model consists of finding the most likely cluster assignment for\neach data-point, as well as finding the parameters of the clusters themselves.\nIn many mixture models, this is difficult with current learning methods, where\nthe most common approach is to employ monotone learning algorithms e.g. the\nconventional expectation-maximisation algorithm. While effective, the success\nof any monotone algorithm is crucially dependant on good parameter\ninitialisation, where a common choice is $K$-means initialisation, commonly\nemployed for Gaussian mixture models.\n  For other types of mixture models, the path to good initialisation parameters\nis often unclear and may require a problem-specific solution. To this end, we\npropose a general heuristic learning algorithm that utilises Boltzmann\nexploration to assign each observation to a specific base distribution within\nthe mixture model, which we call Boltzmann exploration expectation-maximisation\n(BEEM). With BEEM, hard assignments allow straight forward parameter learning\nfor each base distribution by conditioning only on its assigned observations.\nConsequently, it can be applied to mixtures of any base distribution where\nsingle component parameter learning is tractable. The stochastic learning\nprocedure is able to escape local optima and is thus insensitive to parameter\ninitialisation. We show competitive performance on a number of synthetic\nbenchmark cases as well as on real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:15:04 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Edman", "Mathias", ""], ["Dhir", "Neil", ""]]}, {"id": "1912.08915", "submitter": "Georg  Stadler", "authors": "Karina Koval, Alen Alexanderian, Georg Stadler", "title": "Optimal experimental design under irreducible uncertainty for linear\n  inverse problems governed by PDEs", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/ab89c5", "report-no": null, "categories": "math.OC cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for computing A-optimal sensor placements for\ninfinite-dimensional Bayesian linear inverse problems governed by PDEs with\nirreducible model uncertainties. Here, irreducible uncertainties refers to\nuncertainties in the model that exist in addition to the parameters in the\ninverse problem, and that cannot be reduced through observations. Specifically,\ngiven a statistical distribution for the model uncertainties, we compute the\noptimal design that minimizes the expected value of the posterior covariance\ntrace. The expected value is discretized using Monte Carlo leading to an\nobjective function consisting of a sum of trace operators and a binary-inducing\npenalty. Minimization of this objective requires a large number of PDE solves\nin each step. To make this problem computationally tractable, we construct a\ncomposite low-rank basis using a randomized range finder algorithm to eliminate\nforward and adjoint PDE solves. We also present a novel formulation of the\nA-optimal design objective that requires the trace of an operator in the\nobservation rather than the parameter space. The binary structure is enforced\nusing a weighted regularized $\\ell_0$-sparsification approach. We present\nnumerical results for inference of the initial condition in a subsurface flow\nproblem with inherent uncertainty in the flow fields and in the initial times.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 22:16:31 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 02:15:54 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Koval", "Karina", ""], ["Alexanderian", "Alen", ""], ["Stadler", "Georg", ""]]}, {"id": "1912.08993", "submitter": "Bai Jiang", "authors": "Bai Jiang, Qiang Sun", "title": "Bayesian high-dimensional linear regression with generic spike-and-slab\n  priors", "comments": "17 pages for main file, 13 pages for appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike-and-slab priors are popular Bayesian solutions for high-dimensional\nlinear regression problems. Previous theoretical studies on spike-and-slab\nmethods focus on specific prior formulations and use prior-dependent conditions\nand analyses, and thus can not be generalized directly. In this paper, we\npropose a class of generic spike-and-slab priors and develop a unified\nframework to rigorously assess their theoretical properties. Technically, we\nprovide general conditions under which generic spike-and-slab priors can\nachieve the nearly-optimal posterior contraction rate and the model selection\nconsistency. Our results include those of Narisetty and He (2014) and Castillo\net al. (2015) as special cases.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 02:42:54 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 21:03:12 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Jiang", "Bai", ""], ["Sun", "Qiang", ""]]}, {"id": "1912.09146", "submitter": "Marc Ditzhaus", "authors": "Marc Ditzhaus, Roland Fried and Markus Pauly", "title": "QANOVA: Quantile-based Permutation Methods For General Factorial Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population means and standard deviations are the most common estimands to\nquantify effects in factorial layouts. In fact, most statistical procedures in\nsuch designs are built towards inferring means or contrasts thereof. For more\nrobust analyses, we consider the population median, the interquartile range\n(IQR) and more general quantile combinations as estimands in which we formulate\nnull hypotheses and calculate compatible confidence regions. Based upon\nsimultaneous multivariate central limit theorems and corresponding resampling\nresults, we derive asymptotically correct procedures in general, potentially\nheteroscedastic, factorial designs with univariate endpoints. Special cases\ncover robust tests for the population median or the IQR in arbitrary crossed\none-, two- and higher-way layouts with potentially heteroscedastic error\ndistributions. In extensive simulations we analyze their small sample\nproperties and also conduct an illustrating data analysis comparing children's\nheight and weight from different countries.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 11:54:51 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 15:58:14 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Ditzhaus", "Marc", ""], ["Fried", "Roland", ""], ["Pauly", "Markus", ""]]}, {"id": "1912.09185", "submitter": "Zhenyu Zhang", "authors": "Zhenyu Zhang, Akihiko Nishimura, Paul Bastide, Xiang Ji, Rebecca P.\n  Payne, Philip Goulder, Philippe Lemey, Marc A. Suchard", "title": "Large-scale inference of correlation among mixed-type biological traits\n  with phylogenetic multivariate probit models", "comments": "24 pages, 6 figures, 2 tables. Version accepted by Annals of Applied\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring concerted changes among biological traits along an evolutionary\nhistory remains an important yet challenging problem. Besides adjusting for\nspurious correlation induced from the shared history, the task also requires\nsufficient flexibility and computational efficiency to incorporate multiple\ncontinuous and discrete traits as data size increases. To accomplish this, we\njointly model mixed-type traits by assuming latent parameters for binary\noutcome dimensions at the tips of an unknown tree informed by molecular\nsequences. This gives rise to a phylogenetic multivariate probit model. With\nlarge sample sizes, posterior computation under this model is problematic, as\nit requires repeated sampling from a high-dimensional truncated normal\ndistribution. Current best practices employ multiple-try rejection sampling\nthat suffers from slow-mixing and a computational cost that scales\nquadratically in sample size. We develop a new inference approach that exploits\n1) the bouncy particle sampler (BPS) based on piecewise deterministic Markov\nprocesses to simultaneously sample all truncated normal dimensions, and 2)\nnovel dynamic programming that reduces the cost of likelihood and gradient\nevaluations for BPS to linear in sample size. In an application with 535 HIV\nviruses and 24 traits that necessitates sampling from a 12,840-dimensional\ntruncated normal, our method makes it possible to estimate the across-trait\ncorrelation and detect factors that affect the pathogen's capacity to cause\ndisease. This inference framework is also applicable to a broader class of\ncovariance structures beyond comparative biology.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 13:33:52 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 16:04:04 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2019 15:40:31 GMT"}, {"version": "v4", "created": "Wed, 23 Sep 2020 22:24:40 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Zhang", "Zhenyu", ""], ["Nishimura", "Akihiko", ""], ["Bastide", "Paul", ""], ["Ji", "Xiang", ""], ["Payne", "Rebecca P.", ""], ["Goulder", "Philip", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1912.09204", "submitter": "Samvel Gasparyan", "authors": "Samvel B. Gasparyan, Folke Folkvaljon, Olof Bengtsson, Joan\n  Buenconsejo, Gary G. Koch", "title": "Adjusted Win Ratio with Stratification: Calculation Methods and\n  Interpretation", "comments": null, "journal-ref": "Statistical Methods in Medical Research 2020", "doi": "10.1177/0962280220942558", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The win ratio is a general method of comparing locations of distributions of\ntwo independent, ordinal random variables, and it can be estimated without\ndistributional assumptions. In this paper we provide a unified theory of win\nratio estimation in the presence of stratification and adjustment by a numeric\nvariable. Building step by step on the estimate of the crude win ratio we\ncompare corresponding tests with well known nonparametric tests of group\ndifference (Wilcoxon rank-sum test, Fligner-Plicello test,\nCochran-Mantel-Haenszel test, test based on the regression on ranks and the\nrank ANCOVA test). We show that the win ratio gives an interpretable treatment\neffect measure with corresponding test to detect treatment effect difference\nunder minimal assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 14:02:56 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Gasparyan", "Samvel B.", ""], ["Folkvaljon", "Folke", ""], ["Bengtsson", "Olof", ""], ["Buenconsejo", "Joan", ""], ["Koch", "Gary G.", ""]]}, {"id": "1912.09468", "submitter": "Deyu Ming", "authors": "Deyu Ming and Serge Guillas", "title": "Linked Gaussian Process Emulation for Systems of Computer Models using\n  Mat\\'ern Kernels and Adaptive Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art linked Gaussian process offers a way to build analytical\nemulators for systems of computer models. We generalize the closed form\nexpressions for the linked Gaussian process under the squared exponential\nkernel to a class of Mat\\'ern kernels, that are essential in advanced\napplications. An iterative procedure to construct linked Gaussian processes as\nsurrogate models for any feed-forward systems of computer models is presented\nand illustrated on a feed-back coupled satellite system. We also introduce an\nadaptive design algorithm that could increase the approximation accuracy of\nlinked Gaussian process surrogates with reduced computational costs on running\nexpensive computer systems, by allocating runs and refining emulators of\nindividual sub-models based on their heterogeneous functional complexity.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 18:51:40 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 15:11:21 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 17:27:18 GMT"}, {"version": "v4", "created": "Sun, 7 Feb 2021 15:49:27 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ming", "Deyu", ""], ["Guillas", "Serge", ""]]}, {"id": "1912.09526", "submitter": "Jeremy Ash", "authors": "Jeremy R. Ash and Jacqueline M. Hughes-Oliver", "title": "Confidence Bands and Hypothesis Test Methods for Recall and Precision\n  Curves at Extremely Small Fractions with Applications to Drug Discovery", "comments": "41 pages, 7 figures, 13 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR cs.LG q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In virtual screening for drug discovery, recall curves are used to assess the\nperformance of ranking algorithms, in which recall is a function of the\nfraction of data prioritized for experimental testing. Unfortunately,\nresearchers almost never consider the uncertainty in the estimation of the\nrecall curve when benchmarking algorithms. We confirm that a recently developed\nprocedure for estimating pointwise confidence intervals for recall curves --\nand closely related variants, such as precision curves -- can be applied to a\nvariety of simulated data sets representative of those typically encountered in\nvirtual screening. Since it is more desirable in benchmarks to present the\nuncertainty of performance over a range of testing fractions, we extend the\npointwise confidence interval procedure to allow for the estimation of\nconfidence bands for these curves. We also present hypothesis test methods to\ndetermine significant differences between the curves for competing algorithms.\nWe show these methods have high power to detect significant differences at a\nrange of small fractions typically tested, while maintaining control of type I\nerror rate. These methods enable statistically rigorous comparisons of virtual\nscreening algorithms using a metric that quantifies the aspect of performance\nthat is of primary interest.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 20:08:03 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Ash", "Jeremy R.", ""], ["Hughes-Oliver", "Jacqueline M.", ""]]}, {"id": "1912.09560", "submitter": "Zhengxiao Li", "authors": "Zhengxiao Li, Jan Beirlant, Shengwang Meng", "title": "Generalizing the log-Moyal distribution and regression models for heavy\n  tailed loss data", "comments": "30 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic loss data are known to be heavy-tailed. Practitioners then need\nmodels that are able to capture both tail and modal parts of claim data. To\nthis purpose, a new parametric family of loss distributions is proposed as a\ngamma mixture of the generalized log-Moyal distribution from Bhati and Ravi\n(2018), termed the generalized log-Moyal gamma distribution (GLMGA). We discuss\nthe probabilistic characteristics of the GLMGA, and statistical estimation of\nthe parameters through maximum likelihood. While the GLMGA distribution is a\nspecial case of the GB2 distribution, we show that this simpler model is\neffective in regression modelling of large and modal loss data. A fire claim\ndata set reported in Cummins et al. (1990) and a Chinese earthquake loss data\nset are used to illustrate the applicability of the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 21:55:42 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Li", "Zhengxiao", ""], ["Beirlant", "Jan", ""], ["Meng", "Shengwang", ""]]}, {"id": "1912.09623", "submitter": "Rui Duan", "authors": "Rui Duan, Yang Ning, Yong Chen", "title": "Heterogeneity-aware and communication-efficient distributed statistical\n  inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multicenter research, individual-level data are often protected against\nsharing across sites. To overcome the barrier of data sharing, many distributed\nalgorithms, which only require sharing aggregated information, have been\ndeveloped. The existing distributed algorithms usually assume the data are\nhomogeneously distributed across sites. This assumption ignores the important\nfact that the data collected at different sites may come from various\nsub-populations and environments, which can lead to heterogeneity in the\ndistribution of the data. Ignoring the heterogeneity may lead to erroneous\nstatistical inference. In this paper, we propose distributed algorithms which\naccount for the heterogeneous distributions by allowing site-specific nuisance\nparameters. The proposed methods extend the surrogate likelihood approach to\nthe heterogeneous setting by applying a novel density ratio tilting method to\nthe efficient score function. The proposed algorithms maintain the same\ncommunication cost as the existing communication-efficient algorithms. We\nestablish a non-asymptotic risk bound for the proposed distributed estimator\nand its limiting distribution in the two-index asymptotic setting which allows\nboth sample size per site and the number of sites to go to infinity. In\naddition, we show that the asymptotic variance of the estimator attains the\nCram\\'er-Rao lower bound when the number of sites is in rate smaller than the\nsample size at each site. Finally, we use simulation studies and a real data\napplication to demonstrate the validity and feasibility of the proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 03:14:07 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 21:01:58 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Duan", "Rui", ""], ["Ning", "Yang", ""], ["Chen", "Yong", ""]]}, {"id": "1912.09664", "submitter": "Muxuan Liang", "authors": "Yi Li, Muxuan Liang, Lu Mao, Sijian Wang", "title": "Robust Estimation and Variable Selection for the Accelerated Failure\n  Time Model", "comments": "21 pages, , 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers robust modeling of the survival time for cancer\npatients. Accurate prediction can be helpful for developing therapeutic and\ncare strategies. We propose a unified Expectation-Maximization approach\ncombined with the L1-norm penalty to perform variable selection and obtain\nparameter estimation simultaneously for the accelerated failure time model with\nright-censored survival data. Our approach can be used with general loss\nfunctions, and reduces to the well-known Buckley-James method when the\nsquared-error loss is used without regularization. To mitigate the effects of\noutliers and heavy-tailed noise in the real application, we advocate the use of\nrobust loss functions under our proposed framework. Simulation studies are\nconducted to evaluate the performance of the proposed approach with different\nloss functions, and an application to an ovarian carcinoma study is provided.\nMeanwhile, we extend our approach by incorporating the group structure of\ncovariates.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 07:01:26 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Li", "Yi", ""], ["Liang", "Muxuan", ""], ["Mao", "Lu", ""], ["Wang", "Sijian", ""]]}, {"id": "1912.09755", "submitter": "Jimoh Olawale Ajadi", "authors": "Jimoh Olawale Ajadi, Zezhong Wang, and Inez Maria Zwetsloot", "title": "A Review of Dispersion Control Charts for Multivariate Individual\n  Observations", "comments": "43 pages, 2 Figures, 9 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multivariate control chart is designed to monitor process parameters of\nmultiple correlated quality characteristics. Often data on multivariate\nprocesses are collected as individual observations, i.e. as vectors one at the\ntime. Various control charts have been proposed in the literature to monitor\nthe covariance matrix of a process when individual observations are collected.\nIn this study, we review this literature; we find 30 relevant articles from the\nperiod 1987-2019. We group the articles into five categories. We observe that\nless research has been done on CUSUM, high-dimensional and non-parametric type\ncontrol charts for monitoring the process covariance matrix. We describe each\nproposed method, state their advantages, and limitations. Finally, we give\nsuggestions for future research.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 11:04:31 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Ajadi", "Jimoh Olawale", ""], ["Wang", "Zezhong", ""], ["Zwetsloot", "Inez Maria", ""]]}, {"id": "1912.09790", "submitter": "Rachael Mountain", "authors": "Rachael Mountain and Chris Sherlock", "title": "Recruitment prediction for multi-centre clinical trials based on a\n  hierarchical Poisson-gamma model: asymptotic analysis and improved intervals", "comments": "21 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse predictions of future recruitment to a multi-centre clinical trial\nbased on a maximum-likelihood fitting of a commonly used hierarchical\nPoisson-Gamma model for recruitments at individual centres. We consider the\nasymptotic accuracy of quantile predictions in the limit as the number of\nrecruitment centres grows large and find that, in an important sense, the\naccuracy of the quantiles does not improve as the number of centres increases.\nWhen predicting the number of further recruits in an additional time period,\nthe accuracy degrades as the ratio of the additional time to the census time\nincreases, whereas when predicting the amount of additional time to recruit a\nfurther $n^+_\\bullet$ patients, the accuracy degrades as the ratio of\n$n^+_\\bullet$ to the number recruited up to the census period increases. Our\nanalysis suggests an improved quantile predictor. Simulation studies verify\nthat the predicted pattern holds for typical recruitment scenarios in clinical\ntrials and verify the much improved coverage properties of prediction intervals\nobtained from our quantile predictor. In the process of extending the\napplicability of our methodology, we show that in terms of the accuracy of all\ninteger moments it is always better to approximate the sum of independent gamma\nrandom variables by a single gamma random variable matched on the first two\nmoments than by the moment-matched Gaussian available from the central limit\ntheorem.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 12:24:44 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 10:14:43 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Mountain", "Rachael", ""], ["Sherlock", "Chris", ""]]}, {"id": "1912.09883", "submitter": "Elvira Di Nardo Prof.", "authors": "Elvira Di Nardo and Rosaria Simone", "title": "A Model-Based Fuzzy Analysis of Questionnaires", "comments": "22 pages, 9 figures, 8 Tables", "journal-ref": "Statistical methods and Applications, 28:187-215 (2019)", "doi": "10.1007/s10260-018-00443-9", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dealing with veracity of data analytics, fuzzy methods are more and more\nrelying on probabilistic and statistical techniques to underpin their\napplicability. Conversely, standard statistical models usually disregard to\ntake into account the inherent fuzziness of choices and this issue is\nparticularly worthy of note in customers' satisfaction surveys, since there are\ndifferent shades of evaluations that classical statistical tools fail to catch.\nGiven these motivations, the paper introduces a model-based fuzzy analysis of\nquestionnaire with sound statistical foundation, driven by the design of a\nhybrid method that sets in between fuzzy evaluation systems and statistical\nmodelling. The proposal is advanced on the basis of \\cub mixture models to\naccount for uncertainty in ordinal data analysis and moves within the general\nframework of Intuitionistic Fuzzy Set theory to allow membership,\nnon-membership, vagueness and accuracy assessments. Particular emphasis is\ngiven to defuzzification procedures that enable uncertainty measures also at an\naggregated level. An application to a survey run at the University of Naples\nFederico II about the evaluation of Orientation Services supports the efficacy\nof the proposal.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 15:30:43 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Di Nardo", "Elvira", ""], ["Simone", "Rosaria", ""]]}, {"id": "1912.09936", "submitter": "Nima Hejazi", "authors": "Iv\\'an D\\'iaz and Nima S. Hejazi and Kara E. Rudolph and Mark J. van\n  der Laan", "title": "Non-parametric efficient causal mediation with intermediate confounders", "comments": null, "journal-ref": "Biometrika, 2020", "doi": "10.1093/biomet/asaa085", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interventional effects for mediation analysis were proposed as a solution to\nthe lack of identifiability of natural (in)direct effects in the presence of a\nmediator-outcome confounder affected by exposure. We present a theoretical and\ncomputational study of the properties of the interventional (in)direct effect\nestimands based on the efficient influence fucntion (EIF) in the non-parametric\nstatistical model. We use the EIF to develop two asymptotically optimal,\nnon-parametric estimators that leverage data-adaptive regression for estimation\nof the nuisance parameters: a one-step estimator and a targeted minimum loss\nestimator. A free and open source \\texttt{R} package implementing our proposed\nestimators is made available on GitHub. We further present results establishing\nthe conditions under which these estimators are consistent, multiply robust,\n$n^{1/2}$-consistent and efficient. We illustrate the finite-sample performance\nof the estimators and corroborate our theoretical results in a simulation\nstudy. We also demonstrate the use of the estimators in our motivating\napplication to elucidate the mechanisms behind the unintended harmful effects\nthat a housing intervention had on adolescent girls' risk behavior.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 16:54:12 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 19:51:32 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["D\u00edaz", "Iv\u00e1n", ""], ["Hejazi", "Nima S.", ""], ["Rudolph", "Kara E.", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1912.09983", "submitter": "Hunyong Cho", "authors": "Hunyong Cho, Nicholas P. Jewell, and Michael R. Kosorok", "title": "Interval censored recursive forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the interval censored recursive forests (ICRF) which is an\niterative tree ensemble method for interval censored survival data. This\nnonparametric regression estimator makes the best use of censored information\nby iteratively updating the survival estimate, and can be viewed as a\nself-consistent estimator with convergence monitored using out-of-bag samples.\nSplitting rules optimized for interval censored data are developed and\nkernel-smoothing is applied. The ICRF displays the highest prediction accuracy\namong competing nonparametric methods in most of the simulations and in an\napplied example to avalanche data. An R package icrf is available for\nimplementation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:10:35 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 05:35:05 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 17:03:37 GMT"}, {"version": "v4", "created": "Thu, 20 May 2021 14:51:00 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Cho", "Hunyong", ""], ["Jewell", "Nicholas P.", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1912.10334", "submitter": "David Puelz", "authors": "Lane F. Burgette, David Puelz, P. Richard Hahn", "title": "A Symmetric Prior for Multinomial Probit Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitted probabilities from widely used Bayesian multinomial probit models can\ndepend strongly on the choice of a base category, which is used to uniquely\nidentify the parameters of the model. This paper proposes a novel\nidentification strategy, and associated prior distribution for the model\nparameters, that renders the prior symmetric with respect to relabeling the\noutcome categories. The new prior permits an efficient Gibbs algorithm that\nsamples rank-deficient covariance matrices without resorting to\nMetropolis-Hastings updates.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 20:44:09 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 03:05:08 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2020 03:54:19 GMT"}, {"version": "v4", "created": "Sun, 17 May 2020 22:56:10 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Burgette", "Lane F.", ""], ["Puelz", "David", ""], ["Hahn", "P. Richard", ""]]}, {"id": "1912.10337", "submitter": "Mingyuan Zhou", "authors": "Dandan Guo, Bo Chen, Ruiying Lu, Mingyuan Zhou", "title": "Recurrent Hierarchical Topic-Guided RNN for Language Generation", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To simultaneously capture syntax and global semantics from a text corpus, we\npropose a new larger-context recurrent neural network (RNN) based language\nmodel, which extracts recurrent hierarchical semantic structure via a dynamic\ndeep topic model to guide natural language generation. Moving beyond a\nconventional RNN-based language model that ignores long-range word dependencies\nand sentence order, the proposed model captures not only intra-sentence word\ndependencies, but also temporal transitions between sentences and\ninter-sentence topic dependencies. For inference, we develop a hybrid of\nstochastic-gradient Markov chain Monte Carlo and recurrent autoencoding\nvariational Bayes. Experimental results on a variety of real-world text corpora\ndemonstrate that the proposed model not only outperforms larger-context\nRNN-based language models, but also learns interpretable recurrent multilayer\ntopics and generates diverse sentences and paragraphs that are syntactically\ncorrect and semantically coherent.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 21:11:35 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 22:22:58 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Guo", "Dandan", ""], ["Chen", "Bo", ""], ["Lu", "Ruiying", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1912.10472", "submitter": "Tzviel Frostig", "authors": "Tzviel Frostig and Yoav Benjamini", "title": "Testing the equality of multivariate means when $p>n$ by combining the\n  Hoteling and Simes tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method of testing the shift between mean vectors of two\nmultivariate Gaussian random variables in a high-dimensional setting\nincorporating the possible dependency and allowing $p > n$. This method is a\ncombination of two well-known tests: the Hotelling test and the Simes test. The\ntests are integrated by sampling several dimensions at each iteration, testing\neach using the Hotelling test, and combining their results using the Simes\ntest. We prove that this procedure is valid asymptotically. This procedure can\nbe extended to handle non-equal covariance matrices by plugging in the\nappropriate extension of the Hotelling test. Using a simulation study, we show\nthat the proposed test is advantageous over state-of-the-art tests in many\nscenarios and robust to violation of the Gaussian assumption.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 16:33:48 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Frostig", "Tzviel", ""], ["Benjamini", "Yoav", ""]]}, {"id": "1912.10492", "submitter": "Jakob Raymaekers", "authors": "Jakob Raymaekers and Ruben H. Zamar", "title": "Pooled variable scaling for cluster analysis", "comments": "29 pages, 32 figures", "journal-ref": null, "doi": "10.1093/bioinformatics/btaa243", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for scaling prior to cluster analysis based on the\nconcept of pooled variance. Unlike available scaling procedures such as the\nstandard deviation and the range, our proposed scale avoids dampening the\nbeneficial effect of informative clustering variables. We confirm through an\nextensive simulation study and applications to well known real data examples\nthat the proposed scaling method is safe and generally useful. Finally, we use\nour approach to cluster a high dimensional genomic dataset consisting of gene\nexpression data for several specimens of breast cancer cells tissue.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 17:48:58 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 21:26:11 GMT"}, {"version": "v3", "created": "Sat, 25 Jul 2020 22:01:38 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Raymaekers", "Jakob", ""], ["Zamar", "Ruben H.", ""]]}, {"id": "1912.10496", "submitter": "Leah F. South", "authors": "Leah F. South, Chris Nemeth and Chris J. Oates", "title": "Discussion of \"Unbiased Markov chain Monte Carlo with couplings\" by\n  Pierre E. Jacob, John O'Leary and Yves F. Atchad\\'e", "comments": "This comment includes an appendix which was not included in the\n  printed JRSS B discussion. Version 2 has been shorted slightly to meet word\n  limit requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a contribution for the discussion on \"Unbiased Markov chain Monte\nCarlo with couplings\" by Pierre E. Jacob, John O'Leary and Yves F. Atchad\\'e to\nappear in the Journal of the Royal Statistical Society Series B.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 17:59:23 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 11:17:49 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["South", "Leah F.", ""], ["Nemeth", "Chris", ""], ["Oates", "Chris J.", ""]]}, {"id": "1912.10536", "submitter": "Ruocheng Guo", "authors": "Ruocheng Guo and Jundong Li and Huan Liu", "title": "Counterfactual Evaluation of Treatment Assignment Functions with\n  Networked Observational Data", "comments": "10 pages, 5 figures, Accepted to SDM'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual evaluation of novel treatment assignment functions (e.g.,\nadvertising algorithms and recommender systems) is one of the most crucial\ncausal inference problems for practitioners. Traditionally, randomized\ncontrolled trials (A/B tests) are performed to evaluate treatment assignment\nfunctions. However, such trials can be time-consuming, expensive, and even\nunethical in some cases. Therefore, offline counterfactual evaluation of\ntreatment assignment functions becomes a pressing issue because a massive\namount of observational data is available in today's big data era.\nCounterfactual evaluation requires handling the hidden confounders -- the\nunmeasured features which causally influence both the treatment assignment and\nthe outcome. To deal with the hidden confounders, most of the existing methods\nrely on the assumption of no hidden confounders. However, this assumption can\nbe untenable in the context of massive observational data. When such data comes\nwith network information, the later can be potentially useful to correct hidden\nconfounding bias. As such, we first formulate a novel problem, counterfactual\nevaluation of treatment assignment functions with networked observational data.\nThen, we investigate the following research questions: How can we utilize\nnetwork information in counterfactual evaluation? Can network information\nimprove the estimates in counterfactual evaluation? Toward answering these\nquestions, first, we propose a novel framework, \\emph{Counterfactual Network\nEvaluator} (CONE), which (1) learns partial representations of latent\nconfounders under the supervision of observed treatments and outcomes; and (2)\ncombines them for counterfactual evaluation. Then through extensive\nexperiments, we corroborate the effectiveness of CONE. The results imply that\nincorporating network information mitigates hidden confounding bias in\ncounterfactual evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 21:13:10 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Guo", "Ruocheng", ""], ["Li", "Jundong", ""], ["Liu", "Huan", ""]]}, {"id": "1912.10602", "submitter": "Hiroshi Haramoto", "authors": "Hiroshi Haramoto", "title": "Study on upper limit of sample sizes for a two-level test in NIST\n  SP800-22", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NIST SP800-22 is one of the most widely used statistical testing tools for\npseudorandom number generators (PRNGs). This tool consists of 15 tests\n(one-level tests) and two additional tests (two-level tests). Each one-level\ntest provides one or more $p$-values. The two-level tests measure the\nuniformity of the obtained $p$-values for a fixed one-level test. One of the\ntwo-level tests categorizes the $p$-values into ten intervals of equal length,\nand apply a chi-squared goodness-of-fit test. This two-level test is often more\npowerful than one-level tests, but sometimes it rejects even good PRNGs when\nthe sample size at the second level is too large, since it detects\napproximation errors in the computation of $p$-values. In this paper, we\npropose a practical upper limit of the sample size in this two-level test, for\neach of six tests appeared in SP800-22. These upper limits are derived by the\nchi-squared discrepancy between the distribution of the approximated $p$-values\nand the uniform distribution $U(0, 1)$. We also computed a \"risky\" sample size\nat the second level for each one-level test. Our experiments show that the\ntwo-level test with the proposed upper limit gives appropriate results, while\nusing the risky size often rejects even good PRNGs. We also propose another\nimprovement: to use the exact probability for the ten categories in the\ncomputation of goodness-of-fit at the two-level test. This allows us to\nincrease the sample size at the second level, and would make the test more\nsensitive than the NIST's recommending usage.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 03:33:06 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 14:10:44 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 15:07:23 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Haramoto", "Hiroshi", ""]]}, {"id": "1912.10610", "submitter": "Panos Toulis", "authors": "Azeem Shaikh and Panos Toulis", "title": "Randomization Tests in Observational Studies with Staggered Adoption of\n  Treatment", "comments": "30 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of inference in observational studies with\ntime-varying adoption of treatment. In addition to an unconfoundedness\nassumption that the potential outcomes are independent of the times at which\nunits adopt treatment conditional on the units' observed characteristics, our\nanalysis assumes that the time at which each unit adopts treatment follows a\nCox proportional hazards model. This assumption permits the time at which each\nunit adopts treatment to depend on the observed characteristics of the unit,\nbut imposes the restriction that the probability of multiple units adopting\ntreatment at the same time is zero. In this context, we study Fisher-style\nrandomization tests of a null hypothesis that specifies that there is no\ntreatment effect for all units and all time periods in a distributional sense.\nWe first show that an infeasible test that treats the parameters of the Cox\nmodel as known has rejection probability no greater than the nominal level in\nfinite samples. We then establish that the feasible test that replaces these\nparameters with consistent estimators has limiting rejection probability no\ngreater than the nominal level. In a simulation study, we examine the practical\nrelevance of our theoretical results, including robustness to misspecification\nof the model for the time at which each unit adopts treatment. Finally, we\nprovide an empirical application of our methodology using the synthetic\ncontrol-based test statistic and tobacco legislation data found in Abadie et.\nal. (2010).\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 03:53:54 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 02:38:51 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Shaikh", "Azeem", ""], ["Toulis", "Panos", ""]]}, {"id": "1912.10742", "submitter": "Mathieu Carri\\`ere", "authors": "Mathieu Carri\\`ere and Bertrand Michel", "title": "Statistical analysis of Mapper for stochastic and multivariate filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reeb spaces, as well as their discretized versions called Mappers, are common\ndescriptors used in Topological Data Analysis, with plenty of applications in\nvarious fields of science, such as computational biology and data\nvisualization, among others. The stability and quantification of the rate of\nconvergence of the Mapper to the Reeb space has been studied a lot in recent\nworks [BBMW19, CO17, CMO18, MW16], focusing on the case where a scalar-valued\nfilter is used for the computation of Mapper. On the other hand, much less is\nknown in the multivariate case, when the codomain of the filter is\n$\\mathbb{R}^p$, and in the general case, when it is a general metric space $(Z,\nd_Z)$, instead of $\\mathbb{R}$. The few results that are available in this\nsetting [DMW17, MW16] can only handle continuous topological spaces and cannot\nbe used as is for finite metric spaces representing data, such as point clouds\nand distance matrices. In this article, we introduce a slight modification of\nthe usual Mapper construction and we give risk bounds for estimating the Reeb\nspace using this estimator. Our approach applies in particular to the setting\nwhere the filter function used to compute Mapper is also estimated from data,\nsuch as the eigenfunctions of PCA. Our results are given with respect to the\nGromov-Hausdorff distance, computed with specific filter-based pseudometrics\nfor Mappers and Reeb spaces defined in [DMW17]. We finally provide applications\nof this setting in statistics and machine learning for different kinds of\ntarget filters, as well as numerical experiments that demonstrate the relevance\nof our approach\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 11:32:07 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 11:31:02 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Carri\u00e8re", "Mathieu", ""], ["Michel", "Bertrand", ""]]}, {"id": "1912.10829", "submitter": "Chainarong Amornbunchornvej", "authors": "Chainarong Amornbunchornvej, Elena Zheleva, and Tanya Y. Berger-Wolf", "title": "Variable-lag Granger Causality for Time Series Analysis", "comments": "This paper will be appeared in the proceeding of 2019 IEEE\n  International Conference on Data Science and Advanced Analytics (DSAA). The R\n  package is available at https://github.com/DarkEyes/VLTimeSeriesCausality", "journal-ref": "Proceedings of 2019 IEEE International Conference on Data Science\n  and Advanced Analytics (DSAA)", "doi": "10.1109/DSAA.2019.00016", "report-no": null, "categories": "cs.LG econ.EM q-bio.QM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger causality is a fundamental technique for causal inference in time\nseries data, commonly used in the social and biological sciences. Typical\noperationalizations of Granger causality make a strong assumption that every\ntime point of the effect time series is influenced by a combination of other\ntime series with a fixed time delay. However, the assumption of the fixed time\ndelay does not hold in many applications, such as collective behavior,\nfinancial markets, and many natural phenomena. To address this issue, we\ndevelop variable-lag Granger causality, a generalization of Granger causality\nthat relaxes the assumption of the fixed time delay and allows causes to\ninfluence effects with arbitrary time delays. In addition, we propose a method\nfor inferring variable-lag Granger causality relations. We demonstrate our\napproach on an application for studying coordinated collective behavior and\nshow that it performs better than several existing methods in both simulated\nand real-world datasets. Our approach can be applied in any domain of time\nseries analysis.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 23:38:48 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Amornbunchornvej", "Chainarong", ""], ["Zheleva", "Elena", ""], ["Berger-Wolf", "Tanya Y.", ""]]}, {"id": "1912.10842", "submitter": "Mohammad Bhuiyan", "authors": "Mohammad Alfrad Nobel Bhuiyan, Heidi Sucharew, Rhonda Szczesniak,\n  Marepalli Rao, Jessica Woo, Jane Khoury, Md Monir Hossain", "title": "Bayesian Shape Invariant Model for Latent Growth Curve with\n  Time-Invariant Covariates", "comments": null, "journal-ref": null, "doi": "10.31031/OABB.2021.03.000559", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the attention-deficit hyperactivity disorder (ADHD) study, children are\nprescribed different stimulant medications. The height measurements are\nrecorded longitudinally along with the medication time. Differences among the\npatients are captured by the parameters suggested the Superimposition by\nTranslation and Rotation (SITAR) model using three subject-specific parameters\nto estimate their deviation from the mean growth curve. In this paper, we\ngeneralize the SITAR model in a Bayesian way with time-invariant covariates.\nThe time-invariant model allows us to predict latent growth factors. Since\npatients suffer from a common disease, they usually exhibit a similar pattern,\nand it is natural to build a nonlinear model that is shaped invariant. The\nmodel is semi-parametric, where the population time curve is modeled with a\nnatural cubic spline. The original shape invariant growth curve model,\nmotivated by epidemiological research on the evolution of pubertal heights over\ntime, fits the underlying shape function for height over age and estimates\nsubject-specific deviations from this curve in terms of size, tempo, and\nvelocity using maximum likelihood. The usefulness of the model is illustrated\nin the attention deficit hyperactivity disorder (ADHD) study. Further, we\ndemonstrated the effect of stimulant medications on pubertal growth by gender.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 13:52:56 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 13:22:45 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Bhuiyan", "Mohammad Alfrad Nobel", ""], ["Sucharew", "Heidi", ""], ["Szczesniak", "Rhonda", ""], ["Rao", "Marepalli", ""], ["Woo", "Jessica", ""], ["Khoury", "Jane", ""], ["Hossain", "Md Monir", ""]]}, {"id": "1912.10981", "submitter": "Virgilio Gomez-Rubio", "authors": "Virgilio G\\'omez-Rubio and Michela Cameletti and Marta Blangiardo", "title": "Missing data analysis and imputation via latent Gaussian Markov random\n  fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we recast the problem of missing values in the covariates of a\nregression model as a latent Gaussian Markov random field (GMRF) model in a\nfully Bayesian framework. Our proposed approach is based on the definition of\nthe covariate imputation sub-model as a latent effect with a GMRF structure. We\nshow how this formulation works for continuous covariates and provide some\ninsight on how this could be extended to categorical covariates.\n  The resulting Bayesian hierarchical model naturally fits within the\nintegrated nested Laplace approximation (INLA) framework, which we use for\nmodel fitting. Hence, our work fills an important gap in the INLA methodology\nas it allows to treat models with missing values in the covariates.\n  As in any other fully Bayesian framework, by relying on INLA for model\nfitting it is possible to formulate a joint model for the data, the imputed\ncovariates and their missingness mechanism. In this way, we are able to tackle\nthe more general problem of assessing the missingness mechanism by conducting a\nsensitivity analysis on the different alternatives to model the non-observed\ncovariates.\n  Finally, we illustrate the proposed approach with two examples on modeling\nhealth risk factors and disease mapping. Here, we rely on two different\nimputation mechanisms based on a typical multiple linear regression and a\nspatial model, respectively. Given the speed of model fitting with INLA we are\nable to fit joint models in a short time, and to easily conduct sensitivity\nanalyses.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 17:16:28 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["G\u00f3mez-Rubio", "Virgilio", ""], ["Cameletti", "Michela", ""], ["Blangiardo", "Marta", ""]]}, {"id": "1912.11218", "submitter": "Yuling Yao", "authors": "Yuling Yao", "title": "Bayesian Aggregation", "comments": "minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general challenge in statistics is prediction in the presence of multiple\ncandidate models or learning algorithms. Model aggregation tries to combine all\npredictive distributions from individual models, which is more stable and\nflexible than single model selection. In this article we describe when and how\nto aggregate models under the lens of Bayesian decision theory. Among two\nwidely used methods, Bayesian model averaging (BMA) and Bayesian stacking, we\ncompare their predictive performance, and review their theoretical optimality,\nprobabilistic interpretation, practical implementation, and extensions in\ncomplex models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 05:54:30 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 21:29:07 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yao", "Yuling", ""]]}, {"id": "1912.11335", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen", "title": "A Continuous-Time Dynamic Choice Measurement Model for Problem-Solving\n  Process Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problem solving has been recognized as a central skill that today's students\nneed to thrive and shape their world. As a result, the measurement of\nproblem-solving competency has received much attention in education in recent\nyears. A popular tool for the measurement of problem solving is simulated\ninteractive tasks, which require students to uncover some of the information\nneeded to solve the problem through interactions with a computer-simulated\nenvironment. A computer log file records a student's problem-solving process in\ndetails, including his/her actions and the time stamps of these actions. It\nthus provides rich information for the measurement of students' problem-solving\ncompetency. On the other hand, extracting useful information from log files is\na challenging task, due to its complex data structure. In this paper, we show\nhow log file process data can be viewed as a marked point process, based on\nwhich we propose a continuous-time dynamic choice model. The proposed model can\nserve as a measurement model for scaling students along the latent traits of\nproblem-solving competency and action speed, based on data from one or multiple\ntasks. A real data example is given based on data from Program for\nInternational Student Assessment 2012.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:26:27 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 10:06:34 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chen", "Yunxiao", ""]]}, {"id": "1912.11436", "submitter": "Sivaraman Balakrishnan", "authors": "Larry Wasserman and Aaditya Ramdas and Sivaraman Balakrishnan", "title": "Universal Inference", "comments": "To appear in the Proceedings of the National Academy of Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general method for constructing hypothesis tests and confidence\nsets that have finite sample guarantees without regularity conditions. We refer\nto such procedures as \"universal.\" The method is very simple and is based on a\nmodified version of the usual likelihood ratio statistic, that we call \"the\nsplit likelihood ratio test\" (split LRT). The method is especially appealing\nfor irregular statistical models. Canonical examples include mixture models and\nmodels that arise in shape-constrained inference. %mixture models and\nshape-constrained models are just two examples. Constructing tests and\nconfidence sets for such models is notoriously difficult. Typical inference\nmethods, like the likelihood ratio test, are not useful in these cases because\nthey have intractable limiting distributions. In contrast, the method we\nsuggest works for any parametric model and also for some nonparametric models.\nThe split LRT can also be used with profile likelihoods to deal with nuisance\nparameters, and it can also be run sequentially to yield anytime-valid\n$p$-values and confidence sequences.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 16:52:08 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 18:48:34 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 22:38:36 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Wasserman", "Larry", ""], ["Ramdas", "Aaditya", ""], ["Balakrishnan", "Sivaraman", ""]]}, {"id": "1912.11466", "submitter": "Mohammad Bhuiyan", "authors": "Mohammad Alfrad Nobel Bhuiyan, Michael J Wathen, M Bhaskara Rao", "title": "Power Comparisons in 2x2 Contingency Tables: Odds Ratio versus Pearson\n  Correlation versus Canonical Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  It is an important inferential problem to test no association between two\nbinary variables based on data. Tests based on the sample odds ratio are\ncommonly used. We bring in a competing test based on the Pearson correlation\ncoefficient. In particular, the Odds ratio does not extend to higher order\ncontingency tables, whereas Pearson correlation does. It is important to\nunderstand how Pearson correlation stacks against the odds ratio in 2x2 tables.\nAnother measure of association is the canonical correlation. In this paper, we\nexamine how competitive Pearson correlation is vis-\\`a-vis odds ratio in terms\nof power in the binary context, contrasting further with both the Wald Z and\nRao Score tests. We generated an extensive collection of joint distributions of\nthe binary variables and estimated the power of the tests under each joint\nalternative distribution based on random samples. The consensus is that none of\nthe tests dominates the other.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 18:42:43 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Bhuiyan", "Mohammad Alfrad Nobel", ""], ["Wathen", "Michael J", ""], ["Rao", "M Bhaskara", ""]]}, {"id": "1912.11542", "submitter": "Garritt Page", "authors": "Garritt L. Page, Fernando A. Quintana, David B. Dahl", "title": "Dependent Random Partition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of modeling a dependent sequence of random partitions.\nIt is well-known that a random measure in Bayesian nonparametrics induces a\ndistribution over random partitions. The community has therefore assumed that\nthe best approach to obtain a dependent sequence of random partitions is\nthrough modeling dependent random measures. We argue that this approach is\nproblematic and show that the random partition model induced by dependent\nBayesian nonparametric priors exhibit counter-intuitive dependence among\npartitions even though the dependence for the sequence of random probability\nmeasures is intuitive. Because of this, we advocate instead to model the\nsequence of random partitions directly when clustering is of principal\ninterest. To this end, we develop a class of dependent random partition models\nthat explicitly models dependence in a sequence of partitions. We derive\nconditional and marginal properties of the joint partition model and devise\ncomputational strategies when employing the method in Bayesian modeling. In the\ncase of temporal dependence, we demonstrate through simulation how the\nmethodology produces partitions that evolve gently and naturally over time. We\nfurther illustrate the utility of the method by applying it to an environmental\ndata set that exhibits spatio-temporal dependence.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 21:20:35 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 01:41:36 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Page", "Garritt L.", ""], ["Quintana", "Fernando A.", ""], ["Dahl", "David B.", ""]]}, {"id": "1912.11703", "submitter": "Yan Liu", "authors": "Minggen Lu, Yan Liu, Chin-Shang Li, Jianguo Sun", "title": "An efficient penalized estimation approach for a semi-parametric linear\n  transformation model with interval-censored data", "comments": "20 pages, 5 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider efficient estimation of flexible transformation models with\ninterval-censored data. To reduce the dimension of semi-parametric models, the\nunknown monotone transformation function is approximated via monotone splines.\nA penalization technique is used to provide more computationally efficient\nestimation of all parameters. To accomplish model fitting, a computationally\nefficient nested iterative expectation-maximization (EM) based algorithm is\ndeveloped for estimation, and an easily implemented variance-covariance\napproach is proposed for inference on regression parameters. Theoretically, we\nshow that the estimator of the transformation function achieves the optimal\nrate of convergence and the estimators of regression parameters are\nasymptotically normal and efficient. The penalized procedure is assessed\nthrough extensive numerical experiments and further illustrated via two real\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 18:11:36 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lu", "Minggen", ""], ["Liu", "Yan", ""], ["Li", "Chin-Shang", ""], ["Sun", "Jianguo", ""]]}, {"id": "1912.11736", "submitter": "Arthur Charpentier", "authors": "Arthur Charpentier and Emmanuel Flachaire", "title": "Pareto models for risk management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Pareto model is very popular in risk management, since simple analytical\nformulas can be derived for financial downside risk measures (Value-at-Risk,\nExpected Shortfall) or reinsurance premiums and related quantities (Large Claim\nIndex, Return Period). Nevertheless, in practice, distributions are (strictly)\nPareto only in the tails, above (possible very) large threshold. Therefore, it\ncould be interesting to take into account second order behavior to provide a\nbetter fit. In this article, we present how to go from a strict Pareto model to\nPareto-type distributions. We discuss inference, and derive formulas for\nvarious measures and indices, and finally provide applications on insurance\nlosses and financial risks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 00:56:52 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Charpentier", "Arthur", ""], ["Flachaire", "Emmanuel", ""]]}, {"id": "1912.11740", "submitter": "Michael Byrd", "authors": "Michael Byrd and Monnie McGee", "title": "A Simple Correction Procedure for High-Dimensional Generalized Linear\n  Models with Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high-dimensional generalized linear models when the covariates\nare contaminated by measurement error. Estimates from errors-in-variables\nregression models are well-known to be biased in traditional low-dimensional\nsettings if the error is unincorporated. Such models have recently become of\ninterest when regularizing penalties are added to the estimation procedure.\nUnfortunately, correcting for the mismeasurements can add undue computational\ndifficulties onto the optimization, which a new tool set for practitioners to\nsuccessfully use the models. We investigate a general procedure that utilizes\nthe recently proposed Imputation-Regularized Optimization algorithm for\nhigh-dimensional errors-in-variables models, which we implement for continuous,\nbinary, and count response type. Crucially, our method allows for off-the-shelf\nlinear regression methods to be employed in the presence of contaminated\ncovariates. We apply our correction to gene microarray data, and illustrate\nthat it results in a great reduction in the number of false positives whilst\nstill retaining most true positives.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 01:13:31 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 21:49:47 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Byrd", "Michael", ""], ["McGee", "Monnie", ""]]}, {"id": "1912.11798", "submitter": "Morteza Raeisi", "authors": "Morteza Raeisi and Gholamhossein Yari", "title": "A further result on the aging properties of an extended additive hazard\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The passing of time is an important factor for covariates in the additive and\nproportional hazard models. According to this idea, the extended additive\nhazard model (EAHM) is introduced by considering the time-varying effects of\ncovariates and is investigated several properties of this model related to\nreliability analysis. In this paper, we obtain a further result for the EAHM\nwith respect to the aging properties.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 08:06:58 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Raeisi", "Morteza", ""], ["Yari", "Gholamhossein", ""]]}, {"id": "1912.11827", "submitter": "David Ginsbourger", "authors": "Lea Friedli, David Ginsbourger, Jonas Bhend", "title": "Area-covering postprocessing of ensemble precipitation forecasts using\n  topographical and seasonal conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic weather forecasts from ensemble systems require statistical\npostprocessing to yield calibrated and sharp predictive distributions. This\npaper presents an area-covering postprocessing method for ensemble\nprecipitation predictions. We rely on the ensemble model output statistics\n(EMOS) approach, which generates probabilistic forecasts with a parametric\ndistribution whose parameters depend on (statistics of) the ensemble\nprediction. A case study with daily precipitation predictions across\nSwitzerland highlights that postprocessing at observation locations indeed\nimproves high-resolution ensemble forecasts, with 4.5% CRPS reduction on\naverage in the case of a lead time of 1 day. Our main aim is to achieve such an\nimprovement without binding the model to stations, by leveraging topographical\ncovariates. Specifically, regression coefficients are estimated by weighting\nthe training data in relation to the topographical similarity between their\nstation of origin and the prediction location. In our case study, this approach\nis found to reproduce the performance of the local model without using local\nhistorical data for calibration. We further identify that one key difficulty is\nthat postprocessing often degrades the performance of the ensemble forecast\nduring summer and early autumn. To mitigate, we additionally estimate on the\ntraining set whether postprocessing at a specific location is expected to\nimprove the prediction. If not, the direct model output is used. This extension\nreduces the CRPS of the topographical model by up to another 1.7% on average at\nthe price of a slight degradation in calibration. In this case, the highest\nimprovement is achieved for a lead time of 4 days.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 10:24:32 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 13:16:38 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 12:33:04 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Friedli", "Lea", ""], ["Ginsbourger", "David", ""], ["Bhend", "Jonas", ""]]}, {"id": "1912.11833", "submitter": "Yuxiao Li", "authors": "Yuxiao Li and Ying Sun", "title": "A Multi-Site Stochastic Weather Generator for High-Frequency\n  Precipitation Using Censored Skew-Symmetric Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic weather generators (SWGs) are digital twins of complex weather\nprocesses and widely used in agriculture and urban design. Due to improved\nmeasuring instruments, an accurate SWG for high-frequency precipitation is now\npossible. However, high-frequency precipitation data are more zero-inflated,\nskewed, and heavy-tailed than common (hourly or daily) precipitation data.\nTherefore, classical methods that either model precipitation occurrence\nindependently of their intensity or assume that the precipitation follows a\ncensored meta-Gaussian process may not be appropriate. In this work, we propose\na novel multi-site precipitation generator that drives both occurrence and\nintensity by a censored non-Gaussian vector autoregression model with\nskew-symmetric dynamics. The proposed SWG is advantageous in modeling skewed\nand heavy-tailed data with direct physical and statistical interpretations. We\napply the proposed model to 30-second precipitation based on the data obtained\nfrom a dense gauge network in Lausanne, Switzerland. In addition to reproducing\nthe high-frequency precipitation, the model can provide accurate predictions as\nthe long short-term memory (LSTM) network but with uncertainties and more\ninterpretable results.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 11:02:59 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 13:08:59 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Li", "Yuxiao", ""], ["Sun", "Ying", ""]]}, {"id": "1912.11848", "submitter": "Andreas Kryger Jensen", "authors": "Andreas Kryger Jensen and Claus Thorn Ekstr{\\o}m", "title": "Quantifying the Trendiness of Trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News media often report that the trend of some public health outcome has\nchanged. These statements are frequently based on longitudinal data, and the\nchange in trend is typically found to have occurred at the most recent data\ncollection time point - if no change had occurred the story is less likely to\nbe reported. Such claims may potentially influence public health decisions on a\nnational level.\n  We propose two measures for quantifying the trendiness of trends. Assuming\nthat reality evolves in continuous time we define what constitutes a trend and\na change in trend, and introduce a probabilistic Trend Direction Index. This\nindex has the interpretation of the probability that a latent characteristic\nhas changed monotonicity at any given time conditional on observed data. We\nalso define an index of Expected Trend Instability quantifying the expected\nnumber of changes in trend on an interval.\n  Using a latent Gaussian Process model we show how the Trend Direction Index\nand the Expected Trend Instability can be estimated in a Bayesian framework and\nuse the methods to analyze the proportion of smokers in Denmark during the last\n20 years, and the development of new COVID-19 cases in Italy from February 24th\nonwards.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 12:05:03 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 20:09:18 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Jensen", "Andreas Kryger", ""], ["Ekstr\u00f8m", "Claus Thorn", ""]]}, {"id": "1912.11928", "submitter": "Subha Maity", "authors": "Subha Maity, Yuekai Sun, and Moulinath Banerjee", "title": "Communication-Efficient Integrative Regression in High-Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of meta-analysis in high-dimensional settings in which\nthe data sources we wish to integrate are similar but non-identical. To borrow\nstrength across such heterogeneous data sources, we introduce a global\nparameter that addresses several identification issues. We also propose a\none-shot estimator of the global parameter that preserves the anonymity of the\ndata sources and converges at a rate that depends on the size of the combined\ndataset. Finally, we demonstrate the benefits of our approach on a large-scale\ndrug treatment dataset involving several different cancer cell lines.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 20:30:57 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Maity", "Subha", ""], ["Sun", "Yuekai", ""], ["Banerjee", "Moulinath", ""]]}, {"id": "1912.12049", "submitter": "Luca Scrucca", "authors": "Luca Scrucca and Alessio Serafini", "title": "Projection pursuit based on Gaussian mixtures and evolutionary\n  algorithms", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, 2019, 28:4,\n  847-860", "doi": "10.1080/10618600.2019.1598871", "report-no": null, "categories": "stat.ML cs.LG cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a projection pursuit (PP) algorithm based on Gaussian mixture\nmodels (GMMs). The negentropy obtained from a multivariate density estimated by\nGMMs is adopted as the PP index to be maximised. For a fixed dimension of the\nprojection subspace, the GMM-based density estimation is projected onto that\nsubspace, where an approximation of the negentropy for Gaussian mixtures is\ncomputed. Then, Genetic Algorithms (GAs) are used to find the optimal,\northogonal projection basis by maximising the former approximation. We show\nthat this semi-parametric approach to PP is flexible and allows highly\ninformative structures to be detected, by projecting multivariate datasets onto\na subspace, where the data can be feasibly visualised. The performance of the\nproposed approach is shown on both artificial and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 10:25:41 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Scrucca", "Luca", ""], ["Serafini", "Alessio", ""]]}, {"id": "1912.12150", "submitter": "Cencheng Shen", "authors": "Cencheng Shen, Sambit Panda, Joshua T. Vogelstein", "title": "The Chi-Square Test of Distance Correlation", "comments": "21 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance correlation has gained much recent attention in the data science\ncommunity: the sample statistic is straightforward to compute and\nasymptotically equals zero if and only if independence, making it an ideal\nchoice to discover any type of dependency structure given sufficient sample\nsize. One major bottleneck is the testing process: because the null\ndistribution of distance correlation depends on the underlying random variables\nand metric choice, it typically requires a permutation test to estimate the\nnull and compute the p-value, which is very costly for large amount of data. To\novercome the difficulty, in this paper we propose a chi-square test for\ndistance correlation. Method-wise, the chi-square test is non-parametric,\nextremely fast, and applicable to bias-corrected distance correlation using any\nstrong negative type metric or characteristic kernel. The test exhibits a\nsimilar testing power as the standard permutation test, and can be utilized for\nK-sample and partial testing. Theory-wise, we show that the underlying\nchi-square distribution well approximates and dominates the limiting null\ndistribution in upper tail, prove the chi-square test can be valid and\nuniversally consistent for testing independence, and establish a testing power\ninequality with respect to the permutation test.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 15:16:40 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 15:08:41 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 21:53:47 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2020 21:35:39 GMT"}, {"version": "v5", "created": "Fri, 14 May 2021 18:09:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Shen", "Cencheng", ""], ["Panda", "Sambit", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1912.12353", "submitter": "Kevin He", "authors": "Kevin He, Ji Zhu, Jian Kang, Yi Li", "title": "Minorization-Maximization-based Steepest Ascent for Large-scale Survival\n  Analysis with Time-Varying Effects: Application to the National Kidney\n  Transplant Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time-varying effects model is a flexible and powerful tool for modeling\nthe dynamic changes of covariate effects. However, in survival analysis, its\ncomputational burden increases quickly as the number of sample sizes or\npredictors grows. Traditional methods that perform well for moderate sample\nsizes and low-dimensional data do not scale to massive data. Analysis of\nnational kidney transplant data with a massive sample size and large number of\npredictors defy any existing statistical methods and software. In view of these\ndifficulties, we propose a Minorization-Maximization-based steepest ascent\nprocedure for estimating the time-varying effects. Leveraging the block\nstructure formed by the basis expansions, the proposed procedure iteratively\nupdates the optimal block-wise direction along which the approximate increase\nin the log-partial likelihood is maximized. The resulting estimates ensure the\nascent property and serve as refinements of the previous step. The performance\nof the proposed method is examined by simulations and applications to the\nanalysis of national kidney transplant data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 22:07:06 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["He", "Kevin", ""], ["Zhu", "Ji", ""], ["Kang", "Jian", ""], ["Li", "Yi", ""]]}, {"id": "1912.12446", "submitter": "Peter Rousseeuw", "authors": "Jakob Raymaekers and Peter J. Rousseeuw", "title": "Handling cellwise outliers by sparse regression and robust covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-analytic method for detecting cellwise outliers. Given a\nrobust covariance matrix, outlying cells (entries) in a row are found by the\ncellHandler technique which combines lasso regression with a stepwise\napplication of constructed cutoff values. The penalty term of the lasso has a\nphysical interpretation as the total distance that suspicious cells need to\nmove in order to bring their row into the fold. For estimating a cellwise\nrobust covariance matrix we construct a detection-imputation method which\nalternates between flagging outlying cells and updating the covariance matrix\nas in the EM algorithm. The proposed methods are illustrated by simulations and\non real data about volatile organic compounds in children.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 11:48:39 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 11:16:49 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Raymaekers", "Jakob", ""], ["Rousseeuw", "Peter J.", ""]]}, {"id": "1912.12524", "submitter": "Marina Riabiz Ms.", "authors": "Simon Godsill, Marina Riabiz, Ioannis Kontoyiannis", "title": "The L\\'evy State Space Model", "comments": "V1:8 pages, 4 figures. V2: References updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new class of state space models based on\nshot-noise simulation representations of non-Gaussian L\\'evy-driven linear\nsystems, represented as stochastic differential equations. In particular a\nconditionally Gaussian version of the models is proposed that is able to\ncapture heavy-tailed non-Gaussianity while retaining tractability for inference\nprocedures. We focus on a canonical class of such processes, the\n$\\alpha$-stable L\\'evy processes, which retain important properties such as\nself-similarity and heavy-tails, while emphasizing that broader classes of\nnon-Gaussian L\\'evy processes may be handled by similar methodology. An\nimportant feature is that we are able to marginalise both the skewness and the\nscale parameters of these challenging models from posterior probability\ndistributions. The models are posed in continuous time and so are able to deal\nwith irregular data arrival times. Example modelling and inference procedures\nare provided using Rao-Blackwellised sequential Monte Carlo applied to a\ntwo-dimensional Langevin model, and this is tested on real exchange rate data.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 21:32:54 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 12:37:03 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Godsill", "Simon", ""], ["Riabiz", "Marina", ""], ["Kontoyiannis", "Ioannis", ""]]}, {"id": "1912.12550", "submitter": "Abhijit Mandal", "authors": "Abhijit Mandal and Samiran Ghosh", "title": "Robust Variable Selection Criteria for the Penalized Regression", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust variable selection procedure using a divergence based\nM-estimator combined with a penalty function. It produces robust estimates of\nthe regression parameters and simultaneously selects the important explanatory\nvariables. An efficient algorithm based on the quadratic approximation of the\nestimating equation is constructed. The asymptotic distribution and the\ninfluence function of the regression coefficients are derived. The widely used\nmodel selection procedures based on the Mallows's $C_p$ statistic and Akaike\ninformation criterion (AIC) often show very poor performance in the presence of\nheavy-tailed error or outliers. For this purpose, we introduce robust versions\nof these information criteria based on our proposed method. The simulation\nstudies show that the robust variable selection technique outperforms the\nclassical likelihood-based techniques in the presence of outliers. The\nperformance of the proposed method is also explored through the real data\nanalysis.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 00:04:49 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Mandal", "Abhijit", ""], ["Ghosh", "Samiran", ""]]}, {"id": "1912.12551", "submitter": "Ryan Langendorf", "authors": "Ryan E. Langendorf and Debra S. Goldberg", "title": "Aligning Statistical Dynamics Captures Biological Network Functioning", "comments": "Supplementary Information included, 35 pages total, 8 main text & 4\n  SI figures, and 3 SI tables. Accompanying software at\n  https://cran.r-project.org/package=netcom", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.MN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical studies of graphs have contributed enormously to our understanding\nof complex systems. Known today as network science, what was originally a\ntheoretical study of graphs has grown into a more scientific exploration of\ncommunities spanning the physical, biological, and social. However, as the\nquantity and types of networks have grown so has their heterogeneity in quality\nand specificity. This has hampered efforts to develop general network theory\ncapable of inferring functioning and predicting dynamics across study systems.\nWe have successfully approached this challenge by aligning networks to each\nother rather than comparing parameter estimates from individually fitted models\nor properties of edge topologies. By comparing the predictability of\nstatistical dynamics originating from each network's constituent nodes we were\nable to build a functional classifier that distinguished underlying processes\nin both synthetic and real-world network data spanning the entire biological\nscale from cellular machinery to ecosystems.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 00:06:57 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Langendorf", "Ryan E.", ""], ["Goldberg", "Debra S.", ""]]}, {"id": "1912.12571", "submitter": "David Frazier", "authors": "Ruben Loaiza-Maya, Gael M. Martin, and David T. Frazier", "title": "Focused Bayesian Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for conducting Bayesian prediction that delivers\naccurate predictions without correctly specifying the unknown true data\ngenerating process. A prior is defined over a class of plausible predictive\nmodels. After observing data, we update the prior to a posterior over these\nmodels, via a criterion that captures a user-specified measure of predictive\naccuracy. Under regularity, this update yields posterior concentration onto the\nelement of the predictive class that maximizes the expectation of the accuracy\nmeasure. In a series of simulation experiments and empirical examples we find\nnotable gains in predictive accuracy relative to conventional likelihood-based\nprediction.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 02:38:13 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 04:59:57 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Loaiza-Maya", "Ruben", ""], ["Martin", "Gael M.", ""], ["Frazier", "David T.", ""]]}, {"id": "1912.12755", "submitter": "Eugene Geis", "authors": "Eugene Geis", "title": "Stochastic Approximation EM for Exploratory Item Factor Analysis", "comments": "131 pages, 57 figures, A dissertation for completion of PhD in\n  psychometrics at Rutgers Graduate School of Education", "journal-ref": "Statistics in Medicine, Volume 38, Issue 21, page 3997 (2019)", "doi": "10.7282/t3-7k3j-6x67 10.1002/sim.8217", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic approximation EM algorithm (SAEM) is described for the\nestimation of item and person parameters given test data coded as dichotomous\nor ordinal variables. The method hinges upon the eigenanalysis of missing\nvariables sampled as augmented data; the augmented data approach was introduced\nby Albert's seminal work applying Gibbs sampling to Item Response Theory in\n1992. Similar to maximum likelihood factor analysis, the factor structure in\nthis Bayesian approach depends only on sufficient statistics, which are\ncomputed from the missing latent data. A second feature of the SAEM algorithm\nis the use of the Robbins-Monro procedure for establishing convergence.\nContrary to Expectation Maximization methods where costly integrals must be\ncalculated, this method is well-suited for highly multidimensional data, and an\nannealing method is implemented to prevent convergence to a local maximum\nlikelihood. Multiple calculations of errors applied within this framework of\nMarkov Chain Monte Carlo are presented to delineate the uncertainty of\nparameter estimates. Given the nature of EFA (exploratory factor analysis), an\nalgorithm is formalized leveraging the Tracy-Widom distribution for the\nretention of factors extracted from an eigenanalysis of the sufficient\nstatistic of the covariance of the augmented data matrix. Simulation conditions\nof dichotomous and polytomous data, from one to ten dimensions of factor\nloadings, are used to assess statistical accuracy and to gauge computational\ntime of the EFA approach of this IRT-specific implementation of the SAEM\nalgorithm. Finally, three applications of this methodology are also reported\nthat demonstrate the effectiveness of the method for enabling timely analyses\nas well as substantive interpretations when this method is applied to real\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 23:09:54 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Geis", "Eugene", ""]]}, {"id": "1912.12783", "submitter": "Shixin Xu", "authors": "Yu Chen, Jin Cheng, Arvind Gupta, Huaxiong Huang, Shixin Xu", "title": "Numerical Method for Parameter Inference of Nonlinear ODEs with Partial\n  Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter inference of dynamical systems is a challenging task faced by many\nresearchers and practitioners across various fields. In many applications, it\nis common that only limited variables are observable. In this paper, we propose\na method for parameter inference of a system of nonlinear coupled ODEs with\npartial observations. Our method combines fast Gaussian process based gradient\nmatching (FGPGM) and deterministic optimization algorithms. By using initial\nvalues obtained by Bayesian steps with low sampling numbers, our deterministic\noptimization algorithm is both accurate and efficient.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 02:23:25 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chen", "Yu", ""], ["Cheng", "Jin", ""], ["Gupta", "Arvind", ""], ["Huang", "Huaxiong", ""], ["Xu", "Shixin", ""]]}, {"id": "1912.12867", "submitter": "Martin Spindler", "authors": "Xi Chen, Ye Luo, Martin Spindler", "title": "Adaptive Discrete Smoothing for High-Dimensional and Nonlinear Panel\n  Data", "comments": "18 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a data-driven smoothing technique for\nhigh-dimensional and non-linear panel data models. We allow for individual\nspecific (non-linear) functions and estimation with econometric or machine\nlearning methods by using weighted observations from other individuals. The\nweights are determined by a data-driven way and depend on the similarity\nbetween the corresponding functions and are measured based on initial\nestimates. The key feature of such a procedure is that it clusters individuals\nbased on the distance / similarity between them, estimated in a first stage.\nOur estimation method can be combined with various statistical estimation\nprocedures, in particular modern machine learning methods which are in\nparticular fruitful in the high-dimensional case and with complex,\nheterogeneous data. The approach can be interpreted as a \\textquotedblleft\nsoft-clustering\\textquotedblright\\ in comparison to\ntraditional\\textquotedblleft\\ hard clustering\\textquotedblright that assigns\neach individual to exactly one group. We conduct a simulation study which shows\nthat the prediction can be greatly improved by using our estimator. Finally, we\nanalyze a big data set from didichuxing.com, a leading company in\ntransportation industry, to analyze and predict the gap between supply and\ndemand based on a large set of covariates. Our estimator clearly performs much\nbetter in out-of-sample prediction compared to existing linear panel data\nestimators.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 09:50:58 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 16:39:10 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Chen", "Xi", ""], ["Luo", "Ye", ""], ["Spindler", "Martin", ""]]}, {"id": "1912.12870", "submitter": "Tomas Masak", "authors": "Tomas Masak, Victor M. Panaretos", "title": "Spatiotemporal Covariance Estimation by Shifted Partial Tracing", "comments": "39 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of covariance estimation for replicated space-time\nprocesses from the functional data analysis perspective. Due to the challenges\nto computational and statistical efficiency posed by the dimensionality of the\nproblem, common paradigms in the space-time processes literature typically\nadopt parametric models, invariances, and/or separability. Replicated outcomes\nmay allow one to do away with parametric specifications, but considerations of\nstatistical and computational efficiency often compel the use of separability,\neven though the assumption may fail in practice. In this paper, we consider the\nproblem of non-parametric covariance estimation, under \"local\" departures from\nseparability. Specifically, we consider a setting where the underlying random\nfield's second order structure is nearly separable, in that it may fail to be\nseparable only locally (either due to noise contamination or due to the\npresence of a non-separable short-range dependent signal component). That is,\nthe covariance is an additive perturbation of a separable component by a\nnon-separable but banded component. We introduce non-parametric estimators\nhinging on the novel concept of shifted partial tracing, which is capable of\nestimating the model computationally efficiently under dense observation. Due\nto the denoising properties of shifted partial tracing, our methods are shown\nto yield consistent estimators of the separable part of the covariance even\nunder noisy discrete observation, without the need for smoothing. Further to\nderiving the convergence rates and limit theorems, we also show that the\nimplementation of our estimators, including for the purpose of prediction,\ncomes at no computational overhead relative to a separable model. Finally, we\ndemonstrate empirical performance and computational feasibility of our methods\nin an extensive simulation study and on a real data set.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 10:22:32 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Masak", "Tomas", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "1912.12880", "submitter": "Juan  Monge", "authors": "Juan Francisco Monge", "title": "The Concordance coefficient: An alternative to the Kruskal-Wallis test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kendall rank correlation coefficient is used to measure the ordinal\nassociation between two measurements. In this paper, we introduce the\nConcordance coefficient as a generalization of the Kendall rank correlation,\nand illustrate its use to measure the ordinal association between quantity and\nquality measures when two or more samples are considered. In this sense, the\nConcordance coefficient can be seen as a generalization of the Kendall rank\ncorrelation coefficient and an alternative to the non-parametric mean\nrank-based methods to compare two or more samples. A comparison of the proposed\nConcordance coefficient and the classical Kruskal-Wallis statistic is presented\nthrough a comparison of exact distributions of both statistics.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 11:05:35 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 18:18:28 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Monge", "Juan Francisco", ""]]}, {"id": "1912.12894", "submitter": "Olga Kaiser", "authors": "Dimitri Igdalov, Olga Kaiser, Ilia Horenko", "title": "On a simultaneous parameter inference and missing data imputation for\n  nonstationary autoregressive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of missing data in time-series analysis\nfocusing on (a) estimation of model parameters in the presence of missing data\nand (b) reconstruction of missing data. Standard approaches used to solve these\nproblems like the maximum likelihood estimation or the Bayesian inference rely\non a priori assumptions like the Gaussian or stationary behavior of missing\ndata and might lead to biased results where these assumptions are unfulfilled.\nIn order to go beyond, we extend the Finite Element Methodology (FEM) for\nVector Auto-Regressive models with eXogenous factors and bounded variation of\nthe model parameters (FEM-VARX) towards handling the missing data problem. The\npresented approach estimates the model parameters and reconstructs the missing\ndata in the considered time series and in the involved exogenous factors,\nsimultaneously. The resulting computational framework was compared to the\nstate-of-art methodologies on a set of test-cases and is available as\nopen-source software.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 11:51:39 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Igdalov", "Dimitri", ""], ["Kaiser", "Olga", ""], ["Horenko", "Ilia", ""]]}, {"id": "1912.12945", "submitter": "Nathan Kallus", "authors": "Nathan Kallus, Xiaojie Mao, Masatoshi Uehara", "title": "Localized Debiased Machine Learning: Efficient Inference on Quantile\n  Treatment Effects and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the efficient estimation of a low-dimensional parameter in an\nestimating equation involving high-dimensional nuisances that depend on the\nparameter of interest. An important example is the (local) quantile treatment\neffect ((L)QTE) in causal inference, for which the efficient estimating\nequation involves as a nuisance the covariate-conditional cumulative\ndistribution function evaluated at the quantile to be estimated. Debiased\nmachine learning (DML) is a data-splitting approach to address the need to\nestimate nuisances using flexible machine learning methods that may not satisfy\nstrong metric entropy conditions, but applying it to problems with\nparameter-dependent nuisances is impractical. For (L)QTE estimation, DML\nrequires we learn the whole conditional cumulative distribution function,\nconditioned on potentially high-dimensional covariates, which is far more\nchallenging than the standard supervised regression task in machine learning.\nWe instead propose localized debiased machine learning (LDML), a new\ndata-splitting approach that avoids this burdensome step and needs only\nestimate the nuisances at a single initial rough guess for the parameter. For\n(L)QTE estimation, this involves just learning two binary regression (i.e.,\nclassification) models, for which many standard, time-tested machine learning\nmethods exist, and the initial rough guess may be given by inverse propensity\nweighting. We prove that under lax rate conditions on nuisances, our estimator\nhas the same favorable asymptotic behavior as the infeasible oracle estimator\nthat solves the estimating equation with the unknown true nuisance functions.\nThus, our proposed approach uniquely enables practically-feasible and\ntheoretically-grounded efficient estimation of important quantities in causal\ninference such as (L)QTEs and in other coarsened data settings.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 14:42:52 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 17:02:07 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 16:50:56 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2020 20:23:00 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kallus", "Nathan", ""], ["Mao", "Xiaojie", ""], ["Uehara", "Masatoshi", ""]]}, {"id": "1912.13063", "submitter": "Adriano Zambom", "authors": "Adriano Zanin Zambom, Seonjin Kim, Nancy Lopes Garcia", "title": "Variable Length Markov Chain with Exogenous Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chains with variable length are useful stochastic models for data\ncompression that avoid the curse of dimensionality faced by that full Markov\nChains. In this paper we introduce a Variable Length Markov Chain whose\ntransition probabilities depend not only on the state history but also on\nexogenous covariates through a logistic model. The goal of the proposed\nprocedure is to obtain the context of the process, that is, the history of the\nprocess that is relevant for predicting the next state, together with the\nestimated coefficients corresponding to the significant exogenous variables. We\nshow that the proposed method is consistent in the sense that the probability\nthat the estimated context and the coefficients are equal to the true data\ngenerating mechanism tend to 1 as the sample size increases. Simulations\nsuggest that, when covariates do contribute for the transition probability, the\nproposed procedure outperforms variable length Markov Chains that do not\nconsider covariates while yielding comparable results when covariates are not\npresent.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 19:11:12 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Zambom", "Adriano Zanin", ""], ["Kim", "Seonjin", ""], ["Garcia", "Nancy Lopes", ""]]}, {"id": "1912.13084", "submitter": "Yi Zhao", "authors": "Yi Zhao, Brian S. Caffo and Joshua B. Ewen", "title": "B-Value and Empirical Equivalence Bound: A New Procedure of Hypothesis\n  Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a two-stage procedure for hypothesis testing, where\nthe first stage is conventional hypothesis testing and the second is an\nequivalence testing procedure using an introduced Empirical Equivalence Bound.\nIn 2016, the American Statistical Association released a policy statement on\nP-values to clarify the proper use and interpretation in response to the\ncriticism of reproducibility and replicability in scientific findings. A recent\nsolution to improve reproducibility and transparency in statistical hypothesis\ntesting is to integrate P-values (or confidence intervals) with practical or\nscientific significance. Similar ideas have been proposed via the equivalence\ntest, where the goal is to infer equality under a presumption (null) of\ninequality of parameters. However, in these testing procedures, the definition\nof scientific significance/equivalence can be subjective. To circumvent this\ndrawback, we introduce a B-value and the Empirical Equivalence Bound, which are\nboth estimated from the data. Performing a second-stage equivalence test, our\nprocedure offers an opportunity to correct for false positive discoveries and\nimprove the reproducibility in findings across studies.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 21:07:05 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Zhao", "Yi", ""], ["Caffo", "Brian S.", ""], ["Ewen", "Joshua B.", ""]]}, {"id": "1912.13119", "submitter": "Garritt Page", "authors": "Garritt L. Page, Fernando A. Quintana, Peter M\\\"uller", "title": "Clustering and Prediction with Variable Dimension Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applied fields incomplete covariate vectors are commonly encountered.\nIt is well known that this can be problematic when making inference on model\nparameters, but its impact on prediction performance is less understood. We\ndevelop a method based on covariate dependent partition models that seamlessly\nhandles missing covariates while completely avoiding any type of imputation.\nThe method we develop allows in-sample predictions as well as out-of-sample\nprediction, even if the missing pattern in the new subjects' incomplete\ncovariate vector was not seen in the training data. Any data type, including\ncategorical or continuous covariates are permitted. In simulation studies the\nproposed method compares favorably. We illustrate the method in two application\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 00:00:41 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 18:48:38 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Page", "Garritt L.", ""], ["Quintana", "Fernando A.", ""], ["M\u00fcller", "Peter", ""]]}, {"id": "1912.13144", "submitter": "Carter Butts", "authors": "Carter T. Butts", "title": "A Dynamic Process Reference Model for Sparse Networks with Reciprocity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many social and other networks exhibit stable size scaling relationships,\nsuch that features such as mean degree or reciprocation rates change slowly or\nare approximately constant as the number of vertices increases. Statistical\nnetwork models built on top of simple Bernoulli baseline (or reference)\nmeasures often behave unrealistically in this respect, leading to the\ndevelopment of sparse reference models that preserve features such as mean\ndegree scaling. In this paper, we generalize recent work on the\nmicro-foundations of such reference models to the case of sparse directed\ngraphs with non-vanishing reciprocity, providing a dynamic process\ninterpretation of the emergence of stable macroscopic behavior.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 02:07:08 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Butts", "Carter T.", ""]]}, {"id": "1912.13188", "submitter": "Ivan Stelmakh", "authors": "Ivan Stelmakh, Nihar B. Shah and Aarti Singh", "title": "On Testing for Biases in Peer Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the issue of biases in scholarly research, specifically, in peer\nreview. There is a long standing debate on whether exposing author identities\nto reviewers induces biases against certain groups, and our focus is on\ndesigning tests to detect the presence of such biases. Our starting point is a\nremarkable recent work by Tomkins, Zhang and Heavlin which conducted a\ncontrolled, large-scale experiment to investigate existence of biases in the\npeer reviewing of the WSDM conference. We present two sets of results in this\npaper. The first set of results is negative, and pertains to the statistical\ntests and the experimental setup used in the work of Tomkins et al. We show\nthat the test employed therein does not guarantee control over false alarm\nprobability and under correlations between relevant variables coupled with any\nof the following conditions, with high probability, can declare a presence of\nbias when it is in fact absent: (a) measurement error, (b) model mismatch, (c)\nreviewer calibration. Moreover, we show that the setup of their experiment may\nitself inflate false alarm probability if (d) bidding is performed in non-blind\nmanner or (e) popular reviewer assignment procedure is employed. Our second set\nof results is positive and is built around a novel approach to testing for\nbiases that we propose. We present a general framework for testing for biases\nin (single vs. double blind) peer review. We then design hypothesis tests that\nunder minimal assumptions guarantee control over false alarm probability and\nnon-trivial power even under conditions (a)--(c) as well as propose an\nalternative experimental setup which mitigates issues (d) and (e). Finally, we\nshow that no statistical test can improve over the non-parametric tests we\nconsider in terms of the assumptions required to control for the false alarm\nprobability.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 06:17:29 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Stelmakh", "Ivan", ""], ["Shah", "Nihar B.", ""], ["Singh", "Aarti", ""]]}, {"id": "1912.13351", "submitter": "Antonio Quintero-Rincon", "authors": "Antonio Quintero-Rincon, Maria Eugenia Fontecha, Carlos D'Giano", "title": "Driver fatigue EEG signals detection by using robust univariate analysis", "comments": "9 pages, 4 figures, 1 table, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driver fatigue is a major cause of traffic accidents and the\nelectroencephalogram (EEG) is considered one of the most reliable predictors of\nfatigue. This paper proposes a novel, simple and fast method for driver fatigue\ndetection that can be implemented in real-time systems by using a\nsingle-channel on the scalp. The method based on the robust univariate analysis\nof EEG signals is composed of two stages. First, the most significant channel\nfrom EEG raw is selected according to the maximum variance. In the second\nstage, this single channel will be used to detect the fatigue EEG signal by\nextracting four feature parameters. Two parameters estimated from the robust\nunivariate analysis, namely mean and covariance, and two classical statistics\nparameters such as variance and covariance that help to tune the robust\nanalysis. Next, an ensemble bagged decision trees classifier is used in order\nto discriminate fatigue signals from alert signals. The proposed algorithm is\ndemonstrated on 24 EEG signals from the Jiangxi University of Technology\ndatabase using only the most significant channel found, which is located in the\nleft tempo-parietal region where spatial awareness and visual-spatial\nnavigation are shared, in terms of 92.7% accuracy with 1.8 seconds of time\ndelay.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 15:22:24 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Quintero-Rincon", "Antonio", ""], ["Fontecha", "Maria Eugenia", ""], ["D'Giano", "Carlos", ""]]}]