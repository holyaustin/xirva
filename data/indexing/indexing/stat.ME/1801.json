[{"id": "1801.00105", "submitter": "Tzee-Ming Huang", "authors": "Yu-Hsiang Cheng and Tzee-Ming Huang and Su-Yun Huang", "title": "Random Partitioning and Distribution-based Thresholding for Iterative\n  Variable Screening in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In big data analysis, a simple task such as linear regression can become very\nchallenging as the variable dimension $p$ grows. As a result, variable\nscreening is inevitable in many scientific studies. In recent years, randomized\nalgorithms have become a new trend and are playing an increasingly important\nrole for large scale data analysis. In this article, we combine the ideas of\nvariable screening and random partitioning to propose a new iterative variable\nscreening method. For moderate sized $p$ of order $O(n^{2-\\delta})$, we propose\na basic algorithm that adopts a distribution-based thresholding rule. For very\nlarge $p$, we further propose a two-stage procedure. This two-stage procedure\nfirst performs a random partitioning to divide predictors into subsets of\nmanageable size of order $O(n^{2-\\delta})$ for variable screening, where\n$\\delta >0$ can be an arbitrarily small positive number. Random partitioning is\nrepeated a few times. Next, the final estimate of variable subset is obtained\nby integrating results obtained from multiple random partitions. Simulation\nstudies show that our method works well and outperforms some renowned\ncompetitors. Real data applications are presented. Our algorithms are able to\nhandle predictors in the size of millions.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 09:12:21 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 11:47:26 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Cheng", "Yu-Hsiang", ""], ["Huang", "Tzee-Ming", ""], ["Huang", "Su-Yun", ""]]}, {"id": "1801.00141", "submitter": "Jules Ellis", "authors": "Jules L. Ellis, Jakub Pecanka, and Jelle Goeman", "title": "Gaining power in multiple testing of interval hypotheses via\n  conditionalization", "comments": null, "journal-ref": null, "doi": "10.1093/biostatistics/kxy042", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel procedure for improving multiple testing\nprocedures (MTPs) under scenarios when the null hypothesis $p$-values tend to\nbe stochastically larger than standard uniform (referred to as 'inflated'). An\nimportant class of problems for which this occurs are tests of interval\nhypotheses. The new procedure starts with a set of $p$-values and discards\nthose with values above a certain pre-selected threshold while the rest are\ncorrected (scaled-up) by the value of the threshold. Subsequently, a chosen\nfamily-wise error rate (FWER) or false discovery rate (FDR) MTP is applied to\nthe set of corrected $p$-values only. We prove the general validity of this\nprocedure under independence of $p$-values, and for the special case of the\nBonferroni method we formulate several sufficient conditions for the control of\nthe FWER. It is demonstrated that this 'filtering' of $p$-values can yield\nconsiderable gains of power under scenarios with inflated null hypotheses\n$p$-values.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 15:11:04 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Ellis", "Jules L.", ""], ["Pecanka", "Jakub", ""], ["Goeman", "Jelle", ""]]}, {"id": "1801.00152", "submitter": "Chaoyu Yu", "authors": "Chaoyu Yu and Peter D. Hoff", "title": "Adaptive Sign Error Control", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiple testing scenarios, typically the sign of a parameter is inferred\nwhen its estimate exceeds some significance threshold in absolute value.\nTypically, the significance threshold is chosen to control the experimentwise\ntype I error rate, family-wise type I error rate or the false discovery rate.\nHowever, controlling these error rates does not explicitly control the sign\nerror rate. In this paper, we propose two procedures for adaptively selecting\nan experimentwise significance threshold in order to control the sign error\nrate. The first controls the sign error rate conservatively, without any\ndistributional assumptions on the parameters of interest. The second is an\nempirical Bayes procedure, and achieves optimal performance asymptotically when\na model for the distribution of the parameters is correctly specified. We also\ndiscuss an adaptive procedure to minimize the sign error rate when the\nexperimentwise type I error rate is held fixed.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 16:55:33 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Yu", "Chaoyu", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1801.00175", "submitter": "Martial Longla", "authors": "Martial Longla, Magda Peligrad", "title": "New robust confidence intervals for the mean under dependence", "comments": "14 pages that provide a central limit theorem, a functional central\n  limit theorem and applications for some long range dependent sequences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to indicate a new method for constructing normal\nconfidence intervals for the mean, when the data is coming from stochastic\nstructures with possibly long memory, especially when the dependence structure\nis not known or even the existence of the density function. More precisely we\nintroduce a random smoothing suggested by the kernel estimators for the\nregression function. Applications are presented to linear processes and\nreversible Markov chains with long memory.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 18:37:51 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Longla", "Martial", ""], ["Peligrad", "Magda", ""]]}, {"id": "1801.00211", "submitter": "Soudeep Deb", "authors": "Soudeep Deb and Ruey S. Tsay", "title": "Spatio-temporal models with space-time interaction and their\n  applications to air pollution data", "comments": null, "journal-ref": null, "doi": "10.5705/ss.202017.0561", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is of utmost importance to have a clear understanding of the status of air\npollution and to provide forecasts and insights about the air quality to the\ngeneral public and researchers in environmental studies. Previous studies of\nspatio-temporal models showed that even a short-term exposure to high\nconcentrations of atmospheric fine particulate matters can be hazardous to the\nhealth of ordinary people. In this study, we develop a spatio-temporal model\nwith space-time interaction for air pollution data. The proposed model uses a\nparametric space-time interaction component along with the spatial and temporal\ncomponents in the mean structure, and introduces a random-effects component\nspecified in the form of zero-mean spatio-temporal processes. For application,\nwe analyze the air pollution data (PM2.5) from 66 monitoring stations across\nTaiwan.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 00:09:20 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Deb", "Soudeep", ""], ["Tsay", "Ruey S.", ""]]}, {"id": "1801.00243", "submitter": "Catherine Forbes", "authors": "Zhichao Liu, Catherine S. Forbes and Heather M. Anderson", "title": "A Robust Bayesian Exponentially Tilted Empirical Likelihood Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new Bayesian approach for analysing moment condition\nmodels in the situation where the data may be contaminated by outliers. The\napproach builds upon the foundations developed by Schennach (2005) who proposed\nthe Bayesian exponentially tilted empirical likelihood (BETEL) method,\njustified by the fact that an empirical likelihood (EL) can be interpreted as\nthe nonparametric limit of a Bayesian procedure when the implied probabilities\nare obtained from maximizing entropy subject to some given moment constraints.\nConsidering the impact that outliers are thought to have on the estimation of\npopulation moments, we develop a new robust BETEL (RBETEL) inferential\nmethodology to deal with this potential problem. We show how the BETEL methods\nare linked to the recent work of Bissiri, Holmes and Walker (2016) who propose\na general framework to update prior belief via a loss function. A controlled\nsimulation experiment is conducted to investigate the performance of the RBETEL\nmethod. We find that the proposed methodology produces reliable posterior\ninference for the fundamental relationships that are embedded in the majority\nof the data, even when outliers are present. The method is also illustrated in\nan empirical study relating brain weight to body weight using a dataset\ncontaining sixty-five different land animal species.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 06:22:48 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Liu", "Zhichao", ""], ["Forbes", "Catherine S.", ""], ["Anderson", "Heather M.", ""]]}, {"id": "1801.00319", "submitter": "Pulong Ma", "authors": "Pulong Ma, Bledar A. Konomi, Emily L. Kang", "title": "An Additive Approximate Gaussian Process Model for Large Spatio-Temporal\n  Data", "comments": "Accepted in Environmetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a large ground-level ozone dataset, we propose a new\ncomputationally efficient additive approximate Gaussian process. The proposed\nmethod incorporates a computational-complexity-reduction method and a separable\ncovariance function, which can flexibly capture various spatio-temporal\ndependence structure. The first component is able to capture nonseparable\nspatio-temporal variability while the second component captures the separable\nvariation. Based on a hierarchical formulation of the model, we are able to\nutilize the computational advantages of both components and perform efficient\nBayesian inference. To demonstrate the inferential and computational benefits\nof the proposed method, we carry out extensive simulation studies assuming\nvarious scenarios of underlying spatio-temporal covariance structure. The\nproposed method is also applied to analyze large spatio-temporal measurements\nof ground-level ozone in the Eastern United States.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 17:17:16 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 01:38:06 GMT"}, {"version": "v3", "created": "Fri, 7 Jun 2019 17:19:58 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Ma", "Pulong", ""], ["Konomi", "Bledar A.", ""], ["Kang", "Emily L.", ""]]}, {"id": "1801.00332", "submitter": "Andreas Dzemski", "authors": "Andreas Dzemski and Ryo Okui", "title": "Confidence set for group membership", "comments": "41 pages, supplementary materials (37 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our confidence set quantifies the statistical uncertainty from data-driven\ncluster assignment in clustered panel models. It covers the true cluster\nmemberships jointly for all units with pre-specified probability and is\nconstructed by inverting many simultaneous unit-specific one-sided tests for\ngroup membership. We justify our approach under $N, T \\to \\infty$ asymptotics\nusing tools from high-dimensional statistics, some of which we extend or\ndevelop in this paper. We provide an empirical application as well as Monte\nCarlo evidence that the confidence set has adequate coverage in finite samples.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 18:18:55 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 18:10:18 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 14:10:34 GMT"}, {"version": "v4", "created": "Wed, 30 Dec 2020 20:41:04 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Dzemski", "Andreas", ""], ["Okui", "Ryo", ""]]}, {"id": "1801.00364", "submitter": "Martin Spindler", "authors": "Jannis Kueck, Ye Luo, Martin Spindler, Zigan Wang", "title": "Estimation and Inference of Treatment Effects with $L_2$-Boosting in\n  High-Dimensional Settings", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical researchers are increasingly faced with rich data sets containing\nmany controls or instrumental variables, making it essential to choose an\nappropriate approach to variable selection. In this paper, we provide results\nfor valid inference after post- or orthogonal $L_2$-Boosting is used for\nvariable selection. We consider treatment effects after selecting among many\ncontrol variables and instrumental variable models with potentially many\ninstruments. To achieve this, we establish new results for the rate of\nconvergence of iterated post-$L_2$-Boosting and orthogonal $L_2$-Boosting in a\nhigh-dimensional setting similar to Lasso, i.e., under approximate sparsity\nwithout assuming the beta-min condition. These results are extended to the 2SLS\nframework and valid inference is provided for treatment effect analysis. We\ngive extensive simulation results for the proposed methods and compare them\nwith Lasso. In an empirical application, we construct efficient IVs with our\nproposed methods to estimate the effect of pre-merger overlap of bank branch\nnetworks in the US on the post-merger stock returns of the acquirer bank.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 22:15:57 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 17:45:15 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Kueck", "Jannis", ""], ["Luo", "Ye", ""], ["Spindler", "Martin", ""], ["Wang", "Zigan", ""]]}, {"id": "1801.00382", "submitter": "Tzee-Ming Huang", "authors": "Yu-Hsiang Cheng, Tzee-Ming Huang, Su-Fen Yang", "title": "A clustering method for misaligned curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering misaligned curves. According to our\nsimilarity measure, two curves are considered similar if they have the same\nshape after being aligned, and the warping function does not differ from the\nidentity function very much.\n  A clustering method is proposed, which updates curves so that similar curves\nbecome more similar, and then combines curves that are similar enough to form\nclusters. The proposed method needs to be used together with a clustering index\nand a set of combination thresholds.\n  Simulation results are presented to demonstrate the performance of this\napproach under different parameter settings and clustering indexes. Two real\ndata applications are included.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 02:14:08 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Cheng", "Yu-Hsiang", ""], ["Huang", "Tzee-Ming", ""], ["Yang", "Su-Fen", ""]]}, {"id": "1801.00548", "submitter": "Ahmed Attia", "authors": "Azam Moosavi, Ahmed Attia and Adrian Sandu", "title": "A Machine Learning Approach to Adaptive Covariance Localization", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": "CSTR-01", "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation plays a key role in large-scale atmospheric weather\nforecasting, where the state of the physical system is estimated from model\noutputs and observations, and is then used as initial condition to produce\naccurate future forecasts. The Ensemble Kalman Filter (EnKF) provides a\npractical implementation of the statistical solution of the data assimilation\nproblem and has gained wide popularity as. This success can be attributed to\nits simple formulation and ease of implementation. EnKF is a Monte-Carlo\nalgorithm that solves the data assimilation problem by sampling the probability\ndistributions involved in Bayes theorem. Because of this, all flavors of EnKF\nare fundamentally prone to sampling errors when the ensemble size is small. In\ntypical weather forecasting applications, the model state space has dimension\n$10^{9}-10^{12}$, while the ensemble size typically ranges between $30-100$\nmembers. Sampling errors manifest themselves as long-range spurious\ncorrelations and have been shown to cause filter divergence. To alleviate this\neffect covariance localization dampens spurious correlations between state\nvariables located at a large distance in the physical space, via an empirical\ndistance-dependent function. The quality of the resulting analysis and forecast\nis greatly influenced by the choice of the localization function parameters,\ne.g., the radius of influence. The localization radius is generally tuned\nempirically to yield desirable results.This work, proposes two adaptive\nalgorithms for covariance localization in the EnKF framework, both based on a\nmachine learning approach. The first algorithm adapts the localization radius\nin time, while the second algorithm tunes the localization radius in both time\nand space. Numerical experiments carried out with the Lorenz-96 model, and a\nquasi-geostrophic model, reveal the potential of the proposed machine learning\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 04:39:59 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 02:41:03 GMT"}, {"version": "v3", "created": "Sun, 11 Feb 2018 04:30:49 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Moosavi", "Azam", ""], ["Attia", "Ahmed", ""], ["Sandu", "Adrian", ""]]}, {"id": "1801.00569", "submitter": "Jeremie Houssineau", "authors": "Jeremie Houssineau", "title": "Parameter estimation with a class of outer probability measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the interplay between random and deterministic phenomena using a\nrepresentation of uncertainty based on the measure-theoretic concept of outer\nmeasure. The meaning of the analogues of different probabilistic concepts is\ninvestigated and examples of application are given. The novelty of this article\nlies mainly in the suitability of the tools introduced for jointly representing\nrandom and deterministic uncertainty. These tools are shown to yield intuitive\nresults in simple situations and to generalise easily to more complex cases.\nConnections with Dempster-Shafer theory, the empirical Bayes methods and\ngeneralised Bayesian inference are also highlighted.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 07:51:56 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 09:29:16 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 15:58:36 GMT"}, {"version": "v4", "created": "Sun, 19 Apr 2020 19:32:19 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Houssineau", "Jeremie", ""]]}, {"id": "1801.00571", "submitter": "Jeremie Houssineau", "authors": "Jeremie Houssineau", "title": "A linear algorithm for multi-target tracking in the context of\n  possibility theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a modelling framework for multi-target tracking based on\npossibility theory and illustrate its ability to account for the general lack\nof knowledge that the target-tracking practitioner must deal with when working\nwith real data. We also introduce and study variants of the notions of point\nprocess and intensity function, which lead to the derivation of an analogue of\nthe probability hypothesis density (PHD) filter. The gains provided by the\nconsidered modelling framework in terms of flexibility lead to the loss of some\nof the abilities that the PHD filter possesses; in particular the estimation of\nthe number of targets by integration of the intensity function. Yet, the\nproposed recursion displays a number of advantages such as facilitating the\nintroduction of observation-driven birth schemes and the modelling the absence\nof information on the initial number of targets in the scene. The performance\nof the proposed approach is demonstrated on simulated data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 08:03:42 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 11:37:31 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 20:29:05 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Houssineau", "Jeremie", ""]]}, {"id": "1801.00591", "submitter": "Fabio Rapallo", "authors": "Roberto Fontana and Fabio Rapallo", "title": "Unions of Orthogonal Arrays and their aberrations via Hilbert bases", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generate all the Orthogonal Arrays (OAs) of a given size n and strength t\nas the union of a collection of OAs which belong to an inclusion-minimal set of\nOAs. We derive a formula for computing the (Generalized) Word Length Pattern of\na union of OAs that makes use of their polynomial counting functions. In this\nway the best OAs according to the Generalized Minimum Aberration criterion can\nbe found by simply exploring a relatively small set of counting functions. The\nclasses of OAs with 5 binary factors, strength 2, and sizes 16 and 20 are fully\ndescribed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 09:59:27 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Fontana", "Roberto", ""], ["Rapallo", "Fabio", ""]]}, {"id": "1801.00644", "submitter": "Reagan Mozer", "authors": "Reagan Mozer, Luke Miratrix, Aaron Russell Kaufman, L. Jason\n  Anastasopoulos", "title": "Matching with Text Data: An Experimental Evaluation of Methods for\n  Matching Documents and of Measuring Match Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching for causal inference is a well-studied problem, but standard methods\nfail when the units to match are text documents: the high-dimensional and rich\nnature of the data renders exact matching infeasible, causes propensity scores\nto produce incomparable matches, and makes assessing match quality difficult.\nIn this paper, we characterize a framework for matching text documents that\ndecomposes existing methods into: (1) the choice of text representation, and\n(2) the choice of distance metric. We investigate how different choices within\nthis framework affect both the quantity and quality of matches identified\nthrough a systematic multifactor evaluation experiment using human subjects.\nAltogether we evaluate over 100 unique text matching methods along with 5\ncomparison methods taken from the literature. Our experimental results identify\nmethods that generate matches with higher subjective match quality than current\nstate-of-the-art techniques. We enhance the precision of these results by\ndeveloping a predictive model to estimate the match quality of pairs of text\ndocuments as a function of our various distance scores. This model, which we\nfind successfully mimics human judgment, also allows for approximate and\nunsupervised evaluation of new procedures. We then employ the identified best\nmethod to illustrate the utility of text matching in two applications. First,\nwe engage with a substantive debate in the study of media bias by using text\nmatching to control for topic selection when comparing news articles from\nthirteen news sources. We then show how conditioning on text data leads to more\nprecise causal inferences in an observational study examining the effects of a\nmedical intervention.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 13:47:43 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 17:14:00 GMT"}, {"version": "v3", "created": "Sun, 15 Apr 2018 20:13:44 GMT"}, {"version": "v4", "created": "Mon, 11 Jun 2018 19:40:04 GMT"}, {"version": "v5", "created": "Fri, 29 Jun 2018 09:17:43 GMT"}, {"version": "v6", "created": "Wed, 3 Oct 2018 20:57:57 GMT"}, {"version": "v7", "created": "Wed, 13 Mar 2019 19:50:07 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Mozer", "Reagan", ""], ["Miratrix", "Luke", ""], ["Kaufman", "Aaron Russell", ""], ["Anastasopoulos", "L. Jason", ""]]}, {"id": "1801.00718", "submitter": "Charles Truong", "authors": "Charles Truong, Laurent Oudre, Nicolas Vayatis", "title": "Selective review of offline change point detection methods", "comments": null, "journal-ref": "Signal Processing, 167:107299, 2020", "doi": "10.1016/j.sigpro.2019.107299", "report-no": null, "categories": "cs.CE stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a selective survey of algorithms for the offline\ndetection of multiple change points in multivariate time series. A general yet\nstructuring methodological strategy is adopted to organize this vast body of\nwork. More precisely, detection algorithms considered in this review are\ncharacterized by three elements: a cost function, a search method and a\nconstraint on the number of changes. Each of those elements is described,\nreviewed and discussed separately. Implementations of the main algorithms\ndescribed in this article are provided within a Python package called ruptures.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 16:44:08 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 10:19:28 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 08:54:35 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Truong", "Charles", ""], ["Oudre", "Laurent", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1801.00736", "submitter": "Manuel Oviedo de la Fuente", "authors": "Manuel Febrero-Bande, Wenceslao Gonz\\'alez-Manteiga and Manuel Oviedo\n  de la Fuente", "title": "Variable selection in Functional Additive Regression Models", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the problem of variable selection in regression models\nin the case of functional variables that may be mixed with other type of\nvariables (scalar, multivariate, directional, etc.). Our proposal begins with a\nsimple null model and sequentially selects a new variable to be incorporated\ninto the model based on the use of distance correlation proposed by\n\\cite{Szekely2007}. For the sake of simplicity, this paper only uses additive\nmodels. However, the proposed algorithm may assess the type of contribution\n(linear, non linear, ...) of each variable. The algorithm has shown quite\npromising results when applied to simulations and real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 17:28:34 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 16:55:42 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Febrero-Bande", "Manuel", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["de la Fuente", "Manuel Oviedo", ""]]}, {"id": "1801.00753", "submitter": "Franz J. Kir\\'aly", "authors": "Frithjof Gressmann, Franz J. Kir\\'aly, Bilal Mateen and Harald\n  Oberhauser", "title": "Probabilistic supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modelling and supervised learning are central to modern data\nscience. With predictions from an ever-expanding number of supervised black-box\nstrategies - e.g., kernel methods, random forests, deep learning aka neural\nnetworks - being employed as a basis for decision making processes, it is\ncrucial to understand the statistical uncertainty associated with these\npredictions.\n  As a general means to approach the issue, we present an overarching framework\nfor black-box prediction strategies that not only predict the target but also\ntheir own predictions' uncertainty. Moreover, the framework allows for fair\nassessment and comparison of disparate prediction strategies. For this, we\nformally consider strategies capable of predicting full distributions from\nfeature variables, so-called probabilistic supervised learning strategies.\n  Our work draws from prior work including Bayesian statistics, information\ntheory, and modern supervised machine learning, and in a novel synthesis leads\nto (a) new theoretical insights such as a probabilistic bias-variance\ndecomposition and an entropic formulation of prediction, as well as to (b) new\nalgorithms and meta-algorithms, such as composite prediction strategies,\nprobabilistic boosting and bagging, and a probabilistic predictive independence\ntest.\n  Our black-box formulation also leads (c) to a new modular interface view on\nprobabilistic supervised learning and a modelling workflow API design, which we\nhave implemented in the newly released skpro machine learning toolbox,\nextending the familiar modelling interface and meta-modelling functionality of\nsklearn. The skpro package provides interfaces for construction, composition,\nand tuning of probabilistic supervised learning strategies, together with\norchestration features for validation and comparison of any such strategy - be\nit frequentist, Bayesian, or other.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 18:08:49 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 21:22:42 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 14:30:27 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Gressmann", "Frithjof", ""], ["Kir\u00e1ly", "Franz J.", ""], ["Mateen", "Bilal", ""], ["Oberhauser", "Harald", ""]]}, {"id": "1801.00802", "submitter": "Shu Yang", "authors": "Shu Yang, and Peng Ding", "title": "Combining multiple observational data sources to estimate causal effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The era of big data has witnessed an increasing availability of multiple data\nsources for statistical analyses. We consider estimation of causal effects\ncombining big main data with unmeasured confounders and smaller validation data\nwith supplementary information on these confounders. Under the unconfoundedness\nassumption with completely observed confounders, the smaller validation data\nallow for constructing consistent estimators for causal effects, but the big\nmain data can only give error-prone estimators in general. However, by\nleveraging the information in the big main data in a principled way, we can\nimprove the estimation efficiencies yet preserve the consistencies of the\ninitial estimators based solely on the validation data. Our framework applies\nto asymptotically normal estimators, including the commonly-used regression\nimputation, weighting, and matching estimators, and does not require a correct\nspecification of the model relating the unmeasured confounders to the observed\nvariables. We also propose appropriate bootstrap procedures, which makes our\nmethod straightforward to implement using software routines for existing\nestimators.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 19:10:53 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 13:35:54 GMT"}, {"version": "v3", "created": "Sat, 20 Apr 2019 12:39:18 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Yang", "Shu", ""], ["Ding", "Peng", ""]]}, {"id": "1801.00816", "submitter": "Debashis Ghosh", "authors": "Debashis Ghosh", "title": "Relaxed covariate overlap and margin-based causal effect estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In most nonrandomized observational studies, differences between treatment\ngroups may arise not only due to the treatment but also because of the effect\nof confounders. Therefore, causal inference regarding the treatment effect is\nnot as straightforward as in a randomized trial. To adjust for confounding due\nto measured covariates, a variety of methods based on the potential outcomes\nframework are used to estimate average treatment effects. One of the key\nassumptions is treatment positivity, which states that the probability of\ntreatment is bounded away from zero and one for any possible combination of the\nconfounders. Methods for performing causal inference when this assumption is\nviolated are relatively limited. In this article, we discuss a new\nbalance-related condition involving the convex hulls of treatment groups, which\nI term relaxed covariate overlap. An advantage of this concept is that it can\nbe linked to a concept from machine learning, termed the margin. Introduction\nof relaxed covariate overlap leads to an approach in which one can perform\ncausal inference in a three-step manner. The methodology is illustrated with\ntwo examples.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 20:13:35 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 03:49:59 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 03:08:16 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Ghosh", "Debashis", ""]]}, {"id": "1801.00865", "submitter": "Chris McKennan", "authors": "Chris McKennan and Dan Nicolae", "title": "Accounting for unobserved covariates with varying degrees of\n  estimability in high dimensional biological data", "comments": "12 pages of main text, 34 pages including proofs of all results, 3\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important phenomenon in high dimensional biological data is the presence\nof unobserved covariates that can have a significant impact on the measured\nresponse. When these factors are also correlated with the covariate(s) of\ninterest (i.e. disease status), ignoring them can lead to increased type I\nerror and spurious false discovery rate estimates. We show that depending on\nthe strength of this correlation and the informativeness of the observed data\nfor the latent factors, previously proposed estimators for the effect of the\ncovariate of interest that attempt to account for unobserved covariates are\nasymptotically biased, which corroborates previous practitioners' observations\nthat these estimators tend to produce inflated test statistics. We then provide\nan estimator that corrects the bias and prove it has the same asymptotic\ndistribution as the ordinary least squares estimator when every covariate is\nobserved. Lastly, we use previously published DNA methylation data to show our\nmethod can more accurately estimate the direct effect of asthma on methylation\nthan previously published methods, which underestimate the correlation between\nasthma and latent cell type heterogeneity. Our re-analysis shows that the\nmajority of the variability in methylation due to asthma in those data is\nactually mediated through cell composition.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 00:01:51 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 19:36:15 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["McKennan", "Chris", ""], ["Nicolae", "Dan", ""]]}, {"id": "1801.00973", "submitter": "Yong Li", "authors": "Yong Li, Xiaobin Liu, Jun Yu and Tao Zeng", "title": "A New Wald Test for Hypothesis Testing Based on MCMC outputs", "comments": "Bayesian $\\chi^2$ test; Decision theory; Wald test; Markov chain\n  Monte Carlo; Latent variable models", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new and convenient $\\chi^2$ wald test based on MCMC outputs\nis proposed for hypothesis testing. The new statistic can be explained as MCMC\nversion of Wald test and has several important advantages that make it very\nconvenient in practical applications. First, it is well-defined under improper\nprior distributions and avoids Jeffrey-Lindley's paradox. Second, it's\nasymptotic distribution can be proved to follow the $\\chi^2$ distribution so\nthat the threshold values can be easily calibrated from this distribution.\nThird, it's statistical error can be derived using the Markov chain Monte Carlo\n(MCMC) approach. Fourth, most importantly, it is only based on the posterior\nMCMC random samples drawn from the posterior distribution. Hence, it is only\nthe by-product of the posterior outputs and very easy to compute. In addition,\nwhen the prior information is available, the finite sample theory is derived\nfor the proposed test statistic. At last, the usefulness of the test is\nillustrated with several applications to latent variable models widely used in\neconomics and finance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 12:25:25 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Li", "Yong", ""], ["Liu", "Xiaobin", ""], ["Yu", "Jun", ""], ["Zeng", "Tao", ""]]}, {"id": "1801.01003", "submitter": "Michelle Liou PhD", "authors": "Jiun-Wei Liou, Michelle Liou, Philip E. Cheng, Chin-Chiuan Lin", "title": "An Information Analysis on Modeling Interaction Effects in Logistic\n  Regression", "comments": "Following the JASA article (2010) by Cheng et al., this article\n  introduces a constructive scheme for selecting parsimonious models for a\n  multivariate contingency table using logistic regression (i.e., a\n  well-defined target variable plus several predictors). Please email your\n  comments and questions to the corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Akaike information criterion (AIC) is commonly used to select a logistic\nregression model for optimal prediction of a binary response by a specified\nfamily of models. It however lacks a convincing method of prescribing a proper\nfamily of models using the desired predictors and their interaction effects.\nFor an alternative approach to model selection, we propose a direct selection\nscheme which first identifies the indispensable regressors as main-effect\npredictors, then examines significant interaction effects between the selected\npredictors such that a logistic model is constructed. The two-step selection\nscheme is formulated by testing for valid information identity between the\nresponse and the predictors, from which the most parsimonious logistic model is\nderived from the least set of indispensable predictors and interaction effects.\nAs a byproduct, the minimum AIC model is easily found in a neighborhood of the\nselected model. The scheme is employed to yield the logistic model for\npredicting the acquisition of professional licenses in a survey of employed\nyouth workers.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 13:56:07 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 05:06:10 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Liou", "Jiun-Wei", ""], ["Liou", "Michelle", ""], ["Cheng", "Philip E.", ""], ["Lin", "Chin-Chiuan", ""]]}, {"id": "1801.01013", "submitter": "Torben Martinussen", "authors": "Torben Martinussen and Stijn Vansteelandt", "title": "Instrumental variables estimation with competing risk data", "comments": "arXiv admin note: text overlap with arXiv:1608.00818", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-to-event analyses are often plagued by both -- possibly unmeasured --\nconfounding and competing risks. To deal with the former, the use of\ninstrumental variables for effect estimation is rapidly gaining ground. We show\nhow to make use of such variables in competing risk analyses. In particular, we\nshow how to infer the effect of an arbitrary exposure on cause-specific hazard\nfunctions under a semi-parametric model that imposes relatively weak\nrestrictions on the observed data distribution. The proposed approach is\nflexible accommodating exposures and instrumental variables of arbitrary type,\nand enables covariate adjustment. It makes use of closed-form estimators that\ncan be recursively calculated, and is shown to perform well in simulation\nstudies. We also demonstrates its use in an application on the effect of\nmammography screening on the risk of dying from breast cancer\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 13:18:23 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Martinussen", "Torben", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "1801.01220", "submitter": "Changshuai Wei", "authors": "Changshuai Wei and Qing Lu", "title": "Generalized Similarity U: A Non-parametric Test of Association Based on\n  Similarity", "comments": null, "journal-ref": "Bioinformatics (2017): btx103", "doi": "10.1093/bioinformatics/btx103", "report-no": null, "categories": "stat.ME q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Second generation sequencing technologies are being increasingly used for\ngenetic association studies, where the main research interest is to identify\nsets of genetic variants that contribute to various phenotype. The phenotype\ncan be univariate disease status, multivariate responses and even\nhigh-dimensional outcomes. Considering the genotype and phenotype as two\ncomplex objects, this also poses a general statistical problem of testing\nassociation between complex objects. We here proposed a similarity-based test,\ngeneralized similarity U (GSU), that can test the association between complex\nobjects. We first studied the theoretical properties of the test in a general\nsetting and then focused on the application of the test to sequencing\nassociation studies. Based on theoretical analysis, we proposed to use\nLaplacian kernel based similarity for GSU to boost power and enhance\nrobustness. Through simulation, we found that GSU did have advantages over\nexisting methods in terms of power and robustness. We further performed a whole\ngenome sequencing (WGS) scan for Alzherimer Disease Neuroimaging Initiative\n(ADNI) data, identifying three genes, APOE, APOC1 and TOMM40, associated with\nimaging phenotype. We developed a C++ package for analysis of whole genome\nsequencing data using GSU. The source codes can be downloaded at\nhttps://github.com/changshuaiwei/gsu.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 01:43:31 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Wei", "Changshuai", ""], ["Lu", "Qing", ""]]}, {"id": "1801.01242", "submitter": "Johan Dahlin PhD", "authors": "Johan Dahlin, Adrian Wills, Brett Ninness", "title": "Sparse Bayesian ARX models with flexible noise distributions", "comments": "17 pages, 4 figures. Accepted for publication in the Proceedings of\n  the 18th IFAC Symposium on System Identification (SYSID). Typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating linear dynamic system models\nwhen the observations are corrupted by random disturbances with nonstandard\ndistributions. The paper is particularly motivated by applications where sensor\nimperfections involve significant contribution of outliers or wrap-around\nissues resulting in multi-modal distributions such as commonly encountered in\nrobotics applications. As will be illustrated, these nonstandard measurement\nerrors can dramatically compromise the effectiveness of standard estimation\nmethods, while a computational Bayesian approach developed here is demonstrated\nto be equally effective as standard methods in standard measurement noise\nscenarios, but dramatically more effective in nonstandard measurement noise\ndistribution scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 04:30:22 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 00:08:53 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 11:23:00 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Dahlin", "Johan", ""], ["Wills", "Adrian", ""], ["Ninness", "Brett", ""]]}, {"id": "1801.01267", "submitter": "Tiejun Tong", "authors": "Jiandong Shi and Dehui Luo and Hong Weng and Xian-Tao Zeng and Lu Lin\n  and Tiejun Tong", "title": "How to estimate the sample mean and standard deviation from the five\n  number summary?", "comments": "18 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some clinical studies, researchers may report the five number summary\n(including the sample median, the first and third quartiles, and the minimum\nand maximum values) rather than the sample mean and standard deviation. To\nconduct meta-analysis for pooling studies, one needs to first estimate the\nsample mean and standard deviation from the five number summary. A number of\nstudies have been proposed in the recent literature to solve this problem.\nHowever, none of the existing estimators for the standard deviation is\nsatisfactory for practical use. After a brief review of the existing\nliterature, we point out that Wan et al.'s method (BMC Med Res Methodol 14:135,\n2014) has a serious limitation in estimating the standard deviation from the\nfive number summary. To improve it, we propose a smoothly weighted estimator by\nincorporating the sample size information and derive the optimal weight for the\nnew estimator. For ease of implementation, we also provide an approximation\nformula of the optimal weight and a shortcut formula for estimating the\nstandard deviation from the five number summary. The performance of the\nproposed estimator is evaluated through two simulation studies. In comparison\nwith Wan et al.'s estimator, our new estimator provides a more accurate\nestimate for normal data and performs favorably for non-normal data. In real\ndata analysis, our new method is also able to provide a more accurate estimate\nof the true sample standard deviation than the existing method. In this paper,\nwe propose an optimal estimator of the standard deviation from the five number\nsummary. Together with the optimal mean estimator in Luo et al. (Stat Methods\nMed Res, in press, 2017), our new methods have improved the existing literature\nand will make a solid contribution to meta-analysis and evidence-based\nmedicine.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 08:05:57 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Shi", "Jiandong", ""], ["Luo", "Dehui", ""], ["Weng", "Hong", ""], ["Zeng", "Xian-Tao", ""], ["Lin", "Lu", ""], ["Tong", "Tiejun", ""]]}, {"id": "1801.01278", "submitter": "Michelle Liou PhD", "authors": "Philip E.Cheng, Jiun-Wei Liou, Hung-Wen Kao, Michelle Liou", "title": "A Constructive Procedure for Modeling Categorical Variables: Log-Linear\n  and Logit Models", "comments": "This article sets up a new construction methodology for selecting the\n  most parsimonious log-linear and logit models in any finite-dimensional\n  categorical data table using the analysis of information identity. Please\n  refer this article to arXiv: 1801.01003 [stat.ME] and Cheng et al. (JASA,\n  2010). Email your comments and questions to the corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association between categorical variables in contingency tables is analyzed\nusing the information identities based on multivariate multinomial\ndistributions. A scheme of geometric decompositions of the information\nidentities is developed to identify indispensable predictors and interaction\neffects in the construction of concise log-linear and logit models; it suggests\na new approach for selecting parsimonious log-linear and logit models which\nwould facilitate the search for the minimum AIC models as a byproduct. The\nproposed constructive schemes are illustrated along with the analysis of a\ncontingency data table collected in a study on the risk factors of ischemic\ncerebral stroke.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 09:01:16 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 05:14:49 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Cheng", "Philip E.", ""], ["Liou", "Jiun-Wei", ""], ["Kao", "Hung-Wen", ""], ["Liou", "Michelle", ""]]}, {"id": "1801.01285", "submitter": "Carel F.W. Peeters", "authors": "Bernet S. Kato and Carel F.W. Peeters", "title": "Inequality Constrained Multilevel Models", "comments": "20 pages. Postprint of Chapter 13 in: H. Hoijtink, I. Klugkist, &\n  P.A. Boelen (Eds.). \"Bayesian Evaluation of Informative Hypotheses.\" New\n  York: Springer, 2008: pp. 273-295", "journal-ref": null, "doi": "10.1007/978-0-387-09612-4_13", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilevel or hierarchical data structures can occur in many areas of\nresearch, including economics, psychology, sociology, agriculture, medicine,\nand public health. Over the last 25 years, there has been increasing interest\nin developing suitable techniques for the statistical analysis of multilevel\ndata, and this has resulted in a broad class of models known under the generic\nname of multilevel models. Generally, multilevel models are useful for\nexploring how relationships vary across higher-level units taking into account\nthe within and between cluster variations. Research scientists often have\nsubstantive theories in mind when evaluating data with statistical models.\nSubstantive theories often involve inequality constraints among the parameters\nto translate a theory into a model. This chapter shows how the inequality\nconstrained multilevel linear model can be given a Bayesian formulation, how\nthe model parameters can be estimated using a so-called augmented Gibbs\nsampler, and how posterior probabilities can be computed to assist the\nresearcher in model selection.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 09:19:51 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Kato", "Bernet S.", ""], ["Peeters", "Carel F. W.", ""]]}, {"id": "1801.01326", "submitter": "Yan Wang", "authors": "Yan Wang, Lewi Stone", "title": "Understanding the connections between species distribution models", "comments": "25 pages, 4 figures, 5 tables, and 3 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for accurately predicting species distributions have become essential\ntools for many ecological and conservation problems. For many species,\npresence-background (presence-only) data is the most commonly available type of\nspatial data. A number of important methods have been proposed to model\npresence-background (PB) data, and there have been debates on the connection\nbetween these seemingly disparate methods. The paper begins by studying the\nclose relationship between the LI (Lancaster & Imbens, 1996) and LK (Lele &\nKeim, 2006) models, which were among the first developed methods for analysing\nPB data. The second part of the paper identifies close connections between the\nLK and point process models, as well as the equivalence between the Scaled\nBinomial (SB), Expectation-Maximization (EM), partial likelihood based Lele\n(2009) and LI methods, many of which have not been noted in the literature. We\nclarify that all these methods are the same in their ability to estimate the\nrelative probability (or intensity) of presence from PB data; and the absolute\nprobability of presence, when extra information of the species' prevalence is\nknown. A new unified constrained LK (CLK) method is also proposed as a\ngeneralisation of the better known existing approaches, with less theory\ninvolved and greater ease of implementation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 12:37:25 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Wang", "Yan", ""], ["Stone", "Lewi", ""]]}, {"id": "1801.01464", "submitter": "Roberto Di Mari", "authors": "Roberto Di Mari, Antonio Punzo and Zsuzsa Bakk", "title": "Cluster-weighted latent class modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually in Latent Class Analysis (LCA), external predictors are taken to be\ncluster conditional probability predictors (LC models with covariates), and/or\nscore conditional probability predictors (LC regression models). In such cases,\ntheir distribution is not of interest. Class specific distribution is of\ninterest in the distal outcome model, when the distribution of the external\nvariable(s) is assumed to dependent on LC membership. In this paper, we\nconsider a more general formulation, typical in cluster-weighted models, which\nembeds both the latent class regression and the distal outcome models. This\nallows us to test simultaneously both whether the distribution of the\ncovariate(s) differs across classes, and whether there are significant direct\neffects of the covariate(s) on the indicators, by including most of the\ninformation about the covariate(s) - latent variable relationship. We show the\nadvantages of the proposed modeling approach through a set of population\nstudies and an empirical application on assets ownership of Italian households.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 17:35:44 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Di Mari", "Roberto", ""], ["Punzo", "Antonio", ""], ["Bakk", "Zsuzsa", ""]]}, {"id": "1801.01489", "submitter": "Aaron Fisher", "authors": "Aaron Fisher, Cynthia Rudin, Francesca Dominici", "title": "All Models are Wrong, but Many are Useful: Learning a Variable's\n  Importance by Studying an Entire Class of Prediction Models Simultaneously", "comments": "The final, published article is now available at\n  http://www.jmlr.org/papers/v20/18-760.html . This version contains minor\n  changes to the introductory sections, and some typo corrections. The title of\n  this article changed twice during the revision process (see v1 and v3 on\n  arxiv)", "journal-ref": "Journal of Machine Learning Research 20 (177), 1-81, 2019", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable importance (VI) tools describe how much covariates contribute to a\nprediction model's accuracy. However, important variables for one\nwell-performing model (for example, a linear model\n$f(\\mathbf{x})=\\mathbf{x}^{T}\\beta$ with a fixed coefficient vector $\\beta$)\nmay be unimportant for another model. In this paper, we propose model class\nreliance (MCR) as the range of VI values across all well-performing model in a\nprespecified class. Thus, MCR gives a more comprehensive description of\nimportance by accounting for the fact that many prediction models, possibly of\ndifferent parametric forms, may fit the data well. In the process of deriving\nMCR, we show several informative results for permutation-based VI estimates,\nbased on the VI measures used in Random Forests. Specifically, we derive\nconnections between permutation importance estimates for a single prediction\nmodel, U-statistics, conditional variable importance, conditional causal\neffects, and linear model coefficients. We then give probabilistic bounds for\nMCR, using a novel, generalizable technique. We apply MCR to a public data set\nof Broward County criminal records to study the reliance of recidivism\nprediction models on sex and race. In this application, MCR can be used to help\ninform VI for unknown, proprietary models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 18:53:04 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 19:25:36 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 04:20:23 GMT"}, {"version": "v4", "created": "Tue, 20 Aug 2019 05:35:24 GMT"}, {"version": "v5", "created": "Mon, 23 Dec 2019 18:32:06 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Fisher", "Aaron", ""], ["Rudin", "Cynthia", ""], ["Dominici", "Francesca", ""]]}, {"id": "1801.01525", "submitter": "Leo Duan", "authors": "Leo L Duan, Alexander L Young, Akihiko Nishimura, David B Dunson", "title": "Bayesian Constraint Relaxation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior information often takes the form of parameter constraints. Bayesian\nmethods include such information through prior distributions having constrained\nsupport. By using posterior sampling algorithms, one can quantify uncertainty\nwithout relying on asymptotic approximations. However, sharply constrained\npriors are (a) not necessary in some settings; and (b) tend to limit modeling\nscope to a narrow set of distributions that are tractable computationally.\nInspired by the vast literature that replaces the slab-and-spike prior with a\ncontinuous approximation, we propose to replace the sharp indicator function of\nthe constraint with an exponential kernel, thereby creating a\nclose-to-constrained neighborhood within the Euclidean space in which the\nconstrained subspace is embedded. This kernel decays with distance from the\nconstrained space at a rate depending on a relaxation hyperparameter. By\navoiding the sharp constraint, we enable use of off-the-shelf posterior\nsampling algorithms, such as Hamiltonian Monte Carlo, facilitating automatic\ncomputation in broad models. We study the constrained and relaxed distributions\nunder multiple settings, and theoretically quantify their differences. We\nillustrate the method through multiple novel modeling examples.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 19:49:20 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 13:34:09 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Duan", "Leo L", ""], ["Young", "Alexander L", ""], ["Nishimura", "Akihiko", ""], ["Dunson", "David B", ""]]}, {"id": "1801.01529", "submitter": "Daniel Nevo", "authors": "Daniel Nevo, Tsuyoshi Hamada, Shuji Ogino and Molin Wang", "title": "A novel calibration framework for survival analysis when a binary\n  covariate is measured at sparse time points", "comments": null, "journal-ref": null, "doi": "10.1093/biostatistics/kxy063", "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goals in clinical and cohort studies often include evaluation of the\nassociation of a time-dependent binary treatment or exposure with a survival\noutcome. Recently, several impactful studies targeted the association between\naspirin-taking and survival following colorectal cancer diagnosis. Due to\nsurgery, aspirin-taking value is zero at baseline and may change its value to\none at some time point. Estimating this association is complicated by having\nonly intermittent measurements on aspirin-taking. Naive, commonly-used, methods\ncan lead to substantial bias. We present a class of calibration models for the\ndistribution of the time of status change of the binary covariate. Estimates\nobtained from these models are then incorporated into the proportional hazard\npartial likelihood in a natural way. We develop nonparametric, semiparametric\nand parametric calibration models, and derive asymptotic theory for the methods\nthat we implement in the aspirin and colorectal cancer study. Our methodology\nallows to include additional baseline variables in the calibration models for\nthe status change time of the binary covariate. We further develop a risk-set\ncalibration approach that is more useful in settings in which the association\nbetween the binary covariate and survival is strong.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 19:55:32 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Nevo", "Daniel", ""], ["Hamada", "Tsuyoshi", ""], ["Ogino", "Shuji", ""], ["Wang", "Molin", ""]]}, {"id": "1801.01602", "submitter": "Qiang Sun", "authors": "Jianqing Fan, Qiang Sun, Wen-Xin Zhou, Ziwei Zhu", "title": "Principal component analysis for big data", "comments": "review article, in press with Wiley StatsRef", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data is transforming our world, revolutionizing operations and analytics\neverywhere, from financial engineering to biomedical sciences. The complexity\nof big data often makes dimension reduction techniques necessary before\nconducting statistical inference. Principal component analysis, commonly\nreferred to as PCA, has become an essential tool for multivariate data analysis\nand unsupervised dimension reduction, the goal of which is to find a lower\ndimensional subspace that captures most of the variation in the dataset. This\narticle provides an overview of methodological and theoretical developments of\nPCA over the last decade, with focus on its applications to big data analytics.\nWe first review the mathematical formulation of PCA and its theoretical\ndevelopment from the view point of perturbation analysis. We then briefly\ndiscuss the relationship between PCA and factor analysis as well as its\napplications to large covariance estimation and multiple testing. PCA also\nfinds important applications in many modern machine learning problems, and we\nfocus on community detection, ranking, mixture model and manifold learning in\nthis paper.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 01:17:53 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Fan", "Jianqing", ""], ["Sun", "Qiang", ""], ["Zhou", "Wen-Xin", ""], ["Zhu", "Ziwei", ""]]}, {"id": "1801.01669", "submitter": "Xin Shi", "authors": "Xin Shi, Robert Qiu, Xing He, Zenan Ling, Haosen Yang, Lei Chu", "title": "Early Anomaly Detection and Location in Distribution Network: A\n  Data-Driven Approach", "comments": "10 pages, submitted to IET Generation, Transmission and Distribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The measurement data collected from the supervisory control and data\nacquisition (SCADA) system installed in distribution network can reflect the\noperational state of the network effectively. In this paper, a random matrix\ntheory (RMT) based approach is developed for early anomaly detection and\nlocalization by using the data. For every feeder in the distribution network, a\ncorresponding data matrix is formed. Based on the Marchenko-Pastur Law for the\nempirical spectral analysis of covariance `signal+noise' matrix, the linear\neigenvalue statistics are introduced to indicate the anomaly, and the outliers\nand their corresponding eigenvectors are analyzed for locating the anomaly. As\nfor the low observability feeders in the distribution network, an increasing\ndata dimension algorithm is designed for the formulated low-dimensional\nmatrices being more accurately analyzed. The developed approach can detect and\nlocalize the anomaly at an early stage, and it is robust to random disturbance\nand measurement error. Cases on Matpower simulation data and real SCADA data\ncorroborate the feasibility of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 08:32:17 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 08:41:38 GMT"}, {"version": "v3", "created": "Sun, 21 Jul 2019 03:45:44 GMT"}, {"version": "v4", "created": "Wed, 11 Mar 2020 16:24:30 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Shi", "Xin", ""], ["Qiu", "Robert", ""], ["He", "Xing", ""], ["Ling", "Zenan", ""], ["Yang", "Haosen", ""], ["Chu", "Lei", ""]]}, {"id": "1801.01748", "submitter": "Sacha van Albada", "authors": "Sacha Jennifer van Albada, Peter A. Robinson", "title": "Transformation of arbitrary distributions to the normal distribution\n  with application to EEG test-retest reliability", "comments": null, "journal-ref": "J. Neurosci. Methods 161: 205-211 (2007)", "doi": "10.1016/j.jneumeth.2006.11.004", "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many variables in the social, physical, and biosciences, including\nneuroscience, are non-normally distributed. To improve the statistical\nproperties of such data, or to allow parametric testing, logarithmic or logit\ntransformations are often used. Box-Cox transformations or ad hoc methods are\nsometimes used for parameters for which no transformation is known to\napproximate normality. However, these methods do not always give good agreement\nwith the Gaussian. A transformation is discussed that maps probability\ndistributions as closely as possible to the normal distribution, with exact\nagreement for continuous distributions. To illustrate, the transformation is\napplied to a theoretical distribution, and to quantitative\nelectroencephalographic (qEEG) measures from repeat recordings of 32 subjects\nwhich are highly non-normal. Agreement with the Gaussian was better than using\nlogarithmic, logit, or Box-Cox transformations. Since normal data have\npreviously been shown to have better test-retest reliability than non-normal\ndata under fairly general circumstances, the implications of our transformation\nfor the test-retest reliability of parameters were investigated. Reliability\nwas shown to improve with the transformation, where the improvement was\ncomparable to that using Box-Cox. An advantage of the general transformation is\nthat it does not require laborious optimization over a range of parameters or a\ncase-specific choice of form.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 13:09:49 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["van Albada", "Sacha Jennifer", ""], ["Robinson", "Peter A.", ""]]}, {"id": "1801.01950", "submitter": "Jia Zhang", "authors": "Jia Zhang, Xin Chen and Wang Zhou", "title": "High Dimensional Elliptical Sliced Inverse Regression in non-Gaussian\n  Distributions", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliced inverse regression (SIR) is the most widely-used sufficient dimension\nreduction method due to its simplicity, generality and computational\nefficiency. However, when the distribution of the covariates deviates from the\nmultivariate normal distribution, the estimation efficiency of SIR is rather\nlow. In this paper, we propose a robust alternative to SIR - called elliptical\nsliced inverse regression (ESIR) for analysing high dimensional, elliptically\ndistributed data. There are wide range of applications of the elliptically\ndistributed data, especially in finance and economics where the distribution of\nthe data is often heavy-tailed. To tackle the heavy-tailed elliptically\ndistributed covariates, we novelly utilize the multivariate Kendall's tau\nmatrix in a framework of so-called generalized eigenvector problem for\nsufficient dimension reduction. Methodologically, we present a practical\nalgorithm for our method. Theoretically, we investigate the asymptotic behavior\nof the ESIR estimator and obtain the corresponding convergence rate under high\ndimensional setting. Quantities of simulation results show that ESIR\nsignificantly improves the estimation efficiency in heavy-tailed scenarios. A\nstock exchange data analysis also demonstrates the effectiveness of our method.\nMoreover, ESIR can be easily extended to most other sufficient dimension\nreduction methods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 00:58:16 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Zhang", "Jia", ""], ["Chen", "Xin", ""], ["Zhou", "Wang", ""]]}, {"id": "1801.01990", "submitter": "Yoav Zemel", "authors": "Valentina Masarotto and Victor M. Panaretos and Yoav Zemel", "title": "Procrustes Metrics on Covariance Operators and Optimal Transportation of\n  Gaussian Processes", "comments": "30 pages", "journal-ref": "Invited paper, Special Issue on Manifold Statistics, Sankhya A\n  81(1):172-213, 2019", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance operators are fundamental in functional data analysis, providing\nthe canonical means to analyse functional variation via the celebrated\nKarhunen--Lo\\`eve expansion. These operators may themselves be subject to\nvariation, for instance in contexts where multiple functional populations are\nto be compared. Statistical techniques to analyse such variation are intimately\nlinked with the choice of metric on covariance operators, and the intrinsic\ninfinite-dimensionality of these operators. In this paper, we describe the\nmanifold geometry of the space of trace-class infinite-dimensional covariance\noperators and associated key statistical properties, under the recently\nproposed infinite-dimensional version of the Procrustes metric. We identify\nthis space with that of centred Gaussian processes equipped with the\nWasserstein metric of optimal transportation. The identification allows us to\nprovide a complete description of those aspects of this manifold geometry that\nare important in terms of statistical inference, and establish key properties\nof the Fr\\'echet mean of a random sample of covariances, as well as generative\nmodels that are canonical for such metrics and link with the problem of\nregistration of functional data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 08:43:25 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Masarotto", "Valentina", ""], ["Panaretos", "Victor M.", ""], ["Zemel", "Yoav", ""]]}, {"id": "1801.02400", "submitter": "Daniel Oberski", "authors": "Daniel L. Oberski, Jeroen K. Vermunt", "title": "The Expected Parameter Change (EPC) for Local Dependence Assessment in\n  Binary Data Latent Class Models", "comments": "R code implementing our proposal and including both example datasets\n  is available online as supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary data latent class models crucially assume local independence,\nviolations of which can seriously bias the results. We present two tools for\nmonitoring local dependence in binary data latent class models: the \"Expected\nParameter Change\" (EPC) and a generalized EPC, estimating the substantive size\nand direction of possible local dependencies. The asymptotic and finite sample\nbehavior of the measures is studied, and two applications to the U.S. Census\nestimation of Hispanic ethnicity and medical experts' ratings of x-rays\ndemonstrate its value in arriving at a model that balances realism and\nparsimony.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 12:31:12 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Oberski", "Daniel L.", ""], ["Vermunt", "Jeroen K.", ""]]}, {"id": "1801.02512", "submitter": "Josmar Mazucheli", "authors": "J. Mazucheli and A. F. B. Menezes and S. Chakraborty", "title": "On the one parameter unit-Lindley distribution and its associated\n  regression model for proportion data", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper considering the transformation $X=\\frac{Y}{1+Y}$, where $Y\n\\sim\\text{Lindley}(\\theta)$, we propose the unit-Lindley distribution and\ninvestigate some of its mathematical properties. A important fact associated\nwith this new distribution is that is possible to obtain the analytical\nexpression for bias correction of the maximum likelihood estimator. Moreover,\nit belongs to the exponential family. This distribution allows us to\nincorporate covariates directly in the mean and consequently to quantify the\ninfluence on the average of the response variable. Finally, a practical\napplication is present and it is shown that our model fits much better than the\nBeta regression.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 15:41:58 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Mazucheli", "J.", ""], ["Menezes", "A. F. B.", ""], ["Chakraborty", "S.", ""]]}, {"id": "1801.02597", "submitter": "Claudia Di Caterina", "authors": "Claudia Di Caterina, Giuliana Cortese, Nicola Sartori", "title": "Monte Carlo modified profile likelihood in models for clustered data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main focus of the analysts who deal with clustered data is usually not on\nthe clustering variables, and hence the group-specific parameters are treated\nas nuisance. If a fixed effects formulation is preferred and the total number\nof clusters is large relative to the single-group sizes, classical frequentist\ntechniques relying on the profile likelihood are often misleading. The use of\nalternative tools, such as modifications to the profile likelihood or\nintegrated likelihoods, for making accurate inference on a parameter of\ninterest can be complicated by the presence of nonstandard modelling and/or\nsampling assumptions. We show here how to employ Monte Carlo simulation in\norder to approximate the modified profile likelihood in some of these\nunconventional frameworks. The proposed solution is widely applicable and is\nshown to retain the usual properties of the modified profile likelihood. The\napproach is examined in two instances particularly relevant in applications,\ni.e. missing-data models and survival models with unspecified censoring\ndistribution. The effectiveness of the proposed solution is validated via\nsimulation studies and two clinical trial applications.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 18:20:53 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 08:29:24 GMT"}, {"version": "v3", "created": "Sat, 29 Dec 2018 15:44:18 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Di Caterina", "Claudia", ""], ["Cortese", "Giuliana", ""], ["Sartori", "Nicola", ""]]}, {"id": "1801.02817", "submitter": "Leying Guan", "authors": "Leying Guan", "title": "Test Error Estimation after Model Selection Using Validation Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When performing supervised learning with the model selected using validation\nerror from sample splitting and cross validation, the minimum value of the\nvalidation error can be biased downward. We propose two simple methods that use\nthe errors produced in the validating step to estimate the test error after\nmodel selection, and we focus on the situations where we select the model by\nminimizing the validation error and the randomized validation error. Our\nmethods do not require model refitting, and the additional computational cost\nis negligible. In the setting of sample splitting, we show that, the proposed\ntest error estimates have biases of size $o(1/\\sqrt{n})$ under suitable\nassumptions. We also propose to use the bootstrap to construct confidence\nintervals for the test error based on this result. We apply our proposed\nmethods to a number of simulations and examine their performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 06:46:53 GMT"}, {"version": "v2", "created": "Sun, 11 Feb 2018 03:29:35 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Guan", "Leying", ""]]}, {"id": "1801.02946", "submitter": "Thomas Opitz", "authors": "Raphael Huser, Thomas Opitz, Emeric Thibaud", "title": "Max-infinitely divisible models and inference for spatial extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many environmental processes, recent studies have shown that the\ndependence strength is decreasing when quantile levels increase. This implies\nthat the popular max-stable models are inadequate to capture the rate of joint\ntail decay, and to estimate joint extremal probabilities beyond observed\nlevels. We here develop a more flexible modeling framework based on the class\nof max-infinitely divisible processes, which extend max-stable processes while\nretaining dependence properties that are natural for maxima. We propose two\nparametric constructions for max-infinitely divisible models, which relax the\nmax-stability property but remain close to some popular max-stable models\nobtained as special cases. The first model considers maxima over a finite,\nrandom number of independent observations, while the second model generalizes\nthe spectral representation of max-stable processes. Inference is performed\nusing a pairwise likelihood. We illustrate the benefits of our new modeling\nframework on Dutch wind gust maxima calculated over different time units.\nResults strongly suggest that our proposed models outperform other natural\nmodels, such as the Student-t copula process and its max-stable limit, even for\nlarge block sizes.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 14:29:56 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2018 04:28:28 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 08:39:57 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Huser", "Raphael", ""], ["Opitz", "Thomas", ""], ["Thibaud", "Emeric", ""]]}, {"id": "1801.02954", "submitter": "Sean Van Der Merwe", "authors": "Sean van der Merwe", "title": "A method for Bayesian regression modelling of composition data", "comments": "10 pages, 1 figure, 2 tables", "journal-ref": "South African Statistical Journal, 53, 55-64 (2019)\n  https://hdl.handle.net/10520/EJC-14af74f1cc", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and industrial processes produce data that is best analysed\nas vectors of relative values, often called compositions or proportions. The\nDirichlet distribution is a natural distribution to use for composition or\nproportion data. It has the advantage of a low number of parameters, making it\nthe parsimonious choice in many cases. In this paper we consider the case where\nthe outcome of a process is Dirichlet, dependent on one or more explanatory\nvariables in a regression setting. We explore some existing approaches to this\nproblem, and then introduce a new simulation approach to fitting such models,\nbased on the Bayesian framework. We illustrate the advantages of the new\napproach through simulated examples and an application in sport science. These\nadvantages include: increased accuracy of fit, increased power for inference,\nand the ability to introduce random effects without additional complexity in\nthe analysis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 14:40:49 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["van der Merwe", "Sean", ""]]}, {"id": "1801.03019", "submitter": "Gemma Moran", "authors": "Gemma E. Moran, Veronika Rockova and Edward I. George", "title": "Variance prior forms for high-dimensional Bayesian variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of high dimensional variable selection for the Gaussian\nlinear model when the unknown error variance is also of interest. In this\npaper, we show that the use of conjugate shrinkage priors for Bayesian variable\nselection can have detrimental consequences for such variance estimation. Such\npriors are often motivated by the invariance argument of Jeffreys (1961).\nRevisiting this work, however, we highlight a caveat that Jeffreys himself\nnoticed; namely that biased estimators can result from inducing dependence\nbetween parameters a priori. In a similar way, we show that conjugate priors\nfor linear regression, which induce prior dependence, can lead to such\nunderestimation in the Bayesian high-dimensional regression setting. Following\nJeffreys, we recommend as a remedy to treat regression coefficients and the\nerror variance as independent a priori. Using such an independence prior\nframework, we extend the Spike-and-Slab Lasso of Rockova and George (2018) to\nthe unknown variance case. This extended procedure outperforms both the fixed\nvariance approach and alternative penalized likelihood methods on simulated\ndata. On the protein activity dataset of Clyde and Parmigiani (1998), the\nSpike-and-Slab Lasso with unknown variance achieves lower cross-validation\nerror than alternative penalized likelihood methods, demonstrating the gains in\npredictive accuracy afforded by simultaneous error variance estimation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 16:11:20 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 04:35:40 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Moran", "Gemma E.", ""], ["Rockova", "Veronika", ""], ["George", "Edward I.", ""]]}, {"id": "1801.03132", "submitter": "Chen Wang", "authors": "Chen Wang, Suzhen Wang, Fuyan Shi, Zaixiang Wang", "title": "Robust Propensity Score Computation Method based on Machine Learning\n  with Label-corrupted Data", "comments": "26 pages, 4 figures, 8tables, to be submitted to peer-review journals\n  soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In biostatistics, propensity score is a common approach to analyze the\nimbalance of covariate and process confounding covariates to eliminate\ndifferences between groups. While there are an abundant amount of methods to\ncompute propensity score, a common issue of them is the corrupted labels in the\ndataset. For example, the data collected from the patients could contain\nsamples that are treated mistakenly, and the computing methods could\nincorporate them as a misleading information. In this paper, we propose a\nMachine Learning-based method to handle the problem. Specifically, we utilize\nthe fact that the majority of sample should be labeled with the correct\ninstance and design an approach to first cluster the data with spectral\nclustering and then sample a new dataset with a distribution processed from the\nclustering results. The propensity score is computed by Xgboost, and a\nmathematical justification of our method is provided in this paper. The\nexperimental results illustrate that xgboost propensity scores computing with\nthe data processed by our method could outperform the same method with original\ndata, and the advantages of our method increases as we add some artificial\ncorruptions to the dataset. Meanwhile, the implementation of xgboost to compute\npropensity score for multiple treatments is also a pioneering work in the area.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 20:35:31 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Wang", "Chen", ""], ["Wang", "Suzhen", ""], ["Shi", "Fuyan", ""], ["Wang", "Zaixiang", ""]]}, {"id": "1801.03184", "submitter": "Ian Vernon Dr", "authors": "Ian Vernon, Samuel E. Jackson, Jonathan A. Cumming", "title": "Known Boundary Emulation of Complex Computer Models", "comments": "31 pages, 12 figures. Version accepted for publication by SIAM/ASA\n  Journal on Uncertainty Quantification. Updated references and slightly\n  shortened section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer models are now widely used across a range of scientific disciplines\nto describe various complex physical systems, however to perform full\nuncertainty quantification we often need to employ emulators. An emulator is a\nfast statistical construct that mimics the complex computer model, and greatly\naids the vastly more computationally intensive uncertainty quantification\ncalculations that a serious scientific analysis often requires. In some cases,\nthe complex model can be solved far more efficiently for certain parameter\nsettings, leading to boundaries or hyperplanes in the input parameter space\nwhere the model is essentially known. We show that for a large class of\nGaussian process style emulators, multiple boundaries can be formally\nincorporated into the emulation process, by Bayesian updating of the emulators\nwith respect to the boundaries, for trivial computational cost. The resulting\nupdated emulator equations are given analytically. This leads to emulators that\npossess increased accuracy across large portions of the input parameter space.\nWe also describe how a user can incorporate such boundaries within standard\nblack box GP emulation packages that are currently available, without altering\nthe core code. Appropriate designs of model runs in the presence of known\nboundaries are then analysed, with two kinds of general purpose designs\nproposed. We then apply the improved emulation and design methodology to an\nimportant systems biology model of hormonal crosstalk in Arabidopsis Thaliana.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 23:27:53 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 11:51:07 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Vernon", "Ian", ""], ["Jackson", "Samuel E.", ""], ["Cumming", "Jonathan A.", ""]]}, {"id": "1801.03238", "submitter": "Jiarui Lu", "authors": "Jiarui Lu, Pixu Shi and Hongzhe Li", "title": "Generalized Linear Models with Linear Constraints for Microbiome\n  Compositional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by regression analysis for microbiome compositional data, this\npaper considers generalized linear regression analysis with compositional\ncovariates, where a group of linear constraints on regression coefficients are\nimposed to account for the compositional nature of the data and to achieve\nsubcompositional coherence. A penalized likelihood estimation procedure using a\ngeneralized accelerated proximal gradient method is developed to efficiently\nestimate the regression coefficients. A de-biased procedure is developed to\nobtain asymptotically unbiased and normally distributed estimates, which leads\nto valid confidence intervals of the regression coefficients. Simulations\nresults show the correctness of the coverage probability of the confidence\nintervals and smaller variances of the estimates when the appropriate linear\nconstraints are imposed. The methods are illustrated by a microbiome study in\norder to identify bacterial species that are associated with inflammatory bowel\ndisease (IBD) and to predict IBD using fecal microbiome.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 04:48:02 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Lu", "Jiarui", ""], ["Shi", "Pixu", ""], ["Li", "Hongzhe", ""]]}, {"id": "1801.03539", "submitter": "Randall Reese", "authors": "Randall Reese, Xiaotian Dai, Guifang Fu", "title": "Strong Sure Screening of Ultra-high Dimensional Categorical Data", "comments": "Preprint of Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature screening for ultra high dimensional feature spaces plays a critical\nrole in the analysis of data sets whose predictors exponentially exceed the\nnumber of observations. Such data sets are becoming increasingly prevalent in\nareas such as bioinformatics, medical imaging, and social network analysis.\nFrequently, these data sets have both categorical response and categorical\ncovariates, yet extant feature screening literature rarely considers such data\ntypes. We propose a new screening procedure rooted in the Cochran-Armitage\ntrend test. Our method is specifically applicable for data where both the\nresponse and predictors are categorical. Under a set of reasonable conditions,\nwe demonstrate that our screening procedure has the strong sure screening\nproperty, which extends the seminal results of Fan and Lv. A series of four\nsimulations are used to investigate the performance of our method relative to\nthree other screening methods. We also apply a two-stage iterative approach to\na real data example by first employing our proposed method, and then further\nscreening a subset of selected covariates using lasso, adaptive-lasso and\nelastic net regularization.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 20:05:08 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 00:12:57 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Reese", "Randall", ""], ["Dai", "Xiaotian", ""], ["Fu", "Guifang", ""]]}, {"id": "1801.03583", "submitter": "Judea Pearl", "authors": "Karthika Mohan and Judea Pearl", "title": "Graphical Models for Processing Missing Data", "comments": "34 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": "r473-L", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews recent advances in missing data research using graphical\nmodels to represent multivariate dependencies. We first examine the limitations\nof traditional frameworks from three different perspectives:\n\\textit{transparency, estimability and testability}. We then show how\nprocedures based on graphical models can overcome these limitations and provide\nmeaningful performance guarantees even when data are Missing Not At Random\n(MNAR). In particular, we identify conditions that guarantee consistent\nestimation in broad categories of missing data problems, and derive procedures\nfor implementing this estimation. Finally we derive testable implications for\nmissing data models in both MAR (Missing At Random) and MNAR categories.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 23:26:02 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 20:50:41 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Mohan", "Karthika", ""], ["Pearl", "Judea", ""]]}, {"id": "1801.03596", "submitter": "Marius Hofert", "authors": "Marius Hofert, Wayne Oldford, Avinash Prasad, Mu Zhu", "title": "A framework for measuring dependence between random vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework for quantifying dependence between random vectors is introduced.\nWith the notion of a collapsing function, random vectors are summarized by\nsingle random variables, called collapsed random variables in the framework.\nUsing this framework, a general graphical assessment of independence between\ngroups of random variables for arbitrary collapsing functions is provided.\nMeasures of association computed from the collapsed random variables are then\nused to measure the dependence between random vectors. To this end, suitable\ncollapsing functions are presented. Furthermore, the notion of a collapsed\ndistribution function and collapsed copula are introduced and investigated for\ncertain collapsing functions. This investigation yields a multivariate\nextension of the Kendall distribution and its corresponding Kendall copula for\nwhich some properties and examples are provided. In addition, non-parametric\nestimators for the collapsed measures of dependence are provided along with\ntheir corresponding asymptotic properties. Finally, data applications to\nbioinformatics and finance are presented.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 00:32:23 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Hofert", "Marius", ""], ["Oldford", "Wayne", ""], ["Prasad", "Avinash", ""], ["Zhu", "Mu", ""]]}, {"id": "1801.03635", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy, Sivaraman Balakrishnan, Max G'Sell", "title": "Sharp instruments for classifying compliers and generalizing causal\n  effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that, without restricting treatment effect heterogeneity,\ninstrumental variable (IV) methods only identify \"local\" effects among\ncompliers, i.e., those subjects who take treatment only when encouraged by the\nIV. Local effects are controversial since they seem to only apply to an\nunidentified subgroup; this has led many to denounce these effects as having\nlittle policy relevance. However, we show that such pessimism is not always\nwarranted: it is possible in some cases to accurately predict who compliers\nare, and obtain tight bounds on more generalizable effects in identifiable\nsubgroups. We propose methods for doing so and study their estimation error and\nasymptotic properties, showing that these tasks can in theory be accomplished\neven with very weak IVs. We go on to introduce a new measure of IV quality\ncalled \"sharpness\", which reflects the variation in compliance explained by\ncovariates, and captures how well one can identify compliers and obtain tight\nbounds on identifiable subgroup effects. We develop an estimator of sharpness,\nand show that it is asymptotically efficient under weak conditions. Finally we\nexplore finite-sample properties via simulation, and apply the methods to study\ncanvassing effects on voter turnout. We propose that sharpness should be\npresented alongside strength to assess IV quality.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 05:31:18 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 22:11:00 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Kennedy", "Edward H.", ""], ["Balakrishnan", "Sivaraman", ""], ["G'Sell", "Max", ""]]}, {"id": "1801.03791", "submitter": "Benjamin All\\'evius", "authors": "Benjamin All\\'evius", "title": "On the precision matrix of an irregularly sampled AR(1) process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irregularly sampled AR(1) processes appear in many computationally demanding\napplications. This text provides an analytical expression for the precision\nmatrix of such a process, and gives efficient algorithms for density evaluation\nand simulation, implemented in the R package irregulAR1.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 14:44:51 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 07:29:19 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["All\u00e9vius", "Benjamin", ""]]}, {"id": "1801.03896", "submitter": "Rina Barber", "authors": "Rina Foygel Barber and Emmanuel J. Cand\\`es and Richard J. Samworth", "title": "Robust inference with knockoffs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the variable selection problem, which seeks to identify important\nvariables influencing a response $Y$ out of many candidate features $X_1,\n\\ldots, X_p$. We wish to do so while offering finite-sample guarantees about\nthe fraction of false positives - selected variables $X_j$ that in fact have no\neffect on $Y$ after the other features are known. When the number of features\n$p$ is large (perhaps even larger than the sample size $n$), and we have no\nprior knowledge regarding the type of dependence between $Y$ and $X$, the\nmodel-X knockoffs framework nonetheless allows us to select a model with a\nguaranteed bound on the false discovery rate, as long as the distribution of\nthe feature vector $X=(X_1,\\dots,X_p)$ is exactly known. This model selection\nprocedure operates by constructing \"knockoff copies'\" of each of the $p$\nfeatures, which are then used as a control group to ensure that the model\nselection algorithm is not choosing too many irrelevant features. In this work,\nwe study the practical setting where the distribution of $X$ could only be\nestimated, rather than known exactly, and the knockoff copies of the $X_j$'s\nare therefore constructed somewhat incorrectly. Our results, which are free of\nany modeling assumption whatsoever, show that the resulting model selection\nprocedure incurs an inflation of the false discovery rate that is proportional\nto our errors in estimating the distribution of each feature $X_j$ conditional\non the remaining features $\\{X_k:k\\neq j\\}$. The model-X knockoff framework is\ntherefore robust to errors in the underlying assumptions on the distribution of\n$X$, making it an effective method for many practical applications, such as\ngenome-wide association studies, where the underlying distribution on the\nfeatures $X_1,\\dots,X_p$ is estimated accurately but not known exactly.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 17:42:16 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 16:33:50 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 18:45:15 GMT"}, {"version": "v4", "created": "Mon, 11 Feb 2019 16:48:59 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Cand\u00e8s", "Emmanuel J.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1801.03901", "submitter": "Omer Weissbrod", "authors": "Omer Weissbrod, Shachar Kaufman, David Golan and Saharon Rosset", "title": "Maximum Likelihood for Gaussian Process Classification and Generalized\n  Linear Mixed Models under Case-Control Sampling", "comments": null, "journal-ref": "JMLR (108):1-30, 2019", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data sets in various domains often include units that were sampled\nnon-randomly from the population and have a latent correlation structure. Here\nwe investigate a common form of this setting, where every unit is associated\nwith a latent variable, all latent variables are correlated, and the\nprobability of sampling a unit depends on its response. Such settings often\narise in case-control studies, where the sampled units are correlated due to\nspatial proximity, family relations, or other sources of relatedness. Maximum\nlikelihood estimation in such settings is challenging from both a computational\nand statistical perspective, necessitating approximations that take the\nsampling scheme into account. We propose a family of approximate likelihood\napproaches which combine composite likelihood and expectation propagation. We\ndemonstrate the efficacy of our solutions via extensive simulations. We utilize\nthem to investigate the genetic architecture of several complex disorders\ncollected in case-control genetic association studies, where hundreds of\nthousands of genetic variants are measured for every individual, and the\nunderlying disease liabilities of individuals are correlated due to genetic\nsimilarity. Our work is the first to provide a tractable likelihood-based\nsolution for case-control data with complex dependency structures.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 17:56:06 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 14:32:17 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 15:00:25 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Weissbrod", "Omer", ""], ["Kaufman", "Shachar", ""], ["Golan", "David", ""], ["Rosset", "Saharon", ""]]}, {"id": "1801.03989", "submitter": "Grant Izmirlian", "authors": "Grant Izmirlian", "title": "Average Power and $\\lambda$-power in Multiple Testing Scenarios when the\n  Benjamini-Hochberg False Discovery Rate Procedure is Used", "comments": "49 pages, 2 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss several approaches to defining power in studies designed around\nthe Benjamini-Hochberg (BH) false discovery rate (FDR) procedure. We focus\nprimarily on the \\textit{average power} and the $\\lambda$-\\textit{power}, which\nare the expected true positive fraction and the probability that the true\npositive fraction exceeds $\\lambda$, respectively. We prove results concerning\nstrong consistency and asymptotic normality for the positive call fraction\n(PCF), the true positive fraction (TPF) and false discovery fraction (FDF).\nConvergence of their corresponding expected values, including a convergence\nresult for the average power, follow as a corollaries. After reviewing what is\nknown about convergence in distribution of the errors of the plugin procedure,\n(Genovese, 2004), we prove central limit theorems for fully empirical versions\nof the PCF, TPF, and FDF, using a result for stopped stochastic processes. The\ncentral limit theorem (CLT) for the TPF is used to obtain an approximate\nexpression for the $\\lambda$-power, while the CLT for the FDF is used to\nintroduce an approximate procedure for determining a suitably small nominal FDR\nthat results in a speicified bound on the FDF with stipulated high probability.\nThe paper also contains the results of a large simulation study covering a\nfairly substantial portion of the space of possible inputs encountered in\napplication of the results in the design of a biomarker study, a micro-array\nexperiment and a GWAS study.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 21:05:23 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 22:53:53 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Izmirlian", "Grant", ""]]}, {"id": "1801.04058", "submitter": "Jared Murray", "authors": "Jared S. Murray", "title": "Multiple Imputation: A Review of Practical and Theoretical Findings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation is a straightforward method for handling missing data in\na principled fashion. This paper presents an overview of multiple imputation,\nincluding important theoretical results and their practical implications for\ngenerating and using multiple imputations. A review of strategies for\ngenerating imputations follows, including recent developments in flexible joint\nmodeling and sequential regression/chained equations/fully conditional\nspecification approaches. Finally, we compare and contrast different methods\nfor generating imputations on a range of criteria before identifying promising\navenues for future research.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 05:29:16 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Murray", "Jared S.", ""]]}, {"id": "1801.04111", "submitter": "Emanuele  Giorgi", "authors": "Emanuele Giorgi", "title": "On the goodness-of-fit of generalized linear geostatistical models", "comments": "In press on Spatial Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalization of Zhang's coefficient of determination to\ngeneralized linear geostatistical models and illustrate its application to\nriver-blindness mapping. The generalized coefficient of determination has a\nmore intuitive interpretation than other measures of predictive performance and\nallows to assess the individual contribution of each explanatory variable and\nthe random effects to spatial prediction. The developed methodology is also\nmore widely applicable to any generalized linear mixed model.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 09:50:40 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Giorgi", "Emanuele", ""]]}, {"id": "1801.04202", "submitter": "Zheng Zhang", "authors": "Chunrong Ai, Oliver Linton, Zheng Zhang", "title": "A Simple and Efficient Estimation Method for Models with Nonignorable\n  Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple and efficient estimation procedure for the model\nwith non-ignorable missing data studied by Morikawa and Kim (2016). Their\nsemiparametrically efficient estimator requires explicit nonparametric\nestimation and so suffers from the curse of dimensionality and requires a\nbandwidth selection. We propose an estimation method based on the Generalized\nMethod of Moments (hereafter GMM). Our method is consistent and asymptotically\nnormal regardless of the number of moments chosen. Furthermore, if the number\nof moments increases appropriately our estimator can achieve the semiparametric\nefficiency bound derived in Morikawa and Kim (2016), but under weaker\nregularity conditions. Moreover, our proposed estimator and its consistent\ncovariance matrix are easily computed with the widely available GMM package. We\npropose two data-based methods for selection of the number of moments. A small\nscale simulation study reveals that the proposed estimation indeed out-performs\nthe existing alternatives in finite samples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 15:33:44 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Ai", "Chunrong", ""], ["Linton", "Oliver", ""], ["Zhang", "Zheng", ""]]}, {"id": "1801.04309", "submitter": "Zheyang Wu", "authors": "Hong Zhang, Tiejun Tong, John E Landers, Zheyang Wu", "title": "TFisher Tests: Optimal and Adaptive Thresholding for Combining\n  $p$-Values", "comments": "46 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For testing a group of hypotheses, tremendous $p$-value combination methods\nhave been developed and widely applied since 1930's. Some methods (e.g., the\nminimal $p$-value) are optimal for sparse signals, and some others (e.g.,\nFisher's combination) are optimal for dense signals. To address a wide spectrum\nof signal patterns, this paper proposes a unifying family of statistics, called\nTFisher, with general $p$-value truncation and weighting schemes. Analytical\ncalculations for the $p$-value and the statistical power of TFisher under\ngeneral hypotheses are given. Optimal truncation and weighting parameters are\nstudied based on Bahadur Efficiency (BE) and the proposed Asymptotic Power\nEfficiency (APE), which is superior to BE for studying the signal detection\nproblem. A soft-thresholding scheme is shown to be optimal for signal detection\nin a large space of signal patterns. When prior information of signal pattern\nis unavailable, an omnibus test, oTFisher, can adapt to the given data.\nSimulations evidenced the accuracy of calculations and validated the\ntheoretical properties. The TFisher tests were applied to analyzing a whole\nexome sequencing data of amyotrophic lateral sclerosis. Relevant tests and\ncalculations have been implemented into an R package $TFisher$ and published on\nthe CRAN.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 20:41:16 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Zhang", "Hong", ""], ["Tong", "Tiejun", ""], ["Landers", "John E", ""], ["Wu", "Zheyang", ""]]}, {"id": "1801.04490", "submitter": "Martial Longla", "authors": "Martial Longla, Siva Sivaganesan", "title": "On a statistical approach to mate choices in reproduction", "comments": "13 pages without graphs or pictures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a probabilistic approach to modeling the movements of subjects\nthrough multiple stages, with \"stays\" or survival at each stage for a random\nlength of time, and ending at a desired final stage. We use conditional Markov\nchains with exponential survival times to model the movement of each subject.\nThis is motivated by a study to learn about of the choices that different types\nof female turkeys make in choosing a male turkey, and in particular, the\ndifferences in male choices between groups of females. In this paper, we\npropose a model for the subjects' movements toward the final stage, and provide\nmaximum likelihood estimation of the model parameters. We also provide results\nrelating to certain questions of interest, such as the distribution of the\nnumber of subjects reaching a stage and the probability that a subject reaches\nthe final stage, and develop methods for estimating these quantities and\ntesting statistical hypotheses of interest.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 23:06:39 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Longla", "Martial", ""], ["Sivaganesan", "Siva", ""]]}, {"id": "1801.04716", "submitter": "Kris Peremans", "authors": "Kris Peremans and Stefan Van Aelst", "title": "Robust Inference for Seemingly Unrelated Regression Models", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmva.2018.05.002", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seemingly unrelated regression models generalize linear regression models by\nconsidering multiple regression equations that are linked by contemporaneously\ncorrelated disturbances. Robust inference for seemingly unrelated regression\nmodels is considered. MM-estimators are introduced to obtain estimators that\nhave both a high breakdown point and a high normal efficiency. A fast and\nrobust bootstrap procedure is developed to obtain robust inference for these\nestimators. Confidence intervals for the model parameters as well as hypothesis\ntests for linear restrictions of the regression coefficients in seemingly\nunrelated regression models are constructed. Moreover, in order to evaluate the\nneed for a seemingly unrelated regression model, a robust procedure is proposed\nto test for the presence of correlation among the disturbances. The performance\nof the fast and robust bootstrap inference is evaluated empirically in\nsimulation studies and illustrated on real data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 10:01:53 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 15:05:49 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 12:20:30 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Peremans", "Kris", ""], ["Van Aelst", "Stefan", ""]]}, {"id": "1801.04925", "submitter": "Joni Virta", "authors": "Markus Matilainen, Klaus Nordhausen, Joni Virta", "title": "On the number of signals in multivariate time series", "comments": null, "journal-ref": "Lecture Notes in Computer Science 2018, Vol 10891, pp 248-258", "doi": "10.1007/978-3-319-93764-9_24", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assume a second-order source separation model where the observed\nmultivariate time series is a linear mixture of latent, temporally uncorrelated\ntime series with some components pure white noise. To avoid the modelling of\nnoise, we extract the non-noise latent components using some standard method,\nallowing the modelling of the extracted univariate time series individually. An\nimportant question is the determination of which of the latent components are\nof interest in modelling and which can be considered as noise. Bootstrap-based\nmethods have recently been used in determining the latent dimension in various\nmethods of unsupervised and supervised dimension reduction and we propose a set\nof similar estimation strategies for second-order stationary time series.\nSimulation studies and a sound wave example are used to show the method's\neffectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 10:25:58 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Matilainen", "Markus", ""], ["Nordhausen", "Klaus", ""], ["Virta", "Joni", ""]]}, {"id": "1801.04978", "submitter": "Ian Dryden", "authors": "Kwang-Rae Kim, Ian L. Dryden, Huiling Le and Katie E. Severn", "title": "Smoothing splines on Riemannian manifolds, with applications to 3D shape\n  space", "comments": "35 pages, 12 figures. Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been increasing interest in statistical analysis of data lying in\nmanifolds. This paper generalizes a smoothing spline fitting method to\nRiemannian manifold data based on the technique of unrolling and unwrapping\noriginally proposed in Jupp and Kent (1987) for spherical data. In particular\nwe develop such a fitting procedure for shapes of configurations in general\n$m$-dimensional Euclidean space, extending our previous work for two\ndimensional shapes. We show that parallel transport along a geodesic on Kendall\nshape space is linked to the solution of a homogeneous first-order differential\nequation, some of whose coefficients are implicitly defined functions. This\nfinding enables us to approximate the procedure of unrolling and unwrapping by\nsimultaneously solving such equations numerically, and so to find numerical\nsolutions for smoothing splines fitted to higher dimensional shape data. This\nfitting method is applied to the analysis of some dynamic 3D peptide data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 20:15:36 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 11:15:39 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 10:25:05 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Kim", "Kwang-Rae", ""], ["Dryden", "Ian L.", ""], ["Le", "Huiling", ""], ["Severn", "Katie E.", ""]]}, {"id": "1801.05007", "submitter": "Qi Liu", "authors": "Qi Liu, Anindya Bhadra, and William S. Cleveland", "title": "Divide and Recombine for Large and Complex Data: Model Likelihood\n  Functions using MCMC", "comments": "6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Divide & Recombine (D&R), big data are divided into subsets, each analytic\nmethod is applied to subsets, and the outputs are recombined. This enables deep\nanalysis and practical computational performance. An innovate D\\&R procedure is\nproposed to compute likelihood functions of data-model (DM) parameters for big\ndata. The likelihood-model (LM) is a parametric probability density function of\nthe DM parameters. The density parameters are estimated by fitting the density\nto MCMC draws from each subset DM likelihood function, and then the fitted\ndensities are recombined. The procedure is illustrated using normal and\nskew-normal LMs for the logistic regression DM.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 20:53:52 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Liu", "Qi", ""], ["Bhadra", "Anindya", ""], ["Cleveland", "William S.", ""]]}, {"id": "1801.05029", "submitter": "Fatih Dikbas", "authors": "Fatih Dikbas", "title": "Compositional Correlation for Detecting Real Associations Among Time\n  Series", "comments": "The submitted pdf file contains the main manuscipt (30 pages) and the\n  Dropbox link to the supplementary pdf file containing the Python code of the\n  CompCorr software and the supplementary figures and tables (1808 pages) is\n  provided at the end of the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Correlation remains to be one of the most widely used statistical tools for\nassessing the strength of relationships between data series. This paper\npresents a novel compositional correlation method for detecting linear and\nnonlinear relationships by considering the averages of all parts of all\npossible compositions of the data series instead of considering the averages of\nthe whole series. The approach enables cumulative contribution of all local\nassociations to the resulting correlation value. The method is applied on two\ndifferent datasets: a set of four simple nonlinear polynomial functions and the\nexpression time series data of 4381 budding yeast (saccharomyces cerevisiae)\ngenes. The obtained results show that the introduced compositional correlation\nmethod is capable of determining real direct and inverse linear, nonlinear and\nmonotonic relationships. Comparisons with Pearson's correlation, Spearman's\ncorrelation, distance correlation and the simulated annealing genetic algorithm\nmaximal information coefficient (SGMIC) have shown that the presented method is\ncapable of detecting important associations which were not detected by the\ncompared methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 12:29:22 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Dikbas", "Fatih", ""]]}, {"id": "1801.05041", "submitter": "Jiaying Gu", "authors": "Jiaying Gu and Stanislav Volgushev", "title": "Panel Data Quantile Regression with Grouped Fixed Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces estimation methods for grouped latent heterogeneity in\npanel data quantile regression. We assume that the observed individuals come\nfrom a heterogeneous population with a finite number of types. The number of\ntypes and group membership is not assumed to be known in advance and is\nestimated by means of a convex optimization problem. We provide conditions\nunder which group membership is estimated consistently and establish asymptotic\nnormality of the resulting estimators. Simulations show that the method works\nwell in finite samples when T is reasonably large. To illustrate the proposed\nmethodology we study the effects of the adoption of Right-to-Carry concealed\nweapon laws on violent crime rates using panel data of 51 U.S. states from 1977\n- 2010.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 21:49:20 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 21:27:04 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Gu", "Jiaying", ""], ["Volgushev", "Stanislav", ""]]}, {"id": "1801.05048", "submitter": "Antonio Lijoi", "authors": "Federico Camerlenghi and David B. Dunson and Antonio Lijoi and Igor\n  Pr\\\"unster and Abel Rodr\\'iguez", "title": "Latent nested nonparametric priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete random structures are important tools in Bayesian nonparametrics and\nthe resulting models have proven effective in density estimation, clustering,\ntopic modeling and prediction, among others. In this paper, we consider nested\nprocesses and study the dependence structures they induce. Dependence ranges\nbetween homogeneity, corresponding to full exchangeability, and maximum\nheterogeneity, corresponding to (unconditional) independence across samples.\nThe popular nested Dirichlet process is shown to degenerate to the fully\nexchangeable case when there are ties across samples at the observed or latent\nlevel. To overcome this drawback, inherent to nesting general discrete random\nmeasures, we introduce a novel class of latent nested processes. These are\nobtained by adding common and group-specific completely random measures and,\nthen, normalising to yield dependent random probability measures. We provide\nresults on the partition distributions induced by latent nested processes, and\ndevelop an Markov Chain Monte Carlo sampler for Bayesian inferences. A test for\ndistributional homogeneity across groups is obtained as a by product. The\nresults and their inferential implications are showcased on synthetic and real\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 22:12:52 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Camerlenghi", "Federico", ""], ["Dunson", "David B.", ""], ["Lijoi", "Antonio", ""], ["Pr\u00fcnster", "Igor", ""], ["Rodr\u00edguez", "Abel", ""]]}, {"id": "1801.05108", "submitter": "Matt Wand Professor", "authors": "Wilson Y. Chen and Matt P. Wand", "title": "Factor graph fragmentization of expectation propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation propagation is a general approach to fast approximate inference\nfor graphical models. The existing literature treats models separately when it\ncomes to deriving and coding expectation propagation inference algorithms. This\ncomes at the cost of similar, long-winded algebraic steps being repeated and\nslowing down algorithmic development. We demonstrate how factor graph\nfragmentization can overcome this impediment. This involves adoption of the\nmessage passing on a factor graph approach to expectation propagation and\nidentification of factor graph sub-graphs, which we call fragments, that are\ncommon to wide classes of models. Key fragments and their corresponding\nmessages are catalogued which means that their algebra does not need to be\nrepeated. This allows compartmentalization of coding and efficient software\ndevelopment.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 03:34:45 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Chen", "Wilson Y.", ""], ["Wand", "Matt P.", ""]]}, {"id": "1801.05242", "submitter": "Jon Cockayne", "authors": "Jon Cockayne and Chris Oates and Ilse Ipsen and Mark Girolami", "title": "A Bayesian Conjugate Gradient Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental task in numerical computation is the solution of large linear\nsystems. The conjugate gradient method is an iterative method which offers\nrapid convergence to the solution, particularly when an effective\npreconditioner is employed. However, for more challenging systems a substantial\nerror can be present even after many iterations have been performed. The\nestimates obtained in this case are of little value unless further information\ncan be provided about the numerical error. In this paper we propose a novel\nstatistical model for this numerical error set in a Bayesian framework. Our\napproach is a strict generalisation of the conjugate gradient method, which is\nrecovered as the posterior mean for a particular choice of prior. The estimates\nobtained are analysed with Krylov subspace methods and a contraction result for\nthe posterior is presented. The method is then analysed in a simulation study\nas well as being applied to a challenging problem in medical imaging.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 13:18:11 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 13:35:28 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 10:15:36 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Cockayne", "Jon", ""], ["Oates", "Chris", ""], ["Ipsen", "Ilse", ""], ["Girolami", "Mark", ""]]}, {"id": "1801.05244", "submitter": "Silvia Polettini", "authors": "Cinzia Carota, Maurizio Filippone, Silvia Polettini", "title": "Assessing Bayesian Nonparametric Log-Linear Models: an application to\n  Disclosure Risk estimation", "comments": "32 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for identification of models with good predictive\nperformances in the family of Bayesian log-linear mixed models with Dirichlet\nprocess random effects. Such a problem arises in many different applications;\nhere we consider it in the context of disclosure risk estimation, an\nincreasingly relevant issue raised by the increasing demand for data collected\nunder a pledge of confidentiality. Two different criteria are proposed and\njointly used via a two-stage selection procedure, in a M-open view. The first\nstage is devoted to identifying a path of search; then, at the second, a small\nnumber of nonparametric models is evaluated through an application-specific\nscore based Bayesian information criterion. We test our method on a variety of\ncontingency tables based on microdata samples from the US Census Bureau and the\nItalian National Security Administration, treated here as populations, and\ncarefully discuss its features. This leads us to a journey around different\nforms and sources of bias along which we show that (i) while based on the so\ncalled \"score+search\" paradigm, our method is by construction well protected\nfrom the selection-induced bias, and (ii) models with good performances are\ninvariably characterized by an extraordinarily simple structure of fixed\neffects. The complexity of model selection - a very challenging and difficult\ntask in a strictly parametric context with large and sparse tables - is\ntherefore significantly defused by our approach. An attractive collateral\nresult of our analysis are fruitful new ideas about modeling in small area\nestimation problems, where interest is in total counts over cells with a small\nnumber of observations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 13:20:31 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Carota", "Cinzia", ""], ["Filippone", "Maurizio", ""], ["Polettini", "Silvia", ""]]}, {"id": "1801.05695", "submitter": "Carles Bret\\'o", "authors": "Carles Bret\\'o and Edward L. Ionides and Aaron A. King", "title": "Panel data analysis via mechanistic models", "comments": "Accepted for publication in Journal of the American Statistical\n  Association", "journal-ref": null, "doi": "10.1080/01621459.2019.1604367", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panel data, also known as longitudinal data, consist of a collection of time\nseries. Each time series, which could itself be multivariate, comprises a\nsequence of measurements taken on a distinct unit. Mechanistic modeling\ninvolves writing down scientifically motivated equations describing the\ncollection of dynamic systems giving rise to the observations on each unit. A\ndefining characteristic of panel systems is that the dynamic interaction\nbetween units should be negligible. Panel models therefore consist of a\ncollection of independent stochastic processes, generally linked through shared\nparameters while also having unit-specific parameters. To give the scientist\nflexibility in model specification, we are motivated to develop a framework for\ninference on panel data permitting the consideration of arbitrary nonlinear,\npartially observed panel models. We build on iterated filtering techniques that\nprovide likelihood-based inference on nonlinear partially observed Markov\nprocess models for time series data. Our methodology depends on the latent\nMarkov process only through simulation; this plug-and-play property ensures\napplicability to a large class of models. We demonstrate our methodology on a\ntoy example and two epidemiological case studies. We address inferential and\ncomputational issues arising due to the combination of model complexity and\ndataset size.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 14:54:50 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 11:56:37 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bret\u00f3", "Carles", ""], ["Ionides", "Edward L.", ""], ["King", "Aaron A.", ""]]}, {"id": "1801.05725", "submitter": "Donald Williams Mr.", "authors": "Donald R. Williams, Juho Piironen, Aki Vehtari and Philippe Rast", "title": "Bayesian Estimation of Gaussian Graphical Models with Predictive\n  Covariance Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian graphical models are used for determining conditional relationships\nbetween variables. This is accomplished by identifying off-diagonal elements in\nthe inverse-covariance matrix that are non-zero. When the ratio of variables\n(p) to observations (n) approaches one, the maximum likelihood estimator of the\ncovariance matrix becomes unstable and requires shrinkage estimation. Whereas\nseveral classical (frequentist) methods have been introduced to address this\nissue, fully Bayesian methods remain relatively uncommon in practice and\nmethodological literatures. Here we introduce a Bayesian method for estimating\nsparse matrices, in which conditional relationships are determined with\nprojection predictive selection. With this method, that uses Kullback-Leibler\ndivergence and cross-validation for neighborhood selection, we reconstruct the\ninverse-covariance matrix in both low and high-dimensional settings. Through\nsimulation and applied examples, we characterized performance compared to\nseveral Bayesian methods and the graphical lasso, in addition to TIGER that\nsimilarly estimates the inverse-covariance matrix with regression. Our results\ndemonstrate that projection predictive selection not only has superior\nperformance compared to selecting the most probable model and Bayesian model\naveraging, particularly for high-dimensional data, but also compared to the the\nBayesian and classical glasso methods. Further, we show that estimating the\ninverse-covariance matrix with multiple regression is often more accurate, with\nrespect to various loss functions, and efficient than direct estimation. In\nlow-dimensional settings, we demonstrate that projection predictive selection\nalso provides competitive performance. We have implemented the projection\npredictive method for covariance selection in the R package GGMprojpred\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 16:06:12 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 14:57:17 GMT"}, {"version": "v3", "created": "Fri, 6 Apr 2018 02:40:23 GMT"}, {"version": "v4", "created": "Tue, 31 Jul 2018 15:40:31 GMT"}, {"version": "v5", "created": "Sat, 4 Aug 2018 20:00:05 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Williams", "Donald R.", ""], ["Piironen", "Juho", ""], ["Vehtari", "Aki", ""], ["Rast", "Philippe", ""]]}, {"id": "1801.05832", "submitter": "Renato J Cintra", "authors": "D. F. G. Coelho, R. J. Cintra, V. S. Dimitrov", "title": "Efficient Computation of the 8-point DCT via Summation by Parts", "comments": "Fixed Fig. 1 with the block diagram of the proposed architecture.\n  Manuscript contains 13 pages, 4 figures, 2 tables", "journal-ref": "J Sign Process Syst (2017)", "doi": "10.1007/s11265-017-1270-6", "report-no": null, "categories": "cs.DS cs.NA math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new fast algorithm for the 8-point discrete cosine\ntransform (DCT) based on the summation-by-parts formula. The proposed method\nconverts the DCT matrix into an alternative transformation matrix that can be\ndecomposed into sparse matrices of low multiplicative complexity. The method is\ncapable of scaled and exact DCT computation and its associated fast algorithm\nachieves the theoretical minimal multiplicative complexity for the 8-point DCT.\nDepending on the nature of the input signal simplifications can be introduced\nand the overall complexity of the proposed algorithm can be further reduced.\nSeveral types of input signal are analyzed: arbitrary, null mean, accumulated,\nand null mean/accumulated signal. The proposed tool has potential application\nin harmonic detection, image enhancement, and feature extraction, where input\nsignal DC level is discarded and/or the signal is required to be integrated.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 19:21:15 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 21:43:07 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Coelho", "D. F. G.", ""], ["Cintra", "R. J.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1801.05859", "submitter": "Helio M. de Oliveira", "authors": "H. M. de Oliveira, R. J. Cintra, R. C. de Oliveira", "title": "A Kotel'nikov Representation for Wavelets", "comments": "6 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA eess.AS eess.SP math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a wavelet representation using baseband signals, by\nexploiting Kotel'nikov results. Details of how to obtain the processes of\nenvelope and phase at low frequency are shown. The archetypal interpretation of\nwavelets as an analysis with a filter bank of constant quality factor is\nrevisited on these bases. It is shown that if the wavelet spectral support is\nlimited into the band $[f_m,f_M]$, then an orthogonal analysis is guaranteed\nprovided that $f_M \\leq 3f_m$, a quite simple result, but that invokes some\nparallel with the Nyquist rate. Nevertheless, in cases of orthogonal wavelets\nwhose spectrum does not verify this condition, it is shown how to construct an\n\"equivalent\" filter bank with no spectral overlapping.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 20:33:16 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["de Oliveira", "H. M.", ""], ["Cintra", "R. J.", ""], ["de Oliveira", "R. C.", ""]]}, {"id": "1801.05861", "submitter": "Yiou Li", "authors": "Yiou Li, Xinwei Deng", "title": "An Efficient Algorithm for Elastic I-optimal Design of Generalized\n  Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized linear models (GLMs) are widely used in statistical analysis\nand the related design issues are undoubtedly challenging. The state-of-the-art\nworks mostly apply to design criteria on the estimates of regression\ncoefficients. The prediction accuracy is usually critical in modern decision\nmaking and artificial intelligence applications. It is of importance to study\noptimal designs from the prediction aspects for generalized linear models. In\nthis work, we consider the Elastic I-optimality as a prediction-oriented design\ncriterion for generalized linear models, and develop efficient algorithms for\nsuch $\\text{EI}$-optimal designs. By investigating theoretical properties for\nthe optimal weights of any set of design points and extending the general\nequivalence theorem to the $\\text{EI}$-optimality for GLMs, the proposed\nefficient algorithm adequately combines the Fedorov-Wynn algorithm and\nmultiplicative algorithm. It achieves great computational efficiency with\nguaranteed convergence. Numerical examples are conducted to evaluate the\nfeasibility and computational efficiency of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 20:52:41 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 18:28:44 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 10:50:12 GMT"}, {"version": "v4", "created": "Mon, 20 Apr 2020 10:04:27 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Li", "Yiou", ""], ["Deng", "Xinwei", ""]]}, {"id": "1801.05913", "submitter": "Matthew Goodman", "authors": "Matthew Goodman, Lori Chibnik, Tianxi Cai", "title": "Variance Components Genetic Association Test for Zero-inflated Count\n  Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonly in biomedical research, studies collect data in which an outcome\nmeasure contains informative excess zeros; for example when observing the\nburden of neuritic plaques in brain pathology studies, those who show none\ncontribute to our understanding of neurodegenerative disease. The outcome may\nbe characterized by a mixture distribution with one component being the\n`structural zero' and the other component being a Poisson distribution. We\npropose a novel variance components score test of genetic association between a\nset of genetic markers and a zero-inflated count outcome from a mixture\ndistribution. This test shares advantageous properties with SNP-set tests which\nhave been previously devised for standard continuous or binary outcomes, such\nas the Sequence Kernel Association Test (SKAT). In particular, our method has\nsuperior statistical power compared to competing methods, especially when there\nis correlation within the group of markers, and when the SNPs are associated\nwith both the mixing proportion and the rate of the Poisson distribution. We\napply the method to Alzheimer's data from the Rush University Religious Orders\nStudy and Memory and Aging Project (ROSMAP), where as proof of principle we\nfind highly significant associations with the APOE gene, in both the\n`structural zero' and `count' parameters, when applied to a zero-inflated\nneuritic plaques count outcome.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 02:47:12 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Goodman", "Matthew", ""], ["Chibnik", "Lori", ""], ["Cai", "Tianxi", ""]]}, {"id": "1801.06069", "submitter": "Johan Steen", "authors": "Johan Steen, Stijn Vansteelandt", "title": "Graphical models for mediation analysis", "comments": null, "journal-ref": null, "doi": "10.1201/9780429463976-17", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis seeks to infer how much of the effect of an exposure on an\noutcome can be attributed to specific pathways via intermediate variables or\nmediators. This requires identification of so-called path-specific effects.\nThese express how a change in exposure affects those intermediate variables\n(along certain pathways), and how the resulting changes in those variables in\nturn affect the outcome (along subsequent pathways). However, unlike\nidentification of total effects, adjustment for confounding is insufficient for\nidentification of path-specific effects because their magnitude is also\ndetermined by the extent to which individuals who experience large exposure\neffects on the mediator, tend to experience relatively small or large mediator\neffects on the outcome. This chapter therefore provides an accessible review of\nidentification strategies under general nonparametric structural equation\nmodels (with possibly unmeasured variables), which rule out certain such\ndependencies. In particular, it is shown which path-specific effects can be\nidentified under such models, and how this can be done.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 14:52:56 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Steen", "Johan", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "1801.06229", "submitter": "Dominik Rothenh\\\"ausler", "authors": "Dominik Rothenh\\\"ausler, Nicolai Meinshausen, Peter B\\\"uhlmann, Jonas\n  Peters", "title": "Anchor regression: heterogeneous data meets causality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting a response variable from a set of\ncovariates on a data set that differs in distribution from the training data.\nCausal parameters are optimal in terms of predictive accuracy if in the new\ndistribution either many variables are affected by interventions or only some\nvariables are affected, but the perturbations are strong. If the training and\ntest distributions differ by a shift, causal parameters might be too\nconservative to perform well on the above task. This motivates anchor\nregression, a method that makes use of exogeneous variables to solve a\nrelaxation of the causal minimax problem by considering a modification of the\nleast-squares loss. The procedure naturally provides an interpolation between\nthe solutions of ordinary least squares and two-stage least squares. We prove\nthat the estimator satisfies predictive guarantees in terms of distributional\nrobustness against shifts in a linear class; these guarantees are valid even if\nthe instrumental variables assumptions are violated. If anchor regression and\nleast squares provide the same answer (anchor stability), we establish that OLS\nparameters are invariant under certain distributional changes. Anchor\nregression is shown empirically to improve replicability and protect against\ndistributional shifts.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 20:32:09 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 15:05:09 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 22:14:23 GMT"}, {"version": "v4", "created": "Fri, 8 May 2020 18:50:04 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Rothenh\u00e4usler", "Dominik", ""], ["Meinshausen", "Nicolai", ""], ["B\u00fchlmann", "Peter", ""], ["Peters", "Jonas", ""]]}, {"id": "1801.06282", "submitter": "Bo Ning", "authors": "Bo Ning, Subhashis Ghosal and Jewell Thomas", "title": "Bayesian method for causal inference in spatially-correlated\n  multivariate time series", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Measuring the causal impact of an advertising campaign on sales is an\nessential task for advertising companies. Challenges arise when companies run\nadvertising campaigns in multiple stores which are spatially correlated, and\nwhen the sales data have a low signal-to-noise ratio which makes the\nadvertising effects hard to detect. This paper proposes a solution to address\nboth of these challenges. A novel Bayesian method is proposed to detect weaker\nimpacts and a multivariate structural time series model is used to capture the\nspatial correlation between stores through placing a $\\mathcal{G}$-Wishart\nprior on the precision matrix. The new method is to compare two posterior\ndistributions of a latent variable---one obtained by using the observed data\nfrom the test stores and the other one obtained by using the data from their\ncounterfactual potential outcomes. The counterfactual potential outcomes are\nestimated from the data of synthetic controls, each of which is a linear\ncombination of sales figures at many control stores over the causal period.\nControl stores are selected using a revised Expectation-Maximization variable\nselection (EMVS) method. A two-stage algorithm is proposed to estimate the\nparameters of the model. To prevent the prediction intervals from being\nexplosive, a stationarity constraint is imposed on the local linear trend of\nthe model through a recently proposed prior. The benefit of using this prior is\ndiscussed in this paper. A detailed simulation study shows the effectiveness of\nusing our proposed method to detect weaker causal impact. The new method is\napplied to measure the causal effect of an advertising campaign for a consumer\nproduct sold at stores of a large national retail chain.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 03:14:48 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 02:24:47 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 17:38:09 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Ning", "Bo", ""], ["Ghosal", "Subhashis", ""], ["Thomas", "Jewell", ""]]}, {"id": "1801.06296", "submitter": "Rico Krueger", "authors": "Rico Krueger, Akshay Vij, Taha H. Rashidi", "title": "A Dirichlet Process Mixture Model of Discrete Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mixed multinomial logit (MNL) model, which leverages the\ntruncated stick-breaking process representation of the Dirichlet process as a\nflexible nonparametric mixing distribution. The proposed model is a Dirichlet\nprocess mixture model and accommodates discrete representations of\nheterogeneity, like a latent class MNL model. Yet, unlike a latent class MNL\nmodel, the proposed discrete choice model does not require the analyst to fix\nthe number of mixture components prior to estimation, as the complexity of the\ndiscrete mixing distribution is inferred from the evidence. For posterior\ninference in the proposed Dirichlet process mixture model of discrete choice,\nwe derive an expectation maximisation algorithm. In a simulation study, we\ndemonstrate that the proposed model framework can flexibly capture\ndifferently-shaped taste parameter distributions. Furthermore, we empirically\nvalidate the model framework in a case study on motorists' route choice\npreferences and find that the proposed Dirichlet process mixture model of\ndiscrete choice outperforms a latent class MNL model and mixed MNL models with\ncommon parametric mixing distributions in terms of both in-sample fit and\nout-of-sample predictive ability. Compared to extant modelling approaches, the\nproposed discrete choice model substantially abbreviates specification\nsearches, as it relies on less restrictive parametric assumptions and does not\nrequire the analyst to specify the complexity of the discrete mixing\ndistribution prior to estimation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 05:12:16 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Krueger", "Rico", ""], ["Vij", "Akshay", ""], ["Rashidi", "Taha H.", ""]]}, {"id": "1801.06327", "submitter": "Masahiro Tanaka", "authors": "Masahiro Tanaka", "title": "Bayesian Inference of Local Projections with Roughness Penalty Priors", "comments": "Forthcoming in Computational Economics", "journal-ref": "Computational Economics 55, 629-651 (2020)", "doi": "10.1007/s10614-019-09905-y", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A local projection is a statistical framework that accounts for the\nrelationship between an exogenous variable and an endogenous variable, measured\nat different time points. Local projections are often applied in impulse\nresponse analyses and direct forecasting. While local projections are becoming\nincreasingly popular because of their robustness to misspecification and their\nflexibility, they are less statistically efficient than standard methods, such\nas vector autoregression. In this study, we seek to improve the statistical\nefficiency of local projections by developing a fully Bayesian approach that\ncan be used to estimate local projections using roughness penalty priors. By\nincorporating such prior-induced smoothness, we can use information contained\nin successive observations to enhance the statistical efficiency of an\ninference. We apply the proposed approach to an analysis of monetary policy in\nthe United States, showing that the roughness penalty priors successfully\nestimate the impulse response functions and improve the predictive accuracy of\nlocal projections.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 07:53:51 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 11:47:00 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 06:30:03 GMT"}, {"version": "v4", "created": "Thu, 27 Jun 2019 11:46:21 GMT"}, {"version": "v5", "created": "Sat, 13 Jul 2019 07:05:51 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Tanaka", "Masahiro", ""]]}, {"id": "1801.06375", "submitter": "Robson Machado", "authors": "Robson J. M. Machado, Ardo van den Hout, Giampiero Marra", "title": "Penalised maximum likelihood estimation in multistate models for\n  interval-censored data", "comments": "20 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multistate models can be used to describe transitions over time across\nstates. In the presence of interval-censored times for transitions, the\nlikelihood is constructed using transition probabilities. Models are specified\nusing proportional hazards model for the transitions. Time-dependency is\nusually defined by parametric models, which can be too restrictive.\nNonparametric hazards specification with splines allow for flexible modelling\nof time-dependency without making strong model assumptions. Penalised maximum\nlikelihood is used to estimate the models. Selecting the optimal amount of\nsmoothing is challenging as the problem involves multiple penalties. We propose\nan automatic and efficient method to estimate multistate models with splines in\nthe presence of interval-censoring. The method is illustrated with a data\nanalysis and a simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 11:55:22 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Machado", "Robson J. M.", ""], ["Hout", "Ardo van den", ""], ["Marra", "Giampiero", ""]]}, {"id": "1801.06477", "submitter": "Minh-Lien Jeanne Nguyen", "authors": "Minh-Lien Jeanne Nguyen (LMO)", "title": "Nonparametric method for space conditional density estimation in\n  moderately large dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating a conditional density in\nmoderately large dimensions. Much more informative than regression functions,\nconditional densities are of main interest in recent methods, particularly in\nthe Bayesian framework (studying the posterior distribution, finding its\nmodes...). Considering a recently studied family of kernel estimators, we\nselect a pointwise multivariate bandwidth by revisiting the greedy algorithm\nRodeo (Regularisation Of Derivative Expectation Operator). The method addresses\nseveral issues: being greedy and computationally efficient by an iterative\nprocedure, avoiding the curse of high dimensionality under some suitably\ndefined sparsity conditions by early variable selection during the procedure,\nconverging at a quasi-optimal minimax rate.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 16:08:18 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Nguyen", "Minh-Lien Jeanne", "", "LMO"]]}, {"id": "1801.06532", "submitter": "Tung-Lung Wu", "authors": "Tung-Lung Wu", "title": "Distribution-free runs-based control charts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose distribution-free runs-based control charts for detecting location\nshifts. Using the fact that given the number of total successes, the outcomes\nof a sequence of Bernoulli trials are random permutations, we are able to\ncontrol the conditional probability of a signal detected at current time given\nthat there is not alarm before at a pre-determined level. This leads to a\ndesired in-control average run length and data-dependent control limits. Two\ncommon runs statistics, the longest run statistic and the scan statitsic, are\nstudied in detail and their exact conditional distributions given the number of\ntotal successes are obtained using the finite Markov chain imbedding technique.\nNumerical results are given to evaluate the performance of the proposed control\ncharts.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 18:52:52 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Wu", "Tung-Lung", ""]]}, {"id": "1801.06533", "submitter": "Khalifa Es-Sebaiy", "authors": "Azzouz Dermoune, Khalifa Es-Sebaiy, Mohammed Es.Sebaiy and Jabrane\n  Moustaaid", "title": "Parametrizations, weights, and optimal prediction: Part 1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of the annual mean temperature prediction. The years\ntaken into account and the corresponding annual mean temperatures are denoted\nby $0,\\ldots, n$ and $t_0$, $\\ldots$, $t_n$, respectively. We propose to\npredict the temperature $t_{n+1}$ using the data $t_0$, $\\ldots$, $t_n$. For\neach $0\\leq l\\leq n$ and each parametrization $\\Theta^{(l)}$ of the Euclidean\nspace $\\mathbb{R}^{l+1}$ we construct a list of weights for the data\n$\\{t_0,\\ldots, t_l\\}$ based on the rows of $\\Theta^{(l)}$ which are correlated\nwith the constant trend. Using these weights we define a list of predictors of\n$t_{l+1}$ from the data $t_0$, $\\ldots$, $t_l$. We analyse how the\nparametrization affects the prediction, and provide three optimality criteria\nfor the selection of weights and parametrization. We illustrate our results for\nthe annual mean temperature of France and Morocco.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 23:25:55 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Dermoune", "Azzouz", ""], ["Es-Sebaiy", "Khalifa", ""], ["Sebaiy", "Mohammed Es.", ""], ["Moustaaid", "Jabrane", ""]]}, {"id": "1801.06594", "submitter": "Joanne Beer", "authors": "Joanne C. Beer, Howard J. Aizenstein, Stewart J. Anderson, and Robert\n  T. Krafty", "title": "Incorporating Prior Information with Fused Sparse Group Lasso:\n  Application to Prediction of Clinical Measures from Neuroimages", "comments": "36 pages, 6 figures; expanded author's footnote; revised simulation\n  study results (Figures 2 to 4, Table 2, conclusions unchanged); revised ABIDE\n  Application results (Table 3, Figure 6, conclusions unchanged)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting clinical variables from whole-brain neuroimages is a high\ndimensional problem that requires some type of feature selection or extraction.\nPenalized regression is a popular embedded feature selection method for high\ndimensional data. For neuroimaging applications, spatial regularization using\nthe $\\ell_1$ or $\\ell_2$ norm of the image gradient has shown good performance,\nyielding smooth solutions in spatially contiguous brain regions. However,\nrecently enormous resources have been devoted to establishing structural and\nfunctional brain connectivity networks that can be used to define spatially\ndistributed yet related groups of voxels. We propose using the fused sparse\ngroup lasso penalty to encourage structured, sparse, interpretable solutions by\nincorporating prior information about spatial and group structure among voxels.\nWe present optimization steps for fused sparse group lasso penalized regression\nusing the alternating direction method of multipliers algorithm. With\nsimulation studies and in application to real fMRI data from the Autism Brain\nImaging Data Exchange, we demonstrate conditions under which fusion and group\npenalty terms together outperform either of them alone. Supplementary materials\nfor this article are available online.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 23:03:34 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 18:22:12 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 21:59:03 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Beer", "Joanne C.", ""], ["Aizenstein", "Howard J.", ""], ["Anderson", "Stewart J.", ""], ["Krafty", "Robert T.", ""]]}, {"id": "1801.06739", "submitter": "Daniel Farewell", "authors": "Daniel Farewell, Rhian Daniel, Shaun Seaman", "title": "Missing at random: a stochastic process perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer a natural and extensible measure-theoretic treatment of missingness\nat random. Within the standard missing data framework, we give a novel\ncharacterisation of the observed data as a stopping-set sigma algebra. We\ndemonstrate that the usual missingness at random conditions are equivalent to\nrequiring particular stochastic processes to be adapted to a set-indexed\nfiltration of the complete data: measurability conditions that suffice to\nensure the likelihood factorisation necessary for ignorability. Our rigorous\nstatement of the missing at random conditions also clarifies a common\nconfusion: what is fixed, and what is random?\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 23:08:26 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Farewell", "Daniel", ""], ["Daniel", "Rhian", ""], ["Seaman", "Shaun", ""]]}, {"id": "1801.06768", "submitter": "Xun Huan", "authors": "Khachik Sargsyan, Xun Huan, Habib N. Najm", "title": "Embedded Model Error Representation for Bayesian Model Calibration", "comments": "Preprint 34 pages, 13 figures; v1 submitted on January 19, 2018; v2\n  submitted on February 5, 2019. v2 changes: addition of various clarifications\n  and references, and minor language edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.comp-ph physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model error estimation remains one of the key challenges in uncertainty\nquantification and predictive science. For computational models of complex\nphysical systems, model error, also known as structural error or model\ninadequacy, is often the largest contributor to the overall predictive\nuncertainty. This work builds on a recently developed framework of embedded,\ninternal model correction, in order to represent and quantify structural\nerrors, together with model parameters, within a Bayesian inference context. We\nfocus specifically on a Polynomial Chaos representation with additive\nmodification of existing model parameters, enabling a non-intrusive procedure\nfor efficient approximate likelihood construction, model error estimation, and\ndisambiguation of model and data errors' contributions to predictive\nuncertainty. The framework is demonstrated on several synthetic examples, as\nwell as on a chemical ignition problem.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 04:57:24 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 17:18:26 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Sargsyan", "Khachik", ""], ["Huan", "Xun", ""], ["Najm", "Habib N.", ""]]}, {"id": "1801.07012", "submitter": "Jocelyn Chauvet", "authors": "Jocelyn Chauvet (IMAG), Catherine Trottier (IMAG, UM3), Xavier Bry\n  (IMAG), Fr\\'ed\\'eric Mortier", "title": "Extension de la r\\'egression lin\\'eaire g\\'en\\'eralis\\'ee sur\n  composantes supervis\\'ees (SCGLR) aux donn\\'ees group\\'ees", "comments": "in French", "journal-ref": "48\\`emes Journ\\'ees de Statistique de la SFdS, May 2016,\n  Montpellier, France", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address component-based regularisation of a multivariate Generalized\nLinear Mixed Model. A set of random responses Y is modelled by a GLMM, using a\nset X of explanatory variables and a set T of additional covariates. Variables\nin X are assumed many and redundant: generalized linear mixed regression\ndemands regularisation with respect to X. By contrast, variables in T are\nassumed few and selected so as to demand no regularisation. Regularisation is\nperformed building an appropriate number of orthogonal components that both\ncontribute to model Y and capture relevant structural information in X. We\npropose to optimize a SCGLR-specific criterion within a Schall's algorithm in\norder to estimate the model. This extension of SCGLR is tested on simulated and\nreal data, and compared to Ridge-and Lasso-based regularisations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 09:53:13 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Chauvet", "Jocelyn", "", "IMAG"], ["Trottier", "Catherine", "", "IMAG, UM3"], ["Bry", "Xavier", "", "IMAG"], ["Mortier", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1801.07063", "submitter": "Vincent Vandewalle", "authors": "Matthieu Marbac and Vincent Vandewalle", "title": "A tractable Multi-Partitions Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of model-based clustering, a model allowing several latent\nclass variables is proposed. This model assumes that the distribution of the\nobserved data can be factorized into several independent blocks of variables.\nEach block is assumed to follow a latent class model ({\\it i.e.,} mixture with\nconditional independence assumption). The proposed model includes variable\nselection, as a special case, and is able to cope with the mixed-data setting.\nThe simplicity of the model allows to estimate the repartition of the variables\ninto blocks and the mixture parameters simultaneously, thus avoiding to run EM\nalgorithms for each possible repartition of variables into blocks. For the\nproposed method, a model is defined by the number of blocks, the number of\nclusters inside each block and the repartition of variables into block. Model\nselection can be done with two information criteria, the BIC and the MICL, for\nwhich an efficient optimization is proposed. The performances of the model are\ninvestigated on simulated and real data. It is shown that the proposed method\ngives a rich interpretation of the dataset at hand ({\\it i.e.,} analysis of the\nrepartition of the variables into blocks and analysis of the clusters produced\nby each block of variables).\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 12:24:33 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Marbac", "Matthieu", ""], ["Vandewalle", "Vincent", ""]]}, {"id": "1801.07278", "submitter": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez", "authors": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez, Maria Durban, Dae-Jin Lee and\n  Paul H. C. Eilers", "title": "On the estimation of variance parameters in non-standard generalised\n  linear mixed models: Application to penalised smoothing", "comments": null, "journal-ref": "Statistics and Computing, 2018", "doi": "10.1007/s11222-018-9818-2", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for the estimation of variance parameters in\ngeneralised linear mixed models. The method has its roots in Harville (1977)'s\nwork, but it is able to deal with models that have a precision matrix for the\nrandom-effect vector that is linear in the inverse of the variance parameters\n(i.e., the precision parameters). We call the method SOP (Separation of\nOverlapping Precision matrices). SOP is based on applying the method of\nsuccessive approximations to easy-to-compute estimate updates of the variance\nparameters. These estimate updates have an appealing form: they are the ratio\nof a (weighted) sum of squares to a quantity related to effective degrees of\nfreedom. We provide the sufficient and necessary conditions for these estimates\nto be strictly positive. An important application field of SOP is penalised\nregression estimation of models where multiple quadratic penalties act on the\nsame regression coefficients. We discuss in detail two of those models:\npenalised splines for locally adaptive smoothness and for hierarchical curve\ndata. Several data examples in these settings are presented.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 19:03:02 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 09:48:41 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Rodr\u00edguez-\u00c1lvarez", "Mar\u00eda Xos\u00e9", ""], ["Durban", "Maria", ""], ["Lee", "Dae-Jin", ""], ["Eilers", "Paul H. C.", ""]]}, {"id": "1801.07310", "submitter": "Alexander Volfovsky", "authors": "Panos Toulis and Alexander Volfovsky and Edoardo M. Airoldi", "title": "Propensity score methodology in the presence of network entanglement\n  between treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In experimental design and causal inference, it may happen that the treatment\nis not defined on individual experimental units, but rather on pairs or, more\ngenerally, on groups of units. For example, teachers may choose pairs of\nstudents who do not know each other to teach a new curriculum; regulators might\nallow or disallow merging of firms, and biologists may introduce or inhibit\ninteractions between genes or proteins. In this paper, we formalize this\nexperimental setting, and we refer to the individual treatments in such setting\nas entangled treatments. We then consider the special case where individual\ntreatments depend on a common population quantity, and develop theory and\nmethodology to deal with this case. In our target applications, the common\npopulation quantity is a network, and the individual treatments are defined as\nfunctions of the change in the network between two specific time points. Our\nfocus is on estimating the causal effect of entangled treatments in\nobservational studies where entangled treatments are endogenous and cannot be\ndirectly manipulated. When treatment cannot be manipulated, be it entangled or\nnot, it is necessary to account for the treatment assignment mechanism to avoid\nselection bias, commonly through a propensity score methodology. In this paper,\nwe quantify the extent to which classical propensity score methodology ignores\ntreatment entanglement, and characterize the bias in the estimated causal\neffects. To characterize such bias we introduce a novel similarity function\nbetween propensity score models, and a practical approximation of it, which we\nuse to quantify model misspecification of propensity scores due to\nentanglement. One solution to avoid the bias in the presence of entangled\ntreatments is to model the change in the network, directly, and calculate an\nindividual unit's propensity score by averaging treatment assignments over this\nchange.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 20:38:29 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Toulis", "Panos", ""], ["Volfovsky", "Alexander", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1801.07318", "submitter": "Lorin Crawford", "authors": "Lorin Crawford, Seth R. Flaxman, Daniel E. Runcie, Mike West", "title": "Variable Prioritization in Nonlinear Black Box Methods: A Genetic\n  Association Case Study", "comments": "28 pages, 5 figures, 1 tables; Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The central aim in this paper is to address variable selection questions in\nnonlinear and nonparametric regression. Motivated by statistical genetics,\nwhere nonlinear interactions are of particular interest, we introduce a novel\nand interpretable way to summarize the relative importance of predictor\nvariables. Methodologically, we develop the \"RelATive cEntrality\" (RATE)\nmeasure to prioritize candidate genetic variants that are not just marginally\nimportant, but whose associations also stem from significant covarying\nrelationships with other variants in the data. We illustrate RATE through\nBayesian Gaussian process regression, but the methodological innovations apply\nto other \"black box\" methods. It is known that nonlinear models often exhibit\ngreater predictive accuracy than linear models, particularly for phenotypes\ngenerated by complex genetic architectures. With detailed simulations and two\nreal data association mapping studies, we show that applying RATE enables an\nexplanation for this improved performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 20:57:39 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 23:32:04 GMT"}, {"version": "v3", "created": "Mon, 27 Aug 2018 00:36:45 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Crawford", "Lorin", ""], ["Flaxman", "Seth R.", ""], ["Runcie", "Daniel E.", ""], ["West", "Mike", ""]]}, {"id": "1801.07328", "submitter": "Wendy Chan", "authors": "Wendy Chan", "title": "An Evaluation of Bounding Approaches for Generalization", "comments": "35 pages, 4 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statisticians have recently developed propensity score methods to improve\ngeneralizations from randomized experiments that do not employ random sampling.\nHowever, these methods typically rely on assumptions whose plausibility may be\nquestionable in practice. In this article, we introduce and discuss bounding,\nan approach that is based on alternative assumptions that may be more plausible\nin a given study. The bounding framework nonparametrically estimates population\nparameters using a range of plausible values that are consistent with the\nobserved characteristics of the data. We illustrate how the bounds can be\ntightened using three approaches: imposing an alternative assumption based on\nmonotonicity, redefining the population of inference, and using propensity\nscore stratification. Using the results from two simulation studies, we examine\nthe conditions under which bounds for the population parameter are tightened.\nWe conclude with an application of bounding to SimCalc, a cluster randomized\ntrial that evaluated the effectiveness of a technology aid on mathematics\nachievement.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 21:34:54 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 15:18:15 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Chan", "Wendy", ""]]}, {"id": "1801.07683", "submitter": "Shangsi Wang", "authors": "Shangsi Wang, Cencheng Shen, Alexandra Badea, Carey E. Priebe, Joshua\n  T. Vogelstein", "title": "Signal Subgraph Estimation Via Vertex Screening", "comments": "8 pages excluding appendix, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph classification and regression have wide applications in a variety of\ndomains. A graph is a complex and high-dimensional object, which poses great\nchallenges to traditional machine learning algorithms. Accurately and\nefficiently locating a small signal subgraph dependent on the label of interest\ncan dramatically improve the performance of subsequent statistical inference.\nMoreover, estimating a signal subgraph can aid humans with interpreting these\nresults. We present a vertex screening method to identify the signal subgraph\nwhen given multiple graphs and associated labels. The method utilizes\ndistance-based correlation to screen the vertices, and allows the subsequent\nclassification and regression to be performed on a small induced subgraph. We\ndemonstrate that this method is consistent in recovering signal vertices and\nleads to better classification performance via theory and numerical\nexperiments. We apply the vertex screening algorithm on human and murine graphs\nderived from functional and structural magnetic resonance images to analyze the\nsite effects and sex differences.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 18:08:50 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Wang", "Shangsi", ""], ["Shen", "Cencheng", ""], ["Badea", "Alexandra", ""], ["Priebe", "Carey E.", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1801.07785", "submitter": "Randall Reese", "authors": "Randall Reese, Xiaotian Dai, Guifang Fu", "title": "Strong Sure Screening of Ultra-high Dimensional Data with Interaction\n  Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrahigh dimensional data sets are becoming increasingly prevalent in areas\nsuch as bioinformatics, medical imaging, and social network analysis. Sure\nindependent screening of such data is commonly used to analyze such data.\nNevertheless, few methods exist for screening for interactions among\npredictors. Moreover, extant interaction screening methods prove to be highly\ninaccurate when applied to data sets exhibiting strong interactive effects, but\nweak marginal effects, on the response. We propose a new interaction screening\nprocedure based on joint cumulants which is not inhibited by such limitations.\nUnder a collection of sensible conditions, we demonstrate that our interaction\nscreening procedure has the strong sure screening property. Four simulations\nare used to investigate the performance of our method relative to two other\ninteraction screening methods. We also apply a two-stage analysis to a real\ndata example by first employing our proposed method, and then further examining\na subset of selected covariates using multifactor dimensionality reduction.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 21:51:26 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 00:25:08 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 15:22:27 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Reese", "Randall", ""], ["Dai", "Xiaotian", ""], ["Fu", "Guifang", ""]]}, {"id": "1801.07793", "submitter": "Yeawon Yoo", "authors": "Yeawon Yoo, Adolfo R. Escobedo, and J. Kyle Skolfield", "title": "A New Correlation Coefficient for Aggregating Non-strict and Incomplete\n  Rankings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.CO math.OC stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a correlation coefficient that is designed to deal with a\nvariety of ranking formats including those containing non-strict (i.e.,\nwith-ties) and incomplete (i.e., unknown) preferences. The correlation\ncoefficient is designed to enforce a neutral treatment of incompleteness\nwhereby no assumptions are made about individual preferences involving unranked\nobjects. The new measure, which can be regarded as a generalization of the\nseminal Kendall tau correlation coefficient, is proven to satisfy a set of\nmetric-like axioms and to be equivalent to a recently developed ranking\ndistance function associated with Kemeny aggregation. In an effort to further\nunify and enhance both robust ranking methodologies, this work proves the\nequivalence of an additional distance and correlation-coefficient pairing in\nthe space of non-strict incomplete rankings. These connections induce new exact\noptimization methodologies: a specialized branch and bound algorithm and an\nexact integer programming formulation. Moreover, the bridging of these\ncomplementary theories reinforces the singular suitability of the featured\ncorrelation coefficient to solve the general consensus ranking problem. The\nlatter premise is bolstered by an accompanying set of experiments on random\ninstances, which are generated via a herein developed sampling technique\nconnected with the classic Mallows distribution of ranking data. Associated\nexperiments with the branch and bound algorithm demonstrate that, as data\nbecomes noisier, the featured correlation coefficient yields relatively fewer\nalternative optimal solutions and that the aggregate rankings tend to be closer\nto an underlying ground truth shared by a majority.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 22:14:45 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 09:45:09 GMT"}, {"version": "v3", "created": "Tue, 5 Jun 2018 00:55:49 GMT"}, {"version": "v4", "created": "Sat, 16 Feb 2019 20:38:43 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Yoo", "Yeawon", ""], ["Escobedo", "Adolfo R.", ""], ["Skolfield", "J. Kyle", ""]]}, {"id": "1801.07832", "submitter": "Aaron Ellison", "authors": "Ronny Vallejos and Hannah L Buckley and Bradley S Case and Jonathan\n  Acosta and Aaron M Ellison", "title": "Sensitivity of codispersion to noise and error in ecological and\n  environmental data", "comments": "20 pages, 14 figures", "journal-ref": "Forests 9, 679 (2018)", "doi": "10.3390/f9110679", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Codispersion analysis is a new statistical method developed to assess spatial\ncovariation between two spatial processes that may not be isotropic or\nstationary. Its application to anisotropic ecological datasets have provided\nnew insights into mechanisms underlying observed patterns of species\ndistributions and the relationship between individual species and underlying\nenvironmental gradients. However, the performance of the codispersion\ncoefficient when there is noise or measurement error (\"contamination\") in the\ndata has been addressed only theoretically. Here, we use Monte Carlo\nsimulations and real datasets to investigate the sensitivity of codispersion to\nfour types of contamination commonly seen in many real-world environmental and\necological studies. Three of these involved examining codispersion of a spatial\ndataset with a contaminated version of itself. The fourth examined differences\nin codisperson between plants and soil conditions, where the estimates of soil\ncharacteristics were based on complete or thinned datasets. In all cases, we\nfound that estimates of codispersion were robust when contamination, such as\ndata thinning, was relatively low (<15\\%), but were sensitive to larger\npercentages of contamination. We also present a useful method for imputing\nmissing spatial data and discuss several aspects of the codispersion\ncoefficient when applied to noisy data to gain more insight about the\nperformance of codispersion in practice.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 01:41:49 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Vallejos", "Ronny", ""], ["Buckley", "Hannah L", ""], ["Case", "Bradley S", ""], ["Acosta", "Jonathan", ""], ["Ellison", "Aaron M", ""]]}, {"id": "1801.07873", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, David J. Nott, Robert Kohn", "title": "Gaussian variational approximation for high-dimensional state space\n  models", "comments": "Significantly revised, especially the multivariate stochastic\n  volatility model example", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our article considers a Gaussian variational approximation of the posterior\ndensity in a high-dimensional state space model. The variational parameters to\nbe optimized are the mean vector and the covariance matrix of the\napproximation. The number of parameters in the covariance matrix grows as the\nsquare of the number of model parameters, so it is necessary to find simple yet\neffective parameterizations of the covariance structure when the number of\nmodel parameters is large. We approximate the joint posterior distribution over\nthe high-dimensional state vectors by a dynamic factor model, having Markovian\ntime dependence and a factor covariance structure for the states. This gives a\nreduced description of the dependence structure for the states, as well as a\ntemporal conditional independence structure similar to that in the true\nposterior. The usefulness of the approach is illustrated for prediction in two\nhigh-dimensional applications that are challenging for Markov chain Monte Carlo\nsampling. The first is a spatio-temporal model for the spread of the Eurasian\nCollared-Dove across North America; the second is a Wishart-based multivariate\nstochastic volatility model for financial returns.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 06:30:46 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 05:05:24 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 01:28:06 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Quiroz", "Matias", ""], ["Nott", "David J.", ""], ["Kohn", "Robert", ""]]}, {"id": "1801.07905", "submitter": "Veronica Vinciotti Dr", "authors": "Alina Peluso, Veronica Vinciotti, Keming Yu", "title": "Discrete Weibull generalised additive model: an application to count\n  fertility data", "comments": "19 pages, 5 figures, 7 tables", "journal-ref": null, "doi": "10.1111/rssc.12311", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fertility plans, measured by the number of planned children, have been found\nto be affected by education and family background via complex tail\ndependencies. This challenge was previously met with the use of non-parametric\njittering approaches. This paper shows how a novel generalized additive model\nbased on a discrete Weibull distribution provides partial effects of the\ncovariates on fertility plans which are comparable to jittering, without the\ninherent drawback of crossing conditional quantiles. The model has some\nadditional desirable features: both over- and under-dispersed data can be\nmodelled by this distribution, the conditional quantiles have a simple analytic\nform and the likelihood is the same of that of a continuous Weibull\ndistribution with interval-censored data. The latter means that efficient\nimplementations are already available, in the R package gamlss, for a range of\nmodels and inferential procedures, and at a fraction of the time compared to\nthe jittering and COM-Poisson approaches, showing potential for the wide\napplicability of this approach to the modelling of count data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 08:30:58 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Peluso", "Alina", ""], ["Vinciotti", "Veronica", ""], ["Yu", "Keming", ""]]}, {"id": "1801.07981", "submitter": "Veronica Vinciotti Dr", "authors": "Luigi Augugliaro, Antonino Abbruzzo, Veronica Vinciotti", "title": "L1-Penalized Censored Gaussian Graphical Model", "comments": null, "journal-ref": null, "doi": "10.1093/biostatistics/kxy043", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical lasso is one of the most used estimators for inferring genetic\nnetworks. Despite its diffusion, there are several fields in applied research\nwhere the limits of detection of modern measurement technologies make the use\nof this estimator theoretically unfounded, even when the assumption of a\nmultivariate Gaussian distribution is satisfied. Typical examples are data\ngenerated by polymerase chain reactions and flow cytometer. The combination of\ncensoring and high-dimensionality make inference of the underlying genetic\nnetworks from these data very challenging. In this paper we propose an\n$\\ell_1$-penalized Gaussian graphical model for censored data and derive two\nEM-like algorithms for inference. By an extensive simulation study, we evaluate\nthe computational efficiency of the proposed algorithms and show that our\nproposal overcomes existing competitors when censored data are available. We\napply the proposed method to gene expression data coming from microfluidic\nRT-qPCR technology in order to make inference on the regulatory mechanisms of\nblood development.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 13:36:20 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Augugliaro", "Luigi", ""], ["Abbruzzo", "Antonino", ""], ["Vinciotti", "Veronica", ""]]}, {"id": "1801.08007", "submitter": "Ricardo Cris\\'ostomo", "authors": "Ricardo Crisostomo, Lorena Couso", "title": "Financial density forecasts: A comprehensive comparison of risk-neutral\n  and historical schemes", "comments": "Journal of Forecasting, 2018", "journal-ref": null, "doi": "10.1002/for.2521", "report-no": null, "categories": "q-fin.RM math.PR q-fin.ST stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the forecasting ability of the most commonly used benchmarks\nin financial economics. We approach the usual caveats of probabilistic\nforecasts studies -small samples, limited models and non-holistic validations-\nby performing a comprehensive comparison of 15 predictive schemes during a time\nperiod of over 21 years. All densities are evaluated in terms of their\nstatistical consistency, local accuracy and forecasting errors. Using a new\ncomposite indicator, the Integrated Forecast Score (IFS), we show that\nrisk-neutral densities outperform historical-based predictions in terms of\ninformation content. We find that the Variance Gamma model generates the\nhighest out-of-sample likelihood of observed prices and the lowest predictive\nerrors, whereas the ARCH-based GJR-FHS delivers the most consistent forecasts\nacross the entire density range. In contrast, lognormal densities, the Heston\nmodel or the Breeden-Litzenberger formula yield biased predictions and are\nrejected in statistical tests.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 14:50:49 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 14:28:47 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Crisostomo", "Ricardo", ""], ["Couso", "Lorena", ""]]}, {"id": "1801.08120", "submitter": "Rong Ma", "authors": "Rong Ma, T. Tony Cai, Hongzhe Li", "title": "Optimal Estimation of Simultaneous Signals Using Absolute Inner Product\n  with Applications to Integrative Genomics", "comments": null, "journal-ref": "Statistica Sinica (2020)", "doi": "10.5705/ss.202019.0445", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating the summary statistics from genome-wide association study\n(\\textsc{gwas}) and expression quantitative trait loci (e\\textsc{qtl}) data\nprovides a powerful way of identifying the genes whose expression levels are\npotentially associated with complex diseases. A parameter called $T$-score that\nquantifies the genetic overlap between a gene and the disease phenotype based\non the summary statistics is introduced based on the mean values of two\nGaussian sequences. Specifically, given two independent samples\n$\\mathbf{x}_n\\sim N(\\theta, \\Sigma_1)$ and $\\mathbf{y}_n\\sim N(\\mu, \\Sigma_2)$,\nthe $T$-score is defined as $\\sum_{i=1}^n |\\theta_i\\mu_i|$, a non-smooth\nfunctional, which characterizes the amount of shared signals between two\nabsolute normal mean vectors $|\\theta|$ and $|\\mu|$. Using approximation\ntheory, estimators are constructed and shown to be minimax rate-optimal and\nadaptive over various parameter spaces. Simulation studies demonstrate the\nsuperiority of the proposed estimators over existing methods. The method is\napplied to an integrative analysis of heart failure genomics datasets and we\nidentify several genes and biological pathways that are potentially causal to\nhuman heart failure.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 18:40:50 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 23:41:12 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 14:43:13 GMT"}, {"version": "v4", "created": "Mon, 5 Oct 2020 02:20:52 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Ma", "Rong", ""], ["Cai", "T. Tony", ""], ["Li", "Hongzhe", ""]]}, {"id": "1801.08184", "submitter": "James Salter", "authors": "James M Salter, Daniel B Williamson, John Scinocca and Viatcheslav\n  Kharin", "title": "Uncertainty quantification for computer models with spatial output using\n  calibration-optimal bases", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2018.1514306", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The calibration of complex computer codes using uncertainty quantification\n(UQ) methods is a rich area of statistical methodological development. When\napplying these techniques to simulators with spatial output, it is now standard\nto use principal component decomposition to reduce the dimensions of the\noutputs in order to allow Gaussian process emulators to predict the output for\ncalibration. We introduce the `terminal case', in which the model cannot\nreproduce observations to within model discrepancy, and for which standard\ncalibration methods in UQ fail to give sensible results. We show that even when\nthere is no such issue with the model, the standard decomposition on the\noutputs can and usually does lead to a terminal case analysis. We present a\nsimple test to allow a practitioner to establish whether their experiment will\nresult in a terminal case analysis, and a methodology for defining\ncalibration-optimal bases that avoid this whenever it is not inevitable. We\npresent the optimal rotation algorithm for doing this, and demonstrate its\nefficacy for an idealised example for which the usual principal component\nmethods fail. We apply these ideas to the CanAM4 model to demonstrate the\nterminal case issue arising for climate models. We discuss climate model tuning\nand the estimation of model discrepancy within this context, and show how the\noptimal rotation algorithm can be used in developing practical climate model\ntuning tools.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 20:51:14 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 15:46:25 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Salter", "James M", ""], ["Williamson", "Daniel B", ""], ["Scinocca", "John", ""], ["Kharin", "Viatcheslav", ""]]}, {"id": "1801.08248", "submitter": "Yunda Huang", "authors": "Yunda Huang, Yuanyuan Zhang, Zong Zhang, Peter B. Gilbert", "title": "Generating survival times using Cox proportional hazards models with\n  cyclic time-varying covariates, with application to a multiple-dose\n  monoclonal antibody clinical trial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In two harmonized efficacy studies to prevent HIV infection through multiple\ninfusions of the monoclonal antibody VRC01, a key objective is to evaluate\nwhether the serum concentration of VRC01, which changes cyclically over time\nalong with the infusion schedule, is associated with the rate of HIV infection.\nSimulation studies are needed in the development of such survival models. In\nthis paper, we consider simulating event time data with a continuous\ntime-varying covariate whose values vary with time through multiple drug\nadministration cycles, and whose effect on survival changes differently before\nand after a threshold within each cycle. The latter accommodates settings with\na zero-protection biomarker threshold above which the drug provides a varying\nlevel of protection depending on the biomarker level, but below which the drug\nprovides no protection. We propose two simulation approaches: one based on\nsimulating survival data under a single-dose regimen first before data are\naggregated over multiple doses, and another based on simulating survival data\ndirectly under a multiple-dose regimen. We generate time-to-event data\nfollowing a Cox proportional hazards model based on inverting the cumulative\nhazard function and a log link function for relating the hazard function to the\ncovariates. The method's validity is assessed in two sets of simulation\nexperiments. The results indicate that the proposed procedures perform well in\nproducing data that conform to their cyclic nature and assumptions of the Cox\nproportional hazards model.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 00:46:30 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Huang", "Yunda", ""], ["Zhang", "Yuanyuan", ""], ["Zhang", "Zong", ""], ["Gilbert", "Peter B.", ""]]}, {"id": "1801.08256", "submitter": "Ishanu Chattopadhyay", "authors": "Ishanu Chattopadhyay", "title": "A Hilbert Space of Stationary Ergodic Processes", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying meaningful signal buried in noise is a problem of interest\narising in diverse scenarios of data-driven modeling. We present here a\ntheoretical framework for exploiting intrinsic geometry in data that resists\nnoise corruption, and might be identifiable under severe obfuscation. Our\napproach is based on uncovering a valid complete inner product on the space of\nergodic stationary finite valued processes, providing the latter with the\nstructure of a Hilbert space on the real field. This rigorous construction,\nbased on non-standard generalizations of the notions of sum and scalar\nmultiplication of finite dimensional probability vectors, allows us to\nmeaningfully talk about \"angles\" between data streams and data sources, and,\nmake precise the notion of orthogonal stochastic processes. In particular, the\nrelative angles appear to be preserved, and identifiable, under severe noise,\nand will be developed in future as the underlying principle for robust\nclassification, clustering and unsupervised featurization algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 02:18:49 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Chattopadhyay", "Ishanu", ""]]}, {"id": "1801.08495", "submitter": "Fabrizio Leisen", "authors": "Alfred Kume, Fabrizio Leisen and Antonio Lijoi", "title": "Limiting behaviour of the stationary search cost distribution driven by\n  a generalized gamma process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a list of labeled objects that are organized in a heap. At each\ntime, object $j$ is selected with probability $p_j$ and moved to the top of the\nheap. This procedure defines a Markov chain on the set of permutations which is\nreferred to in the literature as Move-to-Front rule. The present contribution\nfocuses on the stationary search cost, namely the position of the requested\nitem in the heap when the Markov chain is in equilibrium. We consider the\nscenario where the number of objects is infinite and the probabilities $p_j$'s\nare defined as the normalization of the increments of a subordinator. In this\nsetting, we provide an exact formula for the moments of any order of the\nstationary search cost distribution. We illustrate the new findings in the case\nof a generalized gamma subordinator and deal with an extension to the\ntwo--parameter Poisson--Dirichlet process, also known as Pitman--Yor process.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 17:21:15 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Kume", "Alfred", ""], ["Leisen", "Fabrizio", ""], ["Lijoi", "Antonio", ""]]}, {"id": "1801.08512", "submitter": "Jana Jankova", "authors": "Jana Jankova and Sara van de Geer", "title": "Inference in high-dimensional graphical models", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a selected overview of methodology and theory for estimation and\ninference on the edge weights in high-dimensional directed and undirected\nGaussian graphical models. For undirected graphical models, two main explicit\nconstructions are provided: one based on a global method that maximizes the\njoint likelihood (the graphical Lasso) and one based on a local (nodewise)\nmethod that sequentially applies the Lasso to estimate the neighbourhood of\neach node. The proposed estimators lead to confidence intervals for edge\nweights and recovery of the edge structure. We evaluate their empirical\nperformance in an extensive simulation study. The theoretical guarantees for\nthe methods are achieved under a sparsity condition relative to the sample size\nand regularity conditions. For directed acyclic graphs, we apply similar ideas\nto construct confidence intervals for edge weights, when the directed acyclic\ngraph is identifiable.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 18:21:41 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Jankova", "Jana", ""], ["van de Geer", "Sara", ""]]}, {"id": "1801.08646", "submitter": "Tania Roy", "authors": "Tania Roy, Hsieh Fushing, Xunde Li, Brenda McCowan and Rob Atwill", "title": "Information Content of a Phylogenetic Tree in a Data Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic trees in genetics and biology in general are all binary. We make\nan attempt to answer one fundamental question: Is such binary branching from\nthe coarsest to the finest scales sustained by data? We convert this question\ninto an equivalent one: where is the structural information of tree in a data\nmatrix? Results from this conceptual as well as computing issue afford us to\nconclude a negative answer: Each branch being split into two at each inter-node\nof tree from the top to bottom levels is a man-made structure. The data-driven\ncomputing paradigm Data Mechanics is employed here to reveal that information\nof tree is composed of a set of selected temperatures (or scales), each of\nwhich has a clustering composition strictly regulated by a temperature-specific\ncluster-sharing probability matrix. The resultant Data Cloud Geometry (DCG)\ntree on the space of species is proposed as the authentic structure contained\nin data. Particularly each core clusters on the finest scale, the bottom level,\nof DCG tree should not be further partitioned because of uniformity. Beyond the\nfinest scale, the branching of DCG tree is primarily based on probability,\nwhich induces an Ultrametric satisfying super triangular inequality property.\nThis Ultrametric property differentiates DCG tree from all popular trees based\non Hierarchical clustering (HC) algorithm, which typically employs an\nempirical, often ad hoc distance measure. Since this measure is regulated by\nthe triangular inequality, it is not capable of producing a \"flat\" branch, in\nwhich all its members (more than two) have equal distances to each others. We\ndemonstrate such information content on an illustrative zoo data first, and\nthen on two genomic data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 01:25:27 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Roy", "Tania", ""], ["Fushing", "Hsieh", ""], ["Li", "Xunde", ""], ["McCowan", "Brenda", ""], ["Atwill", "Rob", ""]]}, {"id": "1801.08852", "submitter": "Kevin Lu", "authors": "Boris Buchmann, Kevin W. Lu, Dilip B. Madan", "title": "Calibration for Weak Variance-Alpha-Gamma Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weak variance-alpha-gamma process is a multivariate L\\'evy process\nconstructed by weakly subordinating Brownian motion, possibly with correlated\ncomponents with an alpha-gamma subordinator. It generalises the\nvariance-alpha-gamma process of Semeraro constructed by traditional\nsubordination. We compare three calibration methods for the weak\nvariance-alpha-gamma process, method of moments, maximum likelihood estimation\n(MLE) and digital moment estimation (DME). We derive a condition for Fourier\ninvertibility needed to apply MLE and show in our simulations that MLE produces\na better fit when this condition holds, while DME produces a better fit when it\nis violated. We also find that the weak variance-alpha-gamma process exhibits a\nwider range of dependence and produces a significantly better fit than the\nvariance-alpha-gamma process on an S&P500-FTSE100 data set, and that DME\nproduces the best fit in this situation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 15:42:11 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 07:48:28 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2018 07:18:26 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Buchmann", "Boris", ""], ["Lu", "Kevin W.", ""], ["Madan", "Dilip B.", ""]]}, {"id": "1801.08929", "submitter": "Matthew Levine", "authors": "Matthew E. Levine, David J. Albers, George Hripcsak", "title": "Methodological variations in lagged regression for detecting physiologic\n  drug effects in EHR data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We studied how lagged linear regression can be used to detect the physiologic\neffects of drugs from data in the electronic health record (EHR). We\nsystematically examined the effect of methodological variations ((i) time\nseries construction, (ii) temporal parameterization, (iii) intra-subject\nnormalization, (iv) differencing (lagged rates of change achieved by taking\ndifferences between consecutive measurements), (v) explanatory variables, and\n(vi) regression models) on performance of lagged linear methods in this\ncontext. We generated two gold standards (one knowledge-base derived, one\nexpert-curated) for expected pairwise relationships between 7 drugs and 4 labs,\nand evaluated how the 64 unique combinations of methodological perturbations\nreproduce gold standards. Our 28 cohorts included patients in Columbia\nUniversity Medical Center/NewYork-Presbyterian Hospital clinical database. The\nmost accurate methods achieved AUROC of 0.794 for knowledge-base derived gold\nstandard (95%CI [0.741, 0.847]) and 0.705 for expert-curated gold standard (95%\nCI [0.629, 0.781]). We observed a 0.633 mean AUROC (95%CI [0.610, 0.657],\nexpert-curated gold standard) across all methods that re-parameterize time\naccording to sequence and use either a joint autoregressive model with\ndifferencing or an independent lag model without differencing. The complement\nof this set of methods achieved a mean AUROC close to 0.5, indicating the\nimportance of these choices. We conclude that time- series analysis of EHR data\nwill likely rely on some of the beneficial pre-processing and modeling\nmethodologies identified, and will certainly benefit from continued careful\nanalysis of methodological perturbations. This study found that methodological\nvariations, such as pre-processing and representations, significantly affect\nresults, exposing the importance of evaluating these components when comparing\nmachine-learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 18:43:32 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Levine", "Matthew E.", ""], ["Albers", "David J.", ""], ["Hripcsak", "George", ""]]}, {"id": "1801.08961", "submitter": "Ivan Fernandez-Val", "authors": "Iv\\'an Fern\\'andez-Val, Aico van Vuuren, and Francis Vella", "title": "Nonseparable Sample Selection Models with Censored Selection Rules", "comments": "53 pages, 4 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider identification and estimation of nonseparable sample selection\nmodels with censored selection rules. We employ a control function approach and\ndiscuss different objects of interest based on (1) local effects conditional on\nthe control function, and (2) global effects obtained from integration over\nranges of values of the control function. We derive the conditions for the\nidentification of these different objects and suggest strategies for\nestimation. Moreover, we provide the associated asymptotic theory. These\nstrategies are illustrated in an empirical investigation of the determinants of\nfemale wages in the United Kingdom.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 19:54:44 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 21:50:32 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["van Vuuren", "Aico", ""], ["Vella", "Francis", ""]]}, {"id": "1801.09002", "submitter": "Sophia Kyriakou", "authors": "Sophia Kyriakou, Ioannis Kosmidis, Nicola Sartori", "title": "Median bias reduction in random-effects meta-analysis and\n  meta-regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random-effects models are frequently used to synthesise information from\ndifferent studies in meta-analysis. While likelihood-based inference is\nattractive both in terms of limiting properties and of implementation, its\napplication in random-effects meta-analysis may result in misleading\nconclusions, especially when the number of studies is small to moderate. The\ncurrent paper shows how methodology that reduces the asymptotic bias of the\nmaximum likelihood estimator of the variance component can also substantially\nimprove inference about the mean effect size. The results are derived for the\nmore general framework of random-effects meta-regression, which allows the mean\neffect size to vary with study-specific covariates.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 23:17:08 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 08:54:28 GMT"}, {"version": "v3", "created": "Tue, 6 Feb 2018 13:31:44 GMT"}, {"version": "v4", "created": "Thu, 22 Mar 2018 14:32:33 GMT"}, {"version": "v5", "created": "Wed, 23 May 2018 18:29:35 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Kyriakou", "Sophia", ""], ["Kosmidis", "Ioannis", ""], ["Sartori", "Nicola", ""]]}, {"id": "1801.09037", "submitter": "Keli Liu", "authors": "Keli Liu, Jelena Markovic, and Robert Tibshirani", "title": "More powerful post-selection inference, with application to the Lasso", "comments": "Corrected Figures showing simulation results for \"full\" model\n  coefficients (Figures 2,3,4). Previous results for naive and Bonferroni were\n  mistakenly based on partial model coefficients; in the new results the naive\n  and Bonferroni intervals are longer. Results for TG_V, TG_M, TG_Ms intervals\n  remain unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigators often use the data to generate interesting hypotheses and then\nperform inference for the generated hypotheses. P-values and confidence\nintervals must account for this explorative data analysis. A fruitful method\nfor doing so is to condition any inferences on the components of the data used\nto generate the hypotheses, thus preventing information in those components\nfrom being used again. Some currently popular methods \"over-condition\", leading\nto wide intervals. We show how to perform the minimal conditioning in a\ncomputationally tractable way. In high dimensions, even this minimal\nconditioning can lead to intervals that are too wide to be useful, suggesting\nthat up to now the cost of hypothesis generation has been underestimated. We\nshow how to generate hypotheses in a strategic manner that sharply reduces the\ncost of data exploration and results in useful confidence intervals. Our\ndiscussion focuses on the problem of post-selection inference after fitting a\nlasso regression model, but we also outline its extension to a much more\ngeneral setting.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 05:16:23 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 19:38:56 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Liu", "Keli", ""], ["Markovic", "Jelena", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1801.09126", "submitter": "Tania Roy", "authors": "Fushing Hsieh, Kevin Fujii, Tania Roy, Cho-Jui Hsieh, Brenda McCowan", "title": "Graphic displays of MLB pitching mechanics and its evolutions in\n  PITCHf/x data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systemic and idiosyncratic patterns in pitching mechanics of 24 top starting\npitchers in Major League Baseball (MLB) are extracted and discovered from\nPITCHf/x database. These evolving patterns across different pitchers or seasons\nare represented through three exclusively developed graphic displays.\nUnderstanding on such patterned evolutions will be beneficial for pitchers'\nwellbeing in signaling potential injury, and will be critical for expert\nknowledge in comparing pitchers. Based on data-driven computing, a universal\ncomposition of patterns is identified on all pitchers' mutual conditional\nentropy matrices. The first graphic display reveals that this universality\naccommodates physical laws as well as systemic characteristics of pitching\nmechanics. Such visible characters point to large scale factors for\ndifferentiating between distinct clusters of pitchers, and simultaneously lead\nto detailed factors for comparing individual pitchers. The second graphic\ndisplay shows choices of features that are able to express a pitcher's\nseason-by-season pitching contents via a series of 3(+2)D point-cloud\ngeometries. The third graphic display exhibits exquisitely a pitcher's\nidiosyncratic pattern-information of pitching across seasons by demonstrating\nall his pitch-subtype evolutions. These heatmap-based graphic displays are\nplatforms for visualizing and understanding pitching mechanics.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 19:28:45 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Hsieh", "Fushing", ""], ["Fujii", "Kevin", ""], ["Roy", "Tania", ""], ["Hsieh", "Cho-Jui", ""], ["McCowan", "Brenda", ""]]}, {"id": "1801.09236", "submitter": "Jordan Awan", "authors": "Jordan Awan and Aleksandra Slavkovic", "title": "Structure and Sensitivity in Differential Privacy: Comparing K-Norm\n  Mechanisms", "comments": "40 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy (DP), provides a framework for provable privacy\nprotection against arbitrary adversaries, while allowing the release of summary\nstatistics and synthetic data. We address the problem of releasing a noisy\nreal-valued statistic vector $T$, a function of sensitive data under DP, via\nthe class of $K$-norm mechanisms with the goal of minimizing the noise added to\nachieve privacy. First, we introduce the sensitivity space of $T$, which\nextends the concepts of sensitivity polytope and sensitivity hull to the\nsetting of arbitrary statistics $T$. We then propose a framework consisting of\nthree methods for comparing the $K$-norm mechanisms: 1) a multivariate\nextension of stochastic dominance, 2) the entropy of the mechanism, and 3) the\nconditional variance given a direction, to identify the optimal $K$-norm\nmechanism. In all of these criteria, the optimal $K$-norm mechanism is\ngenerated by the convex hull of the sensitivity space. Using our methodology,\nwe extend the objective perturbation and functional mechanisms and apply these\ntools to logistic and linear regression, allowing for private releases of\nstatistical results. Via simulations and an application to a housing price\ndataset, we demonstrate that our proposed methodology offers a substantial\nimprovement in utility for the same level of risk.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 14:27:04 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 18:38:10 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 21:28:30 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Awan", "Jordan", ""], ["Slavkovic", "Aleksandra", ""]]}, {"id": "1801.09299", "submitter": "Cyril Chimisov", "authors": "Cyril Chimisov, Krzysztof Latuszynski, Gareth Roberts", "title": "Adapting The Gibbs Sampler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of Adaptive MCMC has been fueled on the one hand by its\nsuccess in applications, and on the other hand, by mathematically appealing and\ncomputationally straightforward optimisation criteria for the Metropolis\nalgorithm acceptance rate (and, equivalently, proposal scale). Similarly\nprincipled and operational criteria for optimising the selection probabilities\nof the Random Scan Gibbs Sampler have not been devised to date.\n  In the present work, we close this gap and develop a general purpose Adaptive\nRandom Scan Gibbs Sampler that adapts the selection probabilities. The\nadaptation is guided by optimising the $L_2-$spectral gap for the target's\nGaussian analogue, gradually, as target's global covariance is learned by the\nsampler. The additional computational cost of the adaptation represents a small\nfraction of the total simulation effort. ` We present a number of moderately-\nand high-dimensional examples, including truncated Gaussians, Bayesian\nHierarchical Models and Hidden Markov Models, where significant computational\ngains are empirically observed for both, Adaptive Gibbs, and Adaptive\nMetropolis within Adaptive Gibbs version of the algorithm. We argue that\nAdaptive Random Scan Gibbs Samplers can be routinely implemented and\nsubstantial computational gains will be observed across many typical Gibbs\nsampling problems.\n  We shall give conditions under which ergodicity of the adaptive algorithms\ncan be established.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 21:54:44 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Chimisov", "Cyril", ""], ["Latuszynski", "Krzysztof", ""], ["Roberts", "Gareth", ""]]}, {"id": "1801.09309", "submitter": "Cyril Chimisov", "authors": "Cyril Chimisov, Krzysztof Latuszynski, Gareth Roberts", "title": "Air Markov Chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of Adapted Increasingly Rarely Markov Chain Monte Carlo\n(AirMCMC) algorithms where the underlying Markov kernel is allowed to be\nchanged based on the whole available chain output but only at specific time\npoints separated by an increasing number of iterations. The main motivation is\nthe ease of analysis of such algorithms. Under the assumption of either\nsimultaneous or (weaker) local simultaneous geometric drift condition, or\nsimultaneous polynomial drift we prove the $L_2-$convergence, Weak and Strong\nLaws of Large Numbers (WLLN, SLLN), Central Limit Theorem (CLT), and discuss\nhow our approach extends the existing results. We argue that many of the known\nAdaptive MCMC algorithms may be transformed into the corresponding Air\nversions, and provide an empirical evidence that performance of the Air version\nstays virtually the same.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 22:20:31 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Chimisov", "Cyril", ""], ["Latuszynski", "Krzysztof", ""], ["Roberts", "Gareth", ""]]}, {"id": "1801.09338", "submitter": "Abolfazl Safikhani", "authors": "Tapabrata Maiti, Abolfazl Safikhani, Ping-Shou Zhong", "title": "Uncertainty Estimation in Functional Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis is proved to be useful in many scientific\napplications. The physical process is observed as curves and often there are\nseveral curves observed due to multiple subjects, providing the replicates in\nstatistical sense. The recent literature develops several techniques for\nregistering the curves and associated model estimation. However, very little\nhas been investigated for statistical inference, specifically uncertainty\nestimation. In this article, we consider functional linear mixed modeling\napproach to combine several curves. We concentrate measuring uncertainty when\nthe functional linear mixed models are used for prediction. Although measuring\nthe uncertainty is paramount interest in any statistical prediction, there is\nno closed form expression available for functional mixed effects models. In\nmany real life applications only a finite number of curves can be observed. In\nsuch situations it is important to asses the error rate for any valid\nstatistical statement. We derive theoretically valid approximation of\nuncertainty measurements that are suitable along with modified estimation\ntechniques. We illustrate our methods by numerical examples and compared with\nother existing literature as appropriate. Our method is computationally simple\nand often outperforms the other methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 01:32:17 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Maiti", "Tapabrata", ""], ["Safikhani", "Abolfazl", ""], ["Zhong", "Ping-Shou", ""]]}, {"id": "1801.09418", "submitter": "Harrie Hendriks", "authors": "Harrie Hendriks", "title": "Test Martingales for bounded random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random sample from a random variable $T$ which is bounded from above,\n$T\\le\\tau$ a.s., we define processes that are positive supermartingales if\n$E(T)\\ge\\mu$. Such processes are called test martingales. Tests of the\nsupermartingale hypothesis implicitly test the hypothesis $H_0:E(T)\\ge\\mu$. We\nconstruct test martingales that lead to tests with power 1. We also construct\nconfidence upper bounds. We extend the techniques to testing $H_0:E(T)=\\mu$ and\nconstructing confidence intervals.\n  In financial auditing random sampling is proposed as one of the possible\ntechniques to gather enough assurance to be able to state that there is no\n'material' misstatement in a financial report. The goal of our work is to\nprovide a mathematical context that could represent such process of gathering\nassurance by means of repeated random sampling.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 09:37:43 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 08:09:43 GMT"}, {"version": "v3", "created": "Sun, 18 Feb 2018 15:11:00 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Hendriks", "Harrie", ""]]}, {"id": "1801.09456", "submitter": "Tiejun Tong", "authors": "Dehui Luo and Xiang Wan and Jiming Liu and Tiejun Tong", "title": "Testing normality using the summary statistics with application to\n  meta-analysis", "comments": "48 pages, 11 figures and 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the most important tool to provide high-level evidence-based medicine,\nresearchers can statistically summarize and combine data from multiple studies\nby conducting meta-analysis. In meta-analysis, mean differences are frequently\nused effect size measurements to deal with continuous data, such as the Cohen's\nd statistic and Hedges' g statistic values. To calculate the mean difference\nbased effect sizes, the sample mean and standard deviation are two essential\nsummary measures. However, many of the clinical reports tend not to directly\nrecord the sample mean and standard deviation. Instead, the sample size,\nmedian, minimum and maximum values and/or the first and third quartiles are\nreported. As a result, researchers have to transform the reported information\nto the sample mean and standard deviation for further compute the effect size.\nSince most of the popular transformation methods were developed upon the\nnormality assumption of the underlying data, it is necessary to perform a\npre-test before transforming the summary statistics. In this article, we had\nintroduced test statistics for three popular scenarios in meta-analysis. We\nsuggests medical researchers to perform a normality test of the selected\nstudies before using them to conduct further analysis. Moreover, we applied\nthree different case studies to demonstrate the usage of the newly proposed\ntest statistics. The real data case studies indicate that the new test\nstatistics are easy to apply in practice and by following the recommended path\nto conduct the meta-analysis, researchers can obtain more reliable conclusions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 11:30:56 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Luo", "Dehui", ""], ["Wan", "Xiang", ""], ["Liu", "Jiming", ""], ["Tong", "Tiejun", ""]]}, {"id": "1801.09519", "submitter": "Geert Van Kollenburg", "authors": "Geert H. van Kollenburg, Joris Mulder, Jeroen K. Vermunt", "title": "The Lazy Bootstrap. A Fast Resampling Method for Evaluating Latent Class\n  Model Fit", "comments": "This is an adaptation of chapter of a PhD dissertation available at\n  https://pure.uvt.nl/portal/files/19030880/Kollenburg_Computer_13_11_2017.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latent class model is a powerful unsupervised clustering algorithm for\ncategorical data. Many statistics exist to test the fit of the latent class\nmodel. However, traditional methods to evaluate those fit statistics are not\nalways useful. Asymptotic distributions are not always known, and empirical\nreference distributions can be very time consuming to obtain. In this paper we\npropose a fast resampling scheme with which any type of model fit can be\nassessed. We illustrate it here on the latent class model, but the methodology\ncan be applied in any situation.\n  The principle behind the lazy bootstrap method is to specify a statistic\nwhich captures the characteristics of the data that a model should capture\ncorrectly. If those characteristics in the observed data and in model-generated\ndata are very different we can assume that the model could not have produced\nthe observed data. With this method we achieve the flexibility of tests from\nthe Bayesian framework, while only needing maximum likelihood estimates. We\nprovide a step-wise algorithm with which the fit of a model can be assessed\nbased on the characteristics we as researcher find important. In a Monte Carlo\nstudy we show that the method has very low type I errors, for all illustrated\nstatistics. Power to reject a model depended largely on the type of statistic\nthat was used and on sample size. We applied the method to an empirical data\nset on clinical subgroups with risk of Myocardial infarction and compared the\nresults directly to the parametric bootstrap. The results of our method were\nhighly similar to those obtained by the parametric bootstrap, while the\nrequired computations differed three orders of magnitude in favour of our\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 14:18:22 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["van Kollenburg", "Geert H.", ""], ["Mulder", "Joris", ""], ["Vermunt", "Jeroen K.", ""]]}, {"id": "1801.09657", "submitter": "Deanna Needell", "authors": "Denali Molitor and Deanna Needell", "title": "Matrix Completion for Structured Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to predict or fill-in missing data, often referred to as matrix\ncompletion, is a common challenge in today's data-driven world. Previous\nstrategies typically assume that no structural difference between observed and\nmissing entries exists. Unfortunately, this assumption is woefully unrealistic\nin many applications. For example, in the classic Netflix challenge, in which\none hopes to predict user-movie ratings for unseen films, the fact that the\nviewer has not watched a given movie may indicate a lack of interest in that\nmovie, thus suggesting a lower rating than otherwise expected. We propose\nadjusting the standard nuclear norm minimization strategy for matrix completion\nto account for such structural differences between observed and unobserved\nentries by regularizing the values of the unobserved entries. We show that the\nproposed method outperforms nuclear norm minimization in certain settings.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 18:31:47 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Molitor", "Denali", ""], ["Needell", "Deanna", ""]]}, {"id": "1801.09728", "submitter": "Zhonglei Wang", "authors": "Jae Kwang Kim, Zhonglei Wang", "title": "Sampling techniques for big data analysis in finite population inference", "comments": "24 pages, 3 tables", "journal-ref": null, "doi": "10.1111/insr.12290", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In analyzing big data for finite population inference, it is critical to\nadjust for the selection bias in the big data. In this paper, we propose two\nmethods of reducing the selection bias associated with the big data sample. The\nfirst method uses a version of inverse sampling by incorporating auxiliary\ninformation from external sources, and the second one borrows the idea of data\nintegration by combining the big data sample with an independent probability\nsample. Two simulation studies show that the proposed methods are unbiased and\nhave better coverage rates than their alternatives. In addition, the proposed\nmethods are easy to implement in practice.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 19:40:20 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Kim", "Jae Kwang", ""], ["Wang", "Zhonglei", ""]]}, {"id": "1801.09739", "submitter": "Thomas Nagler", "authors": "Thomas Nagler, Christian Bumann, Claudia Czado", "title": "Model selection in sparse high-dimensional vine copula models with\n  application to portfolio risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vine copulas allow to build flexible dependence models for an arbitrary\nnumber of variables using only bivariate building blocks. The number of\nparameters in a vine copula model increases quadratically with the dimension,\nwhich poses new challenges in high-dimensional applications. To alleviate the\ncomputational burden and risk of overfitting, we propose a modified Bayesian\ninformation criterion (BIC) tailored to sparse vine copula models. We show that\nthe criterion can consistently distinguish between the true and alternative\nmodels under less stringent conditions than the classical BIC. The new\ncriterion can be used to select the hyper-parameters of sparse model classes,\nsuch as truncated and thresholded vine copulas. We propose a computationally\nefficient implementation and illustrate the benefits of the new concepts with a\ncase study where we model the dependence in a large stock stock portfolio.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 20:10:38 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 17:17:38 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 11:37:46 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Nagler", "Thomas", ""], ["Bumann", "Christian", ""], ["Czado", "Claudia", ""]]}, {"id": "1801.09795", "submitter": "Eduardo Elias Ribeiro Junior", "authors": "Eduardo E. Ribeiro Jr, Walmes M. Zeviani, Wagner H. Bonat, Clarice G.\n  B. Dem\\'etrio, John Hinde", "title": "Reparametrization of COM-Poisson Regression Models with Applications in\n  the Analysis of Experimental Data", "comments": "23 pages, 12 figures and 7 tables. Submitted article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the analysis of count data often the equidispersion assumption is not\nsuitable, hence the Poisson regression model is inappropriate. As a\ngeneralization of the Poisson distribution, the COM-Poisson distribution can\ndeal with under-, equi- and overdispersed count data. It is a member of the\nexponential family of distributions and has well known special cases. In spite\nof the nice properties of the COM-Poisson distribution, its location parameter\ndoes not correspond to the expectation, which complicates the interpretation of\nregression models. In this paper, we propose a straightforward\nreparametrization of the COM-Poisson distribution based on an approximation to\nthe expectation of this distribution. The main advantage of our new\nparametrization is the straightforward interpretation of the regression\ncoefficients in terms of the expectation, as usual in the context of\ngeneralized linear models. Furthermore, the estimation and inference for the\nnew COM-Poisson regression model can be done based on the likelihood paradigm.\nWe carried out simulation studies to verify the finite sample properties of the\nmaximum likelihood estimators. The results from our simulation study show that\nthe maximum likelihood estimators are unbiased and consistent for both\nregression and dispersion parameters. We observed that the empirical\ncorrelation between the regression and dispersion parameter estimators is close\nto zero, which suggests that these parameters are orthogonal. We illustrate the\napplication of the proposed model through the analysis of three data sets with\nover-, under- and equidispersed count data. The study of distribution\nproperties through a consideration of dispersion, zero-inflated and heavy tail\nindexes, together with the results of data analysis show the flexibility over\nstandard approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 23:26:53 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Ribeiro", "Eduardo E.", "Jr"], ["Zeviani", "Walmes M.", ""], ["Bonat", "Wagner H.", ""], ["Dem\u00e9trio", "Clarice G. B.", ""], ["Hinde", "John", ""]]}, {"id": "1801.09834", "submitter": "James Long", "authors": "Zhenfeng Lin and James P. Long", "title": "A Flexible Procedure for Mixture Proportion Estimation in\n  Positive-Unlabeled Learning", "comments": "28 pages (including 9 pages of Technical Notes), 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive--unlabeled (PU) learning considers two samples, a positive set P\nwith observations from only one class and an unlabeled set U with observations\nfrom two classes. The goal is to classify observations in U. Class mixture\nproportion estimation (MPE) in U is a key step in PU learning. Blanchard et al.\n[2010] showed that MPE in PU learning is a generalization of the problem of\nestimating the proportion of true null hypotheses in multiple testing problems.\nMotivated by this idea, we propose reducing the problem to one dimension via\nconstruction of a probabilistic classifier trained on the P and U data sets\nfollowed by application of a one--dimensional mixture proportion method from\nthe multiple testing literature to the observation class probabilities. The\nflexibility of this framework lies in the freedom to choose the classifier and\nthe one--dimensional MPE method. We prove consistency of two mixture proportion\nestimators using bounds from empirical process theory, develop tuning parameter\nfree implementations, and demonstrate that they have competitive performance on\nsimulated waveform data and a protein signaling problem.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 03:02:12 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 22:23:11 GMT"}, {"version": "v3", "created": "Thu, 12 Jul 2018 21:12:26 GMT"}, {"version": "v4", "created": "Fri, 10 Jan 2020 03:30:27 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Lin", "Zhenfeng", ""], ["Long", "James P.", ""]]}, {"id": "1801.09874", "submitter": "Weichi Wu", "authors": "Holger Dette, Weichi Wu", "title": "Change point analysis in non-stationary processes - a mass excess\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of testing if a sequence of means\n$(\\mu_t)_{t =1,\\ldots ,n }$ of a non-stationary time series $(X_t)_{t =1,\\ldots\n,n }$ is stable in the sense that the difference of the means $\\mu_1$ and\n$\\mu_t$ between the initial time $t=1$ and any other time is smaller than a\ngiven level, that is $ | \\mu_1 - \\mu_t | \\leq c $ for all $t =1,\\ldots ,n $. A\ntest for hypotheses of this type is developed using a biascorrected monotone\nrearranged local linear estimator and asymptotic normality of the corresponding\ntest statistic is established. As the asymptotic variance depends on the\nlocation and order of the critical roots of the equation $| \\mu_1 - \\mu_t | =\nc$ a new bootstrap procedure is proposed to obtain critical values and its\nconsistency is established. As a consequence we are able to quantitatively\ndescribe relevant deviations of a non-stationary sequence from its initial\nvalue. The results are illustrated by means of a simulation study and by\nanalyzing data examples.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 07:30:25 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 06:52:16 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Dette", "Holger", ""], ["Wu", "Weichi", ""]]}, {"id": "1801.09894", "submitter": "Mathias Trabs", "authors": "Mathias Trabs", "title": "Bayesian inverse problems with unknown operators", "comments": "24 pages, 2 figures", "journal-ref": "Inverse problems, 34 (8), 085001, 2018", "doi": "10.1088/1361-6420/aac3aa", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Bayesian approach to linear inverse problems when the\nunderlying operator depends on an unknown parameter. Allowing for finite\ndimensional as well as infinite dimensional parameters, the theory covers\nseveral models with different levels of uncertainty in the operator. Using\nproduct priors, we prove contraction rates for the posterior distribution which\ncoincide with the optimal convergence rates up to logarithmic factors. In order\nto adapt to the unknown smoothness, an empirical Bayes procedure is constructed\nbased on Lepski's method. The procedure is illustrated in numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 08:45:46 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 13:08:09 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Trabs", "Mathias", ""]]}, {"id": "1801.09911", "submitter": "Carter Butts", "authors": "Carter T. Butts", "title": "A Dynamic Process Interpretation of the Sparse ERGM Reference Model", "comments": null, "journal-ref": "Journal of Mathematical Sociology, 43(1), 40-57 (2019)", "doi": "10.1080/0022250X.2018.1490737", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential family random graph models (ERGMs) can be understood in terms of\na set of structural biases that act on an underlying reference distribution.\nThis distribution determines many aspects of the behavior and interpretation of\nthe ERGM families incorporating it. One important innovation in this area has\nbeen the development of an ERGM reference model that produces realistic\nbehavior when generalized to sparse networks of varying size. Here, we show\nthat this model can be derived from a latent dynamic process in which tie\nformation takes place within small local settings between which individuals\nmove. This derivation provides one possible micro-process interpretation of the\nsparse ERGM reference model, and sheds light on the conditions under which\nconstant mean degree scaling can emerge.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 09:37:34 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Butts", "Carter T.", ""]]}, {"id": "1801.09956", "submitter": "Moritz Schauer", "authors": "Shota Gugushvili, Frank van der Meulen, Moritz Schauer, Peter Spreij", "title": "Nonparametric Bayesian volatility estimation", "comments": null, "journal-ref": "2017 MATRIX Annals, Springer International Publishing, 2019", "doi": "10.1007/978-3-030-04161-8_19", "report-no": null, "categories": "stat.ME math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given discrete time observations over a fixed time interval, we study a\nnonparametric Bayesian approach to estimation of the volatility coefficient of\na stochastic differential equation. We postulate a histogram-type prior on the\nvolatility with piecewise constant realisations on bins forming a partition of\nthe time interval. The values on the bins are assigned an inverse Gamma Markov\nchain (IGMC) prior. Posterior inference is straightforward to implement via\nGibbs sampling, as the full conditional distributions are available explicitly\nand turn out to be inverse Gamma. We also discuss in detail the hyperparameter\nselection for our method. Our nonparametric Bayesian approach leads to good\npractical results in representative simulation examples. Finally, we apply it\non a classical data set in change-point analysis: weekly closings of the\nDow-Jones industrial averages.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 12:31:06 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 14:48:17 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Gugushvili", "Shota", ""], ["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""], ["Spreij", "Peter", ""]]}, {"id": "1801.10047", "submitter": "Hugo Raguet", "authors": "Hugo Raguet and Amandine Marrel", "title": "Target and Conditional Sensitivity Analysis with Emphasis on Dependence\n  Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of sensitivity analysis of complex phenomena in presence of\nuncertainty, we motivate and precise the idea of orienting the analysis towards\na critical domain of the studied phenomenon. We make a brief history of related\napproaches in the literature, and propose a more general and systematic\napproach. Nonparametric measures of dependence being well-suited to this\napproach, we also make a review of available methods and of their use for\nsensitivity analysis, and clarify some of their properties. As a byproduct, we\nnotably describe a new way of computing correlation ratios for Sobol' indices,\nwhich does not require specific experience plans nor rely on independence of\nthe input factors. Finally, we show on synthetic numerical experiments both the\ninterest of target and conditional sensitivity analysis, and the relevance of\nthe dependence measures.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 17:10:25 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 00:01:55 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Raguet", "Hugo", ""], ["Marrel", "Amandine", ""]]}, {"id": "1801.10243", "submitter": "Kamiar Rahnama Rad", "authors": "Kamiar Rahnama Rad and Arian Maleki", "title": "A scalable estimate of the extra-sample prediction error via approximate\n  leave-one-out", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers the problem of out-of-sample risk estimation under the\nhigh dimensional settings where standard techniques such as $K$-fold cross\nvalidation suffer from large biases. Motivated by the low bias of the\nleave-one-out cross validation (LO) method, we propose a computationally\nefficient closed-form approximate leave-one-out formula (ALO) for a large class\nof regularized estimators. Given the regularized estimate, calculating ALO\nrequires minor computational overhead. With minor assumptions about the data\ngenerating process, we obtain a finite-sample upper bound for $|\\text{LO} -\n\\text{ALO}|$. Our theoretical analysis illustrates that $|\\text{LO} -\n\\text{ALO}| \\rightarrow 0$ with overwhelming probability, when $n,p \\rightarrow\n\\infty$, where the dimension $p$ of the feature vectors may be comparable with\nor even greater than the number of observations, $n$. Despite the\nhigh-dimensionality of the problem, our theoretical results do not require any\nsparsity assumption on the vector of regression coefficients. Our extensive\nnumerical experiments show that $|\\text{LO} - \\text{ALO}|$ decreases as $n,p$\nincrease, revealing the excellent finite sample performance of ALO. We further\nillustrate the usefulness of our proposed out-of-sample risk estimation method\nby an example of real recordings from spatially sensitive neurons (grid cells)\nin the medial entorhinal cortex of a rat.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 22:24:38 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 23:03:47 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 17:27:14 GMT"}, {"version": "v4", "created": "Tue, 11 Feb 2020 17:09:56 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Rad", "Kamiar Rahnama", ""], ["Maleki", "Arian", ""]]}, {"id": "1801.10251", "submitter": "Igor Kheifets", "authors": "Igor L. Kheifets", "title": "Multivariate Specification Tests Based on a Dynamic Rosenblatt Transform", "comments": "Accepted to Computational Statistics and Data Analysis", "journal-ref": null, "doi": "10.1016/j.csda.2018.01.022", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers parametric model adequacy tests for nonlinear\nmultivariate dynamic models. It is shown that commonly used Kolmogorov-type\ntests do not take into account cross-sectional nor time-dependence structure,\nand a test, based on multi-parameter empirical processes, is proposed that\novercomes these problems. The tests are applied to a nonlinear LSTAR-type model\nof joint movements of UK output growth and interest rate spreads. A simulation\nexperiment illustrates the properties of the tests in finite samples.\nAsymptotic properties of the test statistics under the null of correct\nspecification and under the local alternative, and justification of a\nparametric bootstrap to obtain critical values, are provided.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 23:03:13 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Kheifets", "Igor L.", ""]]}, {"id": "1801.10478", "submitter": "Weichi Wu", "authors": "Holger Dette, Weichi Wu, Zhou Zhou", "title": "Change Point Analysis of Correlation in Non-stationary Time Series", "comments": "arXiv admin note: text overlap with arXiv:1503.08610", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A restrictive assumption in change point analysis is \"stationarity under the\nnull hypothesis of no change-point\", which is crucial for asymptotic theory but\nnot very realistic from a practical point of view. For example, if change point\nanalysis for correlations is performed, it is not necessarily clear that the\nmean, marginal variance or higher order moments are constant, even if there is\nno change in the correlation. This paper develops change point analysis for the\ncorrelation structures under less restrictive assumptions. In contrast to\nprevious work, our approach does not require that the mean, variance and fourth\norder joint cumulants are constant under the null hypothesis. Moreover, we also\naddress the problem of detecting relevant change points.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 13:42:12 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Dette", "Holger", ""], ["Wu", "Weichi", ""], ["Zhou", "Zhou", ""]]}, {"id": "1801.10547", "submitter": "Yaakov Malinovsky", "authors": "Gregory Haber and Yaakov Malinovsky", "title": "On the construction of unbiased estimators for the group testing problem", "comments": "Revised version; Submitted on 7 November 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Debiased estimation has long been an area of research in the group testing\nliterature. This has led to the development of several estimators with the goal\nof bias minimization and, recently, an unbiased estimator based on sequential\nbinomial sampling. Previous research, however, has focused heavily on the\nsimple case where no misclassification is assumed and only one trait is to be\ntested. In this paper, we consider the problem of unbiased estimation in these\nbroader areas, giving constructions of such estimators for several cases. We\nshow that, outside of the standard case addressed previously in the literature,\nit is impossible to find any proper unbiased estimator, that is, an estimator\ngiving only values in the parameter space. This is shown to hold generally\nunder any binomial or multinomial sampling plans\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 16:58:47 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 12:48:27 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Haber", "Gregory", ""], ["Malinovsky", "Yaakov", ""]]}, {"id": "1801.10559", "submitter": "Wei Zhong", "authors": "Hengjian Cui and Wei Zhong", "title": "A Distribution-Free Test of Independence and Its Application to Variable\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the importance of measuring the association between the response\nand predictors in high dimensional data, In this article, we propose a new mean\nvariance test of independence between a categorical random variable and a\ncontinuous one based on mean variance index. The mean variance index is zero if\nand only if two variables are independent. Under the independence, we derive an\nexplicit form of its asymptotic null distribution, which provides us with an\nefficient and fast way to compute the empirical p-value in practice. The number\nof classes of the categorical variable is allowed to diverge slowly to the\ninfinity. It is essentially a rank test and thus distribution-free. No\nassumption on the distributions of two random variables is required and the\ntest statistic is invariant under one-to-one transformations. It is resistent\nto heavy-tailed distributions and extreme values. We assess its performance by\nMonte Carlo simulations and demonstrate that the proposed test achieves a\nhigher power in comparison with the existing tests. We apply the proposed MV\ntest to a high dimensional colon cancer gene expression data to detect the\nsignificant genes associated with the tissue syndrome.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 17:22:28 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Cui", "Hengjian", ""], ["Zhong", "Wei", ""]]}, {"id": "1801.10567", "submitter": "Jana Jankova", "authors": "Jana Jankov\\'a and Sara van de Geer", "title": "De-biased sparse PCA: Inference and testing for eigenstructure of large\n  covariance matrices", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse principal component analysis (sPCA) has become one of the most widely\nused techniques for dimensionality reduction in high-dimensional datasets. The\nmain challenge underlying sPCA is to estimate the first vector of loadings of\nthe population covariance matrix, provided that only a certain number of\nloadings are non-zero. In this paper, we propose confidence intervals for\nindividual loadings and for the largest eigenvalue of the population covariance\nmatrix. Given an independent sample $X^i \\in\\mathbb R^p, i = 1,...,n,$\ngenerated from an unknown distribution with an unknown covariance matrix\n$\\Sigma_0$, our aim is to estimate the first vector of loadings and the largest\neigenvalue of $\\Sigma_0$ in a setting where $p\\gg n$. Next to the\nhigh-dimensionality, another challenge lies in the inherent non-convexity of\nthe problem. We base our methodology on a Lasso-penalized M-estimator which,\ndespite non-convexity, may be solved by a polynomial-time algorithm such as\ncoordinate or gradient descent. We show that our estimator achieves the minimax\noptimal rates in $\\ell_1$ and $\\ell_2$-norm. We identify the bias in the\nLasso-based estimator and propose a de-biased sparse PCA estimator for the\nvector of loadings and for the largest eigenvalue of the covariance matrix\n$\\Sigma_0$. Our main results provide theoretical guarantees for asymptotic\nnormality of the de-biased estimator. The major conditions we impose are\nsparsity in the first eigenvector of small order $\\sqrt{n}/\\log p$ and sparsity\nof the same order in the columns of the inverse Hessian matrix of the\npopulation risk.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 17:30:55 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Jankov\u00e1", "Jana", ""], ["van de Geer", "Sara", ""]]}, {"id": "1801.10576", "submitter": "Thomas Nagler", "authors": "Thomas Nagler, Thibault Vatter", "title": "Solving estimating equations with copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to their ability to capture complex dependence structures, copulas are\nfrequently used to glue random variables into a joint model with arbitrary\none-dimensional margins. More recently, they have been applied to solve\nstatistical learning problems such as regression or classification. Framing\nsuch approaches as solutions of estimating equations, we generalize them in a\nunified framework. We derive consistency, asymptotic normality, and validity of\nthe bootstrap for copula-based Z-estimators. The conditions allow for both\ncontinuous and discrete data as well as parametric, nonparametric, and\nsemiparametric estimators of the copula and marginal distributions. The\nversatility of this methodology is illustrated by several theoretical examples,\na simulation study, and an application to financial portfolio allocation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 17:46:49 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 18:56:04 GMT"}, {"version": "v3", "created": "Sun, 17 May 2020 15:10:39 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Nagler", "Thomas", ""], ["Vatter", "Thibault", ""]]}]