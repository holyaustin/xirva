[{"id": "0902.0117", "submitter": "Tewfik Kernane", "authors": "Tewfik Kernane, Zohrh A. Raizah", "title": "Fixed Point Iteration for Estimating The Parameters of Extreme Value\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimations for the parameters of extreme value\ndistributions are discussed in this paper using fixed point iteration. The\ncommonly used numerical approach for addressing this problem is the\nNewton-Raphson approach which requires differentiation unlike the fixed point\niteration which is also easier to implement. Graphical approaches are also\nusually proposed in the literature. We prove that these reduce in fact to the\nfixed point solution proposed in this paper.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2009 08:45:44 GMT"}], "update_date": "2009-02-03", "authors_parsed": [["Kernane", "Tewfik", ""], ["Raizah", "Zohrh A.", ""]]}, {"id": "0902.0240", "submitter": "Kaspar Rufibach", "authors": "Kaspar Rufibach", "title": "An Active Set Algorithm to Estimate Parameters in Generalized Linear\n  Models with Ordered Predictors", "comments": "24 pages, 1 Figure, 3 Tables", "journal-ref": "Comput. Statist. Data Anal. (2010), 54, 1442-1456", "doi": "10.1016/j.csda.2010.01.014", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biomedical studies, researchers are often interested in assessing the\nassociation between one or more ordinal explanatory variables and an outcome\nvariable, at the same time adjusting for covariates of any type. The outcome\nvariable may be continuous, binary, or represent censored survival times. In\nthe absence of precise knowledge of the response function, using monotonicity\nconstraints on the ordinal variables improves efficiency in estimating\nparameters, especially when sample sizes are small. An active set algorithm\nthat can efficiently compute such estimators is proposed, and a\ncharacterization of the solution is provided. Having an efficient algorithm at\nhand is especially relevant when applying likelihood ratio tests in restricted\ngeneralized linear models, where one needs the value of the likelihood at the\nrestricted maximizer. The algorithm is illustrated on a real life data set from\noncology.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2009 10:37:24 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2010 07:13:41 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Rufibach", "Kaspar", ""]]}, {"id": "0902.0506", "submitter": "Jean-Francois Coeurjolly", "authors": "Jean-Fran\\c{c}ois Coeurjolly (LJK), R\\'emy Drouilhet (LJK), Pierre\n  Lafaye De Micheaux (LJK), Jean-Francois Robineau (LJK)", "title": "asympTest: an R package for performing parametric statistical tests and\n  confidence intervals based on the central limit theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an R package implementing large sample tests and\nconfidence intervals (based on the central limit theorem) for various\nparameters. The one and two sample mean and variance contexts are considered.\nThe statistics for all the tests are expressed in the same form, which\nfacilitates their presentation. In the variance parameter cases, the asymptotic\nrobustness of the classical tests depends on the departure of the data\ndistribution from normality measured in terms of the kurtosis of the\ndistribution.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2009 13:39:48 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2009 15:16:14 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Coeurjolly", "Jean-Fran\u00e7ois", "", "LJK"], ["Drouilhet", "R\u00e9my", "", "LJK"], ["De Micheaux", "Pierre Lafaye", "", "LJK"], ["Robineau", "Jean-Francois", "", "LJK"]]}, {"id": "0902.0990", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan", "title": "Directional Clustering Tests Based on Nearest Neighbor Contingency\n  Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": "KU-EC-09-1", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial interaction between two or more classes or species has important\nimplications in various fields and causes multivariate patterns such as\nsegregation or association. Segregation occurs when members of a class or\nspecies are more likely to be found near members of the same class or\nconspecifics; while association occurs when members of a class or species are\nmore likely to be found near members of another class or species. The null\npatterns considered are random labeling (RL) and complete spatial randomness\n(CSR) of points from two or more classes, which is called \\emph{CSR\nindependence}, henceforth. The clustering tests based on nearest neighbor\ncontingency tables (NNCTs) that are in use in literature are two-sided tests.\nIn this article, we consider the directional (i.e., one-sided) versions of the\ncell-specific NNCT-tests and introduce new directional NNCT-tests for the\ntwo-class case. We analyze the distributional properties; compare the empirical\nsignificant levels and empirical power estimates of the tests using extensive\nMonte Carlo simulations. We demonstrate that the new directional tests have\ncomparable performance with the currently available NNCT-tests in terms of\nempirical size and power. We use four example data sets for illustrative\npurposes and provide guidelines for using these NNCT-tests.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2009 22:05:00 GMT"}], "update_date": "2009-02-09", "authors_parsed": [["Ceyhan", "Elvan", ""]]}, {"id": "0902.1323", "submitter": "Giovanni Montana", "authors": "Brian McWilliams, Giovanni Montana", "title": "Sparse partial least squares for on-line variable selection in\n  multivariate data streams", "comments": "26 pages, 6 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a computationally efficient algorithm for on-line\nvariable selection in multivariate regression problems involving high\ndimensional data streams. The algorithm recursively extracts all the latent\nfactors of a partial least squares solution and selects the most important\nvariables for each factor. This is achieved by means of only one sparse\nsingular value decomposition which can be efficiently updated on-line and in an\nadaptive fashion. Simulation results based on artificial data streams\ndemonstrate that the algorithm is able to select important variables in dynamic\nsettings where the correlation structure among the observed streams is governed\nby a few hidden components and the importance of each variable changes over\ntime. We also report on an application of our algorithm to a multivariate\nversion of the \"enhanced index tracking\" problem using financial data streams.\nThe application consists of performing on-line asset allocation with the\nobjective of overperforming two benchmark indices simultaneously.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2009 17:41:11 GMT"}], "update_date": "2009-02-10", "authors_parsed": [["McWilliams", "Brian", ""], ["Montana", "Giovanni", ""]]}, {"id": "0902.1324", "submitter": "Stephane Chretien", "authors": "Stephane Chretien and Franck Corset", "title": "Using the Eigenvalue Relaxation for Binary Least-Squares Estimation\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to survey the properties of the eigenvalue\nrelaxation for least squares binary problems. This relaxation is a convex\nprogram which is obtained as the Lagrangian dual of the original problem with\nan implicit compact constraint and as such, is a convex problem with polynomial\ntime complexity. Moreover, as a main pratical advantage of this relaxation over\nthe standard Semi-Definite Programming approach, several efficient bundle\nmethods are available for this problem allowing to address problems of very\nlarge dimension. The necessary tools from convex analysis are recalled and\nshown at work for handling the problem of exactness of this relaxation. Two\napplications are described. The first one is the problem of binary image\nreconstruction and the second is the problem of multiuser detection in CDMA\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2009 18:38:06 GMT"}, {"version": "v2", "created": "Mon, 9 Feb 2009 22:30:36 GMT"}], "update_date": "2009-02-10", "authors_parsed": [["Chretien", "Stephane", ""], ["Corset", "Franck", ""]]}, {"id": "0902.1360", "submitter": "Shane Jensen", "authors": "Shane T. Jensen, Blake McShane, and Abraham J. Wyner", "title": "Hierarchical Bayesian Modeling of Hitting Performance in Baseball", "comments": null, "journal-ref": "Bayesian Analysis 2009, Vol. 4, No. 4, 631-652", "doi": "10.1214/09-BA424", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a sophisticated statistical model for predicting the\nhitting performance of Major League baseball players. The Bayesian paradigm\nprovides a principled method for balancing past performance with crucial\ncovariates, such as player age and position. We share information across time\nand across players by using mixture distributions to control shrinkage for\nimproved accuracy. We compare the performance of our model to current\nsabermetric methods on a held-out season (2006), and discuss both successes and\nlimitations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2009 03:09:20 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Jensen", "Shane T.", ""], ["McShane", "Blake", ""], ["Wyner", "Abraham J.", ""]]}, {"id": "0902.1426", "submitter": "Weng Kee Wong", "authors": "Holger Dette, Andrey Pepelyshev, Weng Kee Wong", "title": "Optimal designs for dose-finding experiments in toxicity studies", "comments": "Published in at http://dx.doi.org/10.3150/08-BEJ152 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2009, Vol. 15, No. 1, 124-145", "doi": "10.3150/08-BEJ152", "report-no": "IMS-BEJ-BEJ152", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct optimal designs for estimating fetal malformation rate, prenatal\ndeath rate and an overall toxicity index in a toxicology study under a broad\nrange of model assumptions. We use Weibull distributions to model these rates\nand assume that the number of implants depend on the dose level. We study\nproperties of the optimal designs when the intra-litter correlation coefficient\ndepends on the dose levels in different ways. Locally optimal designs are\nfound, along with robustified versions of the designs that are less sensitive\nto misspecification in the initial values of the model parameters. We also\nreport efficiencies of commonly used designs in toxicological experiments and\nefficiencies of the proposed optimal designs when the true rates have\nnon-Weibull distributions. Optimal design strategies for finding\nmultiple-objective designs in toxicology studies are outlined as well.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2009 12:40:22 GMT"}], "update_date": "2009-02-10", "authors_parsed": [["Dette", "Holger", ""], ["Pepelyshev", "Andrey", ""], ["Wong", "Weng Kee", ""]]}, {"id": "0902.1598", "submitter": "M. Kachour", "authors": "M. Kachour", "title": "p-order rounded integer-valued autoregressive (RINAR(p)) process", "comments": "Submitted to the Electronic Journal of Statistics\n  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": null, "doi": null, "report-no": "IMS-EJS-EJS_2009_369", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An extension of the RINAR(1) process for modelling discrete-time dependent\ncounting processes is considered. The model RINAR(p) investigated here is a\ndirect and natural extension of the real AR(p) model. Compared to classical\nINAR(p) models based on the thinning operator, the new models have several\nadvantages: simple innovation structure ; autoregressive coefficients with\narbitrary signs ; possible negative values for time series ; possible negative\nvalues for the autocorrelation function. The conditions for the stationarity\nand ergodicity, of the RINAR(p) model, are given. For parameter estimation, we\nconsider the least squares estimator and we prove its consistency under\nsuitable identifiability condition. Simulation experiments as well as analysis\nof real data sets are carried out to assess the performance of the model.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2009 08:05:15 GMT"}], "update_date": "2009-02-11", "authors_parsed": [["Kachour", "M.", ""]]}, {"id": "0902.2165", "submitter": "Jeremie Bigot", "authors": "Jeremie Bigot", "title": "On the usefulness of Meyer wavelets for deconvolution and density\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to show the usefulness of Meyer wavelets for the\nclassical problem of density estimation and for density deconvolution from\nnoisy observations. By using such wavelets, the computation of the empirical\nwavelet coefficients relies on the fast Fourier transform of the data and on\nthe fact that Meyer wavelets are band-limited functions. This makes such\nestimators very simple to compute and this avoids the problem of evaluating\nwavelets at non-dyadic points which is the main drawback of classical\nwavelet-based density estimators. Our approach is based on term-by-term\nthresholding of the empirical wavelet coefficients with random thresholds\ndepending on an estimation of the variance of each coefficient. Such estimators\nare shown to achieve the same performances of an oracle estimator up to a\nlogarithmic term. These estimators also achieve near-minimax rates of\nconvergence over a large class of Besov spaces. A simulation study is proposed\nto show the good finite sample performances of the estimator for both problems\nof direct density estimation and density deconvolution.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2009 16:55:31 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2009 13:31:25 GMT"}, {"version": "v3", "created": "Tue, 24 Feb 2009 16:53:10 GMT"}, {"version": "v4", "created": "Fri, 27 Feb 2009 10:42:43 GMT"}, {"version": "v5", "created": "Tue, 24 Mar 2009 17:00:13 GMT"}, {"version": "v6", "created": "Thu, 26 Mar 2009 15:08:08 GMT"}, {"version": "v7", "created": "Fri, 27 Mar 2009 08:59:51 GMT"}], "update_date": "2009-03-27", "authors_parsed": [["Bigot", "Jeremie", ""]]}, {"id": "0902.2296", "submitter": "Werner Ehm", "authors": "Werner Ehm, J\\\"urgen Kornmeier, Sven Heinrich", "title": "Multiple testing via successive subdivision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sequential multiple testing procedure recently introduced by Heinrich, Bach\nand Kornmeier allows to \"zoom in\" on, and thus identify regions with highly\nsignificant departures from null-hypotheses. The purpose of this note is to\nstate a cognate of this procedure in general form and to prove that it controls\nthe familywise error. Two possible applications are briefly indicated.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2009 11:49:17 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2009 10:58:48 GMT"}], "update_date": "2009-04-30", "authors_parsed": [["Ehm", "Werner", ""], ["Kornmeier", "J\u00fcrgen", ""], ["Heinrich", "Sven", ""]]}, {"id": "0902.2924", "submitter": "Pierre Alquier", "authors": "Pierre Alquier (LPMA, CREST), Olivier Wintenberger (CEREMADE)", "title": "Model selection for weakly dependent time series forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing a stationary time series, we propose a two-step procedure for the\nprediction of the next value of the time series. The first step follows machine\nlearning theory paradigm and consists in determining a set of possible\npredictors as randomized estimators in (possibly numerous) different predictive\nmodels. The second step follows the model selection paradigm and consists in\nchoosing one predictor with good properties among all the predictors of the\nfirst steps. We study our procedure for two different types of bservations:\ncausal Bernoulli shifts and bounded weakly dependent processes. In both cases,\nwe give oracle inequalities: the risk of the chosen predictor is close to the\nbest prediction risk in all predictive models that we consider. We apply our\nprocedure for predictive models such as linear predictors, neural networks\npredictors and non-parametric autoregressive.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2009 13:40:29 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2009 08:08:48 GMT"}, {"version": "v3", "created": "Mon, 6 Sep 2010 18:36:01 GMT"}, {"version": "v4", "created": "Tue, 3 Jul 2012 13:14:15 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Alquier", "Pierre", "", "LPMA, CREST"], ["Wintenberger", "Olivier", "", "CEREMADE"]]}, {"id": "0902.2994", "submitter": "Giovanni Mana", "authors": "D Calonico, F Levi, L Lorini and G Mana", "title": "Bayesian inference of a negative quantity from positive measurement\n  results", "comments": "10 pages, 4 figures, submitted to Metrologia", "journal-ref": null, "doi": "10.1088/0026-1394/46/3/014", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the Bayesian analysis is applied to assign a probability\ndensity to the value of a quantity having a definite sign. This analysis is\nlogically consistent with the results, positive or negative, of repeated\nmeasurements. Results are used to estimate the atom density shift in a caesium\nfountain clock. The comparison with the classical statistical analysis is also\nreported and the advantages of the Bayesian approach for the realization of the\ntime unit are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2009 20:47:41 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Calonico", "D", ""], ["Levi", "F", ""], ["Lorini", "L", ""], ["Mana", "G", ""]]}, {"id": "0902.3060", "submitter": "Mathieu Ribatet", "authors": "Simone A. Padoan (EFLUM), Mathieu Ribatet, Scott A. Sisson", "title": "Likelihood-based inference for max-stable processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has seen max-stable processes emerge as a common tool for the\nstatistical modeling of spatial extremes. However, their application is\ncomplicated due to the unavailability of the multivariate density function, and\nso likelihood-based methods remain far from providing a complete and flexible\nframework for inference. In this article we develop inferentially practical,\nlikelihood-based methods for fitting max-stable processes derived from a\ncomposite-likelihood approach. The procedure is sufficiently reliable and\nversatile to permit the simultaneous modeling of marginal and dependence\nparameters in the spatial context at a moderate computational cost. The utility\nof this methodology is examined via simulation, and illustrated by the analysis\nof U.S. precipitation extremes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2009 07:37:33 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2009 07:02:07 GMT"}], "update_date": "2009-02-23", "authors_parsed": [["Padoan", "Simone A.", "", "EFLUM"], ["Ribatet", "Mathieu", ""], ["Sisson", "Scott A.", ""]]}, {"id": "0902.3319", "submitter": "Aurore Delaigle", "authors": "Aurore Delaigle, Peter Hall, Tatiyana V. Apanasovich", "title": "Weighted least squares methods for prediction in the functional data\n  linear model", "comments": "Submitted to the Electronic Journal of Statistics\n  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": null, "doi": null, "report-no": "IMS-EJS-EJS_2009_379", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of prediction in functional linear regression is conventionally\naddressed by reducing dimension via the standard principal component basis. In\nthis paper we show that an alternative basis chosen through weighted\nleast-squares, or weighted least-squares itself, can be more effective when the\nexperimental errors are heteroscedastic. We give a concise theoretical result\nwhich demonstrates the effectiveness of this approach, even when the model for\nthe variance is inaccurate, and we explore the numerical properties of the\nmethod. We show too that the advantages of the suggested adaptive techniques\nare not found only in low-dimensional aspects of the problem; rather, they\naccrue almost equally among all dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2009 08:07:34 GMT"}], "update_date": "2009-02-20", "authors_parsed": [["Delaigle", "Aurore", ""], ["Hall", "Peter", ""], ["Apanasovich", "Tatiyana V.", ""]]}, {"id": "0902.3343", "submitter": "Sarjinder Singh", "authors": "Sarjinder Singh, Raghunath Arnab", "title": "On calibration of design weights", "comments": "Submitted to the Test Annals of Probability\n  (http://www.imstat.org/aap/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": null, "doi": null, "report-no": "T-TEST-SS_2009_42", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present investigation, we build a bridge between the generalized\nregression (GREG) estimator due to Deville and Sarndal (1992) and the linear\nregression estimator due to Hansen, Hurwitz and Madow (1953) in the presence of\nsingle auxiliary variable. The bridge confirms that the sum of calibrated\nweights should be equal to sum of design weights as pointed out by Singh (2003,\n2004, 2006) and Stearns and Singh (2008). An important modification in the\nstatistical packages such as GES, SUDAAN etc. has been suggested.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2009 10:58:15 GMT"}], "update_date": "2009-02-20", "authors_parsed": [["Singh", "Sarjinder", ""], ["Arnab", "Raghunath", ""]]}, {"id": "0902.3656", "submitter": "Emilio Porcu", "authors": "Viktor P. Zastavnyi, Emilio Porcu", "title": "Space-time covariance functions with compact support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize completely the Gneiting class of space-time covariance\nfunctions and give more relaxed conditions on the involved functions. We then\nshow necessary conditions for the construction of compactly supported functions\nof the Gneiting type. These conditions are very general since they do not\ndepend on the Euclidean norm. Finally, we discuss a general class of positive\ndefinite functions, used for multivariate Gaussian random fields. For this\nclass, we show necessary criteria for its generator to be compactly supported.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2009 14:14:21 GMT"}], "update_date": "2009-02-24", "authors_parsed": [["Zastavnyi", "Viktor P.", ""], ["Porcu", "Emilio", ""]]}, {"id": "0902.3714", "submitter": "Shu Yang", "authors": "Shu Yang and Eric D. Kolaczyk", "title": "Target Detection via Network Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A method of `network filtering' has been proposed recently to detect the\neffects of certain external perturbations on the interacting members in a\nnetwork. However, with large networks, the goal of detection seems a priori\ndifficult to achieve, especially since the number of observations available\noften is much smaller than the number of variables describing the effects of\nthe underlying network. Under the assumption that the network possesses a\ncertain sparsity property, we provide a formal characterization of the accuracy\nwith which the external effects can be detected, using a network filtering\nsystem that combines Lasso regression in a sparse simultaneous equation model\nwith simple residual analysis. We explore the implications of the technical\nconditions underlying our characterization, in the context of various network\ntopologies, and we illustrate our method using simulated data.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2009 04:02:21 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2010 02:49:29 GMT"}], "update_date": "2010-01-28", "authors_parsed": [["Yang", "Shu", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "0902.3725", "submitter": "K. P. Unnikrishnan", "authors": "Casey Diekman, Kohinoor Dasgupta, Vijay Nair, P.S. Sastry, K.P.\n  Unnikrishnan", "title": "Statistical Inference of Functional Connectivity in Neuronal Networks\n  using Frequent Episodes", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.DB q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the spatio-temporal network structure of brain activity from\nmulti-neuronal data streams is one of the biggest challenges in neuroscience.\nRepeating patterns of precisely timed activity across a group of neurons is\npotentially indicative of a microcircuit in the underlying neural tissue.\nFrequent episode discovery, a temporal data mining framework, has recently been\nshown to be a computationally efficient method of counting the occurrences of\nsuch patterns. In this paper, we propose a framework to determine when the\ncounts are statistically significant by modeling the counting process. Our\nmodel allows direct estimation of the strengths of functional connections\nbetween neurons with improved resolution over previously published methods. It\ncan also be used to rank the patterns discovered in a network of neurons\naccording to their strengths and begin to reconstruct the graph structure of\nthe network that produced the spike data. We validate our methods on simulated\ndata and present analysis of patterns discovered in data from cultures of\ncortical neurons.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2009 02:34:27 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Diekman", "Casey", ""], ["Dasgupta", "Kohinoor", ""], ["Nair", "Vijay", ""], ["Sastry", "P. S.", ""], ["Unnikrishnan", "K. P.", ""]]}, {"id": "0902.3837", "submitter": "Jiashun Jin", "authors": "Peter Hall, Jiashun Jin", "title": "Innovated higher criticism for detecting sparse signals in correlated\n  noise", "comments": "Published in at http://dx.doi.org/10.1214/09-AOS764 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2010, Vol. 38, No. 3, 1686-1732", "doi": "10.1214/09-AOS764", "report-no": "IMS-AOS-AOS764", "categories": "math.ST math.SP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher criticism is a method for detecting signals that are both sparse and\nweak. Although first proposed in cases where the noise variables are\nindependent, higher criticism also has reasonable performance in settings where\nthose variables are correlated. In this paper we show that, by exploiting the\nnature of the correlation, performance can be improved by using a modified\napproach which exploits the potential advantages that correlation has to offer.\nIndeed, it turns out that the case of independent noise is the most difficult\nof all, from a statistical viewpoint, and that more accurate signal detection\n(for a given level of signal sparsity and strength) can be obtained when\ncorrelation is present. We characterize the advantages of correlation by\nshowing how to incorporate them into the definition of an optimal detection\nboundary. The boundary has particularly attractive properties when correlation\ndecays at a polynomial rate or the correlation matrix is Toeplitz.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2009 02:36:44 GMT"}, {"version": "v2", "created": "Mon, 4 Oct 2010 12:50:28 GMT"}], "update_date": "2010-10-05", "authors_parsed": [["Hall", "Peter", ""], ["Jin", "Jiashun", ""]]}, {"id": "0902.3977", "submitter": "Alain Celisse", "authors": "Sylvain Arlot (LIENS), Alain Celisse", "title": "Segmentation of the mean of heteroscedastic data via cross-validation", "comments": null, "journal-ref": "Statistics and Computing (2009) electronic", "doi": "10.1007/s11222-010-9196-x", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of detecting abrupt changes in the mean of a\nheteroscedastic signal by model selection, without knowledge on the variations\nof the noise. A new family of change-point detection procedures is proposed,\nshowing that cross-validation methods can be successful in the heteroscedastic\nframework, whereas most existing procedures are not robust to\nheteroscedasticity. The robustness to heteroscedasticity of the proposed\nprocedures is supported by an extensive simulation study, together with recent\ntheoretical results. An application to Comparative Genomic Hybridization (CGH)\ndata is provided, showing that robustness to heteroscedasticity can indeed be\nrequired for their analysis.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2009 19:27:48 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2009 13:51:05 GMT"}], "update_date": "2011-02-01", "authors_parsed": [["Arlot", "Sylvain", "", "LIENS"], ["Celisse", "Alain", ""]]}, {"id": "0902.4111", "submitter": "Jonathan Lilly", "authors": "Jonathan M. Lilly and Sofia C. Olhede", "title": "Bivariate Instantaneous Frequency and Bandwidth", "comments": null, "journal-ref": "Lilly, J. M., and S. C. Olhede (2010). Bivariate instantaneous\n  frequency and bandwidth. IEEE Transactions on Signal Processing, 58 (2),\n  591--603", "doi": "10.1109/TSP.2009.2031729", "report-no": null, "categories": "stat.ME physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalizations of instantaneous frequency and instantaneous bandwidth to\na bivariate signal are derived. These are uniquely defined whether the signal\nis represented as a pair of real-valued signals, or as one analytic and one\nanti-analytic signal. A nonstationary but oscillatory bivariate signal has a\nnatural representation as an ellipse whose properties evolve in time, and this\nrepresentation provides a simple geometric interpretation for the bivariate\ninstantaneous moments. The bivariate bandwidth is shown to consist of three\nterms measuring the degree of instability of the time-varying ellipse:\namplitude modulation with fixed eccentricity, eccentricity modulation, and\norientation modulation or precession. An application to the analysis of data\nfrom a free-drifting oceanographic float is presented and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2009 10:17:29 GMT"}, {"version": "v2", "created": "Sat, 15 Oct 2011 16:41:18 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["Lilly", "Jonathan M.", ""], ["Olhede", "Sofia C.", ""]]}, {"id": "0902.4137", "submitter": "Lars Holden", "authors": "Lars Holden, Ola Haug", "title": "A mixture model for unsupervised tail estimation", "comments": "Submitted to the Electronic Journal of Statistics\n  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": null, "doi": null, "report-no": "IMS-EJS-EJS_2009_384", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method to combine several densities such that each\ndensity dominates a separate part of a joint distribution. The method is fully\nunsupervised, i.e. the parameters in the densities and the thresholds are\nsimultaneously estimated. The approach uses cdf functions in the mixing. This\nmakes it easy to estimate parameters and the resulting density is smooth. Our\nmethod may be used both when the tails are heavier and lighter than the rest of\nthe distribution. The presented model is compared with other published models\nand a very simple model using a univariate transformation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2009 12:42:07 GMT"}], "update_date": "2009-02-25", "authors_parsed": [["Holden", "Lars", ""], ["Haug", "Ola", ""]]}, {"id": "0902.4380", "submitter": "Nicole Kraemer", "authors": "Gilles Blanchard, Nicole Kraemer", "title": "Kernel Partial Least Squares is Universally Consistent", "comments": "18 pages, no figures", "journal-ref": "JMLR Workshop and Conference Proceedings 9 (AISTATS 2010) 57-64,\n  2010", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the statistical consistency of kernel Partial Least Squares\nRegression applied to a bounded regression learning problem on a reproducing\nkernel Hilbert space. Partial Least Squares stands out of well-known classical\napproaches as e.g. Ridge Regression or Principal Components Regression, as it\nis not defined as the solution of a global cost minimization procedure over a\nfixed model nor is it a linear estimator. Instead, approximate solutions are\nconstructed by projections onto a nested set of data-dependent subspaces. To\nprove consistency, we exploit the known fact that Partial Least Squares is\nequivalent to the conjugate gradient algorithm in combination with early\nstopping. The choice of the stopping rule (number of iterations) is a crucial\npoint. We study two empirical stopping rules. The first one monitors the\nestimation error in each iteration step of Partial Least Squares, and the\nsecond one estimates the empirical complexity in terms of a condition number.\nBoth stopping rules lead to universally consistent estimators provided the\nkernel is universal.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2009 14:25:48 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2010 17:32:09 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Blanchard", "Gilles", ""], ["Kraemer", "Nicole", ""]]}]