[{"id": "0909.0934", "submitter": "Xin Gao Dr.", "authors": "Xin Gao, Daniel Q. Pu, Yuehua Wu and Hong Xu", "title": "Tuning parameter selection for penalized likelihood estimation of\n  inverse covariance matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a Gaussian graphical model, the conditional independence between two\nvariables are characterized by the corresponding zero entries in the inverse\ncovariance matrix. Maximum likelihood method using the smoothly clipped\nabsolute deviation (SCAD) penalty (Fan and Li, 2001) and the adaptive LASSO\npenalty (Zou, 2006) have been proposed in literature. In this article, we\nestablish the result that using Bayesian information criterion (BIC) to select\nthe tuning parameter in penalized likelihood estimation with both types of\npenalties can lead to consistent graphical model selection. We compare the\nempirical performance of BIC with cross validation method and demonstrate the\nadvantageous performance of BIC criterion for tuning parameter selection\nthrough simulation studies.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2009 18:25:22 GMT"}], "update_date": "2009-09-07", "authors_parsed": [["Gao", "Xin", ""], ["Pu", "Daniel Q.", ""], ["Wu", "Yuehua", ""], ["Xu", "Hong", ""]]}, {"id": "0909.1008", "submitter": "Christian P. Robert", "authors": "Christian P. Robert, Nicolas Chopin, Judith Rousseau", "title": "Rejoinder: Harold Jeffreys's Theory of Probability Revisited", "comments": "Published in at http://dx.doi.org/10.1214/09-STS284REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science (2009), Vol. 24, No. 2, 191-194", "doi": "10.1214/09-STS284REJ", "report-no": "IMS-STS-STS284REJ", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are grateful to all discussants of our re-visitation for their strong\nsupport in our enterprise and for their overall agreement with our perspective.\nFurther discussions with them and other leading statisticians showed that the\nlegacy of Theory of Probability is alive and lasting. [arXiv:0804.3173]\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2009 09:06:10 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2010 09:57:48 GMT"}], "update_date": "2010-10-11", "authors_parsed": [["Robert", "Christian P.", ""], ["Chopin", "Nicolas", ""], ["Rousseau", "Judith", ""]]}, {"id": "0909.1046", "submitter": "Didier Girard", "authors": "Didier A. Girard (IPS)", "title": "Asymptotic near-efficiency of the ''Gibbs-energy (GE) and\n  empirical-variance'' estimating functions for fitting Mat{\\'e}rn models --\n  II: Accounting for measurement errors via ''conditional GE mean''", "comments": "The previous version (version 2) considered both the case with\n  measurement errors (also called ''nugget-effect'' or simply ''noise'') and\n  the no-noise case. The no-noise case is now in Girard (2016) with more\n  detailed proofs and two additional (wrt version 2) results: a consistency\n  result is proved and the restriction $ \\nu \\geq 1/2$ is eliminated. This\n  version 3 is devoted to the case with measurement errors, and also gives the\n  analogs of these two additional results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider one realization of a continuous-time Gaussian process $Z$ which\nbelongs to the Mat\\' ern family with known ``regularity'' index $\\nu >0$. For\nestimating the autocorrelation-range and the variance of $Z$ from $n$\nobservations on a fine grid, we studied in Girard (2016) the GE-EV method which\nsimply retains the empirical variance (EV) and equates it to a candidate\n``Gibbs energy (GE)'' i.e.~the quadratic form ${\\bf z}^T R^{-1} {\\bf z}/n$\nwhere ${\\bf z}$ is the vector of observations and $R$ is the autocorrelation\nmatrix for ${\\bf z}$ associated with a candidate range. The present study\nconsiders the case where the observation is ${\\bf z}$ plus a Gaussian white\nnoise whose variance is known. We propose to simply bias-correct EV and to\nreplace GE by its conditional mean given the observation. We show that the\nratio of the large-$n$ mean squared error of the resulting CGEM-EV estimate of\nthe range-parameter to the one of its maximum likelihood estimate, and the\nanalog ratio for the variance-parameter, have the same behavior than in the\nno-noise case: they both converge, when the grid-step tends to $0$, toward a\nconstant, only function of $\\nu$, surprisingly close to $1$ provided $\\nu$ is\nnot too large. We also obtain, for all $\\nu$, convergence to 1 of the analog\nratio for the microergodic-parameter.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2009 18:40:28 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2012 16:48:21 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2019 12:14:27 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Girard", "Didier A.", "", "IPS"]]}, {"id": "0909.1123", "submitter": "Heng Lian", "authors": "Heng Lian", "title": "Shrinkage Tuning Parameter Selection in Precision Matrices Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature provides many computational and modeling approaches for\ncovariance matrices estimation in a penalized Gaussian graphical models but\nrelatively little study has been carried out on the choice of the tuning\nparameter. This paper tries to fill this gap by focusing on the problem of\nshrinkage parameter selection when estimating sparse precision matrices using\nthe penalized likelihood approach. Previous approaches typically used K-fold\ncross-validation in this regard. In this paper, we first derived the\ngeneralized approximate cross-validation for tuning parameter selection which\nis not only a more computationally efficient alternative, but also achieves\nsmaller error rate for model fitting compared to leave-one-out\ncross-validation. For consistency in the selection of nonzero entries in the\nprecision matrix, we employ a Bayesian information criterion which provably can\nidentify the nonzero conditional correlations in the Gaussian model. Our\nsimulations demonstrate the general superiority of the two proposed selectors\nin comparison with leave-one-out cross-validation, ten-fold cross-validation\nand Akaike information criterion.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2009 00:47:12 GMT"}], "update_date": "2009-09-08", "authors_parsed": [["Lian", "Heng", ""]]}, {"id": "0909.1373", "submitter": "Seyoung Kim", "authors": "Seyoung Kim, Eric P. Xing", "title": "Tree-guided group lasso for multi-response regression with structured\n  sparsity, with an application to eQTL mapping", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS549 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 1095-1117", "doi": "10.1214/12-AOAS549", "report-no": "IMS-AOAS-AOAS549", "categories": "stat.ML q-bio.GN q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a sparse multi-response regression\nfunction, with an application to expression quantitative trait locus (eQTL)\nmapping, where the goal is to discover genetic variations that influence\ngene-expression levels. In particular, we investigate a shrinkage technique\ncapable of capturing a given hierarchical structure over the responses, such as\na hierarchical clustering tree with leaf nodes for responses and internal nodes\nfor clusters of related responses at multiple granularity, and we seek to\nleverage this structure to recover covariates relevant to each\nhierarchically-defined cluster of responses. We propose a tree-guided group\nlasso, or tree lasso, for estimating such structured sparsity under\nmulti-response regression by employing a novel penalty function constructed\nfrom the tree. We describe a systematic weighting scheme for the overlapping\ngroups in the tree-penalty such that each regression coefficient is penalized\nin a balanced manner despite the inhomogeneous multiplicity of group\nmemberships of the regression coefficients due to overlaps among groups. For\nefficient optimization, we employ a smoothing proximal gradient method that was\noriginally developed for a general class of structured-sparsity-inducing\npenalties. Using simulated and yeast data sets, we demonstrate that our method\nshows a superior performance in terms of both prediction errors and recovery of\ntrue sparsity patterns, compared to other methods for learning a\nmultivariate-response regression.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2009 02:29:12 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2009 19:36:16 GMT"}, {"version": "v3", "created": "Fri, 28 Sep 2012 11:17:56 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Kim", "Seyoung", ""], ["Xing", "Eric P.", ""]]}, {"id": "0909.1527", "submitter": "Violeta Calian", "authors": "V. Calian, G. Stefansson, L. P. Folkow, A.S. Blix", "title": "Estimating migration proportions from discretely observed continuous\n  diffusion processes", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model two time and space scales discrete observations by using a unique\ncontinuous diffusion process with time dependent coefficient. We define new\nparameters for the large scale model as functions of the small scale\ndistribution cumulants. We use the non - uniform distribution of the\nobservation time intervals to obtain consistent and unbiased estimators for\nthese parameters. Closed form expressions for migration proportions between\nspatial domains are derived as functions of these parameters. The models are\napplied to estimate migration patterns from satellite tag data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2009 17:46:33 GMT"}], "update_date": "2009-09-09", "authors_parsed": [["Calian", "V.", ""], ["Stefansson", "G.", ""], ["Folkow", "L. P.", ""], ["Blix", "A. S.", ""]]}, {"id": "0909.1685", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Structure Variability in Bayesian Networks", "comments": "21 pages, 4 figures", "journal-ref": "merged and published as part of Bayesian Analysis 2013, 8(3),\n  505-532", "doi": null, "report-no": "Working Paper 13 - 2009, Department of Statistical Sciences,\n  University of Padova", "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of a Bayesian network encodes most of the information about the\nprobability distribution of the data, which is uniquely identified given some\ngeneral distributional assumptions. Therefore it's important to study the\nvariability of its network structure, which can be used to compare the\nperformance of different learning algorithms and to measure the strength of any\narbitrary subset of arcs.\n  In this paper we will introduce some descriptive statistics and the\ncorresponding parametric and Monte Carlo tests on the undirected graph\nunderlying the structure of a Bayesian network, modeled as a multivariate\nBernoulli random variable.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2009 11:52:12 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2009 16:22:16 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2009 15:14:13 GMT"}, {"version": "v4", "created": "Sun, 16 May 2010 23:13:11 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "0909.1884", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot (LIENS, INRIA Paris - Rocquencourt), Francis Bach\n  (LIENS, INRIA Paris - Rocquencourt)", "title": "Data-driven calibration of linear estimators with minimal penalties", "comments": "Advances in Neural Information Processing Systems (NIPS 2009),\n  Vancouver : Canada (2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of selecting among several linear estimators\nin non-parametric regression; this includes model selection for linear\nregression, the choice of a regularization parameter in kernel ridge\nregression, spline smoothing or locally weighted regression, and the choice of\na kernel in multiple kernel learning. We propose a new algorithm which first\nestimates consistently the variance of the noise, based upon the concept of\nminimal penalty, which was previously introduced in the context of model\nselection. Then, plugging our variance estimate in Mallows' $C_L$ penalty is\nproved to lead to an algorithm satisfying an oracle inequality. Simulation\nexperiments with kernel ridge regression and multiple kernel learning show that\nthe proposed algorithm often improves significantly existing calibration\nprocedures such as generalized cross-validation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 08:14:16 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2011 07:13:39 GMT"}], "update_date": "2011-09-15", "authors_parsed": [["Arlot", "Sylvain", "", "LIENS, INRIA Paris - Rocquencourt"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "0909.1915", "submitter": "Ikhlef  Bechar", "authors": "Ikhlef Bechar", "title": "Non-asymptotic model selection for linear non least-squares estimation\n  in regression models and inverse problems", "comments": "39 pages including the references, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to address the common problem of linear estimation in linear\nstatistical models by using a model selection approach via penalization.\nDepending then on the framework in which the linear statistical model is\nconsidered namely the regression framework or the inverse problem framework, a\ndata-driven model selection criterion is obtained either under general\nassumptions, or under the mild assumption of model identifiability\nrespectively. The proposed approach was stimulated by the important recent\nnon-asymptotic model selection results due to Birg\\'e and Massart mainly (Birge\nand Massart 2007), and our results in this paper, like theirs, are\nnon-asymptotic and turn to be sharp.\n  Our main contribution in this paper resides in the fact that these linear\nestimators are not necessarily least-squares estimators but can be any linear\nestimators. The proposed approach finds therefore potential applications in\ncountless fields of engineering and applied science (image science, signal\nprocessing,applied statistics, coding, to name a few) in which one is\ninterested in recovering some unknown vector quantity of interest as the one,\nfor example, which achieves the best trade-off between a term of fidelity to\ndata, and a term of regularity or/and parsimony of the solution. The proposed\napproach provides then such applications with an interesting model selection\nframework that allows them to achieve such a goal.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2009 10:24:18 GMT"}], "update_date": "2009-09-11", "authors_parsed": [["Bechar", "Ikhlef", ""]]}, {"id": "0909.2332", "submitter": "David Hardoon", "authors": "David R. Hardoon, Zakria Hussain, John Shawe-Taylor", "title": "A Nonconformity Approach to Model Selection for SVMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the issue of model selection and the use of the nonconformity\n(strangeness) measure in batch learning. Using the nonconformity measure we\npropose a new training algorithm that helps avoid the need for Cross-Validation\nor Leave-One-Out model selection strategies. We provide a new generalisation\nerror bound using the notion of nonconformity to upper bound the loss of each\ntest example and show that our proposed approach is comparable to standard\nmodel selection methods, but with theoretical guarantees of success and faster\nconvergence. We demonstrate our novel model selection technique using the\nSupport Vector Machine.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2009 13:31:41 GMT"}], "update_date": "2009-09-15", "authors_parsed": [["Hardoon", "David R.", ""], ["Hussain", "Zakria", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "0909.2904", "submitter": "Shohei Shimizu", "authors": "Yusuke Komatsu, Shohei Shimizu, Hidetoshi Shimodaira", "title": "Computing p-values of LiNGAM outputs via Multiscale Bootstrap", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural equation models and Bayesian networks have been widely used to\nstudy causal relationships between continuous variables. Recently, a\nnon-Gaussian method called LiNGAM was proposed to discover such causal models\nand has been extended in various directions. An important problem with LiNGAM\nis that the results are affected by the random sampling of the data as with any\nstatistical method. Thus, some analysis of the statistical reliability or\nconfidence level should be conducted. A common method to evaluate a confidence\nlevel is a bootstrap method. However, a confidence level computed by ordinary\nbootstrap method is known to be biased as a probability-value ($p$-value) of\nhypothesis testing. In this paper, we propose a new procedure to apply an\nadvanced bootstrap method called multiscale bootstrap to compute confidence\nlevels, i.e., p-values, of LiNGAM outputs. The multiscale bootstrap method\ngives unbiased $p$-values with asymptotic much higher accuracy. Experiments on\nartificial data demonstrate the utility of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2009 01:34:16 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2010 04:12:33 GMT"}], "update_date": "2010-06-23", "authors_parsed": [["Komatsu", "Yusuke", ""], ["Shimizu", "Shohei", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "0909.3034", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan", "title": "Spatial Clustering Tests Based on Domination Number of a New Random\n  Digraph Family", "comments": "30 pages, 7 table, and 19 figures", "journal-ref": null, "doi": null, "report-no": "Technical Report # KU-EC-09-6", "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the domination number of a parametrized random digraph family called\nproportional-edge proximity catch digraphs (PCDs) for testing multivariate\nspatial point patterns. This digraph family is based on relative positions of\ndata points from various classes. We extend the results on the distribution of\nthe domination number of proportional-edge PCDs, and use the domination number\nas a statistic for testing segregation and association against complete spatial\nrandomness. We demonstrate that the domination number of the PCD has binomial\ndistribution when size of one class is fixed while the size of the other (whose\npoints constitute the vertices of the digraph) tends to infinity and asymptotic\nnormality when sizes of both classes tend to infinity. We evaluate the finite\nsample performance of the test by Monte Carlo simulations, prove the\nconsistency of the test under the alternatives, and suggest corrections for the\nsupport restriction on the class of points of interest and for small samples.\nWe find the optimal parameters for testing each of the segregation and\nassociation alternatives. Furthermore, the methodology discussed in this\narticle is valid for data in higher dimensions also.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2009 14:59:16 GMT"}], "update_date": "2009-09-17", "authors_parsed": [["Ceyhan", "Elvan", ""]]}, {"id": "0909.3052", "submitter": "Patrick Perry", "authors": "Patrick O. Perry", "title": "Cross-Validation for Unsupervised Learning", "comments": "Ph.D. thesis, Stanford University, 2009. Adviser: Art B. Owen. 165\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation (CV) is a popular method for model-selection. Unfortunately,\nit is not immediately obvious how to apply CV to unsupervised or exploratory\ncontexts. This thesis discusses some extensions of cross-validation to\nunsupervised learning, specifically focusing on the problem of choosing how\nmany principal components to keep. We introduce the latent factor model, define\nan objective criterion, and show how CV can be used to estimate the intrinsic\ndimensionality of a data set. Through both simulation and theory, we\ndemonstrate that cross-validation is a valuable tool for unsupervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2009 16:16:48 GMT"}], "update_date": "2009-09-17", "authors_parsed": [["Perry", "Patrick O.", ""]]}, {"id": "0909.3704", "submitter": "Amit Zeisel", "authors": "Amit Zeisel, Or Zuk, Eytan Domany", "title": "FDR control with adaptive procedures and FDR monotonicity", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS399 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2A, 943-968", "doi": "10.1214/10-AOAS399", "report-no": "IMS-AOAS-AOAS399", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The steep rise in availability and usage of high-throughput technologies in\nbiology brought with it a clear need for methods to control the False Discovery\nRate (FDR) in multiple tests. Benjamini and Hochberg (BH) introduced in 1995 a\nsimple procedure and proved that it provided a bound on the expected value,\n$\\mathit{FDR}\\leq q$. Since then, many authors tried to improve the BH bound,\nwith one approach being designing adaptive procedures, which aim at estimating\nthe number of true null hypothesis in order to get a better FDR bound. Our two\nmain rigorous results are the following: (i) a theorem that provides a bound on\nthe FDR for adaptive procedures that use any estimator for the number of true\nhypotheses ($m_0$), (ii) a theorem that proves a monotonicity property of\ngeneral BH-like procedures, both for the case where the hypotheses are\nindependent. We also propose two improved procedures for which we prove FDR\ncontrol for the independent case, and demonstrate their advantages over several\navailable bounds, on simulated data and on a large number of gene expression\ndata sets. Both applications are simple and involve a similar amount of\ncomputation as the original BH procedure. We compare the performance of our\nproposed procedures with BH and other procedures and find that in most cases we\nget more power for the same level of statistical significance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2009 08:13:29 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2011 12:57:10 GMT"}], "update_date": "2011-08-11", "authors_parsed": [["Zeisel", "Amit", ""], ["Zuk", "Or", ""], ["Domany", "Eytan", ""]]}, {"id": "0909.4046", "submitter": "Paul Rochet", "authors": "Fabrice Gamboa (M\\'ethodes d'Analyse Stochastique des Codes et\n  Traitements Num\\'eriques), Jean-Michel Loubes (LM-Orsay), Paul Rochet (IMT)", "title": "Maximum Entropy Estimation for Survey sampling", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration methods have been widely studied in survey sampling over the last\ndecades. Viewing calibration as an inverse problem, we extend the calibration\ntechnique by using a maximum entropy method. Finding the optimal weights is\nachieved by considering random weights and looking for a discrete distribution\nwhich maximizes an entropy under the calibration constraint. This method points\na new frame for the computation of such estimates and the investigation of its\nstatistical properties.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2009 19:32:17 GMT"}], "update_date": "2009-09-23", "authors_parsed": [["Gamboa", "Fabrice", "", "M\u00e9thodes d'Analyse Stochastique des Codes et\n  Traitements Num\u00e9riques"], ["Loubes", "Jean-Michel", "", "LM-Orsay"], ["Rochet", "Paul", "", "IMT"]]}, {"id": "0909.4129", "submitter": "Yaming Yu", "authors": "Yaming Yu", "title": "Efficient Simulation of a Bivariate Exponential Conditionals\n  Distribution", "comments": "only 5 pages!", "journal-ref": "Computational Statistics & Data Analysis 52 (2008) 2273-2276", "doi": "10.1016/j.csda.2007.10.013", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bivariate distribution with exponential conditionals (BEC) is introduced\nby Arnold and Strauss [Bivariate distributions with exponential conditionals,\nJ. Amer. Statist. Assoc. 83 (1988) 522--527]. This work presents a simple and\nfast algorithm for simulating random variates from this density.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2009 03:47:31 GMT"}], "update_date": "2009-09-24", "authors_parsed": [["Yu", "Yaming", ""]]}, {"id": "0909.4551", "submitter": "Yaakov Malinovsky", "authors": "Yaakov Malinovsky and Yosef Rinott", "title": "Prediction of Ordered Random Effects in a Simple Small Area Model", "comments": "30 pages, 6 figures", "journal-ref": "Statistica Sinica 20 (2010), 697-714", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of a vector of ordered parameters or part of it arises naturally\nin the context of Small Area Estimation (SAE). For example, one may want to\nestimate the parameters associated with the top ten areas, the best or worst\narea, or a certain percentile. We use a simple SAE model to show that\nestimation of ordered parameters by the corresponding ordered estimates of each\narea separately does not yield good results with respect to MSE. Shrinkage-type\npredictors, with an appropriate amount of shrinkage for the particular problem\nof ordered parameters, are considerably better, and their performance is close\nto that of the optimal predictors, which cannot in general be computed\nexplicitly.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2009 20:50:28 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Malinovsky", "Yaakov", ""], ["Rinott", "Yosef", ""]]}, {"id": "0909.4856", "submitter": "Marloes Maathuis", "authors": "Marloes H. Maathuis and Michael G. Hudgens", "title": "Nonparametric inference for competing risks current status data with\n  continuous, discrete or grouped observation times", "comments": "16 pages, 3 figures", "journal-ref": "Biometrika 2011, Vol. 98, No. 2, 325-340", "doi": "10.1093/biomet/asq083", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New methods and theory have recently been developed to nonparametrically\nestimate cumulative incidence functions for competing risks survival data\nsubject to current status censoring. In particular, the limiting distribution\nof the nonparametric maximum likelihood estimator and a simplified \"naive\nestimator\" have been established under certain smoothness conditions. In this\npaper, we establish the large-sample behavior of these estimators in two\nadditional models, namely when the observation time distribution has discrete\nsupport and when the observation times are grouped. These asymptotic results\nare applied to the construction of confidence intervals in the three different\nmodels. The methods are illustrated on two data sets regarding the cumulative\nincidence of (i) different types of menopause from a cross-sectional sample of\nwomen in the United States and (ii) subtype-specific HIV infection from a\nsero-prevalence study in injecting drug users in Thailand.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2009 11:06:10 GMT"}, {"version": "v2", "created": "Wed, 25 Aug 2010 10:38:32 GMT"}, {"version": "v3", "created": "Mon, 20 Dec 2010 20:01:30 GMT"}], "update_date": "2012-01-12", "authors_parsed": [["Maathuis", "Marloes H.", ""], ["Hudgens", "Michael G.", ""]]}, {"id": "0909.5262", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Nicholas G. Polson", "title": "Particle learning of Gaussian process models for sequential design and\n  optimization", "comments": "18 pages, 5 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a simulation-based method for the online updating of Gaussian\nprocess regression and classification models. Our method exploits sequential\nMonte Carlo to produce a fast sequential design algorithm for these models\nrelative to the established MCMC alternative. The latter is less ideal for\nsequential design since it must be restarted and iterated to convergence with\nthe inclusion of each new design point. We illustrate some attractive ensemble\naspects of our SMC approach, and show how active learning heuristics may be\nimplemented via particles to optimize a noisy function or to explore\nclassification boundaries online.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2009 04:25:10 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2010 10:24:46 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2010 12:20:42 GMT"}], "update_date": "2010-07-07", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Polson", "Nicholas G.", ""]]}, {"id": "0909.5299", "submitter": "Melvin Varughese", "authors": "Melvin M. Varughese", "title": "Parameter Estimation for Multivariate Diffusion Systems", "comments": "22 pages, 4 figures. Accepted in Computational Statistics and Data\n  Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion processes are widely used for modelling real-world phenomena.\nExcept for select cases however, analytical expressions do not exist for a\ndiffusion process' transitional probabilities. It is proposed that the cumulant\ntruncation procedure can be applied to predict the evolution of the cumulants\nof the system. These predictions may be subsequently used within the\nsaddlepoint procedure to approximate the transitional probabilities. An\napproximation to the likelihood of the diffusion system is then easily derived.\nThe method is applicable for a wide-range of diffusion systems - including\nmultivariate, irreducible diffusion systems that existing estimation schemes\nstruggle with. Not only is the accuracy of the saddlepoint comparable with the\nHermite expansion - a popular approximation to a diffusion system's\ntransitional density - it also appears to be less susceptible to increasing\nlags between successive samplings of the diffusion process. Furthermore, the\nsaddlepoint is more stable in regions of the parameter space that are far from\nthe maximum likelihood estimates. Hence, the saddlepoint method can be\nnaturally incorporated within a Markov Chain Monte Carlo (MCMC) routine in\norder to provide reliable estimates and credibility intervals of the diffusion\nmodel's parameters. The method is applied to fit the Heston model to daily\nobservations of the S&P 500 and VIX indices from December 2009 to November\n2010.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2009 10:09:51 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2012 02:03:09 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Varughese", "Melvin M.", ""]]}, {"id": "0909.5369", "submitter": "Christian P. Robert", "authors": "Christian P. Robert", "title": "On the relevance of the Bayesian approach to Statistics", "comments": "This paper is written in conjunction with the 3rd Bayesian\n  econometrics meeting that took place at the Rimini Centre for Economic\n  Analysis on July 01-02, 2009. A version will eventually be published in the\n  Review of Economic Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue here about the relevance and the ultimate unity of the Bayesian\napproach in a neutral and agnostic manner. Our main theme is that Bayesian data\nanalysis is an effective tool for handling complex models, as proven by the\nincreasing proportion of Bayesian studies in the applied sciences. We disregard\nin this essay the philosophical debates on the deeper meaning of probability\nand on the random nature of parameters as things of the past that do a\ndisservice to the approach and are incomprehensible to most bystanders.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2009 19:30:09 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2010 18:07:05 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2010 10:45:01 GMT"}], "update_date": "2010-03-26", "authors_parsed": [["Robert", "Christian P.", ""]]}, {"id": "0909.5390", "submitter": "Victoria Zinde-Walsh", "authors": "Victoria Zinde-Walsh", "title": "Errors-in-variables models: a generalized functions approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification in errors-in-variables regression models was recently extended\nto wide models classes by S. Schennach (Econometrica, 2007) (S) via use of\ngeneralized functions. In this paper the problems of non- and semi- parametric\nidentification in such models are re-examined. Nonparametric identification\nholds under weaker assumptions than in (S); the proof here does not rely on\ndecomposition of generalized functions into ordinary and singular parts, which\nmay not hold. A consistent nonparametric plug-in estimator for regression\nfunctions in the space of absolutely integrable functions constructed.\nSemiparametric identification via a finite set of moments is shown to hold for\nclasses of functions that are explicitly characterized; unlike (S) existence of\na moment generating function for the measurement error is not required.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2009 17:08:58 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Zinde-Walsh", "Victoria", ""]]}, {"id": "0909.5673", "submitter": "Christian P. Robert", "authors": "Christian P. Robert, Kerrie L. Mengersen and Carla Chen", "title": "Model choice versus model criticism", "comments": "This is a comment on the recent paper by Ratmann, Andrieu, Wiuf, and\n  Richardson (PNAS, 106), submitted too late for PNAS to consider it", "journal-ref": null, "doi": "10.1073/pnas.0911260107", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new perspectives on ABC and Bayesian model criticisms presented in\nRatmann et al.(2009) are challenging standard approaches to Bayesian model\nchoice. We discuss here some issues arising from the authors' approach,\nincluding prior influence, model assessment and criticism, and the meaning of\nerror in ABC.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2009 17:35:43 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2009 03:50:56 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Robert", "Christian P.", ""], ["Mengersen", "Kerrie L.", ""], ["Chen", "Carla", ""]]}]