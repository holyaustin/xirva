[{"id": "2010.00050", "submitter": "Ian Dryden", "authors": "Katie E. Severn, Ian L. Dryden and Simon P. Preston", "title": "Non-parametric regression for networks", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data are becoming increasingly available, and so there is a need to\ndevelop suitable methodology for statistical analysis. Networks can be\nrepresented as graph Laplacian matrices, which are a type of manifold-valued\ndata. Our main objective is to estimate a regression curve from a sample of\ngraph Laplacian matrices conditional on a set of Euclidean covariates, for\nexample in dynamic networks where the covariate is time. We develop an adapted\nNadaraya-Watson estimator which has uniform weak consistency for estimation\nusing Euclidean and power Euclidean metrics. We apply the methodology to the\nEnron email corpus to model smooth trends in monthly networks and highlight\nanomalous networks. Another motivating application is given in corpus\nlinguistics, which explores trends in an author's writing style over time based\non word co-occurrence networks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 18:30:50 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Severn", "Katie E.", ""], ["Dryden", "Ian L.", ""], ["Preston", "Simon P.", ""]]}, {"id": "2010.00060", "submitter": "Cheng-Shang Chang", "authors": "Yi-Jheng Lin, Che-Hao Yu, Tzu-Hsuan Liu, Cheng-Shang Chang, Wen-Tsuen\n  Chen", "title": "Constructions and Comparisons of Pooling Matrices for Pooled Testing of\n  COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.DM cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In comparison with individual testing, group testing (also known as pooled\ntesting) is more efficient in reducing the number of tests and potentially\nleading to tremendous cost reduction. As indicated in the recent article posted\non the US FDA website, the group testing approach for COVID-19 has received a\nlot of interest lately. There are two key elements in a group testing\ntechnique: (i) the pooling matrix that directs samples to be pooled into\ngroups, and (ii) the decoding algorithm that uses the group test results to\nreconstruct the status of each sample. In this paper, we propose a new family\nof pooling matrices from packing the pencil of lines (PPoL) in a finite\nprojective plane. We compare their performance with various pooling matrices\nproposed in the literature, including 2D-pooling, P-BEST, and Tapestry, using\nthe two-stage definite defectives (DD) decoding algorithm. By conducting\nextensive simulations for a range of prevalence rates up to 5%, our numerical\nresults show that there is no pooling matrix with the lowest relative cost in\nthe whole range of the prevalence rates. To optimize the performance, one\nshould choose the right pooling matrix, depending on the prevalence rate. The\nfamily of PPoL matrices can dynamically adjust their column weights according\nto the prevalence rates and could be a better alternative than using a fixed\npooling matrix.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 19:05:23 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 13:51:59 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Lin", "Yi-Jheng", ""], ["Yu", "Che-Hao", ""], ["Liu", "Tzu-Hsuan", ""], ["Chang", "Cheng-Shang", ""], ["Chen", "Wen-Tsuen", ""]]}, {"id": "2010.00061", "submitter": "Fei Gao", "authors": "Fei Gao, Fan Xia, Kwun Chuen Gary Chan", "title": "Defining and Estimating Subgroup Mediation Effects with Semi-Competing\n  Risks Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many medical studies, an ultimate failure event such as death is likely to\nbe affected by the occurrence and timing of other intermediate clinical events.\nBoth event times are subject to censoring by loss-to-follow-up but the\nnonterminal event may further be censored by the occurrence of the primary\noutcome, but not vice versa. To study the effect of an intervention on both\nevents, the intermediate event may be viewed as a mediator, but conventional\ndefinition of direct and indirect effects is not applicable due to\nsemi-competing risks data structure. We define three principal strata based on\nwhether the potential intermediate event occurs before the potential failure\nevent, which allow proper definition of direct and indirect effects in one\nstratum whereas total effects are defined for all strata. We discuss the\nidentification conditions for stratum-specific effects, and proposed a\nsemiparametric estimator based on a multivariate logistic stratum membership\nmodel and within-stratum proportional hazards models for the event times. By\ntreating the unobserved stratum membership as a latent variable, we propose an\nEM algorithm for computation. We study the asymptotic properties of the\nestimators by the modern empirical process theory and examine the performance\nof the estimators in numerical studies.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 19:08:03 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 17:47:58 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Gao", "Fei", ""], ["Xia", "Fan", ""], ["Chan", "Kwun Chuen Gary", ""]]}, {"id": "2010.00165", "submitter": "Mamadou Yauck", "authors": "Mamadou Yauck, Erica E. M. Moodie, Herak Apelian, Alain Fourmigue,\n  Daniel Grace, Trevor A. Hart, Gilles Lambert, Joseph Cox", "title": "Neighbourhood Bootstrap for Respondent-Driven Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-Driven Sampling (RDS) is a form of link-tracing sampling, a\nsampling technique used for `hard-to-reach' populations that aims to leverage\nindividuals' social relationships to reach potential participants. While the\nmethodological focus has been restricted to the estimation of population\nproportions, there is a growing interest in the estimation of uncertainty for\nRDS as recent findings suggest that most variance estimators underestimate\nvariability. Recently, Baraff et al. (2016) proposed the \\textit{tree\nbootstrap} method based on resampling the RDS recruitment tree, and empirically\nshowed that this method outperforms current bootstrap methods. However, some\nfindings suggest that the tree bootstrap (severely) overestimates uncertainty.\nIn this paper, we propose the \\textit{neighbourhood} bootstrap method for\nquantifiying uncertainty in RDS. We prove the consistency of our method under\nsome conditions and investigate its finite sample performance, through a\nsimulation study, under realistic RDS sampling assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 01:24:58 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 17:18:55 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 06:58:04 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Yauck", "Mamadou", ""], ["Moodie", "Erica E. M.", ""], ["Apelian", "Herak", ""], ["Fourmigue", "Alain", ""], ["Grace", "Daniel", ""], ["Hart", "Trevor A.", ""], ["Lambert", "Gilles", ""], ["Cox", "Joseph", ""]]}, {"id": "2010.00181", "submitter": "Martin Slawski", "authors": "Zhenbang Wang, Emanuel Ben-David, Martin Slawski", "title": "Estimation in exponential family Regression based on linked data\n  contaminated by mismatch error", "comments": "51 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of matching records in multiple files can be a challenging and\nerror-prone task. Linkage error can considerably affect subsequent statistical\nanalysis based on the resulting linked file. Several recent papers have studied\npost-linkage linear regression analysis with the response variable in one file\nand the covariates in a second file from the perspective of the \"Broken Sample\nProblem\" and \"Permuted Data\". In this paper, we present an extension of this\nline of research to exponential family response given the assumption of a small\nto moderate number of mismatches. A method based on observation-specific\noffsets to account for potential mismatches and $\\ell_1$-penalization is\nproposed, and its statistical properties are discussed. We also present\nsufficient conditions for the recovery of the correct correspondence between\ncovariates and responses if the regression parameter is known. The proposed\napproach is compared to established baselines, namely the methods by\nLahiri-Larsen and Chambers, both theoretically and empirically based on\nsynthetic and real data. The results indicate that substantial improvements\nover those methods can be achieved even if only limited information about the\nlinkage process is available.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 02:38:38 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 15:37:41 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wang", "Zhenbang", ""], ["Ben-David", "Emanuel", ""], ["Slawski", "Martin", ""]]}, {"id": "2010.00232", "submitter": "Fabio Rapallo", "authors": "Fabio Rapallo", "title": "Analysis of the weighted kappa and its maximum with Markov moves", "comments": "22 pages, including an appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the notion of Markov move from Algebraic Statistics is used to\nanalyze the weighted kappa indices in rater agreement problems. In particular,\nthe problem of the maximum kappa and its dependence on the choice of the\nweighting schemes are discussed. The Markov moves are also used in a simulated\nannealing algorithm to actually find the configuration of maximum agreement.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 07:40:37 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Rapallo", "Fabio", ""]]}, {"id": "2010.00271", "submitter": "Felix Laumann", "authors": "Felix Laumann and Julius von K\\\"ugelgen and Mauricio Barahona", "title": "Kernel Two-Sample and Independence Tests for Non-Stationary Random\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two-sample and independence tests with the kernel-based MMD and HSIC have\nshown remarkable results on i.i.d. data and stationary random processes.\nHowever, these statistics are not directly applicable to non-stationary random\nprocesses, a prevalent form of data in many scientific disciplines. In this\nwork, we extend the application of MMD and HSIC to non-stationary settings by\nassuming access to independent realisations of the underlying random process.\nThese realisations - in the form of non-stationary time-series measured on the\nsame temporal grid - can then be viewed as i.i.d. samples from a multivariate\nprobability distribution, to which MMD and HSIC can be applied. We further show\nhow to choose suitable kernels over these high-dimensional spaces by maximising\nthe estimated test power with respect to the kernel hyper-parameters. In\nexperiments on synthetic data, we demonstrate superior performance of our\nproposed approaches in terms of test power when compared to current\nstate-of-the-art functional or multivariate two-sample and independence tests.\nFinally, we employ our methods on a real socio-economic dataset as an example\napplication.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 09:29:51 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 17:21:56 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 09:32:08 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Laumann", "Felix", ""], ["von K\u00fcgelgen", "Julius", ""], ["Barahona", "Mauricio", ""]]}, {"id": "2010.00408", "submitter": "Pierre Alquier", "authors": "Pierre Alquier, Badr-Eddine Ch\\'erief-Abdellatif, Alexis Derumigny,\n  Jean-David Fermanian", "title": "Estimation of copulas via Maximum Mean Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with robust inference for parametric copula models.\nEstimation using Canonical Maximum Likelihood might be unstable, especially in\nthe presence of outliers. We propose to use a procedure based on the Maximum\nMean Discrepancy (MMD) principle. We derive non-asymptotic oracle inequalities,\nconsistency and asymptotic normality of this new estimator. In particular, the\noracle inequality holds without any assumption on the copula family, and can be\napplied in the presence of outliers or under misspecification. Moreover, in our\nMMD framework, the statistical inference of copula models for which there\nexists no density with respect to the Lebesgue measure on $[0,1]^d$, as the\nMarshall-Olkin copula, becomes feasible. A simulation study shows the\nrobustness of our new procedures, especially compared to pseudo-maximum\nlikelihood estimation. An R package implementing the MMD estimator for copula\nmodels is available.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 13:50:17 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Alquier", "Pierre", ""], ["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""], ["Derumigny", "Alexis", ""], ["Fermanian", "Jean-David", ""]]}, {"id": "2010.00433", "submitter": "Brian Segal", "authors": "Brian D. Segal and W. Katherine Tan", "title": "A note on the amount of information borrowed from external data in\n  hybrid controlled trials with time-to-event outcomes", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In situations where it is difficult to enroll patients in randomized\ncontrolled trials, external data can improve efficiency and feasibility. In\nsuch cases, adaptive trial designs could be used to decrease enrollment in the\ncontrol arm of the trial by updating the randomization ratio at the interim\nanalysis. Updating the randomization ratio requires an estimate of the amount\nof information effectively borrowed from external data, which is typically done\nwith a linear approximation. However, this linear approximation is not always a\nreliable estimate, which could potentially lead to sub-optimal randomization\nratio updates. In this note, we highlight this issue through simulations for\nexponential time-to-event outcomes, because in this simple setting there is an\nexact solution available for comparison. We also propose a potential\ngeneralization that could complement the linear approximation in more complex\nsettings, discuss challenges for this generalization, and recommend best\npractices for computing and interpreting estimates of the effective number of\nevents borrowed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 14:28:19 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Segal", "Brian D.", ""], ["Tan", "W. Katherine", ""]]}, {"id": "2010.00503", "submitter": "Alexander Franks", "authors": "Alexander Franks", "title": "Reducing Subspace Models for Large-Scale Covariance Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an envelope model for joint mean and covariance regression in the\nlarge $p$, small $n$ setting. In contrast to existing envelope methods, which\nimprove mean estimates by incorporating estimates of the covariance structure,\nwe focus on identifying covariance heterogeneity by incorporating information\nabout mean-level differences. We use a Monte Carlo EM algorithm to identify a\nlow-dimensional subspace which explains differences in both means and\ncovariances as a function of covariates, and then use MCMC to estimate the\nposterior uncertainty conditional on the inferred low-dimensional subspace. We\ndemonstrate the utility of our model on a motivating application on the\nmetabolomics of aging. We also provide R code which can be used to develop and\ntest other generalizations of the response envelope model.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:53:38 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Franks", "Alexander", ""]]}, {"id": "2010.00692", "submitter": "Tao Liu", "authors": "Tao Liu, Joseph W. Hogan, Lisa Wang, Shangxuan Zhang, Rami Kantor", "title": "Optimal Allocation of Gold Standard Testing under Constrained\n  Availability: Application to Assessment of HIV Treatment Failure", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2013.810149", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Health Organization (WHO) guidelines for monitoring the\neffectiveness of HIV treatment in resource-limited settings (RLS) are mostly\nbased on clinical and immunological markers (e.g., CD4 cell counts). Recent\nresearch indicates that the guidelines are inadequate and can result in high\nerror rates. Viral load (VL) is considered the \"gold standard\", yet its\nwidespread use is limited by cost and infrastructure. In this paper, we propose\na diagnostic algorithm that uses information from routinely-collected clinical\nand immunological markers to guide a selective use of VL testing for diagnosing\nHIV treatment failure, under the assumption that VL testing is available only\nat a certain portion of patient visits. Our algorithm identifies the patient\nsubpopulation, such that the use of limited VL testing on them minimizes a\npre-defined risk (e.g., misdiagnosis error rate). Diagnostic properties of our\nproposal algorithm are assessed by simulations. For illustration, data from the\nMiriam Hospital Immunology Clinic (RI, USA) are analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 21:31:39 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Liu", "Tao", ""], ["Hogan", "Joseph W.", ""], ["Wang", "Lisa", ""], ["Zhang", "Shangxuan", ""], ["Kantor", "Rami", ""]]}, {"id": "2010.00729", "submitter": "Xiao Han", "authors": "Xiao Han and Xin Tong", "title": "Individual-centered partial information in social networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing statistical network analysis literature assumes a global view\nof the network, under which community detection, testing, and other statistical\nprocedures are developed. Yet in the real world, people frequently make\ndecisions based on their partial understanding of network information. As\nindividuals barely know beyond friends' friends, we assume that an individual\nof interest knows all paths of length up to $L=2$ that originate from them. As\na result, this individual's perceived adjacency matrix $\\bbB$ differs\nsignificantly from the usual adjacency matrix $\\bbA$ based on the global\ninformation. The new individual-centered partial information framework sparks\nan array of fascinating endeavors from theory to practice. Key general\nproperties on the eigenvalues and eigenvectors of $\\bbB_E$, a major term of\n$\\bbB$, are derived. These general results, coupled with the classic stochastic\nblock model, lead to a new theory-backed spectral approach to detecting the\ncommunity memberships based on an anchored individual's partial information.\nReal data analysis delivers interesting insights that result from individuals'\nheterogeneous knowledge, yet these insights cannot be obtained from global\nnetwork analysis.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 23:48:54 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 00:55:35 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Han", "Xiao", ""], ["Tong", "Xin", ""]]}, {"id": "2010.00781", "submitter": "Chi-Kuang Yeh", "authors": "Chi-Kuang Yeh, Gregory Rice, Joel A. Dubin", "title": "Evaluating real-time probabilistic forecasts with application to\n  National Basketball Association outcome prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the goal of evaluating real-time forecasts of home team win\nprobabilities in the National Basketball Association, we develop new tools for\nmeasuring the quality of continuously updated probabilistic forecasts. This\nincludes introducing calibration surface plots, and simple graphical summaries\nof them, to evaluate at a glance whether a given continuously updated\nprobability forecasting method is well-calibrated, as well as developing\nstatistical tests and graphical tools to evaluate the skill, or relative\nperformance, of two competing continuously updated forecasting methods. These\ntools are studied by means of a Monte Carlo simulation study of simulated\nbasketball games, and demonstrated in an application to evaluate the\ncontinuously updated forecasts published by the United States-based\nmultinational sports network ESPN on its principle webpage {\\tt espn.com}. This\napplication lends statistical evidence that the forecasts published there are\nwell-calibrated, and exhibit improved skill over several na\\\"ive models, but do\nnot demonstrate significantly improved skill over simple logistic regression\nmodels based solely on a measurement of each teams' relative strength, and the\nevolving score difference throughout the game.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 05:16:23 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Yeh", "Chi-Kuang", ""], ["Rice", "Gregory", ""], ["Dubin", "Joel A.", ""]]}, {"id": "2010.00794", "submitter": "Sayani Gupta", "authors": "Sayani Gupta, Rob J Hyndman, Dianne Cook and Antony Unwin", "title": "Visualizing probability distributions across bivariate cyclic temporal\n  granularities", "comments": "32 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconstructing a time index into time granularities can assist in exploration\nand automated analysis of large temporal data sets. This paper describes\nclasses of time deconstructions using linear and cyclic time granularities.\nLinear granularities respect the linear progression of time such as hours,\ndays, weeks and months. Cyclic granularities can be circular such as\nhour-of-the-day, quasi-circular such as day-of-the-month, and aperiodic such as\npublic holidays. The hierarchical structure of granularities creates a nested\nordering: hour-of-the-day and second-of-the-minute are single-order-up.\nHour-of-the-week is multiple-order-up, because it passes over day-of-the-week.\nMethods are provided for creating all possible granularities for a time index.\nA recommendation algorithm provides an indication whether a pair of\ngranularities can be meaningfully examined together (a \"harmony\"), or when they\ncannot (a \"clash\").\n  Time granularities can be used to create data visualizations to explore for\nperiodicities, associations and anomalies. The granularities form categorical\nvariables (ordered or unordered) which induce groupings of the observations.\nAssuming a numeric response variable, the resulting graphics are then displays\nof distributions compared across combinations of categorical variables.\n  The methods implemented in the open source R package `gravitas` are\nconsistent with a tidy workflow, with probability distributions examined using\nthe range of graphics available in `ggplot2`.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 05:47:43 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Gupta", "Sayani", ""], ["Hyndman", "Rob J", ""], ["Cook", "Dianne", ""], ["Unwin", "Antony", ""]]}, {"id": "2010.00948", "submitter": "Narayan Puthanmadam Subramaniyam", "authors": "Narayan Puthanmadam Subramaniyam, Reik V. Donner, Davide Caron,\n  Gabriella Panuccio, Jari Hyttinen", "title": "Causal coupling inference from multivariate time series based on ordinal\n  partition transition networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying causal relationships is a challenging yet crucial problem in many\nfields of science like epidemiology, climatology, ecology, genomics, economics\nand neuroscience, to mention only a few. Recent studies have demonstrated that\nordinal partition transition networks (OPTNs) allow inferring the coupling\ndirection between two dynamical systems. In this work, we generalize this\nconcept to the study of the interactions among multiple dynamical systems and\nwe propose a new method to detect causality in multivariate observational data.\nBy applying this method to numerical simulations of coupled linear stochastic\nprocesses as well as two examples of interacting nonlinear dynamical systems\n(coupled Lorenz systems and a network of neural mass models), we demonstrate\nthat our approach can reliably identify the direction of interactions and the\nassociated coupling delays. Finally, we study real-world observational\nmicroelectrode array electrophysiology data from rodent brain slices to\nidentify the causal coupling structures underlying epileptiform activity. Our\nresults, both from simulations and real-world data, suggest that OPTNs can\nprovide a complementary and robust approach to infer causal effect networks\nfrom multivariate observational data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 12:25:02 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 06:19:15 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 06:28:21 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 21:36:12 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Subramaniyam", "Narayan Puthanmadam", ""], ["Donner", "Reik V.", ""], ["Caron", "Davide", ""], ["Panuccio", "Gabriella", ""], ["Hyttinen", "Jari", ""]]}, {"id": "2010.00950", "submitter": "Jakob Raymaekers", "authors": "Jakob Raymaekers and Ruben H. Zamar", "title": "Regularized K-means through hard-thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a framework of regularized $K$-means methods based on direct\npenalization of the size of the cluster centers. Different penalization\nstrategies are considered and compared through simulation and theoretical\nanalysis. Based on the results, we propose HT $K$-means, which uses an $\\ell_0$\npenalty to induce sparsity in the variables. Different techniques for selecting\nthe tuning parameter are discussed and compared. The proposed method stacks up\nfavorably with the most popular regularized $K$-means methods in an extensive\nsimulation study. Finally, HT $K$-means is applied to several real data\nexamples. Graphical displays are presented and used in these examples to gain\nmore insight into the datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 12:29:32 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Raymaekers", "Jakob", ""], ["Zamar", "Ruben H.", ""]]}, {"id": "2010.00957", "submitter": "Kaspar Rufibach", "authors": "Steven Sun and Hans-Jochen Weber and Emily Butler and Kaspar Rufibach\n  and Satrajit Roychoudhury", "title": "Estimands in Hematologic Oncology Trials", "comments": "5 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimand framework included in the addendum to the ICH E9 guideline\nfacilitates discussions to ensure alignment between the key question of\ninterest, the analysis, and interpretation. Therapeutic knowledge and drug\nmechanism play a crucial role in determining the strategy and defining the\nestimand for clinical trial designs. Clinical trials in patients with\nhematological malignancies often present unique challenges for trial design due\nto complexity of treatment options and existence of potential curative but\nhighly risky procedures, e.g. stem cell transplant or treatment sequence across\ndifferent phases (induction, consolidation, maintenance). Here, we illustrate\nhow to apply the estimand framework in hematological clinical trials and how\nthe estimand framework can address potential difficulties in trial result\ninterpretation.\n  This paper is a result of a cross-industry collaboration to connect the\nInternational Conference on Harmonisation (ICH) E9 addendum concepts to\napplications. Three randomized phase 3 trials will be used to consider common\nchallenges including intercurrent events in hematologic oncology trials to\nillustrate different scientific questions and the consequences of the estimand\nchoice for trial design, data collection, analysis, and interpretation.\nTemplate language for describing estimand in both study protocols and\nstatistical analysis plans is suggested for statisticians' reference.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:38:48 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Sun", "Steven", ""], ["Weber", "Hans-Jochen", ""], ["Butler", "Emily", ""], ["Rufibach", "Kaspar", ""], ["Roychoudhury", "Satrajit", ""]]}, {"id": "2010.01084", "submitter": "Wei Deng", "authors": "Wei Deng and Qi Feng and Georgios Karagiannis and Guang Lin and Faming\n  Liang", "title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC\n  via Variance Reduction", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown\npromise in accelerating the convergence in non-convex learning; however, an\nexcessively large correction for avoiding biases from noisy energy estimators\nhas limited the potential of the acceleration. To address this issue, we study\nthe variance reduction for noisy energy estimators, which promotes much more\neffective swaps. Theoretically, we provide a non-asymptotic analysis on the\nexponential acceleration for the underlying continuous-time Markov jump\nprocess; moreover, we consider a generalized Girsanov theorem which includes\nthe change of Poisson measure to overcome the crude discretization based on the\nGr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein\n($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and\nobtain the state-of-the-art results in optimization and uncertainty estimates\nfor synthetic experiments and image data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 16:23:35 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 16:24:47 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Deng", "Wei", ""], ["Feng", "Qi", ""], ["Karagiannis", "Georgios", ""], ["Lin", "Guang", ""], ["Liang", "Faming", ""]]}, {"id": "2010.01184", "submitter": "Felipe Maia Polo", "authors": "Felipe Maia Polo, Renato Vicente", "title": "Covariate Shift Adaptation in High-Dimensional and Divergent\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real world applications of supervised learning methods, training and test\nsets are often sampled from the distinct distributions and we must resort to\ndomain adaptation techniques. One special class of techniques is Covariate\nShift Adaptation, which allows practitioners to obtain good generalization\nperformance in the distribution of interest when domains differ only by the\nmarginal distribution of features. Traditionally, Covariate Shift Adaptation is\nimplemented using Importance Weighting which may fail in high-dimensional\nsettings due to small Effective Sample Sizes (ESS). In this paper, we propose\n(i) a connection between ESS, high-dimensional settings and generalization\nbounds and (ii) a simple, general and theoretically sound approach to combine\nfeature selection and Covariate Shift Adaptation. The new approach yields good\nperformance with improved ESS.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 20:22:59 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 19:27:37 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Polo", "Felipe Maia", ""], ["Vicente", "Renato", ""]]}, {"id": "2010.01289", "submitter": "Yue Li", "authors": "Yue Li and Ilmun Kim and Yuting Wei", "title": "Randomized tests for high-dimensional regression: A more efficient and\n  powerful solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of testing the global null in the high-dimensional\nregression models when the feature dimension $p$ grows proportionally to the\nnumber of observations $n$. Despite a number of prior work studying this\nproblem, whether there exists a test that is model-agnostic, efficient to\ncompute and enjoys high power, still remains unsettled. In this paper, we\nanswer this question in the affirmative by leveraging the random projection\ntechniques, and propose a testing procedure that blends the classical $F$-test\nwith a random projection step. When combined with a systematic choice of the\nprojection dimension, the proposed procedure is proved to be minimax optimal\nand, meanwhile, reduces the computation and data storage requirements. We\nillustrate our results in various scenarios when the underlying feature matrix\nexhibits an intrinsic lower dimensional structure (such as approximate block\nstructure or has exponential/polynomial eigen-decay), and it turns out that the\nproposed test achieves sharp adaptive rates. Our theoretical findings are\nfurther validated by comparisons to other state-of-the-art tests on the\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 06:41:57 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Li", "Yue", ""], ["Kim", "Ilmun", ""], ["Wei", "Yuting", ""]]}, {"id": "2010.01382", "submitter": "Gerhard Tutz", "authors": "Gerhard Tutz", "title": "A Taxonomy of Polytomous Item Response Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common framework is provided that comprises classical ordinal item response\nmodels as the cumulative, sequential and adjacent categories models as well as\nnominal response models and item response tree models. The taxonomy is based on\nthe ways binary models can be seen as building blocks of the various models. In\nparticular one can distinguish between conditional and unconditional model\ncomponents. Conditional models are by far the larger class of models containing\nthe adjacent categories model and the whole class of hierarchically structured\nmodels. The latter is introduced as a class of models that comprises binary\ntrees and hierarchically structured models that use ordinal models\nconditionally. The study of the binary models contained in latent trait models\nclarifies the relation between models and the interpretation of item\nparameters. It is also used to distinguish between ordinal and nominal models\nby giving a conceptualization of ordinal models. The taxonomy differs from\nprevious taxonomies by focusing on the structured use of dichotomizations\ninstead of the role of parameterizations.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 16:18:34 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Tutz", "Gerhard", ""]]}, {"id": "2010.01396", "submitter": "Joshua Chang", "authors": "Joshua C. Chang and Julia Porcino and Elizabeth K. Rasch and Larry\n  Tang", "title": "Regularized Bayesian calibration and scoring of the WD-FAB IRT model\n  improves predictive performance over maximum marginal likelihood", "comments": "Submitted QOL", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item response theory (IRT) is the statistical paradigm underlying a dominant\nfamily of generative probabilistic models for test responses, used to quantify\ntraits in individuals relative to target populations. The graded response model\n(GRM) is a particular IRT model that is used for ordered polytomous test\nresponses. Both the development and the application of the GRM and other IRT\nmodels require statistical decisions. For formulating these models\n(calibration), one needs to decide on methodologies for item selection,\ninference, and regularization. For applying these models (test scoring), one\nneeds to make similar decisions, often prioritizing computational tractability\nand/or interpretability. In many applications, such as in the Work Disability\nFunctional Assessment Battery (WD-FAB), tractability implies approximating an\nindividual's score distribution using estimates of mean and variance, and\nobtaining that score conditional on only point estimates of the calibrated\nmodel. In this manuscript, we evaluate the calibration and scoring of models\nunder this common use-case using Bayesian cross-validation. Applied to the\nWD-FAB responses collected for the National Institutes of Health, we assess the\npredictive power of implementations of the GRM based on their ability to yield,\non validation sets of respondents, estimates of latent ability with uncertainty\nthat are most predictive of patterns of item responses. IRT models in-general\nhave the concrete interpretation of latent abilities, combining with item\nparameters, to produce predictions of response patterns. Our main finding is\nthat regularized Bayesian calibration of the GRM outperforms the prior-free\nempirical Bayesian procedure of maximum marginal likelihood. We also motivate\nthe use of compactly supported priors in test scoring.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 17:25:12 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chang", "Joshua C.", ""], ["Porcino", "Julia", ""], ["Rasch", "Elizabeth K.", ""], ["Tang", "Larry", ""]]}, {"id": "2010.01492", "submitter": "Bin Peng", "authors": "Yayi Yan and Jiti Gao and Bin Peng", "title": "A Class of Time-Varying Vector Moving Average Models: Nonparametric\n  Kernel Estimation and Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate dynamic time series models are widely encountered in practical\nstudies, e.g., modelling policy transmission mechanism and measuring\nconnectedness between economic agents. To better capture the dynamics, this\npaper proposes a wide class of multivariate dynamic models with time-varying\ncoefficients, which have a general time-varying vector moving average (VMA)\nrepresentation, and nest, for instance, time-varying vector autoregression\n(VAR), time-varying vector autoregression moving-average (VARMA), and so forth\nas special cases. The paper then develops a unified estimation method for the\nunknown quantities before an asymptotic theory for the proposed estimators is\nestablished. In the empirical study, we investigate the transmission mechanism\nof monetary policy using U.S. data, and uncover a fall in the volatilities of\nexogenous shocks. In addition, we find that (i) monetary policy shocks have\nless influence on inflation before and during the so-called Great Moderation,\n(ii) inflation is more anchored recently, and (iii) the long-run level of\ninflation is below, but quite close to the Federal Reserve's target of two\npercent after the beginning of the Great Moderation period.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 06:52:54 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Yan", "Yayi", ""], ["Gao", "Jiti", ""], ["Peng", "Bin", ""]]}, {"id": "2010.01706", "submitter": "Victoire Michal", "authors": "Sixia Chen, David Haziza and Victoire Michal", "title": "Efficient multiply robust imputation in the presence of influential\n  units in surveys", "comments": "39 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item nonresponse is a common issue in surveys. Because unadjusted estimators\nmay be biased in the presence of nonresponse, it is common practice to impute\nthe missing values with the objective of reducing the nonresponse bias as much\nas possible. However, commonly used imputation procedures may lead to unstable\nestimators of population totals/means when influential units are present in the\nset of respondents. In this article, we consider the class of multiply robust\nimputation procedures that provide some protection against the failure of\nunderlying model assumptions. We develop an efficient version of multiply\nrobust estimators based on the concept of conditional bias, a measure of\ninfluence. We present the results of a simulation study to show the benefits of\nthe proposed method in terms of bias and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 22:20:01 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chen", "Sixia", ""], ["Haziza", "David", ""], ["Michal", "Victoire", ""]]}, {"id": "2010.01772", "submitter": "Wei Liang", "authors": "Wei Liang and Ying Yan", "title": "Empirical Likelihood-Based Estimation and Inference in Randomized\n  Controlled Trials with High-Dimensional Covariates", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a data-adaptive empirical likelihood-based approach\nfor treatment effect estimation and inference, which overcomes the obstacle of\nthe traditional empirical likelihood-based approaches in the high-dimensional\nsetting by adopting penalized regression and machine learning methods to model\nthe covariate-outcome relationship. In particular, we show that our procedure\nsuccessfully recovers the true variance of Zhang's treatment effect estimator\n(Zhang, 2018) by utilizing a data-splitting technique. Our proposed estimator\nis proved to be asymptotically normal and semiparametric efficient under mild\nregularity conditions. Simulation studies indicate that our estimator is more\nefficient than the estimator proposed by Wager et al. (2016) when random\nforests are employed to model the covariate-outcome relationship. Moreover,\nwhen multiple machine learning models are imposed, our estimator is at least as\nefficient as any regular estimator with a single machine learning model. We\ncompare our method to existing ones using the ACTG175 data and the GSE118657\ndata, and confirm the outstanding performance of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 04:40:55 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 02:54:09 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Liang", "Wei", ""], ["Yan", "Ying", ""]]}, {"id": "2010.01844", "submitter": "Nadja Klein Prof. Dr.", "authors": "Nadja Klein, Michael Stanley Smith, David J. Nott", "title": "Deep Distributional Time Series Models and the Probabilistic Forecasting\n  of Intraday Electricity Prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) with rich feature vectors of past values can\nprovide accurate point forecasts for series that exhibit complex serial\ndependence. We propose two approaches to constructing deep time series\nprobabilistic models based on a variant of RNN called an echo state network\n(ESN). The first is where the output layer of the ESN has stochastic\ndisturbances and a shrinkage prior for additional regularization. The second\napproach employs the implicit copula of an ESN with Gaussian disturbances,\nwhich is a deep copula process on the feature space. Combining this copula with\na non-parametrically estimated marginal distribution produces a deep\ndistributional time series model. The resulting probabilistic forecasts are\ndeep functions of the feature vector and also marginally calibrated. In both\napproaches, Bayesian Markov chain Monte Carlo methods are used to estimate the\nmodels and compute forecasts. The proposed models are suitable for the complex\ntask of forecasting intraday electricity prices. Using data from the Australian\nNational Electricity Market, we show that our deep time series models provide\naccurate short term probabilistic price forecasts, with the copula model\ndominating. Moreover, the models provide a flexible framework for incorporating\nprobabilistic forecasts of electricity demand as additional features, which\nincreases upper tail forecast accuracy from the copula model significantly.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 08:02:29 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 11:17:24 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Klein", "Nadja", ""], ["Smith", "Michael Stanley", ""], ["Nott", "David J.", ""]]}, {"id": "2010.01927", "submitter": "Konstantinos Fokianos", "authors": "Sergios Agapiou, Andreas Anastasiou, Anastassia Baxevani, Tasos\n  Christofides, Elisavet Constantinou, Georgios Hadjigeorgiou, Christos\n  Nicolaides, Georgios Nikolopoulos, and Konstantinos Fokianos", "title": "Modeling of Covid-19 Pandemic in Cyprus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Republic of Cyprus is a small island in the southeast of Europe and\nmember of the European Union. The first wave of COVID-19 in Cyprus started in\nearly March, 2020 (imported cases) and peaked in late March-early April. The\nhealth authorities responded rapidly and rigorously to the COVID-19 pandemic by\nscaling-up testing, increasing efforts to trace and isolate contacts of cases,\nand implementing measures such as closures of educational institutions, and\ntravel and movement restrictions. The pandemic was also a unique opportunity\nthat brought together experts from various disciplines including\nepidemiologists, clinicians, mathematicians, and statisticians. The aim of this\npaper is to present the efforts of this new, multidisciplinary research team in\nmodelling the COVID-19 pandemic in the Republic of Cyprus.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 11:33:39 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Agapiou", "Sergios", ""], ["Anastasiou", "Andreas", ""], ["Baxevani", "Anastassia", ""], ["Christofides", "Tasos", ""], ["Constantinou", "Elisavet", ""], ["Hadjigeorgiou", "Georgios", ""], ["Nicolaides", "Christos", ""], ["Nikolopoulos", "Georgios", ""], ["Fokianos", "Konstantinos", ""]]}, {"id": "2010.02071", "submitter": "Zheng Chen", "authors": "Jingjing Lyu, Yawen Hou and Zheng Chen", "title": "The use of restricted mean time lost under competing risks data", "comments": null, "journal-ref": "BMC Medical Research Methodology 2020. 20:197", "doi": "10.1186/s12874-020-01040-9", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Under competing risks, the commonly used sub-distribution hazard\nratio (SHR) is not easy to interpret clinically and is valid only under the\nproportional sub-distribution hazard (SDH) assumption. This paper introduces an\nalternative statistical measure: the restricted mean time lost (RMTL). Methods:\nFirst, the definition and estimation methods of the measures are introduced.\nSecond, based on the differences in RMTLs, a basic difference test (Diff) and a\nsupremum difference test (sDiff) are constructed. Then, the corresponding\nsample size estimation method is proposed. The statistical properties of the\nmethods and the estimated sample size are evaluated using Monte Carlo\nsimulations, and these methods are also applied to two real examples. Results:\nThe simulation results show that sDiff performs well and has relatively high\ntest efficiency in most situations. Regarding sample size calculation, sDiff\nexhibits good performance in various situations. The methods are illustrated\nusing two examples. Conclusions: RMTL can meaningfully summarize treatment\neffects for clinical decision making, which can then be reported with the SDH\nratio for competing risks data. The proposed sDiff test and the two calculated\nsample size formulas have wide applicability and can be considered in real data\nanalysis and trial design.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 15:08:45 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Lyu", "Jingjing", ""], ["Hou", "Yawen", ""], ["Chen", "Zheng", ""]]}, {"id": "2010.02108", "submitter": "Nick Doudchenko", "authors": "Nick Doudchenko, Minzhengxiong Zhang, Evgeni Drynkin, Edoardo Airoldi,\n  Vahab Mirrokni, Jean Pouget-Abadie", "title": "Causal Inference with Bipartite Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite experiments are a recent object of study in causal inference,\nwhereby treatment is applied to one set of units and outcomes of interest are\nmeasured on a different set of units. These experiments are particularly useful\nin settings where strong interference effects occur between units of a\nbipartite graph. In market experiments for example, assigning treatment at the\nseller-level and measuring outcomes at the buyer-level (or vice-versa) may lead\nto causal models that better account for the interference that naturally occurs\nbetween buyers and sellers. While bipartite experiments have been shown to\nimprove the estimation of causal effects in certain settings, the analysis must\nbe done carefully so as to not introduce unnecessary bias. We leverage the\ngeneralized propensity score literature to show that we can obtain unbiased\nestimates of causal effects for bipartite experiments under a standard set of\nassumptions. We also discuss the construction of confidence sets with proper\ncoverage probabilities. We evaluate these methods using a bipartite graph from\na publicly available dataset studied in previous work on bipartite experiments,\nshowing through simulations a significant bias reduction and improved coverage.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 15:48:31 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 20:56:05 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 03:24:48 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Doudchenko", "Nick", ""], ["Zhang", "Minzhengxiong", ""], ["Drynkin", "Evgeni", ""], ["Airoldi", "Edoardo", ""], ["Mirrokni", "Vahab", ""], ["Pouget-Abadie", "Jean", ""]]}, {"id": "2010.02121", "submitter": "Siyun Yang", "authors": "Siyun Yang, Elizabeth Lorenzi, Georgia Papadogeorgou, Daniel M.\n  Wojdyla, Fan Li, and Laine E. Thomas", "title": "Propensity Score Weighting for Causal Subgroup Analysis", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common goal in comparative effectiveness research is to estimate treatment\neffects on pre-specified subpopulations of patients. Though widely used in\nmedical research, causal inference methods for such subgroup analysis remain\nunderdeveloped, particularly in observational studies. In this article, we\ndevelop a suite of analytical methods and visualization tools for causal\nsubgroup analysis. First, we introduce the estimand of subgroup weighted\naverage treatment effect and provide the corresponding propensity score\nweighting estimator. We show that balancing covariates within a subgroup bounds\nthe bias of the estimator of subgroup causal effects. Second, we design a new\ndiagnostic graph -- the Connect-S plot -- for visualizing the subgroup\ncovariate balance. Finally, we propose to use the overlap weighting method to\nachieve exact balance within subgroups. We further propose a method that\ncombines overlap weighting and LASSO, to balance the bias-variance tradeoff in\nsubgroup analysis. Extensive simulation studies are presented to compare the\nproposed method with several existing methods. We apply the proposed methods to\nthe Patient-centered Results for Uterine Fibroids (COMPARE-UF) registry data to\nevaluate alternative management options for uterine fibroids for relief of\nsymptoms and quality of life.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 16:09:17 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 02:28:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yang", "Siyun", ""], ["Lorenzi", "Elizabeth", ""], ["Papadogeorgou", "Georgia", ""], ["Wojdyla", "Daniel M.", ""], ["Li", "Fan", ""], ["Thomas", "Laine E.", ""]]}, {"id": "2010.02247", "submitter": "Peter Jan van Leeuwen", "authors": "Peter Jan van Leeuwen and Michael DeCaria and Nachiketa Chakaborty and\n  Manuel Pulido", "title": "A Framework for Causal Discovery in non-intervenable systems", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IR cs.IT math.IT nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many frameworks exist to infer cause and effect relations in complex\nnonlinear systems but a complete theory is lacking. A new framework is\npresented that is fully nonlinear, provides a complete information theoretic\ndisentanglement of causal processes, allows for nonlinear interactions between\ncauses, identifies the causal strength of missing or unknown processes, and can\nanalyze systems that cannot be represented on standard graphs. The basic\nbuilding blocks are information theoretic measures such as (conditional) mutual\ninformation and a new concept called certainty that monotonically increases\nwith the information available about the target process. The framework is\npresented in detail and compared with other existing frameworks, and the\ntreatment of confounders is discussed. It is tested on several highly\nsimplified stochastic processes to demonstrate how blocking and gateways are\nhandled, and on the chaotic Lorentz 1963 system. It is shown that the framework\nprovides information on the local dynamics, but also reveals information on the\nlarger scale structure of the underlying attractor. While there are systems\nwith structures that the framework cannot disentangle, it is argued that any\ncausal framework that is based on integrated quantities will miss out\npotentially important information of the underlying probability density\nfunctions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:02:13 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 21:24:30 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["van Leeuwen", "Peter Jan", ""], ["DeCaria", "Michael", ""], ["Chakaborty", "Nachiketa", ""], ["Pulido", "Manuel", ""]]}, {"id": "2010.02288", "submitter": "Xin Bing", "authors": "Xin Bing and Florentina Bunea and Marten Wegkamp", "title": "Detecting approximate replicate components of a high-dimensional random\n  vector with latent structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional feature vectors are likely to contain sets of measurements\nthat are approximate replicates of one another. In complex applications, or\nautomated data collection, these feature sets are not known a priori, and need\nto be determined. This work proposes a class of latent factor models on the\nobserved high-dimensional random vector $X \\in \\mathbb{R}^p$, for defining,\nidentifying and estimating the index set of its approximately replicate\ncomponents. The model class is parametrized by a $p \\times K$ loading matrix\n$A$ that contains a hidden sub-matrix whose rows can be partitioned into groups\nof parallel vectors. Under this model class, a set of approximate replicate\ncomponents of $X$ corresponds to a set of parallel rows in $A$: these entries\nof $X$ are, up to scale and additive error, the same linear combination of the\n$K$ latent factors; the value of $K$ is itself unknown. The problem of finding\napproximate replicates in $X$ reduces to identifying, and estimating, the\nlocation of the hidden sub-matrix within $A$, and of the partition of its row\nindex set $H$. Both $H$ and its partiton can be fully characterized in terms of\na new family of criteria based on the correlation matrix of $X$, and their\nidentifiability, as well as that of the unknown latent dimension $K$, are\nobtained as consequences. The constructive nature of the identifiability\narguments enables computationally efficient procedures, with consistency\nguarantees. When $A$ has the errors-in-variable parametrization, the difficulty\nof the problem is elevated. The task becomes that of separating out groups of\nparallel rows that are proportional to canonical basis vectors from other dense\nparallel rows in $A$. This is met under a scale assumption, via a principled\nway of selecting the target row indices, guided by the succesive maximization\nof Schur complements of appropriate covariance matrices.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 19:03:24 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Bing", "Xin", ""], ["Bunea", "Florentina", ""], ["Wegkamp", "Marten", ""]]}, {"id": "2010.02326", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen and Xiaoou Li", "title": "Determining the Number of Factors in High-dimensional Generalised Latent\n  Factor Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a generalisation of the classical linear factor model, generalised latent\nfactor models are a useful tool for analysing multivariate data of different\ntypes, including binary choices and counts. In this paper, we propose an\ninformation criterion to determine the number of factors in generalised latent\nfactor models. The consistency of the proposed information criterion is\nestablished under a high-dimensional setting where both the sample size and the\nnumber of manifest variables grow to infinity and data may have many missing\nvalues. To establish this consistency result, an error bound is established for\nthe parameter estimates that improves the existing results and may be of\nindependent theoretical interest. Simulation shows that the proposed criterion\nhas good finite sample performance. An application to Eysenck's personality\nquestionnaire confirms the three-factor structure of this personality survey.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:36:01 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Chen", "Yunxiao", ""], ["Li", "Xiaoou", ""]]}, {"id": "2010.02332", "submitter": "Steven Winter", "authors": "Steven Winter, Zhengwu Zhang, and David Dunson", "title": "Multi-scale graph principal component analysis for connectomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In brain connectomics, the cortical surface is parcellated into different\nregions of interest (ROIs) prior to statistical analysis. The brain connectome\nfor each individual can then be represented as a graph, with the nodes\ncorresponding to ROIs and edges to connections between ROIs. Such a graph can\nbe summarized as an adjacency matrix, with each cell containing the strength of\nconnection between a pair of ROIs. These matrices are symmetric with the\ndiagonal elements corresponding to self-connections typically excluded. A major\ndisadvantage of such representations of the connectome is their sensitivity to\nthe chosen ROIs, including critically the number of ROIs and hence the scale of\nthe graph. As the scale becomes finer and more ROIs are used, graphs become\nincreasingly sparse. Clearly, the results of downstream statistical analyses\ncan be highly dependent on the chosen parcellation. To solve this problem, we\npropose a multi-scale graph factorization, which links together scale-specific\nfactorizations through a common set of individual-specific scores. These scores\nsummarize an individual's brain structure combining information across\nmeasurement scales. We obtain a simple and efficient algorithm for\nimplementation, and illustrate substantial advantages over single scale\napproaches in simulations and analyses of the Human Connectome Project dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:58:43 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Winter", "Steven", ""], ["Zhang", "Zhengwu", ""], ["Dunson", "David", ""]]}, {"id": "2010.02385", "submitter": "Phillip Sundin", "authors": "Phillip T. Sundin, Catherine M. Crespi", "title": "Power Analysis for Stepped Wedge Trials with Two Treatments", "comments": "9 figures, 0 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stepped wedge designs (SWDs) are designs for cluster randomized trials that\nfeature staggered, unidirectional cross-over, typically from a control to a\ntreatment condition. Existing literature on statistical power for SWDs\nprimarily focuses on designs with a single treatment. However, SWDs with\nmultiple treatments are being proposed and conducted. We present a linear mixed\nmodel for a SWD with two treatments, with and without an interaction between\nthem. We derive closed form solutions for the standard errors of the treatment\neffect coefficients for such models along with power calculation methods. We\nconsider repeated cross-sectional designs as well as open and closed cohort\ndesigns and different random effect structures. Design features are examined to\ndetermine their impact on power for main treatment and interaction effects.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 23:07:01 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Sundin", "Phillip T.", ""], ["Crespi", "Catherine M.", ""]]}, {"id": "2010.02424", "submitter": "Nick Terry", "authors": "Nick Terry and Youngjun Choe", "title": "Splitting Gaussian Process Regression for Streaming Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes offer a flexible kernel method for regression. While\nGaussian processes have many useful theoretical properties and have proven\npractically useful, they suffer from poor scaling in the number of\nobservations. In particular, the cubic time complexity of updating standard\nGaussian process models make them generally unsuitable for application to\nstreaming data. We propose an algorithm for sequentially partitioning the input\nspace and fitting a localized Gaussian process to each disjoint region. The\nalgorithm is shown to have superior time and space complexity to existing\nmethods, and its sequential nature permits application to streaming data. The\nalgorithm constructs a model for which the time complexity of updating is\ntightly bounded above by a pre-specified parameter. To the best of our\nknowledge, the model is the first local Gaussian process regression model to\nachieve linear memory complexity. Theoretical continuity properties of the\nmodel are proven. We demonstrate the efficacy of the resulting model on\nmulti-dimensional regression tasks for streaming data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 01:37:13 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Terry", "Nick", ""], ["Choe", "Youngjun", ""]]}, {"id": "2010.02425", "submitter": "Robert Vandermeulen", "authors": "Robert A. Vandermeulen", "title": "Improving Nonparametric Density Estimation with Tensor Decompositions", "comments": "20 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While nonparametric density estimators often perform well on low dimensional\ndata, their performance can suffer when applied to higher dimensional data,\nowing presumably to the curse of dimensionality. One technique for avoiding\nthis is to assume no dependence between features and that the data are sampled\nfrom a separable density. This allows one to estimate each marginal\ndistribution independently thereby avoiding the slow rates associated with\nestimating the full joint density. This is a strategy employed in naive Bayes\nmodels and is analogous to estimating a rank-one tensor. In this paper we\ninvestigate whether these improvements can be extended to other simplified\ndependence assumptions which we model via nonnegative tensor decompositions. In\nour central theoretical results we prove that restricting estimation to\nlow-rank nonnegative PARAFAC or Tucker decompositions removes the\ndimensionality exponent on bin width rates for multidimensional histograms.\nThese results are validated experimentally with high statistical significance\nvia direct application of existing nonnegative tensor factorization to\nhistogram estimators.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 01:39:09 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Vandermeulen", "Robert A.", ""]]}, {"id": "2010.02482", "submitter": "Anru R. Zhang", "authors": "Yuchen Zhou and Anru R. Zhang and Lili Zheng and Yazhen Wang", "title": "Optimal High-order Tensor SVD via Tensor-Train Orthogonal Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.NA math.NA stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper studies a general framework for high-order tensor SVD. We propose\na new computationally efficient algorithm, tensor-train orthogonal iteration\n(TTOI), that aims to estimate the low tensor-train rank structure from the\nnoisy high-order tensor observation. The proposed TTOI consists of\ninitialization via TT-SVD (Oseledets, 2011) and new iterative backward/forward\nupdates. We develop the general upper bound on estimation error for TTOI with\nthe support of several new representation lemmas on tensor matricizations. By\ndeveloping a matching information-theoretic lower bound, we also prove that\nTTOI achieves the minimax optimality under the spiked tensor model. The merits\nof the proposed TTOI are illustrated through applications to estimation and\ndimension reduction of high-order Markov processes, numerical studies, and a\nreal data example on New York City taxi travel records. The software of the\nproposed algorithm is available online.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 05:18:24 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Zhou", "Yuchen", ""], ["Zhang", "Anru R.", ""], ["Zheng", "Lili", ""], ["Wang", "Yazhen", ""]]}, {"id": "2010.02521", "submitter": "Molei Liu", "authors": "Molei Liu, Yi Zhang, Tianxi Cai", "title": "Doubly Robust Covariate Shift Regression with Semi-nonparametric\n  Nuisance Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contemporary statistical learning, covariate shift correction plays an\nimportant role when distribution of the testing data is shifted from the\ntraining data. Importance weighting is used to adjust for this but is not\nrobust to model misspecifcation or excessive estimation error. In this paper,\nwe propose a doubly robust covariate shift regression approach that introduces\nan imputation model for the targeted response, and uses it to augment the\nimportance weighting equation. With a novel semi-nonparametric construction for\nthe two nuisance models, our method is less prone to the curse of\ndimensionality compared to the nonparametric approaches, and is less prone to\nmodel mis-specification than the parametric approach. To remove the overfitting\nbias of the nonparametric components under potential model mis-specification,\nwe construct calibrated moment estimating equations for the semi-nonparametric\nmodels. We show that our estimator is root-n consistent when at least one\nnuisance model is correctly specified, estimation for the parametric part of\nthe nuisance models achieves parametric rate, and the nonparametric components\nare rate doubly robust. Simulation studies demonstrate that our method is more\nrobust and efficient than existing parametric and fully nonparametric (machine\nlearning) estimators under various configurations. We also examine the utility\nof our method through a real example about transfer learning of phenotyping\nalgorithm for bipolar disorder. Finally, we propose ways to improve the\n(intrinsic) efficiency of our estimator and to incorporate high dimensional or\nmachine learning models with our proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 06:50:27 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 07:38:46 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Liu", "Molei", ""], ["Zhang", "Yi", ""], ["Cai", "Tianxi", ""]]}, {"id": "2010.02574", "submitter": "Sonja Kuhnt", "authors": "Dominik Kirchhoff, Sonja Kuhnt", "title": "Gaussian Process Models with Low-Rank Correlation Matrices for Both\n  Continuous and Categorical Inputs", "comments": "19 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method that uses low-rank approximations of cross-correlation\nmatrices in mixed continuous and categorical Gaussian Process models. This new\nmethod -- called Low-Rank Correlation (LRC) -- offers the ability to flexibly\nadapt the number of parameters to the problem at hand by choosing an\nappropriate rank of the approximation. Furthermore, we present a systematic\napproach of defining test functions that can be used for assessing the accuracy\nof models or optimization methods that are concerned with both continuous and\ncategorical inputs. We compare LRC to existing approaches of modeling the\ncross-correlation matrix. It turns out that the new approach performs well in\nterms of estimation of cross-correlations and response surface prediction.\nTherefore, LRC is a flexible and useful addition to existing methods,\nespecially for increasing numbers of combinations of levels of the categorical\ninputs.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:38:35 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Kirchhoff", "Dominik", ""], ["Kuhnt", "Sonja", ""]]}, {"id": "2010.02848", "submitter": "Zhu Wang", "authors": "Zhu Wang", "title": "Unified Robust Estimation via the COCO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation is concerned with how to provide reliable parameter\nestimates in the presence of outliers. Numerous robust loss functions have been\nproposed in regression and classification, along with various computing\nalgorithms. This article proposes a unified framework for loss function\nconstruction and parameter estimation. The CC-family contains composite of\nconcave and convex functions. The properties of the CC-family are investigated,\nand CC-estimation is innovatively conducted via composite optimization by\nconjugation operator (COCO). The weighted estimators are simple to implement,\ndemonstrate robust quality in penalized generalized linear models and support\nvector machines, and can be conveniently extended to even more broad\napplications with existing software.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:10:51 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wang", "Zhu", ""]]}, {"id": "2010.02968", "submitter": "Georgios Papayiannis", "authors": "Georgios I. Papayiannis, Stelios Psarakis, Athanasios N. Yannacopoulos", "title": "Statistical monitoring of functional data using the notion of Fr\\'echet\n  mean combined with the framework of the deformation models", "comments": "31 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to investigate possible advances obtained by the\nimplementation of the framework of Fr\\'echet mean and the generalized sense of\nmean that it offers, in the field of statistical process monitoring and\ncontrol. In particular, the case of non-linear profiles which are described by\ndata in functional form is considered and a framework combining the notion of\nFr\\'echet mean and deformation models is developed. The proposed monitoring\napproach is implemented to the intra-day air pollution monitoring task in the\ncity of Athens where the capabilities and advantages of the method are\nillustrated.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 18:49:51 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Papayiannis", "Georgios I.", ""], ["Psarakis", "Stelios", ""], ["Yannacopoulos", "Athanasios N.", ""]]}, {"id": "2010.02994", "submitter": "Andrew Holbrook", "authors": "Andrew J. Holbrook, Xiang Ji and Marc A. Suchard", "title": "Bayesian mitigation of spatial coarsening for a Hawkes model applied to\n  gunfire, wildfire and viral contagion", "comments": "To appear in AOAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-exciting spatiotemporal Hawkes processes have found increasing use in\nthe study of large-scale public health threats ranging from gun violence and\nearthquakes to wildfires and viral contagion. Whereas many such applications\nfeature locational uncertainty, i.e., the exact spatial positions of individual\nevents are unknown, most Hawkes model analyses to date have ignored spatial\ncoarsening present in the data. Three particular 21st century public health\ncrises -- urban gun violence, rural wildfires and global viral spread --\npresent qualitatively and quantitatively varying uncertainty regimes that\nexhibit (a) different collective magnitudes of spatial coarsening, (b) uniform\nand mixed magnitude coarsening, (c) differently shaped uncertainty regions and\n-- less orthodox -- (d) locational data distributed within the `wrong'\neffective space. We explicitly model such uncertainties in a Bayesian manner\nand jointly infer unknown locations together with all parameters of a\nreasonably flexible Hawkes model, obtaining results that are practically and\nstatistically distinct from those obtained while ignoring spatial coarsening.\nThis work also features two different secondary contributions: first, to\nfacilitate Bayesian inference of locations and background rate parameters, we\nmake a subtle yet crucial change to an established kernel-based rate model; and\nsecond, to facilitate the same Bayesian inference at scale, we develop a\nmassively parallel implementation of the model's log-likelihood gradient with\nrespect to locations and thus avoid its quadratic computational cost in the\ncontext of Hamiltonian Monte Carlo. Our examples involve thousands of\nobservations and allow us to demonstrate practicality at moderate scales.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:46:03 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 22:17:30 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Holbrook", "Andrew J.", ""], ["Ji", "Xiang", ""], ["Suchard", "Marc A.", ""]]}, {"id": "2010.03051", "submitter": "Amanda Gentzel", "authors": "Amanda Gentzel, Purva Pruthi, David Jensen", "title": "How and Why to Use Experimental Data to Evaluate Methods for\n  Observational Causal Inference", "comments": null, "journal-ref": "In Proceedings of the International Conference on Machine Learning\n  (ICML) 2021", "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods that infer causal dependence from observational data are central to\nmany areas of science, including medicine, economics, and the social sciences.\nA variety of theoretical properties of these methods have been proven, but\nempirical evaluation remains a challenge, largely due to the lack of\nobservational data sets for which treatment effect is known. We describe and\nanalyze observational sampling from randomized controlled trials (OSRCT), a\nmethod for evaluating causal inference methods using data from randomized\ncontrolled trials (RCTs). This method can be used to create constructed\nobservational data sets with corresponding unbiased estimates of treatment\neffect, substantially increasing the number of data sets available for\nempirical evaluation of causal inference methods. We show that, in expectation,\nOSRCT creates data sets that are equivalent to those produced by randomly\nsampling from empirical data sets in which all potential outcomes are\navailable. We then perform a large-scale evaluation of seven causal inference\nmethods over 37 data sets, drawn from RCTs, as well as simulators, real-world\ncomputational systems, and observational data sets augmented with a synthetic\nresponse variable. We find notable performance differences when comparing\nacross data from different sources, demonstrating the importance of using data\nfrom a variety of sources when evaluating any causal inference method.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 21:44:01 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 17:19:30 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Gentzel", "Amanda", ""], ["Pruthi", "Purva", ""], ["Jensen", "David", ""]]}, {"id": "2010.03053", "submitter": "Michalis Titsias", "authors": "Michalis K. Titsias, Jakub Sygnowski, Yutian Chen", "title": "Sequential Changepoint Detection in Neural Networks with Checkpoints", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for online changepoint detection and simultaneous\nmodel learning which is applicable to highly parametrized models, such as deep\nneural networks. It is based on detecting changepoints across time by\nsequentially performing generalized likelihood ratio tests that require only\nevaluations of simple prediction score functions. This procedure makes use of\ncheckpoints, consisting of early versions of the actual model parameters, that\nallow to detect distributional changes by performing predictions on future\ndata. We define an algorithm that bounds the Type I error in the sequential\ntesting procedure. We demonstrate the efficiency of our method in challenging\ncontinual learning applications with unknown task changepoints, and show\nimproved performance compared to online Bayesian changepoint detection.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 21:49:54 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Titsias", "Michalis K.", ""], ["Sygnowski", "Jakub", ""], ["Chen", "Yutian", ""]]}, {"id": "2010.03088", "submitter": "Piotr Szyma\\'nski", "authors": "Piotr Szyma\\'nski, Kyle Gorman", "title": "Is the Best Better? Bayesian Statistical Model Comparison for Natural\n  Language Processing", "comments": "Accepted to EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work raises concerns about the use of standard splits to compare\nnatural language processing models. We propose a Bayesian statistical model\ncomparison technique which uses k-fold cross-validation across multiple data\nsets to estimate the likelihood that one model will outperform the other, or\nthat the two will produce practically equivalent results. We use this technique\nto rank six English part-of-speech taggers across two data sets and three\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 23:37:28 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Szyma\u0144ski", "Piotr", ""], ["Gorman", "Kyle", ""]]}, {"id": "2010.03111", "submitter": "Eric Lock", "authors": "Eric F. Lock", "title": "Bayesian Distance Weighted Discrimination", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance weighted discrimination (DWD) is a linear discrimination method that\nis particularly well-suited for classification tasks with high-dimensional\ndata. The DWD coefficients minimize an intuitive objective function, which can\nsolved very efficiently using state-of-the-art optimization techniques.\nHowever, DWD has not yet been cast into a model-based framework for statistical\ninference. In this article we show that DWD identifies the mode of a proper\nBayesian posterior distribution, that results from a particular link function\nfor the class probabilities and a shrinkage-inducing proper prior distribution\non the coefficients. We describe a relatively efficient Markov chain Monte\nCarlo (MCMC) algorithm to simulate from the true posterior under this Bayesian\nframework. We show that the posterior is asymptotically normal and derive the\nmean and covariance matrix of its limiting distribution. Through several\nsimulation studies and an application to breast cancer genomics we demonstrate\nhow the Bayesian approach to DWD can be used to (1) compute well-calibrated\nposterior class probabilities, (2) assess uncertainty in the DWD coefficients\nand resulting sample scores, (3) improve power via semi-supervised analysis\nwhen not all class labels are available, and (4) automatically determine a\npenalty tuning parameter within the model-based framework. R code to perform\nBayesian DWD is available at https://github.com/lockEF/BayesianDWD .\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 02:15:04 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Lock", "Eric F.", ""]]}, {"id": "2010.03141", "submitter": "Yasuyuki Hamura", "authors": "Yasuyuki Hamura", "title": "Bayesian Shrinkage Approaches to Unbalanced Problems of Estimation and\n  Prediction on the Basis of Negative Multinomial Samples", "comments": "34 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we treat estimation and prediction problems where negative\nmultinomial variables are observed and in particular consider unbalanced\nsettings. First, the problem of estimating multiple negative multinomial\nparameter vectors under the standardized squared error loss is treated and a\nnew empirical Bayes estimator which dominates the UMVU estimator under suitable\nconditions is derived. Second, we consider estimation of the joint predictive\ndensity of several multinomial tables under the Kullback-Leibler divergence and\nobtain a sufficient condition under which the Bayesian predictive density with\nrespect to a hierarchical shrinkage prior dominates the Bayesian predictive\ndensity with respect to the Jeffreys prior. Third, our proposed Bayesian\nestimator and predictive density give risk improvements in simulations.\nFinally, the problem of estimating the joint predictive density of negative\nmultinomial variables is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 03:54:05 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Hamura", "Yasuyuki", ""]]}, {"id": "2010.03293", "submitter": "Daan Crommelin", "authors": "Nick Verheul and Daan Crommelin", "title": "Stochastic parameterization with VARX processes", "comments": null, "journal-ref": "Commun. Appl. Math. Comput. Sci. 16 (2021) 33-57", "doi": "10.2140/camcos.2021.16.33", "report-no": null, "categories": "stat.ME cs.LG cs.NA math.NA physics.ao-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we investigate a data-driven stochastic methodology to\nparameterize small-scale features in a prototype multiscale dynamical system,\nthe Lorenz '96 (L96) model. We propose to model the small-scale features using\na vector autoregressive process with exogenous variable (VARX), estimated from\ngiven sample data. To reduce the number of parameters of the VARX we impose a\ndiagonal structure on its coefficient matrices. We apply the VARX to two\ndifferent configurations of the 2-layer L96 model, one with common parameter\nchoices giving unimodal invariant probability distributions for the L96 model\nvariables, and one with non-standard parameters giving trimodal distributions.\nWe show through various statistical criteria that the proposed VARX performs\nvery well for the unimodal configuration, while keeping the number of\nparameters linear in the number of model variables. We also show that the\nparameterization performs accurately for the very challenging trimodal L96\nconfiguration by allowing for a dense (non-diagonal) VARX covariance matrix.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 09:11:34 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Verheul", "Nick", ""], ["Crommelin", "Daan", ""]]}, {"id": "2010.03302", "submitter": "Alan Huang", "authors": "Alan Huang, Lucas Sippel, and Thomas Fung", "title": "A consistent second-order discrete kernel smoother", "comments": "11 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The histogram estimator of a discrete probability mass function often\nexhibits undesirable properties related to zero probability estimation both\nwithin the observed range of counts and outside into the tails of the\ndistribution. To circumvent this, we formulate a novel second-order discrete\nkernel smoother based on the recently developed mean-parametrized\nConway--Maxwell--Poisson distribution. Two automated bandwidth selection\napproaches, one based on a simple minimization of the Kullback--Leibler\ndivergence and another based on a more computationally demanding\ncross-validation criterion, are introduced. Both methods exhibit excellent\nsmall- and large-sample performance. Computational results on simulated\ndatasets from a range of target distributions illustrate the flexibility and\naccuracy of the proposed method compared to existing smoothed and unsmoothed\nestimators. The method is applied to the modelling of somite counts in\nearthworms, and the number of development days of insect pests on the Hura\ntree.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 09:30:25 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 07:07:13 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Huang", "Alan", ""], ["Sippel", "Lucas", ""], ["Fung", "Thomas", ""]]}, {"id": "2010.03390", "submitter": "Yifan Cui", "authors": "Yifan Cui and Eric Tchetgen Tchetgen", "title": "On a necessary and sufficient identification condition of optimal\n  treatment regimes with an instrumental variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmeasured confounding is a threat to causal inference and individualized\ndecision making. Similar to Cui and Tchetgen Tchetgen (2020); Qiu et al.\n(2020); Han (2020a), we consider the problem of identification of optimal\nindividualized treatment regimes with a valid instrumental variable. Han\n(2020a) provided an alternative identifying condition of optimal treatment\nregimes using the conditional Wald estimand of Cui and Tchetgen Tchetgen\n(2020); Qiu et al. (2020) when treatment assignment is subject to endogeneity\nand a valid binary instrumental variable is available. In this note, we provide\na necessary and sufficient condition for identification of optimal treatment\nregimes using the conditional Wald estimand. Our novel condition is necessarily\nimplied by those of Cui and Tchetgen Tchetgen (2020); Qiu et al. (2020); Han\n(2020a) and may continue to hold in a variety of potential settings not covered\nby prior results.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 13:03:42 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Cui", "Yifan", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "2010.03501", "submitter": "Kejia Wang", "authors": "Chang-Xing Ma, Kejia Wang", "title": "Testing the Equality of Proportions for Combined Unilateral and\n  Bilateral Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurements are generally collected as unilateral or bilateral data in\nclinical trials or observational studies. For example, in ophthalmologic\nstudies, statistical tests are often based on one or two eyes of an individual.\nFor bilateral data, recent literatures have shown some testing procedures that\ntake into account the intra-class correlation between two eyes of the same\nperson. Ma et al. (2015) investigated three testing procedures under Rosner's\nmodel. In this paper, we extend Ma's work for bilateral data to combined\nbilateral and unilateral data. The proposed procedures are based on the\nlikelihood estimate algorithm derived from the root of 4th order polynomial\nequations and fisher scoring iterations. Simulation studies are performed to\ncompare the testing procedures under different parameter configurations. The\nresult shows that score test has satisfactory type I error rates and powers.\nTherefore, we recommend score test for testing the equality of proportions. We\nillustrate the application of the proposed methods with a double-blind\nrandomized clinical trial.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 16:11:16 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Ma", "Chang-Xing", ""], ["Wang", "Kejia", ""]]}, {"id": "2010.03509", "submitter": "Moritz Schauer", "authors": "Frank van der Meulen and Moritz Schauer", "title": "Automatic Backward Filtering Forward Guiding for Markov processes and\n  graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We incorporate discrete and continuous time Markov processes as building\nblocks into probabilistic graphical models with latent and observed variables.\nWe introduce the automatic Backward Filtering Forward Guiding (BFFG) paradigm\n(Mider et al., 2020) for programmable inference on latent states and model\nparameters. Our starting point is a generative model, a forward description of\nthe probabilistic process dynamics. We backpropagate the information provided\nby observations through the model to transform the generative (forward) model\ninto a pre-conditional model guided by the data. It approximates the actual\nconditional model with known likelihood-ratio between the two. The backward\nfilter and the forward change of measure are suitable to be incorporated into a\nprobabilistic programming context because they can be formulated as a set of\ntransformation rules.\n  The guided generative model can be incorporated in different approaches to\nefficiently sample latent states and parameters conditional on observations. We\nshow applicability in a variety of settings, including Markov chains with\ndiscrete state space, interacting particle systems, state space models,\nbranching diffusions and Gamma processes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 16:30:13 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 16:50:29 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""]]}, {"id": "2010.03587", "submitter": "Zengyi Li", "authors": "Zengyi Li, Yubei Chen, Friedrich T. Sommer", "title": "A Neural Network MCMC sampler that maximizes Proposal Entropy", "comments": "conference submission preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) methods sample from unnormalized probability\ndistributions and offer guarantees of exact sampling. However, in the\ncontinuous case, unfavorable geometry of the target distribution can greatly\nlimit the efficiency of MCMC methods. Augmenting samplers with neural networks\ncan potentially improve their efficiency. Previous neural network based\nsamplers were trained with objectives that either did not explicitly encourage\nexploration, or used a L2 jump objective which could only be applied to well\nstructured distributions. Thus it seems promising to instead maximize the\nproposal entropy for adapting the proposal to distributions of any shape. To\nallow direct optimization of the proposal entropy, we propose a neural network\nMCMC sampler that has a flexible and tractable proposal distribution.\nSpecifically, our network architecture utilizes the gradient of the target\ndistribution for generating proposals. Our model achieves significantly higher\nefficiency than previous neural network MCMC techniques in a variety of\nsampling tasks. Further, the sampler is applied on training of a convergent\nenergy-based model of natural images. The adaptive sampler achieves unbiased\nsampling with significantly higher proposal entropy than Langevin dynamics\nsampler.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 18:01:38 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Li", "Zengyi", ""], ["Chen", "Yubei", ""], ["Sommer", "Friedrich T.", ""]]}, {"id": "2010.03700", "submitter": "Xiucai Ding", "authors": "Xiucai Ding, Dengdeng Yu, Zhengwu Zhang and Dehan Kong", "title": "Multivariate functional responses low rank regression with an\n  application to brain imaging data", "comments": "Canadian Journal of Statistics(accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multivariate functional responses low rank regression model with\npossible high dimensional functional responses and scalar covariates. By\nexpanding the slope functions on a set of sieve basis, we reconstruct the basis\ncoefficients as a matrix. To estimate these coefficients, we propose an\nefficient procedure using nuclear norm regularization. We also derive error\nbounds for our estimates and evaluate our method using simulations. We further\napply our method to the Human Connectome Project neuroimaging data to predict\ncortical surface motor task-evoked functional magnetic resonance imaging\nsignals using various clinical covariates to illustrate the usefulness of our\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 00:20:44 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Ding", "Xiucai", ""], ["Yu", "Dengdeng", ""], ["Zhang", "Zhengwu", ""], ["Kong", "Dehan", ""]]}, {"id": "2010.03783", "submitter": "David Issa Mattos", "authors": "David Issa Mattos, Jan Bosch, Helena Holmstr\\\"om Olsson", "title": "Statistical Models for the Analysis of Optimization Algorithms with\n  Benchmark Functions", "comments": null, "journal-ref": "IEEE Transactions on Evolutionary Computation\n  (DOI:10.1109/TEVC.2021.3081167)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequentist statistical methods, such as hypothesis testing, are standard\npractice in papers that provide benchmark comparisons. Unfortunately, these\nmethods have often been misused, e.g., without testing for their statistical\ntest assumptions or without controlling for family-wise errors in multiple\ngroup comparisons, among several other problems. Bayesian Data Analysis (BDA)\naddresses many of the previously mentioned shortcomings but its use is not\nwidely spread in the analysis of empirical data in the evolutionary computing\ncommunity. This paper provides three main contributions. First, we motivate the\nneed for utilizing Bayesian data analysis and provide an overview of this\ntopic. Second, we discuss the practical aspects of BDA to ensure that our\nmodels are valid and the results transparent. Finally, we provide five\nstatistical models that can be used to answer multiple research questions. The\nonline appendix provides a step-by-step guide on how to perform the analysis of\nthe models discussed in this paper, including the code for the statistical\nmodels, the data transformations and the discussed tables and figures.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 06:01:29 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 00:51:47 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 10:24:28 GMT"}, {"version": "v4", "created": "Sat, 15 May 2021 07:54:30 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Mattos", "David Issa", ""], ["Bosch", "Jan", ""], ["Olsson", "Helena Holmstr\u00f6m", ""]]}, {"id": "2010.03792", "submitter": "Masahiro Kato", "authors": "Masahiro Kato and Shota Yasui and Kenichiro McAlinn", "title": "The Adaptive Doubly Robust Estimator for Policy Evaluation in Adaptive\n  Experiments and a Paradox Concerning Logging Policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The doubly robust (DR) estimator, which consists of two nuisance parameters,\nthe conditional mean outcome and the logging policy (the probability of\nchoosing an action), is crucial in causal inference. This paper proposes a DR\nestimator for dependent samples obtained from adaptive experiments. To obtain\nan asymptotically normal semiparametric estimator from dependent samples with\nnon-Donsker nuisance estimators, we propose adaptive-fitting as a variant of\nsample-splitting. We also report an empirical paradox that our proposed DR\nestimator tends to show better performances compared to other estimators\nutilizing the true logging policy. While a similar phenomenon is known for\nestimators with i.i.d. samples, traditional explanations based on asymptotic\nefficiency cannot elucidate our case with dependent samples. We confirm this\nhypothesis through simulation studies.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 06:42:48 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 14:10:11 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 17:07:15 GMT"}, {"version": "v4", "created": "Tue, 8 Jun 2021 19:40:40 GMT"}, {"version": "v5", "created": "Fri, 18 Jun 2021 22:17:48 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kato", "Masahiro", ""], ["Yasui", "Shota", ""], ["McAlinn", "Kenichiro", ""]]}, {"id": "2010.03827", "submitter": "Maria D. Ruiz-Medina", "authors": "Torres-Signes, M.P. Fr\\'ias, J. Mateu and M.D. Ruiz-Medina", "title": "A spatial functional count model for heterogeneity analysis in time", "comments": "37 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A spatial curve dynamical model framework is adopted for functional\nprediction of counts in a spatiotemporal log-Gaussian Cox process model. Our\nspatial functional estimation approach handles both wavelet-based heterogeneity\nanalysis in time, and spectral analysis in space. Specifically, model fitting\nis achieved by minimising the information divergence or relative entropy\nbetween the multiscale model underlying the data and the corresponding\ncandidates in the spatial spectral domain. A simulation study is carried out\nwithin the family of log-Gaussian Spatial Autoregressive l2-valued processes\n(SARl2 processes) to illustrate the asymptotic properties of the proposed\nspatial functional estimators. We apply our modelling strategy to\nspatiotemporal prediction of respiratory disease mortality.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 08:03:33 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Torres-Signes", "", ""], ["Fr\u00edas", "M. P.", ""], ["Mateu", "J.", ""], ["Ruiz-Medina", "M. D.", ""]]}, {"id": "2010.03828", "submitter": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez", "authors": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez and Mar\\'ia Durb\\'an and Paul\n  H.C. Eilers and Dae-Jin Lee and Francisco Gonzalez", "title": "Multidimensional Adaptive Penalised Splines with Application to Neurons'\n  Activity Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  P-spline models have achieved great popularity both in statistical and in\napplied research. A possible drawback of P-spline is that they assume a smooth\ntransition of the covariate effect across its whole domain. In some practical\napplications, however, it is desirable and needed to adapt smoothness locally\nto the data, and adaptive P-splines have been suggested. Yet, the extra\nflexibility afforded by adaptive P-spline models is obtained at the cost of a\nhigh computational burden, especially in a multidimensional setting.\nFurthermore, to the best of our knowledge, the literature lacks proposals for\nadaptive P-splines in more than two dimensions. Motivated by the need for\nanalysing data derived from experiments conducted to study neurons' activity in\nthe visual cortex, this work presents a novel locally adaptive anisotropic\nP-spline model in two (e.g., space) and three (space and time) dimensions.\nEstimation is based on the recently proposed SOP (Separation of Overlapping\nPrecision matrices) method, which provides the speed we look for. The practical\nperformance of the proposal is evaluated through simulations, and comparisons\nwith alternative methods are reported. In addition to the spatio-temporal\nanalysis of the data that motivated this work, we also discuss an application\nin two dimensions on the absenteeism of workers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 08:05:23 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 17:03:46 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Rodr\u00edguez-\u00c1lvarez", "Mar\u00eda Xos\u00e9", ""], ["Durb\u00e1n", "Mar\u00eda", ""], ["Eilers", "Paul H. C.", ""], ["Lee", "Dae-Jin", ""], ["Gonzalez", "Francisco", ""]]}, {"id": "2010.03832", "submitter": "Olivier Wintenberger", "authors": "Marco Oesting, Olivier Wintenberger (LPSM UMR 8001)", "title": "Estmiation of the Spectral Measure from Convex Combinations of Regularly\n  Varying Random Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extremal dependence structure of a regularly varying random vector X is\nfully described by its limiting spectral measure. In this paper, we investigate\nhow to recover characteristics of the measure, such as extremal coefficients,\nfrom the extremal behaviour of convex combinations of components of X. Our\nconsiderations result in a class of new estimators of moments of the\ncorresponding combinations for the spectral vector. We show asymp-totic\nnormality by means of a functional limit theorem and, focusing on the\nestimation of extremal coefficients, we verify that the minimal asymptotic\nvariance can be achieved by a plug-in estimator.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 08:20:24 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Oesting", "Marco", "", "LPSM UMR 8001"], ["Wintenberger", "Olivier", "", "LPSM UMR 8001"]]}, {"id": "2010.03985", "submitter": "Giri Gopalan", "authors": "Giri Gopalan, Christopher K. Wikle", "title": "A higher-order singular value decomposition tensor emulator for\n  spatio-temporal simulators", "comments": null, "journal-ref": "Journal of Agricultural, Biological, and Environmental Statistics\n  2021", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce methodology to construct an emulator for environmental and\necological spatio-temporal processes that uses the higher order singular value\ndecomposition (HOSVD) as an extension of singular value decomposition (SVD)\napproaches to emulation. Some important advantages of the method are that it\nallows for the use of a combination of supervised learning methods (e.g.,\nrandom forests and Gaussian process regression) and also allows for the\nprediction of process values at spatial locations and time points that were not\nused in the training sample. The method is demonstrated with two applications:\nthe first is a periodic solution to a shallow ice approximation partial\ndifferential equation from glaciology, and second is an agent-based model of\ncollective animal movement. In both cases, we demonstrate the value of\ncombining different machine learning models for accurate emulation. In\naddition, in the agent-based model case we demonstrate the ability of the\ntensor emulator to successfully capture individual behavior in space and time.\nWe demonstrate via a real data example the ability to perform Bayesian\ninference in order to learn parameters governing collective animal behavior.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 16:31:54 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 01:31:20 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Gopalan", "Giri", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "2010.04044", "submitter": "Tullio Mancini", "authors": "Tullio Mancini, Hector Calvo-Pardo, and Jose Olmo", "title": "Prediction intervals for Deep Neural Networks", "comments": "35 pages, 3 Figures, and 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to propose a suitable method for constructing\nprediction intervals for the output of neural network models. To do this, we\nadapt the extremely randomized trees method originally developed for random\nforests to construct ensembles of neural networks. The extra-randomness\nintroduced in the ensemble reduces the variance of the predictions and yields\ngains in out-of-sample accuracy. An extensive Monte Carlo simulation exercise\nshows the good performance of this novel method for constructing prediction\nintervals in terms of coverage probability and mean square prediction error.\nThis approach is superior to state-of-the-art methods extant in the literature\nsuch as the widely used MC dropout and bootstrap procedures. The out-of-sample\naccuracy of the novel algorithm is further evaluated using experimental\nsettings already adopted in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:11:28 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 09:16:19 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Mancini", "Tullio", ""], ["Calvo-Pardo", "Hector", ""], ["Olmo", "Jose", ""]]}, {"id": "2010.04058", "submitter": "Stephane Robin", "authors": "St\\'ephane Robin and Luca Scrucca", "title": "Mixture-based estimation of entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The entropy is a measure of uncertainty that plays a central role in\ninformation theory. When the distribution of the data is unknown, an estimate\nof the entropy needs be obtained from the data sample itself. We propose a\nsemi-parametric estimate, based on a mixture model approximation of the\ndistribution of interest. The estimate can rely on any type of mixture, but we\nfocus on Gaussian mixture model to demonstrate its accuracy and versatility.\nPerformance of the proposed approach is assessed through a series of simulation\nstudies. We also illustrate its use on two real-life data examples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:23:27 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Robin", "St\u00e9phane", ""], ["Scrucca", "Luca", ""]]}, {"id": "2010.04076", "submitter": "Andreas Hagemann", "authors": "Andreas Hagemann", "title": "Inference with a single treated cluster", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I introduce a generic method for inference about a scalar parameter in\nresearch designs with a finite number of heterogeneous clusters where only a\nsingle cluster received treatment. This situation is commonplace in\ndifference-in-differences estimation but the test developed here applies more\ngenerally. I show that the test controls size and has power under asymptotics\nwhere the number of observations within each cluster is large but the number of\nclusters is fixed. The test combines weighted, approximately Gaussian parameter\nestimates with a rearrangement procedure to obtain its critical values. The\nweights needed for most empirically relevant situations are tabulated in the\npaper. Calculation of the critical values is computationally simple and does\nnot require simulation or resampling. The rearrangement test is highly robust\nto situations where some clusters are much more variable than others. Examples\nand an empirical application are provided.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:59:47 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Hagemann", "Andreas", ""]]}, {"id": "2010.04218", "submitter": "Martin Kroll", "authors": "Martin Kroll", "title": "Adaptive spectral density estimation by model selection under local\n  differential privacy", "comments": "33 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study spectral density estimation under local differential privacy.\nAnonymization is achieved through truncation followed by Laplace perturbation.\nWe select our estimator from a set of candidate estimators by a penalized\ncontrast criterion. This estimator is shown to attain nearly the same rate of\nconvergence as the best estimator from the candidate set. A key ingredient of\nthe proof are recent results on concentration of quadratic forms in terms of\nsub-exponential random variables obtained in arXiv:1903.05964. We illustrate\nour findings in a small simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 19:00:08 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Kroll", "Martin", ""]]}, {"id": "2010.04253", "submitter": "Nathan Wikle", "authors": "Nathan B. Wikle, Ephraim M. Hanks, Lucas R.F. Henneman, and Corwin M.\n  Zigler", "title": "A Mechanistic Model of Annual Sulfate Concentrations in the United\n  States", "comments": "21 pages, 5 figures. For associated code and supplementary material,\n  see https://github.com/nbwikle/mechanisticSO4-supp_material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a mechanistic model to analyze the impact of sulfur dioxide\nemissions from coal-fired power plants on average sulfate concentrations in the\ncentral United States. A multivariate Ornstein-Uhlenbeck (OU) process is used\nto approximate the dynamics of the underlying space-time chemical transport\nprocess, and its distributional properties are leveraged to specify novel\nprobability models for spatial data (i.e., spatially-referenced data with no\ntemporal replication) that are viewed as either a snapshot or a time-averaged\nobservation of the OU process. Air pollution transport dynamics determine the\nmean and covariance structure of our atmospheric sulfate model, allowing us to\ninfer which process dynamics are driving observed air pollution concentrations.\nWe use these inferred dynamics to assess the regulatory impact of flue-gas\ndesulfurization (FGD) technologies on human exposure to sulfate aerosols.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 20:49:10 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Wikle", "Nathan B.", ""], ["Hanks", "Ephraim M.", ""], ["Henneman", "Lucas R. F.", ""], ["Zigler", "Corwin M.", ""]]}, {"id": "2010.04485", "submitter": "Daniel Nevo", "authors": "Daniel Nevo and Malka Gorfine", "title": "Causal inference for semi-competing risks data", "comments": "35 pages, 3 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging challenge for time-to-event data is studying semi-competing\nrisks, namely when two event times are of interest: a non-terminal event time\n(e.g. age at disease diagnosis), and a terminal event time (e.g. age at death).\nThe non-terminal event is observed only if it precedes the terminal event,\nwhich may occur before or after the non-terminal event. Studying treatment or\nintervention effects on the dual event times is complicated because for some\nunits, the non-terminal event may occur under one treatment value but not under\nthe other. Until recently, existing approaches (e.g., the survivor average\ncausal effect) generally disregarded the time-to-event nature of both outcomes.\nMore recent research focused on principal strata effects within time-varying\npopulations under Bayesian approaches. In this paper, we propose alternative\nnon time-varying estimands, based on a single stratification of the population.\nWe present a novel assumption utilizing the time-to-event nature of the data,\nwhich is weaker than the often-invoked monotonicity assumption. We derive\nresults on partial identifiability, suggest a sensitivity analysis approach,\nand give conditions under which full identification is possible. Finally, we\npresent non-parametric and semi-parametric estimation methods for\nright-censored data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 10:19:20 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Nevo", "Daniel", ""], ["Gorfine", "Malka", ""]]}, {"id": "2010.04492", "submitter": "Binyan Jiang", "authors": "Binyan Jiang, Jailing Li, Qiwei Yao", "title": "Autoregressive Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a first-order autoregressive model for dynamic network processes\nin which edges change over time while nodes remain unchanged. The model depicts\nthe dynamic changes explicitly. It also facilitates simple and efficient\nstatistical inference such as the maximum likelihood estimators which are\nproved to be (uniformly) consistent and asymptotically normal. The model\ndiagnostic checking can be carried out easily using a permutation test. The\nproposed model can apply to any network processes with various underlying\nstructures but with independent edges. As an illustration, an autoregressive\nstochastic block model has been investigated in depth, which characterizes the\nlatent communities by the transition probabilities over time. This leads to a\nmore effective spectral clustering algorithm for identifying the latent\ncommunities. Inference for a change point is incorporated into the\nautoregressive stochastic block model to cater for possible structure changes.\nThe developed asymptotic theory as well as the simulation study affirms the\nperformance of the proposed methods. Application with three real data sets\nillustrates both relevance and usefulness of the proposed models.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 10:46:50 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 16:09:17 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Jiang", "Binyan", ""], ["Li", "Jailing", ""], ["Yao", "Qiwei", ""]]}, {"id": "2010.04515", "submitter": "Raanju Ragavendar Sundararajan", "authors": "Raanju R. Sundararajan", "title": "Principal Component Analysis using Frequency Components of Multivariate\n  Time Series", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction techniques for multivariate time series decompose the\nobserved series into a few useful independent/orthogonal univariate components.\nWe develop a spectral domain method for multivariate second-order stationary\ntime series that linearly transforms the observed series into several groups of\nlower-dimensional multivariate subseries. These multivariate subseries have\nnon-zero spectral coherence among components within a group but have zero\nspectral coherence among components across groups. The observed series is\nexpressed as a sum of frequency components whose variances are proportional to\nthe spectral matrices at the respective frequencies. The demixing matrix is\nthen estimated using an eigendecomposition on the sum of the variance matrices\nof these frequency components and its asymptotic properties are derived.\nFinally, a consistent test on the cross-spectrum of pairs of components is used\nto find the desired segmentation into the lower-dimensional subseries. The\nnumerical performance of the proposed method is illustrated through simulation\nexamples and an application to modeling and forecasting wind data is presented.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 11:55:39 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Sundararajan", "Raanju R.", ""]]}, {"id": "2010.04557", "submitter": "Franck Picard", "authors": "Anna Bonnet and Claire Lacour and Franck Picard and Vincent Rivoirard", "title": "Uniform Deconvolution for Poisson Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the estimation of the intensity of a Poisson process in the\npresence of a uniform noise. We propose a kernel-based procedure fully\ncalibrated in theory and practice. We show that our adaptive estimator is\noptimal from the oracle and minimax points of view, and provide new lower\nbounds when the intensity belongs to a Sobolev ball. By developing the\nGoldenshluger-Lepski methodology in the case of deconvolution for Poisson\nprocesses, we propose an optimal data-driven selection of the kernel's\nbandwidth, and we provide a heuristic framework to calibrate the estimator in\npractice. Our method is illustrated on the spatial repartition of replication\norigins along the human genome.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 13:23:02 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Bonnet", "Anna", ""], ["Lacour", "Claire", ""], ["Picard", "Franck", ""], ["Rivoirard", "Vincent", ""]]}, {"id": "2010.04666", "submitter": "Alex Rodrigo dos Santos Sousa", "authors": "Alex Rodrigo dos Santos Sousa", "title": "Asymmetric prior in wavelet shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In bayesian wavelet shrinkage, the already proposed priors to wavelet\ncoefficients are assumed to be symmetric around zero. Although this assumption\nis reasonable in many applications, it is not general. The present paper\nproposes the use of an asymmetric shrinkage rule based on the discrete mixture\nof a point mass function at zero and an asymmetric beta distribution as prior\nto the wavelet coefficients in a non-parametric regression model. Statistical\nproperties such as bias, variance, classical and bayesian risks of the\nassociated asymmetric rule are provided and performances of the proposed rule\nare obtained in simulation studies involving artificial asymmetric distributed\ncoefficients and the Donoho-Johnstone test functions. Application in a seismic\nreal dataset is also analyzed.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 16:23:47 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Sousa", "Alex Rodrigo dos Santos", ""]]}, {"id": "2010.04680", "submitter": "Megan Murray", "authors": "Megan Hollister Murray and Jeffrey D. Blume", "title": "False Discovery Rate Computation: Illustrations and Modifications", "comments": "This article includes 18 pages and 9 figures. It is in process of\n  being submitted to \"The R Journal\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  False discovery rates (FDR) are an essential component of statistical\ninference, representing the propensity for an observed result to be mistaken.\nFDR estimates should accompany observed results to help the user contextualize\nthe relevance and potential impact of findings. This paper introduces a new\nuser-friendly R package for computing FDRs and adjusting p-values for FDR\ncontrol. These tools respect the critical difference between the adjusted\np-value and the estimated FDR for a particular finding, which are sometimes\nnumerically identical but are often confused in practice. Newly augmented\nmethods for estimating the null proportion of findings - an important part of\nthe FDR estimation procedure - are proposed and evaluated. The package is\nbroad, encompassing a variety of methods for FDR estimation and FDR control,\nand includes plotting functions for easy display of results. Through extensive\nillustrations, we strongly encourage wider reporting of false discovery rates\nfor observed findings.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 17:03:50 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Murray", "Megan Hollister", ""], ["Blume", "Jeffrey D.", ""]]}, {"id": "2010.04814", "submitter": "Pedro H. C. Sant'Anna", "authors": "Jonathan Roth and Pedro H. C. Sant'Anna", "title": "When Is Parallel Trends Sensitive to Functional Form?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper assesses when the validity of difference-in-differences and\nrelated estimators depends on functional form. We provide a novel\ncharacterization: the parallel trends assumption holds under all strictly\nmonotonic transformations of the outcome if and only if a stronger \"parallel\ntrends\"-type condition holds for the cumulative distribution function of\nuntreated potential outcomes. This condition is satisfied if and essentially\nonly if the population can be partitioned into a subgroup for which treatment\nis effectively randomly assigned and a remaining subgroup for which the\ndistribution of untreated potential outcomes is stable over time. We show\nfurther that it is impossible to construct any estimator that is consistent (or\nunbiased) for the average treatment effect on the treated (ATT) without either\nimposing functional form restrictions or imposing assumptions that identify the\nfull distribution of untreated potential outcomes. Our results suggest that\nresearchers who wish to point-identify the ATT should justify one of the\nfollowing: (i) why treatment is as-if randomly assigned, (ii) why the chosen\nfunctional form is correct at the exclusion of others, or (iii) a method for\ninferring the entire counterfactual distribution of untreated potential\noutcomes.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 21:25:43 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 23:16:31 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 23:38:15 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Roth", "Jonathan", ""], ["Sant'Anna", "Pedro H. C.", ""]]}, {"id": "2010.04896", "submitter": "Jeffrey Miller", "authors": "Jeffrey W. Miller and Scott L. Carter", "title": "Inference in generalized bilinear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent factor models are widely used to discover and adjust for hidden\nvariation in modern applications. However, most methods do not fully account\nfor uncertainty in the latent factors, which can lead to miscalibrated\ninferences such as overconfident p-values. In this article, we develop a fast\nand accurate method of uncertainty quantification in generalized bilinear\nmodels, which are a flexible extension of generalized linear models to include\nlatent factors as well as row covariates, column covariates, and interactions.\nIn particular, we introduce delta propagation, a general technique for\npropagating uncertainty among model components using the delta method. Further,\nwe provide a rapidly converging algorithm for maximum a posteriori GBM\nestimation that extends earlier methods by estimating row and column\ndispersions. In simulation studies, we find that our method provides\napproximately correct frequentist coverage of most parameters of interest. We\ndemonstrate on RNA-seq gene expression analysis and copy ratio estimation in\ncancer genomics.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 04:18:29 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Miller", "Jeffrey W.", ""], ["Carter", "Scott L.", ""]]}, {"id": "2010.05117", "submitter": "George Gui", "authors": "George Gui", "title": "Combining Observational and Experimental Data Using First-stage\n  Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized controlled trials generate experimental variation that can\ncredibly identify causal effects, but often suffer from limited scale, while\nobservational datasets are large, but often violate desired identification\nassumptions. To improve estimation efficiency, I propose a method that combines\nexperimental and observational datasets when 1) units from these two datasets\nare sampled from the same population and 2) some characteristics of these units\nare observed. I show that if these characteristics can partially explain\ntreatment assignment in the observational data, they can be used to derive\nmoment restrictions that, in combination with the experimental data, improve\nestimation efficiency. I outline three estimators (weighting, shrinkage, or\nGMM) for implementing this strategy, and show that my methods can reduce\nvariance by up to 50% in typical experimental designs; therefore, only half of\nthe experimental sample is required to attain the same statistical precision.\nIf researchers are allowed to design experiments differently, I show that they\ncan further improve the precision by directly leveraging this correlation\nbetween characteristics and assignment. I apply my method to a search listing\ndataset from Expedia that studies the causal effect of search rankings, and\nshow that the method can substantially improve the precision.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 23:29:27 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 05:26:24 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 21:55:08 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Gui", "George", ""]]}, {"id": "2010.05220", "submitter": "Erin Gabriel", "authors": "Erin E. Gabriel, Arvid Sj\\\"olander, Michael C. Sachs", "title": "Nonparametric bounds for causal effects in imperfect randomized\n  experiments", "comments": "35 pages, 5 figures, includes supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonignorable missingness and noncompliance can occur even in well-designed\nrandomized experiments making the intervention effect that the experiment was\ndesigned to estimate nonidentifiable. Nonparametric causal bounds provide a way\nto narrow the range of possible values for a nonidentifiable causal effect with\nminimal assumptions. We derive novel bounds for the causal risk difference for\na binary outcome and intervention in randomized experiments with nonignorable\nmissingness caused by a variety of mechanisms and with or without\nnoncompliance. We illustrate the use of the proposed bounds in our motivating\ndata example of peanut consumption on the development of peanut allergies in\ninfants.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 10:51:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Gabriel", "Erin E.", ""], ["Sj\u00f6lander", "Arvid", ""], ["Sachs", "Michael C.", ""]]}, {"id": "2010.05287", "submitter": "Vincenzo Nardelli", "authors": "Giuseppe Arbia and Vincenzo Nardelli", "title": "On Spatial Lag Models estimated using crowdsourcing, web-scraping or\n  other unconventionally collected data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Big Data revolution is challenging the state-of-the-art statistical and\neconometric techniques not only for the computational burden connected with the\nhigh volume and speed which data are generated, but even more for the variety\nof sources through which data are collected (Arbia, 2021). This paper\nconcentrates specifically on this last aspect. Common examples of non\ntraditional Big Data sources are represented by crowdsourcing (data voluntarily\ncollected by individuals) and web scraping (data extracted from websites and\nreshaped in a structured dataset). A common characteristic to these\nunconventional data collections is the lack of any precise statistical sample\ndesign, a situation described in statistics as 'convenience sampling'. As it is\nwell known, in these conditions no probabilistic inference is possible. To\novercome this problem, Arbia et al. (2018) proposed the use of a special form\nof post-stratification (termed 'post-sampling'), with which data are\nmanipulated prior their use in an inferential context. In this paper we\ngeneralize this approach using the same idea to estimate a Spatial Lag Model\n(SLM). We start showing through a Monte Carlo study that using data collected\nwithout a proper design, parameters' estimates can be biased. Secondly, we\npropose a post sampling strategy to tackle this problem. We show that the\nproposed strategy indeed achieves a bias-reduction, but at the price of a\nconcomitant increase in the variance of the estimators. We thus suggest an\nMSE-correction operational strategy. The paper also contains a formal\nderivation of the increase in variance implied by the post-sampling procedure\nand concludes with an empirical application of the method in the estimation of\na hedonic price model in the city of Milan using web scraped data.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 17:19:27 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Arbia", "Giuseppe", ""], ["Nardelli", "Vincenzo", ""]]}, {"id": "2010.05320", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Kaiying Ji and Ufuk Beyaztas", "title": "Granger causality of bivariate stationary curve time series", "comments": "17 pages, 3 figures, to appear at the Journal of Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study causality between bivariate curve time series using the Granger\ncausality generalized measures of correlation. With this measure, we can\ninvestigate which curve time series Granger-causes the other; in turn, it helps\ndetermine the predictability of any two curve time series. Illustrated by a\nclimatology example, we find that the sea surface temperature Granger-causes\nthe sea-level atmospheric pressure. Motivated by a portfolio management\napplication in finance, we single out those stocks that lead or lag behind\nDow-Jones industrial averages. Given a close relationship between S&P 500 index\nand crude oil price, we determine the leading and lagging variables.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 19:03:18 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 21:55:57 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Shang", "Han Lin", ""], ["Ji", "Kaiying", ""], ["Beyaztas", "Ufuk", ""]]}, {"id": "2010.05504", "submitter": "Ajmal Oodally", "authors": "Ajmal Oodally and Estelle Kuhn and Klara Goethals and Luc Duchateau", "title": "Modeling dependent survival data through random effects with spatial\n  correlation at the subject level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical phenomena such as infectious diseases are often investigated by\nfollowing up subjects longitudinally, thus generating time to event data. The\nspatial aspect of such data is also of primordial importance, as many\ninfectious diseases are transmitted from one subject to another. In this paper,\na spatially correlated frailty model is introduced that accommodates for the\ncorrelation between subjects based on the distance between them. Estimates are\nobtained through a stochastic approximation version of the Expectation\nMaximization algorithm combined with a Monte-Carlo Markov Chain, for which\nconvergence is proven. The novelty of this model is that spatial correlation is\nintroduced for survival data at the subject level, each subject having its own\nfrailty. This univariate spatially correlated frailty model is used to analyze\nspatially dependent malaria data, and its results are compared with other\nstandard models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 07:57:23 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Oodally", "Ajmal", ""], ["Kuhn", "Estelle", ""], ["Goethals", "Klara", ""], ["Duchateau", "Luc", ""]]}, {"id": "2010.05593", "submitter": "Giovanni Saraceno", "authors": "Giovanni Saraceno, Abhik Ghosh, Ayanendranath Basu, Claudio\n  Agostinelli", "title": "Robust Estimation under Linear Mixed Models: The Minimum Density Power\n  Divergence Approach", "comments": "25 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-life data sets can be analyzed using Linear Mixed Models (LMMs).\nSince these are ordinarily based on normality assumptions, under small\ndeviations from the model the inference can be highly unstable when the\nassociated parameters are estimated by classical methods. On the other hand,\nthe density power divergence (DPD) family, which measures the discrepancy\nbetween two probability density functions, has been successfully used to build\nrobust estimators with high stability associated with minimal loss in\nefficiency. Here, we develop the minimum DPD estimator (MDPDE) for independent\nbut non identically distributed observations in LMMs. We prove the theoretical\nproperties, including consistency and asymptotic normality. The influence\nfunction and sensitivity measures are studied to explore the robustness\nproperties. As a data based choice of the MDPDE tuning parameter $\\alpha$ is\nvery important, we propose two candidates as \"optimal\" choices, where\noptimality is in the sense of choosing the strongest downweighting that is\nnecessary for the particular data set. We conduct a simulation study comparing\nthe proposed MDPDE, for different values of $\\alpha$, with the S-estimators,\nM-estimators and the classical maximum likelihood estimator, considering\ndifferent levels of contamination. Finally, we illustrate the performance of\nour proposal on a real-data example.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 10:50:50 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Saraceno", "Giovanni", ""], ["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""], ["Agostinelli", "Claudio", ""]]}, {"id": "2010.05619", "submitter": "Carel F.W. Peeters", "authors": "Carel F.W. Peeters, Anders Ellern Bilgrau, Wessel N. van Wieringen", "title": "rags2ridges: A One-Stop-Shop for Graphical Modeling of High-Dimensional\n  Precision Matrices", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graphical model is an undirected network representing the conditional\nindependence properties between random variables. Graphical modeling has become\npart and parcel of systems or network approaches to multivariate data, in\nparticular when the variable dimension exceeds the observation dimension.\nrags2ridges is an R package for graphical modeling of high-dimensional\nprecision matrices. It provides a modular framework for the extraction,\nvisualization, and analysis of Gaussian graphical models from high-dimensional\ndata. Moreover, it can handle the incorporation of prior information as well as\nmultiple heterogeneous data classes. As such, it provides a one-stop-shop for\ngraphical modeling of high-dimensional precision matrices. The functionality of\nthe package is illustrated with an example dataset pertaining to blood-based\nmetabolite measurements in persons suffering from Alzheimer's Disease.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 11:43:54 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Peeters", "Carel F. W.", ""], ["Bilgrau", "Anders Ellern", ""], ["van Wieringen", "Wessel N.", ""]]}, {"id": "2010.05749", "submitter": "Jiandong Shi", "authors": "Jiandong Shi, Dehui Luo, Xiang Wan, Yue Liu, Jiming Liu, Zhaoxiang\n  Bian, Tiejun Tong", "title": "Detecting the skewness of data from the sample size and the five-number\n  summary", "comments": "36 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For clinical studies with continuous outcomes, when the data are potentially\nskewed, researchers may choose to report the whole or part of the five-number\nsummary (the sample median, the first and third quartiles, and the minimum and\nmaximum values), rather than the sample mean and standard deviation. For the\nstudies with skewed data, if we include them in the classical meta-analysis for\nnormal data, it may yield misleading or even wrong conclusions. In this paper,\nwe develop a flow chart and three new tests for detecting the skewness of data\nfrom the sample size and the five-number summary. Simulation studies\ndemonstrate that our new tests are able to control the type I error rates, and\nmeanwhile provide good statistical power. A real data example is also analyzed\nto demonstrate the usefulness of the skewness tests in meta-analysis and\nevidence-based practice.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:41:07 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Shi", "Jiandong", ""], ["Luo", "Dehui", ""], ["Wan", "Xiang", ""], ["Liu", "Yue", ""], ["Liu", "Jiming", ""], ["Bian", "Zhaoxiang", ""], ["Tong", "Tiejun", ""]]}, {"id": "2010.05774", "submitter": "Sagar Samtani", "authors": "Sagar Samtani, Hongyi Zhu, Balaji Padmanabhan, Yidong Chai, Hsinchun\n  Chen", "title": "Deep Learning for Information Systems Research", "comments": "56 pages total, 1 page title and authors, 42 pages main text, 13\n  pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) has rapidly emerged as a key disruptive\ntechnology in the 21st century. At the heart of modern AI lies Deep Learning\n(DL), an emerging class of algorithms that has enabled today's platforms and\norganizations to operate at unprecedented efficiency, effectiveness, and scale.\nDespite significant interest, IS contributions in DL have been limited, which\nwe argue is in part due to issues with defining, positioning, and conducting DL\nresearch. Recognizing the tremendous opportunity here for the IS community,\nthis work clarifies, streamlines, and presents approaches for IS scholars to\nmake timely and high-impact contributions. Related to this broader goal, this\npaper makes five timely contributions. First, we systematically summarize the\nmajor components of DL in a novel Deep Learning for Information Systems\nResearch (DL-ISR) schematic that illustrates how technical DL processes are\ndriven by key factors from an application environment. Second, we present a\nnovel Knowledge Contribution Framework (KCF) to help IS scholars position their\nDL contributions for maximum impact. Third, we provide ten guidelines to help\nIS scholars generate rigorous and relevant DL-ISR in a systematic, high-quality\nfashion. Fourth, we present a review of prevailing journal and conference\nvenues to examine how IS scholars have leveraged DL for various research\ninquiries. Finally, we provide a unique perspective on how IS scholars can\nformulate DL-ISR inquiries by carefully considering the interplay of business\nfunction(s), application areas(s), and the KCF. This perspective intentionally\nemphasizes inter-disciplinary, intra-disciplinary, and cross-IS tradition\nperspectives. Taken together, these contributions provide IS scholars a timely\nframework to advance the scale, scope, and impact of deep learning research.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 15:23:05 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Samtani", "Sagar", ""], ["Zhu", "Hongyi", ""], ["Padmanabhan", "Balaji", ""], ["Chai", "Yidong", ""], ["Chen", "Hsinchun", ""]]}, {"id": "2010.05870", "submitter": "Sigrunn Holbek S{\\o}rbye", "authors": "Sigrunn H. S{\\o}rbye, Pedro G. Nicolau and H{\\aa}vard Rue", "title": "Model-based bias correction for short AR(1) and AR(2) processes", "comments": "21 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of autoregressive (AR) processes is extensively used to model\ntemporal dependence in observed time series. Such models are easily available\nand routinely fitted using freely available statistical software like R. A\npotential caveat in analyzing short time series is that commonly applied\nestimators for the coefficients of AR processes are severely biased. This paper\nsuggests a model-based approach for bias correction of well-known estimators\nfor the coefficients of first and second-order stationary AR processes, taking\nthe sampling distribution of the original estimator into account. This is\nachieved by modeling the relationship between the true and estimated AR\ncoefficients using weighted orthogonal polynomial regression, fitted to a huge\nnumber of simulations. The finite-sample distributions of the new estimators\nare approximated using transformations of skew-normal densities and their\nproperties are demonstrated by simulations and in the analysis of a real\necological data set. The new estimators are easily available in our\naccompanying R-package ARbiascorrect for time series of length n = 10, 11, ...\n, 50, where original estimates are found using exact or conditional maximum\nlikelihood, Burg's method or the Yule-Walker equations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:18:57 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["S\u00f8rbye", "Sigrunn H.", ""], ["Nicolau", "Pedro G.", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "2010.05980", "submitter": "Adam Kapelner", "authors": "Adam Kapelner and Abba Krieger", "title": "A Matching Procedure for Sequential Experiments that Iteratively Learns\n  which Covariates Improve Power", "comments": "43 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dynamic allocation procedure that increases power and efficiency\nwhen measuring an average treatment effect in sequential randomized trials\nexploiting some subjects' previous assessed responses. Subjects arrive\nsequentially and are either randomized or paired to a previously randomized\nsubject and administered the alternate treatment. The pairing is made via a\ndynamic matching criterion that iteratively learns which specific covariates\nare important to the response. We develop estimators for the average treatment\neffect as well as an exact test. We illustrate our method's increase in\nefficiency and power over other allocation procedures in both simulated\nscenarios and a clinical trial dataset. An R package \"SeqExpMatch\" for use by\npractitioners is available.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:12:40 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 13:44:03 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kapelner", "Adam", ""], ["Krieger", "Abba", ""]]}, {"id": "2010.06063", "submitter": "Laura Kubatko", "authors": "Andrew Richards and Laura Kubatko", "title": "Bayesian Weighted Triplet and Quartet Methods for Species Tree Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference of the evolutionary histories of species, commonly represented by a\nspecies tree, is complicated by the divergent evolutionary history of different\nparts of the genome. Different loci on the genome can have different histories\nfrom the underlying species tree (and each other) due to processes such as\nincomplete lineage sorting (ILS), gene duplication and loss, and horizontal\ngene transfer. The multispecies coalescent is a commonly used model for\nperforming inference on species and gene trees in the presence of ILS. This\npaper introduces Lily-T and Lily-Q, two new methods for species tree inference\nunder the multispecies coalescent. We then compare them to two frequently used\nmethods, SVDQuartets and ASTRAL, using simulated and empirical data. Both\nmethods generally showed improvement over SVDQuartets, and Lily-Q was superior\nto Lily-T for most simulation settings. The comparison to ASTRAL was more mixed\n- Lily-Q tended to be better than ASTRAL when the length of recombination-free\nloci was short, when the coalescent population parameter {\\theta} was small, or\nwhen the internal branch lengths were longer.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 22:54:59 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Richards", "Andrew", ""], ["Kubatko", "Laura", ""]]}, {"id": "2010.06090", "submitter": "Shrabanti Chowdhury", "authors": "Saptarshi Chatterjee, Shrabanti Chowdhury and Sanjib Basu", "title": "A Model-free Approach for Testing Association", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The question of association between outcome and feature is generally framed\nin the context of a model on functional and distributional forms. Our\nmotivating application is that of identifying serum biomarkers of angiogenesis,\nenergy metabolism, apoptosis, and inflammation, predictive of recurrence after\nlung resection in node-negative non-small cell lung cancer patients with tumor\nstage T2a or less. We propose an omnibus approach for testing association that\nis free of assumptions on functional forms and distributions and can be used as\na black box method. This proposed maximal permutation test is based on the idea\nof thresholding, is readily implementable and is computationally efficient. We\nillustrate that the proposed omnibus tests maintain their levels and have\nstrong power as black box tests for detecting linear, nonlinear and\nquantile-based associations, even with outlier-prone and heavy-tailed error\ndistributions and under nonparametric setting. We additionally illustrate the\nuse of this approach in model-free feature screening and further examine the\nlevel and power of these tests for binary outcome. We compare the performance\nof the proposed omnibus tests with comparator methods in our motivating\napplication to identify preoperative serum biomarkers associated with non-small\ncell lung cancer recurrence in early stage patients.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 00:27:35 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Chatterjee", "Saptarshi", ""], ["Chowdhury", "Shrabanti", ""], ["Basu", "Sanjib", ""]]}, {"id": "2010.06103", "submitter": "Qianqian Zhu", "authors": "Hua Liu, Songhua Tan and Qianqian Zhu", "title": "Quasi-maximum Likelihood Inference for Linear Double Autoregressive\n  Models", "comments": "5 table and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the quasi-maximum likelihood inference including\nestimation, model selection and diagnostic checking for linear double\nautoregressive (DAR) models, where all asymptotic properties are established\nunder only fractional moment of the observed process. We propose a Gaussian\nquasi-maximum likelihood estimator (G-QMLE) and an exponential quasi-maximum\nlikelihood estimator (E-QMLE) for the linear DAR model, and establish the\nconsistency and asymptotic normality for both estimators. Based on the G-QMLE\nand E-QMLE, two Bayesian information criteria are proposed for model selection,\nand two mixed portmanteau tests are constructed to check the adequacy of fitted\nmodels. Moreover, we compare the proposed G-QMLE and E-QMLE with the existing\ndoubly weighted quantile regression estimator in terms of the asymptotic\nefficiency and numerical performance. Simulation studies illustrate the\nfinite-sample performance of the proposed inference tools, and a real example\non the Bitcoin return series shows the usefulness of the proposed inference\ntools.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 01:19:06 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Liu", "Hua", ""], ["Tan", "Songhua", ""], ["Zhu", "Qianqian", ""]]}, {"id": "2010.06110", "submitter": "Xuefei Guan", "authors": "Jingjing He, Xuefei Guan", "title": "Bayesian inference under small sample size -- A noninformative prior\n  approach", "comments": "22 pages, 52 (sub)figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian inference method for problems with small samples and sparse data\nis presented in this paper. A general type of prior ($\\propto 1/\\sigma^{q}$) is\nproposed to formulate the Bayesian posterior for inference problems under small\nsample size. It is shown that this type of prior can represents a broad range\nof priors such as classical noninformative priors and asymptotically locally\ninvariant priors. It is further shown in this study that such priors can be\nderived as the limiting states of Normal-Inverse-Gamma conjugate priors,\nallowing for analytical evaluations of Bayesian posteriors and predictors. The\nperformance of different noninformative priors under small sample size is\ncompared using the global likelihood. The method of Laplace approximation is\nemployed to evaluate the global likelihood. A numerical linear regression\nproblem and a realistic fatigue reliability problem are used to demonstrate the\nmethod and identify the optimal noninformative prior. Results indicate the\npredictor using Jeffreys' prior outperforms others. The advantage of the\nnoninformative Bayesian estimator over the regular least square estimator under\nsmall sample size is shown.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 01:47:06 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["He", "Jingjing", ""], ["Guan", "Xuefei", ""]]}, {"id": "2010.06147", "submitter": "Daniel Mork", "authors": "Daniel Mork, Ander Wilson", "title": "Treed distributed lag nonlinear models", "comments": "31 pages, 1 table, 4 figures", "journal-ref": null, "doi": "10.1093/biostatistics/kxaa051", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In studies of maternal exposure to air pollution a children's health outcome\nis regressed on exposures observed during pregnancy. The distributed lag\nnonlinear model (DLNM) is a statistical method commonly implemented to estimate\nan exposure-time-response function when it is postulated the exposure effect is\nnonlinear. Previous implementations of the DLNM estimate an\nexposure-time-response surface parameterized with a bivariate basis expansion.\nHowever, basis functions such as splines assume smoothness across the entire\nexposure-time-response surface, which may be unrealistic in settings where the\nexposure is associated with the outcome only in a specific time window. We\npropose a framework for estimating the DLNM based on Bayesian additive\nregression trees. Our method operates using a set of regression trees that each\nassume piecewise constant relationships across the exposure-time space. In a\nsimulation, we show that our model outperforms spline-based models when the\nexposure-time surface is not smooth, while both methods perform similarly in\nsettings where the true surface is smooth. Importantly, the proposed approach\nis lower variance and more precisely identifies critical windows during which\nexposure is associated with a future health outcome. We apply our method to\nestimate the association between maternal exposure to PM$_{2.5}$ and birth\nweight in a Colorado USA birth cohort.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 03:41:53 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 15:16:57 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 15:22:36 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Mork", "Daniel", ""], ["Wilson", "Ander", ""]]}, {"id": "2010.06370", "submitter": "Subhrajyoty Roy", "authors": "Ritwik Bhaduri, Subhrajyoty Roy and Sankar K. Pal", "title": "Rough-Fuzzy CPD: A Gradual Change Point Detection Algorithm", "comments": "25 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changepoint detection is the problem of finding abrupt or gradual changes in\ntime series data when the distribution of the time series changes\nsignificantly. There are many sophisticated statistical algorithms for solving\nchangepoint detection problem, although there is not much work devoted towards\ngradual changepoints as compared to abrupt ones. Here we present a new approach\nto solve changepoint detection problem using fuzzy rough set theory which is\nable to detect such gradual changepoints. An expression for the rough-fuzzy\nestimate of changepoints is derived along with its mathematical properties\nconcerning fast computation. In a statistical hypothesis testing framework,\nasymptotic distribution of the proposed statistic on both single and multiple\nchangepoints is derived under null hypothesis enabling multiple changepoint\ndetection. Extensive simulation studies have been performed to investigate how\nsimple crude statistical measures of disparity can be subjected to improve\ntheir efficiency in estimation of gradual changepoints. Also, the said\nrough-fuzzy estimate is robust to signal-to-noise ratio, high degree of\nfuzziness in true changepoints and also to hyper parameter values. Simulation\nstudies reveal that the proposed method beats other fuzzy methods and also\npopular crisp methods like WBS, PELT and BOCD in detecting gradual\nchangepoints. The applicability of the estimate is demonstrated using multiple\nreal-life datasets including Covid-19. We have developed the python package\n\"roufcp\" for broader dissemination of the methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 02:36:46 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Bhaduri", "Ritwik", ""], ["Roy", "Subhrajyoty", ""], ["Pal", "Sankar K.", ""]]}, {"id": "2010.06373", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti and Irene Crimaldi", "title": "Generalized Rescaled Polya urn and its statistical applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Generalized Rescaled Polya (GRP) urn, that provides a\ngenerative model for a chi-squared test of goodness of fit for the long-term\nprobabilities of clustered data, with independence between clusters and\ncorrelation, due to a reinforcement mechanism, inside each cluster. We apply\nthe proposed test to a data set of Twitter posts about COVID-19 pandemic: in a\nfew words, for a classical chi-squared test the data result strongly\nsignificant for the rejection of the null hypothesis (the daily long-run\nsentiment rate remains constant), but, taking into account the correlation\namong data, the introduced test leads to a different conclusion. Beside the\nstatistical application, we point out that the GRP urn is a simple variant of\nthe standard Eggenberger-Polya urn, that, with suitable choices of the\nparameters, shows \"local\" reinforcement, almost sure convergence of the\nempirical mean to a deterministic limit and different asymptotic behaviours of\nthe predictive mean. Moreover, the study of this model provides the opportunity\nto analyze stochastic approximation dynamics, that are unusual in the related\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:30:02 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 15:14:54 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 10:14:38 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Aletti", "Giacomo", ""], ["Crimaldi", "Irene", ""]]}, {"id": "2010.06430", "submitter": "Alexandros Rekkas", "authors": "Alexandros Rekkas, David van Klaveren, Patrick B. Ryan, Ewout W.\n  Steyerberg, David M. Kent, Peter R. Rijnbeek", "title": "A standardized framework for risk-based assessment of treatment effect\n  heterogeneity in observational healthcare databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aim: One of the aims of the Observation Health Data Sciences and Informatics\n(OHDSI) initiative is population-level treatment effect estimation in large\nobservational databases. Since treatment effects are well-known to vary across\ngroups of patients with different baseline risk, we aimed to extend the OHDSI\nmethods library with a framework for risk-based assessment of treatment effect\nheterogeneity.\n  Materials and Methods: The proposed framework consists of five steps: 1)\ndefinition of the problem, i.e. the population, the treatment, the comparator\nand the outcome(s) of interest; 2) identification of relevant databases; 3)\ndevelopment of a prediction model for the outcome(s) of interest; 4) estimation\nof propensity scores within strata of predicted risk and estimation of relative\nand absolute treatment effect within strata of predicted risk; 5) evaluation\nand presentation of results.\n  Results: We demonstrate our framework by evaluating heterogeneity of the\neffect of angiotensin-converting enzyme (ACE) inhibitors versus beta blockers\non a set of 9 outcomes of interest across three observational databases. With\nincreasing risk of acute myocardial infarction we observed increasing absolute\nbenefits, i.e. from -0.03% to 0.54% in the lowest to highest risk groups.\nCough-related absolute harms decreased from 4.1% to 2.6%.\n  Conclusions: The proposed framework may be useful for the evaluation of\nheterogeneity of treatment effect on observational data that are mapped to the\nOMOP Common Data Model. The proof of concept study demonstrates its feasibility\nin large observational data. Further insights may arise by application to\nsafety and effectiveness questions across the global data network.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 14:48:31 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Rekkas", "Alexandros", ""], ["van Klaveren", "David", ""], ["Ryan", "Patrick B.", ""], ["Steyerberg", "Ewout W.", ""], ["Kent", "David M.", ""], ["Rijnbeek", "Peter R.", ""]]}, {"id": "2010.06451", "submitter": "Ray Bai", "authors": "Ray Bai, Veronika Rockova, Edward I. George", "title": "Spike-and-Slab Meets LASSO: A Review of the Spike-and-Slab LASSO", "comments": "34 pages, 2 tables, 3 figures. Section 3.3 was added to illustrate\n  the method", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data sets have become ubiquitous in the past few decades,\noften with many more covariates than observations. In the frequentist setting,\npenalized likelihood methods are the most popular approach for variable\nselection and estimation in high-dimensional data. In the Bayesian framework,\nspike-and-slab methods are commonly used as probabilistic constructs for\nhigh-dimensional modeling. Within the context of linear regression, Rockova and\nGeorge (2018) introduced the spike-and-slab LASSO (SSL), an approach based on a\nprior which provides a continuum between the penalized likelihood LASSO and the\nBayesian point-mass spike-and-slab formulations. Since its inception, the\nspike-and-slab LASSO has been extended to a variety of contexts, including\ngeneralized linear models, factor analysis, graphical models, and nonparametric\nregression. The goal of this paper is to survey the landscape surrounding\nspike-and-slab LASSO methodology. First we elucidate the attractive properties\nand the computational tractability of SSL priors in high dimensions. We then\nreview methodological developments of the SSL and outline several theoretical\ndevelopments. We illustrate the methodology on both simulated and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:09:13 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 02:38:01 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 17:10:13 GMT"}, {"version": "v4", "created": "Sat, 8 May 2021 01:56:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Bai", "Ray", ""], ["Rockova", "Veronika", ""], ["George", "Edward I.", ""]]}, {"id": "2010.06465", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta, Karim Zouaoui-Boudjeltia, Christos Kotsalos,\n  Alexandre Rousseau, Daniel Ribeiro de Sousa, Jean-Marc Desmet, Alain Van\n  Meerhaeghe, Antonietta Mira, Bastien Chopard", "title": "Interpretable pathological test for Cardio-vascular disease: Approximate\n  Bayesian computation with distance learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardio/cerebrovascular diseases (CVD) have become one of the major health\nissue in our societies. But recent studies show that the present clinical tests\nto detect CVD are ineffectual as they do not consider different stages of\nplatelet activation or the molecular dynamics involved in platelet interactions\nand are incapable to consider inter-individual variability. Here we propose a\nstochastic platelet deposition model and an inferential scheme for uncertainty\nquantification of these parameters using Approximate Bayesian Computation and\ndistance learning. Finally we show that our methodology can learn biologically\nmeaningful parameters, which are the specific dysfunctioning parameters in each\ntype of patients, from data collected from healthy volunteers and patients.\nThis work opens up an unprecedented opportunity of personalized pathological\ntest for CVD detection and medical treatment. Also our proposed methodology can\nbe used to other fields of science where we would need machine learning tools\nto be interpretable.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:20:21 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Zouaoui-Boudjeltia", "Karim", ""], ["Kotsalos", "Christos", ""], ["Rousseau", "Alexandre", ""], ["de Sousa", "Daniel Ribeiro", ""], ["Desmet", "Jean-Marc", ""], ["Van Meerhaeghe", "Alain", ""], ["Mira", "Antonietta", ""], ["Chopard", "Bastien", ""]]}, {"id": "2010.06583", "submitter": "Philipp Frank", "authors": "Philipp Frank and Torsten A. En{\\ss}lin", "title": "Probabilistic simulation of partial differential equations", "comments": "13 pages, 4 figures, submitted to Physical Review E", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA astro-ph.IM cs.NA physics.comp-ph stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer simulations of differential equations require a time discretization,\nwhich inhibits to identify the exact solution with certainty. Probabilistic\nsimulations take this into account via uncertainty quantification. The\nconstruction of a probabilistic simulation scheme can be regarded as Bayesian\nfiltering by means of probabilistic numerics. Gaussian prior based filters,\nspecifically Gauss-Markov priors, have successfully been applied to simulation\nof ordinary differential equations (ODEs) and give rise to filtering problems\nthat can be solved efficiently. This work extends this approach to partial\ndifferential equations (PDEs) subject to periodic boundary conditions and\nutilizes continuous Gaussian processes in space and time to arrive at a\nBayesian filtering problem structurally similar to the ODE setting. The usage\nof a process that is Markov in time and statistically homogeneous in space\nleads to a probabilistic spectral simulation method that allows for an\nefficient realization. Furthermore, the Bayesian perspective allows the\nincorporation of methods developed within the context of information field\ntheory such as the estimation of the power spectrum associated with the prior\ndistribution, to be jointly estimated along with the solution of the PDE.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 10:07:51 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Frank", "Philipp", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "2010.06688", "submitter": "Youssef Anzarmou Mr", "authors": "Youssef Anzarmou, Abdallah Mkhadri and Karim Oualkacha", "title": "The Kendall Interaction Filter for Variable Interaction Screening in\n  Ultra High Dimensional Classification Problems", "comments": "44 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accounting for important interaction effects can improve prediction of many\nstatistical learning models. Identification of relevant interactions, however,\nis a challenging issue owing to their ultrahigh-dimensional nature. Interaction\nscreening strategies can alleviate such issues. However, due to heavier tail\ndistribution and complex dependence structure of interaction effects,\ninnovative robust and/or model-free methods for screening interactions are\nrequired to better scale analysis of complex and high-throughput data. In this\nwork, we develop a new model-free interaction screening method, termed Kendall\nInteraction Filter (KIF), for the classification in high-dimensional settings.\nThe KIF method suggests a weighted-sum measure, which compares the overall to\nthe within-cluster Kendall's $\\tau$ of pairs of predictors, to select\ninteractive couples of features. The proposed KIF measure captures relevant\ninteractions for the clusters response-variable, handles continuous,\ncategorical or a mixture of continuous-categorical features, and is invariant\nunder monotonic transformations. We show that the KIF measure enjoys the sure\nscreening property in the high-dimensional setting under mild conditions,\nwithout imposing sub-exponential moment assumptions on the features'\ndistributions. We illustrate the favorable behavior of the proposed methodology\ncompared to the methods in the same category using simulation studies, and we\nconduct real data analyses to demonstrate its utility.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 21:01:50 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Anzarmou", "Youssef", ""], ["Mkhadri", "Abdallah", ""], ["Oualkacha", "Karim", ""]]}, {"id": "2010.06698", "submitter": "Joshua Hunte", "authors": "Joshua Hunte, Martin Neil, Norman Fenton", "title": "Product risk assessment: a Bayesian network approach", "comments": "32 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.AI stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Product risk assessment is the overall process of determining whether a\nproduct, which could be anything from a type of washing machine to a type of\nteddy bear, is judged safe for consumers to use. There are several methods used\nfor product risk assessment, including RAPEX, which is the primary method used\nby regulators in the UK and EU. However, despite its widespread use, we\nidentify several limitations of RAPEX including a limited approach to handling\nuncertainty and the inability to incorporate causal explanations for using and\ninterpreting test data. In contrast, Bayesian Networks (BNs) are a rigorous,\nnormative method for modelling uncertainty and causality which are already used\nfor risk assessment in domains such as medicine and finance, as well as\ncritical systems generally. This article proposes a BN model that provides an\nimproved systematic method for product risk assessment that resolves the\nidentified limitations with RAPEX. We use our proposed method to demonstrate\nrisk assessments for a teddy bear and a new uncertified kettle for which there\nis no testing data and the number of product instances is unknown. We show\nthat, while we can replicate the results of the RAPEX method, the BN approach\nis more powerful and flexible.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 16:40:03 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Hunte", "Joshua", ""], ["Neil", "Martin", ""], ["Fenton", "Norman", ""]]}, {"id": "2010.06802", "submitter": "Xiaowei Zhang", "authors": "Liang Ding and Xiaowei Zhang", "title": "Sample and Computationally Efficient Simulation Metamodeling in High\n  Dimensions", "comments": "main body: 42 pages; supplemental material: 28 pages; 12 figures in\n  total", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Stochastic kriging has been widely employed for simulation metamodeling to\npredict the response surface of a complex simulation model. However, its use is\nlimited to cases where the design space is low-dimensional, because the number\nof design points required for stochastic kriging to produce accurate\nprediction, in general, grows exponentially in the dimension of the design\nspace. The large sample size results in both a prohibitive sample cost for\nrunning the simulation model and a severe computational challenge due to the\nneed of inverting large covariance matrices. Based on tensor Markov kernels and\nsparse grid experimental designs, we develop a novel methodology that\ndramatically alleviates the curse of dimensionality. We show that the sample\ncomplexity of the proposed methodology grows very mildly in the dimension, even\nunder model misspecification. We also develop fast algorithms that compute\nstochastic kriging in its exact form without any approximation schemes. We\ndemonstrate via extensive numerical experiments that our methodology can handle\nproblems with a design space of more than 10,000 dimensions, improving both\nprediction accuracy and computational efficiency by orders of magnitude\nrelative to typical alternative methods in practice.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 04:10:07 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 14:29:35 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ding", "Liang", ""], ["Zhang", "Xiaowei", ""]]}, {"id": "2010.06830", "submitter": "Span Spanbauer", "authors": "Span Spanbauer, Ian Hunter", "title": "Coarse-Grained Nonlinear System Identification", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Coarse-Grained Nonlinear Dynamics, an efficient and universal\nparameterization of nonlinear system dynamics based on the Volterra series\nexpansion. These models require a number of parameters only quasilinear in the\nsystem's memory regardless of the order at which the Volterra expansion is\ntruncated; this is a superpolynomial reduction in the number of parameters as\nthe order becomes large. This efficient parameterization is achieved by\ncoarse-graining parts of the system dynamics that depend on the product of\ntemporally distant input samples; this is conceptually similar to the\ncoarse-graining that the fast multipole method uses to achieve $\\mathcal{O}(n)$\nsimulation of n-body dynamics. Our efficient parameterization of nonlinear\ndynamics can be used for regularization, leading to Coarse-Grained Nonlinear\nSystem Identification, a technique which requires very little experimental data\nto identify accurate nonlinear dynamic models. We demonstrate the properties of\nthis approach on a simple synthetic problem. We also demonstrate this approach\nexperimentally, showing that it identifies an accurate model of the nonlinear\nvoltage to luminosity dynamics of a tungsten filament with less than a second\nof experimental data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 06:45:51 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Spanbauer", "Span", ""], ["Hunter", "Ian", ""]]}, {"id": "2010.06937", "submitter": "Martin Tveten", "authors": "Martin Tveten, Idris A. Eckley, Paul Fearnhead", "title": "Scalable changepoint and anomaly detection in cross-correlated data with\n  an application to condition monitoring", "comments": "48 pages, 25 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a condition monitoring application arising from subsea\nengineering we derive a novel, scalable approach to detecting anomalous mean\nstructure in a subset of correlated multivariate time series. Given the need to\nanalyse such series efficiently we explore a computationally efficient\napproximation of the maximum likelihood solution to the resulting modelling\nframework, and develop a new dynamic programming algorithm for solving the\nresulting Binary Quadratic Programme when the precision matrix of the time\nseries at any given time-point is banded. Through a comprehensive simulation\nstudy, we show that the resulting methods perform favourably compared to\ncompeting methods both in the anomaly and change detection settings, even when\nthe sparsity structure of the precision matrix estimate is misspecified. We\nalso demonstrate its ability to correctly detect faulty time-periods of a pump\nwithin the motivating application.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 10:37:03 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 18:49:09 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Tveten", "Martin", ""], ["Eckley", "Idris A.", ""], ["Fearnhead", "Paul", ""]]}, {"id": "2010.06941", "submitter": "Sonia Dias Mrs", "authors": "S. Dias, P. Brito and P. Amaral", "title": "Discriminant Analysis of Distributional Data viaFractional Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address classification of distributional data, where units are described\nby histogram or interval-valued variables. The proposed approach uses a linear\ndiscriminant function where distributions or intervals are represented by\nquantile functions, under specific assumptions. This discriminant function\nallows defining a score for each unit, in the form of a quantile function,\nwhich is used to classify the units in two a priori groups, using the Mallows\ndistance. There is a diversity of application areas for the proposed linear\ndiscriminant method. In this work we classify the airline companies operating\nin NY airports based on air time and arrival/departure delays, using a full\nyear fights.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 10:44:06 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Dias", "S.", ""], ["Brito", "P.", ""], ["Amaral", "P.", ""]]}, {"id": "2010.06968", "submitter": "Niels Olsen", "authors": "Niels Lundtorp Olsen", "title": "Stochastic modelling of Gaussian processes by improper linear\n  functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various approaches to stochastic processes exist, noting that key properties\nsuch as measurability and continuity are not trivially satisfied. We introduce\na new theory for Gaussian processes using improper linear functionals. Using a\ncollection of i.i.d. standard normal variables, we define Gaussian white noise\nand discuss its properties. This is extended to general Gaussian processes on\nHilbert space, where the variance is allowed to be any suitable operator. Our\nmain focus is $L^2$ spaces, and we discuss criteria for Gaussian processes to\nbe continuous in this setting. Finally, we outline a framework for statistical\ninference using the presented theory with focus on the special case of\n$L^2[0,1]$. We introduce the Fredholm determinant into the functional\nlog-likelihood. We demonstrate that the naive functional log-likelihood is not\nconsistent with the multivariate likelihood. A correction term is introduced,\nand we prove an asymptotical result.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 11:21:49 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Olsen", "Niels Lundtorp", ""]]}, {"id": "2010.06994", "submitter": "Alejandro Catalina Feli\\'u", "authors": "Alejandro Catalina, Paul-Christian B\\\"urkner, Aki Vehtari", "title": "Projection Predictive Inference for Generalized Linear and Additive\n  Multilevel Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projection predictive inference is a decision theoretic Bayesian approach\nthat decouples model estimation from decision making. Given a reference model\npreviously built including all variables present in the data, projection\npredictive inference projects its posterior onto a constrained space of a\nsubset of variables. Variable selection is then performed by sequentially\nadding relevant variables until predictive performance is satisfactory.\nPreviously, projection predictive inference has been demonstrated only for\ngeneralized linear models (GLMs) and Gaussian processes (GPs) where it showed\nsuperior performance to competing variable selection procedures. In this work,\nwe extend projection predictive inference to support variable and structure\nselection for generalized linear multilevel models (GLMMs) and generalized\nadditive multilevel models (GAMMs). Our simulative and real-word experiments\ndemonstrate that our method can drastically reduce the model complexity\nrequired to reach reference predictive performance and achieve good frequency\nproperties.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 12:14:38 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Catalina", "Alejandro", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Vehtari", "Aki", ""]]}, {"id": "2010.07038", "submitter": "David Conal Higgins", "authors": "David Higgins", "title": "OnRAMP for Regulating AI in Medical Products", "comments": "46 pages, 3 tables, 1 figure. Published in Advanced Intelligent\n  Systems, July 2021. (See DOI link)", "journal-ref": null, "doi": "10.1002/aisy.202100042", "report-no": null, "categories": "cs.CY cs.AI stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Medical Artificial Intelligence (AI) involves the application of machine\nlearning algorithms to biomedical datasets in order to improve medical\npractices. Products incorporating medical AI require certification before\ndeployment in most jurisdictions. To date, clear pathways for regulating\nmedical AI are still under development. Below the level of formal pathways lies\nthe actual practice of developing a medical AI solution. This Perspective\nproposes best practice guidelines for development compatible with the\nproduction of a regulatory package which, regardless of the formal regulatory\npath, will form a core component of a certification process. The approach is\npredicated on a statistical risk perspective, typical of medical device\nregulators, and a deep understanding of machine learning methodologies. These\nguidelines will allow all parties to communicate more clearly in the\ndevelopment of a common Good Machine Learning Practice (GMLP), and thus lead to\nthe enhanced development of both medical AI products and regulations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:02:30 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 14:52:24 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 15:47:15 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 14:41:05 GMT"}, {"version": "v5", "created": "Mon, 1 Feb 2021 14:51:39 GMT"}, {"version": "v6", "created": "Mon, 26 Jul 2021 11:51:05 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Higgins", "David", ""]]}, {"id": "2010.07064", "submitter": "Onur Teymur", "authors": "Onur Teymur, Jackson Gorham, Marina Riabiz and Chris. J. Oates", "title": "Optimal quantisation of probability measures using maximum mean\n  discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several researchers have proposed minimisation of maximum mean discrepancy\n(MMD) as a method to quantise probability measures, i.e., to approximate a\ntarget distribution by a representative point set. We consider sequential\nalgorithms that greedily minimise MMD over a discrete candidate set. We propose\na novel non-myopic algorithm and, in order to both improve statistical\nefficiency and reduce computational cost, we investigate a variant that applies\nthis technique to a mini-batch of the candidate set at each iteration. When the\ncandidate points are sampled from the target, the consistency of these new\nalgorithm - and their mini-batch variants - is established. We demonstrate the\nalgorithms on a range of important computational problems, including\noptimisation of nodes in Bayesian cubature and the thinning of Markov chain\noutput.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:09:48 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 16:03:37 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 09:43:56 GMT"}, {"version": "v4", "created": "Fri, 12 Feb 2021 11:40:18 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Teymur", "Onur", ""], ["Gorham", "Jackson", ""], ["Riabiz", "Marina", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2010.07065", "submitter": "Rasmus Erlemann", "authors": "Bo Henry Lindqvist, Rasmus Erlemann, Gunnar Taraldsen", "title": "Conditional Monte Carlo revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Monte Carlo refers to sampling from the conditional distribution\nof a random vector X given the value T(X) = t for a function T(X). Classical\nconditional Monte Carlo methods were designed for estimating conditional\nexpectations of functions of X by sampling from unconditional distributions\nobtained by certain weighting schemes. The basic ingredients were the use of\nimportance sampling and change of variables. In the present paper we\nreformulate the problem by introducing an artificial parametric model,\nrepresenting the conditional distribution of X given T(X)=t within this new\nmodel. The key is to provide the parameter of the artificial model by a\ndistribution. The approach is illustrated by several examples, which are\nparticularly chosen to illustrate conditional sampling in cases where such\nsampling is not straightforward. A simulation study and an application to\ngoodness-of-fit testing of real data are also given.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:11:17 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Lindqvist", "Bo Henry", ""], ["Erlemann", "Rasmus", ""], ["Taraldsen", "Gunnar", ""]]}, {"id": "2010.07147", "submitter": "Jing Lei", "authors": "Xiaoyu Hu and Jing Lei", "title": "A Distribution-Free Test of Covariate Shift Using Conformal Prediction", "comments": "43 pages, 2 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariate shift is a common and important assumption in transfer learning and\ndomain adaptation to treat the distributional difference between the training\nand testing data. We propose a nonparametric test of covariate shift using the\nconformal prediction framework. The construction of our test statistic combines\nrecent developments in conformal prediction with a novel choice of conformity\nscore, resulting in a valid and powerful test statistic under very general\nsettings. To our knowledge, this is the first successful attempt of using\nconformal prediction for testing statistical hypotheses. Our method is suitable\nfor modern machine learning scenarios where the data has high dimensionality\nand large sample sizes, and can be effectively combined with existing\nclassification algorithms to find good conformity score functions. The\nperformance of the proposed method is demonstrated in synthetic and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 15:03:29 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Hu", "Xiaoyu", ""], ["Lei", "Jing", ""]]}, {"id": "2010.07164", "submitter": "Soraia Pereira", "authors": "Miguel de Carvalho, Soraia Pereira, Paula Pereira, Patr\\'icia de Zea\n  Bermudez", "title": "An Extreme Value Bayesian Lasso for the Conditional Bulk and Tail", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel regression model for the conditional bulk and\nconditional tail of a possibly heavy-tailed response. The proposed model can be\nused to learn the effect of covariates on an extreme value setting via a\nLasso-type specification based on a Lagrangian restriction. Our model can be\nused to track if some covariates are significant for the bulk, but not for the\ntail---and vice-versa; in addition to this, the proposed model bypasses the\nneed for conditional threshold selection in an extreme value theory framework.\nWe assess the finite-sample performance of the proposed methods through a\nsimulation study that reveals that our method recovers the true conditional\ndistribution over a variety of simulation scenarios, along with being accurate\non variable selection. Rainfall data are used to showcase how the proposed\nmethod can learn to distinguish between key drivers of moderate rainfall,\nagainst those of extreme rainfall.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 15:36:36 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["de Carvalho", "Miguel", ""], ["Pereira", "Soraia", ""], ["Pereira", "Paula", ""], ["Bermudez", "Patr\u00edcia de Zea", ""]]}, {"id": "2010.07204", "submitter": "Soutrik Mandal", "authors": "Soutrik Mandal, Jing Qin and Ruth M. Pfeiffer", "title": "Incorporating survival data into case-control studies with incident and\n  prevalent cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, case-control studies to estimate odds-ratios associating risk\nfactors with disease incidence from logistic regression only include cases with\nnewly diagnosed disease. Recently proposed methods allow incorporating\ninformation on prevalent cases, individuals who survived from disease diagnosis\nto sampling, into cross-sectionally sampled case-control studies under\nparametric assumptions for the survival time after diagnosis. Here we propose\nand study methods to additionally use prospectively observed survival times\nfrom prevalent and incident cases to adjust logistic models for the time\nbetween disease diagnosis and sampling, the backward time, for prevalent cases.\nThis adjustment yields unbiased odds-ratio estimates from case-control studies\nthat include prevalent cases. We propose a computationally simple two-step\ngeneralized method-of-moments estimation procedure. First, we estimate the\nsurvival distribution based on a semi-parametric Cox model using an\nexpectation-maximization algorithm that yields fully efficient estimates and\naccommodates left truncation for the prevalent cases and right censoring. Then,\nwe use the estimated survival distribution in an extension of the logistic\nmodel to three groups (controls, incident and prevalent cases), to accommodate\nthe survival bias in prevalent cases. In simulations, when the amount of\ncensoring was modest, odds-ratios from the two-step procedure were equally\nefficient as those estimated by jointly optimizing the logistic and survival\ndata likelihoods under parametric assumptions. Even with 90% censoring they\nwere as efficient as estimates obtained using only cross-sectionally available\ninformation under parametric assumptions. This indicates that utilizing\nprospective survival data from the cases lessens model dependency and improves\nprecision of association estimates for case-control studies with prevalent\ncases.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:18:18 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 05:38:39 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Mandal", "Soutrik", ""], ["Qin", "Jing", ""], ["Pfeiffer", "Ruth M.", ""]]}, {"id": "2010.07242", "submitter": "Nan Wu", "authors": "David B Dunson, Hau-Tieng Wu and Nan Wu", "title": "Diffusion Based Gaussian Processes on Restricted Domains", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nonparametric regression and spatial process modeling, it is common for\nthe inputs to fall in a restricted subset of Euclidean space. For example, the\nlocations at which spatial data are collected may be restricted to a narrow\nnon-linear subset, such as near the edge of a lake. Typical kernel-based\nmethods that do not take into account the intrinsic geometric of the domain\nacross which observations are collected may produce sub-optimal results. In\nthis article, we focus on solving this problem in the context of Gaussian\nprocess (GP) models, proposing a new class of diffusion-based GPs (DB-GPs),\nwhich learn a covariance that respects the geometry of the input domain. We use\nthe term `diffusion-based' as the idea is to measure intrinsic distances\nbetween inputs in a restricted domain via a diffusion process. As the heat\nkernel is intractable computationally, we approximate the covariance using\nfinitely-many eigenpairs of the Graph Laplacian (GL). Our proposed algorithm\nhas the same order of computational complexity as current GP algorithms using\nsimple covariance kernels. We provide substantial theoretical support for the\nDB-GP methodology, and illustrate performance gains through toy examples,\nsimulation studies, and applications to ecology data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 17:01:29 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Dunson", "David B", ""], ["Wu", "Hau-Tieng", ""], ["Wu", "Nan", ""]]}, {"id": "2010.07348", "submitter": "Adam Peterson", "authors": "Adam Peterson, Veronica Berrocal, Emma Sanchez-Vaznaugh, Brisa Sanchez", "title": "How Close and How Much? Linking Health Outcomes to Built Environment\n  Spatial Distributions", "comments": "23 pages manuscript with 5 page supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Built environment features (BEFs) refer to aspects of the human constructed\nenvironment, which may in turn support or restrict health related behaviors and\nthus impact health. In this paper we are interested in understanding whether\nthe spatial distribution and quantity of fast food restaurants (FFRs) influence\nthe risk of obesity in schoolchildren. To achieve this goal, we propose a\ntwo-stage Bayesian hierarchical modeling framework. In the first stage,\nexamining the position of FFRs relative to that of some reference locations -\nin our case, schools - we model the distances of FFRs from these reference\nlocations as realizations of Inhomogenous Poisson processes (IPP). With the\ngoal of identifying representative spatial patterns of exposure to FFRs, we\nmodel the intensity functions of the IPPs using a Bayesian non-parametric\nviewpoint and specifying a Nested Dirichlet Process prior. The second stage\nmodel relates exposure patterns to obesity, offering two different approaches\nto accommodate uncertainty in the exposure patterns estimated in the first\nstage: in the first approach the odds of obesity at the school level is\nregressed on cluster indicators, each representing a major pattern of exposure\nto FFRs. In the second, we employ Bayesian Kernel Machine regression to relate\nthe odds of obesity to the multivariate vector reporting the degree of\nsimilarity of a given school to all other schools. Our analysis on the\ninfluence of patterns of FFR occurrence on obesity among Californian\nschoolchildren has indicated that, in 2010, among schools that are consistently\nassigned to a cluster, there is a lower odds of obesity amongst 9th graders who\nattend schools with most distant FFR occurrences in a 1-mile radius as compared\nto others.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 18:29:54 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Peterson", "Adam", ""], ["Berrocal", "Veronica", ""], ["Sanchez-Vaznaugh", "Emma", ""], ["Sanchez", "Brisa", ""]]}, {"id": "2010.07462", "submitter": "Hee-Seok Oh", "authors": "Wookyeong Song and Hee-Seok Oh and Yaeji Lim and Ying Kuen Cheung", "title": "Multi-feature Clustering of Step Data using Multivariate Functional\n  Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new statistical method for clustering step data, a\npopular form of health record data easily obtained from wearable devices. Since\nstep data are high-dimensional and zero-inflated, classical methods such as\nK-means and partitioning around medoid (PAM) cannot be applied directly. The\nproposed method is a novel combination of newly constructed variables that\nreflect the inherent features of step data, such as quantity, strength, and\npattern, and a multivariate functional principal component analysis that can\nintegrate all the features of the step data for clustering. The proposed method\nis implemented by applying a conventional clustering method such as K-means and\nPAM to the multivariate functional principal component scores obtained from\nthese variables. Simulation studies and real data analysis demonstrate\nsignificant improvement in clustering quality.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 01:17:55 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Song", "Wookyeong", ""], ["Oh", "Hee-Seok", ""], ["Lim", "Yaeji", ""], ["Cheung", "Ying Kuen", ""]]}, {"id": "2010.07465", "submitter": "David  Nott", "authors": "Yinan Mao, Xueou Wang, David J. Nott and Michael Evans", "title": "Detecting conflicting summary statistics in likelihood-free inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian likelihood-free methods implement Bayesian inference using\nsimulation of data from the model to substitute for intractable likelihood\nevaluations. Most likelihood-free inference methods replace the full data set\nwith a summary statistic before performing Bayesian inference, and the choice\nof this statistic is often difficult. The summary statistic should be\nlow-dimensional for computational reasons, while retaining as much information\nas possible about the parameter. Using a recent idea from the interpretable\nmachine learning literature, we develop some regression-based diagnostic\nmethods which are useful for detecting when different parts of a summary\nstatistic vector contain conflicting information about the model parameters.\nConflicts of this kind complicate summary statistic choice, and detecting them\ncan be insightful about model deficiencies and guide model improvement. The\ndiagnostic methods developed are based on regression approaches to\nlikelihood-free inference, in which the regression model estimates the\nposterior density using summary statistics as features. Deletion and imputation\nof part of the summary statistic vector within the regression model can remove\nconflicts and approximate posterior distributions for summary statistic\nsubsets. A larger than expected change in the estimated posterior density\nfollowing deletion and imputation can indicate a conflict in which inferences\nof interest are affected. The usefulness of the new methods is demonstrated in\na number of real examples.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 01:30:33 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Mao", "Yinan", ""], ["Wang", "Xueou", ""], ["Nott", "David J.", ""], ["Evans", "Michael", ""]]}, {"id": "2010.07594", "submitter": "Xiaohan Yan", "authors": "William B. Nicholson, Xiaohan Yan", "title": "An Improved Online Penalty Parameter Selection Procedure for\n  $\\ell_1$-Penalized Autoregressive with Exogenous Variables", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent developments in the high-dimensional statistical time series\nliterature have centered around time-dependent applications that can be adapted\nto regularized least squares. Of particular interest is the lasso, which both\nserves to regularize and provide feature selection. The lasso requires the\nspecification of a penalty parameter that determines the degree of sparsity to\nimpose. The most popular penalty parameter selection approaches that respect\ntime dependence are very computationally intensive and are not appropriate for\nmodeling certain classes of time series. We propose enhancing a canonical time\nseries model, the autoregressive model with exogenous variables, with a novel\nonline penalty parameter selection procedure that takes advantage of the\nsequential nature of time series data to improve both computational performance\nand forecast accuracy relative to existing methods in both a simulation and\nempirical application involving macroeconomic indicators.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 08:32:27 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Nicholson", "William B.", ""], ["Yan", "Xiaohan", ""]]}, {"id": "2010.07604", "submitter": "Dongjun Kim", "authors": "Dongjun Kim, Kyungwoo Song, YoonYeong Kim, Yongjin Shin, Wanmo Kang,\n  Il-Chul Moon", "title": "Sequential Likelihood-Free Inference with Implicit Surrogate Proposal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference without the access of likelihood, or likelihood-free\ninference, has been a key research topic in simulations, to yield a more\nrealistic generation result. Recent likelihood-free inference updates an\napproximate posterior sequentially with the dataset of the cumulative\nsimulation input-output pairs over inference rounds. Therefore, the dataset is\ngathered through the iterative simulations with sampled inputs from a proposal\ndistribution by MCMC, which becomes the key of inference quality in this\nsequential framework. This paper introduces a new proposal modeling, named as\nImplicit Surrogate Proposal (ISP), to generate a cumulated dataset with further\nsample efficiency. ISP constructs the cumulative dataset in the most diverse\nway by drawing i.i.d samples via a feed-forward fashion, so the posterior\ninference does not suffer from the disadvantages of MCMC caused by its\nnon-i.i.d nature, such as auto-correlation and slow mixing. We analyze the\nconvergence property of ISP in both theoretical and empirical aspects to\nguarantee that ISP provides an asymptotically exact sampler. We demonstrate\nthat ISP outperforms the baseline inference algorithms on simulations with\nmulti-modal posteriors.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 08:59:23 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 16:55:23 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kim", "Dongjun", ""], ["Song", "Kyungwoo", ""], ["Kim", "YoonYeong", ""], ["Shin", "Yongjin", ""], ["Kang", "Wanmo", ""], ["Moon", "Il-Chul", ""]]}, {"id": "2010.07656", "submitter": "Sukjin (Vincent) Han", "authors": "Sukjin (Vincent) Han", "title": "Comment: Individualized Treatment Rules Under Endogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note discusses two recent studies on identification of individualized\ntreatment rules using instrumental variables---Cui and Tchetgen Tchetgen (2020)\nand Qiu et al. (2020). It also proposes identifying assumptions that are\nalternative to what is used in both studies.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 10:46:22 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Sukjin", "", "", "Vincent"], ["Han", "", ""]]}, {"id": "2010.07695", "submitter": "Michele Santacatterina", "authors": "Michele Santacatterina", "title": "Robust weights that optimally balance confounders for estimating\n  marginal hazard ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariate balance is crucial in obtaining unbiased estimates of treatment\neffects in observational studies. Methods based on inverse probability weights\nhave been widely used to estimate treatment effects with observational data.\nMachine learning techniques have been proposed to estimate propensity scores.\nThese techniques however target accuracy instead of covariate balance. Methods\nthat target covariate balance have been successfully proposed and largely\napplied to estimate treatment effects on continuous outcomes. However, in many\nmedical and epidemiological applications, the interest lies in estimating\ntreatment effects on time-to-event outcomes. With this type of data, one of the\nmost common estimands of interest is the marginal hazard ratio of the Cox\nproportional hazard model. In this paper, we start by presenting robust\northogonality weights (ROW), a set of weights obtained by solving a quadratic\nconstrained optimization problem that maximizes precision while constraining\ncovariate balance defined as the sample correlation between confounders and\ntreatment. By doing so, ROW optimally deal with both binary and continuous\ntreatments. We then evaluate the performance of the proposed weights in\nestimating marginal hazard ratios of binary and continuous treatments with\ntime-to-event outcomes in a simulation study. We finally apply ROW on the\nevaluation of the effect of hormone therapy on time to coronary heart disease\nand on the effect of red meat consumption on time to colon cancer among 24,069\npostmenopausal women enrolled in the Women's Health Initiative observational\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 12:12:45 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 18:34:49 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Santacatterina", "Michele", ""]]}, {"id": "2010.07813", "submitter": "Fintan Costello", "authors": "Fintan Costello and Paul Watts", "title": "Distributional Null Hypothesis Testing with the T distribution", "comments": "Under review, The American Statistician (submitted April 15, 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Null Hypothesis Significance Testing (NHST) has long been central to the\nscientific project, guiding theory development and supporting evidence-based\nintervention and decision-making. Recent years, however, have seen growing\nawareness of serious problems with NHST as it is typically used, and hence to\nproposals to limit the use of NHST techniques, to abandon these techniques and\nmove to alternative statistical approaches, or even to ban the use of NHST\nentirely. These proposals are premature, because the observed problems with\nNHST all arise as a consequence of a contingent and in many cases incorrect\nchoice: that of NHST testing against point-form nulls. We show that testing\nagainst distributional, rather than point-form, nulls is better motivated\nmathematically and experimentally, and that the use of distributional nulls\naddresses many problems with the standard point-form NHST approach. We also\nshow that use of distributional nulls allows a form of null hypothesis testing\nthat takes into account both the statistical significance of a given result and\nthe probability of replication of that result in a new experiment. Rather than\nabandoning NHST, we should use the NHST approach in its more general form, with\ndistributional rather than point-form nulls.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 15:04:57 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Costello", "Fintan", ""], ["Watts", "Paul", ""]]}, {"id": "2010.08082", "submitter": "Jaehyeok Shin", "authors": "Jaehyeok Shin, Aaditya Ramdas, Alessandro Rinaldo", "title": "Nonparametric iterated-logarithm extensions of the sequential\n  generalized likelihood ratio test", "comments": "53 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a nonparametric extension of the sequential generalized likelihood\nratio (GLR) test and corresponding time-uniform confidence sequences for the\nmean of a univariate distribution. By utilizing a geometric interpretation of\nthe GLR statistic, we derive a simple analytic upper bound on the probability\nthat it exceeds any prespecified boundary; these are intractable to approximate\nvia simulations due to infinite horizon of the tests and the composite\nnonparametric nulls under consideration. Using time-uniform boundary-crossing\ninequalities, we carry out a unified nonasymptotic analysis of expected sample\nsizes of one-sided and open-ended tests over nonparametric classes of\ndistributions (including sub-Gaussian, sub-exponential, sub-gamma, and\nexponential families). Finally, we present a flexible and practical method to\nconstruct time-uniform confidence sequences that are easily tunable to be\nuniformly close to the pointwise Chernoff bound over any target time interval.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 00:58:58 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 06:18:22 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 02:34:18 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 03:13:34 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Shin", "Jaehyeok", ""], ["Ramdas", "Aaditya", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "2010.08102", "submitter": "Paul Raschky", "authors": "Klaus Ackermann, Simon D. Angus, Paul A. Raschky", "title": "Estimating Sleep & Work Hours from Alternative Data by Segmented\n  Functional Classification Analysis (SFCA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternative data is increasingly adapted to predict human and economic\nbehaviour. This paper introduces a new type of alternative data by\nre-conceptualising the internet as a data-driven insights platform at global\nscale. Using data from a unique internet activity and location dataset drawn\nfrom over 1.5 trillion observations of end-user internet connections, we\nconstruct a functional dataset covering over 1,600 cities during a 7 year\nperiod with temporal resolution of just 15min. To predict accurate temporal\npatterns of sleep and work activity from this data-set, we develop a new\ntechnique, Segmented Functional Classification Analysis (SFCA), and compare its\nperformance to a wide array of linear, functional, and classification methods.\nTo confirm the wider applicability of SFCA, in a second application we predict\nsleep and work activity using SFCA from US city-wide electricity demand\nfunctional data. Across both problems, SFCA is shown to out-perform current\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 02:13:14 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Ackermann", "Klaus", ""], ["Angus", "Simon D.", ""], ["Raschky", "Paul A.", ""]]}, {"id": "2010.08134", "submitter": "Aditya Mishra", "authors": "Aditya Mishra, Dipak K. Dey, Yong Chen, Kun Chen", "title": "Generalized Co-sparse Factor Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate regression techniques are commonly applied to explore the\nassociations between large numbers of outcomes and predictors. In real-world\napplications, the outcomes are often of mixed types, including continuous\nmeasurements, binary indicators, and counts, and the observations may also be\nincomplete. Building upon the recent advances in mixed-outcome modeling and\nsparse matrix factorization, generalized co-sparse factor regression (GOFAR) is\nproposed, which utilizes the flexible vector generalized linear model framework\nand encodes the outcome dependency through a sparse singular value\ndecomposition (SSVD) of the integrated natural parameter matrix. To avoid the\nestimation of the notoriously difficult joint SSVD, GOFAR proposes both\nsequential and parallel unit-rank estimation procedures. By combining the ideas\nof alternating convex search and majorization-minimization, an efficient\nalgorithm with guaranteed convergence is developed to solve the sparse\nunit-rank problem and implemented in the R package gofar. Extensive simulation\nstudies and two real-world applications demonstrate the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 03:39:16 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Mishra", "Aditya", ""], ["Dey", "Dipak K.", ""], ["Chen", "Yong", ""], ["Chen", "Kun", ""]]}, {"id": "2010.08152", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "A multinomial truncated D-vine copula mixed model for the joint\n  meta-analysis of multiple diagnostic tests", "comments": "arXiv admin note: text overlap with arXiv:2006.09278", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an extensive literature on methods for meta-analysis of diagnostic\nstudies, but it mainly focuses on a single test. However, the better\nunderstanding of a particular disease has led to the development of multiple\ntests. A multinomial generalized linear mixed model (GLMM) is recently proposed\nfor the joint meta-analysis of studies comparing multiple tests. We propose a\nnovel model for the joint meta-analysis of multiple tests, which assumes\nindependent multinomial distributions for the counts of each combination of\ntest results in diseased and non-diseased patients, conditional on the latent\nvector of probabilities of each combination of test results in diseased and\nnon-diseased patients. For the random effects distribution of the latent\nproportions, we employ a truncated drawable vine copula that can cover flexible\ndependence structures. The proposed model includes the multinomial GLMM as a\nspecial case, but can also operate on the original scale of the latent\nproportions. Our methodology is demonstrated with a simulation study and using\na meta-analysis of screening for Down syndrome with two tests: shortened\nhumerus and shortened femur. The comparison of our method with the multinomial\nGLMM yields findings in the real data meta-analysis that change the current\nconclusions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 15:19:18 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "2010.08444", "submitter": "Giovanni Saraceno", "authors": "Giovanni Saraceno and Claudio Agostinelli and Luca Greco", "title": "Robust Estimation for Multivariate Wrapped Models", "comments": "18 pages, 4 figures. METRON (2021)", "journal-ref": "Saraceno, G., Agostinelli, C. & Greco, L. Robust estimation for\n  multivariate wrapped models. METRON (2021)", "doi": "10.1007/s40300-021-00214-9", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weighted likelihood technique for robust estimation of a multivariate\nWrapped Normal distribution for data points scattered on a p-dimensional torus\nis proposed. The occurrence of outliers in the sample at hand can badly\ncompromise inference for standard techniques such as maximum likelihood method.\nTherefore, there is the need to handle such model inadequacies in the fitting\nprocess by a robust technique and an effective down-weighting of observations\nnot following the assumed model. Furthermore, the employ of a robust method\ncould help in situations of hidden and unexpected substructures in the data.\nHere, it is suggested to build a set of data-dependent weights based on the\nPearson residuals and solve the corresponding weighted likelihood estimating\nequations. In particular, robust estimation is carried out by using a\nClassification EM algorithm whose M-step is enhanced by the computation of\nweights based on current parameters' values. The finite sample behavior of the\nproposed method has been investigated by a Monte Carlo numerical studies and\nreal data examples.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 15:15:48 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Saraceno", "Giovanni", ""], ["Agostinelli", "Claudio", ""], ["Greco", "Luca", ""]]}, {"id": "2010.08463", "submitter": "Andrii Babii", "authors": "Andrii Babii and Xi Chen and Eric Ghysels and Rohit Kumar", "title": "Binary Choice with Asymmetric Loss in a Data-Rich Environment: Theory\n  and an Application to Racial Justice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of asymmetries in prediction problems arising in economics has\nbeen recognized for a long time. In this paper, we focus on binary choice\nproblems in a data-rich environment with general loss functions. In contrast to\nthe asymmetric regression problems, the binary choice with general loss\nfunctions and high-dimensional datasets is challenging and not well understood.\nEconometricians have studied binary choice problems for a long time, but the\nliterature does not offer computationally attractive solutions in data-rich\nenvironments. In contrast, the machine learning literature has many\ncomputationally attractive algorithms that form the basis for much of the\nautomated procedures that are implemented in practice, but it is focused on\nsymmetric loss functions that are independent of individual characteristics.\nOne of the main contributions of our paper is to show that the theoretically\nvalid predictions of binary outcomes with arbitrary loss functions can be\nachieved via a very simple reweighting of the logistic regression, or other\nstate-of-the-art machine learning techniques, such as boosting or (deep) neural\nnetworks. We apply our analysis to racial justice in pretrial detention.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 16:01:20 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 23:14:42 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 18:32:22 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Babii", "Andrii", ""], ["Chen", "Xi", ""], ["Ghysels", "Eric", ""], ["Kumar", "Rohit", ""]]}, {"id": "2010.08495", "submitter": "Fan Yin", "authors": "Fan Yin, Guanyu Hu, Weining Shen", "title": "Analysis of professional basketball field goal attempts via a Bayesian\n  matrix clustering approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric matrix clustering approach to analyze the\nlatent heterogeneity structure in the shot selection data collected from\nprofessional basketball players in the National Basketball Association (NBA).\nThe proposed method adopts a mixture of finite mixtures framework and fully\nutilizes the spatial information via a mixture of matrix normal distribution\nrepresentation. We propose an efficient Markov chain Monte Carlo algorithm for\nposterior sampling that allows simultaneous inference on both the number of\nclusters and the cluster configurations. We also establish large sample\nconvergence properties for the posterior distribution. The excellent empirical\nperformance of the proposed method is demonstrated via simulation studies and\nan application to shot chart data from selected players in the 2017 18 NBA\nregular season.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 16:53:09 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Yin", "Fan", ""], ["Hu", "Guanyu", ""], ["Shen", "Weining", ""]]}, {"id": "2010.08573", "submitter": "John O'Leary", "authors": "John O'Leary, Guanyang Wang, Pierre E. Jacob", "title": "Maximal couplings of the Metropolis-Hastings algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Couplings play a central role in the analysis of Markov chain Monte Carlo\nalgorithms and appear increasingly often in the algorithms themselves, e.g. in\nconvergence diagnostics, parallelization, and variance reduction techniques.\nExisting couplings of the Metropolis-Hastings algorithm handle the proposal and\nacceptance steps separately and fall short of the upper bound on one-step\nmeeting probabilities given by the coupling inequality. This paper introduces\nmaximal couplings which achieve this bound while retaining the practical\nadvantages of current methods. We consider the properties of these couplings\nand examine their behavior on a selection of numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 18:12:05 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["O'Leary", "John", ""], ["Wang", "Guanyang", ""], ["Jacob", "Pierre E.", ""]]}, {"id": "2010.08611", "submitter": "F. Richard Guo", "authors": "F. Richard Guo, Emilija Perkovi\\'c", "title": "Minimal enumeration of all possible total effects in a Markov\n  equivalence class", "comments": "Corrected Figure 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, when a total causal effect of interest is not\nidentified, the set of all possible effects can be reported instead. This\ntypically occurs when the underlying causal DAG is only known up to a Markov\nequivalence class, or a refinement thereof due to background knowledge. As\nsuch, the class of possible causal DAGs is represented by a maximally oriented\npartially directed acyclic graph (MPDAG), which contains both directed and\nundirected edges. We characterize the minimal additional edge orientations\nrequired to identify a given total effect. A recursive algorithm is then\ndeveloped to enumerate subclasses of DAGs, such that the total effect in each\nsubclass is identified as a distinct functional of the observed distribution.\nThis resolves an issue with existing methods, which often report possible total\neffects with duplicates, namely those that are numerically distinct due to\nsampling variability but are in fact causally identical.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 20:07:20 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 05:49:07 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 00:45:59 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Guo", "F. Richard", ""], ["Perkovi\u0107", "Emilija", ""]]}, {"id": "2010.08627", "submitter": "Qiuyun Zhu", "authors": "Qiuyun Zhu, Yves Atchade", "title": "Minimax Quasi-Bayesian estimation in sparse canonical correlation\n  analysis via a Rayleigh quotient function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a popular statistical technique for\nexploring the relationship between datasets. The estimation of sparse canonical\ncorrelation vectors has emerged in recent years as an important but challenging\nvariation of the CCA problem, with widespread applications. Currently available\nrate-optimal estimators for sparse canonical correlation vectors are expensive\nto compute. We propose a quasi-Bayesian estimation procedure that achieves the\nminimax estimation rate, and yet is easy to compute by Markov Chain Monte Carlo\n(MCMC). The method builds on ([37]) and uses a re-scaled Rayleigh quotient\nfunction as a quasi-log-likelihood. However unlike these authors, we adopt a\nBayesian framework that combines this quasi-log-likelihood with a\nspike-and-slab prior that serves to regularize the inference and promote\nsparsity. We investigated the empirical behavior of the proposed method on both\ncontinuous and truncated data, and we noted that it outperforms several\nstate-of-the-art methods. As an application, we use the methodology to\nmaximally correlate clinical variables and proteomic data for a better\nunderstanding of covid-19 disease.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 21:00:57 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 16:52:56 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Zhu", "Qiuyun", ""], ["Atchade", "Yves", ""]]}, {"id": "2010.08673", "submitter": "Xin Zhang", "authors": "Ian W. McKeague and Xin Zhang", "title": "Significance testing for canonical correlation analysis in high\n  dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing for the presence of linear relationships\nbetween large sets of random variables based on a post-selection inference\napproach to canonical correlation analysis. The challenge is to adjust for the\nselection of subsets of variables having linear combinations with maximal\nsample correlation. To this end, we construct a stabilized one-step estimator\nof the euclidean-norm of the canonical correlations maximized over subsets of\nvariables of pre-specified cardinality. This estimator is shown to be\nconsistent for its target parameter and asymptotically normal provided the\ndimensions of the variables do not grow too quickly with sample size. We also\ndevelop a greedy search algorithm to accurately compute the estimator, leading\nto a computationally tractable omnibus test for the global null hypothesis that\nthere are no linear relationships between any subsets of variables having the\npre-specified cardinality. Further, we develop a confidence interval for the\ntarget parameter that takes the variable selection into account.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 00:06:12 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["McKeague", "Ian W.", ""], ["Zhang", "Xin", ""]]}, {"id": "2010.08676", "submitter": "Anar Amgalan", "authors": "Anar Amgalan, Lilianne R. Mujica-Parodi, Steven S. Skiena", "title": "Fast Spatial Autocorrelation", "comments": "To be published in ICDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical or geographic location proves to be an important feature in many\ndata science models, because many diverse natural and social phenomenon have a\nspatial component. Spatial autocorrelation measures the extent to which locally\nadjacent observations of the same phenomenon are correlated. Although\nstatistics like Moran's $I$ and Geary's $C$ are widely used to measure spatial\nautocorrelation, they are slow: all popular methods run in $\\Omega(n^2)$ time,\nrendering them unusable for large data sets, or long time-courses with moderate\nnumbers of points. We propose a new $S_A$ statistic based on the notion that\nthe variance observed when merging pairs of nearby clusters should increase\nslowly for spatially autocorrelated variables. We give a linear-time algorithm\nto calculate $S_A$ for a variable with an input agglomeration order (available\nat https://github.com/aamgalan/spatial_autocorrelation). For a typical dataset\nof $n \\approx 63,000$ points, our $S_A$ autocorrelation measure can be computed\nin 1 second, versus 2 hours or more for Moran's $I$ and Geary's $C$. Through\nsimulation studies, we demonstrate that $S_A$ identifies spatial correlations\nin variables generated with spatially-dependent model half an order of\nmagnitude earlier than either Moran's $I$ or Geary's $C$. Finally, we prove\nseveral theoretical properties of $S_A$: namely that it behaves as a true\ncorrelation statistic, and is invariant under addition or multiplication by a\nconstant.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 00:24:40 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Amgalan", "Anar", ""], ["Mujica-Parodi", "Lilianne R.", ""], ["Skiena", "Steven S.", ""]]}, {"id": "2010.08703", "submitter": "Aaron Hudson", "authors": "Aaron Hudson and Ali Shojaie", "title": "Statistical Inference for Qualitative Interactions with Applications to\n  Precision Medicine and Differential Network Analysis", "comments": "32 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Qualitative interactions occur when a treatment effect or measure of\nassociation varies in sign by sub-population. Of particular interest in many\nbiomedical settings are absence/presence qualitative interactions, which occur\nwhen an effect is present in one sub-population but absent in another.\nAbsence/presence interactions arise in emerging applications in precision\nmedicine, where the objective is to identify a set of predictive biomarkers\nthat have prognostic value for clinical outcomes in some sub-population but not\nothers. They also arise naturally in gene regulatory network inference, where\nthe goal is to identify differences in networks corresponding to diseased and\nhealthy individuals, or to different subtypes of disease; such differences lead\nto identification of network-based biomarkers for diseases. In this paper, we\nargue that while the absence/presence hypothesis is important, developing a\nstatistical test for this hypothesis is an intractable problem. To overcome\nthis challenge, we approximate the problem in a novel inference framework. In\nparticular, we propose to make inferences about absence/presence interactions\nby quantifying the relative difference in effect size, reasoning that when the\nrelative difference is large, an absence/presence interaction occurs. The\nproposed methodology is illustrated through a simulation study as well as an\nanalysis of breast cancer data from the Cancer Genome Atlas.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 03:22:44 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Hudson", "Aaron", ""], ["Shojaie", "Ali", ""]]}, {"id": "2010.08704", "submitter": "Aaron Hudson", "authors": "Aaron Hudson and Ali Shojaie", "title": "Covariate-Adjusted Inference for Differential Analysis of\n  High-Dimensional Networks", "comments": "42 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differences between biological networks corresponding to disease conditions\ncan help delineate the underlying disease mechanisms. Existing methods for\ndifferential network analysis do not account for dependence of networks on\ncovariates. As a result, these approaches may detect spurious differential\nconnections induced by the effect of the covariates on both the disease\ncondition and the network. To address this issue, we propose a general\ncovariate-adjusted test for differential network analysis. Our method assesses\ndifferential network connectivity by testing the null hypothesis that the\nnetwork is the same for individuals who have identical covariates and only\ndiffer in disease status. We show empirically in a simulation study that the\ncovariate-adjusted test exhibits improved type-I error control compared with\nna\\\"ive hypothesis testing procedures that do not account for covariates. We\nadditionally show that there are settings in which our proposed methodology\nprovides improved power to detect differential connections. We illustrate our\nmethod by applying it to detect differences in breast cancer gene co-expression\nnetworks by subtype.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 03:23:11 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 22:22:06 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 04:41:41 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Hudson", "Aaron", ""], ["Shojaie", "Ali", ""]]}, {"id": "2010.08864", "submitter": "Faming Liang", "authors": "Faming Liang, Jingnan Xue, and Bochao Jia", "title": "Markov Neighborhood Regression for High-Dimensional Inference", "comments": "37 pages, 5 figures", "journal-ref": "Journal of the American Statistical Association, 2020", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an innovative method for constructing confidence\nintervals and assessing p-values in statistical inference for high-dimensional\nlinear models. The proposed method has successfully broken the high-dimensional\ninference problem into a series of low-dimensional inference problems: For each\nregression coefficient $\\beta_i$, the confidence interval and $p$-value are\ncomputed by regressing on a subset of variables selected according to the\nconditional independence relations between the corresponding variable $X_i$ and\nother variables. Since the subset of variables forms a Markov neighborhood of\n$X_i$ in the Markov network formed by all the variables $X_1,X_2,\\ldots,X_p$,\nthe proposed method is coined as Markov neighborhood regression. The proposed\nmethod is tested on high-dimensional linear, logistic and Cox regression. The\nnumerical results indicate that the proposed method significantly outperforms\nthe existing ones. Based on the Markov neighborhood regression, a method of\nlearning causal structures for high-dimensional linear models is proposed and\napplied to identification of drug sensitive genes and cancer driver genes. The\nidea of using conditional independence relations for dimension reduction is\ngeneral and potentially can be extended to other high-dimensional or big data\nproblems as well.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 21:00:37 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Liang", "Faming", ""], ["Xue", "Jingnan", ""], ["Jia", "Bochao", ""]]}, {"id": "2010.08870", "submitter": "Xiaotian Xie", "authors": "Xiaotian Xie, Dimitrios Katselis, Carolyn L. Beck and R. Srikant", "title": "On the Consistency of Maximum Likelihood Estimators for Causal Network\n  Identification", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying parameters of a particular class of\nMarkov chains, called Bernoulli Autoregressive (BAR) processes. The structure\nof any BAR model is encoded by a directed graph. Incoming edges to a node in\nthe graph indicate that the state of the node at a particular time instant is\ninfluenced by the states of the corresponding parental nodes in the previous\ntime instant. The associated edge weights determine the corresponding level of\ninfluence from each parental node. In the simplest setup, the Bernoulli\nparameter of a particular node's state variable is a convex combination of the\nparental node states in the previous time instant and an additional Bernoulli\nnoise random variable. This paper focuses on the problem of edge weight\nidentification using Maximum Likelihood (ML) estimation and proves that the ML\nestimator is strongly consistent for two variants of the BAR model. We\nadditionally derive closed-form estimators for the aforementioned two variants\nand prove their strong consistency.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 21:25:44 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Xie", "Xiaotian", ""], ["Katselis", "Dimitrios", ""], ["Beck", "Carolyn L.", ""], ["Srikant", "R.", ""]]}, {"id": "2010.08875", "submitter": "Tracy Qi Dong", "authors": "Tracy Qi Dong and Jon Wakefield", "title": "Estimating efficacy of measles supplementary immunization activities via\n  discrete-time modeling of disease incidence time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measles is a significant source of global disease burden and child mortality.\nMeasles vaccination through routine immunization (RI) programs in high-burden\nsettings remains a challenge due to poor health care infrastructure and access.\nSupplementary immunization activities (SIA) in the form of vaccination\ncampaigns are therefore implemented to prevent measles outbreaks by reducing\nthe size of the susceptible population. The SIA efficacy, defined as the\nfraction of susceptible population immunized by an SIA, is a critical metric\nfor assessing campaign effectiveness. We propose a discrete-time hidden Markov\nmodel for estimating SIA efficacy and forecasting future incidence trends using\nreported measles incidence data. Our approach extends the time-series\nsusceptible-infected-recovered (TSIR) framework by adding a model component to\ncapture the impact of SIAs on the susceptible population. It also accounts for\nunder-reporting and its associated uncertainty via a two-stage estimation\nprocedure with uncertainty propagation. The proposed model can be used to\nestimate the underlying susceptible population dynamics, assess how many\nsusceptible people were immunized by past SIAs, and forecast incidence trends\nin the future under various hypothetical SIA scenarios. We examine model\nperformance via simulations under various levels of under-reporting, and apply\nthe model to analyze monthly reported measles incidence in Benin from 2012 to\n2018.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 21:56:36 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Dong", "Tracy Qi", ""], ["Wakefield", "Jon", ""]]}, {"id": "2010.08893", "submitter": "Tianhui Zhou", "authors": "Tianhui Zhou, Guangyu Tong, Fan Li, Laine E. Thomas, Fan Li", "title": "PSweight: An R Package for Propensity Score Weighting Analysis", "comments": "18 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Propensity score weighting is an important tool for comparative effectiveness\nresearch.Besides the inverse probability of treatment weights (IPW), recent\ndevelopment has introduced a general class of balancing weights, corresponding\nto alternative target populations and estimands. In particular, the overlap\nweights (OW) lead to optimal covariate balance and estimation efficiency, and a\ntarget population of scientific and policy interest. We develop the R package\nPSweight to provide a comprehensive design and analysis platform for causal\ninference based on propensity score weighting. PSweight supports (i) a variety\nof balancing weights, (ii) binary and multiple treatments,(iii) simple and\naugmented weighting estimators, (iv) nuisance-adjusted sandwich variances,\nand(v) ratio estimands. PSweight also provides diagnostic tables and graphs for\ncovariate balance assessment. We demonstrate the functionality of the package\nusing a data example from the NationalChild Development Survey (NCDS), where we\nevaluate the causal effect of educational attainment on income.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 00:28:18 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 01:06:54 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 02:21:07 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 02:50:43 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zhou", "Tianhui", ""], ["Tong", "Guangyu", ""], ["Li", "Fan", ""], ["Thomas", "Laine E.", ""], ["Li", "Fan", ""]]}, {"id": "2010.08941", "submitter": "Joseph Resch", "authors": "Joseph Resch, Abhyuday Mandal, Pritam Ranjan", "title": "Inverse Problem for Dynamic Computer Simulators via Multiple\n  Scalar-valued Contour Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a dynamic computer simulator that produces a\ntime-series response $y_t(x)$ over $L$ time points, for every given input\nparameter $x$. We propose a method for solving inverse problems, which refer to\nthe finding of a set of inputs that generates a pre-specified simulator output.\nInspired by the sequential approach of contour estimation via expected\nimprovement criterion developed by Ranjan et al. (2008, DOI:\n10.1198/004017008000000541), our proposed method discretizes the target\nresponse series on $k \\; (\\ll L)$ time points, and then iteratively solves $k$\nscalar-valued inverse problems with respect to the discretized targets. We also\npropose to use spline smoothing of the target response series to identify the\noptimal number of knots, $k$, and the actual location of the knots for\ndiscretization. The performance of the proposed methods is compared for several\ntest-function based computer simulators and the motivating real application\nthat uses a rainfall-runoff measurement model named Matlab-Simulink model.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 08:28:12 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 04:50:56 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Resch", "Joseph", ""], ["Mandal", "Abhyuday", ""], ["Ranjan", "Pritam", ""]]}, {"id": "2010.09107", "submitter": "Chen Xu", "authors": "Chen Xu, Yao Xie", "title": "Conformal prediction interval for dynamic time-series", "comments": "Accepted as a long talk/oral (3% of total submissions) in the\n  Proceedings of the 38th International Conference on Machine Learning, PMLR\n  139, 2021 (ICML 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a method to construct distribution-free prediction intervals for\ndynamic time-series, called \\Verb|EnbPI| that wraps around any bootstrap\nensemble estimator to construct sequential prediction intervals. \\Verb|EnbPI|\nis closely related to the conformal prediction (CP) framework but does not\nrequire data exchangeability. Theoretically, these intervals attain\nfinite-sample, \\textit{approximately valid} marginal coverage for broad classes\nof regression functions and time-series with strongly mixing stochastic errors.\nComputationally, \\Verb|EnbPI| avoids overfitting and requires neither\ndata-splitting nor training multiple ensemble estimators; it efficiently\naggregates bootstrap estimators that have been trained. In general,\n\\Verb|EnbPI| is easy to implement, scalable to producing arbitrarily many\nprediction intervals sequentially, and well-suited to a wide range of\nregression functions. We perform extensive real-data analyses to demonstrate\nits effectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 21:05:32 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 14:31:22 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 16:43:03 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 17:41:46 GMT"}, {"version": "v5", "created": "Mon, 10 May 2021 13:15:53 GMT"}, {"version": "v6", "created": "Sat, 15 May 2021 22:21:53 GMT"}, {"version": "v7", "created": "Sun, 23 May 2021 07:58:06 GMT"}, {"version": "v8", "created": "Wed, 2 Jun 2021 02:14:46 GMT"}, {"version": "v9", "created": "Wed, 16 Jun 2021 14:03:23 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Xu", "Chen", ""], ["Xie", "Yao", ""]]}, {"id": "2010.09154", "submitter": "Hongzhi Wang", "authors": "Hongzhi Wang, Qian Xiao and Abhyuday Mandal", "title": "Musings about Constructions of Efficient Latin Hypercube Designs with\n  Flexible Run-sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Latin hypercube designs (LHDs), including maximin distance LHDs,\nmaximum projection LHDs and orthogonal LHDs, are widely used in computer\nexperiments. It is challenging to construct such designs with flexible sizes,\nespecially for large ones. In the current literature, various algebraic methods\nand search algorithms have been proposed for identifying efficient LHDs, each\nhaving its own pros and cons. In this paper, we review, summarize and compare\nsome currently popular methods aiming to provide guidance for experimenters on\nwhat method should be used in practice. Using the R package we developed which\nintegrates and improves various algebraic and searching methods, many of the\ndesigns found in this paper are better than the existing ones. They are easy to\nuse for practitioners and can serve as benchmarks for the future developments\non LHDs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 00:40:10 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 21:39:56 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wang", "Hongzhi", ""], ["Xiao", "Qian", ""], ["Mandal", "Abhyuday", ""]]}, {"id": "2010.09176", "submitter": "Helton Saulo", "authors": "Helton Saulo, Alan Dasilva, V\\'ictor Leiva and Luis S\\'anchez", "title": "Log-symmetric quantile regression models", "comments": "31 pages; 10 Figures; 24 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models based on the log-symmetric family of distributions are\nparticularly useful when the response is strictly positive and asymmetric. In\nthis paper, we propose a class of quantile regression models based on\nreparameterized log-symmetric distributions, which have a quantile parameter.\nTwo Monte Carlo simulation studies are carried out using the R software. The\nfirst one analyzes the performance of the maximum likelihood estimators, the\ninformation criteria AIC, BIC and AICc, and the generalized Cox-Snell and\nrandom quantile residuals. The second one evaluates the performance of the size\nand power of the Wald, likelihood ratio, score and gradient tests. A real box\noffice data set is finally analyzed to illustrate the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 02:50:12 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 14:11:13 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 18:59:07 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Saulo", "Helton", ""], ["Dasilva", "Alan", ""], ["Leiva", "V\u00edctor", ""], ["S\u00e1nchez", "Luis", ""]]}, {"id": "2010.09209", "submitter": "Fintan Costello", "authors": "Fintan Costello and Paul Watts", "title": "Significance and Replication in simple counting experiments:\n  Distributional Null Hypothesis Testing", "comments": "unpublished, 32 pages. arXiv admin note: text overlap with\n  arXiv:2010.07813", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Null Hypothesis Significance Testing (NHST) has long been of central\nimportance to psychology as a science, guiding theory development and\nunderlying the application of evidence-based intervention and decision-making.\nRecent years, however, have seen growing awareness of serious problems with\nNHST as it is typically used; this awareness has led to proposals to limit the\nuse of NHST techniques, to abandon these techniques and move to alternative\nstatistical approaches, or even to ban the use of NHST entirely. These\nproposals are premature, because the observed problems with NHST arise as a\nconsequence of an historically contingent, essentially unmotivated, and\nfundamentally incorrect, choice: that of NHST testing against point-form null\nhypotheses. Using simple counting experiments we give a detailed presentation\nof an alternative, more general approach: that of testing against\ndistributional nulls. We show that this distributional approach is\nwell-motivated mathematically, practically and experimentally, and that the use\nof distributional nulls addresses various problems with the standard point-form\nNHST approach, avoiding issues to do with sample size and allowing a coherent\nestimation of the probability of replication of a given experimental result.\nRather than abandoning NHST, we should use the NHST approach in its most\ngeneral form, with distributional rather than point-form null hypotheses.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 08:10:51 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Costello", "Fintan", ""], ["Watts", "Paul", ""]]}, {"id": "2010.09335", "submitter": "Damjan Vukcevic", "authors": "Jeffrey Pullin, Lyle Gurrin, Damjan Vukcevic", "title": "Statistical Models of Repeated Categorical Ratings: The R package rater", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common occurrence in many disciplines is the need to assign a set of items\ninto categories or classes with known labels. This is often done by one or more\nexpert raters, or sometimes by an automated process. If these assignments, or\n'ratings', are difficult to do, a common tactic is to repeat them by different\nraters, or even by the same rater multiple times on different occasions.\n  We present an R package, rater, available on CRAN, that implements Bayesian\nversions of several statistical models that allow analysis of repeated\ncategorical rating data. Inference is possible for the true underlying (latent)\nclass of each item, as well as the accuracy of each rater.\n  The models are based on, and include, the Dawid-Skene model. We use the Stan\nprobabilistic programming language as the main computational engine.\n  We illustrate usage of rater through a few examples. We also discuss in\ndetail the techniques of marginalisation and conditioning, which are necessary\nfor these models but also apply more generally to other models implemented in\nStan.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 09:17:40 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 09:21:06 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 12:54:53 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Pullin", "Jeffrey", ""], ["Gurrin", "Lyle", ""], ["Vukcevic", "Damjan", ""]]}, {"id": "2010.09386", "submitter": "Armeen Taeb", "authors": "Armeen Taeb, Parikshit Shah, Venkat Chandrasekaran", "title": "Learning Exponential Family Graphical Models with Latent Variables using\n  Regularized Conditional Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting a graphical model to a collection of random variables given sample\nobservations is a challenging task if the observed variables are influenced by\nlatent variables, which can induce significant confounding statistical\ndependencies among the observed variables. We present a new convex relaxation\nframework based on regularized conditional likelihood for latent-variable\ngraphical modeling in which the conditional distribution of the observed\nvariables conditioned on the latent variables is given by an exponential family\ngraphical model. In comparison to previously proposed tractable methods that\nproceed by characterizing the marginal distribution of the observed variables,\nour approach is applicable in a broader range of settings as it does not\nrequire knowledge about the specific form of distribution of the latent\nvariables and it can be specialized to yield tractable approaches to problems\nin which the observed data are not well-modeled as Gaussian. We demonstrate the\nutility and flexibility of our framework via a series of numerical experiments\non synthetic as well as real data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 11:16:26 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Taeb", "Armeen", ""], ["Shah", "Parikshit", ""], ["Chandrasekaran", "Venkat", ""]]}, {"id": "2010.09398", "submitter": "Anna Malinovskaya", "authors": "Anna Malinovskaya and Philipp Otto", "title": "Online network monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of network analysis has found great success in a wide variety\nof disciplines; however, the popularity of these approaches has revealed the\ndifficulty in handling networks whose complexity scales rapidly. One of the\nmain interests in network analysis is the online detection of anomalous\nbehaviour. To overcome the curse of dimensionality, we introduce a network\nsurveillance method bringing together network modelling and statistical process\ncontrol. Our approach is to apply multivariate control charts based on\nexponential smoothing and cumulative sums in order to monitor networks\ndetermined by temporal exponential random graph models (TERGM). This allows us\nto account for temporal dependence, while simultaneously reducing the number of\nparameters to be monitored. The performance of the proposed charts is evaluated\nby calculating the average run length for both simulated and real data. To\nprove the appropriateness of the TERGM to describe network data, some measures\nof goodness of fit are inspected. We demonstrate the effectiveness of the\nproposed approach by an empirical application, monitoring daily flights in the\nUnited States to detect anomalous patterns.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 11:36:24 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 15:39:17 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Malinovskaya", "Anna", ""], ["Otto", "Philipp", ""]]}, {"id": "2010.09443", "submitter": "Jessica Gronsbell", "authors": "Jessica Gronsbell and Molei Liu and Lu Tian and Tianxi Cai", "title": "Efficient Estimation and Evaluation of Prediction Rules in\n  Semi-Supervised Settings under Stratified Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many contemporary applications, large amounts of unlabeled data are\nreadily available while labeled examples are limited. There has been\nsubstantial interest in semi-supervised learning (SSL) which aims to leverage\nunlabeled data to improve estimation or prediction. However, current SSL\nliterature focuses primarily on settings where labeled data is selected\nrandomly from the population of interest. Non-random sampling, while posing\nadditional analytical challenges, is highly applicable to many real world\nproblems. Moreover, no SSL methods currently exist for estimating the\nprediction performance of a fitted model under non-random sampling. In this\npaper, we propose a two-step SSL procedure for evaluating a prediction rule\nderived from a working binary regression model based on the Brier score and\noverall misclassification rate under stratified sampling. In step I, we impute\nthe missing labels via weighted regression with nonlinear basis functions to\naccount for nonrandom sampling and to improve efficiency. In step II, we\naugment the initial imputations to ensure the consistency of the resulting\nestimators regardless of the specification of the prediction model or the\nimputation model. The final estimator is then obtained with the augmented\nimputations. We provide asymptotic theory and numerical studies illustrating\nthat our proposals outperform their supervised counterparts in terms of\nefficiency gain. Our methods are motivated by electronic health records (EHR)\nresearch and validated with a real data analysis of an EHR-based study of\ndiabetic neuropathy.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 12:54:45 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Gronsbell", "Jessica", ""], ["Liu", "Molei", ""], ["Tian", "Lu", ""], ["Cai", "Tianxi", ""]]}, {"id": "2010.09563", "submitter": "Andreas Markoulidakis Mr", "authors": "Andreas Markoulidakis, Khadijeh Taiyari, Peter Holmans, Philip\n  Pallmann, Monica Busse, Mark D. Godley, Beth Ann Griffin", "title": "A tutorial comparing different covariate balancing methods with an\n  application evaluating the causal effects of substance use treatment programs\n  for adolescents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized controlled trials are the gold standard for measuring causal\neffects. However, they are often not always feasible, and causal treatment\neffects must be estimated from observational data. Observational studies do not\nallow robust conclusions about causal relationships unless statistical\ntechniques account for the imbalance of pretreatment confounders across groups\nwhile key assumptions hold. Propensity score and balance weighting (PSBW) are\nuseful techniques that aim to reduce the imbalances between treatment groups by\nweighting the groups to look alike on the observed confounders. There are many\nmethods available to estimate PSBW. However, it is unclear a priori which will\nachieve the best trade-off between covariate balance and effective sample size.\nMoreover, it is critical to assess the validity of key assumptions required for\nrobust estimation of the needed treatment effects, including the overlap and no\nunmeasured confounding assumptions. We present a step-by-step guide to\ncovariate balancing strategies, including how to evaluate overlap, obtain\nestimates of PSBW, check for covariate balance, and assess sensitivity to\nunobserved confounding. We compare the performance of several estimation\nmethods using a case study examining the relative effectiveness of substance\nuse treatment programs and provide a user-friendly web application that can\nimplement the proposed steps.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 14:47:28 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 09:41:34 GMT"}, {"version": "v3", "created": "Sat, 15 May 2021 14:58:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Markoulidakis", "Andreas", ""], ["Taiyari", "Khadijeh", ""], ["Holmans", "Peter", ""], ["Pallmann", "Philip", ""], ["Busse", "Monica", ""], ["Godley", "Mark D.", ""], ["Griffin", "Beth Ann", ""]]}, {"id": "2010.09578", "submitter": "Karthik Bharath", "authors": "Xiaohan Guo, Sebastian Kurtek and Karthik Bharath", "title": "Variograms for spatial functional data with phase variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial, amplitude and phase variations in spatial functional data are\nconfounded. Conclusions from the popular functional trace variogram, which\nquantifies spatial variation, can be misleading when analysing misaligned\nfunctional data with phase variation. To remedy this, we describe a framework\nthat extends amplitude-phase separation methods in functional data to the\nspatial setting, with a view towards performing clustering and spatial\nprediction. We propose a decomposition of the trace variogram into amplitude\nand phase components and quantify how spatial correlations between functional\nobservations manifest in their respective amplitude and phase components. This\nenables us to generate separate amplitude and phase clustering methods for\nspatial functional data, and develop a novel spatial functional interpolant at\nunobserved locations based on combining separate amplitude and phase\npredictions. Through simulations and real data analyses, we found that the\nproposed methods result in more accurate predictions and more interpretable\nclustering results.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:01:43 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Guo", "Xiaohan", ""], ["Kurtek", "Sebastian", ""], ["Bharath", "Karthik", ""]]}, {"id": "2010.09686", "submitter": "Ian Waudby-Smith", "authors": "Ian Waudby-Smith and Aaditya Ramdas", "title": "Estimating means of bounded random variables by betting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives confidence intervals (CI) and time-uniform confidence\nsequences (CS) for the classical problem of estimating an unknown mean from\nbounded observations. We present a general approach for deriving concentration\nbounds, that can be seen as a generalization (and improvement) of the\ncelebrated Chernoff method. At its heart, it is based on deriving a new class\nof composite nonnegative martingales, with strong connections to betting and\nthe method of mixtures. We show how to extend these ideas to sampling without\nreplacement, another heavily studied problem. In all cases, our bounds are\nadaptive to the unknown variance, and empirically vastly outperform competing\napproaches based on Hoeffding or empirical Bernstein inequalities and their\nrecent supermartingale generalizations. In short, we establish a new\nstate-of-the-art for four fundamental problems: CSs and CIs for bounded means,\nwith and without replacement.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:22:03 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 01:29:19 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 00:57:37 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 02:56:49 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Waudby-Smith", "Ian", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2010.09822", "submitter": "Michelle Zhou", "authors": "Qian M. Zhou, Zhe Lu, Russell J. Brooke, Melissa M Hudson, Yan Yuan", "title": "Is the new model better? One metric says yes, but the other says no.\n  Which metric do I use?", "comments": "25 pages, 6 figures, 1 table. Compared to Version 1, the title and\n  overall structure of the manuscript have been changed significantly", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental value (IncV) evaluates the performance change from an existing\nrisk model to a new model. It is one of the key considerations in deciding\nwhether a new risk model performs better than the existing one. Problems arise\nwhen different IncV metrics contradict each other. For example, compared with a\nprescribed-dose model, an ovarian-dose model for predicting acute ovarian\nfailure has a slightly lower area under the receiver operating characteristic\ncurve (AUC) but increases the area under the precision-recall curve (AP) by\n48%. This phenomenon of conflicting conclusions is not uncommon, and it creates\na dilemma in medical decision making. In this article, we examine the\nanalytical connections and differences between two IncV metrics: IncV in AUC\n(IncV-AUC) and IncV in AP (IncV-AP). Additionally, since they are both\nsemi-proper scoring rules, we compare them with a strictly proper scoring rule:\nthe IncV of the scaled Brier score (IncV-sBrS), via a numerical study. We\ndemonstrate that both IncV-AUC and IncV-AP are weighted averages of the changes\n(from the existing model to the new one) in separating the risk score\ndistributions between events and non-events. However, IncV-AP assigns heavier\nweights to the changes in the high-risk group, whereas IncV-AUC weights the\nchanges equally. In the numerical study, we find that IncV-AP has a wide range,\nfrom negative to positive, but the size of IncV-AUC is much smaller. In\naddition, IncV-AP and IncV-sBr Sare highly consistent, but IncV-AUC is\nnegatively correlated with IncV-sBrS and IncV-AP at a low event rate. IncV-AUC\nand IncV-AP are the least consistent among the three pairs, and their\ndifferences are more pronounced as the event rate decreases.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 20:00:45 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 16:54:55 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Zhou", "Qian M.", ""], ["Lu", "Zhe", ""], ["Brooke", "Russell J.", ""], ["Hudson", "Melissa M", ""], ["Yuan", "Yan", ""]]}, {"id": "2010.09906", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro and He Jiang", "title": "On the Consistency of Metric and Non-Metric K-medoids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish the consistency of K-medoids in the context of metric spaces. We\nstart by proving that K-medoids is asymptotically equivalent to K-means\nrestricted to the support of the underlying distribution under general\nconditions, including a wide selection of loss functions. This asymptotic\nequivalence, in turn, enables us to apply the work of Parna (1986) on the\nconsistency of K-means. This general approach applies also to non-metric\nsettings where only an ordering of the dissimilarities is available. We\nconsider two types of ordinal information: one where all quadruple comparisons\nare available; and one where only triple comparisons are available. We provide\nsome numerical experiments to illustrate our theory.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 22:46:14 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Jiang", "He", ""]]}, {"id": "2010.09921", "submitter": "Jun Yu", "authors": "Cheng Meng and Jun Yu and Jingyi Zhang and Ping Ma and Wenxuan Zhong", "title": "Sufficient dimension reduction for classification using principal\n  optimal transport direction", "comments": "18 pages, 4 figures, to be published in 34th Conference on Neural\n  Information Processing Systems (NeurIPS 2020), add the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sufficient dimension reduction is used pervasively as a supervised dimension\nreduction approach. Most existing sufficient dimension reduction methods are\ndeveloped for data with a continuous response and may have an unsatisfactory\nperformance for the categorical response, especially for the binary-response.\nTo address this issue, we propose a novel estimation method of sufficient\ndimension reduction subspace (SDR subspace) using optimal transport. The\nproposed method, named principal optimal transport direction (POTD), estimates\nthe basis of the SDR subspace using the principal directions of the optimal\ntransport coupling between the data respecting different response categories.\nThe proposed method also reveals the relationship among three seemingly\nirrelevant topics, i.e., sufficient dimension reduction, support vector\nmachine, and optimal transport. We study the asymptotic properties of POTD and\nshow that in the cases when the class labels contain no error, POTD estimates\nthe SDR subspace exclusively. Empirical studies show POTD outperforms most of\nthe state-of-the-art linear dimension reduction methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 23:38:31 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 01:48:14 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 04:34:24 GMT"}, {"version": "v4", "created": "Tue, 2 Feb 2021 04:10:15 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Meng", "Cheng", ""], ["Yu", "Jun", ""], ["Zhang", "Jingyi", ""], ["Ma", "Ping", ""], ["Zhong", "Wenxuan", ""]]}, {"id": "2010.09922", "submitter": "Sai Li", "authors": "Sai Li and Zijian Guo", "title": "Causal Inference for Nonlinear Outcome Models with Possibly Invalid\n  Instrumental Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable methods are widely used for inferring the causal effect\nof an exposure on an outcome when the observed relationship is potentially\naffected by unmeasured confounders. Existing instrumental variable methods for\nnonlinear outcome models require stringent identifiability conditions. We\ndevelop a robust causal inference framework for nonlinear outcome models, which\nrelaxes the conventional identifiability conditions. We adopt a flexible\nsemi-parametric potential outcome model and propose new identifiability\nconditions for identifying the model parameters and causal effects. We devise a\nnovel three-step inference procedure for the conditional average treatment\neffect and establish the asymptotic normality of the proposed point estimator.\nWe construct confidence intervals for the causal effect by the bootstrap\nmethod. The proposed method is demonstrated in a large set of simulation\nstudies and is applied to study the causal effects of lipid levels on whether\nthe glucose level is normal or high over a mice dataset.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 23:47:12 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Li", "Sai", ""], ["Guo", "Zijian", ""]]}, {"id": "2010.09958", "submitter": "Shaoyang Ning", "authors": "Dingdong Yi, Shaoyang Ning, Chia-Jung Chang, S. C. Kou", "title": "Forecasting unemployment using Internet search data via PRISM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data generated from the Internet offer great potential for predictive\nanalysis. Here we focus on using online users' Internet search data to forecast\nunemployment initial claims weeks into the future, which provides timely\ninsights into the direction of the economy. To this end, we present a novel\nmethod PRISM (Penalized Regression with Inferred Seasonality Module), which\nuses publicly available online search data from Google. PRISM is a\nsemi-parametric method, motivated by a general state-space formulation, and\nemploys nonparametric seasonal decomposition and penalized regression. For\nforecasting unemployment initial claims, PRISM outperforms all previously\navailable methods, including forecasting during the 2008-2009 financial crisis\nperiod and near-future forecasting during the COVID-19 pandemic period, when\nunemployment initial claims both rose rapidly. The timely and accurate\nunemployment forecasts by PRISM could aid government agencies and financial\ninstitutions to assess the economic trend and make well-informed decisions,\nespecially in the face of economic turbulence.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 01:58:46 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 21:35:14 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Yi", "Dingdong", ""], ["Ning", "Shaoyang", ""], ["Chang", "Chia-Jung", ""], ["Kou", "S. C.", ""]]}, {"id": "2010.09971", "submitter": "Tian Gu", "authors": "Tian Gu, Jeremy M.G. Taylor and Bhramar Mukherjee", "title": "A meta-inference framework to integrate multiple external models into a\n  current study", "comments": null, "journal-ref": null, "doi": "10.1093/biostatistics/kxab017", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is becoming increasingly common for researchers to consider incorporating\nexternal information from large studies to improve the accuracy of statistical\ninference instead of relying on a modestly sized dataset collected internally.\nWith some new predictors only available internally, we aim to build improved\nregression models based on individual-level data from an \"internal\" study while\nincorporating summary-level information from \"external\" models. We propose a\nmeta-analysis framework along with two weighted estimators as the composite of\nempirical Bayes estimators, which combines the estimates from the different\nexternal models. The proposed framework is flexible and robust in the ways that\n(i) it is capable of incorporating external models that use a slightly\ndifferent set of covariates; (ii) it can identify the most relevant external\ninformation and diminish the influence of information that is less compatible\nwith the internal data; and (iii) it nicely balances the bias-variance\ntrade-off while preserving the most efficiency gain. The proposed estimators\nare more efficient than the naive analysis of the internal data and other naive\ncombinations of external estimators.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 02:41:10 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 16:20:57 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gu", "Tian", ""], ["Taylor", "Jeremy M. G.", ""], ["Mukherjee", "Bhramar", ""]]}, {"id": "2010.10017", "submitter": "Fernando Hartwig", "authors": "F. P. Hartwig, J. Bowden, L. Wang, G. Davey Smith, N. M. Davies", "title": "Average causal effect estimation via instrumental variables: the no\n  simultaneous heterogeneity assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Instrumental variables (IVs) can be used to provide evidence as to whether a\ntreatment X has a causal effect on Y. Z is a valid instrument if it satisfies\nthe three core IV assumptions of relevance, independence and the exclusion\nrestriction. Even if the instrument satisfies these assumptions, further\nassumptions are required to estimate the average causal effect (ACE) of X on Y.\nSufficient assumptions for this include: homogeneity in the causal effect of X\non Y; homogeneity in the association of Z with X; and independence between X\nand the causal effect of X on Y. Other assumptions allow identification of\nother causal estimands. For example, the monotonicity assumption allows\nidentifying the average causal effect among compliers. Here, we describe the NO\nSimultaneous Heterogeneity (NOSH) assumption, which requires the heterogeneity\nin the Z-X association and heterogeneity in the X-Y causal effect to be\nindependent. We describe the necessary conditions for NOSH and show that, if\nNOSH holds, conventional IV methods are consistent for the ACE even if both\nhomogeneity assumptions and NEM are violated. We illustrate these ideas using\nsimulations and by re-examining selected published studies.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 04:40:17 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Hartwig", "F. P.", ""], ["Bowden", "J.", ""], ["Wang", "L.", ""], ["Smith", "G. Davey", ""], ["Davies", "N. M.", ""]]}, {"id": "2010.10071", "submitter": "Thorsten Dickhaus", "authors": "Natalia Sirotko-Sibirskaya, Matthias O. Franz, Thorsten Dickhaus", "title": "Volterra bootstrap: Resampling higher-order statistics for strictly\n  stationary univariate time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with nonparametric hypothesis testing of time series\nfunctionals. It is known that the popular autoregressive sieve bootstrap is, in\ngeneral, not valid for statistics whose (asymptotic) distribution depends on\nmoments of order higher than two, irrespective of whether the data come from a\nlinear time series or a nonlinear one. Inspired by nonlinear system theory we\ncircumvent this non-validity by introducing a higher-order bootstrap scheme\nbased on the Volterra series representation of the process. In order to\nestimate coefficients of such a representation efficiently, we rely on the\nalternative formulation of Volterra operators in reproducing kernel Hilbert\nspace. We perform polynomial kernel regression which scales linearly with the\ninput dimensionality and is independent of the degree of nonlinearity. We\nillustrate the applicability of the suggested Volterra-representation-based\nbootstrap procedure in a simulation study where we consider strictly stationary\nlinear and nonlinear processes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 06:59:50 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Sirotko-Sibirskaya", "Natalia", ""], ["Franz", "Matthias O.", ""], ["Dickhaus", "Thorsten", ""]]}, {"id": "2010.10194", "submitter": "Solt Kov\\'acs", "authors": "Solt Kov\\'acs, Housen Li, Lorenz Haubner, Axel Munk, Peter B\\\"uhlmann", "title": "Optimistic search strategy: Change point detection for large-scale data\n  via adaptive logarithmic queries", "comments": "extended Table 1; added Model II and Lemma 5.3; added further minor\n  explanations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a classical and ever reviving topic, change point detection is often\nformulated as a search for the maximum of a gain function describing improved\nfits when segmenting the data. Searching through all candidate split points on\nthe grid for finding the best one requires $O(T)$ evaluations of the gain\nfunction for an interval with $T$ observations. If each evaluation is\ncomputationally demanding (e.g. in high-dimensional models), this can become\ninfeasible. Instead, we propose optimistic search strategies with $O(\\log T)$\nevaluations exploiting specific structure of the gain function.\n  Towards solid understanding of our strategies, we investigate in detail the\nclassical univariate Gaussian change in mean setup. For some of our proposals\nwe prove asymptotic minimax optimality for single and multiple change point\nscenarios. Our search strategies generalize far beyond the theoretically\nanalyzed univariate setup. We illustrate, as an example, massive computational\nspeedup in change point detection for high-dimensional Gaussian graphical\nmodels. More generally, we demonstrate empirically that our optimistic search\nmethods lead to competitive estimation performance while heavily reducing\nrun-time.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 11:09:52 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 23:50:55 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Kov\u00e1cs", "Solt", ""], ["Li", "Housen", ""], ["Haubner", "Lorenz", ""], ["Munk", "Axel", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2010.10244", "submitter": "Yunshan Duan", "authors": "Yunshan Duan, Sue-Jane Wang and Yuan Ji", "title": "Hi3+3: A Model-Assisted Dose-Finding Design Borrowing Historical Data", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background -- In phase I clinical trials, historical data may be available\nthrough multi-regional programs, reformulation of the same drug, or previous\ntrials for a drug under the same class. Statistical designs that borrow\ninformation from historical data can reduce cost, speed up drug development,\nand maintain safety. Purpose -- Based on a hybrid design that partly uses\nprobability models and partly uses algorithmic rules for decision making, we\naim to improve the efficiency of the dose-finding trials in the presence of\nhistorical data, maintain safety for patients, and achieve a level of\nsimplicity for practical applications. Methods -- We propose the Hi3+3 design,\nin which the letter \"H\" represents \"historical data\". We apply the idea in\npower prior to borrow historical data and define the effective sample size\n(ESS) of the prior. Dose-finding decision rules follow the idea in the i3+3\ndesign while incorporating the historical data via the power prior and ESS. The\nproposed Hi3+3 design pretabulates the dosing decisions before the trial\nstarts, a desirable feature for ease of application in practice. Results -- The\nHi3+3 design is superior than the i3+3 design due to information borrow from\nhistorical data. It is capable of maintaining a high level of safety for trial\npatients without sacrificing the ability to identify the correct MTD.\nIllustration of this feature are found in the simulation results. Conclusion --\nWith the demonstrated safety, efficiency, and simplicity, the Hi3+3 design\ncould be a desirable choice for dose-finding trials borrowing historical data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:12:42 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Duan", "Yunshan", ""], ["Wang", "Sue-Jane", ""], ["Ji", "Yuan", ""]]}, {"id": "2010.10275", "submitter": "Ryan Martin", "authors": "Vaidehi Dixit and Ryan Martin", "title": "Estimating a mixing distribution on the sphere using predictive\n  recursion", "comments": "26 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are commonly used when data show signs of heterogeneity and,\noften, it is important to estimate the distribution of the latent variable\nresponsible for that heterogeneity. This is a common problem for data taking\nvalues in a Euclidean space, but the work on mixing distribution estimation\nbased on directional data taking values on the unit sphere is limited. In this\npaper, we propose using the predictive recursion (PR) algorithm to solve for a\nmixture on a sphere. One key feature of PR is its computational efficiency.\nMoreover, compared to likelihood-based methods that only support finite mixing\ndistribution estimates, PR is able to estimate a smooth mixing density. PR's\nasymptotic consistency in spherical mixture models is established, and\nsimulation results showcase its benefits compared to existing likelihood-based\nmethods. We also show two real-data examples to illustrate how PR can be used\nfor goodness-of-fit testing and clustering.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:46:06 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Dixit", "Vaidehi", ""], ["Martin", "Ryan", ""]]}, {"id": "2010.10410", "submitter": "Daren Wang", "authors": "Alessandro Rinaldo, Daren Wang, Qin Wen, Rebecca Willett, Yi Yu", "title": "Localizing Changes in High-Dimensional Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of localizing change points in\nhigh-dimensional linear regression models with piecewise constant regression\ncoefficients. We develop a dynamic programming approach to estimate the\nlocations of the change points whose performance improves upon the current\nstate-of-the-art, even as the dimensionality, the sparsity of the regression\ncoefficients, the temporal spacing between two consecutive change points, and\nthe magnitude of the difference of two consecutive regression coefficient\nvectors are allowed to vary with the sample size. Furthermore, we devise a\ncomputationally-efficient refinement procedure that provably reduces the\nlocalization error of preliminary estimates of the change points. We\ndemonstrate minimax lower bounds on the localization error that nearly match\nthe upper bound on the localization error of our methodology and show that the\nsignal-to-noise condition we impose is essentially the weakest possible based\non information-theoretic arguments. Extensive numerical results support our\ntheoretical findings, and experiments on real air quality data reveal change\npoints supported by historical information not used by the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 16:17:04 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Rinaldo", "Alessandro", ""], ["Wang", "Daren", ""], ["Wen", "Qin", ""], ["Willett", "Rebecca", ""], ["Yu", "Yi", ""]]}, {"id": "2010.10412", "submitter": "Qiong Zhang", "authors": "Qiong Zhang and Jiahua Chen", "title": "Distributed Learning of Finite Gaussian Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in information technology have led to extremely large datasets that\nare often kept in different storage centers. Existing statistical methods must\nbe adapted to overcome the resulting computational obstacles while retaining\nstatistical validity and efficiency. Split-and-conquer approaches have been\napplied in many areas, including quantile processes, regression analysis,\nprincipal eigenspaces, and exponential families. We study split-and-conquer\napproaches for the distributed learning of finite Gaussian mixtures. We\nrecommend a reduction strategy and develop an effective MM algorithm. The new\nestimator is shown to be consistent and retains root-n consistency under some\ngeneral conditions. Experiments based on simulated and real-world data show\nthat the proposed split-and-conquer approach has comparable statistical\nperformance with the global estimator based on the full dataset, if the latter\nis feasible. It can even slightly outperform the global estimator if the model\nassumption does not match the real-world data. It also has better statistical\nand computational performance than some existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 16:17:47 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 23:24:21 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhang", "Qiong", ""], ["Chen", "Jiahua", ""]]}, {"id": "2010.10471", "submitter": "Chayut Wongkamthong", "authors": "Chayut Wongkamthong, Olanrewaju Akande", "title": "A Comparative Study of Imputation Methods for Multivariate Ordinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data remains a very common problem in large datasets, including\nsurvey and census data containing many ordinal responses, such as political\npolls and opinion surveys. Multiple imputation (MI) is usually the go-to\napproach for analyzing such incomplete datasets, and there are indeed several\nimplementations of MI, including methods using generalized linear models,\ntree-based models, and Bayesian non-parametric models. However, there is\nlimited research on the statistical performance of these methods for\nmultivariate ordinal data. In this article, we perform an empirical evaluation\nof several MI methods, including MI by chained equations (MICE) using\nmultinomial logistic regression models, MICE using proportional odds logistic\nregression models, MICE using classification and regression trees, MICE using\nrandom forest, MI using Dirichlet process (DP) mixtures of products of\nmultinomial distributions, and MI using DP mixtures of multivariate normal\ndistributions. We evaluate the methods using simulation studies based on\nordinal variables selected from the 2018 American Community Survey (ACS). Under\nour simulation settings, the results suggest that MI using proportional odds\nlogistic regression models, classification and regression trees and DP mixtures\nof multinomial distributions generally outperform the other methods. In certain\nsettings, MI using multinomial logistic regression models is able to achieve\ncomparable performance, depending on the missing data mechanism and amount of\nmissing data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:30:57 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 01:11:37 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 00:48:03 GMT"}, {"version": "v4", "created": "Sun, 20 Jun 2021 08:26:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wongkamthong", "Chayut", ""], ["Akande", "Olanrewaju", ""]]}, {"id": "2010.10587", "submitter": "Ramya Hariharan", "authors": "Ramya Hariharan", "title": "When to Relax Social Distancing Measures? An ARIMA Based Forecasting\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The spread of the novel coronavirus across various countries is wide and\nrapid. The number of confirmed cases and the reproduction number are some of\nthe epidemiological parameters utilized in scientific studies for the analysis\nand prediction of the viral transmission. The positive rate, an indicator on\nthe extent of testing the population, aids in understanding the severity of the\ninfection in a given geographic location. The positive rate for selected\ncountries has been considered in this study to construct ARIMA based\nstatistical models. The goodness of fit of the models are verified by the\ninvestigation of residuals, Box-Luang test and the forecast error values. The\npositive rates forecasted by the ARIMA models are utilized to investigate the\nscope for implementation of relaxations in social distancing measures in some\ncountries and the necessity to tighten the rules further in some other\ncountries.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 06:48:34 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Hariharan", "Ramya", ""]]}, {"id": "2010.10588", "submitter": "Georgia Salanti", "authors": "Georgia Salanti, Adriani Nikolakopoulou, Orestis Efthimou, Dimitris\n  Mavridis, Matthias Egger, Ian R. White", "title": "Introducing the treatment hierarchy question in network meta-analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Comparative effectiveness research using network meta-analysis\ncan present a hierarchy of competing treatments, from the least to most\npreferable option. However, the research question associated with the hierarchy\nof multiple interventions is never clearly defined in published reviews.\nMethods and Results: We introduce the notion of a treatment hierarchy question\nthat describes the criterion for choosing a specific treatment over one or more\ncompeting alternatives. For example, stakeholders might ask which treatment is\nmost likely to improve mean survival by at least 2 years or which treatment is\nassociated with the longest mean survival. The answers to these two questions\nare not necessarily the same. We discuss the most commonly used ranking metrics\n(quantities that describe or compare the estimated treatment-specific effects),\nhow the metrics produce a treatment hierarchy and the type of treatment\nhierarchy question that each metric can answer. We show that the ranking\nmetrics encompass the uncertainty in the estimation of the treatment effects in\ndifferent ways, which results in different treatment hierarchies. Conclusions:\nNetwork meta-analyses that aim to rank treatments should state in the protocol\nthe treatment hierarchy question they aim to address and employ the appropriate\nranking metric to answer it.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 19:57:08 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Salanti", "Georgia", ""], ["Nikolakopoulou", "Adriani", ""], ["Efthimou", "Orestis", ""], ["Mavridis", "Dimitris", ""], ["Egger", "Matthias", ""], ["White", "Ian R.", ""]]}, {"id": "2010.10786", "submitter": "Yiping Guo", "authors": "Yiping Guo and Howard D. Bondell", "title": "On Robust Probabilistic Principal Component Analysis using Multivariate\n  $t$-Distributions", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is a common multivariate statistical\nanalysis method, and Probabilistic Principal Component Analysis (PPCA) is its\nprobabilistic reformulation under the framework of Gaussian latent variable\nmodel. To improve the robustness of PPCA, it has been proposed to change the\nunderlying Gaussian distributions to multivariate $t$-distributions. Based on\nthe representation of $t$-distribution as a scale mixture of Gaussians, a\nhierarchical model is used for implementation. However, although the robust\nPPCA methods work reasonably well for some simulation studies and real data,\nthe hierarchical model implemented does not yield the equivalent\ninterpretation. In this paper, we present a set of equivalent relationships\nbetween those models, and discuss the performance of robust PPCA methods using\ndifferent multivariate $t$-distributed structures through several simulation\nstudies. In doing so, we clarify a current misrepresentation in the literature,\nand make connections between a set of hierarchical models for robust PPCA.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 06:49:20 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Guo", "Yiping", ""], ["Bondell", "Howard D.", ""]]}, {"id": "2010.10893", "submitter": "Duncan Lee", "authors": "Duncan Lee and Kitty Meeks", "title": "Improved inference for areal unit count data using graph-based\n  optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial correlation in areal unit count data is typically modelled by a set\nof random effects that are assigned a conditional autoregressive (CAR) prior\ndistribution. The spatial correlation structure implied by this model depends\non a binary neighbourhood matrix, where two random effects are assumed to be\npartially autocorrelated if their areal units share a common border, and are\nconditionally independent otherwise. This paper proposes a novel graph-based\noptimisation algorithm for estimating the neighbourhood matrix from the data,\nby viewing the areal units as the vertices of the graph and the neighbour\nrelations as the set of edges. The superiority of our methodology compared to\nusing the border sharing rule is comprehensively evidenced by simulation,\nbefore the method is applied to a new respiratory disease surveillance study in\nthe Greater Glasgow and Clyde Health board in Scotland between 2011 and 2017.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:06:54 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Lee", "Duncan", ""], ["Meeks", "Kitty", ""]]}, {"id": "2010.10896", "submitter": "Yiping Guo", "authors": "Yiping Guo and Howard D. Bondell", "title": "Conditional Density Estimation via Weighted Logistic Regressions", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to the conditional mean as a simple point estimator, the conditional\ndensity function is more informative to describe the distributions with\nmulti-modality, asymmetry or heteroskedasticity. In this paper, we propose a\nnovel parametric conditional density estimation method by showing the\nconnection between the general density and the likelihood function of\ninhomogeneous Poisson process models. The maximum likelihood estimates can be\nobtained via weighted logistic regressions, and the computation can be\nsignificantly relaxed by combining a block-wise alternating maximization scheme\nand local case-control sampling. We also provide simulation studies for\nillustration.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:08:25 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Guo", "Yiping", ""], ["Bondell", "Howard D.", ""]]}, {"id": "2010.10918", "submitter": "Vitalii Makogin", "authors": "Nikolai Leonenko, Vitalii Makogin, Mehmet Siddik Cadirci", "title": "The entropy based goodness of fit tests for generalized von Mises-Fisher\n  distributions and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce some new classes of unimodal rotational invariant directional\ndistributions, which generalize von Mises-Fisher distribution. We propose three\ntypes of distributions, one of which represents axial data. For each new type\nwe provide formulae and short computational study of parameter estimators by\nthe method of moments and the method of maximum likelihood. The main goal of\nthe paper is to develop the goodness of fit test to detect that sample entries\nfollow one of the introduced generalized von Mises--Fisher distribution based\non the maximum entropy principle. We use $k$th nearest neighbour distances\nestimator of Shannon entropy and prove its $L^2$-consistency. We examine the\nbehaviour of the test statistics, find critical values and compute power of the\ntest on simulated samples. We apply the goodness of fit test to local fiber\ndirections in a glass fibre reinforced composite material and detect the\nsamples which follow axial generalized von Mises--Fisher distribution.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:58:55 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Leonenko", "Nikolai", ""], ["Makogin", "Vitalii", ""], ["Cadirci", "Mehmet Siddik", ""]]}, {"id": "2010.10960", "submitter": "Xing Qin", "authors": "Xing Qin, Shuangge Ma and Mengyun Wu", "title": "Gene-gene interaction analysis incorporating network information via a\n  structured Bayesian approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing evidence has shown that gene-gene interactions have important\neffects on biological processes of human diseases. Due to the high\ndimensionality of genetic measurements, existing interaction analysis methods\nusually suffer from a lack of sufficient information and are still\nunsatisfactory. Biological networks have been massively accumulated, allowing\nresearchers to identify biomarkers from a system perspective by utilizing\nnetwork selection (consisting of functionally related biomarkers) as well as\nnetwork structures. In the main-effect analysis, network information has been\nwidely incorporated, leading to biologically more meaningful and more accurate\nestimates. However, there is still a big gap in the context of interaction\nanalysis. In this study, we develop a novel structured Bayesian interaction\nanalysis approach, effectively incorporating the network information. This\nstudy is among the first to identify gene-gene interactions with the assistance\nof network selection for phenotype prediction, while simultaneously\naccommodating the underlying network structures. It innovatively respects the\nmultiple hierarchies among main effects, interactions, and networks. Bayesian\nmethod is adopted, which has been shown to have multiple advantages over some\nother techniques. An efficient variational inference algorithm is developed to\nexplore the posterior distribution. Extensive simulation studies demonstrate\nthe practical superiority of the proposed approach. The analysis of TCGA data\non melanoma and lung cancer leads to biologically sensible findings with\nsatisfactory prediction accuracy and selection stability.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 12:53:48 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 13:02:20 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Qin", "Xing", ""], ["Ma", "Shuangge", ""], ["Wu", "Mengyun", ""]]}, {"id": "2010.11002", "submitter": "Masatoshi Uehara", "authors": "Nathan Kallus, Yuta Saito, Masatoshi Uehara", "title": "Optimal Off-Policy Evaluation from Multiple Logging Policies", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study off-policy evaluation (OPE) from multiple logging policies, each\ngenerating a dataset of fixed size, i.e., stratified sampling. Previous work\nnoted that in this setting the ordering of the variances of different\nimportance sampling estimators is instance-dependent, which brings up a dilemma\nas to which importance sampling weights to use. In this paper, we resolve this\ndilemma by finding the OPE estimator for multiple loggers with minimum variance\nfor any instance, i.e., the efficient one. In particular, we establish the\nefficiency bound under stratified sampling and propose an estimator achieving\nthis bound when given consistent $q$-estimates. To guard against\nmisspecification of $q$-functions, we also provide a way to choose the control\nvariate in a hypothesis class to minimize variance. Extensive experiments\ndemonstrate the benefits of our methods' efficiently leveraging of the\nstratified sampling of off-policy data from multiple loggers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 13:43:48 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Kallus", "Nathan", ""], ["Saito", "Yuta", ""], ["Uehara", "Masatoshi", ""]]}, {"id": "2010.11009", "submitter": "Ilyas Bakbergenuly", "authors": "Elena Kulinskaya, David C. Hoaglin, Joseph Newman, and Ilyas\n  Bakbergenuly", "title": "Simulations for a Q statistic with constant weights to assess\n  heterogeneity in meta-analysis of mean difference", "comments": "16 pages and Appendix with derivations and full simulation results,\n  comprising 60 figures, each presenting 12 combinations of sample sizes and\n  numbers of studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of problems in random-effects meta-analysis arise from the\nconventional $Q$ statistic, which uses estimated inverse-variance (IV) weights.\nIn previous work on standardized mean difference and log-odds-ratio, we found\nsuperior performance with an estimator of the overall effect whose weights use\nonly group-level sample sizes. The $Q$ statistic with those weights has the\nform proposed by DerSimonian and Kacker. The distribution of this $Q$ and the\n$Q$ with IV weights must generally be approximated. We investigate\napproximations for those distributions, as a basis for testing and estimating\nthe between-study variance ($\\tau^2$). Some approximations require the variance\nand third moment of $Q$, which we derive. We describe the design and results of\na simulation study, with mean difference as the effect measure, which provides\na framework for assessing accuracy of the approximations, level and power of\nthe tests, and bias in estimating $\\tau^2$. Use of $Q$ with sample-size-based\nweights and its exact distribution (available for mean difference and evaluated\nby Farebrother's algorithm) provides precise levels even for very small and\nunbalanced sample sizes. The corresponding estimator of $\\tau^2$ is almost\nunbiased for 10 or more small studies. Under these circumstances this\nperformance compares favorably with the extremely liberal behavior of the\nstandard tests of heterogeneity and the largely biased estimators based on\ninverse-variance weights.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 13:48:58 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Kulinskaya", "Elena", ""], ["Hoaglin", "David C.", ""], ["Newman", "Joseph", ""], ["Bakbergenuly", "Ilyas", ""]]}, {"id": "2010.11037", "submitter": "Sai Li", "authors": "Sai Li and T. Tony Cai and Hongzhe Li", "title": "Transfer Learning in Large-scale Gaussian Graphical Models with False\n  Discovery Rate Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning for high-dimensional Gaussian graphical models (GGMs) is\nstudied with the goal of estimating the target GGM by utilizing the data from\nsimilar and related auxiliary studies. The similarity between the target graph\nand each auxiliary graph is characterized by the sparsity of a divergence\nmatrix. An estimation algorithm, Trans-CLIME, is proposed and shown to attain a\nfaster convergence rate than the minimax rate in the single study setting.\nFurthermore, a debiased Trans-CLIME estimator is introduced and shown to be\nelement-wise asymptotically normal. It is used to construct a multiple testing\nprocedure for edge detection with false discovery rate control. The proposed\nestimation and multiple testing procedures demonstrate superior numerical\nperformance in simulations and are applied to infer the gene networks in a\ntarget brain tissue by leveraging the gene expressions from multiple other\nbrain tissues. A significant decrease in prediction errors and a significant\nincrease in power for link detection are observed.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 14:39:14 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Li", "Sai", ""], ["Cai", "T. Tony", ""], ["Li", "Hongzhe", ""]]}, {"id": "2010.11187", "submitter": "Leszek Szczecinski", "authors": "Leszek Szczecinski", "title": "Elo-MOV rating algorithm: Generalization of the Elo algorithm by\n  modelling the discretized Margin of Victory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop a new algorithm for rating of teams (or players) in\none-on-one games by exploiting the observed difference of the game-points (such\nas goals), also known as margin of victory (MOV). Our objective is to obtain\nthe Elo-style algorithm whose operation is simple to implement and to\nunderstand intuitively. This is done in three steps: first, we define the\nprobabilistic model between the teams' skills and the discretized margin of\nvictory (MOV) variable. We thus use a predefined number of discretization\ncategories, which generalizes the model underpinning the Elo algorithm, where\nthe MOV variable is discretized to three categories (win/loss/draw). Second,\nwith the formal probabilistic model at hand, the optimization required by the\nmaximum likelihood (ML) rule is implemented via stochastic gradient (SG); this\nyields a simple on-line rating updates which are identical in general form to\nthose of the Elo algorithm. The main difference lies in the way the scores and\nexpected scores are defined. Third, we propose a simple method to estimate the\ncoefficients of the model, and thus define the operation of the algorithm. This\nis done in closed form using the historical data so the algorithm is tailored\nto the sport of interest and the coefficients defining its operation are\ndetermined in entirely transparent manner. We show numerical examples based on\nthe results of ten seasons of the English Premier Ligue (EPL).\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 03:55:30 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Szczecinski", "Leszek", ""]]}, {"id": "2010.11330", "submitter": "Rachel Nethery", "authors": "Rachel C. Nethery, Nina Katz-Christy, Marianthi-Anna Kioumourtzoglou,\n  Robbie M. Parks, Andrea Schumacher, G. Brooke Anderson", "title": "Integrated causal-predictive machine learning models for tropical\n  cyclone epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strategic preparedness has been shown to reduce the adverse health impacts of\nhurricanes and tropical storms, referred to collectively as tropical cyclones\n(TCs), but its protective impact could be enhanced by a more comprehensive and\nrigorous characterization of TC epidemiology. To generate the insights and\ntools necessary for high-precision TC preparedness, we develop and apply a\nnovel Bayesian machine learning approach that standardizes estimation of\nhistoric TC health impacts, discovers common patterns and sources of\nheterogeneity in those health impacts, and enables identification of\ncommunities at highest health risk for future TCs. The model integrates (1) a\ncausal inference component to quantify the immediate health impacts of recent\nhistoric TCs at high spatial resolution and (2) a predictive component that\ncaptures how TC meteorological features and socioeconomic/demographic\ncharacteristics of impacted communities are associated with health impacts. We\napply it to a rich data platform containing detailed historic TC exposure\ninformation and Medicare claims data. The health outcomes used in our analyses\nare all-cause mortality and cardiovascular- and respiratory-related\nhospitalizations. We report a high degree of heterogeneity in the acute health\nimpacts of historic TCs at both the TC level and the community level, with\nsubstantial increases in respiratory hospitalizations, on average, during a\ntwo-week period surrounding TCs. TC sustained windspeeds are found to be the\nprimary driver of increased mortality and respiratory risk. Our modeling\napproach has broader utility for predicting the health impacts of many types of\nextreme climate events.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 22:06:08 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Nethery", "Rachel C.", ""], ["Katz-Christy", "Nina", ""], ["Kioumourtzoglou", "Marianthi-Anna", ""], ["Parks", "Robbie M.", ""], ["Schumacher", "Andrea", ""], ["Anderson", "G. Brooke", ""]]}, {"id": "2010.11332", "submitter": "Drew Dimmery", "authors": "David Arbour, Drew Dimmery, Anup Rao", "title": "Efficient Balanced Treatment Assignments for Experimentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we reframe the problem of balanced treatment assignment as\noptimization of a two-sample test between test and control units. Using this\nlens we provide an assignment algorithm that is optimal with respect to the\nminimum spanning tree test of Friedman and Rafsky (1979). This assignment to\ntreatment groups may be performed exactly in polynomial time. We provide a\nprobabilistic interpretation of this process in terms of the most probable\nelement of designs drawn from a determinantal point process which admits a\nprobabilistic interpretation of the design. We provide a novel formulation of\nestimation as transductive inference and show how the tree structures used in\ndesign can also be used in an adjustment estimator. We conclude with a\nsimulation study demonstrating the improved efficacy of our method.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 22:06:37 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Arbour", "David", ""], ["Dimmery", "Drew", ""], ["Rao", "Anup", ""]]}, {"id": "2010.11367", "submitter": "Charilaos Kanatsoulis", "authors": "Charilaos I. Kanatsoulis, and Nicholas D. Sidiropoulos", "title": "TeX-Graph: Coupled tensor-matrix knowledge-graph embedding for COVID-19\n  drug repurposing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs) are powerful tools that codify relational behaviour\nbetween entities in knowledge bases. KGs can simultaneously model many\ndifferent types of subject-predicate-object and higher-order relations. As\nsuch, they offer a flexible modeling framework that has been applied to many\nareas, including biology and pharmacology -- most recently, in the fight\nagainst COVID-19. The flexibility of KG modeling is both a blessing and a\nchallenge from the learning point of view. In this paper we propose a novel\ncoupled tensor-matrix framework for KG embedding. We leverage tensor\nfactorization tools to learn concise representations of entities and relations\nin knowledge bases and employ these representations to perform drug repurposing\nfor COVID-19. Our proposed framework is principled, elegant, and achieves 100%\nimprovement over the best baseline in the COVID-19 drug repurposing task using\na recently developed biological KG.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 01:12:54 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 23:44:06 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kanatsoulis", "Charilaos I.", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "2010.11368", "submitter": "Terezinha Ribeiro", "authors": "Terezinha K. A. Ribeiro and Silvia L.P. Ferrari", "title": "Robust estimation in beta regression via maximum Lq-likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beta regression models are widely used for modeling continuous data limited\nto the unit interval, such as proportions, fractions, and rates. The inference\nfor the parameters of beta regression models is commonly based on maximum\nlikelihood estimation. However, it is known to be sensitive to discrepant\nobservations. In some cases, one atypical data point can lead to severe bias\nand erroneous conclusions about the features of interest. In this work, we\ndevelop a robust estimation procedure for beta regression models based on the\nmaximization of a reparameterized Lq-likelihood. The new estimator offers a\ntrade-off between robustness and efficiency through a tuning constant. To\nselect the optimal value of the tuning constant, we propose a data-driven\nmethod which ensures full efficiency in the absence of outliers. We also\nimprove on an alternative robust estimator by applying our data-driven method\nto select its optimum tuning constant. Monte Carlo simulations suggest marked\nrobustness of the two robust estimators with little loss of efficiency.\nApplications to three datasets are presented and discussed. As a by-product of\nthe proposed methodology, residual diagnostic plots based on robust fits\nhighlight outliers that would be masked under maximum likelihood estimation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 01:19:54 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Ribeiro", "Terezinha K. A.", ""], ["Ferrari", "Silvia L. P.", ""]]}, {"id": "2010.11385", "submitter": "Dawei Ding", "authors": "Dawei Ding, George Karabatsos", "title": "Dirichlet Process Mixture Models with Shrinkage Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Dirichlet Process Mixture (DPM) models for prediction and\ncluster-wise variable selection, based on two choices of shrinkage baseline\nprior distributions for the linear regression coefficients, namely the\nHorseshoe prior and Normal-Gamma prior. We show in a simulation study that each\nof the two proposed DPM models tend to outperform the standard DPM model based\non the non-shrinkage normal prior, in terms of predictive, variable selection,\nand clustering accuracy. This is especially true for the Horseshoe model, and\nwhen the number of covariates exceeds the within-cluster sample size. A real\ndata set is analyzed to illustrate the proposed modeling methodology, where\nboth proposed DPM models again attained better predictive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 02:22:10 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 03:55:10 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 17:24:59 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Ding", "Dawei", ""], ["Karabatsos", "George", ""]]}, {"id": "2010.11417", "submitter": "Daisuke Nagakura", "authors": "Daisuke Nagakura", "title": "Positive definiteness of the asymptotic covariance matrix of OLS\n  estimators in parsimonious regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Ghysels, Hill, and Motegi (2020) proposed a test for examining\nwhether a large number of coefficients in linear regression models is zero. The\ntest is called the max test. The test statistic is calculated by first running\nmultiple ordinary least squares (OLS) regressions, each including only one of\nkey regressors, whose coefficients are supposed to be zero under the null, and\nthen taking the maximum value of the squared OLS coefficient estimates of those\nkey regressors. They called these regressions parsimonious regressions. This\npaper answers a question raised in their Remark 2.4; whether the asymptotic\ncovariance matrix of the OLS estimators in the parsimonious regressions is\ngenerally positive definite. The paper shows that it is generally positive\ndefinite, and the result may be utilized to facilitate the calculation of the\nsimulated p value necessary for implementing the max test.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 03:48:20 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 02:22:16 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Nagakura", "Daisuke", ""]]}, {"id": "2010.11449", "submitter": "Andrew Song", "authors": "Andrew H. Song, Demba Ba, Emery N. Brown", "title": "PLSO: A generative framework for decomposing nonstationary time-series\n  into piecewise stationary oscillatory components", "comments": "Uncertainty in Artificial Intelligence (UAI), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To capture the slowly time-varying spectral content of real-world\ntime-series, a common paradigm is to partition the data into approximately\nstationary intervals and perform inference in the time-frequency domain.\nHowever, this approach lacks a corresponding nonstationary time-domain\ngenerative model for the entire data and thus, time-domain inference occurs in\neach interval separately. This results in distortion/discontinuity around\ninterval boundaries and can consequently lead to erroneous inferences based on\nany quantities derived from the posterior, such as the phase. To address these\nshortcomings, we propose the Piecewise Locally Stationary Oscillation (PLSO)\nmodel for decomposing time-series data with slowly time-varying spectra into\nseveral oscillatory, piecewise-stationary processes. PLSO, as a nonstationary\ntime-domain generative model, enables inference on the entire time-series\nwithout boundary effects and simultaneously provides a characterization of its\ntime-varying spectral properties. We also propose a novel two-stage inference\nalgorithm that combines Kalman theory and an accelerated proximal gradient\nalgorithm. We demonstrate these points through experiments on simulated data\nand real neural data from the rat and the human brain.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 05:17:07 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 20:17:34 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 15:32:34 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Song", "Andrew H.", ""], ["Ba", "Demba", ""], ["Brown", "Emery N.", ""]]}, {"id": "2010.11470", "submitter": "Nicolas Verzelen", "authors": "Nicolas Verzelen, Magalie Fromont, Matthieu Lerasle, and Patricia\n  Reynaud-Bouret", "title": "Optimal Change-Point Detection and Localization", "comments": "73 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a times series ${\\bf Y}$ in $\\mathbb{R}^n$, with a piece-wise contant\nmean and independent components, the twin problems of change-point detection\nand change-point localization respectively amount to detecting the existence of\ntimes where the mean varies and estimating the positions of those\nchange-points. In this work, we tightly characterize optimal rates for both\nproblems and uncover the phase transition phenomenon from a global testing\nproblem to a local estimation problem. Introducing a suitable definition of the\nenergy of a change-point, we first establish in the single change-point setting\nthat the optimal detection threshold is $\\sqrt{2\\log\\log(n)}$. When the energy\nis just above the detection threshold, then the problem of localizing the\nchange-point becomes purely parametric: it only depends on the difference in\nmeans and not on the position of the change-point anymore. Interestingly, for\nmost change-point positions, it is possible to detect and localize them at a\nmuch smaller energy level. In the multiple change-point setting, we establish\nthe energy detection threshold and show similarly that the optimal localization\nerror of a specific change-point becomes purely parametric. Along the way,\ntight optimal rates for Hausdorff and $l_1$ estimation losses of the vector of\nall change-points positions are also established. Two procedures achieving\nthese optimal rates are introduced. The first one is a least-squares estimator\nwith a new multiscale penalty that favours well spread change-points. The\nsecond one is a two-step multiscale post-processing procedure whose\ncomputational complexity can be as low as $O(n\\log(n))$. Notably, these two\nprocedures accommodate with the presence of possibly many low-energy and\ntherefore undetectable change-points and are still able to detect and localize\nhigh-energy change-points even with the presence of those nuisance parameters.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 06:26:01 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 23:57:39 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Verzelen", "Nicolas", ""], ["Fromont", "Magalie", ""], ["Lerasle", "Matthieu", ""], ["Reynaud-Bouret", "Patricia", ""]]}, {"id": "2010.11665", "submitter": "Kolyan Ray", "authors": "Kolyan Ray, Botond Szabo, Gabriel Clara", "title": "Spike and slab variational Bayes for high dimensional logistic\n  regression", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) is a popular scalable alternative to Markov chain\nMonte Carlo for Bayesian inference. We study a mean-field spike and slab VB\napproximation of widely used Bayesian model selection priors in sparse\nhigh-dimensional logistic regression. We provide non-asymptotic theoretical\nguarantees for the VB posterior in both $\\ell_2$ and prediction loss for a\nsparse truth, giving optimal (minimax) convergence rates. Since the VB\nalgorithm does not depend on the unknown truth to achieve optimality, our\nresults shed light on effective prior choices. We confirm the improved\nperformance of our VB algorithm over common sparse VB approaches in a numerical\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 12:49:58 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Ray", "Kolyan", ""], ["Szabo", "Botond", ""], ["Clara", "Gabriel", ""]]}, {"id": "2010.11783", "submitter": "Robert Jack", "authors": "Yuting I. Li, G\\\"unther Turk, Paul B. Rohrbach, Patrick Pietzonka,\n  Julian Kappler, Rajesh Singh, Jakub Dolezal, Timothy Ekeh, Lukas Kikuchi,\n  Joseph D. Peterson, Hideki Kobayashi, Michael E. Cates, R. Adhikari, Robert\n  L. Jack", "title": "Efficient Bayesian inference of fully stochastic epidemiological models\n  with applications to COVID-19", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemiological forecasts are beset by uncertainties in the generative model\nfor the disease, and the surveillance process through which data are acquired.\nWe present a Bayesian inference methodology that quantifies these\nuncertainties, for epidemics that are modelled by (possibly) non-stationary,\ncontinuous-time, Markov population processes. The efficiency of the method\nderives from a functional central limit theorem approximation of the\nlikelihood, valid for large populations. We demonstrate the methodology by\nanalysing the early stages of the COVID-19 pandemic in the UK, based on\nage-structured data for the number of deaths. This includes maximum a\nposteriori estimates, MCMC sampling of the posterior, computation of the model\nevidence, and the determination of parameter sensitivities via the Fisher\ninformation matrix. Our methodology is implemented in PyRoss, an open-source\nplatform for analysis of epidemiological compartment models.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 11:46:14 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Li", "Yuting I.", ""], ["Turk", "G\u00fcnther", ""], ["Rohrbach", "Paul B.", ""], ["Pietzonka", "Patrick", ""], ["Kappler", "Julian", ""], ["Singh", "Rajesh", ""], ["Dolezal", "Jakub", ""], ["Ekeh", "Timothy", ""], ["Kikuchi", "Lukas", ""], ["Peterson", "Joseph D.", ""], ["Kobayashi", "Hideki", ""], ["Cates", "Michael E.", ""], ["Adhikari", "R.", ""], ["Jack", "Robert L.", ""]]}, {"id": "2010.11826", "submitter": "Sophie Mathieu", "authors": "Sophie Mathieu and Rainer von Sachs and V\\'eronique Delouille and\n  Laure Lef\\`evre and Christian Ritter", "title": "Nonparametric robust monitoring of time series panel data", "comments": "58 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, a control procedure is required to detect potential\ndeviations in a panel of serially correlated processes. It is common that the\nprocesses are corrupted by noise and that no prior information about the\nin-control data are available for that purpose. This paper suggests a general\nnonparametric monitoring scheme for supervising such a panel with time-varying\nmean and variance. The method is based on a control chart designed by block\nbootstrap, which does not require parametric assumptions on the distribution of\nthe data. The procedure is tailored to cope with strong noise, potentially\nmissing values and absence of in-control series, which is tackled by an\nintelligent exploitation of the information in the panel. Our methodology is\ncompleted by support vector machine procedures to estimate magnitude and form\nof the encountered deviations (such as stepwise shifts or functional drifts).\nThis scheme, though generic in nature, is able to treat an important applied\ndata problem: the control of deviations in a subset of sunspot number\nobservations which are part of the International Sunspot Number, a world\nreference for long-term solar activity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 16:03:58 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mathieu", "Sophie", ""], ["von Sachs", "Rainer", ""], ["Delouille", "V\u00e9ronique", ""], ["Lef\u00e8vre", "Laure", ""], ["Ritter", "Christian", ""]]}, {"id": "2010.11850", "submitter": "Samuel Watson", "authors": "Samuel I. Watson", "title": "Efficient design of geographically-defined clusters with spatial\n  autocorrelation", "comments": null, "journal-ref": null, "doi": "10.1080/02664763.2021.1941807", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clusters form the basis of a number of research study designs including\nsurvey and experimental studies. Cluster-based designs can be less costly but\nalso less efficient than individual-based designs due to correlation between\nindividuals within the same cluster. Their design typically relies on\n\\textit{ad hoc} choices of correlation parameters, and is insensitive to\nvariations in cluster design. This article examines how to efficiently design\nclusters where they are geographically defined by demarcating areas\nincorporating individuals and households or other units. Using geostatistical\nmodels for spatial autocorrelation we generate approximations to within cluster\naverage covariance in order to estimate the effective sample size given\nparticular cluster design parameters. We show how the number of enumerated\nlocations, cluster area, proportion sampled, and sampling method affect the\nefficiency of the design and consider the optimization problem of choosing the\nmost efficient design subject to budgetary constraints. We also consider how\nthe parameters from these approximations can be interpreted simply in terms of\n`real-world' quantities and used in design analysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 16:42:10 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Watson", "Samuel I.", ""]]}, {"id": "2010.12178", "submitter": "Cheng Meng", "authors": "Cheng Meng, Rui Xie, Abhyuday Mandal, Xinlian Zhang, Wenxuan Zhong,\n  and Ping Ma", "title": "LowCon: A design-based subsampling approach in a misspecified linear\n  modeL", "comments": "37pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a measurement constrained supervised learning problem, that is,\n(1) full sample of the predictors are given; (2) the response observations are\nunavailable and expensive to measure. Thus, it is ideal to select a subsample\nof predictor observations, measure the corresponding responses, and then fit\nthe supervised learning model on the subsample of the predictors and responses.\nHowever, model fitting is a trial and error process, and a postulated model for\nthe data could be misspecified. Our empirical studies demonstrate that most of\nthe existing subsampling methods have unsatisfactory performances when the\nmodels are misspecified. In this paper, we develop a novel subsampling method,\ncalled \"LowCon\", which outperforms the competing methods when the working\nlinear model is misspecified. Our method uses orthogonal Latin hypercube\ndesigns to achieve a robust estimation. We show that the proposed design-based\nestimator approximately minimizes the so-called \"worst-case\" bias with respect\nto many possible misspecification terms. Both the simulated and real-data\nanalyses demonstrate the proposed estimator is more robust than several\nsubsample least squares estimators obtained by state-of-the-art subsampling\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 05:56:05 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Meng", "Cheng", ""], ["Xie", "Rui", ""], ["Mandal", "Abhyuday", ""], ["Zhang", "Xinlian", ""], ["Zhong", "Wenxuan", ""], ["Ma", "Ping", ""]]}, {"id": "2010.12185", "submitter": "Chul Moon", "authors": "Chul Moon, Xinlei Wang, Johan Lim", "title": "Empirical Likelihood Inference for Area under the ROC Curve using Ranked\n  Set Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The area under a receiver operating characteristic curve (AUC) is a useful\ntool to assess the performance of continuous-scale diagnostic tests on binary\nclassification. In this article, we propose an empirical likelihood (EL) method\nto construct confidence intervals for the AUC from data collected by ranked set\nsampling (RSS). The proposed EL-based method enables inferences without\nassumptions required in existing nonparametric methods and takes advantage of\nthe sampling efficiency of RSS. We show that for both balanced and unbalanced\nRSS, the EL-based point estimate is the Mann-Whitney statistic, and confidence\nintervals can be obtained from a scaled chi-square distribution. Simulation\nstudies and two case studies on diabetes and chronic kidney disease data show\nthat the proposed method outperforms the existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 06:30:49 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 15:44:44 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 02:01:56 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Moon", "Chul", ""], ["Wang", "Xinlei", ""], ["Lim", "Johan", ""]]}, {"id": "2010.12298", "submitter": "Matthias Holschneider", "authors": "M. Holschneider, K. Ferrat, G. Z\\\"oller, Ch. Molkenthin", "title": "Richter b-value maps from local moments of seismicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a technique to estimate spatially varying seismicity patterns. It\nis based on a Gaussian approximation of the underlying Poisson Process. A link\nfunction is used to estimate local moments of the seismicity from observed\ncatalogues. These are modeled by a nonstationary Gaussian field. We construct a\nprior based on the local distribution of seismic faults. This allows us to\nincorporate geological information into the Bayesian inversion of the observed\nseismicity. In this paper we limit ourselve to the $b$-value field for which we\ncompute the posterior expectations as well as the uncertainties. The technique\nhowever may be applied to other seismically relevant parameters like Omori $c$\nand $p$-values.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 11:00:22 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Holschneider", "M.", ""], ["Ferrat", "K.", ""], ["Z\u00f6ller", "G.", ""], ["Molkenthin", "Ch.", ""]]}, {"id": "2010.12383", "submitter": "Panayiota Touloupou", "authors": "Panayiota Touloupou, Renata Retkute, T Deirdre Hollingsworth, Simon E.\n  F. Spencer", "title": "Statistical methods for linking geostatistical maps and transmission\n  models: Application to lymphatic filariasis in East Africa", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious diseases remain one of the major causes of human mortality and\nsuffering. Mathematical models have been established as an important tool for\ncapturing the features that drive the spread of the disease, predicting the\nprogression of an epidemic and hence guiding the development of strategies to\ncontrol it. Another important area of epidemiological interest is the\ndevelopment of geostatistical methods for the analysis of data from spatially\nreferenced prevalence surveys. Maps of prevalence are useful, not only for\nenabling a more precise disease risk stratification, but also for guiding the\nplanning of more reliable spatial control programmes by identifying affected\nareas. Despite the methodological advances that have been made in each area\nindependently, efforts to link transmission models and geostatistical maps have\nbeen limited. Motivated by this fact, we developed a Bayesian approach that\ncombines fine-scale geostatistical maps of disease prevalence with transmission\nmodels to provide quantitative, spatially explicit projections of the current\nand future impact of control programs against a disease. These estimates can\nthen be used at a local level to identify the effectiveness of suggested\nintervention schemes and allow investigation of alternative strategies. The\nmethodology has been applied to lymphatic filariasis in East Africa to provide\nestimates of the impact of different intervention strategies against the\ndisease.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 12:46:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Touloupou", "Panayiota", ""], ["Retkute", "Renata", ""], ["Hollingsworth", "T Deirdre", ""], ["Spencer", "Simon E. F.", ""]]}, {"id": "2010.12521", "submitter": "Luca Merlo", "authors": "Antonello Maruotti, Luca Merlo and Lea Petrella", "title": "A two-part finite mixture quantile regression model for semi-continuous\n  longitudinal data", "comments": null, "journal-ref": "Statistical Modelling (2021): 1471082X21993603", "doi": "10.1177/1471082X21993603", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a two-part finite mixture quantile regression model for\nsemi-continuous longitudinal data. The proposed methodology allows\nheterogeneity sources that influence the model for the binary response\nvariable, to influence also the distribution of the positive outcomes. As is\ncommon in the quantile regression literature, estimation and inference on the\nmodel parameters are based on the Asymmetric Laplace distribution. Maximum\nlikelihood estimates are obtained through the EM algorithm without parametric\nassumptions on the random effects distribution. In addition, a penalized\nversion of the EM algorithm is presented to tackle the problem of variable\nselection. The proposed statistical method is applied to the well-known RAND\nHealth Insurance Experiment dataset which gives further insights on its\nempirical behavior.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:37:08 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Maruotti", "Antonello", ""], ["Merlo", "Luca", ""], ["Petrella", "Lea", ""]]}, {"id": "2010.12522", "submitter": "Christophe Ley", "authors": "Fatemeh Ghaderinezhad, Christophe Ley and Ben Serrien", "title": "The Wasserstein Impact Measure (WIM): a generally applicable, practical\n  tool for quantifying prior impact in Bayesian statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prior distribution is a crucial building block in Bayesian analysis, and\nits choice will impact the subsequent inference. It is therefore important to\nhave a convenient way to quantify this impact, as such a measure of prior\nimpact will help us to choose between two or more priors in a given situation.\nA recently proposed approach consists in determining the Wasserstein distance\nbetween posteriors resulting from two distinct priors, revealing how close or\ndistant they are. In particular, if one prior is the uniform/flat prior, this\ndistance leads to a genuine measure of prior impact for the other prior. While\nhighly appealing and successful from a theoretical viewpoint, this proposal\nsuffers from practical limitations: it requires prior distributions to be\nnested, posterior distributions should not be of a too complex form, in most\nconsidered settings the exact distance was not computed but sharp upper and\nlower bounds were proposed, and the proposal so far is restricted to scalar\nparameter settings. In this paper, we overcome all these limitations by\nintroducing a practical version of this theoretical approach, namely the\nWasserstein Impact Measure (WIM). In three simulated scenarios, we will compare\nthe WIM to the theoretical Wasserstein approach, as well as to two competitor\nprior impact measures from the literature. We finally illustrate the\nversatility of the WIM by applying it on two datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:42:35 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ghaderinezhad", "Fatemeh", ""], ["Ley", "Christophe", ""], ["Serrien", "Ben", ""]]}, {"id": "2010.12568", "submitter": "Devin Johnson", "authors": "Devin S. Johnson, Brian M. Brost and Mevin B. Hooten", "title": "Greater Than the Sum of its Parts: Computationally Flexible Bayesian\n  Hierarchical Modeling", "comments": "32 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a multistage method for making inference at all levels of a\nBayesian hierarchical model (BHM) using natural data partitions to increase\nefficiency by allowing computations to take place in parallel form using\nsoftware that is most appropriate for each data partition. The full\nhierarchical model is then approximated by the product of independent normal\ndistributions for the data component of the model. In the second stage, the\nBayesian maximum {\\it a posteriori} (MAP) estimator is found by maximizing the\napproximated posterior density with respect to the parameters. If the\nparameters of the model can be represented as normally distributed random\neffects then the second stage optimization is equivalent to fitting a\nmultivariate normal linear mixed model. This method can be extended to account\nfor common fixed parameters shared between data partitions, as well as\nparameters that are distinct between partitions. In the case of distinct\nparameter estimation, we consider a third stage that re-estimates the distinct\nparameters for each data partition based on the results of the second stage.\nThis allows more information from the entire data set to properly inform the\nposterior distributions of the distinct parameters. The method is demonstrated\nwith two ecological data sets and models, a random effects GLM and an\nIntegrated Population Model (IPM). The multistage results were compared to\nestimates from models fit in single stages to the entire data set. Both\nexamples demonstrate that multistage point and posterior standard deviation\nestimates closely approximate those obtained from fitting the models with all\ndata simultaneously and can therefore be considered for fitting hierarchical\nBayesian models when it is computationally prohibitive to do so in one step.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:53:56 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Johnson", "Devin S.", ""], ["Brost", "Brian M.", ""], ["Hooten", "Mevin B.", ""]]}, {"id": "2010.12580", "submitter": "Hamid Eftekhari", "authors": "Hamid Eftekhari, Moulinath Banerjee, Ya'acov Ritov", "title": "Design of $c$-Optimal Experiments for High dimensional Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study random designs that minimize the asymptotic variance of a de-biased\nlasso estimator when a large pool of unlabeled data is available but measuring\nthe corresponding responses is costly. The optimal sampling distribution arises\nas the solution of a semidefinite program. The improvements in efficiency that\nresult from these optimal designs are demonstrated via simulation experiments.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:31:27 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Eftekhari", "Hamid", ""], ["Banerjee", "Moulinath", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "2010.12696", "submitter": "Xiaotian Zheng", "authors": "Xiaotian Zheng, Athanasios Kottas and Bruno Sans\\'o", "title": "On Construction and Estimation of Stationary Mixture Transition\n  Distribution Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture transition distribution time series models build high-order\ndependence through a weighted combination of first-order transition densities\nfor each one of a specified number of lags. We present a framework to construct\nstationary transition mixture distribution models that extend beyond linear,\nGaussian dynamics. We study conditions for first-order strict stationarity\nwhich allow for different constructions with either continuous or discrete\nfamilies for the first-order transition densities given a pre-specified family\nfor the marginal density, and with general forms for the resulting conditional\nexpectations. Inference and prediction are developed under the Bayesian\nframework with particular emphasis on flexible, structured priors for the\nmixture weights. Model properties are investigated both analytically and\nthrough synthetic data examples. Finally, Poisson and Lomax examples are\nillustrated through real data applications.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 22:44:10 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 23:35:07 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zheng", "Xiaotian", ""], ["Kottas", "Athanasios", ""], ["Sans\u00f3", "Bruno", ""]]}, {"id": "2010.12705", "submitter": "Mohsen Soltanifar", "authors": "Mohsen Soltanifar, Michael Escobar, Annie Dupuis, and Russell Schachar", "title": "A Bayesian Mixture Modelling of Stop Signal Reaction Time Distributions", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The distribution of single Stop Signal Reaction Times (SSRT) in the stop\nsignal task (SST) as a measurement of the latency of the unobservable stopping\nprocess has been modeled with a nonparametric method by Hans Colonius (1990)\nand with a Bayesian parametric method by Eric-Jan Wagenmakers and colleagues\n(2012). These methods assume equal impact of the preceding trial type (go/stop)\nin the SST trials on the SSRT distributional estimation without addressing the\ncase of the violated assumption. This study presents the required model by\nconsidering two-state mixture model for the SSRT distribution. It then compares\nthe Bayesian parametric single SSRT and mixture SSRT distributions in the usual\nstochastic order at the individual and the population level under the\nex-Gaussian distributional format. It shows that compared to a single SSRT\ndistribution, the mixture SSRT distribution is more diverse, more positively\nskewed, more leptokurtic, and larger in stochastic order. The size of the\ndisparities in the results also depends on the choice of weights in the mixture\nSSRT distribution. This study confirms that mixture SSRT indices as a constant\nor distribution are significantly larger than their single SSRT counterparts in\nthe related order. This offers a vital improvement in the SSRT estimations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 23:20:42 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 23:19:47 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Soltanifar", "Mohsen", ""], ["Escobar", "Michael", ""], ["Dupuis", "Annie", ""], ["Schachar", "Russell", ""]]}, {"id": "2010.12833", "submitter": "Georgia Papacharalampous", "authors": "Georgia Papacharalampous, Hristos Tyralis, Simon Michael Papalexiou,\n  Andreas Langousis, Sina Khatami, Elena Volpi, Salvatore Grimaldi", "title": "Global-scale massive feature extraction from monthly hydroclimatic time\n  series: Statistical characterizations, spatial patterns and hydrological\n  similarity", "comments": null, "journal-ref": "Science of the Total Environment 767 (2021) 144612", "doi": "10.1016/j.scitotenv.2020.144612", "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hydroclimatic time series analysis focuses on a few feature types (e.g.,\nautocorrelations, trends, extremes), which describe a small portion of the\nentire information content of the observations. Aiming to exploit a larger part\nof the available information and, thus, to deliver more reliable results (e.g.,\nin hydroclimatic time series clustering contexts), here we approach\nhydroclimatic time series analysis differently, i.e., by performing massive\nfeature extraction. In this respect, we develop a big data framework for\nhydroclimatic variable behaviour characterization. This framework relies on\napproximately 60 diverse features and is completely automatic (in the sense\nthat it does not depend on the hydroclimatic process at hand). We apply the new\nframework to characterize mean monthly temperature, total monthly precipitation\nand mean monthly river flow. The applications are conducted at the global scale\nby exploiting 40-year-long time series originating from over 13 000 stations.\nWe extract interpretable knowledge on seasonality, trends, autocorrelation,\nlong-range dependence and entropy, and on feature types that are met less\nfrequently. We further compare the examined hydroclimatic variable types in\nterms of this knowledge and, identify patterns related to the spatial\nvariability of the features. For this latter purpose, we also propose and\nexploit a hydroclimatic time series clustering methodology. This new\nmethodology is based on Breiman's random forests. The descriptive and\nexploratory insights gained by the global-scale applications prove the\nusefulness of the adopted feature compilation in hydroclimatic contexts.\nMoreover, the spatially coherent patterns characterizing the clusters delivered\nby the new methodology build confidence in its future exploitation...\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 08:27:17 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 23:31:22 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Papacharalampous", "Georgia", ""], ["Tyralis", "Hristos", ""], ["Papalexiou", "Simon Michael", ""], ["Langousis", "Andreas", ""], ["Khatami", "Sina", ""], ["Volpi", "Elena", ""], ["Grimaldi", "Salvatore", ""]]}, {"id": "2010.12895", "submitter": "Tao Li", "authors": "Jiyanglin Li and Tao Li", "title": "Some Theoretical Results Concerning Time-varying Nonparametric\n  Regression with Local Stationary Regressors and Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With regard to a three-step estimation procedure, proposed without\ntheoretical discussion by Li and You in Journal of Applied Statistics and\nManagement, for a nonparametric regression model with time-varying regression\nfunction, local stationary regressors and time-varying AR(p) (tvAR(p)) error\nprocess , we established all necessary asymptotic properties for each of\nestimator. We derive the convergence rate and asymptotic normality of the\npreliminary estimation of nonparametric regression function, establish the\nasymptotic distribution of time-varying coefficient functions in the error\nterm, and present the asymptotic property of the refined estimation of\nnonparametric regression function. In addition, with regard to the ULASSO\nmethod for variable selection and constant coefficient detection for error term\nstructure, we show that the ULASSO estimator can identify the true error term\nstructure consistently. We conduct two simulation studies to illustrate the\nfinite sample performance of the estimators and validate our theoretical\ndiscussion on the properties of the estimators.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 13:17:30 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Li", "Jiyanglin", ""], ["Li", "Tao", ""]]}, {"id": "2010.13229", "submitter": "Marina Vannucci", "authors": "Nathan Osborne, Christine B. Peterson, and Marina Vannucci", "title": "Latent Network Estimation and Variable Selection for Compositional Data\n  via Variational EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network estimation and variable selection have been extensively studied in\nthe statistical literature, but only recently have those two challenges been\naddressed simultaneously. In this paper, we seek to develop a novel method to\nsimultaneously estimate network interactions and associations to relevant\ncovariates for count data, and specifically for compositional data, which have\na fixed sum constraint. We use a hierarchical Bayesian model with latent layers\nand employ spike-and-slab priors for both edge and covariate selection. For\nposterior inference, we develop a novel variational inference scheme with an\nexpectation maximization step, to enable efficient estimation. Through\nsimulation studies, we demonstrate that the proposed model outperforms existing\nmethods in its accuracy of network recovery. We show the practical utility of\nour model via an application to microbiome data. The human microbiome has been\nshown to contribute to many of the functions of the human body, and also to be\nlinked with a number of diseases. In our application, we seek to better\nunderstand the interaction between microbes and relevant covariates, as well as\nthe interaction of microbes with each other. We provide a Python implementation\nof our algorithm, called SINC (Simultaneous Inference for Networks and\nCovariates), available online.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 21:52:39 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 02:44:02 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Osborne", "Nathan", ""], ["Peterson", "Christine B.", ""], ["Vannucci", "Marina", ""]]}, {"id": "2010.13325", "submitter": "Jin Liu", "authors": "Jin Liu, Robert A. Perera", "title": "Applying Growth Mixture Model to Assess Heterogeneity in Joint\n  Development with Nonlinear Trajectories in the Framework of Individual\n  Measurement Occasions", "comments": "Draft version 1.3, 06/01/2021. This paper has not been peer reviewed.\n  Please do not copy or cite without author's permission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers continue to be interested in exploring the effects that\ncovariates have on the heterogeneity in trajectories. The inclusion of\ncovariates associated with latent classes allows for a more clear understanding\nof individual differences and a more meaningful interpretation of latent class\nmembership. Many theoretical and empirical studies have focused on\ninvestigating heterogeneity in change patterns of a univariate repeated outcome\nand examining the effects on baseline covariates that inform the cluster\nformation. However, developmental processes rarely unfold in isolation;\ntherefore, empirical researchers often desire to examine two or more outcomes\nover time, hoping to understand their joint development where these outcomes\nand their change patterns are correlated. This study examines the heterogeneity\nin parallel nonlinear trajectories and identifies baseline characteristics as\npredictors of latent classes. Our simulation studies show that the proposed\nmodel can tell the clusters of parallel trajectories apart and provide unbiased\nand accurate point estimates with target coverage probabilities for the\nparameters of interest in general. We illustrate how to apply the model to\ninvestigate the heterogeneity in the joint development of reading and\nmathematics ability from Grade K to 5. In this real-world example, we also\ndemonstrate how to select covariates that contribute the most to the latent\nclasses and transform candidate covariates from a large set into a more\nmanageable set with retaining the meaningful properties of the original set in\nthe structural equation modeling framework.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 15:32:17 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 00:52:49 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 00:27:54 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liu", "Jin", ""], ["Perera", "Robert A.", ""]]}, {"id": "2010.13332", "submitter": "Tianchen Xu", "authors": "Tianchen Xu, Kun Chen, Gen Li", "title": "The More Data, the Better? Demystifying Deletion-Based Methods in Linear\n  Regression with Missing Data", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare two deletion-based methods for dealing with the problem of missing\nobservations in linear regression analysis. One is the complete-case analysis\n(CC, or listwise deletion) that discards all incomplete observations and only\nuses common samples for ordinary least-squares estimation. The other is the\navailable-case analysis (AC, or pairwise deletion) that utilizes all available\ndata to estimate the covariance matrices and applies these matrices to\nconstruct the normal equation. We show that the estimates from both methods are\nasymptotically unbiased and further compare their asymptotic variances in some\ntypical situations. Surprisingly, using more data (i.e., AC) does not\nnecessarily lead to better asymptotic efficiency in many scenarios. Missing\npatterns, covariance structure and true regression coefficient values all play\na role in determining which is better. We further conduct simulation studies to\ncorroborate the findings and demystify what has been missed or misinterpreted\nin the literature. Some detailed proofs and simulation results are available in\nthe online supplemental materials.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 04:25:53 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Xu", "Tianchen", ""], ["Chen", "Kun", ""], ["Li", "Gen", ""]]}, {"id": "2010.13440", "submitter": "Federico Ferraccioli", "authors": "Federico Ferraccioli and Giovanna Menardi", "title": "Modal clustering of matrix-variate data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonparametric formulation of density-based clustering, known as modal\nclustering, draws a correspondence between groups and the attraction domains of\nthe modes of the density function underlying the data. Its probabilistic\nfoundation allows for a natural, yet not trivial, generalization of the\napproach to the matrix-valued setting, increasingly widespread, for example, in\nlongitudinal and multivariate spatio-temporal studies. In this work we\nintroduce nonparametric estimators of matrix-variate distributions based on\nkernel methods, and analyze their asymptotic properties. Additionally, we\npropose a generalization of the mean-shift procedure for the identification of\nthe modes of the estimated density. Given the intrinsic high dimensionality of\nmatrix-variate data, we discuss some locally adaptive solutions to handle the\nproblem. We test the procedure via extensive simulations, also with respect to\nsome competitors, and illustrate its performance through two high-dimensional\nreal data applications.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:25:39 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ferraccioli", "Federico", ""], ["Menardi", "Giovanna", ""]]}, {"id": "2010.13452", "submitter": "Hawre Jalal", "authors": "Hawre Jalal and Fernando Alarid-Escudero", "title": "BayCANN: Streamlining Bayesian Calibration with Artificial Neural\n  Network Metamodeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Bayesian calibration is theoretically superior to standard\ndirect-search algorithm because it can reveal the full joint posterior\ndistribution of the calibrated parameters. However, to date, Bayesian\ncalibration has not been used often in health decision sciences due to\npractical and computational burdens. In this paper we propose to use artificial\nneural networks (ANN) as one solution to these limitations.\n  Methods: Bayesian Calibration using Artificial Neural Networks (BayCANN)\ninvolves (1) training an ANN metamodel on a sample of model inputs and outputs,\nand (2) then calibrating the trained ANN metamodel instead of the full model in\na probabilistic programming language to obtain the posterior joint distribution\nof the calibrated parameters. We demonstrate BayCANN by calibrating a natural\nhistory model of colorectal cancer to adenoma prevalence and cancer incidence\ndata. In addition, we compare the efficiency and accuracy of BayCANN against\nperforming a Bayesian calibration directly on the simulation model using an\nincremental mixture importance sampling (IMIS) algorithm.\n  Results: BayCANN was generally more accurate than IMIS in recovering the\n\"true\" parameter values. The ratio of the absolute ANN deviation from the truth\ncompared to IMIS for eight out of the nine calibrated parameters were less than\none indicating that BayCANN was more accurate than IMIS. In addition, BayCANN\ntook about 15 minutes total compared to the IMIS method which took 80 minutes.\n  Conclusions: In our case study, BayCANN was more accurate than IMIS and was\nfive-folds faster. Because BayCANN does not depend on the structure of the\nsimulation model, it can be adapted to models of various levels of complexity\nwith minor changes to its structure. We provide BayCANN's open-source\nimplementation in R.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:47:39 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Jalal", "Hawre", ""], ["Alarid-Escudero", "Fernando", ""]]}, {"id": "2010.13456", "submitter": "Jeremias Knoblauch", "authors": "Jeremias Knoblauch, Lara Vomfell", "title": "Robust Bayesian Inference for Discrete Outcomes with the Total Variation\n  Distance", "comments": "16p., 7 figs.; authors contributed equally & author order determined\n  by coin flip", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of discrete-valued outcomes are easily misspecified if the data\nexhibit zero-inflation, overdispersion or contamination. Without additional\nknowledge about the existence and nature of this misspecification, model\ninference and prediction are adversely affected. Here, we introduce a robust\ndiscrepancy-based Bayesian approach using the Total Variation Distance (TVD).\nIn the process, we address and resolve two challenges: First, we study\nconvergence and robustness properties of a computationally efficient estimator\nfor the TVD between a parametric model and the data-generating mechanism.\nSecond, we provide an efficient inference method adapted from Lyddon et al.\n(2019) which corresponds to formulating an uninformative nonparametric prior\ndirectly over the data-generating mechanism. Lastly, we empirically demonstrate\nthat our approach is robust and significantly improves predictive performance\non a range of simulated and real world data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:53:06 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Knoblauch", "Jeremias", ""], ["Vomfell", "Lara", ""]]}, {"id": "2010.13523", "submitter": "Yikun Zhang", "authors": "Yikun Zhang, Yen-Chi Chen", "title": "Kernel Smoothing, Mean Shift, and Their Learning Theory with Directional\n  Data", "comments": "92 pages, 11 figures. Accepted to the Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Directional data consist of observations distributed on a (hyper)sphere, and\nappear in many applied fields, such as astronomy, ecology, and environmental\nscience. This paper studies both statistical and computational problems of\nkernel smoothing for directional data. We generalize the classical mean shift\nalgorithm to directional data, which allows us to identify local modes of the\ndirectional kernel density estimator (KDE). The statistical convergence rates\nof the directional KDE and its derivatives are derived, and the problem of mode\nestimation is examined. We also prove the ascending property of the directional\nmean shift algorithm and investigate a general problem of gradient ascent on\nthe unit hypersphere. To demonstrate the applicability of the algorithm, we\nevaluate it as a mode clustering method on both simulated and real-world data\nsets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 01:53:47 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 08:38:34 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhang", "Yikun", ""], ["Chen", "Yen-Chi", ""]]}, {"id": "2010.13568", "submitter": "Kejun He", "authors": "Ya Zhou, Raymond K. W. Wong and Kejun He", "title": "CP Degeneracy in Tensor Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor linear regression is an important and useful tool for analyzing tensor\ndata. To deal with high dimensionality, CANDECOMP/PARAFAC (CP) low-rank\nconstraints are often imposed on the coefficient tensor parameter in the\n(penalized) $M$-estimation. However, we show that the corresponding\noptimization may not be attainable, and when this happens, the estimator is not\nwell-defined. This is closely related to a phenomenon, called CP degeneracy, in\nlow-rank tensor approximation problems. In this article, we provide useful\nresults of CP degeneracy in tensor regression problems. In addition, we provide\na general penalized strategy as a solution to overcome CP degeneracy. The\nasymptotic properties of the resulting estimation are also studied. Numerical\nexperiments are conducted to illustrate our findings.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 16:08:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhou", "Ya", ""], ["Wong", "Raymond K. W.", ""], ["He", "Kejun", ""]]}, {"id": "2010.13591", "submitter": "Sourabh Bhattacharya", "authors": "Sucharita Roy and Sourabh Bhattacharya", "title": "Function Optimization with Posterior Gaussian Derivative Process", "comments": "Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose and develop a novel Bayesian algorithm for\noptimization of functions whose first and second partial derivatives are known.\nThe basic premise is the Gaussian process representation of the function which\ninduces a first derivative process that is also Gaussian. The Bayesian\nposterior solutions of the derivative process set equal to zero, given data\nconsisting of suitable choices of input points in the function domain and their\nfunction values, emulate the stationary points of the function, which can be\nfine-tuned by setting restrictions on the prior in terms of the first and\nsecond derivatives of the objective function. These observations motivate us to\npropose a general and effective algorithm for function optimization that\nattempts to get closer to the true optima adaptively with in-built iterative\nstages. We provide theoretical foundation to this algorithm, proving almost\nsure convergence to the true optima as the number of iterative stages tends to\ninfinity. The theoretical foundation hinges upon our proofs of almost sure\nuniform convergence of the posteriors associated with Gaussian and Gaussian\nderivative processes to the underlying function and its derivatives in\nappropriate fixed-domain infill asymptotics setups; rates of convergence are\nalso available. We also provide Bayesian characterization of the number of\noptima using information inherent in our optimization algorithm. We illustrate\nour Bayesian optimization algorithm with five different examples involving\nmaxima, minima, saddle points and even inconclusiveness. Our examples range\nfrom simple, one-dimensional problems to challenging 50 and 100-dimensional\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 13:59:38 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Roy", "Sucharita", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "2010.13599", "submitter": "Cyrus Samii", "authors": "Peter M. Aronow and Cyrus Samii and Ye Wang", "title": "Design-Based Inference for Spatial Experiments with Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider design-based causal inference in settings where randomized\ntreatments have effects that bleed out into space in complex ways that overlap\nand in violation of the standard \"no interference\" assumption for many causal\ninference methods. We define a spatial \"average marginalized response,\" which\ncharacterizes how, in expectation, units of observation that are a specified\ndistance from an intervention point are affected by treatments at that point,\naveraging over effects emanating from other intervention points. We establish\nconditions for non-parametric identification, asymptotic distributions of\nestimators, and recovery of structural effects. We propose methods for both\nsample-theoretic and permutation-based inference. We provide illustrations\nusing randomized field experiments on forest conservation and health.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:15:31 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 20:14:02 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Aronow", "Peter M.", ""], ["Samii", "Cyrus", ""], ["Wang", "Ye", ""]]}, {"id": "2010.13604", "submitter": "Stefan Stein", "authors": "Stefan Stein, Chenlei Leng", "title": "A Sparse $\\beta$-Model with Covariates for Networks", "comments": "73 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data in the form of networks are increasingly encountered in modern science\nand humanity. This paper concerns a new generative model, suitable for sparse\nnetworks commonly observed in practice, to capture degree heterogeneity and\nhomophily, two stylized features of a typical network. The former is achieved\nby differentially assigning parameters to individual nodes, while the latter is\nmaterialized by incorporating covariates. Similar models in the literature for\nheterogeneity often include as many nodal parameters as the number of nodes,\nleading to over-parametrization and, as a result, strong requirements on the\ndensity of the network. For parameter estimation, we propose the use of the\npenalized likelihood method with an $\\ell_1$ penalty on the nodal parameters,\ngiving rise to a convex optimization formulation which immediately connects our\nestimation procedure to the LASSO literature. We highlight the differences of\nour approach to the LASSO method for logistic regression, emphasizing the\nfeasibility of our model to conduct inference for sparse networks, study the\nfinite-sample error bounds on the excess risk and the $\\ell_1$-error of the\nresulting estimator, and develop a central limit theorem for the parameter\nassociated with the covariates. Simulation and data analysis corroborate the\ndeveloped theory. As a by-product of our main theory, we study what we call the\nErd\\H{o}s-R\\'{e}nyi model with covariates and develop the associated\nstatistical inference for sparse networks, which can be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:19:08 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Stein", "Stefan", ""], ["Leng", "Chenlei", ""]]}, {"id": "2010.13687", "submitter": "Samuel Orso", "authors": "St\\'ephane Guerrier, Mucyo Karemera, Samuel Orso, Maria-Pia\n  Victoria-Feser, Yuming Zhang", "title": "A General Approach for Simulation-based Bias Correction in High\n  Dimensional Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important challenge in statistical analysis lies in controlling the bias\nof estimators due to the ever-increasing data size and model complexity.\nApproximate numerical methods and data features like censoring and\nmisclassification often result in analytical and/or computational challenges\nwhen implementing standard estimators. As a consequence, consistent estimators\nmay be difficult to obtain, especially in complex and/or high dimensional\nsettings. In this paper, we study the properties of a general simulation-based\nestimation framework that allows to construct bias corrected consistent\nestimators. We show that the considered approach leads, under more general\nconditions, to stronger bias correction properties compared to alternative\nmethods. Besides its bias correction advantages, the considered method can be\nused as a simple strategy to construct consistent estimators in settings where\nalternative methods may be challenging to apply. Moreover, the considered\nframework can be easily implemented and is computationally efficient. These\ntheoretical results are highlighted with simulation studies of various commonly\nused models, including the negative binomial regression (with and without\ncensoring) and the logistic regression (with and without misclassification\nerrors). Additional numerical illustrations are provided in the supplementary\nmaterials.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:07:01 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 20:37:49 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Karemera", "Mucyo", ""], ["Orso", "Samuel", ""], ["Victoria-Feser", "Maria-Pia", ""], ["Zhang", "Yuming", ""]]}, {"id": "2010.13704", "submitter": "Denis Rustand", "authors": "Denis Rustand, Janet van Niekerk, Haavard Rue, Christophe Tournigand,\n  Virginie Rondeau, Laurent Briollais", "title": "Bayesian Estimation of Two-Part Joint Models for a Longitudinal\n  Semicontinuous Biomarker and a Terminal Event with R-INLA: Interests for\n  Cancer Clinical Trial Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-part joint models for a longitudinal semicontinuous biomarker and a\nterminal event have been recently introduced based on frequentist estimation.\nThe biomarker distribution is decomposed into a probability of positive value\nand the expected value among positive values. Shared random effects can\nrepresent the association structure between the biomarker and the terminal\nevent. The computational burden increases compared to standard joint models\nwith a single regression model for the biomarker. In this context, the\nfrequentist estimation implemented in the R package frailtypack can be\nchallenging for complex models (i.e., large number of parameters and dimension\nof the random effects). As an alternative, we propose a Bayesian estimation of\ntwo-part joint models based on the Integrated Nested Laplace Approximation\n(INLA) algorithm to alleviate the computational burden and fit more complex\nmodels. Our simulation studies show that R-INLA reduces the computation time\nsubstantially as well as the variability of the parameter estimates and\nimproves the model convergence compared to frailtypack. We contrast the\nBayesian and frequentist approaches in the analysis of two randomized cancer\nclinical trials (GERCOR and PRIME studies), where R-INLA suggests a stronger\nassociation between the biomarker and the risk of event. Moreover, the Bayesian\napproach was able to characterize subgroups of patients associated with\ndifferent responses to treatment in the PRIME study while frailtypack had\nconvergence issues. Our study suggests that the Bayesian approach using R-INLA\nalgorithm enables broader applications of the two-part joint model to clinical\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:41:09 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 08:56:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rustand", "Denis", ""], ["van Niekerk", "Janet", ""], ["Rue", "Haavard", ""], ["Tournigand", "Christophe", ""], ["Rondeau", "Virginie", ""], ["Briollais", "Laurent", ""]]}, {"id": "2010.13774", "submitter": "Ethan Alt", "authors": "Ethan M. Alt, Matthew A. Psioda, Joseph G. Ibrahim", "title": "Bayesian Multivariate Probability of Success Using Historical Data with\n  Strict Control of Family-wise Error Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the cost and duration of phase III and phase IV clinical trials, the\ndevelopment of statistical methods for go/no-go decisions is vital. In this\npaper, we introduce a Bayesian methodology to compute the probability of\nsuccess based on the current data of a treatment regimen for the multivariate\nlinear model. Our approach utilizes a Bayesian seemingly unrelated regression\nmodel, which allows for multiple endpoints to be modeled jointly even if the\ncovariates between the endpoints are different. Correlations between endpoints\nare explicitly modeled. This Bayesian joint modeling approach unifies single\nand multiple testing procedures under a single framework. We develop an\napproach to multiple testing that asymptotically guarantees strict family-wise\nerror rate control, and is more powerful than frequentist approaches to\nmultiplicity. The method effectively yields those of Ibrahim et al. and\nChuang-Stein as special cases, and, to our knowledge, is the only method that\nallows for robust sample size determination for multiple endpoints and/or\nhypotheses and the only method that provides strict family-wise type I error\ncontrol in the presence of multiplicity.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 17:59:11 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Alt", "Ethan M.", ""], ["Psioda", "Matthew A.", ""], ["Ibrahim", "Joseph G.", ""]]}, {"id": "2010.13782", "submitter": "Amir Sepehri", "authors": "Amir Sepehri and Cyrus DiCiccio", "title": "Interpretable Assessment of Fairness During Model Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For companies developing products or algorithms, it is important to\nunderstand the potential effects not only globally, but also on sub-populations\nof users. In particular, it is important to detect if there are certain groups\nof users that are impacted differently compared to others with regard to\nbusiness metrics or for whom a model treats unequally along fairness concerns.\nIn this paper, we introduce a novel hierarchical clustering algorithm to detect\nheterogeneity among users in given sets of sub-populations with respect to any\nspecified notion of group similarity. We prove statistical guarantees about the\noutput and provide interpretable results. We demonstrate the performance of the\nalgorithm on real data from LinkedIn.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 02:31:17 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Sepehri", "Amir", ""], ["DiCiccio", "Cyrus", ""]]}, {"id": "2010.13877", "submitter": "Vadim Marmer", "authors": "Natasha Kang and Vadim Marmer", "title": "Modeling Long Cycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent boom-and-bust cycles are a salient feature of economic and\nfinancial history. Cycles found in the data are stochastic, often highly\npersistent, and span substantial fractions of the sample size. We refer to such\ncycles as \"long\". In this paper, we develop a novel approach to modeling\ncyclical behavior specifically designed to capture long cycles. We show that\nexisting inferential procedures may produce misleading results in the presence\nof long cycles, and propose a new econometric procedure for the inference on\nthe cycle length. Our procedure is asymptotically valid regardless of the cycle\nlength. We apply our methodology to a set of macroeconomic and financial\nvariables for the U.S. We find evidence of long stochastic cycles in the\nstandard business cycle variables, as well as in credit and house prices.\nHowever, we rule out the presence of stochastic cycles in asset market data.\nMoreover, according to our result, financial cycles as characterized by credit\nand house prices tend to be twice as long as business cycles.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 20:09:36 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 19:02:08 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Kang", "Natasha", ""], ["Marmer", "Vadim", ""]]}, {"id": "2010.13904", "submitter": "Muxuan Liang", "authors": "Muxuan Liang and Menggang Yu", "title": "Relative Contrast Estimation and Inference for Treatment Recommendation", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When there are resource constraints, it is important to rank or estimate\ntreatment benefits according to patient characteristics. This facilitates\nprioritization of assigning different treatments. Most existing literature on\nindividualized treatment rules targets absolute conditional treatment effect\ndifferences as the metric for benefits. However, there can be settings where\nrelative differences may better represent such benefits. In this paper, we\nconsider modeling such relative differences that form scale-invariant contrasts\nbetween conditional treatment effects. We show that all scale-invariant\ncontrasts are monotonic transformations of each other. Therefore we posit a\nsingle index model for a particular relative contrast. Identifiability of the\nmodel is enforced via an intuitive $l_2$ norm constraint on index parameters.\nWe then derive estimating equations and efficient scores via semiparametric\nefficiency theory. Based on the efficient score and its variant, we propose a\ntwo-step approach that consists of minimizing a doubly robust loss function and\na subsequent one-step efficiency augmentation procedure to achieve efficiency\nbound. Careful theoretical and numerical studies are provided to show the\nsuperiority of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 21:14:38 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 09:05:42 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 21:24:21 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Liang", "Muxuan", ""], ["Yu", "Menggang", ""]]}, {"id": "2010.13953", "submitter": "Ziyu Xu", "authors": "Ziyu Xu, Aaditya Ramdas", "title": "Dynamic Algorithms for Online Multiple Testing", "comments": "32 pages, 15 figures. Will be published in Mathematical and\n  Scientific Machine Learning 2021 (PMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive new algorithms for online multiple testing that provably control\nfalse discovery exceedance (FDX) while achieving orders of magnitude more power\nthan previous methods. This statistical advance is enabled by the development\nof new algorithmic ideas: earlier algorithms are more \"static\" while our new\nones allow for the dynamical adjustment of testing levels based on the amount\nof wealth the algorithm has accumulated. We demonstrate that our algorithms\nachieve higher power in a variety of synthetic experiments. We also prove that\nSupLORD can provide error control for both FDR and FDX, and controls FDR at\nstopping times. Stopping times are particularly important as they permit the\nexperimenter to end the experiment arbitrarily early while maintaining desired\ncontrol of the FDR. SupLORD is the first non-trivial algorithm, to our\nknowledge, that can control FDR at stopping times in the online setting.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 23:41:54 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 22:25:47 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 15:43:59 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Xu", "Ziyu", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2010.14026", "submitter": "Matthias Kormaksson", "authors": "Matthias Kormaksson (1), Luke J. Kelly (2), Xuan Zhu (1), Sibylle\n  Haemmerle (1), Luminita Pricop (1), and David Ohlssen (1) ((1) Novartis\n  Pharmaceuticals Corporation, (2) Oxford University)", "title": "Sequential knockoffs for continuous and categorical predictors: with\n  application to a large Psoriatic Arthritis clinical trial pool", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knockoffs provide a general framework for controlling the false discovery\nrate when performing variable selection. Much of the Knockoffs literature\nfocuses on theoretical challenges and we recognize a need for bringing some of\nthe current ideas into practice. In this paper we propose a sequential\nalgorithm for generating knockoffs when underlying data consists of both\ncontinuous and categorical (factor) variables. Further, we present a heuristic\nmultiple knockoffs approach that offers a practical assessment of how robust\nthe knockoff selection process is for a given data set. We conduct extensive\nsimulations to validate performance of the proposed methodology. Finally, we\ndemonstrate the utility of the methods on a large clinical data pool of more\nthan $2,000$ patients with psoriatic arthritis evaluated in 4 clinical trials\nwith an IL-17A inhibitor, secukinumab (Cosentyx), where we determine prognostic\nfactors of a well established clinical outcome. The analyses presented in this\npaper could provide a wide range of applications to commonly encountered data\nsets in medical practice and other fields where variable selection is of\nparticular interest.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 03:11:24 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Kormaksson", "Matthias", ""], ["Kelly", "Luke J.", ""], ["Zhu", "Xuan", ""], ["Haemmerle", "Sibylle", ""], ["Pricop", "Luminita", ""], ["Ohlssen", "David", ""]]}, {"id": "2010.14078", "submitter": "Nicole Pashley", "authors": "Nicole E. Pashley and Luke W. Miratrix", "title": "Block what you can, except when you shouldn't", "comments": "arXiv admin note: text overlap with arXiv:1710.10342", "journal-ref": null, "doi": "10.3102/10769986211027240", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several branches of the potential outcome causal inference literature have\ndiscussed the merits of blocking versus complete randomization. Some have\nconcluded it can never hurt the precision of estimates, and some have concluded\nit can hurt. In this paper, we reconcile these apparently conflicting views,\ngive a more thorough discussion of what guarantees no harm, and discuss how\nother aspects of a blocked design can cost, all in terms of precision. We\ndiscuss how the different findings are due to different sampling models and\nassumptions of how the blocks were formed. We also connect these ideas to\ncommon misconceptions, for instance showing that analyzing a blocked experiment\nas if it were completely randomized, a seemingly conservative method, can\nactually backfire in some cases. Overall, we find that blocking can have a\nprice, but that this price is usually small and the potential for gain can be\nlarge. It is hard to go too far wrong with blocking.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:17:19 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 20:35:41 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 21:24:31 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Pashley", "Nicole E.", ""], ["Miratrix", "Luke W.", ""]]}, {"id": "2010.14128", "submitter": "Rowland Seymour", "authors": "R. G. Seymour, D. Sirl, S. Preston, I. L. Dryden, M. J. A. Ellis, B.\n  Perrat, J. Goulding", "title": "The Bayesian Spatial Bradley--Terry Model: Urban Deprivation Modeling in\n  Tanzania", "comments": "23 pages, 7 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the most deprived regions of any country or city is key if policy\nmakers are to design successful interventions. However, locating areas with the\ngreatest need is often surprisingly challenging in developing countries. Due to\nthe logistical challenges of traditional household surveying, official\nstatistics can be slow to be updated; estimates that exist can be coarse, a\nconsequence of prohibitive costs and poor infrastructures; and mass\nurbanisation can render manually surveyed figures rapidly out-of-date.\nComparative judgement models, such as the Bradley--Terry model, offer a\npromising solution. Leveraging local knowledge, elicited via comparisons of\ndifferent areas' affluence, such models can both simplify logistics and\ncircumvent biases inherent to house-hold surveys. Yet widespread adoption\nremains limited, due to the large amount of data existing approaches still\nrequire. We address this via development of a novel Bayesian Spatial\nBradley--Terry model, which substantially decreases the amount of data\ncomparisons required for effective inference. This model integrates a network\nrepresentation of the city or country, along with assumptions of spatial\nsmoothness that allow deprivation in one area to be informed by neighbouring\nareas. We demonstrate the practical effectiveness of this method, through a\nnovel comparative judgement data set collected in Dar es Salaam, Tanzania.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 08:40:26 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 09:06:45 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 13:15:41 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Seymour", "R. G.", ""], ["Sirl", "D.", ""], ["Preston", "S.", ""], ["Dryden", "I. L.", ""], ["Ellis", "M. J. A.", ""], ["Perrat", "B.", ""], ["Goulding", "J.", ""]]}, {"id": "2010.14167", "submitter": "Frederic Loge", "authors": "Fr\\'ed\\'eric Log\\'e (CMAP), R\\'emi Besson (CRC), St\\'ephanie\n  Allassonni\\`ere (CRC)", "title": "Optimisation des parcours patients pour lutter contre l'errance de\n  diagnostic des patients atteints de maladies rares", "comments": "in French. Journ{\\'e}es de Statistiques de la SFDS, May 2020, Nice,\n  France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A patient suffering from a rare disease in France has to wait an average of\ntwo years before being diagnosed. This medical wandering is highly detrimental\nboth for the health system and for patients whose pathology may worsen. There\nexists an efficient network of Centres of Reference for Rare Diseases (CRMR),\nbut patients are often referred to these structures too late. We are\nconsidering a probabilistic modelling of the patient pathway in order to create\na simulator that will allow us to create an alert system that detects wandering\npatients and refers them to a CRMR while considering the potential additional\ncosts associated with these decisions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 09:55:46 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Log\u00e9", "Fr\u00e9d\u00e9ric", "", "CMAP"], ["Besson", "R\u00e9mi", "", "CRC"], ["Allassonni\u00e8re", "St\u00e9phanie", "", "CRC"]]}, {"id": "2010.14340", "submitter": "Paula Saavedra-Nieves", "authors": "Paula Saavedra-Nieves", "title": "Nonparametric estimation of highest density regions for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highest density regions refer to level sets containing points of relatively\nhigh density. Their estimation from a random sample, generated from the\nunderlying density, allows to determine the clusters of the corresponding\ndistribution. This task can be accomplished considering different nonparametric\nperspectives. From a practical point of view, reconstructing highest density\nregions can be interpreted as a way of determining hot-spots, a crucial task\nfor understanding COVID-19 space-time evolution. In this work, we compare the\nbehavior of classical plug-in methods and a recently proposed hybrid algorithm\nfor highest density regions estimation through an extensive simulation study.\nBoth methodologies are applied to analyze a real data set about COVID-19 cases\nin the United States.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 14:56:23 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 12:41:17 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 09:59:31 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Saavedra-Nieves", "Paula", ""]]}, {"id": "2010.14359", "submitter": "Siva Rajesh Kasa", "authors": "Siva Rajesh Kasa and Vaibhav Rajan", "title": "Improved Inference of Gaussian Mixture Copula Model for Clustering and\n  Reproducibility Analysis using Automatic Differentiation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copulas provide a modular parameterization of multivariate distributions that\ndecouples the modeling of marginals from the dependencies between them.\nGaussian Mixture Copula Model (GMCM) is a highly flexible copula that can model\nmany kinds of multi-modal dependencies, as well as asymmetric and tail\ndependencies. They have been effectively used in clustering non-Gaussian data\nand in Reproducibility Analysis, a meta-analysis method designed to verify the\nreliability and consistency of multiple high-throughput experiments. Parameter\nestimation for GMCM is challenging due to its intractable likelihood. The best\nprevious methods have maximized a proxy-likelihood through a Pseudo Expectation\nMaximization (PEM) algorithm. They have no guarantees of convergence or\nconvergence to the correct parameters. In this paper, we use Automatic\nDifferentiation (AD) tools to develop a method, called AD-GMCM, that can\nmaximize the exact GMCM likelihood. In our simulation studies and experiments\nwith real data, AD-GMCM finds more accurate parameter estimates than PEM and\nyields better performance in clustering and Reproducibility Analysis. We\ndiscuss the advantages of an AD-based approach, to address problems related to\nmonotonic increase of likelihood and parameter identifiability in GMCM. We also\nanalyze, for GMCM, two well-known cases of degeneracy of maximum likelihood in\nGMM that can lead to spurious clustering solutions. Our analysis shows that,\nunlike GMM, GMCM is not affected in one of the cases.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 07:37:33 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Kasa", "Siva Rajesh", ""], ["Rajan", "Vaibhav", ""]]}, {"id": "2010.14555", "submitter": "Peng Ding", "authors": "Anqi Zhao, Peng Ding", "title": "Covariate-adjusted Fisher randomization tests for the average treatment\n  effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher's randomization test (FRT) delivers exact $p$-values under the strong\nnull hypothesis of no treatment effect on any units whatsoever and allows for\nflexible covariate adjustment to improve the power. Of interest is whether the\nprocedure could also be valid for testing the weak null hypothesis of zero\naverage treatment effect. Towards this end, we evaluate two general strategies\nfor FRT with covariate-adjusted test statistics: that based on the residuals\nfrom an outcome model with only the covariates, and that based on the output\nfrom an outcome model with both the treatment and the covariates. Based on\ntheory and simulation, we recommend using the ordinary least squares (OLS) fit\nof the observed outcome on the treatment, centered covariates, and their\ninteractions for covariate adjustment, and conducting FRT with the robust\n$t$-value of the treatment as the test statistic. The resulting FRT is\nfinite-sample exact for the strong null hypothesis, asymptotically valid for\nthe weak null hypothesis, and more powerful than the unadjusted analog under\nalternatives, all irrespective of whether the linear model is correctly\nspecified or not. We develop the theory for complete randomization, cluster\nrandomization, stratified randomization, and rerandomization, respectively, and\ngive a recommendation for the test procedure and test statistic under each\ndesign. We first focus on the finite-population perspective and then extend the\nresult to the super-population perspective, highlighting the difference in\nstandard errors. Motivated by the similarity in procedure, we also evaluate the\ndesign-based properties of five existing permutation tests originally for\nlinear models and show the superiority of the proposed FRT for testing the\ntreatment effects.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 18:51:43 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 20:41:13 GMT"}, {"version": "v3", "created": "Thu, 29 Apr 2021 22:39:57 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhao", "Anqi", ""], ["Ding", "Peng", ""]]}, {"id": "2010.14589", "submitter": "Chun-Hao Yang", "authors": "Chun-Hao Yang, Baba C. Vemuri", "title": "Nested Grassmanns for Dimensionality Reduction with Applications to\n  Shape Analysis", "comments": "12 pages, 5 figures. To appear in the 27th international conference\n  on Information Processing in Medical Imaging (IPMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grassmann manifolds have been widely used to represent the geometry of\nfeature spaces in a variety of problems in medical imaging and computer vision\nincluding but not limited to shape analysis, action recognition, subspace\nclustering and motion segmentation. For these problems, the features usually\nlie in a very high-dimensional Grassmann manifold and hence an appropriate\ndimensionality reduction technique is called for in order to curtail the\ncomputational burden. To this end, the Principal Geodesic Analysis (PGA), a\nnonlinear extension of the well known principal component analysis, is\napplicable as a general tool to many Riemannian manifolds. In this paper, we\npropose a novel framework for dimensionality reduction of data in Riemannian\nhomogeneous spaces and then focus on the Grassman manifold which is an example\nof a homogeneous space. Our framework explicitly exploits the geometry of the\nhomogeneous space yielding reduced dimensional nested sub-manifolds that need\nnot be geodesic submanifolds and thus are more expressive. Specifically, we\nproject points in a Grassmann manifold to an embedded lower dimensional\nGrassmann manifold. A salient feature of our method is that it leads to higher\nexpressed variance compared to PGA which we demonstrate via synthetic and real\ndata experiments.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 20:09:12 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 01:48:33 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Yang", "Chun-Hao", ""], ["Vemuri", "Baba C.", ""]]}, {"id": "2010.14622", "submitter": "Runbing Zheng", "authors": "Runbing Zheng, Vince Lyzinski, Carey E. Priebe and Minh Tang", "title": "Vertex nomination between graphs via spectral embedding and quadratic\n  programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a network and a subset of interesting vertices whose identities are\nonly partially known, the vertex nomination problem seeks to rank the remaining\nvertices in such a way that the interesting vertices are ranked at the top of\nthe list. An important variant of this problem is vertex nomination in the\nmulti-graphs setting. Given two graphs $G_1, G_2$ with common vertices and a\nvertex of interest $x \\in G_1$, we wish to rank the vertices of $G_2$ such that\nthe vertices most similar to $x$ are ranked at the top of the list. The current\npaper addresses this problem and proposes a method that first applies adjacency\nspectral graph embedding to embed the graphs into a common Euclidean space, and\nthen solves a penalized linear assignment problem to obtain the nomination\nlists. Since the spectral embedding of the graphs are only unique up to\northogonal transformations, we present two approaches to eliminate this\npotential non-identifiability. One approach is based on orthogonal Procrustes\nand is applicable when there are enough vertices with known correspondence\nbetween the two graphs. Another approach uses adaptive point set registration\nand is applicable when there are few or no vertices with known correspondence.\nWe show that our nomination scheme leads to accurate nomination under a\ngenerative model for pairs of random graphs that are approximately low-rank and\npossibly with pairwise edge correlations. We illustrate our algorithm's\nperformance through simulation studies on synthetic data as well as analysis of\na high-school friendship network and analysis of transition rates between web\npages on the Bing search engine.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 10:50:29 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 17:50:13 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Zheng", "Runbing", ""], ["Lyzinski", "Vince", ""], ["Priebe", "Carey E.", ""], ["Tang", "Minh", ""]]}, {"id": "2010.14638", "submitter": "Yabo Niu", "authors": "Yabo Niu, Nilabja Guha, Debkumar De, Anindya Bhadra, Veerabhadran\n  Baladandayuthapani, Bani K. Mallick", "title": "Bayesian Variable Selection in Multivariate Nonlinear Regression with\n  Graph Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models (GGMs) are well-established tools for probabilistic\nexploration of dependence structures using precision matrices. We develop a\nBayesian method to incorporate covariate information in this GGMs setup in a\nnonlinear seemingly unrelated regression framework. We propose a joint\npredictor and graph selection model and develop an efficient collapsed Gibbs\nsampler algorithm to search the joint model space. Furthermore, we investigate\nits theoretical variable selection properties. We demonstrate our method on a\nvariety of simulated data, concluding with a real data set from the TCPA\nproject.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 21:57:07 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Niu", "Yabo", ""], ["Guha", "Nilabja", ""], ["De", "Debkumar", ""], ["Bhadra", "Anindya", ""], ["Baladandayuthapani", "Veerabhadran", ""], ["Mallick", "Bani K.", ""]]}, {"id": "2010.14734", "submitter": "Derek Beaton", "authors": "Derek Beaton (1) ((1) Rotman Research Institute, Baycrest Health\n  Sciences)", "title": "Generalized eigen, singular value, and partial least squares\n  decompositions: The GSVD package", "comments": "38 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The generalized singular value decomposition (GSVD, a.k.a. \"SVD triplet\",\n\"duality diagram\" approach) provides a unified strategy and basis to perform\nnearly all of the most common multivariate analyses (e.g., principal\ncomponents, correspondence analysis, multidimensional scaling, canonical\ncorrelation, partial least squares). Though the GSVD is ubiquitous, powerful,\nand flexible, it has very few implementations. Here I introduce the GSVD\npackage for R. The general goal of GSVD is to provide a small set of accessible\nfunctions to perform the GSVD and two other related decompositions (generalized\neigenvalue decomposition, generalized partial least squares-singular value\ndecomposition). Furthermore, GSVD helps provide a more unified conceptual\napproach and nomenclature to many techniques. I first introduce the concept of\nthe GSVD, followed by a formal definition of the generalized decompositions.\nNext I provide some key decisions made during development, and then a number of\nexamples of how to use GSVD to implement various statistical techniques. These\nexamples also illustrate one of the goals of GSVD: how others can (or should)\nbuild analysis packages that depend on GSVD. Finally, I discuss the possible\nfuture of GSVD.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 03:57:27 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 13:24:14 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 23:59:48 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Beaton", "Derek", ""]]}, {"id": "2010.14883", "submitter": "Sina Mews", "authors": "Sina Mews, Roland Langrock, Marius \\\"Otting, Houda Yaqine and Jost\n  Reinecke", "title": "Maximum approximate likelihood estimation of general continuous-time\n  state-space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous-time state-space models (SSMs) are flexible tools for analysing\nirregularly sampled sequential observations that are driven by an underlying\nstate process. Corresponding applications typically involve restrictive\nassumptions concerning linearity and Gaussianity to facilitate inference on the\nmodel parameters via the Kalman filter. In this contribution, we provide a\ngeneral continuous-time SSM framework, allowing both the observation and the\nstate process to be non-linear and non-Gaussian. Statistical inference is\ncarried out by maximum approximate likelihood estimation, where multiple\nnumerical integration within the likelihood evaluation is performed via a fine\ndiscretisation of the state process. The corresponding reframing of the SSM as\na continuous-time hidden Markov model, with structured state transitions,\nenables us to apply the associated efficient algorithms for parameter\nestimation and state decoding. We illustrate the modelling approach in a case\nstudy using data from a longitudinal study on delinquent behaviour of\nadolescents in Germany, revealing temporal persistence in the deviation of an\nindividual's delinquency level from the population mean.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 11:05:39 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Mews", "Sina", ""], ["Langrock", "Roland", ""], ["\u00d6tting", "Marius", ""], ["Yaqine", "Houda", ""], ["Reinecke", "Jost", ""]]}, {"id": "2010.15009", "submitter": "Debashis Ghosh", "authors": "Youngjoo Cho, Debashis Ghosh", "title": "Bridging linearity-based and kernel-based sufficient dimension reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a lot of interest in sufficient dimension reduction (SDR)\nmethodologies as well as nonlinear extensions in the statistics literature. In\nthis note, we use classical results regarding metric spaces and positive\ndefinite functions to link linear SDR procedures to their nonlinear\ncounterparts.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 14:36:03 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Cho", "Youngjoo", ""], ["Ghosh", "Debashis", ""]]}, {"id": "2010.15078", "submitter": "Brijesh Singh", "authors": "Brijesh P. Singh and Utpal Dhar Das", "title": "On an Induced Distribution and its Statistical Properties", "comments": "14 pages, 1 table and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study an attempt has been made to propose a way to develop new\ndistribution. For this purpose, we need only idea about distribution function.\nSome important statistical properties of the new distribution like moments,\ncumulants, hazard and survival function has been derived. The renyi entropy,\nshannon entropy has been obtained. Also ML estimate of parameter of the\ndistribution is obtained, that is not closed form. Therefore, numerical\ntechnique is used to estimate the parameter. Some real data sets are used to\ncheck the suitability of this distribution over some other existing\ndistributions such as Lindley, Garima, Shanker and many more. AIC, BIC,\n-2loglikihood, K-S test suggest the proposed distribution works better than\nothers distributions considered in this study.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 14:54:57 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Singh", "Brijesh P.", ""], ["Das", "Utpal Dhar", ""]]}, {"id": "2010.15207", "submitter": "Andrew Lawson", "authors": "Andrew B. Lawson and Joanne Kim", "title": "Space-Time Covid-19 Bayesian SIR modeling in South Carolina", "comments": "18 pages, 14 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0242777", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Covid-19 pandemic has spread across the world since the beginning of\n2020. Many regions have experienced its effects. The state of South Carolina in\nthe USA has seen cases since early March 2020 and a primary peak in early April\n2020. A lockdown was imposed on April 6th but lifting of restrictions started\non April 24th. The daily case and death data as reported by NCHS (deaths) via\nthe New York Times GitHUB repository have been analyzed and approaches to\nmodeling of the data are presented. Prediction is also considered and the role\nof asymptomatic transmission is assessed as a latent unobserved effect. Two\ndifferent time periods are examined and one step prediction is provided.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 20:13:01 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Lawson", "Andrew B.", ""], ["Kim", "Joanne", ""]]}, {"id": "2010.15285", "submitter": "Raif Rustamov", "authors": "Raif M. Rustamov and Subhabrata Majumdar", "title": "Intrinsic Sliced Wasserstein Distances for Comparing Collections of\n  Probability Distributions on Manifolds and Graphs", "comments": "Improved exposition, add resampling based test, source code", "journal-ref": null, "doi": null, "report-no": "TD:102696/2020-10-08", "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collections of probability distributions arise in a variety of statistical\napplications ranging from user activity pattern analysis to brain connectomics.\nIn practice these distributions are represented by histograms over diverse\ndomain types including finite intervals, circles, cylinders, spheres, other\nmanifolds, and graphs. This paper introduces an approach for detecting\ndifferences between two collections of histograms over such general domains. We\npropose the intrinsic slicing construction that yields a novel class of\nWasserstein distances on manifolds and graphs. These distances are Hilbert\nembeddable, allowing us to reduce the histogram collection comparison problem\nto a more familiar mean testing problem in a Hilbert space. We provide two\ntesting procedures one based on resampling and another on combining p-values\nfrom coordinate-wise tests. Our experiments in a variety of data settings show\nthat the resulting tests are powerful and the p-values are well-calibrated.\nExample applications to user activity patterns, spatial data, and brain\nconnectomics are provided.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 23:41:42 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 17:50:49 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Rustamov", "Raif M.", ""], ["Majumdar", "Subhabrata", ""]]}, {"id": "2010.15326", "submitter": "Weinan Wang", "authors": "Weinan Wang, Xi Zhang", "title": "CONQ: CONtinuous Quantile Treatment Effects for Large-Scale Online\n  Controlled Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many industry settings, online controlled experimentation (A/B test) has\nbeen broadly adopted as the gold standard to measure product or feature\nimpacts. Most research has primarily focused on user engagement type metrics,\nspecifically measuring treatment effects at mean (average treatment effects,\nATE), and only a few have been focusing on performance metrics (e.g. latency),\nwhere treatment effects are measured at quantiles. Measuring quantile treatment\neffects (QTE) is challenging due to the myriad difficulties such as dependency\nintroduced by clustered samples, scalability issues, density bandwidth choices,\netc. In addition, previous literature has mainly focused on QTE at some\npre-defined locations, such as P50 or P90, which doesn't always convey the full\npicture. In this paper, we propose a novel scalable non-parametric solution,\nwhich can provide a continuous range of QTE with point-wise confidence\nintervals while circumventing the density estimation altogether. Numerical\nresults show high consistency with traditional methods utilizing asymptotic\nnormality. An end-to-end pipeline has been implemented at Snap Inc., providing\ndaily insights on key performance metrics at a distributional level.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 02:48:20 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Wang", "Weinan", ""], ["Zhang", "Xi", ""]]}, {"id": "2010.15351", "submitter": "Yves Isma\\\"el Ngounou Bakam", "authors": "Yves I. Ngounou Bakam and Denys Pommeret", "title": "Nonparametric estimation of copulas and copula densities by orthogonal\n  projections", "comments": "42 pages, 6 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study nonparametric estimators of copulas and copula\ndensities. We first focus our study on a density copula estimator based on a\npolynomial orthogonal projection of the joint density. A new copula estimator\nis then deduced. Its asymptotic properties are studied: we provide a large\nfunctional class for which this construction is optimal in the minimax and\nmaxiset sense and we propose a method selection for the smoothing parameter. An\nintensive simulation study shows the very good performance of both copulas and\ncopula densities estimators which we compare to a large panel of competitors. A\nreal dataset in actuarial science illustrates this approach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 04:24:31 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Bakam", "Yves I. Ngounou", ""], ["Pommeret", "Denys", ""]]}, {"id": "2010.15368", "submitter": "Chi Chang", "authors": "Chi Chang, Kimberly Kelly, M. Lee Van Horn, Richard T. Houang, Joseph\n  Gardiner, Laurie Van Egeren, Heng-Chieh Wu", "title": "Classification Accuracy and Parameter Estimation in Multilevel Contexts:\n  A Study of Conditional Nonparametric Multilevel Latent Class Analysis", "comments": "40 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current research has two aims. First, to demonstrate the utility\nconditional nonparametric multilevel latent class analysis (NP-MLCA) for\nmulti-site program evaluation using an empirical dataset. Second, to\ninvestigate how classification accuracy and parameter estimation of a\nconditional NP-MLCA are affected by six study factors: the quality of latent\nclass indicators, the number of latent class indicators, level-1 covariate\neffects, cross-level covariate effects, the number of level-2 units, and the\nsize of level-2 units. A total of 96 conditions was examined using a simulation\nstudy. The resulting classification accuracy rates, the power and type-I error\nof cross-level covariate effects and contextual effects suggest that the\nnonparametric multilevel latent class model can be applied broadly in\nmultilevel contexts.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 06:12:11 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 06:32:52 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Chang", "Chi", ""], ["Kelly", "Kimberly", ""], ["Van Horn", "M. Lee", ""], ["Houang", "Richard T.", ""], ["Gardiner", "Joseph", ""], ["Van Egeren", "Laurie", ""], ["Wu", "Heng-Chieh", ""]]}, {"id": "2010.15511", "submitter": "Shunichi Nomura", "authors": "Shunichi Nomura", "title": "An Exact Solution Path Algorithm for SLOPE and Quasi-Spherical OSCAR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorted $L_1$ penalization estimator (SLOPE) is a regularization technique for\nsorted absolute coefficients in high-dimensional regression. By arbitrarily\nsetting its regularization weights $\\lambda$ under the monotonicity constraint,\nSLOPE can have various feature selection and clustering properties. On weight\ntuning, the selected features and their clusters are very sensitive to the\ntuning parameters. Moreover, the exhaustive tracking of their changes is\ndifficult using grid search methods. This study presents a solution path\nalgorithm that provides the complete and exact path of solutions for SLOPE in\nfine-tuning regularization weights. A simple optimality condition for SLOPE is\nderived and used to specify the next splitting point of the solution path. This\nstudy also proposes a new design of a regularization sequence $\\lambda$ for\nfeature clustering, which is called the quasi-spherical and octagonal shrinkage\nand clustering algorithm for regression (QS-OSCAR). QS-OSCAR is designed with a\ncontour surface of the regularization terms most similar to a sphere. Among\nseveral regularization sequence designs, sparsity and clustering performance\nare compared through simulation studies. The numerical observations show that\nQS-OSCAR performs feature clustering more efficiently than other designs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:03:22 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Nomura", "Shunichi", ""]]}, {"id": "2010.15515", "submitter": "Manuele Leonelli", "authors": "Christiane G\\\"orgen, Manuele Leonelli, Orlando Marigliano", "title": "The curved exponential family of a staged tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Staged tree models are a discrete generalization of Bayesian networks. We\nshow that these form curved exponential families and derive their natural\nparameters, sufficient statistic, and cumulant-generating function as functions\nof their graphical representation. We give necessary graphical criteria for\nclassifying regular subfamilies and discuss implications for model selection.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:10:14 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 21:43:00 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["G\u00f6rgen", "Christiane", ""], ["Leonelli", "Manuele", ""], ["Marigliano", "Orlando", ""]]}, {"id": "2010.15604", "submitter": "Carlos Puerto-Santana", "authors": "Carlos Puerto-Santana and Pedro Larra\\~naga and Concha Bielza", "title": "Autoregressive Asymmetric Linear Gaussian Hidden Markov Models", "comments": "34 pages, 16 figures, intended to be published in IEEE Transactions\n  on Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a real life process evolving over time, the relationship between its\nrelevant variables may change. Therefore, it is advantageous to have different\ninference models for each state of the process. Asymmetric hidden Markov models\nfulfil this dynamical requirement and provide a framework where the trend of\nthe process can be expressed as a latent variable. In this paper, we modify\nthese recent asymmetric hidden Markov models to have an asymmetric\nautoregressive component, allowing the model to choose the order of\nautoregression that maximizes its penalized likelihood for a given training\nset. Additionally, we show how inference, hidden states decoding and parameter\nlearning must be adapted to fit the proposed model. Finally, we run experiments\nwith synthetic and real data to show the capabilities of this new model.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 08:58:46 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Puerto-Santana", "Carlos", ""], ["Larra\u00f1aga", "Pedro", ""], ["Bielza", "Concha", ""]]}, {"id": "2010.15709", "submitter": "Dietmar Pfeifer Prof. Dr.", "authors": "Dietmar Pfeifer, Doreen Strassburger, Joerg Philipps", "title": "Modelling and simulation of dependence structures in nonlife insurance\n  with Bernstein copulas", "comments": "paper presented on the International ASTIN Colloquium 2009, Helsinki", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we review Bernstein and grid-type copulas for arbitrary\ndimensions and general grid resolutions in connection with discrete random\nvectors possessing uniform margins. We further suggest a pragmatic way to fit\nthe dependence structure of multivariate data to Bernstein copulas via\ngrid-type copulas and empirical contingency tables. Finally, we discuss a Monte\nCarlo study for the simulation and PML estimation for aggregate dependent\nlosses form observed windstorm and flooding data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 15:50:27 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Pfeifer", "Dietmar", ""], ["Strassburger", "Doreen", ""], ["Philipps", "Joerg", ""]]}, {"id": "2010.15808", "submitter": "Xiangge Luo", "authors": "Xiang Ge Luo, Giusi Moffa, Jack Kuipers", "title": "Learning Bayesian Networks from Ordinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks are a powerful framework for studying the dependency\nstructure of variables in a complex system. The problem of learning Bayesian\nnetworks is tightly associated with the given data type. Ordinal data, such as\nstages of cancer, rating scale survey questions, and letter grades for exams,\nare ubiquitous in applied research. However, existing solutions are mainly for\ncontinuous and nominal data. In this work, we propose an iterative\nscore-and-search method - called the Ordinal Structural EM (OSEM) algorithm -\nfor learning Bayesian networks from ordinal data. Unlike traditional approaches\ndesigned for nominal data, we explicitly respect the ordering amongst the\ncategories. More precisely, we assume that the ordinal variables originate from\nmarginally discretizing a set of Gaussian variables, whose structural\ndependence in the latent space follows a directed acyclic graph. Then, we adopt\nthe Structural EM algorithm and derive closed-form scoring functions for\nefficient graph searching. Through simulation studies, we illustrate the\nsuperior performance of the OSEM algorithm compared to the alternatives and\nanalyze various factors that may influence the learning accuracy. Finally, we\ndemonstrate the practicality of our method with a real-world application on\npsychological survey data from 408 patients with co-morbid symptoms of\nobsessive-compulsive disorder and depression.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:44:41 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Luo", "Xiang Ge", ""], ["Moffa", "Giusi", ""], ["Kuipers", "Jack", ""]]}, {"id": "2010.15817", "submitter": "Nikolaos Ignatiadis", "authors": "Nikolaos Ignatiadis and Panagiotis Lolas", "title": "$\\sigma$-Ridge: group regularized ridge regression via empirical Bayes\n  noise level cross-validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Features in predictive models are not exchangeable, yet common supervised\nmodels treat them as such. Here we study ridge regression when the analyst can\npartition the features into $K$ groups based on external side-information. For\nexample, in high-throughput biology, features may represent gene expression,\nprotein abundance or clinical data and so each feature group represents a\ndistinct modality. The analyst's goal is to choose optimal regularization\nparameters $\\lambda = (\\lambda_1, \\dotsc, \\lambda_K)$ -- one for each group. In\nthis work, we study the impact of $\\lambda$ on the predictive risk of\ngroup-regularized ridge regression by deriving limiting risk formulae under a\nhigh-dimensional random effects model with $p\\asymp n$ as $n \\to \\infty$.\nFurthermore, we propose a data-driven method for choosing $\\lambda$ that\nattains the optimal asymptotic risk: The key idea is to interpret the residual\nnoise variance $\\sigma^2$, as a regularization parameter to be chosen through\ncross-validation. An empirical Bayes construction maps the one-dimensional\nparameter $\\sigma$ to the $K$-dimensional vector of regularization parameters,\ni.e., $\\sigma \\mapsto \\widehat{\\lambda}(\\sigma)$. Beyond its theoretical\noptimality, the proposed method is practical and runs as fast as\ncross-validated ridge regression without feature groups ($K=1$).\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:52:45 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 10:05:36 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ignatiadis", "Nikolaos", ""], ["Lolas", "Panagiotis", ""]]}, {"id": "2010.15972", "submitter": "Karthik Srinivasan", "authors": "Karthik Srinivasan, Amit Kumar, Parameshwaran Iyer, Abhinav Joshi", "title": "Manufacturing Process Optimization using Statistical Methodologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response Surface Methodology (RSM) introduced in the paper (Box & Wilson,\n1951) explores the relationships between explanatory and response variables in\ncomplex settings and provides a framework to identify correct settings for the\nexplanatory variables to yield the desired response. RSM involves setting up\nsequential experimental designs followed by application of elementary\noptimization methods to identify direction of improvement in response. In this\npaper, an application of RSM using a two-factor two-level Central Composite\nDesign (CCD) is explained for a diesel engine nozzle manufacturing sub-process.\nThe analysis shows that one of the factors has a significant influence in\nimproving desired values of the response. The implementation of RSM is done\nusing the DoE plug-in available in R software.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 22:28:45 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Srinivasan", "Karthik", ""], ["Kumar", "Amit", ""], ["Iyer", "Parameshwaran", ""], ["Joshi", "Abhinav", ""]]}, {"id": "2010.15997", "submitter": "Stephanie Clark", "authors": "Stephanie Clark, Rob J Hyndman, Dan Pagendam, Louise M Ryan", "title": "Modern strategies for time series regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses several modern approaches to regression analysis\ninvolving time series data where some of the predictor variables are also\nindexed by time. We discuss classical statistical approaches as well as methods\nthat have been proposed recently in the machine learning literature. The\napproaches are compared and contrasted, and it will be seen that there are\nadvantages and disadvantages to most currently available approaches. There is\nample room for methodological developments in this area. The work is motivated\nby an application involving the prediction of water levels as a function of\nrainfall and other climate variables in an aquifer in eastern Australia.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 23:56:57 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Clark", "Stephanie", ""], ["Hyndman", "Rob J", ""], ["Pagendam", "Dan", ""], ["Ryan", "Louise M", ""]]}, {"id": "2010.16025", "submitter": "Rushani Wijesuriya", "authors": "Rushani Wijesuriya, Margarita Moreno-Betancur, John B. Carlin, Anurika\n  P. De Silva and Katherine J. Lee", "title": "Evaluation of approaches for accommodating interactions and non-linear\n  terms in multiple imputation of incomplete three-level data", "comments": "34 pages, 5 tables and 9 figures (without additional files)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-level data structures arising from repeated measures on individuals\nclustered within larger units are common in health research studies. Missing\ndata are prominent in such studies and are often handled via multiple\nimputation (MI). Although several MI approaches can be used to account for the\nthree-level structure, including adaptations to single- and two-level\napproaches, when the substantive analysis model includes interactions or\nquadratic effects these too need to be accommodated in the imputation model. In\nsuch analyses, substantive model compatible (SMC) MI has shown great promise in\nthe context of single-level data. While there have been recent developments in\nmultilevel SMC MI, to date only one approach that explicitly handles incomplete\nthree-level data is available. Alternatively, researchers can use pragmatic\nadaptations to single- and two-level MI approaches, or two-level SMC-MI\napproaches. We describe the available approaches and evaluate them via\nsimulation in the context of a three three-level random effects analysis models\ninvolving an interaction between the incomplete time-varying exposure and time,\nan interaction between the time-varying exposure and an incomplete time-fixed\nconfounder, or a quadratic effect of the exposure. Results showed that all\napproaches considered performed well in terms of bias and precision when the\ntarget analysis involved an interaction with time, but the three-level SMC MI\napproach performed best when the target analysis involved an interaction\nbetween the time-varying exposure and an incomplete time-fixed confounder, or a\nquadratic effect of the exposure. We illustrate the methods using data from the\nChildhood to Adolescence Transition Study.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 02:23:12 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Wijesuriya", "Rushani", ""], ["Moreno-Betancur", "Margarita", ""], ["Carlin", "John B.", ""], ["De Silva", "Anurika P.", ""], ["Lee", "Katherine J.", ""]]}, {"id": "2010.16061", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "Evaluation: from precision, recall and F-measure to ROC, informedness,\n  markedness and correlation", "comments": "27 pages, 7 figures. Updated and fixed egregious formatting errors\n  (including a table overlapping text) that were introduced by the publisher.\n  This open access journal appears to have been discontinued. arXiv admin note:\n  text overlap with arXiv:1504.00854", "journal-ref": "International Journal of Machine Learning Technology 2:1 (2011),\n  pp.37-63", "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonly used evaluation measures including Recall, Precision, F-Measure and\nRand Accuracy are biased and should not be used without clear understanding of\nthe biases, and corresponding identification of chance or base case levels of\nthe statistic. Using these measures a system that performs worse in the\nobjective sense of Informedness, can appear to perform better under any of\nthese commonly used measures. We discuss several concepts and measures that\nreflect the probability that prediction is informed versus chance. Informedness\nand introduce Markedness as a dual measure for the probability that prediction\nis marked versus chance. Finally we demonstrate elegant connections between the\nconcepts of Informedness, Markedness, Correlation and Significance as well as\ntheir intuitive relationships with Recall and Precision, and outline the\nextension from the dichotomous case to the general multi-class case.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 02:15:11 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "2010.16129", "submitter": "Yuki Atsusaka", "authors": "Yuki Atsusaka and Randolph T. Stevenson", "title": "A Bias-Corrected Estimator for the Crosswise Model with Inattentive\n  Respondents", "comments": "26 pages, 5 figures (main text); 22 pages, 9 figures (Online\n  Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crosswise model is an increasingly popular survey technique to elicit\ncandid answers from respondents on sensitive questions. Recent studies,\nhowever, point out that in the presence of inattentive respondents, the\nconventional estimator of the prevalence of a sensitive attribute is biased\ntoward 0.5. To remedy this problem, we propose a simple design-based bias\ncorrection using an anchor question that has a sensitive item with known\nprevalence. We demonstrate that we can easily estimate and correct for the bias\narising from inattentive respondents without measuring individual-level\nattentiveness. We also offer several useful extensions of our estimator,\nincluding a sensitivity analysis for the conventional estimator, a strategy for\nweighting, a framework for multivariate regressions in which a latent sensitive\ntrait is used as an outcome or a predictor, and tools for power analysis and\nparameter selection. Our method can be easily implemented through our\nopen-source software, cWise.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 09:03:21 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 15:46:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Atsusaka", "Yuki", ""], ["Stevenson", "Randolph T.", ""]]}, {"id": "2010.16229", "submitter": "Federico Ambrogi", "authors": "Federico Ambrogi, Simona Iacobelli, Per Kragh Andersen", "title": "Analyzing differences between restricted mean survival time curves using\n  pseudo-values", "comments": "19 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hazard ratios are ubiquitously used in time to event analysis to quantify\ntreatment effects. Although hazard ratios are invaluable for hypothesis\ntesting, other measures of association, both relative and absolute, may be used\nto fully elucidate study results. Restricted mean survival time differences\nbetween groups have been advocated as useful measures of association. Recent\nwork focused on model-free estimates of the difference in restricted mean\nsurvival for all follow-up times instead of focusing on a single time horizon.\nIn this work a model-based alternative is proposed with estimation using\npseudo-values. A simple approach is proposed easily implementable with\navailable software. It is also possible to compute a confidence region for the\ncurve. As a by-product, the parameter 'time until treatment equipoise' (TUTE)\nis also studied. Examples with crossing survival curves will be used to\nillustrate the different methods together with some simulations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 12:51:02 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Ambrogi", "Federico", ""], ["Iacobelli", "Simona", ""], ["Andersen", "Per Kragh", ""]]}, {"id": "2010.16271", "submitter": "Wouter van Loon", "authors": "Wouter van Loon, Marjolein Fokkema, Botond Szabo, Mark de Rooij", "title": "View selection in multi-view stacking: Choosing the meta-learner", "comments": "37 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view stacking is a framework for combining information from different\nviews (i.e. different feature sets) describing the same set of objects. In this\nframework, a base-learner algorithm is trained on each view separately, and\ntheir predictions are then combined by a meta-learner algorithm. In a previous\nstudy, stacked penalized logistic regression, a special case of multi-view\nstacking, has been shown to be useful in identifying which views are most\nimportant for prediction. In this article we expand this research by\nconsidering seven different algorithms to use as the meta-learner, and\nevaluating their view selection and classification performance in simulations\nand two applications on real gene-expression data sets. Our results suggest\nthat if both view selection and classification accuracy are important to the\nresearch at hand, then the nonnegative lasso, nonnegative adaptive lasso and\nnonnegative elastic net are suitable meta-learners. Exactly which among these\nthree is to be preferred depends on the research context. The remaining four\nmeta-learners, namely nonnegative ridge regression, nonnegative forward\nselection, stability selection and the interpolating predictor, show little\nadvantages in order to be preferred over the other three.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 13:45:14 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["van Loon", "Wouter", ""], ["Fokkema", "Marjolein", ""], ["Szabo", "Botond", ""], ["de Rooij", "Mark", ""]]}]