[{"id": "1312.0099", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran and Milan Stehl\\'ik", "title": "Optimal designs for parameters of shifted Ornstein-Uhlenbeck sheets\n  measured on monotonic sets", "comments": null, "journal-ref": "Statistics and Probability Letters 99 (2015), 114-124", "doi": "10.1016/j.spl.2015.01.006", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurement on sets with a specific geometric shape can be of interest for\nmany important applications (e.g. measurement along the isotherms in structural\nengineering). In the present paper the properties of optimal designs for\nestimating the parameters of shifted Ornstein-Uhlenbeck sheets, that is\nGaussian two-variable random fields with exponential correlation structures,\nare investigated when the processes are observed on monotonic sets. Substantial\ndifferences are demonstrated between the cases when one is interested only in\ntrend parameters and when the whole parameter set is of interest. The\ntheoretical results are illustrated by computer experiments and simulated\nexamples from the field of structure engineering. From the design point of view\nthe most interesting finding of the paper is the loss of efficiency of the\nregular grid design compared to the optimal monotonic design.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 12:17:40 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Stehl\u00edk", "Milan", ""]]}, {"id": "1312.0154", "submitter": "Saskia Becker", "authors": "Saskia Becker", "title": "The Propagation-Separation Approach: Consequences of model\n  misspecification", "comments": "31 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": "WIAS Preprint 1877", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents new results on the Propagation-Separation Approach by\nPolzehl and Spokoiny [2006]. This iterative procedure provides a unified\napproach for nonparametric estimation, sup- posing a local parametric model.\nThe adaptivity of the estimator ensures sensitivity to structural changes.\nOriginally, an additional memory step was included into the algorithm, where\nmost of the theoretical prop- erties were based on. However, in practice, a\nsimplified version of the algorithm is used, where the mem- ory step is\nomitted. Hence, we aim to justify this simplified procedure by means of a\ntheoretical study and numerical simulations. In our previous study [Becker and\nMath\\'e, 2013], we analyzed the simplified Propagation-Separation Approach,\nsupposing piecewise constant parameter functions with sharp discon- tinuities.\nHere, we consider the case of a misspecified model.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 21:27:06 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Becker", "Saskia", ""]]}, {"id": "1312.0294", "submitter": "Giles Hooker", "authors": "Giles Hooker, Stephen P. Ellner", "title": "Goodness of fit in nonlinear dynamics: Misspecified rates or\n  misspecified states?", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS828 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 2, 754-776", "doi": "10.1214/15-AOAS828", "report-no": "IMS-AOAS-AOAS828", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces diagnostic tests for the nature of lack of fit in\nordinary differential equation models (ODEs) proposed for data. We present a\nhierarchy of three possible sources of lack of fit: unaccounted-for stochastic\nvariation, misspecification of functional forms in rate equations, and omission\nof dynamic variables in the description of the system. We represent lack of fit\nby allowing a parameter vector to vary over time, and propose generic testing\nprocedures that do not rely on specific alternative models. Instead, different\nsources for lack of fit are characterized in terms of nonparametric\nrelationships among latent variables. The tests are carried out through a\ncombination of residual bootstrap and permutation methods. We demonstrate the\neffectiveness of these tests on simulated data and on real data from laboratory\necological experiments and electro-cardiogram data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 00:54:15 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2015 13:19:30 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Hooker", "Giles", ""], ["Ellner", "Stephen P.", ""]]}, {"id": "1312.0302", "submitter": "Stephen Walker", "authors": "Tom Shively, Stephen Walker", "title": "On the Equivalence between Bayesian and Classical Hypothesis Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For hypotheses of the type H_0:theta=theta_0 vs H_1:theta ne theta_0 we\ndemonstrate the equivalence of a Bayesian hypothesis test using a Bayes factor\nand the corresponding classical test, for a large class of models, which are\ndetailed in the paper. In particular, we show that the role of the prior and\ncritical region for the Bayes factor test is only to specify the type I error.\nThis is their only role since, as we show, the power function of the Bayes\nfactor test coincides exactly with that of the classical test, once the type I\nerror has been fixed.\n  For more complex tests involving nuisance parameters, we recover the\nclassical test by using Jeffreys prior on the nuisance parameters, while the\nprior on the hypothesized parameters can be arbitrary up to a large class. On\nthe other hand, we show that using proper priors on the nuisance parameters\nresults in a test with uniformly lower power than the classical test.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 01:56:17 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Shively", "Tom", ""], ["Walker", "Stephen", ""]]}, {"id": "1312.0518", "submitter": "Utkarsh Dang", "authors": "Utkarsh J. Dang and Paul D. McNicholas", "title": "Families of Parsimonious Finite Mixtures of Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixtures of regression models offer a flexible framework for\ninvestigating heterogeneity in data with functional dependencies. These models\ncan be conveniently used for unsupervised learning on data with clear\nregression relationships. We extend such models by imposing an\neigen-decomposition on the multivariate error covariance matrix. By\nconstraining parts of this decomposition, we obtain families of parsimonious\nmixtures of regressions and mixtures of regressions with concomitant variables.\nThese families of models account for correlations between multiple responses.\nAn expectation-maximization algorithm is presented for parameter estimation and\nperformance is illustrated on simulated and real data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 17:08:03 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Dang", "Utkarsh J.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1312.0538", "submitter": "Pietro Coretto", "authors": "Pietro Coretto and Francesco Giordano", "title": "Nonparametric estimation of the dynamic range of music signals", "comments": null, "journal-ref": "2017, Australian & New Zealand Journal of Statistics, Vol. 59(4),\n  pp. 389-412", "doi": "10.1111/anzs.12217", "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic range is an important parameter which measures the spread of\nsound power, and for music signals it is a measure of recording quality. There\nare various descriptive measures of sound power, none of which has strong\nstatistical foundations. We start from a nonparametric model for sound waves\nwhere an additive stochastic term has the role to catch transient energy. This\ncomponent is recovered by a simple rate-optimal kernel estimator that requires\na single data-driven tuning. The distribution of its variance is approximated\nby a consistent random subsampling method that is able to cope with the massive\nsize of the typical dataset. Based on the latter, we propose a statistic, and\nan estimation method that is able to represent the dynamic range concept\nconsistently. The behavior of the statistic is assessed based on a large\nnumerical experiment where we simulate dynamic compression on a selection of\nreal music signals. Application of the method to real data also shows how the\nproposed method can predict subjective experts' opinions about the hifi quality\nof a recording.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 18:24:28 GMT"}, {"version": "v2", "created": "Mon, 5 May 2014 15:37:58 GMT"}, {"version": "v3", "created": "Mon, 28 Jul 2014 17:10:17 GMT"}, {"version": "v4", "created": "Wed, 5 Nov 2014 09:57:57 GMT"}, {"version": "v5", "created": "Sat, 4 Apr 2015 12:47:31 GMT"}, {"version": "v6", "created": "Wed, 2 Sep 2015 08:22:14 GMT"}, {"version": "v7", "created": "Thu, 6 Oct 2016 15:48:58 GMT"}, {"version": "v8", "created": "Wed, 14 Feb 2018 07:53:40 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Coretto", "Pietro", ""], ["Giordano", "Francesco", ""]]}, {"id": "1312.0571", "submitter": "Ruixue Fan", "authors": "Ruixue Fan and Shaw-Hwa Lo", "title": "A Robust Model-free Approach for Rare Variants Association Studies\n  Incorporating Gene-Gene and Gene-Environmental Interactions", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0083057", "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently more and more evidence suggests that rare variants with much lower\nminor allele frequencies play significant roles in disease etiology. Advances\nin next-generation sequencing technologies will lead to many more rare variants\nassociation studies. Several statistical methods have been proposed to assess\nthe effect of rare variants by aggregating information from multiple loci\nacross a genetic region and testing the association between the phenotype and\naggregated genotype. One limitation of existing methods is that they only look\ninto the marginal effects of rare variants but do not systematically take into\naccount effects due to interactions among rare variants and between rare\nvariants and environmental factors. In this article, we propose the summation\nof partition approach (SPA), a robust model-free method that is designed\nspecifically for detecting both marginal effects and effects due to gene-gene\n(G-G) and gene-environmental (G-E) interactions for rare variants association\nstudies. SPA has three advantages. First, it accounts for the interaction\ninformation and gains considerable power in the presence of unknown and\ncomplicated G-G or G-E interactions. Secondly, it does not sacrifice the\nmarginal detection power; in the situation when rare variants only have\nmarginal effects it is comparable with the most competitive method in current\nliterature. Thirdly, it is easy to extend and can incorporate more complex\ninteractions; other practitioners and scientists can tailor the procedure to\nfit their own study friendly. Our simulation studies show that SPA is\nconsiderably more powerful than many existing methods in the presence of G-G\nand G-E interactions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 20:05:51 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Fan", "Ruixue", ""], ["Lo", "Shaw-Hwa", ""]]}, {"id": "1312.0646", "submitter": "Ale\\v{s} \\v{Z}iberna", "authors": "Ale\\v{s} \\v{Z}iberna", "title": "Generalized Blockmodeling of Valued Networks", "comments": "22 pages, 6 figures, 3 tables", "journal-ref": "Social networks, Jan. 2007, vol. 29, no. 1, p. 105-126", "doi": "10.1016/j.socnet.2006.04.002", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents several approaches to generalized blockmodeling of valued\nnetworks, where values of the ties are assumed to be measured on at least\ninterval scale. The first approach is a straightforward generalization of the\ngeneralized blockmodeling of binary networks (Doreian et al., 2005) to valued\nblockmodeling. The second approach is homogeneity blockmodeling. The basic idea\nof homogeneity blockmodeling is that the inconsistency of an empirical block\nwith its ideal block can be measured by within block variability of appropriate\nvalues. New ideal blocks appropriate for blockmodeling of valued networks are\npresented together with definitions of their block inconsistencies.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 22:24:08 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2013 08:20:27 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["\u017diberna", "Ale\u0161", ""]]}, {"id": "1312.0652", "submitter": "Adam Ciarleglio", "authors": "Adam Ciarleglio and R. Todd Ogden", "title": "Wavelet-Based Scalar-on-Function Finite Mixture Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical finite mixture regression is useful for modeling the relationship\nbetween scalar predictors and scalar responses arising from subpopulations\ndefined by the differing associations between those predictors and responses.\nHere we extend the classical finite mixture regression model to incorporate\nfunctional predictors by taking a wavelet-based approach in which we represent\nboth the functional predictors and the component-specific coefficient functions\nin terms of an appropriate wavelet basis. In the wavelet representation of the\nmodel, the coefficients corresponding to the functional covariates become the\npredictors. In this setting, we typically have many more predictors than\nobservations. Hence we use a lasso-type penalization to perform variable\nselection and estimation. We also consider an adaptive version of our\nwavelet-based model. We discuss the specification of the model, provide a\nfitting algorithm, and apply and evaluate our method using both simulations and\na real data set from a study of the relationship between cognitive ability and\ndiffusion tensor imaging measures in subjects with multiple sclerosis.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 22:51:06 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Ciarleglio", "Adam", ""], ["Ogden", "R. Todd", ""]]}, {"id": "1312.0761", "submitter": "Maria Giovanna Ranalli", "authors": "M. Giovanna Ranalli, Antonio Arcos, Maria del Mar Rueda, Annalisa\n  Teodoro", "title": "Calibration estimation in dual frame surveys", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survey statisticians make use of the available auxiliary information to\nimprove estimates. One important example is given by calibration estimation,\nthat seeks for new weights that are close (in some sense) to the basic design\nweights and that, at the same time, match benchmark constraints on available\nauxiliary information. Recently, multiple frame surveys have gained much\nattention and became largely used by statistical agencies and private\norganizations to decrease sampling costs or to reduce frame undercoverage\nerrors that could occur with the use of only a single sampling frame. Much\nattention has been devoted to the introduction of different ways of combining\nestimates coming from the different frames. We will extend the calibration\nparadigm, developed so far for one frame surveys, to the estimation of the\ntotal of a variable of interest in dual frame surveys as a general tool to\ninclude auxiliary information, also available at different levels. In fact,\ncalibration allows us to handle different types of auxiliary information and\ncan be shown to encompass as a special cases some of the methods already\nproposed in the literature. The theoretical properties of the proposed class of\nestimators are derived and discussed, a set of simulation studies is conducted\nto compare the efficiency of the procedure in presence of different sets of\nauxiliary variables. Finally, the proposed methodology is applied to data from\nthe Barometer of Culture of Andalusia survey.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 10:29:13 GMT"}, {"version": "v2", "created": "Thu, 22 May 2014 10:07:39 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Ranalli", "M. Giovanna", ""], ["Arcos", "Antonio", ""], ["Rueda", "Maria del Mar", ""], ["Teodoro", "Annalisa", ""]]}, {"id": "1312.0859", "submitter": "Utkarsh Dang", "authors": "Utkarsh J. Dang and Paul D. McNicholas", "title": "Accelerated Failure Time Models for Competing Risks in a Cluster\n  Weighted Modelling Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach for dealing with censored competing risks regression data is\nproposed. This is implemented by a mixture of accelerated failure time (AFT)\nmodels for a competing risks scenario within a cluster-weighted modelling (CWM)\nframework. Specifically, we make use of the log-normal AFT model here but any\ncommonly used AFT model can be utilized. The alternating expectation\nconditional maximization algorithm (AECM) is used for parameter estimation and\nbootstrapping for standard error estimation. Finally, we present our results on\nsome simulated and real competing risks data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 15:41:34 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Dang", "Utkarsh J.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1312.0906", "submitter": "Michael Betancourt", "authors": "M. J. Betancourt, Mark Girolami", "title": "Hamiltonian Monte Carlo for Hierarchical Models", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical modeling provides a framework for modeling the complex\ninteractions typical of problems in applied statistics. By capturing these\nrelationships, however, hierarchical models also introduce distinctive\npathologies that quickly limit the efficiency of most common methods of in-\nference. In this paper we explore the use of Hamiltonian Monte Carlo for\nhierarchical models and demonstrate how the algorithm can overcome those\npathologies in practical applications.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 19:08:11 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Betancourt", "M. J.", ""], ["Girolami", "Mark", ""]]}, {"id": "1312.0915", "submitter": "Kushal  Dey", "authors": "Kushal Kumar Dey and Sourabh Bhattacharya", "title": "On Geometric Ergodicity of Additive and Multiplicative Transformation\n  Based Markov Chain Monte Carlo in High Dimensions", "comments": "38 pages, has appeared in Brazilian Journal of Probability and\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Dutta and Bhattacharya (2013) introduced a novel Markov Chain Monte\nCarlo methodology that can simultaneously update all the components of high\ndimensional parameters using simple deterministic transformations of a\none-dimensional random variable drawn from any arbitrary distribution defined\non a relevant support. The methodology, which the authors refer to as\nTransformation-based Markov Chain Monte Carlo (TMCMC), greatly enhances\ncomputational speed and acceptance rate in high-dimensional problems. Two\nsignificant transformations associated with TMCMC are additive and\nmultiplicative transformations. Combinations of additive and multiplicative\ntransformations are also of much interest. In this work we investigate\ngeometric ergodicity associated with additive and multiplicative TMCMC, along\nwith their combinations, and illustrate their efficiency in practice with\nsimulation studies.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 19:46:36 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2015 03:37:03 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2016 18:32:15 GMT"}, {"version": "v4", "created": "Mon, 23 Jan 2017 14:44:34 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Dey", "Kushal Kumar", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1312.0961", "submitter": "Neville Ball", "authors": "N. Ball", "title": "Rigorous Confidence Intervals on Critical Thresholds in 3 Dimensions", "comments": null, "journal-ref": null, "doi": "10.1007/s10955-014-1018-7", "report-no": null, "categories": "stat.ME math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the method of Balister, Bollob\\'as and Walters for determining\nrigorous confidence intervals for the critical threshold of two dimensional\nlattices to three (and higher) dimensional lattices. We describe a method for\ndetermining a full confidence interval and apply it to show that the critical\nthreshold for bond percolation on the simple cubic lattice is between 0.2485\nand 0.2490 with 99.9999% confidence, and the critical threshold for site\npercolation on the same lattice is between 0.3110 and 0.3118 with 99.9999%\nconfidence.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 17:27:34 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Ball", "N.", ""]]}, {"id": "1312.1268", "submitter": "Peter Aronow", "authors": "Peter M. Aronow, Alexander Coppock, Forrest W. Crawford, Donald P.\n  Green", "title": "Combining List Experiment and Direct Question Estimates of Sensitive\n  Behavior Prevalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survey respondents may give untruthful answers to sensitive questions when\nasked directly. In recent years, researchers have turned to the list experiment\n(also known as the item count technique) to overcome this difficulty. While\nlist experiments may be less prone to bias than direct questioning, list\nexperiments are also more susceptible to sampling variability. We show that\nresearchers do not have to abandon direct questioning altogether in order to\ngain the advantages of list experimentation. We develop a nonparametric\nestimator of the prevalence of sensitive behaviors that combines list\nexperimentation and direct questioning. We prove that this estimator is\nasymptotically more efficient than the standard difference-in-means estimator,\nand we provide a basis for inference using Wald-type confidence intervals.\nAdditionally, leveraging information from the direct questioning, we derive two\nnonparametric placebo tests of the identifying assumptions for the list\nexperiment. We demonstrate the effectiveness of our combined estimator and\nplacebo tests with an original survey experiment.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 18:21:48 GMT"}, {"version": "v2", "created": "Sun, 1 Jun 2014 23:29:17 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Aronow", "Peter M.", ""], ["Coppock", "Alexander", ""], ["Crawford", "Forrest W.", ""], ["Green", "Donald P.", ""]]}, {"id": "1312.1473", "submitter": "Francesco Audrino Prof.Dr.", "authors": "Francesco Audrino and Lorenzo Camponovo", "title": "Oracle Properties and Finite Sample Inference of the Adaptive Lasso for\n  Time Series Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive new theoretical results on the properties of the adaptive least\nabsolute shrinkage and selection operator (adaptive lasso) for time series\nregression models. In particular, we investigate the question of how to conduct\nfinite sample inference on the parameters given an adaptive lasso model for\nsome fixed value of the shrinkage parameter. Central in this study is the test\nof the hypothesis that a given adaptive lasso parameter equals zero, which\ntherefore tests for a false positive. To this end we construct a simple testing\nprocedure and show, theoretically and empirically through extensive Monte Carlo\nsimulations, that the adaptive lasso combines efficient parameter estimation,\nvariable selection, and valid finite sample inference in one step. Moreover, we\nanalytically derive a bias correction factor that is able to significantly\nimprove the empirical coverage of the test on the active variables. Finally, we\napply the introduced testing procedure to investigate the relation between the\nshort rate dynamics and the economy, thereby providing a statistical foundation\n(from a model choice perspective) to the classic Taylor rule monetary policy\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 09:04:43 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Audrino", "Francesco", ""], ["Camponovo", "Lorenzo", ""]]}, {"id": "1312.1591", "submitter": "James Barrett", "authors": "James E. Barrett and Anthony C. C. Coolen", "title": "Gaussian process regression for survival data with competing risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply Gaussian process (GP) regression, which provides a powerful\nnon-parametric probabilistic method of relating inputs to outputs, to survival\ndata consisting of time-to-event and covariate measurements. In this context,\nthe covariates are regarded as the `inputs' and the event times are the\n`outputs'. This allows for highly flexible inference of non-linear\nrelationships between covariates and event times. Many existing methods, such\nas the ubiquitous Cox proportional hazards model, focus primarily on the hazard\nrate which is typically assumed to take some parametric or semi-parametric\nform. Our proposed model belongs to the class of accelerated failure time\nmodels where we focus on directly characterising the relationship between\ncovariates and event times without any explicit assumptions on what form the\nhazard rates take. It is straightforward to include various types and\ncombinations of censored and truncated observations. We apply our approach to\nboth simulated and experimental data. We then apply multiple output GP\nregression, which can handle multiple potentially correlated outputs for each\ninput, to competing risks survival data where multiple event types can occur.\nBy tuning one of the model parameters we can control the extent to which the\nmultiple outputs (the time-to-event for each risk) are dependent thus allowing\nthe specification of correlated risks. Simulation studies suggest that in some\ncases assuming dependence can lead to more accurate predictions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 15:59:27 GMT"}, {"version": "v2", "created": "Fri, 5 Sep 2014 11:04:14 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Barrett", "James E.", ""], ["Coolen", "Anthony C. C.", ""]]}, {"id": "1312.1622", "submitter": "Andrew Dahl", "authors": "Andy Dahl, Victoria Hore, Valentina Iotchkova, Jonathan Marchini", "title": "Network inference in matrix-variate Gaussian models with non-independent\n  noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring a graphical model or network from observational data from a large\nnumber of variables is a well studied problem in machine learning and\ncomputational statistics. In this paper we consider a version of this problem\nthat is relevant to the analysis of multiple phenotypes collected in genetic\nstudies. In such datasets we expect correlations between phenotypes and between\nindividuals. We model observations as a sum of two matrix normal variates such\nthat the joint covariance function is a sum of Kronecker products. This model,\nwhich generalizes the Graphical Lasso, assumes observations are correlated due\nto known genetic relationships and corrupted with non-independent noise. We\nhave developed a computationally efficient EM algorithm to fit this model. On\nsimulated datasets we illustrate substantially improved performance in network\nreconstruction by allowing for a general noise distribution.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 17:24:10 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Dahl", "Andy", ""], ["Hore", "Victoria", ""], ["Iotchkova", "Valentina", ""], ["Marchini", "Jonathan", ""]]}, {"id": "1312.1869", "submitter": "Anjishnu  Banerjee", "authors": "Anjishnu Banerjee, Joshua Vogelstein, David Dunson", "title": "Parallel inversion of huge covariance matrices", "comments": "17 pages, 3 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An extremely common bottleneck encountered in statistical learning algorithms\nis inversion of huge covariance matrices, examples being in evaluating Gaussian\nlikelihoods for a large number of data points. We propose general parallel\nalgorithms for inverting positive definite matrices, which are nearly rank\ndeficient. Such matrix inversions are needed in Gaussian process computations,\namong other settings, and remain a bottleneck even with the increasing\nliterature on low rank approximations. We propose a general class of algorithms\nfor parallelizing computations to dramatically speed up computation time by\norders of magnitude exploiting multicore architectures. We implement our\nalgorithm on a cloud computing platform, providing pseudo and actual code. The\nalgorithm can be easily implemented on any multicore parallel computing\nresource. Some illustrations are provided to give a flavor for the gains and\nwhat becomes possible in freeing up this bottleneck.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 14:14:31 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Banerjee", "Anjishnu", ""], ["Vogelstein", "Joshua", ""], ["Dunson", "David", ""]]}, {"id": "1312.1903", "submitter": "Helen Ogden", "authors": "Helen Ogden", "title": "A sequential reduction method for inference in generalized linear mixed\n  models", "comments": "17 pages, 3 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood for the parameters of a generalized linear mixed model\ninvolves an integral which may be of very high dimension. Because of this\nintractability, many approximations to the likelihood have been proposed, but\nall can fail when the model is sparse, in that there is only a small amount of\ninformation available on each random effect. The sequential reduction method\ndescribed in this paper exploits the dependence structure of the posterior\ndistribution of the random effects to reduce substantially the cost of finding\nan accurate approximation to the likelihood in models with sparse structure.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 16:06:38 GMT"}, {"version": "v2", "created": "Fri, 29 Aug 2014 08:38:58 GMT"}], "update_date": "2014-09-01", "authors_parsed": [["Ogden", "Helen", ""]]}, {"id": "1312.2041", "submitter": "John Storey", "authors": "Wei Hao, Minsun Song, and John D. Storey", "title": "Probabilistic models of genetic variation in structured populations\n  applied to global human studies", "comments": "Wei Hao and Minsun Song contributed equally to this work", "journal-ref": null, "doi": "10.1093/bioinformatics/btv641", "report-no": null, "categories": "q-bio.PE q-bio.GN q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern population genetics studies typically involve genome-wide genotyping\nof individuals from a diverse network of ancestries. An important, unsolved\nproblem is how to formulate and estimate probabilistic models of observed\ngenotypes that allow for complex population structure. We formulate two general\nprobabilistic models, and we propose computationally efficient algorithms to\nestimate them. First, we show how principal component analysis (PCA) can be\nutilized to estimate a general model that includes the well-known\nPritchard-Stephens-Donnelly mixed-membership model as a special case. Noting\nsome drawbacks of this approach, we introduce a new \"logistic factor analysis\"\n(LFA) framework that seeks to directly model the logit transformation of\nprobabilities underlying observed genotypes in terms of latent variables that\ncapture population structure. We demonstrate these advances on data from the\nhuman genome diversity panel and 1000 genomes project, where we are able to\nidentify SNPs that are highly differentiated with respect to structure while\nmaking minimal modeling assumptions.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 00:14:17 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 03:41:05 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Hao", "Wei", ""], ["Song", "Minsun", ""], ["Storey", "John D.", ""]]}, {"id": "1312.2079", "submitter": "Hasinur Khan", "authors": "Md Hasinur Rahaman Khan and J. Ewart H. Shaw", "title": "Variable Selection for Survival Data with A Class of Adaptive Elastic\n  Net Techniques", "comments": "28 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": "CRiSM working paper, P. No. 13-17, Department of Statistics,\n  University of Warwick, UK", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accelerated failure time (AFT) models have proved useful in many\ncontexts, though heavy censoring (as for example in cancer survival) and high\ndimensionality (as for example in microarray data) cause difficulties for model\nfitting and model selection. We propose new approaches to variable selection\nfor censored data, based on AFT models optimized using regularized weighted\nleast squares. The regularized technique uses a mixture of L1 and L2 norm\npenalties under two proposed elastic net type approaches. One is the the\nadaptive elastic net and the other is weighted elastic net. The approaches\nextend the original approaches proposed by Ghosh (2007), and Hong and Zhang\n(2010) respectively. We also extend the two proposed approaches by adding\ncensoring observations as constraints into their model optimization frameworks.\nThe approaches are evaluated on microarray and by simulation. We compare the\nperformance of these approaches with six other variable selection\ntechniques--three are generally used for censored data and the other three are\ncorrelation-based greedy methods used for high-dimensional data.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 10:03:14 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Khan", "Md Hasinur Rahaman", ""], ["Shaw", "J. Ewart H.", ""]]}, {"id": "1312.2098", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Christopher R. Genovese, Larry Wasserman", "title": "Uncertainty Measures and Limiting Distributions for Filament Estimation", "comments": "Submitted to 30th Annual Symposium on Computational Geometry\n  (SoCG2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A filament is a high density, connected region in a point cloud. There are\nseveral methods for estimating filaments but these methods do not provide any\nmeasure of uncertainty. We give a definition for the uncertainty of estimated\nfilaments and we study statistical properties of the estimated filaments. We\nshow how to estimate the uncertainty measures and we construct confidence sets\nbased on a bootstrapping technique. We apply our methods to astronomy data and\nearthquake data.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 14:46:34 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Genovese", "Christopher R.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1312.2248", "submitter": "Antonio Irpino PhD", "authors": "Antonio Irpino", "title": "Basic univariate and bivariate statistics for symbolic data: a critical\n  review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some proofs of the problems of the basic statistics proposed for numeric\nsymbolic data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2013 19:36:39 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Irpino", "Antonio", ""]]}, {"id": "1312.2291", "submitter": "Paulo C. Marques F.", "authors": "Paulo C. Marques F. and Carlos A. de B. Pereira", "title": "Predictive analysis of microarray data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarray gene expression data are analyzed by means of a Bayesian\nnonparametric model, with emphasis on prediction of future observables,\nyielding a method for selection of differentially expressed genes and a\nclassifier.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 01:53:06 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 20:25:47 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["F.", "Paulo C. Marques", ""], ["Pereira", "Carlos A. de B.", ""]]}, {"id": "1312.2369", "submitter": "Vincent Bremhorst", "authors": "Vincent Bremhorst and Philippe Lambert", "title": "Flexible estimation in cure survival models using Bayesian P-splines", "comments": "31 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the analysis of survival data, it is usually assumed that any unit will\nexperience the event of interest if it is observed for a sufficient long time.\nHowever, one can explicitly assume that an unknown proportion of the population\nunder study will never experience the monitored event. The promotion time\nmodel, which has a biological motivation, is one of the survival models taking\nthis feature into account. The promotion time model assumes that the failure\ntime of each subject is generated by the minimum of N latent event times which\nare independent with a common distribution independent of N. We propose an\nextension which allows the covariates to influence simultaneously the\nprobability of being cured and the latent distribution. We estimate the latent\ndistribution using a flexible Cox proportional hazard model where the logarithm\nof the baseline hazard function is specified using Bayesian P-splines.\nIntroducing covariates in the latent distribution implies that the population\nhazard function might not have a proportional hazard structure. However, the\nuse of the P-splines provides a smooth estimation of the population hazard\nratio over time. We propose a restricted use of the model when the follow up of\nthe study is not sufficiently long. A simulation study evaluating the accuracy\nof our methodology is presented. The proposed model is illustrated on data from\nthe phase III Melanoma e1684 clinical trial.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 10:28:40 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 14:11:59 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Bremhorst", "Vincent", ""], ["Lambert", "Philippe", ""]]}, {"id": "1312.2393", "submitter": "Isobel Claire Gormley Dr.", "authors": "Gift Nyamundanda, Isobel Claire Gormley and Lorraine Brennan", "title": "A dynamic probabilistic principal components model for the analysis of\n  longitudinal metabolomic data", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a longitudinal metabolomics study, multiple metabolites are measured from\nseveral observations at many time points. Interest lies in reducing the\ndimensionality of such data and in highlighting influential metabolites which\nchange over time. A dynamic probabilistic principal components analysis (DPPCA)\nmodel is proposed to achieve dimension reduction while appropriately modelling\nthe correlation due to repeated measurements. This is achieved by assuming an\nautoregressive model for some of the model parameters. Linear mixed models are\nsubsequently used to identify influential metabolites which change over time.\nThe proposed model is used to analyse data from a longitudinal metabolomics\nanimal study.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 11:38:43 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Nyamundanda", "Gift", ""], ["Gormley", "Isobel Claire", ""], ["Brennan", "Lorraine", ""]]}, {"id": "1312.2533", "submitter": "Hasinur Khan", "authors": "Md Hasinur Rahaman Khan, J. Ewart H. Shaw", "title": "On Dealing with Censored Largest Observations under Weighted Least\n  Squares", "comments": "23 pages, 7 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": "CRiSM working paper, Paper No. 13-07 (2013)", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When observations are subject to right censoring, weighted least squares with\nappropriate weights (to adjust for censoring) is sometimes used for parameter\nestimation. With Stute's weighted least squares method, when the largest\nobservation is censored ($Y_{(n)}^+$), it is natural to apply the\nredistribution to the right algorithm of Efron (1967). However, Efron's\nredistribution algorithm can lead to bias and inefficiency in estimation. This\nstudy explains the issues clearly and proposes some alternative ways of\ntreating $Y_{(n)}^+$. The first four proposed approaches are based on the well\nknown Buckley--James (1979) method of imputation with the Efron's tail\ncorrection and the last approach is indirectly based on a general mean\nimputation technique in literature. All the new schemes use penalized weighted\nleast squares optimized by quadratic programming implemented with the\naccelerated failure time models. Furthermore, two novel additional imputation\napproaches are proposed to impute the tail tied censored observations that are\noften found in survival analysis with heavy censoring. Several simulation\nstudies and real data analysis demonstrated that the proposed approaches\ngenerally outperform Efron's redistribution approach and lead to considerably\nsmaller mean squared error and bias estimates.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 18:14:47 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 09:30:09 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Khan", "Md Hasinur Rahaman", ""], ["Shaw", "J. Ewart H.", ""]]}, {"id": "1312.2645", "submitter": "Sharmodeep Bhattacharyya", "authors": "Sharmodeep Bhattacharyya, Peter J. Bickel", "title": "Subsampling bootstrap of count features of networks", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1338 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 6, 2384-2411", "doi": "10.1214/15-AOS1338", "report-no": "IMS-AOS-AOS1338", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of stochastic models of networks is quite important in light of the\nhuge influx of network data in social, information and bio sciences, but a\nproper statistical analysis of features of different stochastic models of\nnetworks is still underway. We propose bootstrap subsampling methods for\nfinding empirical distribution of count features or ``moments'' (Bickel, Chen\nand Levina [Ann. Statist. 39 (2011) 2280-2301]) and smooth functions of these\nfeatures for the networks. Using these methods, we cannot only estimate the\nvariance of count features but also get good estimates of such feature counts,\nwhich are usually expensive to compute numerically in large networks. In our\npaper, we prove theoretical properties of the bootstrap estimates of variance\nof the count features as well as show their efficacy through simulation. We\nalso use the method on some real network data for estimation of variance and\nexpectation of some count features.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 02:27:25 GMT"}, {"version": "v2", "created": "Fri, 10 Oct 2014 15:14:35 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 10:06:22 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Bhattacharyya", "Sharmodeep", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1312.2753", "submitter": "Paul Harris", "authors": "Binbin Lu, Paul Harris, Martin Charlton, Chris Brunsdon", "title": "The GWmodel R package: Further Topics for Exploring Spatial\n  Heterogeneity using Geographically Weighted Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a collection of local models, termed geographically\nweighted (GW) models, that can be found within the GWmodel R package. A GW\nmodel suits situations when spatial data are poorly described by the global\nform, and for some regions the localised fit provides a better description. The\napproach uses a moving window weighting technique, where a collection of local\nmodels are estimated at target locations. Commonly, model parameters or outputs\nare mapped so that the nature of spatial heterogeneity can be explored and\nassessed. In particular, we present case studies using: (i) GW summary\nstatistics and a GW principal components analysis; (ii) advanced GW regression\nfits and diagnostics; (iii) associated Monte Carlo significance tests for\nnon-stationarity; (iv) a GW discriminant analysis; and (v) enhanced kernel\nbandwidth selection procedures. General Election data sets from the Republic of\nIreland and US are used for demonstration. This study is designed to complement\na companion GWmodel study, which focuses on basic and robust GW models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 11:01:23 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Lu", "Binbin", ""], ["Harris", "Paul", ""], ["Charlton", "Martin", ""], ["Brunsdon", "Chris", ""]]}, {"id": "1312.2923", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski, Sofia C. Olhede, Jonathan M. Lilly, Eric Danioux", "title": "Lagrangian Time Series Models for Ocean Surface Drifter Trajectories", "comments": "21 pages, 10 figures", "journal-ref": "Journal of the Royal Statistical Society (Series C, Applied\n  Statistics), 65(1), 29-50, 2016", "doi": "10.1111/rssc.12112", "report-no": null, "categories": "stat.AP physics.ao-ph physics.flu-dyn stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes stochastic models for the analysis of ocean surface\ntrajectories obtained from freely-drifting satellite-tracked instruments. The\nproposed time series models are used to summarise large multivariate datasets\nand infer important physical parameters of inertial oscillations and other\nocean processes. Nonstationary time series methods are employed to account for\nthe spatiotemporal variability of each trajectory. Because the datasets are\nlarge, we construct computationally efficient methods through the use of\nfrequency-domain modelling and estimation, with the data expressed as\ncomplex-valued time series. We detail how practical issues related to sampling\nand model misspecification may be addressed using semi-parametric techniques\nfor time series, and we demonstrate the effectiveness of our stochastic models\nthrough application to both real-world data and to numerical model output.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 19:34:43 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 22:44:56 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2015 00:05:09 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Lilly", "Jonathan M.", ""], ["Danioux", "Eric", ""]]}, {"id": "1312.3078", "submitter": "Bernhard Klar", "authors": "Christian Goldmann, Bernhard Klar and Simos G. Meintanis", "title": "Data Transformations and Goodness-of-Fit Tests for Type-II Right\n  Censored Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest several goodness-of-fit methods which are appropriate with Type-II\nright censored data. Our strategy is to transform the original observations\nfrom a censored sample into an approximately i.i.d. sample of normal variates\nand then perform a standard goodness-of-fit test for normality on the\ntransformed observations. A simulation study with several well known parametric\ndistributions under testing reveals the sampling properties of the methods. We\nalso provide theoretical analysis of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 08:44:33 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Goldmann", "Christian", ""], ["Klar", "Bernhard", ""], ["Meintanis", "Simos G.", ""]]}, {"id": "1312.3398", "submitter": "Cyrus Samii", "authors": "Peter M. Aronow, Cyrus Samii, Valentina A. Assenova", "title": "Cluster-Robust Variance Estimation for Dyadic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dyadic data are common in the social sciences, although inference for such\nsettings involves accounting for a complex clustering structure. Many analyses\nin the social sciences fail to account for the fact that multiple dyads share a\nmember, and that errors are thus likely correlated across these dyads. We\npropose a nonparametric sandwich-type robust variance estimator for linear\nregression to account for such clustering in dyadic data. We enumerate\nconditions for estimator consistency. We also extend our results to repeated\nand weighted observations, including directed dyads and longitudinal data, and\nprovide an implementation for generalized linear models such as logistic\nregression. We examine empirical performance with simulations and applications\nto international relations and speed dating.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 03:48:34 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2014 13:10:39 GMT"}, {"version": "v3", "created": "Mon, 3 Nov 2014 00:34:02 GMT"}, {"version": "v4", "created": "Tue, 5 May 2015 23:03:57 GMT"}, {"version": "v5", "created": "Sat, 23 May 2015 01:56:00 GMT"}, {"version": "v6", "created": "Fri, 12 Jun 2015 18:21:56 GMT"}, {"version": "v7", "created": "Mon, 6 Jul 2015 19:45:45 GMT"}, {"version": "v8", "created": "Wed, 22 Jul 2015 21:18:35 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Aronow", "Peter M.", ""], ["Samii", "Cyrus", ""], ["Assenova", "Valentina A.", ""]]}, {"id": "1312.3482", "submitter": "Efstratia Charitidou", "authors": "Efstratia Charitidou, Dimitris Fouskakis and Ioannis Ntzoufras", "title": "Bayesian transformation family selection: moving towards a transformed\n  Gaussian universe", "comments": "29 pages, 3 figures, 4 tables, 2 Appendix sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of transformation selection is thoroughly treated from a Bayesian\nperspective. Several families of transformations are considered with a view to\nachieving normality: the Box-Cox, the Modulus, the Yeo & Johnson and the Dual\ntransformation. Markov chain Monte Carlo algorithms have been constructed in\norder to sample from the posterior distribution of the transformation parameter\n$\\lambda_T$ associated with each competing family $T$. We investigate different\napproaches to constructing compatible prior distributions for $\\lambda_T$ over\nalternative transformation families, using a unit-information power-prior\napproach and an alternative normal prior with approximate unit-information\ninterpretation. Selection and discrimination between different transformation\nfamilies is attained via posterior model probabilities. We demonstrate the\nefficiency of our approach using a variety of simulated datasets. Although\nthere is no choice of transformation family that can be universally applied to\nall problems, empirical evidence suggests that some particular data structures\nare best treated by specific transformation families. For example, skewness is\nassociated with the Box-Cox family while fat-tailed distributions are\nefficiently treated using the Modulus transformation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 13:47:05 GMT"}], "update_date": "2013-12-13", "authors_parsed": [["Charitidou", "Efstratia", ""], ["Fouskakis", "Dimitris", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "1312.3516", "submitter": "Bharath Sriperumbudur", "authors": "Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Aapo\n  Hyv\\\"arinen and Revant Kumar", "title": "Density Estimation in Infinite Dimensional Exponential Families", "comments": "58 pages, 8 figures; Fixed some errors and typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an infinite dimensional exponential family,\n$\\mathcal{P}$ of probability densities, which are parametrized by functions in\na reproducing kernel Hilbert space, $H$ and show it to be quite rich in the\nsense that a broad class of densities on $\\mathbb{R}^d$ can be approximated\narbitrarily well in Kullback-Leibler (KL) divergence by elements in\n$\\mathcal{P}$. The main goal of the paper is to estimate an unknown density,\n$p_0$ through an element in $\\mathcal{P}$. Standard techniques like maximum\nlikelihood estimation (MLE) or pseudo MLE (based on the method of sieves),\nwhich are based on minimizing the KL divergence between $p_0$ and\n$\\mathcal{P}$, do not yield practically useful estimators because of their\ninability to efficiently handle the log-partition function. Instead, we propose\nan estimator, $\\hat{p}_n$ based on minimizing the \\emph{Fisher divergence},\n$J(p_0\\Vert p)$ between $p_0$ and $p\\in \\mathcal{P}$, which involves solving a\nsimple finite-dimensional linear system. When $p_0\\in\\mathcal{P}$, we show that\nthe proposed estimator is consistent, and provide a convergence rate of\n$n^{-\\min\\left\\{\\frac{2}{3},\\frac{2\\beta+1}{2\\beta+2}\\right\\}}$ in Fisher\ndivergence under the smoothness assumption that $\\log\np_0\\in\\mathcal{R}(C^\\beta)$ for some $\\beta\\ge 0$, where $C$ is a certain\nHilbert-Schmidt operator on $H$ and $\\mathcal{R}(C^\\beta)$ denotes the image of\n$C^\\beta$. We also investigate the misspecified case of $p_0\\notin\\mathcal{P}$\nand show that $J(p_0\\Vert\\hat{p}_n)\\rightarrow \\inf_{p\\in\\mathcal{P}}J(p_0\\Vert\np)$ as $n\\rightarrow\\infty$, and provide a rate for this convergence under a\nsimilar smoothness condition as above. Through numerical simulations we\ndemonstrate that the proposed estimator outperforms the non-parametric kernel\ndensity estimator, and that the advantage with the proposed estimator grows as\n$d$ increases.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 15:09:25 GMT"}, {"version": "v2", "created": "Sat, 26 Apr 2014 16:26:29 GMT"}, {"version": "v3", "created": "Sun, 2 Nov 2014 03:51:53 GMT"}, {"version": "v4", "created": "Fri, 26 May 2017 14:42:48 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Sriperumbudur", "Bharath", ""], ["Fukumizu", "Kenji", ""], ["Gretton", "Arthur", ""], ["Hyv\u00e4rinen", "Aapo", ""], ["Kumar", "Revant", ""]]}, {"id": "1312.3723", "submitter": "Shanshan Wang", "authors": "Shanshan Wang and Hengjian Cui", "title": "Partial Penalized Likelihood Ratio Test under Sparse Case", "comments": "26 pages, 3 figures,6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is concern with testing the low-dimensional parameters of interest\nwith divergent dimensional data and variable selection for the rest under the\nsparse case. A consistent test via the partial penalized likelihood approach,\ncalled the partial penalized likelihood ratio test statistic is derived, and\nits asymptotic distributions under the null hypothesis and the local\nalternatives of order $n^{-1/2}$ are obtained under some regularity conditions.\nMeanwhile, the oracle property of the partial penalized likelihood estimator\nalso holds. The proposed partial penalized likelihood ratio test statistic\noutperforms the full penalized likelihood ratio test statistic in term of size\nand power, and performs as well as the classical likelihood ratio test\nstatistic. Moreover, the proposed method obtains the variable selection results\nas well as the p-values of testing. Numerical simulations and an analysis of\nProstate Cancer data confirm our theoretical findings and demonstrate the\npromising performance of the proposed partial penalized likelihood in\nhypothesis testing and variable selection.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 07:52:29 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 02:48:37 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Wang", "Shanshan", ""], ["Cui", "Hengjian", ""]]}, {"id": "1312.3958", "submitter": "Christian R\\\"over", "authors": "Christian R\\\"over, Stefan Andreas and Tim Friede", "title": "Evidence synthesis for count distributions based on heterogeneous and\n  incomplete aggregated data", "comments": "17 pages, 3 figures, 2 tables", "journal-ref": "Biometrical Journal, 58(1):170-185, 2016", "doi": "10.1002/bimj.201300288", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of count data is commonly done using Poisson models. Negative\nbinomial models are a straightforward and readily motivated generalization for\nthe case of overdispersed data, i.e., when the observed variance is greater\nthan expected under a Poissonian model. Rate and overdispersion parameters then\nneed to be considered jointly, which in general is not trivial. Here we are\nconcerned with evidence synthesis in the case where the reporting of data is\nrather heterogeneous, i.e., events are reported either in terms of mean event\ncounts, the proportion of event-free patients, or rate estimates and standard\nerrors. Either figure carries some information about the relevant parameters,\nand it is the joint modeling that allows for coherent inference on the\nparameters of interest. The methods are motivated and illustrated by a\nsystematic review in chronic obstructive pulmonary disease.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 21:19:06 GMT"}, {"version": "v2", "created": "Mon, 23 Jun 2014 22:23:22 GMT"}, {"version": "v3", "created": "Wed, 19 Nov 2014 10:23:20 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["R\u00f6ver", "Christian", ""], ["Andreas", "Stefan", ""], ["Friede", "Tim", ""]]}, {"id": "1312.4058", "submitter": "Hasinur Khan", "authors": "Md Hasinur Rahaman Khan, J. Ewart H. Shaw", "title": "Robust Bias Estimation for Kaplan-Meier Survival Estimator with\n  Jackknifing", "comments": "17 pages, 3 figures, 5 tables. CRiSM working paper, Paper No. 13-09\n  (2013), University of Warwick, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For studying or reducing the bias of functionals of the Kaplan-Meier survival\nestimator, the jackknifing approach of Stute and Wang (1994) is natural. We\nhave studied the behavior of the jackknife estimate of bias under different\nconfigurations of the censoring level, sample size, and the censoring and\nsurvival time distributions. The empirical research reveals some new findings\nabout robust calculation of the bias, particularly for higher censoring levels.\nWe have extended their jackknifing approach to cover the case where the largest\nobservation is censored, using the imputation methods for the largest\nobservations proposed in Khan and Shaw (2013b). This modification to the\nexisting formula reduces the number of conditions for creating jackknife bias\nestimates to one from the original two, and also avoids the problem that the\nKaplan--Meier estimator can be badly underestimated by the existing jackknife\nformula.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2013 16:12:23 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Khan", "Md Hasinur Rahaman", ""], ["Shaw", "J. Ewart H.", ""]]}, {"id": "1312.4094", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Ivan Fernandez-Val, Stefan Hoderlein, Hajo\n  Holzmann, and Whitney Newey", "title": "Nonparametric Identification in Panels using Quantiles", "comments": "36 pages, 1 table, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers identification and estimation of ceteris paribus effects\nof continuous regressors in nonseparable panel models with time homogeneity.\nThe effects of interest are derivatives of the average and quantile structural\nfunctions of the model. We find that these derivatives are identified with two\ntime periods for \"stayers\", i.e. for individuals with the same regressor values\nin two time periods. We show that the identification results carry over to\nmodels that allow location and scale time effects. We propose nonparametric\nseries methods and a weighted bootstrap scheme to estimate and make inference\non the identified effects. The bootstrap proposed allows uniform inference for\nfunction-valued parameters such as quantile effects uniformly over a region of\nquantile indices and/or regressor values. An empirical application to Engel\ncurve estimation with panel data illustrates the results.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2013 00:31:29 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2014 17:15:58 GMT"}, {"version": "v3", "created": "Tue, 5 Aug 2014 15:16:35 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fernandez-Val", "Ivan", ""], ["Hoderlein", "Stefan", ""], ["Holzmann", "Hajo", ""], ["Newey", "Whitney", ""]]}, {"id": "1312.4479", "submitter": "Jean-Baptiste Durand", "authors": "Pierre Fernique (VP, AGAP), Jean-Baptiste Durand (VP, INRIA Grenoble\n  Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann), Yann Gu\\'edon (VP, AGAP)", "title": "Parametric Modelling of Multivariate Count Data Using Probabilistic\n  Graphical Models", "comments": null, "journal-ref": "3rd Workshop on Algorithmic issues for Inference in Graphical\n  Models - AIGM13, Paris : France (2013)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate count data are defined as the number of items of different\ncategories issued from sampling within a population, which individuals are\ngrouped into categories. The analysis of multivariate count data is a recurrent\nand crucial issue in numerous modelling problems, particularly in the fields of\nbiology and ecology (where the data can represent, for example, children counts\nassociated with multitype branching processes), sociology and econometrics. We\nfocus on I) Identifying categories that appear simultaneously, or on the\ncontrary that are mutually exclusive. This is achieved by identifying\nconditional independence relationships between the variables; II)Building\nparsimonious parametric models consistent with these relationships; III)\nCharacterising and testing the effects of covariates on the joint distribution\nof the counts. To achieve these goals, we propose an approach based on\ngraphical probabilistic models, and more specifically partially directed\nacyclic graphs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 19:38:35 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Fernique", "Pierre", "", "VP, AGAP"], ["Durand", "Jean-Baptiste", "", "VP, INRIA Grenoble\n  Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann"], ["Gu\u00e9don", "Yann", "", "VP, AGAP"]]}, {"id": "1312.4584", "submitter": "Marco Oesting", "authors": "Marco Oesting and Martin Schlather and Petra Friederichs", "title": "Statistical Post-Processing of Forecasts for Extremes Using Bivariate\n  Brown-Resnick Processes with an Application to Wind Gusts", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the forecasts of weather extremes, we propose a joint spatial\nmodel for the observations and the forecasts, based on a bivariate\nBrown-Resnick process. As the class of stationary bivariate Brown-Resnick\nprocesses is fully characterized by the class of pseudo cross-variograms, we\ncontribute to the theorical understanding of pseudo cross-variograms refining\nthe knowledge of the asymptotic behaviour of all their components and\nintroducing a parsimonious, but flexible parametric model. Both findings are of\ninterest in classical geostatistics on their own. The proposed model is applied\nto real observation and forecast data for extreme wind gusts at 119 stations in\nNorthern Germany.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 22:39:25 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 08:42:46 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Oesting", "Marco", ""], ["Schlather", "Martin", ""], ["Friederichs", "Petra", ""]]}, {"id": "1312.4645", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts, Rob Hall, and Stephen E. Fienberg", "title": "A Bayesian Approach to Graphical Record Linkage and De-duplication", "comments": "39 pages, 8 figures, 8 tables. Longer version of arXiv:1403.0211, In\n  press, Journal of the American Statistical Association: Theory and Methods\n  (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised approach for linking records across arbitrarily\nmany files, while simultaneously detecting duplicate records within files. Our\nkey innovation involves the representation of the pattern of links between\nrecords as a bipartite graph, in which records are directly linked to latent\ntrue individuals, and only indirectly linked to other records. This flexible\nrepresentation of the linkage structure naturally allows us to estimate the\nattributes of the unique observable people in the population, calculate\ntransitive linkage probabilities across records (and represent this visually),\nand propagate the uncertainty of record linkage into later analyses. Our method\nmakes it particularly easy to integrate record linkage with post-processing\nprocedures such as logistic regression, capture-recapture, etc. Our linkage\nstructure lends itself to an efficient, linear-time, hybrid Markov chain Monte\nCarlo algorithm, which overcomes many obstacles encountered by previously\nrecord linkage approaches, despite the high-dimensional parameter space. We\nillustrate our method using longitudinal data from the National Long Term Care\nSurvey and with data from the Italian Survey on Household and Wealth, where we\nassess the accuracy of our method and show it to be better in terms of error\nrates and empirical scalability than other approaches in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 05:20:04 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 20:18:29 GMT"}, {"version": "v3", "created": "Fri, 31 Oct 2014 21:18:51 GMT"}, {"version": "v4", "created": "Sat, 31 Oct 2015 03:13:21 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Steorts", "Rebecca C.", ""], ["Hall", "Rob", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1312.4675", "submitter": "Gael Martin Prof", "authors": "Simone D. Grose, Gael M. Martin and Donald S. Poskitt", "title": "Bias Correction of Persistence Measures in Fractionally Integrated\n  Models", "comments": "This paper is a revision of http://arxiv.org/abs/1312.4675 and hence\n  supersedes that paper. The paper is also now forthcoming in Journal of Time\n  Series Analysis (JTSA). A Supplementary Appendix to the paper is currently\n  available at\n  http://users.monash.edu.au/~gmartin/Grose_Martin_Poskitt_on_line_appendix.pdf\n  and will eventually be available on line at JTSA", "journal-ref": "Published in Journal of Time Series Analysis, 2015. Volume 36,\n  721-740", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the accuracy of bootstrap-based bias correction of\npersistence measures for long memory fractionally integrated processes. The\nbootstrap method is based on the semi-parametric sieve approach, with the\ndynamics in the long memory process captured by an autoregressive\napproximation. With a view to improving accuracy, the sieve method is also\napplied to data pre-filtered by a semi-parametric estimate of the long memory\nparameter. Both versions of the bootstrap technique are used to estimate the\nfinite sample distributions of the sample autocorrelation coefficients and the\nimpulse response coefficients and, in turn, to bias-adjust these statistics.\nThe accuracy of the resultant estimators in the case of the autocorrelation\ncoefficients is also compared with that yielded by analytical bias adjustment\nmethods when available. The basic sieve technique is seen to yield a reduction\nin the bias of both persistence measures. The pre-filtered sieve produces a\nsubstantial further reduction in the bias of the estimated impulse response\nfunction, whilst the extra improvement yielded by pre-filtering in the case of\nthe sample autocorrelation function is shown to depend heavily on the accuracy\nof the pre-filter.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 07:42:55 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 20:14:15 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Grose", "Simone D.", ""], ["Martin", "Gael M.", ""], ["Poskitt", "Donald S.", ""]]}, {"id": "1312.4797", "submitter": "Malgorzata Roos", "authors": "Malgorzata Roos, Thiago G. Martins, Leonhard Held, Havard Rue", "title": "Sensitivity analysis for Bayesian hierarchical models", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior sensitivity examination plays an important role in applied Bayesian\nanalyses. This is especially true for Bayesian hierarchical models, where\ninterpretability of the parameters within deeper layers in the hierarchy\nbecomes challenging. In addition, lack of information together with\nidentifiability issues may imply that the prior distributions for such models\nhave an undesired influence on the posterior inference. Despite its relevance,\ninformal approaches to prior sensitivity analysis are currently used. They\nrequire repetitive re-runs of the model with ad-hoc modified base prior\nparameter values. Other formal approaches to prior sensitivity analysis suffer\nfrom a lack of popularity in practice, mainly due to their high computational\ncost and absence of software implementation. We propose a novel formal approach\nto prior sensitivity analysis which is fast and accurate. It quantifies\nsensitivity without the need for a model re-run. We develop a ready-to-use\npriorSens package in R for routine prior sensitivity investigation by R-INLA.\nThroughout a series of examples we show how our approach can be used to detect\nhigh prior sensitivities of some parameters as well as identifiability issues\nin possibly over-parametrized Bayesian hierarchical models.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 14:19:31 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Roos", "Malgorzata", ""], ["Martins", "Thiago G.", ""], ["Held", "Leonhard", ""], ["Rue", "Havard", ""]]}, {"id": "1312.5002", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko, Grigory Sokolov and Wenyu Du", "title": "Efficient Performance Evaluation of the Generalized Shiryaev--Roberts\n  Detection Procedure in a Multi-Cyclic Setup", "comments": "33 pages, 2 figures, 4 tables, accepted for publication in Applied\n  Stochastic Models in Business and Industry", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a numerical method to evaluate the performance of the emerging\nGeneralized Shiryaev--Roberts (GSR) change-point detection procedure in a\n\"minimax-ish\" multi-cyclic setup where the procedure of choice is applied\nrepetitively (cyclically) and the change is assumed to take place at an unknown\ntime moment in a distant-future stationary regime. Specifically, the proposed\nmethod is based on the integral-equations approach and uses the collocation\ntechnique with the basis functions chosen so as to exploit a certain\nchange-of-measure identity and the GSR detection statistic's unique martingale\nproperty. As a result, the method's accuracy and robustness improve, as does\nits efficiency since using the change-of-measure ploy the Average Run Length\n(ARL) to false alarm and the Stationary Average Detection Delay (STADD) are\ncomputed simultaneously. We show that the method's rate of convergence is\nquadratic and supply a tight upperbound on its error. We conclude with a case\nstudy and confirm experimentally that the proposed method's accuracy and rate\nof convergence are robust with respect to three factors: (a) partition fineness\n(coarse vs. fine), (b) change magnitude (faint vs. contrast), and (c) the level\nof the ARL to false alarm (low vs. high). Since the method is designed not\nrestricted to a particular data distribution or to a specific value of the GSR\ndetection statistic's headstart, this work may help gain greater insight into\nthe characteristics of the GSR procedure and aid a practitioner to design the\nGSR procedure as needed while fully utilizing its potential.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 23:22:23 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Polunchenko", "Aleksey S.", ""], ["Sokolov", "Grigory", ""], ["Du", "Wenyu", ""]]}, {"id": "1312.5054", "submitter": "Elisabeth Waldmann", "authors": "Elisabeth Waldmann, Fabian Sobotka, Thomas Kneib", "title": "Bayesian Geoadditive Expectile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression classes modeling more than the mean of the response have found a\nlot of attention in the last years. Expectile regression is a special and\ncomputationally convenient case of this family of models. Expectiles offer a\nquantile-like characterisation of a complete distribution and include the mean\nas a special case. In the frequentist framework the impact of a lot of\ncovariates with very different structures have been made possible. We propose\nBayesian expectile regression based on the asymmetric normal distribution. This\nrenders possible incorporating for example linear, nonlinear, spatial and\nrandom effects in one model. Furthermore a detailed inference on the estimated\nparameters can be conducted. Proposal densities based on iterativly weighted\nleast squares updates for the resulting Markov chain Monte Carlo (MCMC)\nsimulation algorithm are proposed and the potential of the approach for\nextending the flexibility of expectile regression towards complex\nsemiparametric regression specifications is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 07:35:55 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Waldmann", "Elisabeth", ""], ["Sobotka", "Fabian", ""], ["Kneib", "Thomas", ""]]}, {"id": "1312.5207", "submitter": "Massimiliano Tamborrino", "authors": "M. Tamborrino, and S. Ditlevsen, and P Lansky", "title": "Parameter inference from hitting times for perturbed Brownian motion", "comments": "17 pages, 6 figure", "journal-ref": "Lifetime Data Anal., 21:331--352, 2015", "doi": "10.1007/s10985-014-9307-7", "report-no": null, "categories": "stat.ME math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A latent internal process describes the state of some system, e.g. the social\ntension in a political conflict, the strength of an industrial component or the\nhealth status of a person. When this process reaches a predefined threshold,\nthe process terminates and an observable event occurs, e.g. the political\nconflict finishes, the industrial component breaks down or the person has a\nheart attack. Imagine an intervention, e.g., a political decision, maintenance\nof a component or a medical treatment, is initiated to the process before the\nevent occurs. How can we evaluate whether the intervention had an effect?\n  To answer this question we describe the effect of the intervention through\nparameter changes of the law governing the internal process. Then, the time\ninterval between the start of the process and the final event is divided into\ntwo subintervals: the time from the start to the instant of intervention,\ndenoted by $S$, and the time between the intervention and the threshold\ncrossing, denoted by $R$. The first question studied here is: What is the joint\ndistribution of $(S,R)$? The theoretical expression is provided and serves as a\nbasis to answer the main question: Can we estimate the parameters of the model\nfrom observations of $S$ and $R$ and compare them statistically? Maximum\nlikelihood estimators are illustrated on simulated data under the assumption\nthat the process before and after the intervention is described by the same\ntype of model, i.e. a Brownian motion, but with different parameters.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 16:28:42 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Tamborrino", "M.", ""], ["Ditlevsen", "S.", ""], ["Lansky", "P", ""]]}, {"id": "1312.5306", "submitter": "Patrick J. Wolfe", "authors": "Sofia C. Olhede and Patrick J. Wolfe", "title": "Network histograms and universality of blockmodel approximation", "comments": "27 pages, 4 figures; revised version with link to software", "journal-ref": "Proceedings of the National Academy of Sciences of the USA 2014,\n  Vol. 111, No. 41, 14722-14727", "doi": "10.1073/pnas.1400374111", "report-no": null, "categories": "stat.ME cs.SI math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we introduce the network histogram: a statistical summary of\nnetwork interactions, to be used as a tool for exploratory data analysis. A\nnetwork histogram is obtained by fitting a stochastic blockmodel to a single\nobservation of a network dataset. Blocks of edges play the role of histogram\nbins, and community sizes that of histogram bandwidths or bin sizes. Just as\nstandard histograms allow for varying bandwidths, different blockmodel\nestimates can all be considered valid representations of an underlying\nprobability model, subject to bandwidth constraints. Here we provide methods\nfor automatic bandwidth selection, by which the network histogram approximates\nthe generating mechanism that gives rise to exchangeable random graphs. This\nmakes the blockmodel a universal network representation for unlabeled graphs.\nWith this insight, we discuss the interpretation of network communities in\nlight of the fact that many different community assignments can all give an\nequally valid representation of such a network. To demonstrate the\nfidelity-versus-interpretability tradeoff inherent in considering different\nnumbers and sizes of communities, we analyze two publicly available networks -\npolitical weblogs and student friendships - and discuss how to interpret the\nnetwork histogram when additional information related to node and edge labeling\nis present.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 20:50:06 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 09:18:30 GMT"}, {"version": "v3", "created": "Mon, 1 Sep 2014 21:34:36 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Olhede", "Sofia C.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "1312.5530", "submitter": "Julie Bessac", "authors": "Julie Bessac and Pierre Ailliot and Valerie Monbet", "title": "Gaussian linear state-space model for wind fields in the North-East\n  Atlantic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A space-time model for wind fields is proposed. It aims at simulating\nrealistic wind conditions with a focus on reproducing the space-time motions of\nthe meteorological systems. A Gaussian linear state-space model is used where\nthe latent state may be interpreted as regional wind condition and the\nobservation equation links regional and local scales. Parameter estimation is\nperformed by combining a method of moment and the EM algorithm whose\nperformances are discussed using simulation studies. The model is fitted to\n6-hourly reanalysis data in the North-East Atlantic. It is shown that the\nfitted model is interpretable and provide a good description of important\nproperties of the space-time covariance function of the data, such as the non\nfull-symmetry induced by prevailing flows in this area.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 13:12:40 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Bessac", "Julie", ""], ["Ailliot", "Pierre", ""], ["Monbet", "Valerie", ""]]}, {"id": "1312.5776", "submitter": "Michael Newton", "authors": "Nicholas C. Henderson and Michael A. Newton", "title": "Making the cut: improved ranking and selection for large-scale inference", "comments": "28 pages, 6 figures", "journal-ref": "JRSSB 2016", "doi": "10.1111/rssb.12131", "report-no": "UW Stat TR 1175 v7", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying leading measurement units from a large collection is a common\ninference task in various domains of large-scale inference. Testing approaches,\nwhich measure evidence against a null hypothesis rather than effect magnitude,\ntend to overpopulate lists of leading units with those associated with low\nmeasurement error. By contrast, local maximum likelihood (ML) approaches tend\nto favor units with high measurement error. Available Bayesian and empirical\nBayesian approaches rely on specialized loss functions that result in similar\ndeficiencies. We describe and evaluate a generic empirical Bayesian ranking\nprocedure that populates the list of top units in a way that maximizes the\nexpected overlap between the true and reported top lists for all list sizes.\nThe procedure relates unit-specific posterior upper tail probabilities with\ntheir empirical distribution to yield a ranking variable. It discounts\nhigh-variance units less than popular non-ML methods and thus achieves improved\noperating characteristics in the models considered.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 23:10:23 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2014 03:57:18 GMT"}, {"version": "v3", "created": "Tue, 20 May 2014 16:36:30 GMT"}, {"version": "v4", "created": "Thu, 11 Dec 2014 17:53:34 GMT"}, {"version": "v5", "created": "Tue, 8 Sep 2015 01:20:53 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Henderson", "Nicholas C.", ""], ["Newton", "Michael A.", ""]]}, {"id": "1312.5782", "submitter": "Daisy Phillips", "authors": "Daisy Phillips, Debashis Ghosh", "title": "Testing the disjunction hypothesis using Voronoi diagrams with\n  applications to genetics", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS707 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 2, 801-823", "doi": "10.1214/13-AOAS707", "report-no": "IMS-AOAS-AOAS707", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing of the disjunction hypothesis is appropriate when each gene or\nlocation studied is associated with multiple $p$-values, each of which is of\nindividual interest. This can occur when more than one aspect of an underlying\nprocess is measured. For example, cancer researchers may hope to detect genes\nthat are both differentially expressed on a transcriptomic level and show\nevidence of copy number aberration. Currently used methods of $p$-value\ncombination for this setting are overly conservative, resulting in very low\npower for detection. In this work, we introduce a method to test the\ndisjunction hypothesis by using cumulative areas from the Voronoi diagram of\ntwo-dimensional vectors of $p$-values. Our method offers much improved power\nover existing methods, even in challenging situations, while maintaining\nappropriate error control. We apply the approach to data from two published\nstudies: the first aims to detect periodic genes of the organism\nSchizosaccharomyces pombe, and the second aims to identify genes associated\nwith prostate cancer.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 00:08:00 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 12:33:03 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Phillips", "Daisy", ""], ["Ghosh", "Debashis", ""]]}, {"id": "1312.5789", "submitter": "Annalisa Cerquetti", "authors": "Annalisa Cerquetti", "title": "Yet another application of marginals of multivariate Gibbs distributions", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give yet another example of the usefulness of working with marginals of\nmultivariate Gibbs distributions (Cerquetti, 2013) in deriving Bayesian\nnonparametric estimators under Gibbs priors in species sampling problems. Here\nin particular we substantially reduce length and complexity of the proofs in\nBacallado et al. (2013, Th. 1, and Th. 2) for looking backward probabilities\nunder incomplete information.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 01:29:12 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Cerquetti", "Annalisa", ""]]}, {"id": "1312.5904", "submitter": "Christopher K. Wikle", "authors": "Christopher K. Wikle, Ralph F. Milliff, Radu Herbei, William B. Leeds", "title": "Modern Statistical Methods in Oceanography: A Hierarchical Perspective", "comments": "Published in at http://dx.doi.org/10.1214/13-STS436 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 4, 466-486", "doi": "10.1214/13-STS436", "report-no": "IMS-STS-STS436", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processes in ocean physics, air-sea interaction and ocean biogeochemistry\nspan enormous ranges in spatial and temporal scales, that is, from molecular to\nplanetary and from seconds to millennia. Identifying and implementing\nsustainable human practices depend critically on our understandings of key\naspects of ocean physics and ecology within these scale ranges. The set of all\nocean data is distorted such that three- and four-dimensional (i.e.,\ntime-dependent) in situ data are very sparse, while observations of surface and\nupper ocean properties from space-borne platforms have become abundant in the\npast few decades. Precisions in observations of all types vary as well. In the\nface of these challenges, the interface between Statistics and Oceanography has\nproven to be a fruitful area for research and the development of useful models.\nWith the recognition of the key importance of identifying, quantifying and\nmanaging uncertainty in data and models of ocean processes, a hierarchical\nperspective has become increasingly productive. As examples, we review a\nheterogeneous mix of studies from our own work demonstrating Bayesian\nhierarchical model applications in ocean physics, air-sea interaction, ocean\nforecasting and ocean ecosystem models. This review is by no means exhaustive\nand we have endeavored to identify hierarchical modeling work reported by\nothers across the broad range of ocean-related topics reported in the\nstatistical literature. We conclude by noting relevant ocean-statistics\nproblems on the immediate research horizon, and some technical challenges they\npose, for example, in terms of nonlinearity, dimensionality and computing.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 11:57:58 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Wikle", "Christopher K.", ""], ["Milliff", "Ralph F.", ""], ["Herbei", "Radu", ""], ["Leeds", "William B.", ""]]}, {"id": "1312.5934", "submitter": "Frederic Paik Schoenberg", "authors": "Andrew Bray, Frederic Paik Schoenberg", "title": "Assessment of Point Process Models for Earthquake Forecasting", "comments": "Published in at http://dx.doi.org/10.1214/13-STS440 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 4, 510-520", "doi": "10.1214/13-STS440", "report-no": "IMS-STS-STS440", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for forecasting earthquakes are currently tested prospectively in\nwell-organized testing centers, using data collected after the models and their\nparameters are completely specified. The extent to which these models agree\nwith the data is typically assessed using a variety of numerical tests, which\nunfortunately have low power and may be misleading for model comparison\npurposes. Promising alternatives exist, especially residual methods such as\nsuper-thinning and Voronoi residuals. This article reviews some of these tests\nand residual methods for determining the goodness of fit of earthquake\nforecasting models.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 13:14:35 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Bray", "Andrew", ""], ["Schoenberg", "Frederic Paik", ""]]}, {"id": "1312.5957", "submitter": "Rohmatul Fajriyah", "authors": "Rohmatul Fajriyah", "title": "A study of convolution models for background correction of BeadArrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The RMA, since its introduction in \\cite{Iri03a, Iri03b, Iri06}, has gained\npopularity among bioinformaticians. It has evolved from the exponential-normal\nconvolution to the gamma-normal convolution, from single to two channels and\nfrom the Affymetrix to the Illumina platform.\n  The Illumina design has provided two probe types: the regular and the control\nprobes. This design is very suitable for studying the probability distribution\nof both and one can apply the convolution model to compute the true intensity\nestimator. The availability of benchmarking data set at Illumina platform, the\n{\\it Illumina spike-in}, helps researchers to evaluate their proposed method\nfor Illumina BeadArrays.\n  In this paper, we study the existing convolution models for background\ncorrection of Illumina BeadArrays in the literature and give a new estimator\nfor the true intensity, where the intensity value is exponentially or gamma\ndistributed and the noise has lognormal distribution. We compare the\nperformance of the models on the Illumina spike-in data set, based on various\ncriteria, for example, root and mean square error, $L_{1}$ error,\nKullback-Leibler coefficient, and some adapted criteria from Affycomp\n\\cite{Cop04}. We then provide a simulation study to measure the consistency of\nthe error of background correction and the parametrization. We also study the\nperformance of all models on the FFPE data set.\n  Our study shows that our GLNn model is the optimal one for the benchmarking\ndata set with benchmarking criteria, while the gamma-normal model has the best\nperformance for the benchmarking data set with simulation criteria. At the\npublic data set of FFPE, the gamma-normal and the exponential-gamma models with\nMLE cannot be used and our proposed models ELN and GLN have the best\nperformance, showing a moderate error in background correction and in the\nparametrization.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 14:26:40 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Fajriyah", "Rohmatul", ""]]}, {"id": "1312.5967", "submitter": "Rohmatul Fajriyah", "authors": "Rohmatul Fajriyah", "title": "Generalized beta convolution model of the true intensity for the\n  Illumina BeadArrays", "comments": null, "journal-ref": "Journal of The Thai Statistical Association, Vol. 13(2), July 2015", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarray data come from many steps of production and have been known to\ncontain noise. The pre-processing is implemented to reduce the noise, where the\nbackground is corrected. Prior to further analysis, many Illumina BeadArrays\nusers had applied the convolution model, a model which had been adapted from\nwhen it was first developed on the Affymetrix platform, to adjust the intensity\nvalue: corrected background intensity value.\n  Several models based on different underlying distributions and or parameters\nestimation methods have been proposed and applied. For instance : the\nexponential-gamma, the normal-gamma and the exponential-normal convolutions\nwith a maximum likelihood estimation, non-parametric, Bayesian and moment\nmethods of the parameters estimation, including two recent\nexponential-lognormal and gamma-lognormal convolutions.\n  In this paper, we propose models and derive the corrected background\nintensity based on the generalized betas and the generalized beta-normal\nconvolutions as a generalization of the existing models.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 14:47:01 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 19:42:27 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Fajriyah", "Rohmatul", ""]]}, {"id": "1312.6102", "submitter": "Hiroaki Kaido", "authors": "Hiroaki Kaido", "title": "Asymptotically Efficient Estimation of Weighted Average Derivatives with\n  an Interval Censored Variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the identification and estimation of weighted average\nderivatives of conditional location functionals including conditional mean and\nconditional quantiles in settings where either the outcome variable or a\nregressor is interval-valued. Building on Manski and Tamer (2002) who study\nnonparametric bounds for mean regression with interval data, we characterize\nthe identified set of weighted average derivatives of regression functions.\nSince the weighted average derivatives do not rely on parametric specifications\nfor the regression functions, the identified set is well-defined without any\nparametric assumptions. Under general conditions, the identified set is compact\nand convex and hence admits characterization by its support function. Using\nthis characterization, we derive the semiparametric efficiency bound of the\nsupport function when the outcome variable is interval-valued. We illustrate\nefficient estimation by constructing an efficient estimator of the support\nfunction for the case of mean regression with an interval censored outcome.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:34:01 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Kaido", "Hiroaki", ""]]}, {"id": "1312.6341", "submitter": "Gongjun Xu", "authors": "Bodhisattva Sen and Gongjun Xu", "title": "Model Based Bootstrap Methods for Interval Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the performance of model based bootstrap methods for\nconstructing point-wise confidence intervals around the survival function with\ninterval censored data. We show that bootstrapping from the nonparametric\nmaximum likelihood estimator of the survival function is inconsistent for both\nthe current status and case 2 interval censoring models. A model based smoothed\nbootstrap procedure is proposed and shown to be consistent. In addition,\nsimulation studies are conducted to illustrate the (in)-consistency of the\nbootstrap methods. Our conclusions in the interval censoring model would extend\nmore generally to estimators in regression models that exhibit non-standard\nrates of convergence.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2013 03:57:13 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Sen", "Bodhisattva", ""], ["Xu", "Gongjun", ""]]}, {"id": "1312.6348", "submitter": "Hidetoshi Shimodaira", "authors": "Hidetoshi Shimodaira", "title": "Higher-order accuracy of multiscale-double bootstrap for testing regions", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider hypothesis testing for the null hypothesis being represented as\nan arbitrary-shaped region in the parameter space. We compute an approximate\np-value by counting how many times the null hypothesis holds in bootstrap\nreplicates. This frequency, known as bootstrap probability, is widely used in\nevolutionary biology, but often reported as biased in the literature. Based on\nthe asymptotic theory of bootstrap confidence intervals, there have been some\nnew attempts for adjusting the bias via bootstrap probability without direct\naccess to the parameter value. One such an attempt is the double bootstrap\nwhich adjusts the bias by bootstrapping the bootstrap probability. Another new\nattempt is the multiscale bootstrap which is similar to the m-out-of-n\nbootstrap but very unusually extrapolating the bootstrap probability to $m=-n$.\nIn this paper, we employ these two attempts at the same time, and call the new\nprocedure as multiscale-double bootstrap. By focusing on the multivariate\nnormal model, we investigate higher-order asymptotics up to fourth-order\naccuracy. Geometry of the region plays important roles in the asymptotic\ntheory. It was known in the literature that the curvature of the boundary\nsurface of the region determines the bias of bootstrap probability. We found\nout that the curvature-of-curvature determines the remaining bias of double\nbootstrap. The multiscale bootstrap removes these biases. The multiscale-double\nbootstrap is fourth order accurate with coverage probability erring only\n$O(n^{-2})$, and it is robust against computational error of parameter\nestimation used for generating bootstrap replicates from the null distribution.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2013 07:01:41 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 13:18:09 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Shimodaira", "Hidetoshi", ""]]}, {"id": "1312.6397", "submitter": "Peter Hoff", "authors": "Peter David Hoff", "title": "Equivariant and scale-free Tucker decomposition models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyses of array-valued datasets often involve reduced-rank array\napproximations, typically obtained via least-squares or truncations of array\ndecompositions. However, least-squares approximations tend to be noisy in\nhigh-dimensional settings, and may not be appropriate for arrays that include\ndiscrete or ordinal measurements. This article develops methodology to obtain\nlow-rank model-based representations of continuous, discrete and ordinal data\narrays. The model is based on a parameterization of the mean array as a\nmultilinear product of a reduced-rank core array and a set of index-specific\northogonal eigenvector matrices. It is shown how orthogonally equivariant\nparameter estimates can be obtained from Bayesian procedures under invariant\nprior distributions. Additionally, priors on the core array are developed that\nact as regularizers, leading to improved inference over the standard\nleast-squares estimator, and providing robustness to misspecification of the\narray rank. This model-based approach is extended to accommodate discrete or\nordinal data arrays using a semiparametric transformation model. The resulting\nlow-rank representation is scale-free, in the sense that it is invariant to\nmonotonic transformations of the data array. In an example analysis of a\nmultivariate discrete network dataset, this scale-free approach provides a more\ncomplete description of data patterns.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2013 16:01:32 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Hoff", "Peter David", ""]]}, {"id": "1312.6407", "submitter": "Mauro Bernardi", "authors": "Mauro Bernardi, Antonello Maruotti and Lea Petrella", "title": "Multivariate Markov-Switching models and tail risk interdependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov switching models are often used to analyze financial returns because\nof their ability to capture frequently observed stylized facts. In this paper\nwe consider a multivariate Student-t version of the model as a viable\nalternative to the usual multivariate Gaussian distribution, providing a\nnatural robust extension that accounts for heavy-tails and time varying\nnon-linear correlations. Moreover, these modelling assumptions allow us to\ncapture extreme tail co-movements which are of fundamental importance to assess\nthe underlying dependence structure of asset returns during extreme events such\nas financial crisis. For the considered model we provide new risk\ninterdependence measures which generalize the existing ones, like the\nConditional Value-at-Risk (CoVaR). The proposed measures aim to capture\ninterconnections among multiple connecting market participants which is\nparticularly relevant during period of crisis when several institutions may\ncontemporaneously experience distress instances. Those measures are\nanalytically evaluated on the predictive distribution of the modes in order to\nprovide a forward-looking risk quantification. Application on a set of U.S.\nbanks is considered to show that the right specification of the model\nconditional distribution along with a multiple risk interdependence measure may\nhelp to better understand how the overall risk is shared among institutions.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2013 17:19:40 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2014 10:04:11 GMT"}, {"version": "v3", "created": "Sun, 2 Mar 2014 08:33:10 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Bernardi", "Mauro", ""], ["Maruotti", "Antonello", ""], ["Petrella", "Lea", ""]]}, {"id": "1312.6414", "submitter": "Atul Mallik", "authors": "Atul Mallik and Moulinath Banerjee and Michael Woodroofe", "title": "Baseline zone estimation in two dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the region on which a non-parametric\nregression function is at its baseline level in two dimensions. The baseline\nlevel typically corresponds to the minimum/maximum of the function and\nestimating such regions or their complements is pertinent to several problems\narising in edge estimation, environmental statistics, fMRI and related fields.\nWe assume the baseline region to be convex and estimate it via fitting a\n`stump' function to approximate $p$-values obtained from tests for deviation of\nthe regression function from its baseline level. The estimates, obtained using\nan algorithm originally developed for constructing convex contours of a\ndensity, are studied in two different sampling settings, one where several\nresponses can be obtained at a number of different covariate-levels\n(dose-response) and the other involving limited number of response values per\ncovariate (standard regression). The shape of the baseline region and the\nsmoothness of the regression function at its boundary play a critical role in\ndetermining the rate of convergence of our estimate: for a regression function\nwhich is `p-regular' at the boundary of the convex baseline region, our\nestimate converges at a rate $N^{2/(4p+3)}$ in the dose-response setting, $N$\nbeing the total budget, and its analogue in the standard regression setting\nconverges at a rate of $N^{1/(2p+2)}$. Extensions to non-convex baseline\nregions are explored as well.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2013 18:35:45 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Mallik", "Atul", ""], ["Banerjee", "Moulinath", ""], ["Woodroofe", "Michael", ""]]}, {"id": "1312.6471", "submitter": "Pierre Pinson", "authors": "Pierre Pinson", "title": "Wind Energy: Forecasting Challenges for Its Operational Management", "comments": "Published in at http://dx.doi.org/10.1214/13-STS445 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 4, 564-585", "doi": "10.1214/13-STS445", "report-no": "IMS-STS-STS445", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Renewable energy sources, especially wind energy, are to play a larger role\nin providing electricity to industrial and domestic consumers. This is already\nthe case today for a number of European countries, closely followed by the US\nand high growth countries, for example, Brazil, India and China. There exist a\nnumber of technological, environmental and political challenges linked to\nsupplementing existing electricity generation capacities with wind energy.\nHere, mathematicians and statisticians could make a substantial contribution at\nthe interface of meteorology and decision-making, in connection with the\ngeneration of forecasts tailored to the various operational decision problems\ninvolved. Indeed, while wind energy may be seen as an environmentally friendly\nsource of energy, full benefits from its usage can only be obtained if one is\nable to accommodate its variability and limited predictability. Based on a\nshort presentation of its physical basics, the importance of considering wind\npower generation as a stochastic process is motivated. After describing\nrepresentative operational decision-making problems for both market\nparticipants and system operators, it is underlined that forecasts should be\nissued in a probabilistic framework. Even though, eventually, the forecaster\nmay only communicate single-valued predictions. The existing approaches to wind\npower forecasting are subsequently described, with focus on single-valued\npredictions, predictive marginal densities and space-time trajectories.\nUpcoming challenges related to generating improved and new types of forecasts,\nas well as their verification and value to forecast users, are finally\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 06:49:21 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Pinson", "Pierre", ""]]}, {"id": "1312.6536", "submitter": "Peter J. Diggle", "authors": "Peter J. Diggle, Paula Moraga, Barry Rowlingson, Benjamin M. Taylor", "title": "Spatial and Spatio-Temporal Log-Gaussian Cox Processes: Extending the\n  Geostatistical Paradigm", "comments": "Published in at http://dx.doi.org/10.1214/13-STS441 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 4, 542-563", "doi": "10.1214/13-STS441", "report-no": "IMS-STS-STS441", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we first describe the class of log-Gaussian Cox processes\n(LGCPs) as models for spatial and spatio-temporal point process data. We\ndiscuss inference, with a particular focus on the computational challenges of\nlikelihood-based inference. We then demonstrate the usefulness of the LGCP by\ndescribing four applications: estimating the intensity surface of a spatial\npoint process; investigating spatial segregation in a multi-type process;\nconstructing spatially continuous maps of disease risk from spatially discrete\ndata; and real-time health surveillance. We argue that problems of this kind\nfit naturally into the realm of geostatistics, which traditionally is defined\nas the study of spatially continuous processes using spatially discrete\nobservations at a finite number of locations. We suggest that a more useful\ndefinition of geostatistics is by the class of scientific problems that it\naddresses, rather than by particular models or data formats.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 12:17:49 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Diggle", "Peter J.", ""], ["Moraga", "Paula", ""], ["Rowlingson", "Barry", ""], ["Taylor", "Benjamin M.", ""]]}, {"id": "1312.6611", "submitter": "Daniel Taylor-Rodriguez", "authors": "Daniel Taylor-Rodriguez, Andrew Womack, and Nikolay Bliznyuk", "title": "Bayesian Variable Selection on Model Spaces Constrained by Heredity\n  Conditions", "comments": "Main text: 21 pages, 3 figures. Supplementary documentation: NA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates Bayesian variable selection when there is a\nhierarchical dependence structure on the inclusion of predictors in the model.\nIn particular, we study the type of dependence found in polynomial response\nsurfaces of orders two and higher, whose model spaces are required to satisfy\nweak or strong heredity conditions. These conditions restrict the inclusion of\nhigher-order terms depending upon the inclusion of lower-order parent terms. We\ndevelop classes of priors on the model space, investigate their theoretical and\nfinite sample properties, and provide a Metropolis-Hastings algorithm for\nsearching the space of models. The tools proposed allow fast and thorough\nexploration of model spaces that account for hierarchical polynomial structure\nin the predictors and provide control of the inclusion of false positives in\nhigh posterior probability models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 17:20:59 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 05:23:21 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Taylor-Rodriguez", "Daniel", ""], ["Womack", "Andrew", ""], ["Bliznyuk", "Nikolay", ""]]}, {"id": "1312.6815", "submitter": "J. Martin van Zyl", "authors": "J. Martin van Zyl", "title": "A goodness-of-fit test based on the empirical characteristic function\n  and a comparison of tests for normality", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normal distribution has the unique property that the cumulant generating\nfunction has only two terms, namely those involving the mean and the variance.\nThis property is used to construct a simple by using the log of the modulus of\nthe empirical characteristic function to what would be expected under\nnormality. The test statistic is easy to calculate. Using a simulation study\nthe proposed test is shown to have excellent power, especially in large\nsamples.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 13:33:34 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 11:33:03 GMT"}, {"version": "v3", "created": "Thu, 13 Mar 2014 11:48:39 GMT"}, {"version": "v4", "created": "Wed, 16 Sep 2015 09:33:12 GMT"}, {"version": "v5", "created": "Fri, 29 Apr 2016 10:34:35 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["van Zyl", "J. Martin", ""]]}, {"id": "1312.6966", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Herv\\'e Glotin, Allou Sam\\'e", "title": "Model-based functional mixture discriminant analysis with hidden process\n  regression for curve classification", "comments": null, "journal-ref": "Neurocomputing, Volume 112, Pages 153-163, July 2013", "doi": "10.1016/j.neucom.2012.10.030", "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the modeling and the classification of functional\ndata presenting regime changes over time. We propose a new model-based\nfunctional mixture discriminant analysis approach based on a specific hidden\nprocess regression model that governs the regime changes over time. Our\napproach is particularly adapted to handle the problem of complex-shaped\nclasses of curves, where each class is potentially composed of several\nsub-classes, and to deal with the regime changes within each homogeneous\nsub-class. The proposed model explicitly integrates the heterogeneity of each\nclass of curves via a mixture model formulation, and the regime changes within\neach sub-class through a hidden logistic process. Each class of complex-shaped\ncurves is modeled by a finite number of homogeneous clusters, each of them\nbeing decomposed into several regimes. The model parameters of each class are\nlearned by maximizing the observed-data log-likelihood by using a dedicated\nexpectation-maximization (EM) algorithm. Comparisons are performed with\nalternative curve classification approaches, including functional linear\ndiscriminant analysis and functional mixture discriminant analysis with\npolynomial regression mixtures and spline regression mixtures. Results obtained\non simulated data and real data show that the proposed approach outperforms the\nalternative approaches in terms of discrimination, and significantly improves\nthe curves approximation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:08:47 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Glotin", "Herv\u00e9", ""], ["Sam\u00e9", "Allou", ""]]}, {"id": "1312.6967", "submitter": "Faicel Chamroukhi", "authors": "Allou Sam\\'e, Faicel Chamroukhi, G\\'erard Govaert, Patrice Aknin", "title": "Model-based clustering and segmentation of time series with changes in\n  regime", "comments": null, "journal-ref": "Advances in Data Analysis and Classification, December 2011,\n  Volume 5, Issue 4, pp 301-321", "doi": "10.1007/s11634-011-0096-5", "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture model-based clustering, usually applied to multidimensional data, has\nbecome a popular approach in many data analysis problems, both for its good\nstatistical properties and for the simplicity of implementation of the\nExpectation-Maximization (EM) algorithm. Within the context of a railway\napplication, this paper introduces a novel mixture model for dealing with time\nseries that are subject to changes in regime. The proposed approach consists in\nmodeling each cluster by a regression model in which the polynomial\ncoefficients vary according to a discrete hidden process. In particular, this\napproach makes use of logistic functions to model the (smooth or abrupt)\ntransitions between regimes. The model parameters are estimated by the maximum\nlikelihood method solved by an Expectation-Maximization algorithm. The proposed\napproach can also be regarded as a clustering approach which operates by\nfinding groups of time series having common changes in regime. In addition to\nproviding a time series partition, it therefore provides a time series\nsegmentation. The problem of selecting the optimal numbers of clusters and\nsegments is solved by means of the Bayesian Information Criterion (BIC). The\nproposed approach is shown to be efficient using a variety of simulated time\nseries and real-world time series of electrical power consumption from rail\nswitching operations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:11:04 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Sam\u00e9", "Allou", ""], ["Chamroukhi", "Faicel", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6968", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "A hidden process regression model for functional data description.\n  Application to curve discrimination", "comments": null, "journal-ref": "Neurocomputing, Volume 73, Issues 7-9, March 2010, Pages 1210-1221", "doi": "10.1016/j.neucom.2009.12.023", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for functional data description is proposed in this paper. It\nconsists of a regression model with a discrete hidden logistic process which is\nadapted for modeling curves with abrupt or smooth regime changes. The model\nparameters are estimated in a maximum likelihood framework through a dedicated\nExpectation Maximization (EM) algorithm. From the proposed generative model, a\ncurve discrimination rule is derived using the Maximum A Posteriori rule. The\nproposed model is evaluated using simulated curves and real world curves\nacquired during railway switch operations, by performing comparisons with the\npiecewise regression approach in terms of curve modeling and classification.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:13:09 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6969", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "Time series modeling by a regression approach based on a latent process", "comments": null, "journal-ref": "Neural Networks 22(5-6): 593-602 (2009)", "doi": "10.1016/j.neunet.2009.06.040", "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series are used in many domains including finance, engineering,\neconomics and bioinformatics generally to represent the change of a measurement\nover time. Modeling techniques may then be used to give a synthetic\nrepresentation of such data. A new approach for time series modeling is\nproposed in this paper. It consists of a regression model incorporating a\ndiscrete hidden logistic process allowing for activating smoothly or abruptly\ndifferent polynomial regression models. The model parameters are estimated by\nthe maximum likelihood method performed by a dedicated Expectation Maximization\n(EM) algorithm. The M step of the EM algorithm uses a multi-class Iterative\nReweighted Least-Squares (IRLS) algorithm to estimate the hidden process\nparameters. To evaluate the proposed approach, an experimental study on\nsimulated data and real world data was performed using two alternative\napproaches: a heteroskedastic piecewise regression model using a global\noptimization algorithm based on dynamic programming, and a Hidden Markov\nRegression Model whose parameters are estimated by the Baum-Welch algorithm.\nFinally, in the context of the remote monitoring of components of the French\nrailway infrastructure, and more particularly the switch mechanism, the\nproposed approach has been applied to modeling and classifying time series\nrepresenting the condition measurements acquired during switch operations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:13:55 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6974", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Piecewise regression mixture for simultaneous functional data clustering\n  and optimal segmentation", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel mixture model-based approach for simultaneous\nclustering and optimal segmentation of functional data which are curves\npresenting regime changes. The proposed model consists in a finite mixture of\npiecewise polynomial regression models. Each piecewise polynomial regression\nmodel is associated with a cluster, and within each cluster, each piecewise\npolynomial component is associated with a regime (i.e., a segment). We derive\ntwo approaches for learning the model parameters. The former is an estimation\napproach and consists in maximizing the observed-data likelihood via a\ndedicated expectation-maximization (EM) algorithm. A fuzzy partition of the\ncurves in K clusters is then obtained at convergence by maximizing the\nposterior cluster probabilities. The latter however is a classification\napproach and optimizes a specific classification likelihood criterion through a\ndedicated classification expectation-maximization (CEM) algorithm. The optimal\ncurve segmentation is performed by using dynamic programming. In the\nclassification approach, both the curve clustering and the optimal segmentation\nare performed simultaneously as the CEM learning proceeds. We show that the\nclassification approach is the probabilistic version that generalizes the\ndeterministic K-means-like algorithm proposed in H\\'ebrail et al. (2010). The\nproposed approach is evaluated using simulated curves and real-world curves.\nComparisons with alternatives including regression mixture models and the\nK-means like algorithm for piecewise regression demonstrate the effectiveness\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:54:05 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2014 23:23:20 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1312.6978", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "Mod\\`ele \\`a processus latent et algorithme EM pour la r\\'egression non\n  lin\\'eaire", "comments": null, "journal-ref": "Revue des Nouvelles Technologies de l'Information (RNTI),\n  Statistique et nouvelles technologies de l'information (2011) 15-32", "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non linear regression approach which consists of a specific regression\nmodel incorporating a latent process, allowing various polynomial regression\nmodels to be activated preferentially and smoothly, is introduced in this\npaper. The model parameters are estimated by maximum likelihood performed via a\ndedicated expecation-maximization (EM) algorithm. An experimental study using\nsimulated and real data sets reveals good performances of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 14:21:48 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6994", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "A regression model with a hidden logistic process for signal\n  parametrization", "comments": "In Proceedings of the XVIIth European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning (ESANN), Pages\n  503-508, 2009, Bruges, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for signal parametrization, which consists of a specific\nregression model incorporating a discrete hidden logistic process, is proposed.\nThe model parameters are estimated by the maximum likelihood method performed\nby a dedicated Expectation Maximization (EM) algorithm. The parameters of the\nhidden logistic process, in the inner loop of the EM algorithm, are estimated\nusing a multi-class Iterative Reweighted Least-Squares (IRLS) algorithm. An\nexperimental study using simulated and real data reveals good performances of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 18:07:41 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.7001", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert and Patrice Aknin", "title": "A regression model with a hidden logistic process for feature extraction\n  from time series", "comments": "In Proceedings of the International Joint Conference on Neural\n  Networks (IJCNN), 2009, Atlanta, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for feature extraction from time series is proposed in this\npaper. This approach consists of a specific regression model incorporating a\ndiscrete hidden logistic process. The model parameters are estimated by the\nmaximum likelihood method performed by a dedicated Expectation Maximization\n(EM) algorithm. The parameters of the hidden logistic process, in the inner\nloop of the EM algorithm, are estimated using a multi-class Iterative\nReweighted Least-Squares (IRLS) algorithm. A piecewise regression algorithm and\nits iterative variant have also been considered for comparisons. An\nexperimental study using simulated and real data reveals good performances of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 18:48:12 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.7007", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Her\\'e Glotin, C\\'eline Rabouy", "title": "Functional Mixture Discriminant Analysis with hidden process regression\n  for curve classification", "comments": "In Proceedings of the XXth European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning (ESANN), Pages\n  281-286, 2012, Bruges, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new mixture model-based discriminant analysis approach for\nfunctional data using a specific hidden process regression model. The approach\nallows for fitting flexible curve-models to each class of complex-shaped curves\npresenting regime changes. The model parameters are learned by maximizing the\nobserved-data log-likelihood for each class by using a dedicated\nexpectation-maximization (EM) algorithm. Comparisons on simulated data with\nalternative approaches show that the proposed approach provides better results.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 19:23:39 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Glotin", "Her\u00e9", ""], ["Rabouy", "C\u00e9line", ""]]}, {"id": "1312.7011", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "Classification automatique de donn\\'ees temporelles en classes\n  ordonn\\'ees", "comments": "in French, 44\\`emes Journ\\'ees de Statistique, SFdS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method of segmenting temporal data into ordered\nclasses. It is based on mixture models and a discrete latent process, which\nenables to successively activates the classes. The classification can be\nperformed by maximizing the likelihood via the EM algorithm or by\nsimultaneously optimizing the model parameters and the partition by the CEM\nalgorithm. These two algorithms can be seen as alternatives to Fisher's\nalgorithm, which improve its computing time.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 19:45:07 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.7018", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Herv\\'e Glotin", "title": "Mixture model-based functional discriminant analysis for curve\n  classification", "comments": "In Proceedings of the 2012 International Joint Conference on Neural\n  Networks (IJCNN), 2012, Pages: 1-8, Brisbane, Australia", "journal-ref": null, "doi": "10.1109/IJCNN.2012.6252818", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical approaches for Functional Data Analysis concern the paradigm for\nwhich the individuals are functions or curves rather than finite dimensional\nvectors. In this paper, we particularly focus on the modeling and the\nclassification of functional data which are temporal curves presenting regime\nchanges over time. More specifically, we propose a new mixture model-based\ndiscriminant analysis approach for functional data using a specific hidden\nprocess regression model. Our approach is particularly adapted to both handle\nthe problem of complex-shaped classes of curves, where each class is composed\nof several sub-classes, and to deal with the regime changes within each\nhomogeneous sub-class. The model explicitly integrates the heterogeneity of\neach class of curves via a mixture model formulation, and the regime changes\nwithin each sub-class through a hidden logistic process. The approach allows\ntherefore for fitting flexible curve-models to each class of complex-shaped\ncurves presenting regime changes through an unsupervised learning scheme, to\nautomatically summarize it into a finite number of homogeneous clusters, each\nof them is decomposed into several regimes. The model parameters are learned by\nmaximizing the observed-data log-likelihood for each class by using a dedicated\nexpectation-maximization (EM) algorithm. Comparisons on simulated data and real\ndata with alternative approaches, including functional linear discriminant\nanalysis and functional mixture discriminant analysis with polynomial\nregression mixtures and spline regression mixtures, show that the proposed\napproach provides better results regarding the discrimination results and\nsignificantly improves the curves approximation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 20:35:20 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Glotin", "Herv\u00e9", ""]]}, {"id": "1312.7022", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Robust EM algorithm for model-based curve clustering", "comments": "In Proceedings of the 2013 International Joint Conference on Neural\n  Networks (IJCNN), 2013, Dallas, TX, USA", "journal-ref": "In Proceedings of the 2013 International Joint Conference on\n  Neural Networks (IJCNN), 2013, Dallas, TX, USA", "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering approaches concern the paradigm of exploratory data\nanalysis relying on the finite mixture model to automatically find a latent\nstructure governing observed data. They are one of the most popular and\nsuccessful approaches in cluster analysis. The mixture density estimation is\ngenerally performed by maximizing the observed-data log-likelihood by using the\nexpectation-maximization (EM) algorithm. However, it is well-known that the EM\nalgorithm initialization is crucial. In addition, the standard EM algorithm\nrequires the number of clusters to be known a priori. Some solutions have been\nprovided in [31, 12] for model-based clustering with Gaussian mixture models\nfor multivariate data. In this paper we focus on model-based curve clustering\napproaches, when the data are curves rather than vectorial data, based on\nregression mixtures. We propose a new robust EM algorithm for clustering\ncurves. We extend the model-based clustering approach presented in [31] for\nGaussian mixture models, to the case of curve clustering by regression\nmixtures, including polynomial regression mixtures as well as spline or\nB-spline regressions mixtures. Our approach both handles the problem of\ninitialization and the one of choosing the optimal number of clusters as the EM\nlearning proceeds, rather than in a two-fold scheme. This is achieved by\noptimizing a penalized log-likelihood criterion. A simulation study confirms\nthe potential benefit of the proposed algorithm in terms of robustness\nregarding initialization and funding the actual number of clusters.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 21:04:08 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1312.7024", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, Patrice Aknin, G\\'erard Govaert", "title": "Model-based clustering with Hidden Markov Model regression for time\n  series with regime changes", "comments": "In Proceedings of the 2011 International Joint Conference on Neural\n  Networks (IJCNN), 2011, Pages 2814 - 2821, San Jose, California", "journal-ref": null, "doi": "10.1109/IJCNN.2011.6033590", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel model-based clustering approach for clustering\ntime series which present changes in regime. It consists of a mixture of\npolynomial regressions governed by hidden Markov chains. The underlying hidden\nprocess for each cluster activates successively several polynomial regimes\nduring time. The parameter estimation is performed by the maximum likelihood\nmethod through a dedicated Expectation-Maximization (EM) algorithm. The\nproposed approach is evaluated using simulated time series and real-world time\nseries issued from a railway diagnosis application. Comparisons with existing\napproaches for time series clustering, including the stand EM for Gaussian\nmixtures, $K$-means clustering, the standard mixture of regression models and\nmixture of Hidden Markov Models, demonstrate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 21:25:41 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Aknin", "Patrice", ""], ["Govaert", "G\u00e9rard", ""]]}, {"id": "1312.7214", "submitter": "Aidong Adam Ding", "authors": "A. Adam Ding and Yi Li", "title": "Copula Correlation: An Equitable Dependence Measure and Extension of\n  Pearson's Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Science, Reshef et al. (2011) proposed the concept of equitability for\nmeasures of dependence between two random variables. To this end, they proposed\na novel measure, the maximal information coefficient (MIC). Recently a PNAS\npaper (Kinney and Atwal, 2014) gave a mathematical definition for equitability.\nThey proved that MIC in fact is not equitable, while a fundamental information\ntheoretic measure, the mutual information (MI), is self-equitable. In this\npaper, we show that MI also does not correctly reflect the proportion of\ndeterministic signals hidden in noisy data. We propose a new equitability\ndefinition based on this scenario. The copula correlation (Ccor), based on the\nL1-distance of copula density, is shown to be equitable under both definitions.\nWe also prove theoretically that Ccor is much easier to estimate than MI.\nNumerical studies illustrate the properties of the measures.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 08:36:37 GMT"}, {"version": "v2", "created": "Sat, 22 Mar 2014 02:27:32 GMT"}, {"version": "v3", "created": "Sat, 2 Aug 2014 19:52:57 GMT"}, {"version": "v4", "created": "Fri, 14 Aug 2015 00:36:28 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Ding", "A. Adam", ""], ["Li", "Yi", ""]]}, {"id": "1312.7260", "submitter": "Alan E. Gelfand", "authors": "Alan E. Gelfand, Souparno Ghosh, James S. Clark", "title": "Scaling Integral Projection Models for Analyzing Size Demography", "comments": "Published in at http://dx.doi.org/10.1214/13-STS444 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 4, 641-658", "doi": "10.1214/13-STS444", "report-no": "IMS-STS-STS444", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historically, matrix projection models (MPMs) have been employed to study\npopulation dynamics with regard to size, age or structure. To work with\ncontinuous traits, in the past decade, integral projection models (IPMs) have\nbeen proposed. Following the path for MPMs, currently, IPMs are handled first\nwith a fitting stage, then with a projection stage. Model fitting has, so far,\nbeen done only with individual-level transition data. These data are used in\nthe fitting stage to estimate the demographic functions (survival, growth,\nfecundity) that comprise the kernel of the IPM specification. The estimated\nkernel is then iterated from an initial trait distribution to obtain what is\ninterpreted as steady state population behavior. Such projection results in\ninference that does not align with observed temporal distributions. This might\nbe expected; a model for population level projection should be fitted with\npopulation level transitions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 13:29:15 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Gelfand", "Alan E.", ""], ["Ghosh", "Souparno", ""], ["Clark", "James S.", ""]]}, {"id": "1312.7439", "submitter": "Rolf Sundberg", "authors": "Rolf Sundberg, Uwe Feldmann", "title": "Distribution-free factor analysis - Estimation theory and applicability\n  to high-dimensional data", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here provide a distribution-free approach to the random factor analysis\nmodel. We show that it leads to the same estimating equations as for the\nclassical ML estimates under normality, but more easily derived, and valid also\nin the case of more variables than observations ($p>n$). For this case we also\nadvocate a simple iteration method. In an illustration with $p=2000$ and $n=22$\nit was seen to lead to convergence after just a few iterations. We show that\nthere is no reason to expect Heywood cases to appear, and that the factor\nscores will typically be precisely estimated/predicted as soon as $p$ is large.\nWe state as a general conjecture that the nice behaviour is not despite $p>n$,\nbut because $p>n$.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2013 14:38:44 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Sundberg", "Rolf", ""], ["Feldmann", "Uwe", ""]]}, {"id": "1312.7485", "submitter": "Elias Bareinboim", "authors": "Elias Bareinboim and Judea Pearl", "title": "A General Algorithm for Deciding Transportability of Experimental\n  Results", "comments": null, "journal-ref": "Journal of Causal Inference, 2013; 1(1): 107-134", "doi": "10.1515/jci-2012-0004", "report-no": null, "categories": "cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizing empirical findings to new environments, settings, or populations\nis essential in most scientific explorations. This article treats a particular\nproblem of generalizability, called \"transportability\", defined as a license to\ntransfer information learned in experimental studies to a different population,\non which only observational studies can be conducted. Given a set of\nassumptions concerning commonalities and differences between the two\npopulations, Pearl and Bareinboim (2011) derived sufficient conditions that\npermit such transfer to take place. This article summarizes their findings and\nsupplements them with an effective procedure for deciding when and how\ntransportability is feasible. It establishes a necessary and sufficient\ncondition for deciding when causal effects in the target population are\nestimable from both the statistical information available and the causal\ninformation transferred from the experiments. The article further provides a\ncomplete algorithm for computing the transport formula, that is, a way of\ncombining observational and experimental information to synthesize bias-free\nestimate of the desired causal relation. Finally, the article examines the\ndifferences between transportability and other variants of generalizability.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 00:54:47 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Bareinboim", "Elias", ""], ["Pearl", "Judea", ""]]}, {"id": "1312.7525", "submitter": "Lixing Zhu", "authors": "Lu Lin, Feng Li, Kangning Wang, Lixing Zhu", "title": "Asymptotic Composite Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composition methodologies in the current literature are mainly to promote\nestimation efficiency via direct composition, either, of initial estimators or\nof objective functions. In this paper, composite estimation is investigated for\nboth estimation efficiency and bias reduction. To this end, a novel method is\nproposed by utilizing a regression relationship between initial estimators and\nvalues of model-independent parameter in an asymptotic sense. The resulting\nestimators could have smaller limiting variances than those of initial\nestimators, and for nonparametric regression estimation, could also have faster\nconvergence rate than the classical optimal rate that the corresponding initial\nestimators can achieve. The simulations are carried out to examine its\nperformance in finite sample situations.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 12:10:45 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Lin", "Lu", ""], ["Li", "Feng", ""], ["Wang", "Kangning", ""], ["Zhu", "Lixing", ""]]}, {"id": "1312.7567", "submitter": "Larry Wasserman", "authors": "Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli and\n  Larry Wasserman", "title": "Nonparametric Inference For Density Modes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive nonparametric confidence intervals for the eigenvalues of the\nHessian at modes of a density estimate. This provides information about the\nstrength and shape of modes and can also be used as a significance test. We use\na data-splitting approach in which potential modes are identified using the\nfirst half of the data and inference is done with the second half of the data.\nTo get valid confidence sets for the eigenvalues, we use a bootstrap based on\nan elementary-symmetric-polynomial (ESP) transformation. This leads to valid\nbootstrap confidence sets regardless of any multiplicities in the eigenvalues.\nWe also suggest a new method for bandwidth selection, namely, choosing the\nbandwidth to maximize the number of significant modes. We show by example that\nthis method works well. Even when the true distribution is singular, and hence\ndoes not have a density, (in which case cross validation chooses a zero\nbandwidth), our method chooses a reasonable bandwidth.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 18:13:41 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Genovese", "Christopher", ""], ["Perone-Pacifico", "Marco", ""], ["Verdinelli", "Isabella", ""], ["Wasserman", "Larry", ""]]}, {"id": "1312.7712", "submitter": "Yosihiko Ogata", "authors": "Yosihiko Ogata", "title": "A Prospect of Earthquake Prediction Research", "comments": "Published in at http://dx.doi.org/10.1214/13-STS439 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 4, 521-541", "doi": "10.1214/13-STS439", "report-no": "IMS-STS-STS439", "categories": "stat.ME physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earthquakes occur because of abrupt slips on faults due to accumulated stress\nin the Earth's crust. Because most of these faults and their mechanisms are not\nreadily apparent, deterministic earthquake prediction is difficult. For\neffective prediction, complex conditions and uncertain elements must be\nconsidered, which necessitates stochastic prediction. In particular, a large\namount of uncertainty lies in identifying whether abnormal phenomena are\nprecursors to large earthquakes, as well as in assigning urgency to the\nearthquake. Any discovery of potentially useful information for earthquake\nprediction is incomplete unless quantitative modeling of risk is considered.\nTherefore, this manuscript describes the prospect of earthquake predictability\nresearch to realize practical operational forecasting in the near future.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 13:38:42 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Ogata", "Yosihiko", ""]]}, {"id": "1312.7851", "submitter": "Lucas Janson", "authors": "Lucas Janson, William Fithian, Trevor Hastie", "title": "Effective Degrees of Freedom: A Flawed Metaphor", "comments": null, "journal-ref": null, "doi": "10.1093/biomet/asv019", "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To most applied statisticians, a fitting procedure's degrees of freedom is\nsynonymous with its model complexity, or its capacity for overfitting to data.\nIn particular, it is often used to parameterize the bias-variance tradeoff in\nmodel selection. We argue that, contrary to folk intuition, model complexity\nand degrees of freedom are not synonymous and may correspond very poorly. We\nexhibit and theoretically explore various examples of fitting procedures for\nwhich degrees of freedom is not monotonic in the model complexity parameter,\nand can exceed the total dimension of the response space. Even in very simple\nsettings, the degrees of freedom can exceed the dimension of the ambient space\nby an arbitrarily large amount. We show the degrees of freedom for any\nnon-convex projection method can be unbounded.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 20:21:15 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 22:17:46 GMT"}, {"version": "v3", "created": "Mon, 14 Jul 2014 02:25:47 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Janson", "Lucas", ""], ["Fithian", "William", ""], ["Hastie", "Trevor", ""]]}]