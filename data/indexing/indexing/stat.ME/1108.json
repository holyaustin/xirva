[{"id": "1108.0251", "submitter": "Roberto D. Pascual-Marqui", "authors": "Roberto D. Pascual-Marqui, Rolando J. Biscay, Pedro A. Valdes-Sosa,\n  Jorge Bosch-Bayard, Jorge J. Riera-Diaz", "title": "Cortical current source connectivity by means of partial coherence\n  fields", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Beatles Report 2011-08-01", "categories": "stat.AP physics.bio-ph q-bio.NC stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  An important field of research in functional neuroimaging is the discovery of\nintegrated, distributed brain systems and networks, whose different regions\nneed to work in unison for normal functioning.\n  The EEG is a non-invasive technique that can provide information for massive\nconnectivity analyses. Cortical signals of time varying electric neuronal\nactivity can be estimated from the EEG. Although such techniques have very high\ntime resolution, two cortical signals even at distant locations will appear to\nbe highly similar due to the low spatial resolution nature of the EEG.\n  In this study a method for eliminating the effect of common sources due to\nlow spatial resolution is presented. It is based on an efficient estimation of\nthe whole-cortex partial coherence matrix. Using as a starting point any linear\nEEG tomography that satisfies the EEG forward equation, it is shown that the\ngeneralized partial coherences for the cortical grey matter current density\ntime series are invariant to the selected tomography. It is empirically shown\nwith simulation experiments that the generalized partial coherences have higher\nspatial resolution than the classical coherences. The results demonstrate that\nwith as little as 19 electrodes, lag-connected brain regions can often be\nmissed and misplaced even with lagged coherence measures, while the new method\ndetects and localizes correctly the connected regions using the lagged partial\ncoherences.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2011 07:07:01 GMT"}], "update_date": "2011-08-20", "authors_parsed": [["Pascual-Marqui", "Roberto D.", ""], ["Biscay", "Rolando J.", ""], ["Valdes-Sosa", "Pedro A.", ""], ["Bosch-Bayard", "Jorge", ""], ["Riera-Diaz", "Jorge J.", ""]]}, {"id": "1108.0298", "submitter": "Krista Gile", "authors": "Krista J. Gile and Mark S. Handcock", "title": "Network Model-Assisted Inference from Respondent-Driven Sampling Data", "comments": "38 pages, 11 figures, under review. Includes supplemental materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-Driven Sampling is a method to sample hard-to-reach human\npopulations by link-tracing over their social networks. Beginning with a\nconvenience sample, each person sampled is given a small number of uniquely\nidentified coupons to distribute to other members of the target population,\nmaking them eligible for enrollment in the study. This can be an effective\nmeans to collect large diverse samples from many populations.\n  Inference from such data requires specialized techniques for two reasons.\nUnlike in standard sampling designs, the sampling process is both partially\nbeyond the control of the researcher, and partially implicitly defined.\nTherefore, it is not generally possible to directly compute the sampling\nweights necessary for traditional design-based inference. Any likelihood-based\ninference requires the modeling of the complex sampling process often beginning\nwith a convenience sample. We introduce a model-assisted approach, resulting in\na design-based estimator leveraging a working model for the structure of the\npopulation over which sampling is conducted.\n  We demonstrate that the new estimator has improved performance compared to\nexisting estimators and is able to adjust for the bias induced by the selection\nof the initial sample. We present sensitivity analyses for unknown population\nsizes and the misspecification of the working network model. We develop a\nbootstrap procedure to compute measures of uncertainty. We apply the method to\nthe estimation of HIV prevalence in a population of injecting drug users (IDU)\nin the Ukraine, and show how it can be extended to include application-specific\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2011 12:37:06 GMT"}], "update_date": "2011-08-02", "authors_parsed": [["Gile", "Krista J.", ""], ["Handcock", "Mark S.", ""]]}, {"id": "1108.0445", "submitter": "Surya Tokdar Surya Tokdar", "authors": "Surya T Tokdar", "title": "Adaptive Gaussian Predictive Process Approximation", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of knots selection for Gaussian predictive process\nmethodology. Predictive process approximation provides an effective solution to\nthe cubic order computational complexity of Gaussian process models. This\napproximation crucially depends on a set of points, called knots, at which the\noriginal process is retained, while the rest is approximated via a\ndeterministic extrapolation. Knots should be few in number to keep the\ncomputational complexity low, but provide a good coverage of the process domain\nto limit approximation error. We present theoretical calculations to show that\ncoverage must be judged by the canonical metric of the Gaussian process. This\nnecessitates having in place a knots selection algorithm that automatically\nadapts to the changes in the canonical metric affected by changes in the\nparameter values controlling the Gaussian process covariance function. We\npresent an algorithm toward this by employing an incomplete Cholesky\nfactorization with pivoting and dynamic stopping. Although these concepts\nalready exist in the literature, our contribution lies in unifying them into a\nfast algorithm and in using computable error bounds to finesse implementation\nof the predictive process approximation. The resulting adaptive predictive\nprocess offers a substantial automatization of Guassian process model fitting,\nespecially for Bayesian applications where thousands of values of the\ncovariance parameters are to be explored.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2011 22:23:13 GMT"}], "update_date": "2011-08-03", "authors_parsed": [["Tokdar", "Surya T", ""]]}, {"id": "1108.0484", "submitter": "Xiaoru Wu", "authors": "Xiaoru Wu and Zhiliang Ying", "title": "An Empirical Likelihood Approach to Nonparametric Covariate Adjustment\n  in Randomized Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariate adjustment is an important tool in the analysis of randomized\nclinical trials and observational studies. It can be used to increase\nefficiency and thus power, and to reduce possible bias. While most statistical\ntests in randomized clinical trials are nonparametric in nature, approaches for\ncovariate adjustment typically rely on specific regression models, such as the\nlinear model for a continuous outcome, the logistic regression model for a\ndichotomous outcome and the Cox model for survival time. Several recent efforts\nhave focused on model-free covariate adjustment. This paper makes use of the\nempirical likelihood method and proposes a nonparametric approach to covariate\nadjustment. A major advantage of the new approach is that it automatically\nutilizes covariate information in an optimal way without fitting nonparametric\nregression. The usual asymptotic properties, including the Wilks-type result of\nconvergence to a chi-square distribution for the empirical likelihood ratio\nbased test, and asymptotic normality for the corresponding maximum empirical\nlikelihood estimator, are established. It is also shown that the resulting test\nis asymptotically most powerful and that the estimator for the treatment effect\nachieves the semiparametric efficiency bound. The new method is applied to the\nGlobal Use of Strategies to Open Occluded Coronary Arteries (GUSTO)-I trial.\nExtensive simulations are conducted, validating the theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2011 05:31:48 GMT"}], "update_date": "2011-08-03", "authors_parsed": [["Wu", "Xiaoru", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1108.0523", "submitter": "Harrison B. Prosper", "authors": "Maurizio Pierini, Harrison B. Prosper, Sezen Sekmen and Maria\n  Spiropulu", "title": "Priors for New Physics", "comments": "33 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of data in terms of multi-parameter models of new physics,\nusing the Bayesian approach, requires the construction of multi-parameter\npriors. We propose a construction that uses elements of Bayesian reference\nanalysis. Our idea is to initiate the chain of inference with the reference\nprior for a likelihood function that depends on a single parameter of interest\nthat is a function of the parameters of the physics model. The reference\nposterior density of the parameter of interest induces on the parameter space\nof the physics model a class of posterior densities. We propose to continue the\nchain of inference with a particular density from this class, namely, the one\nfor which indistinguishable models are equiprobable and use it as the prior for\nsubsequent analysis. We illustrate our method by applying it to the constrained\nminimal supersymmetric Standard Model and two non-universal variants of it.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2011 09:18:02 GMT"}], "update_date": "2011-08-03", "authors_parsed": [["Pierini", "Maurizio", ""], ["Prosper", "Harrison B.", ""], ["Sekmen", "Sezen", ""], ["Spiropulu", "Maria", ""]]}, {"id": "1108.0600", "submitter": "Marco Selig", "authors": "Marco Selig, Niels Oppermann, Torsten A. En{\\ss}lin", "title": "Improving stochastic estimates with inference methods: calculating\n  matrix diagonals", "comments": "9 pages, 6 figures, accepted by Phys. Rev. E; introduction revised,\n  results unchanged; page proofs implemented", "journal-ref": "Phys. Rev. E 85, 021134 (2012)", "doi": "10.1103/PhysRevE.85.021134", "report-no": null, "categories": "astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the diagonal entries of a matrix, that is not directly accessible\nbut only available as a linear operator in the form of a computer routine, is a\ncommon necessity in many computational applications, especially in image\nreconstruction and statistical inference. Here, methods of statistical\ninference are used to improve the accuracy or the computational costs of matrix\nprobing methods to estimate matrix diagonals. In particular, the generalized\nWiener filter methodology, as developed within information field theory, is\nshown to significantly improve estimates based on only a few sampling probes,\nin cases in which some form of continuity of the solution can be assumed. The\nstrength, length scale, and precise functional form of the exploited\nautocorrelation function of the matrix diagonal is determined from the probes\nthemselves. The developed algorithm is successfully applied to mock and real\nworld problems. These performance tests show that, in situations where a matrix\ndiagonal has to be calculated from only a small number of computationally\nexpensive probes, a speedup by a factor of 2 to 10 is possible with the\nproposed method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2011 15:52:44 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2011 13:24:02 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2012 09:03:22 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Selig", "Marco", ""], ["Oppermann", "Niels", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1108.0772", "submitter": "Michel Broniatowski", "authors": "Michel Broniatowski (LSTA)", "title": "Minimum divergence estimators, maximum likelihood and exponential\n  families", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we prove the dual representation formula of the divergence\nbetween two distributions in a parametric model. Resulting estimators for the\ndivergence as for the parameter are derived. These estimators do not make use\nof any grouping nor smoothing. It is proved that all differentiable divergences\ninduce the same estimator of the parameter on any regular exponential family,\nwhich is nothing else but the MLE.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2011 07:43:12 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2011 07:43:15 GMT"}, {"version": "v3", "created": "Mon, 8 Aug 2011 07:28:46 GMT"}, {"version": "v4", "created": "Sat, 20 Aug 2011 17:18:46 GMT"}], "update_date": "2011-08-23", "authors_parsed": [["Broniatowski", "Michel", "", "LSTA"]]}, {"id": "1108.1223", "submitter": "Jay Bartroff", "authors": "Jay Bartroff and Tze Leung Lai", "title": "Incorporating Individual and Collective Ethics into Phase I Cancer Trial\n  Designs", "comments": null, "journal-ref": "Biometrics 67 (2011) p. 596-603", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general framework is proposed for Bayesian model-based designs of Phase I\ncancer trials, in which a general criterion for coherence (Cheung, 2005) of a\ndesign is also developed. This framework can incorporate both \"individual\" and\n\"collective\" ethics into the design of the trial. We propose a new design which\nminimizes a risk function composed of two terms, with one representing the\nindividual risk of the current dose and the other representing the collective\nrisk. The performance of this design, which is measured in terms of the\naccuracy of the estimated target dose at the end of the trial, the toxicity and\noverdose rates, and certain loss functions reflecting the individual and\ncollective ethics, is studied and compared with existing Bayesian model-based\ndesigns and is shown to have better performance than existing designs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2011 21:33:53 GMT"}], "update_date": "2011-08-08", "authors_parsed": [["Bartroff", "Jay", ""], ["Lai", "Tze Leung", ""]]}, {"id": "1108.1260", "submitter": "Heng Lian", "authors": "Peng Lai, Qihua Wang, Heng Lian", "title": "Bias-corrected GEE estimation and smooth-threshold GEE variable\n  selection for single-index models with clustered data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a generalized estimating equations based estimation\napproach and a variable selection procedure for single-index models when the\nobserved data are clustered. Unlike the case of independent observations,\nbias-correction is necessary when general working correlation matrices are used\nin the estimating equations. Our variable selection procedure based on\nsmooth-threshold estimating equations \\citep{Ueki-2009} can automatically\neliminate irrelevant parameters by setting them as zeros and is computationally\nsimpler than alternative approaches based on shrinkage penalty. The resulting\nestimator consistently identifies the significant variables in the index, even\nwhen the working correlation matrix is misspecified. The asymptotic property of\nthe estimator is the same whether or not the nonzero parameters are known (in\nboth cases we use the same estimating equations), thus achieving the oracle\nproperty in the sense of \\cite{Fan-Li-2001}. The finite sample properties of\nthe estimator are illustrated by some simulation examples, as well as a real\ndata application.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2011 06:12:10 GMT"}], "update_date": "2011-08-08", "authors_parsed": [["Lai", "Peng", ""], ["Wang", "Qihua", ""], ["Lian", "Heng", ""]]}, {"id": "1108.1860", "submitter": "Ronald Rousseau", "authors": "Ronald Rousseau", "title": "Percentile rank scores are congruous indicators of relative performance,\n  or aren't they?", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Percentile ranks and the I3 indicator were introduced by Bornmann,\nLeydesdorff, Mutz and Opthof. These two notions are based on the concept of\npercentiles (or quantiles) for discrete data. As several definitions for these\nnotions exist we propose one that we think is suitable in this context. Next we\nshow that if the notion of relative congruous indicators is carefully defined\nthen percentile rank scores are congruous indicators of relative performance.\nThe I3 indicator is a strictly congruous indicator of absolute performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2011 06:34:33 GMT"}], "update_date": "2011-08-10", "authors_parsed": [["Rousseau", "Ronald", ""]]}, {"id": "1108.1884", "submitter": "Steffen Lauritzen", "authors": "Therese Graversen and Steffen Lauritzen", "title": "Estimation of Parameters in DNA Mixture Analysis", "comments": null, "journal-ref": null, "doi": "10.1080/02664763.2013.817549", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Cowell et al. (2007), a Bayesian network for analysis of mixed traces of\nDNA was presented using gamma distributions for modelling peak sizes in the\nelectropherogram. It was demonstrated that the analysis was sensitive to the\nchoice of a variance factor and hence this should be adapted to any new trace\nanalysed. In the present paper we discuss how the variance parameter can be\nestimated by maximum likelihood to achieve this. The unknown proportions of DNA\nfrom each contributor can similarly be estimated by maximum likelihood jointly\nwith the variance parameter. Furthermore we discuss how to incorporate prior\nknowledge about the parameters in a Bayesian analysis. The proposed estimation\nmethods are illustrated through a few examples of applications for calculating\nevidential value in casework and for mixture deconvolution.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2011 08:50:43 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2012 16:13:05 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2013 13:36:55 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Graversen", "Therese", ""], ["Lauritzen", "Steffen", ""]]}, {"id": "1108.1917", "submitter": "Roderick Little", "authors": "Roderick Little", "title": "Calibrated Bayes, for Statistics in General, and Missing Data in\n  Particular", "comments": "Published in at http://dx.doi.org/10.1214/10-STS318 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 162-174", "doi": "10.1214/10-STS318", "report-no": "IMS-STS-STS318", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is argued that the Calibrated Bayesian (CB) approach to statistical\ninference capitalizes on the strength of Bayesian and frequentist approaches to\nstatistical inference. In the CB approach, inferences under a particular model\nare Bayesian, but frequentist methods are useful for model development and\nmodel checking. In this article the CB approach is outlined. Bayesian methods\nfor missing data are then reviewed from a CB perspective. The basic theory of\nthe Bayesian approach, and the closely related technique of multiple\nimputation, is described. Then applications of the Bayesian approach to normal\nmodels are described, both for monotone and nonmonotone missing data patterns.\nSequential Regression Multivariate Imputation and Penalized Spline of\nPropensity Models are presented as two useful approaches for relaxing\ndistributional assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2011 13:02:55 GMT"}], "update_date": "2011-08-10", "authors_parsed": [["Little", "Roderick", ""]]}, {"id": "1108.2120", "submitter": "Malay Ghosh", "authors": "Malay Ghosh", "title": "Objective Priors: An Introduction for Frequentists", "comments": "Published in at http://dx.doi.org/10.1214/10-STS338 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 187-202", "doi": "10.1214/10-STS338", "report-no": "IMS-STS-STS338", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods are increasingly applied in these days in the theory and\npractice of statistics. Any Bayesian inference depends on a likelihood and a\nprior. Ideally one would like to elicit a prior from related sources of\ninformation or past data. However, in its absence, Bayesian methods need to\nrely on some \"objective\" or \"default\" priors, and the resulting posterior\ninference can still be quite valuable. Not surprisingly, over the years, the\ncatalog of objective priors also has become prohibitively large, and one has to\nset some specific criteria for the selection of such priors. Our aim is to\nreview some of these criteria, compare their performance, and illustrate them\nwith some simple examples. While for very large sample sizes, it does not\npossibly matter what objective prior one uses, the selection of such a prior\ndoes influence inference for small or moderate samples. For regular models\nwhere asymptotic normality holds, Jeffreys' general rule prior, the positive\nsquare root of the determinant of the Fisher information matrix, enjoys many\noptimality properties in the absence of nuisance parameters. In the presence of\nnuisance parameters, however, there are many other priors which emerge as\noptimal depending on the criterion selected. One new feature in this article is\nthat a prior different from Jeffreys' is shown to be optimal under the\nchi-square divergence criterion even in the absence of nuisance parameters. The\nlatter is also invariant under one-to-one reparameterization.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2011 08:18:28 GMT"}], "update_date": "2011-08-11", "authors_parsed": [["Ghosh", "Malay", ""]]}, {"id": "1108.2141", "submitter": "Thomas Hotz", "authors": "Thomas Hotz and Stephan Huckemann", "title": "Intrinsic Means on the Circle: Uniqueness, Locus and Asymptotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a comprehensive treatment of local uniqueness, asymptotics\nand numerics for intrinsic means on the circle. It turns out that local\nuniqueness as well as rates of convergence are governed by the distribution\nnear the antipode. In a nutshell, if the distribution there is locally less\nthan uniform, we have local uniqueness and asymptotic normality with a rate of\n1 / \\surdn. With increased proximity to the uniform distribution the rate can\nbe arbitrarly slow, and in the limit, local uniqueness is lost. Further, we\ngive general distributional conditions, e.g. unimodality, that ensure global\nuniqueness. Along the way, we discover that sample means can occur only at the\nvertices of a regular polygon which allows to compute intrinsic sample means in\nlinear time from sorted data. This algorithm is finally applied in a simulation\nstudy demonstrating the dependence of the convergence rates on the behavior of\nthe density at the antipode.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2011 10:28:54 GMT"}], "update_date": "2011-08-11", "authors_parsed": [["Hotz", "Thomas", ""], ["Huckemann", "Stephan", ""]]}, {"id": "1108.2177", "submitter": "Stephen E. Fienberg", "authors": "Stephen E. Fienberg", "title": "Bayesian Models and Methods in Public Policy and Government Settings", "comments": "Published in at http://dx.doi.org/10.1214/10-STS331 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 212-226", "doi": "10.1214/10-STS331", "report-no": "IMS-STS-STS331", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting with the neo-Bayesian revival of the 1950s, many statisticians\nargued that it was inappropriate to use Bayesian methods, and in particular\nsubjective Bayesian methods in governmental and public policy settings because\nof their reliance upon prior distributions. But the Bayesian framework often\nprovides the primary way to respond to questions raised in these settings and\nthe numbers and diversity of Bayesian applications have grown dramatically in\nrecent years. Through a series of examples, both historical and recent, we\nargue that Bayesian approaches with formal and informal assessments of priors\nAND likelihood functions are well accepted and should become the norm in public\nsettings. Our examples include census-taking and small area estimation, US\nelection night forecasting, studies reported to the US Food and Drug\nAdministration, assessing global climate change, and measuring potential\ndeclines in disability among the elderly.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2011 13:29:08 GMT"}], "update_date": "2011-08-11", "authors_parsed": [["Fienberg", "Stephen E.", ""]]}, {"id": "1108.2245", "submitter": "Michael Braun", "authors": "Michael Braun and Paul Damien", "title": "Generalized Direct Sampling for Hierarchical Bayesian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new method to sample from posterior distributions in\nhierarchical models without using Markov chain Monte Carlo. This method, which\nis a variant of importance sampling ideas, is generally applicable to\nhigh-dimensional models involving large data sets. Samples are independent, so\nthey can be collected in parallel, and we do not need to be concerned with\nissues like chain convergence and autocorrelation. Additionally, the method can\nbe used to compute marginal likelihoods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2011 18:54:53 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2011 17:25:48 GMT"}, {"version": "v3", "created": "Thu, 9 Aug 2012 15:19:26 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Braun", "Michael", ""], ["Damien", "Paul", ""]]}, {"id": "1108.2356", "submitter": "J. N. K. Rao", "authors": "J. N. K. Rao", "title": "Impact of Frequentist and Bayesian Methods on Survey Sampling Practice:\n  A Selective Appraisal", "comments": "Published in at http://dx.doi.org/10.1214/10-STS346 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 240-256", "doi": "10.1214/10-STS346", "report-no": "IMS-STS-STS346", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to Hansen, Madow and Tepping [J. Amer. Statist. Assoc. 78 (1983)\n776--793], \"Probability sampling designs and randomization inference are widely\naccepted as the standard approach in sample surveys.\" In this article, reasons\nare advanced for the wide use of this design-based approach, particularly by\nfederal agencies and other survey organizations conducting complex large scale\nsurveys on topics related to public policy. Impact of Bayesian methods in\nsurvey sampling is also discussed in two different directions: nonparametric\ncalibrated Bayesian inferences from large samples and hierarchical Bayes\nmethods for small area estimation based on parametric models.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2011 09:25:33 GMT"}], "update_date": "2011-08-12", "authors_parsed": [["Rao", "J. N. K.", ""]]}, {"id": "1108.2401", "submitter": "Miles Lopes", "authors": "Miles E. Lopes, Laurent J. Jacob, Martin J. Wainwright", "title": "A More Powerful Two-Sample Test in High Dimensions using Random\n  Projection", "comments": "Version 3 is an extended version of our NIPS 2011 conference paper.\n  This should be regarded as the final version and cited as a NIPS 2011 paper.\n  Note that version3=version1. Also, version 2 should be considered as defunct,\n  as it contains an error in the variance formula in equation (4)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the hypothesis testing problem of detecting a shift between the\nmeans of two multivariate normal distributions in the high-dimensional setting,\nallowing for the data dimension p to exceed the sample size n. Specifically, we\npropose a new test statistic for the two-sample test of means that integrates a\nrandom projection with the classical Hotelling T^2 statistic. Working under a\nhigh-dimensional framework with (p,n) tending to infinity, we first derive an\nasymptotic power function for our test, and then provide sufficient conditions\nfor it to achieve greater power than other state-of-the-art tests. Using ROC\ncurves generated from synthetic data, we demonstrate superior performance\nagainst competing tests in the parameter regimes anticipated by our theoretical\nresults. Lastly, we illustrate an advantage of our procedure's false positive\nrate with comparisons on high-dimensional gene expression data involving the\ndiscrimination of different types of cancer.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2011 13:45:47 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2012 07:44:56 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2015 22:33:28 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Lopes", "Miles E.", ""], ["Jacob", "Laurent J.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1108.2477", "submitter": "Kengo Kamatani", "authors": "Kengo Kamatani", "title": "Local degeneracy of Markov chain Monte Carlo methods", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": "10.1051/ps/2014004", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study asymptotic behavior of Monte Carlo method. Local consistency is one\nof an ideal property of Monte Carlo method. However, it may fail to hold local\nconsistency for several reason. In fact, in practice, it is more important to\nstudy such a non-ideal behavior. We call local degeneracy for one of a\nnon-ideal behavior of Monte Carlo methods. We show some equivalent conditions\nfor local degeneracy. As an application we study a Gibbs sampler (data\naugmentation) for cumulative logit model with or without marginal augmentation.\nIt is well known that natural Gibbs sampler does not work well for this model.\nIn a sense of local consistency and degeneracy, marginal augmentation is shown\nto improve the asymptotic property. However, when the number of categories is\nlarge, both methods are not locally consistent.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2011 06:10:19 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2012 09:28:38 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Kamatani", "Kengo", ""]]}, {"id": "1108.2836", "submitter": "Julien Cornebise", "authors": "J. Cornebise, E. Moulines, J. Olsson", "title": "Adaptive sequential Monte Carlo by means of mixture of experts", "comments": "24 pages, 9 figures, under revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appropriately designing the proposal kernel of particle filters is an issue\nof significant importance, since a bad choice may lead to deterioration of the\nparticle sample and, consequently, waste of computational power. In this paper\nwe introduce a novel algorithm adaptively approximating the so-called optimal\nproposal kernel by a mixture of integrated curved exponential distributions\nwith logistic weights. This family of distributions, referred to as mixtures of\nexperts, is broad enough to be used in the presence of multi-modality or\nstrongly skewed distributions. The mixtures are fitted, via online-EM methods,\nto the optimal kernel through minimisation of the Kullback-Leibler divergence\nbetween the auxiliary target and instrumental distributions of the particle\nfilter. At each iteration of the particle filter, the algorithm is required to\nsolve only a single optimisation problem for the whole particle sample,\nyielding an algorithm with only linear complexity. In addition, we illustrate\nin a simulation study how the method can be successfully applied to optimal\nfiltering in nonlinear state-space models.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2011 03:17:37 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2012 14:12:05 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Cornebise", "J.", ""], ["Moulines", "E.", ""], ["Olsson", "J.", ""]]}, {"id": "1108.2838", "submitter": "Plamen Markov", "authors": "Plamen Markov", "title": "Empirical Cummulative Density Function from a Univariate Censored Sample", "comments": "I would like to withdraw version 1 of the paper. The pdf file for ver\n  1 is displayed incorrectly. The content of version 2 is identical. I have\n  changed the formatting", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let F be an unknown univariate distribution function to be estimated from a\nsample containing censored observations and tau be in dom(F). The author has\nderived a novel nonparametric estimator F_hat for F without making any\nassumptions regarding the nature of the censoring mechanism or the distribution\nfunction F. The distribution of F_hat(tau) can be easily and accurately\nestimated even for small sample sizes. The estimator F_hat has significantly\noutperformed the Kaplan Meier estimator in a simulation study with an\nexponential and a lognormal distribution functions F and a censoring mechanism\ndefined by i.i.d. uniform random observation points.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2011 03:30:15 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2011 02:04:36 GMT"}, {"version": "v3", "created": "Wed, 24 Aug 2011 04:12:25 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Markov", "Plamen", ""]]}, {"id": "1108.2883", "submitter": "Surya Tokdar", "authors": "Surya T. Tokdar and Ryan Martin", "title": "Bayesian test of normality versus a Dirichlet process mixture\n  alternative", "comments": "24 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian test of normality for univariate or multivariate data\nagainst alternative nonparametric models characterized by Dirichlet process\nmixture distributions. The alternative models are based on the principles of\nembedding and predictive matching. They can be interpreted to offer random\ngranulation of a normal distribution into a mixture of normals with mixture\ncomponents occupying a smaller volume the farther they are from the\ndistribution center. A scalar parametrization based on latent clustering is\nused to cover an entire spectrum of separation between the normal distributions\nand the alternative models. An efficient sequential importance sampler is\ndeveloped to calculate Bayes factors. Simulations indicate the proposed test\ncan detect non-normality without favoring the nonparametric alternative when\nnormality holds.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2011 15:51:13 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2013 19:56:22 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2013 05:08:00 GMT"}, {"version": "v4", "created": "Thu, 14 Nov 2019 08:37:27 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Tokdar", "Surya T.", ""], ["Martin", "Ryan", ""]]}, {"id": "1108.2986", "submitter": "M{\\aa}ns Thulin", "authors": "M{\\aa}ns Thulin", "title": "Tests for multivariate normality based on canonical correlations", "comments": "21 pages, 5 tables", "journal-ref": "Statistical Methods & Applications, 23, 189-208 (2014)", "doi": "10.1007/s10260-013-0252-5", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new affine invariant tests for multivariate normality, based on\nindependence characterizations of the sample moments of the normal\ndistribution. The test statistics are obtained using canonical correlations\nbetween sets of sample moments, generalizing the Lin-Mudholkar test for\nnormality. The tests are compared to some popular tests based on Mardia's\nskewness and kurtosis measures in an extensive simulation power study and are\nfound to offer higher power against many of the alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2011 12:53:27 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2011 14:56:24 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Thulin", "M\u00e5ns", ""]]}, {"id": "1108.2999", "submitter": "Mohamed Cherfi", "authors": "Mohamed Cherfi", "title": "Dual $\\phi$-divergences estimation in normal models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of robust estimators which are obtained from dual representation of\n$\\phi$-divergences, are studied empirically for the normal location model.\nMembers of this class of estimators are compared, and it is found that they are\nefficient at the true model and offer an attractive alternative to the maximum\nlikelihood, in term of robustness .\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2011 14:07:59 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Cherfi", "Mohamed", ""]]}, {"id": "1108.3072", "submitter": "Ping Li", "authors": "Ping Li, Anshumali Shrivastava, Christian Konig", "title": "Training Logistic Regression and SVM on 200GB Data Using b-Bit Minwise\n  Hashing and Comparisons with Vowpal Wabbit (VW)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generated a dataset of 200 GB with 10^9 features, to test our recent b-bit\nminwise hashing algorithms for training very large-scale logistic regression\nand SVM. The results confirm our prior work that, compared with the VW hashing\nalgorithm (which has the same variance as random projections), b-bit minwise\nhashing is substantially more accurate at the same storage. For example, with\nmerely 30 hashed values per data point, b-bit minwise hashing can achieve\nsimilar accuracies as VW with 2^14 hashed values per data point.\n  We demonstrate that the preprocessing cost of b-bit minwise hashing is\nroughly on the same order of magnitude as the data loading time. Furthermore,\nby using a GPU, the preprocessing cost can be reduced to a small fraction of\nthe data loading time.\n  Minwise hashing has been widely used in industry, at least in the context of\nsearch. One reason for its popularity is that one can efficiently simulate\npermutations by (e.g.,) universal hashing. In other words, there is no need to\nstore the permutation matrix. In this paper, we empirically verify this\npractice, by demonstrating that even using the simplest 2-universal hashing\ndoes not degrade the learning performance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2011 19:53:55 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Li", "Ping", ""], ["Shrivastava", "Anshumali", ""], ["Konig", "Christian", ""]]}, {"id": "1108.3168", "submitter": "Heping Zhang", "authors": "Heping Zhang", "title": "Statistical Analysis in Genetic Studies of Mental Illnesses", "comments": "Published in at http://dx.doi.org/10.1214/11-STS353 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 116-129", "doi": "10.1214/11-STS353", "report-no": "IMS-STS-STS353", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the risk factors for mental illnesses is of significant public\nhealth importance. Diagnosis, stigma associated with mental illnesses,\ncomorbidity, and complex etiologies, among others, make it very challenging to\nstudy mental disorders. Genetic studies of mental illnesses date back at least\na century ago, beginning with descriptive studies based on Mendelian laws of\ninheritance. A variety of study designs including twin studies, family studies,\nlinkage analysis, and more recently, genomewide association studies have been\nemployed to study the genetics of mental illnesses, or complex diseases in\ngeneral. In this paper, I will present the challenges and methods from a\nstatistical perspective and focus on genetic association studies.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 07:13:00 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Zhang", "Heping", ""]]}, {"id": "1108.3234", "submitter": "Carl Morris", "authors": "Carl Morris, Ruoxi Tang", "title": "Estimating Random Effects via Adjustment for Density Maximization", "comments": "Published in at http://dx.doi.org/10.1214/10-STS349 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 271-287", "doi": "10.1214/10-STS349", "report-no": "IMS-STS-STS349", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and evaluate point and interval estimates for the random effects\n$\\theta_i$, having made observations $y_i|\\theta_i\\stackrel{\\m\nathit{ind}}{\\sim}N[\\theta_i,V_i],i=1,...,k$ that follow a two-level Normal\nhierarchical model. Fitting this model requires assessing the Level-2 variance\n$A\\equiv\\operatorname {Var}(\\theta_i)$ to estimate shrinkages $B_i\\equiv\nV_i/(V_i+A)$ toward a (possibly estimated) subspace, with $B_i$ as the target\nbecause the conditional means and variances of $\\theta_i$ depend linearly on\n$B_i$, not on $A$. Adjustment for density maximization, ADM, can do the fitting\nfor any smooth prior on $A$. Like the MLE, ADM bases inferences on two\nderivatives, but ADM can approximate with any Pearson family, with Beta\ndistributions being appropriate because shrinkage factors satisfy $0\\le\nB_i\\le1$. Our emphasis is on frequency properties, which leads to adopting a\nuniform prior on $A\\ge0$, which then puts Stein's harmonic prior (SHP) on the\n$k$ random effects. It is known for the \"equal variances case\" $V_1=...=V_k$\nthat formal Bayes procedures for this prior produce admissible minimax\nestimates of the random effects, and that the posterior variances are large\nenough to provide confidence intervals that meet their nominal coverages.\nSimilar results are seen to hold for our approximating \"ADM-SHP\" procedure for\nequal variances and also for the unequal variances situations checked here. For\nshrinkage coefficient estimation, the ADM-SHP procedure allows an alternative\nfrequency interpretation. Writing $L(A)$ as the likelihood of $B_i$ with $i$\nfixed, ADM-SHP estimates $B_i$ as $\\hat{B_i}=V_i/(V_i+\\hat{A})$ with\n$\\hat{A}\\equiv \\operatorname {argmax}(A*L(A))$. This justifies the term\n\"adjustment for likelihood maximization,\" ALM.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 13:08:10 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Morris", "Carl", ""], ["Tang", "Ruoxi", ""]]}, {"id": "1108.3262", "submitter": "Sourabh Bhattacharya", "authors": "Anurag Ghosh, Soumalya Mukhopadhyay, Sandipan Roy, and Sourabh\n  Bhattacharya", "title": "Bayesian Inference in Nonparametric Dynamic State-Space Models", "comments": "This version contains much greater clarification of the look-up table\n  idea and a theorem regarding this is also proven and included in the\n  supplement. Will appear in Statistical Methodology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce state-space models where the functionals of the observational\nand the evolutionary equations are unknown, and treated as random functions\nevolving with time. Thus, our model is nonparametric and generalizes the\ntraditional parametric state-space models. This random function approach also\nfrees us from the restrictive assumption that the functional forms, although\ntime-dependent, are of fixed forms. The traditional approach of assuming known,\nparametric functional forms is questionable, particularly in state-space\nmodels, since the validation of the assumptions require data on both the\nobserved time series and the latent states; however, data on the latter are not\navailable in state-space models.\n  We specify Gaussian processes as priors of the random functions and exploit\nthe \"look-up table approach\" of \\ctn{Bhattacharya07} to efficiently handle the\ndynamic structure of the model. We consider both univariate and multivariate\nsituations, using the Markov chain Monte Carlo (MCMC) approach for studying the\nposterior distributions of interest. In the case of challenging multivariate\nsituations we demonstrate that the newly developed Transformation-based MCMC\n(TMCMC) of \\ctn{Dutta11} provides interesting and efficient alternatives to the\nusual proposal distributions. We illustrate our methods with a challenging\nmultivariate simulated data set, where the true observational and the\nevolutionary equations are highly non-linear, and treated as unknown. The\nresults we obtain are quite encouraging. Moreover, using our Gaussian process\napproach we analysed a real data set, which has also been analysed by\n\\ctn{Shumway82} and \\ctn{Carlin92} using the linearity assumption. Our analyses\nshow that towards the end of the time series, the linearity assumption of the\nprevious authors breaks down.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 15:05:42 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2012 06:55:48 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2012 10:10:32 GMT"}, {"version": "v4", "created": "Fri, 19 Apr 2013 07:33:53 GMT"}, {"version": "v5", "created": "Fri, 21 Feb 2014 12:52:19 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Ghosh", "Anurag", ""], ["Mukhopadhyay", "Soumalya", ""], ["Roy", "Sandipan", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1108.3423", "submitter": "Meili Baragatti", "authors": "Meili Baragatti (IML), Agn\\`es Grimaud (IML), Denys Pommeret (IML)", "title": "Likelihood-Free Parallel Tempering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computational (ABC) methods (or likelihood-free methods)\nhave appeared in the past fifteen years as useful methods to perform Bayesian\nanalyses when the likelihood is analytically or computationally intractable.\nSeveral ABC methods have been proposed: Monte Carlo Markov Chains (MCMC)\nmethods have been developped by Marjoramet al. (2003) and by Bortotet al.\n(2007) for instance, and sequential methods have been proposed among others by\nSissonet al. (2007), Beaumont et al. (2009) and Del Moral et al. (2009). Until\nnow, while ABC-MCMC methods remain the reference, sequential ABC methods have\nappeared to outperforms them (see for example McKinley et al. (2009) or Sisson\net al. (2007)). In this paper a new algorithm combining population-based MCMC\nmethods with ABC requirements is proposed, using an analogy with the Parallel\nTempering algorithm (Geyer, 1991). Performances are compared with existing ABC\nalgorithms on simulations and on a real example.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 08:41:34 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2011 08:55:11 GMT"}, {"version": "v3", "created": "Mon, 30 Apr 2012 06:54:58 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Baragatti", "Meili", "", "IML"], ["Grimaud", "Agn\u00e8s", "", "IML"], ["Pommeret", "Denys", "", "IML"]]}, {"id": "1108.3457", "submitter": "Nathaniel Schenker", "authors": "Nathaniel Schenker", "title": "Discussion of \"Calibrated Bayes, for Statistics in General, and Missing\n  Data in Particular\" by R. J. A. Little", "comments": "Published in at http://dx.doi.org/10.1214/10-STS318A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 179-184", "doi": "10.1214/10-STS318A", "report-no": "IMS-STS-STS318A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Calibrated Bayes, for Statistics in General, and Missing Data\nin Particular\" by R. Little [arXiv:1108.1917]\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 11:58:25 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Schenker", "Nathaniel", ""]]}, {"id": "1108.3461", "submitter": "Michael D. Larsen", "authors": "Michael D. Larsen", "title": "Discussion of \"Calibrated Bayes, for Statistics in General, and Missing\n  Data in Particular\" by R. J. A. Little", "comments": "Published in at http://dx.doi.org/10.1214/10-STS318B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 175-178", "doi": "10.1214/10-STS318B", "report-no": "IMS-STS-STS318B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Calibrated Bayes, for Statistics in General, and Missing Data\nin Particular\" by R. Little [arXiv:1108.1917]\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 12:02:44 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Larsen", "Michael D.", ""]]}, {"id": "1108.3466", "submitter": "Roderick Little", "authors": "Roderick Little", "title": "Rejoinder", "comments": "Published in at http://dx.doi.org/10.1214/10-STS318REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 185-186", "doi": "10.1214/10-STS318REJ", "report-no": "IMS-STS-STS318REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder of \"Calibrated Bayes, for Statistics in General, and Missing Data\nin Particular\" by R. Little [arXiv:1108.1917]\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 12:30:30 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Little", "Roderick", ""]]}, {"id": "1108.3467", "submitter": "Jos\\'{e} M. Bernardo", "authors": "Jos\\'e M. Bernardo", "title": "Discussion of \"Objective Priors: An Introduction for Frequentists\" by M.\n  Ghosh", "comments": "Published in at http://dx.doi.org/10.1214/11-STS338A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 203-205", "doi": "10.1214/11-STS338A", "report-no": "IMS-STS-STS338A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Objective Priors: An Introduction for Frequentists\" by M.\nGhosh [arXiv:1108.2120]\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 12:33:11 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Bernardo", "Jos\u00e9 M.", ""]]}, {"id": "1108.3473", "submitter": "Trevor Sweeting", "authors": "Trevor Sweeting", "title": "Discussion of \"Objective Priors: An Introduction for Frequentists\" by M.\n  Ghosh", "comments": "Published in at http://dx.doi.org/10.1214/11-STS338B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 206-209", "doi": "10.1214/11-STS338B", "report-no": "IMS-STS-STS338B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Objective Priors: An Introduction for Frequentists\" by M.\nGhosh [arXiv:1108.2120]\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 13:08:32 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Sweeting", "Trevor", ""]]}, {"id": "1108.3474", "submitter": "Malay Ghosh", "authors": "Malay Ghosh", "title": "Rejoinder", "comments": "Published in at http://dx.doi.org/10.1214/11-STS338REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 210-211", "doi": "10.1214/11-STS338REJ", "report-no": "IMS-STS-STS338REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder of ``Objective Priors: An Introduction for Frequentists'' by M.\nGhosh [arXiv:1108.2120]\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 13:17:32 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Ghosh", "Malay", ""]]}, {"id": "1108.3520", "submitter": "Daniel Saban\\'es Bov\\'e", "authors": "Daniel Saban\\'es Bov\\'e and Leonhard Held and G\\\"oran Kauermann", "title": "Mixtures of g-Priors for Generalised Additive Model Selection with\n  Penalised Splines", "comments": "34 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an objective Bayesian approach to the selection of covariates and\ntheir penalised splines transformations in generalised additive models.\nSpecification of a reasonable default prior for the model parameters and\ncombination with a multiplicity-correction prior for the models themselves is\ncrucial for this task. Here we use well-studied and well-behaved continuous\nmixtures of g-priors as default priors. We introduce the methodology in the\nnormal model and extend it to non-normal exponential families. A simulation\nstudy and an application from the literature illustrate the proposed approach.\nAn efficient implementation is available in the R-package \"hypergsplines\".\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 16:53:46 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2012 14:34:18 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Bov\u00e9", "Daniel Saban\u00e9s", ""], ["Held", "Leonhard", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "1108.3596", "submitter": "Srikanth Jagabathula", "authors": "Vivek Farias, Srikanth Jagabathula, Devavrat Shah", "title": "Assortment Optimization Under General Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of static assortment optimization, where the goal is\nto find the assortment of size at most $C$ that maximizes revenues. This is a\nfundamental decision problem in the area of Operations Management. It has been\nshown that this problem is provably hard for most of the important families of\nparametric of choice models, except the multinomial logit (MNL) model. In\naddition, most of the approximation schemes proposed in the literature are\ntailored to a specific parametric structure. We deviate from this and propose a\ngeneral algorithm to find the optimal assortment assuming access to only a\nsubroutine that gives revenue predictions; this means that the algorithm can be\napplied with any choice model. We prove that when the underlying choice model\nis the MNL model, our algorithm can find the optimal assortment efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2011 22:40:29 GMT"}], "update_date": "2011-08-19", "authors_parsed": [["Farias", "Vivek", ""], ["Jagabathula", "Srikanth", ""], ["Shah", "Devavrat", ""]]}, {"id": "1108.3657", "submitter": "David J. Hand", "authors": "David J. Hand", "title": "Discussion of \"Bayesian Models and Methods in Public Policy and\n  Government Settings\" by S. E. Fienberg", "comments": "Published in at http://dx.doi.org/10.1214/11-STS331A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 227-230", "doi": "10.1214/11-STS331A", "report-no": "IMS-STS-STS331A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fienberg convincingly demonstrates that Bayesian models and methods represent\na powerful approach to squeezing illumination from data in public policy\nsettings. However, no school of inference is without its weaknesses, and, in\nthe face of the ambiguities, uncertainties, and poorly posed questions of the\nreal world, perhaps we should not expect to find a formally correct inferential\nstrategy which can be universally applied, whatever the nature of the question:\nwe should not expect to be able to identify a \"norm\" approach. An analogy is\nmade between George Box's \"no models are right, but some are useful,\" and\ninferential systems [arXiv:1108.2177].\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2011 06:33:32 GMT"}], "update_date": "2011-08-19", "authors_parsed": [["Hand", "David J.", ""]]}, {"id": "1108.3904", "submitter": "Heng Lian", "authors": "Heng Lian", "title": "Shrinkage Estimation and Selection for Multiple Functional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional linear regression is a useful extension of simple linear\nregression and has been investigated by many researchers. However, functional\nvariable selection problems when multiple functional observations exist, which\nis the counterpart in the functional context of multiple linear regression, is\nseldom studied. Here we propose a method using group smoothly clipped absolute\ndeviation penalty (gSCAD) which can perform regression estimation and variable\nselection simultaneously. We show the method can identify the true model\nconsistently and discuss construction of pointwise confidence interval for the\nestimated functional coefficients. Our methodology and theory is verified by\nsimulation studies as well as an application to spectrometrics data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2011 06:30:14 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Lian", "Heng", ""]]}, {"id": "1108.3912", "submitter": "Graham Kalton", "authors": "Graham Kalton", "title": "Discussion of \"Bayesian Models and Methods in Public Policy and\n  Government Settings\" by S. E. Fienberg", "comments": "Published in at http://dx.doi.org/10.1214/11-STS331B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 231-234", "doi": "10.1214/11-STS331B", "report-no": "IMS-STS-STS331B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Bayesian Models and Methods in Public Policy and Government\nSettings\" by S. E. Fienberg [arXiv:1108.2177]\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2011 08:09:29 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Kalton", "Graham", ""]]}, {"id": "1108.3913", "submitter": "Alan M. Zaslavsky", "authors": "Alan M. Zaslavsky", "title": "Sampling from a Bayesian Menu", "comments": "Published in at http://dx.doi.org/10.1214/11-STS331C the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 235-237", "doi": "10.1214/11-STS331C", "report-no": "IMS-STS-STS331C", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Bayesian Models and Methods in Public Policy and Government\nSettings\" by S. E. Fienberg [arXiv:1108.2177]\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2011 08:25:47 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Zaslavsky", "Alan M.", ""]]}, {"id": "1108.3914", "submitter": "Stephen E. Fienberg", "authors": "Stephen E. Fienberg", "title": "Rejoinder", "comments": "Published in at http://dx.doi.org/10.1214/11-STS331REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 238-239", "doi": "10.1214/11-STS331REJ", "report-no": "IMS-STS-STS331REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder of \"Bayesian Models and Methods in Public Policy and Government\nSettings\" by S. E. Fienberg [arXiv:1108.2177]\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2011 08:44:30 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Fienberg", "Stephen E.", ""]]}, {"id": "1108.3919", "submitter": "Glen Meeden", "authors": "Glen Meeden", "title": "Discussion of \"Impact of Frequentist and Bayesian Methods on Survey\n  Sampling Practice: A Selective Appraisal\" by J. N. K. Rao", "comments": "Published in at http://dx.doi.org/10.1214/11-STS346A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 257-259", "doi": "10.1214/11-STS346A", "report-no": "IMS-STS-STS346A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Impact of Frequentist and Bayesian Methods on Survey Sampling\nPractice: A Selective Appraisal\" by J. N. K. Rao [arXiv:1108.2356]\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2011 09:02:05 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Meeden", "Glen", ""]]}, {"id": "1108.3931", "submitter": "J. Sedransk", "authors": "J. Sedransk", "title": "Discussion of \"Impact of Frequentist and Bayesian Methods on Survey\n  Sampling Practice: A Selective Appraisal\" by J. N. K. Rao", "comments": "Published in at http://dx.doi.org/10.1214/11-STS346B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 260-261", "doi": "10.1214/11-STS346B", "report-no": "IMS-STS-STS346B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This comment emphasizes the importance of model checking and model fitting\nwhen making inferences about finite population quantities. It also suggests the\nvalue of using unit level models when making inferences for small\nsubpopulations, that is, \"small area\" analyses [arXiv:1108.2356].\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2011 09:27:52 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Sedransk", "J.", ""]]}, {"id": "1108.3938", "submitter": "Eric Slud", "authors": "Eric Slud", "title": "Discussion of \"Impact of Frequentist and Bayesian Methods on Survey\n  Sampling Practice: A Selective Appraisal\" by J. N. K. Rao", "comments": "Published in at http://dx.doi.org/10.1214/11-STS346C the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 262-265", "doi": "10.1214/11-STS346C", "report-no": "IMS-STS-STS346C", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Impact of Frequentist and Bayesian Methods on Survey Sampling\nPractice: A Selective Appraisal\" by J. N. K. Rao [arXiv:1108.2356]\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2011 10:21:08 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Slud", "Eric", ""]]}, {"id": "1108.3940", "submitter": "J. N. K. Rao", "authors": "J. N. K. Rao", "title": "Rejoinder", "comments": "Published in at http://dx.doi.org/10.1214/11-STS346REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 266-270", "doi": "10.1214/11-STS346REJ", "report-no": "IMS-STS-STS346REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder of \"Impact of Frequentist and Bayesian Methods on Survey Sampling\nPractice: A Selective Appraisal\" by J. N. K. Rao [arXiv:1108.2356]\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2011 11:03:44 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Rao", "J. N. K.", ""]]}, {"id": "1108.3941", "submitter": "Claudio Fuentes", "authors": "Claudio Fuentes, George Casella", "title": "Discussion of \"Estimating Random Effects via Adjustment for Density\n  Maximization\" by C. Morris and R. Tang", "comments": "Published in at http://dx.doi.org/10.1214/11-STS349A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 288-290", "doi": "10.1214/11-STS349A", "report-no": "IMS-STS-STS349A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Estimating Random Effects via Adjustment for Density\nMaximization\" by C. Morris and R. Tang [arXiv:1108.3234]\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2011 11:23:19 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Fuentes", "Claudio", ""], ["Casella", "George", ""]]}, {"id": "1108.3946", "submitter": "P. Lahiri", "authors": "P. Lahiri, Santanu Pramanik", "title": "Discussion of \"Estimating Random Effects via Adjustment for Density\n  Maximization\" by C. Morris and R. Tang", "comments": "Published in at http://dx.doi.org/10.1214/11-STS349B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 291-295", "doi": "10.1214/11-STS349B", "report-no": "IMS-STS-STS349B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Estimating Random Effects via Adjustment for Density\nMaximization\" by C. Morris and R. Tang [arXiv:1108.3234]\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2011 11:53:42 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Lahiri", "P.", ""], ["Pramanik", "Santanu", ""]]}, {"id": "1108.3953", "submitter": "Carl Morris", "authors": "Carl Morris", "title": "Rejoinder", "comments": "Published in at http://dx.doi.org/10.1214/11-STS349REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 296-298", "doi": "10.1214/11-STS349REJ", "report-no": "IMS-STS-STS349REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder of \"Estimating Random Effects via Adjustment for Density\nMaximization\" by C. Morris and R. Tang [arXiv:1108.3234]\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2011 12:14:23 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Morris", "Carl", ""]]}, {"id": "1108.4126", "submitter": "Mark Tygert", "authors": "William Perkins, Mark Tygert, and Rachel Ward", "title": "Chi-square and classical exact tests often wildly misreport\n  significance; the remedy lies in computers", "comments": "63 pages, 51 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If a discrete probability distribution in a model being tested for\ngoodness-of-fit is not close to uniform, then forming the Pearson chi-square\nstatistic can involve division by nearly zero. This often leads to serious\ntrouble in practice -- even in the absence of round-off errors -- as the\npresent article illustrates via numerous examples. Fortunately, with the now\nwidespread availability of computers, avoiding all the trouble is simple and\neasy: without the problematic division by nearly zero, the actual values taken\nby goodness-of-fit statistics are not humanly interpretable, but black-box\ncomputer programs can rapidly calculate their precise significance.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2011 16:23:12 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2011 19:10:14 GMT"}], "update_date": "2011-09-16", "authors_parsed": [["Perkins", "William", ""], ["Tygert", "Mark", ""], ["Ward", "Rachel", ""]]}, {"id": "1108.4146", "submitter": "Xun Huan", "authors": "Xun Huan and Youssef M. Marzouk", "title": "Simulation-based optimal Bayesian experimental design for nonlinear\n  systems", "comments": "Preprint 53 pages, 17 figures (54 small figures). v1 submitted to the\n  Journal of Computational Physics on August 4, 2011; v2 submitted on August\n  12, 2012. v2 changes: (a) addition of Appendix B and Figure 17 to address the\n  bias in the expected utility estimator; (b) minor language edits; v3\n  submitted on November 30, 2012. v3 changes: minor edits", "journal-ref": "Journal of Computational Physics 232 (2013) 288-317", "doi": "10.1016/j.jcp.2012.08.013", "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal selection of experimental conditions is essential to maximizing\nthe value of data for inference and prediction, particularly in situations\nwhere experiments are time-consuming and expensive to conduct. We propose a\ngeneral mathematical framework and an algorithmic approach for optimal\nexperimental design with nonlinear simulation-based models; in particular, we\nfocus on finding sets of experiments that provide the most information about\ntargeted sets of parameters.\n  Our framework employs a Bayesian statistical setting, which provides a\nfoundation for inference from noisy, indirect, and incomplete data, and a\nnatural mechanism for incorporating heterogeneous sources of information. An\nobjective function is constructed from information theoretic measures,\nreflecting expected information gain from proposed combinations of experiments.\nPolynomial chaos approximations and a two-stage Monte Carlo sampling method are\nused to evaluate the expected information gain. Stochastic approximation\nalgorithms are then used to make optimization feasible in computationally\nintensive and high-dimensional settings. These algorithms are demonstrated on\nmodel problems and on nonlinear parameter estimation problems arising in\ndetailed combustion kinetics.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2011 22:49:15 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2012 18:46:49 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2012 23:34:15 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Huan", "Xun", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1108.4848", "submitter": "Joshua Habiger", "authors": "Joshua D. Habiger and Edsel A. Pena", "title": "Compound p-Value Statistics for Multiple Testing Procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many multiple testing procedures make use of the p-values from the individual\npairs of hypothesis tests, and are valid if the p-value statistics are\nindependent and uniformly distributed under the null hypotheses. However, it\nhas recently been shown that these types of multiple testing procedures are\ninefficient since such p-values do not depend upon all of the available data.\nThis paper provides tools for constructing compound p-value statistics, which\nare those that depend upon all of the available data, but still satisfy the\nconditions of independence and uniformity under the null hypotheses. As an\nexample, a class of compound p-value statistics for testing for location shifts\nis developed. It is demonstrated, both analytically and through simulations,\nthat multiple testing procedures tend to reject more false null hypotheses when\napplied to these compound p-values rather than the usual p-values, and at the\nsame time still guarantee the desired type I error rate control. The compound\np-values, in conjunction with two different multiple testing methods, are used\nto analyze a real microarray data set. Applying either multiple testing method\nto the compound p-values, instead of the usual p-values, enhances their powers.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2011 14:13:05 GMT"}], "update_date": "2011-08-25", "authors_parsed": [["Habiger", "Joshua D.", ""], ["Pena", "Edsel A.", ""]]}, {"id": "1108.4912", "submitter": "James Lawrence", "authors": "James D. Lawrence, Dr. Robert B. Gramacy, Dr. Len Thomas, Prof.\n  Stephen T. Buckland", "title": "The Importance of Prior Choice in Model Selection: a Density Dependence\n  Example", "comments": "Manuscript as submitted to Ecology Journal Aug 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a Bayesian analysis on abundance data for ten species of North\nAmerican duck, using the results to investigate the evidence in favour of\nbiologically motivated hypotheses about the causes and mechanisms of density\ndependence in these species. We explore the capabilities of our methods to\ndetect density dependent effects, both by simulation and through analyzes of\nreal data. The effect of the prior choice on predictive accuracy is also\nexamined. We conclude that our priors, which are motivated by considering the\ndynamics of the system of interest, offer clear advances over the priors used\nby previous authors for the duck data sets. We use this analysis as a\nmotivating example to demonstrate the importance of careful parameter prior\nselection if we are to perform a balanced model selection procedure. We also\npresent some simple guidelines that can be followed in a wide variety of\nmodelling frameworks where vague parameter prior choice is not a viable option.\nThese will produce parameter priors that not only greatly reduce bias in\nselecting certain models, but improve the predictive ability of the resulting\nmodel-averaged predictor.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2011 18:53:33 GMT"}], "update_date": "2011-08-25", "authors_parsed": [["Lawrence", "James D.", ""], ["Gramacy", "Dr. Robert B.", ""], ["Thomas", "Dr. Len", ""], ["Buckland", "Prof. Stephen T.", ""]]}, {"id": "1108.4920", "submitter": "Jie Yang", "authors": "Jie Yang, Klaus Miescke, Peter McCullagh", "title": "Classification based on a permanental process with cyclic approximation", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a doubly stochastic marked point process model for supervised\nclassification problems. Regardless of the number of classes or the dimension\nof the feature space, the model requires only 2--3 parameters for the\ncovariance function. The classification criterion involves a permanental ratio\nfor which an approximation using a polynomial-time cyclic expansion is\nproposed. The approximation is effective even if the feature region occupied by\none class is a patchwork interlaced with regions occupied by other classes. An\napplication to DNA microarray analysis indicates that the cyclic approximation\nis effective even for high-dimensional data. It can employ feature variables in\nan efficient way to reduce the prediction error significantly. This is critical\nwhen the true classification relies on non-reducible high-dimensional features.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2011 19:51:34 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2011 03:07:17 GMT"}, {"version": "v3", "created": "Thu, 10 May 2012 13:37:51 GMT"}, {"version": "v4", "created": "Wed, 18 Jul 2012 23:38:58 GMT"}], "update_date": "2012-07-20", "authors_parsed": [["Yang", "Jie", ""], ["Miescke", "Klaus", ""], ["McCullagh", "Peter", ""]]}, {"id": "1108.5116", "submitter": "Philippe Rigollet", "authors": "Philippe Rigollet, Alexandre B. Tsybakov", "title": "Sparse Estimation by Exponential Weighting", "comments": "Published in at http://dx.doi.org/10.1214/12-STS393 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 4, 558-575", "doi": "10.1214/12-STS393", "report-no": "IMS-STS-STS393", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a regression model with fixed design and Gaussian noise where the\nregression function can potentially be well approximated by a function that\nadmits a sparse representation in a given dictionary. This paper resorts to\nexponential weights to exploit this underlying sparsity by implementing the\nprinciple of sparsity pattern aggregation. This model selection take on sparse\nestimation allows us to derive sparsity oracle inequalities in several popular\nframeworks, including ordinary sparsity, fused sparsity and group sparsity. One\nstriking aspect of these theoretical results is that they hold under no\ncondition in the dictionary. Moreover, we describe an efficient implementation\nof the sparsity pattern aggregation principle that compares favorably to\nstate-of-the-art procedures on some basic numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2011 15:13:46 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2013 10:05:10 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Rigollet", "Philippe", ""], ["Tsybakov", "Alexandre B.", ""]]}, {"id": "1108.5185", "submitter": "Jingwei  Liu", "authors": "Jingwei Liu, Meizhi Xu", "title": "Function Based Nonlinear Least Squares and Application to\n  Jelinski--Moranda Software Reliability Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function based nonlinear least squares estimation (FNLSE) method is\nproposed and investigated in parameter estimation of Jelinski-Moranda software\nreliability model. FNLSE extends the potential fitting functions of traditional\nleast squares estimation (LSE), and takes the logarithm transformed nonlinear\nleast squares estimation (LogLSE) as a special case. A novel power\ntransformation function based nonlinear least squares estimation (powLSE) is\nproposed and applied to the parameter estimation of Jelinski-Moranda model.\nSolved with Newton-Raphson method, Both LogLSE and powLSE of Jelinski-Moranda\nmodels are applied to the mean time between failures (MTBF) predications on six\nstandard software failure time data sets. The experimental results demonstrate\nthe effectiveness of powLSE with optimal power index compared to the classical\nleast--squares estimation (LSE), maximum likelihood estimation (MLE) and LogLSE\nin terms of recursively relative error (RE) index and Braun statistic index.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2011 20:07:08 GMT"}], "update_date": "2011-08-29", "authors_parsed": [["Liu", "Jingwei", ""], ["Xu", "Meizhi", ""]]}, {"id": "1108.5244", "submitter": "Shuichi Kawano", "authors": "Shuichi Kawano", "title": "Semi-supervised logistic discrimination via labeled data and unlabeled\n  data from different sampling distributions", "comments": "19 pages", "journal-ref": "Statistical Analysis and Data Mining 6 (2013) 472-481", "doi": "10.1002/sam.11204", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the problem of classification method based on both\nlabeled and unlabeled data, where we assume that a density function for labeled\ndata is different from that for unlabeled data. We propose a semi-supervised\nlogistic regression model for classification problem along with the technique\nof covariate shift adaptation. Unknown parameters involved in proposed models\nare estimated by regularization with EM algorithm. A crucial issue in the\nmodeling process is the choices of tuning parameters in our semi-supervised\nlogistic models. In order to select the parameters, a model selection criterion\nis derived from an information-theoretic approach. Some numerical studies show\nthat our modeling procedure performs well in various cases.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 05:38:58 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2012 09:33:24 GMT"}, {"version": "v3", "created": "Sat, 13 Oct 2012 10:26:20 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Kawano", "Shuichi", ""]]}, {"id": "1108.5262", "submitter": "Etienne Roquain", "authors": "Gilles Blanchard, Thorsten Dickhaus, Etienne Roquain (LPMA), Fanny\n  Villers (LPMA)", "title": "On least favorable configurations for step-up-down tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates an open issue related to false discovery rate (FDR)\ncontrol of step-up-down (SUD) multiple testing procedures. It has been\nestablished in earlier literature that for this type of procedure, under some\nbroad conditions, and in an asymptotical sense, the FDR is maximum when the\nsignal strength under the alternative is maximum. In other words, so-called\n\"Dirac uniform configurations\" are asymptotically {\\em least favorable} in this\nsetting. It is known that this property also holds in a non-asymptotical sense\n(for any finite number of hypotheses), for the two extreme versions of SUD\nprocedures, namely step-up and step-down (with extra conditions for the\nstep-down case). It is therefore very natural to conjecture that this\nnon-asymptotical {\\em least favorable configuration} property could more\ngenerally be true for all \"intermediate\" forms of SUD procedures. We prove that\nthis is, somewhat surprisingly, not the case. The argument is based on the\nexact calculations proposed earlier by Roquain and Villers (2011), that we\nextend here by generalizing Steck's recursion to the case of two populations.\nSecondly, we quantify the magnitude of this phenomenon by providing a\nnonasymptotic upper-bound and explicit vanishing rates as a function of the\ntotal number of hypotheses.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 09:13:23 GMT"}], "update_date": "2011-08-29", "authors_parsed": [["Blanchard", "Gilles", "", "LPMA"], ["Dickhaus", "Thorsten", "", "LPMA"], ["Roquain", "Etienne", "", "LPMA"], ["Villers", "Fanny", "", "LPMA"]]}, {"id": "1108.5338", "submitter": "Rui Song", "authors": "Rui Song, Weiwei Wang, Donglin Zeng and Michael R. Kosorok", "title": "Penalized Q-Learning for Dynamic Treatment Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic treatment regime effectively incorporates both accrued information\nand long-term effects of treatment from specially designed clinical trials. As\nthese become more and more popular in conjunction with longitudinal data from\nclinical studies, the development of statistical inference for optimal dynamic\ntreatment regimes is a high priority. This is very challenging due to the\ndifficulties arising form non-regularities in the treatment effect parameters.\nIn this paper, we propose a new reinforcement learning framework called\npenalized Q-learning (PQ-learning), under which the non-regularities can be\nresolved and valid statistical inference established. We also propose a new\nstatistical procedure---individual selection---and corresponding methods for\nincorporating individual selection within PQ-learning. Extensive numerical\nstudies are presented which compare the proposed methods with existing methods,\nunder a variety of non-regular scenarios, and demonstrate that the proposed\napproach is both inferentially and computationally superior. The proposed\nmethod is demonstrated with the data from a depression clinical trial study.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 15:54:38 GMT"}], "update_date": "2011-08-29", "authors_parsed": [["Song", "Rui", ""], ["Wang", "Weiwei", ""], ["Zeng", "Donglin", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1108.5364", "submitter": "Vasileios Maroulas", "authors": "Dwueng-Chwuan Jhwueng and Vasileios Maroulas", "title": "Phylogenetic Ornstein-Uhlenbeck regression curves", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression curves for studying trait relationships are developed herein. The\nadaptive evolution model is considered an Ornstein-Uhlenbeck system whose\nparameters are estimated by a novel engagement of generalized least-squares and\noptimization. Our algorithm is implemented to ecological data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 18:19:23 GMT"}, {"version": "v2", "created": "Mon, 3 Mar 2014 14:42:00 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Jhwueng", "Dwueng-Chwuan", ""], ["Maroulas", "Vasileios", ""]]}, {"id": "1108.6043", "submitter": "Gabriel Terejanu", "authors": "Rebecca Morrison, Corey Bryant, Gabriel Terejanu, Kenji Miki, Serge\n  Prudhomme", "title": "Optimal Data Split Methodology for Model Validation", "comments": "Submitted to International Conference on Modeling, Simulation and\n  Control 2011 (ICMSC'11), San Francisco, USA, 19-21 October, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decision to incorporate cross-validation into validation processes of\nmathematical models raises an immediate question - how should one partition the\ndata into calibration and validation sets? We answer this question\nsystematically: we present an algorithm to find the optimal partition of the\ndata subject to certain constraints. While doing this, we address two critical\nissues: 1) that the model be evaluated with respect to predictions of a given\nquantity of interest and its ability to reproduce the data, and 2) that the\nmodel be highly challenged by the validation set, assuming it is properly\ninformed by the calibration set. This framework also relies on the interaction\nbetween the experimentalist and/or modeler, who understand the physical system\nand the limitations of the model; the decision-maker, who understands and can\nquantify the cost of model failure; and the computational scientists, who\nstrive to determine if the model satisfies both the modeler's and decision\nmaker's requirements. We also note that our framework is quite general, and may\nbe applied to a wide range of problems. Here, we illustrate it through a\nspecific example involving a data reduction model for an ICCD camera from a\nshock-tube experiment located at the NASA Ames Research Center (ARC).\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 19:24:12 GMT"}], "update_date": "2011-08-31", "authors_parsed": [["Morrison", "Rebecca", ""], ["Bryant", "Corey", ""], ["Terejanu", "Gabriel", ""], ["Miki", "Kenji", ""], ["Prudhomme", "Serge", ""]]}]