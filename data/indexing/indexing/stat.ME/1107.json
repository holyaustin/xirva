[{"id": "1107.0189", "submitter": "Sara van de Geer", "authors": "Sara van de Geer and Johannes Lederer", "title": "The Lasso, correlated design, and improved oracle inequalities", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional linear models and the $\\ell_1$-penalized least\nsquares estimator, also known as the Lasso estimator. In literature, oracle\ninequalities have been derived under restricted eigenvalue or compatibility\nconditions. In this paper, we complement this with entropy conditions which\nallow one to improve the dual norm bound, and demonstrate how this leads to new\noracle inequalities. The new oracle inequalities show that a smaller choice for\nthe tuning parameter and a trade-off between $\\ell_1$-norms and small\ncompatibility constants are possible. This implies, in particular for\ncorrelated design, improved bounds for the prediction error of the Lasso\nestimator as compared to the methods based on restricted eigenvalue or\ncompatibility conditions only.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 10:49:35 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["van de Geer", "Sara", ""], ["Lederer", "Johannes", ""]]}, {"id": "1107.0312", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni and Roberto I. Oliveira", "title": "Approximate group context tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variable length Markov chain model associated with a group of\nstationary processes that share the same context tree but each process has\npotentially different conditional probabilities. We propose a new model\nselection and estimation method which is computationally efficient. We develop\noracle and adaptivity inequalities, as well as model selection properties, that\nhold under continuity of the transition probabilities and polynomial\n$\\beta$-mixing. In particular, model misspecification is allowed.\n  These results are applied to interesting families of processes. For Markov\nprocesses, we obtain uniform rate of convergence for the estimation error of\ntransition probabilities as well as perfect model selection results. For chains\nof infinite order with complete connections, we obtain explicit uniform rates\nof convergence on the estimation of conditional probabilities, which have an\nexplicit dependence on the processes' continuity rates. Similar guarantees are\nalso derived for renewal processes.\n  Our results are shown to be applicable to discrete stochastic dynamic\nprogramming problems and to dynamic discrete choice models. We also apply our\nestimator to a linguistic study, based on recent work, by Galves et al (2012),\nof the rhythmic differences between Brazilian and European Portuguese.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 19:57:21 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2015 20:53:45 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2015 04:35:37 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Belloni", "Alexandre", ""], ["Oliveira", "Roberto I.", ""]]}, {"id": "1107.0614", "submitter": "Holger Drees", "authors": "Holger Drees, Laurens de Haan", "title": "Estimating failure probabilities", "comments": "Published at http://dx.doi.org/10.3150/13-BEJ594 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 2, 957-1001", "doi": "10.3150/13-BEJ594", "report-no": "IMS-BEJ-BEJ594", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In risk management, often the probability must be estimated that a random\nvector falls into an extreme failure set. In the framework of bivariate extreme\nvalue theory, we construct an estimator for such failure probabilities and\nanalyze its asymptotic properties under natural conditions. It turns out that\nthe estimation error is mainly determined by the accuracy of the statistical\nanalysis of the marginal distributions if the extreme value approximation to\nthe dependence structure is at least as accurate as the generalized Pareto\napproximation to the marginal distributions. Moreover, we establish confidence\nintervals and briefly discuss generalizations to higher dimensions and issues\narising in practical applications as well.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2011 12:58:58 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2012 13:35:34 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2014 11:48:40 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2015 10:58:56 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Drees", "Holger", ""], ["de Haan", "Laurens", ""]]}, {"id": "1107.0749", "submitter": "Cari G. Kaufman", "authors": "Cari G. Kaufman, Derek Bingham, Salman Habib, Katrin Heitmann, Joshua\n  A. Frieman", "title": "Efficient emulators of computer experiments using compactly supported\n  correlation functions, with an application to cosmology", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS489 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 4, 2470-2492", "doi": "10.1214/11-AOAS489", "report-no": "IMS-AOAS-AOAS489", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical emulators of computer simulators have proven to be useful in a\nvariety of applications. The widely adopted model for emulator building, using\na Gaussian process model with strictly positive correlation function, is\ncomputationally intractable when the number of simulator evaluations is large.\nWe propose a new model that uses a combination of low-order regression terms\nand compactly supported correlation functions to recreate the desired\npredictive behavior of the emulator at a fraction of the computational cost.\nFollowing the usual approach of taking the correlation to be a product of\ncorrelations in each input dimension, we show how to impose restrictions on the\nranges of the correlations, giving sparsity, while also allowing the ranges to\ntrade off against one another, thereby giving good predictive performance. We\nillustrate the method using data from a computer simulator of photometric\nredshift with 20,000 simulator evaluations and 80,000 predictions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2011 22:14:49 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2012 12:05:07 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Kaufman", "Cari G.", ""], ["Bingham", "Derek", ""], ["Habib", "Salman", ""], ["Heitmann", "Katrin", ""], ["Frieman", "Joshua A.", ""]]}, {"id": "1107.0935", "submitter": "Holger Drees", "authors": "Holger Drees", "title": "Bias correction for estimators of the extremal index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the joint asymptotic behavior of so-called blocks estimator of\nthe extremal index, that determines the mean length of clusters of extremes,\nbased on the exceedances over different thresholds. Due to the large bias of\nthese estimators, the resulting estimates are usually very sensitive to the\nchoice of the threshold and thus difficult to interpret. We propose and examine\na bias correction that asymptotically removes the leading bias term while the\nrate of convergence of the random error is preserved.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2011 18:30:45 GMT"}], "update_date": "2011-07-06", "authors_parsed": [["Drees", "Holger", ""]]}, {"id": "1107.1404", "submitter": "Johannes Schmidt-Hieber", "authors": "Johannes Schmidt-Hieber, Axel Munk, and Lutz Duembgen", "title": "Multiscale Methods for Shape Constraints in Deconvolution: Confidence\n  Statements for Qualitative Features", "comments": "55 pages, 5 figures, This is a revised version of a previous paper\n  with the title: \"Multiscale Methods for Shape Constraints in Deconvolution\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive multiscale statistics for deconvolution in order to detect\nqualitative features of the unknown density. An important example covered\nwithin this framework is to test for local monotonicity on all scales\nsimultaneously. We investigate the moderately ill-posed setting, where the\nFourier transform of the error density in the deconvolution model is of\npolynomial decay. For multiscale testing, we consider a calibration, motivated\nby the modulus of continuity of Brownian motion. We investigate the performance\nof our results from both the theoretical and simulation based point of view. A\nmajor consequence of our work is that the detection of qualitative features of\na density in a deconvolution problem is a doable task although the minimax\nrates for pointwise estimation are very slow.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2011 14:33:54 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2012 15:32:08 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2012 07:26:27 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Schmidt-Hieber", "Johannes", ""], ["Munk", "Axel", ""], ["Duembgen", "Lutz", ""]]}, {"id": "1107.1547", "submitter": "Gabriel Terejanu", "authors": "Gabriel Terejanu, Puneet Singla, Tarunraj Singh, Peter D. Scott", "title": "Approximate Interval Method for Epistemic Uncertainty Propagation using\n  Polynomial Chaos and Evidence Theory", "comments": "2010 American Control Conference, Baltimore, Maryland, June 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper builds upon a recent approach to find the approximate bounds of a\nreal function using Polynomial Chaos expansions. Given a function of random\nvariables with compact support probability distributions, the intuition is to\nquantify the uncertainty in the response using Polynomial Chaos expansion and\ndiscard all the information provided about the randomness of the output and\nextract only the bounds of its compact support. To solve for the bounding range\nof polynomials, we transform the Polynomial Chaos expansion in the Bernstein\nform, and use the range enclosure property of Bernstein polynomials to find the\nminimum and maximum value of the response. This procedure is used to propagate\nDempster-Shafer structures on closed intervals through nonlinear functions and\nit is applied on an algebraic challenge problem.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2011 02:20:59 GMT"}], "update_date": "2011-07-11", "authors_parsed": [["Terejanu", "Gabriel", ""], ["Singla", "Puneet", ""], ["Singh", "Tarunraj", ""], ["Scott", "Peter D.", ""]]}, {"id": "1107.1548", "submitter": "Gabriel Terejanu", "authors": "Gabriel Terejanu, Puneet Singla, Tarunraj Singh, Peter D. Scott", "title": "Approximate Propagation of both Epistemic and Aleatory Uncertainty\n  through Dynamic Systems", "comments": "The 13th International Conference on Information Fusion, Edinburgh,\n  UK, July 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.DS math.PR nlin.CD stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When ignorance due to the lack of knowledge, modeled as epistemic uncertainty\nusing Dempster-Shafer structures on closed intervals, is present in the model\nparameters, a new uncertainty propagation method is necessary to propagate both\naleatory and epistemic uncertainty. The new framework proposed here, combines\nboth epistemic and aleatory uncertainty into a second-order uncertainty\nrepresentation which is propagated through a dynamic system driven by white\nnoise. First, a finite parametrization is chosen to model the aleatory\nuncertainty by choosing a representative approximation to the probability\ndensity function conditioned on epistemic variables. The epistemic uncertainty\nis then propagated through the moment evolution equations of the conditional\nprobability density function. This way we are able to model the ignorance when\nthe knowledge about the system is incomplete. The output of the system is a\nDempster-Shafer structure on sets of cumulative distributions which can be\ncombined using different rules of combination and eventually transformed into a\nsingleton cumulative distribution function using Smets' pignistic\ntransformation when decision making is needed.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2011 02:25:20 GMT"}], "update_date": "2011-07-11", "authors_parsed": [["Terejanu", "Gabriel", ""], ["Singla", "Puneet", ""], ["Singh", "Tarunraj", ""], ["Scott", "Peter D.", ""]]}, {"id": "1107.1754", "submitter": "G. Afendras", "authors": "G. Afendras, N. Papadatos", "title": "Strengthened Chernoff-type variance bounds", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJ484 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2014, Vol. 20, No. 1, 245-264", "doi": "10.3150/12-BEJ484", "report-no": "IMS-BEJ-BEJ484", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X$ be an absolutely continuous random variable from the integrated\nPearson family and assume that $X$ has finite moments of any order. Using some\nproperties of the associated orthonormal polynomial system, we provide a class\nof strengthened Chernoff-type variance bounds.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2011 00:42:53 GMT"}, {"version": "v2", "created": "Wed, 16 May 2012 01:37:10 GMT"}, {"version": "v3", "created": "Fri, 18 May 2012 13:19:52 GMT"}, {"version": "v4", "created": "Wed, 5 Feb 2014 09:00:11 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Afendras", "G.", ""], ["Papadatos", "N.", ""]]}, {"id": "1107.1811", "submitter": "Jairo Fuquene", "authors": "Jairo Fuquene and Maria Perez and Luis Pericchi", "title": "Modelling outliers and structural breaks in dynamic linear models with a\n  novel use of a heavy tailed prior for the variances: An alternative to the\n  Inverted Gamma", "comments": "in press Brazilian Journal of Probability and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling outliers and structural breaks in dynamic linear models with a\nnovel use of a heavy tailed prior for the variances: An alternative to the\nInverted Gamma\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2011 19:21:46 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2013 02:43:49 GMT"}], "update_date": "2013-01-28", "authors_parsed": [["Fuquene", "Jairo", ""], ["Perez", "Maria", ""], ["Pericchi", "Luis", ""]]}, {"id": "1107.1919", "submitter": "Jay Bartroff", "authors": "Jay Bartroff, Tze Leung Lai", "title": "Multistage tests of multiple hypotheses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional multiple hypothesis tests use step-up, step-down, or closed\ntesting methods to control the overall error rates. We will discuss marrying\nthese methods with adaptive multistage sampling rules and stopping rules to\nperform efficient multiple hypothesis testing in sequential experimental\ndesigns. The result is a multistage step-down procedure that adaptively tests\nmultiple hypotheses while preserving the family-wise error rate, and extends\nHolm's (1979) step-down procedure to the sequential setting, yielding\nsubstantial savings in sample size with small loss in power.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 01:47:33 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Bartroff", "Jay", ""], ["Lai", "Tze Leung", ""]]}, {"id": "1107.2205", "submitter": "Giusi Moffa", "authors": "Giusi Moffa and Jack Kuipers", "title": "Sequential Monte Carlo EM for multivariate probit models", "comments": "26 pages, 2 figures. In press, Computational Statistics & Data\n  Analysis", "journal-ref": null, "doi": "10.1016/j.csda.2013.10.019", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate probit models (MPM) have the appealing feature of capturing some\nof the dependence structure between the components of multidimensional binary\nresponses. The key for the dependence modelling is the covariance matrix of an\nunderlying latent multivariate Gaussian. Most approaches to MLE in multivariate\nprobit regression rely on MCEM algorithms to avoid computationally intensive\nevaluations of multivariate normal orthant probabilities. As an alternative to\nthe much used Gibbs sampler a new SMC sampler for truncated multivariate\nnormals is proposed. The algorithm proceeds in two stages where samples are\nfirst drawn from truncated multivariate Student $t$ distributions and then\nfurther evolved towards a Gaussian. The sampler is then embedded in a MCEM\nalgorithm. The sequential nature of SMC methods can be exploited to design a\nfully sequential version of the EM, where the samples are simply updated from\none iteration to the next rather than resampled from scratch. Recycling the\nsamples in this manner significantly reduces the computational cost. An\nalternative view of the standard conditional maximisation step provides the\nbasis for an iterative procedure to fully perform the maximisation needed in\nthe EM algorithm. The identifiability of MPM is also thoroughly discussed. In\nparticular, the likelihood invariance can be embedded in the EM algorithm to\nensure that constrained and unconstrained maximisation are equivalent. A simple\niterative procedure is then derived for either maximisation which takes\neffectively no computational time. The method is validated by applying it to\nthe widely analysed Six Cities dataset and on a higher dimensional simulated\nexample. Previous approaches to the Six Cities overly restrict the parameter\nspace but, by considering the correct invariance, the maximum likelihood is\nquite naturally improved when treating the full unrestricted model.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 07:54:26 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2013 20:52:58 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Moffa", "Giusi", ""], ["Kuipers", "Jack", ""]]}, {"id": "1107.2353", "submitter": "David R. Bickel", "authors": "David R. Bickel", "title": "Blending Bayesian and frequentist methods according to the precision of\n  prior information with an application to hypothesis testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following zero-sum game between nature and a statistician blends Bayesian\nmethods with frequentist methods such as p-values and confidence intervals.\nNature chooses a posterior distribution consistent with a set of possible\npriors. At the same time, the statistician selects a parameter distribution for\ninference with the goal of maximizing the minimum Kullback-Leibler information\ngained over a confidence distribution or other benchmark distribution. An\napplication to testing a simple null hypothesis leads the statistician to\nreport a posterior probability of the hypothesis that is informed by both\nBayesian and frequentist methodology, each weighted according how well the\nprior is known.\n  Since neither the Bayesian approach nor the frequentist approach is entirely\nsatisfactory in situations involving partial knowledge of the prior\ndistribution, the proposed procedure reduces to a Bayesian method given\ncomplete knowledge of the prior, to a frequentist method given complete\nignorance about the prior, and to a blend between the two methods given partial\nknowledge of the prior. The blended approach resembles the Bayesian method\nrather than the frequentist method to the precise extent that the prior is\nknown.\n  The problem of testing a point null hypothesis illustrates the proposed\nframework. The blended probability that the null hypothesis is true is equal to\nthe p-value or a lower bound of an unknown Bayesian posterior probability,\nwhichever is greater. Thus, given total ignorance represented by a lower bound\nof 0, the p-value is used instead of any Bayesian posterior probability. At the\nopposite extreme of a known prior, the p-value is ignored. In the intermediate\ncase, the possible Bayesian posterior probability that is closest to the\np-value is used for inference. Thus, both the Bayesian method and the\nfrequentist method influence the inferences made.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 17:17:52 GMT"}], "update_date": "2011-07-19", "authors_parsed": [["Bickel", "David R.", ""]]}, {"id": "1107.2410", "submitter": "Johan Segers", "authors": "Gordon Gudendorf, Johan Segers", "title": "Nonparametric estimation of multivariate extreme-value copulas", "comments": "26 pages; submitted; Universit\\'e catholique de Louvain, Institut de\n  statistique, biostatistique et sciences actuarielles", "journal-ref": null, "doi": null, "report-no": "DP2011/18", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme-value copulas arise in the asymptotic theory for componentwise maxima\nof independent random samples. An extreme-value copula is determined by its\nPickands dependence function, which is a function on the unit simplex subject\nto certain shape constraints that arise from an integral transform of an\nunderlying measure called spectral measure. Multivariate extensions are\nprovided of certain rank-based nonparametric estimators of the Pickands\ndependence function. The shape constraint that the estimator should itself be a\nPickands dependence function is enforced by replacing an initial estimator by\nits best least-squares approximation in the set of Pickands dependence\nfunctions having a discrete spectral measure supported on a sufficiently fine\ngrid. Weak convergence of the standardized estimators is demonstrated and the\nfinite-sample performance of the estimators is investigated by means of a\nsimulation experiment.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2011 20:20:39 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2011 11:17:12 GMT"}], "update_date": "2011-11-30", "authors_parsed": [["Gudendorf", "Gordon", ""], ["Segers", "Johan", ""]]}, {"id": "1107.2446", "submitter": "Brian Mark", "authors": "Brian L. Mark and Yariv Ephraim", "title": "An EM Algorithm for Continuous-time Bivariate Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study properties and parameter estimation of finite-state homogeneous\ncontinuous-time bivariate Markov chains. Only one of the two processes of the\nbivariate Markov chain is observable. The general form of the bivariate Markov\nchain studied here makes no assumptions on the structure of the generator of\nthe chain, and hence, neither the underlying process nor the observable process\nis necessarily Markov. The bivariate Markov chain allows for simultaneous jumps\nof the underlying and observable processes. Furthermore, the inter-arrival time\nof observed events is phase-type. The bivariate Markov chain generalizes the\nbatch Markovian arrival process as well as the Markov modulated Markov process.\nWe develop an expectation-maximization (EM) procedure for estimating the\ngenerator of a bivariate Markov chain, and we demonstrate its performance. The\nprocedure does not rely on any numerical integration or sampling scheme of the\ncontinuous-time bivariate Markov chain. The proposed EM algorithm is equally\napplicable to multivariate Markov chains.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 01:34:59 GMT"}], "update_date": "2011-07-14", "authors_parsed": [["Mark", "Brian L.", ""], ["Ephraim", "Yariv", ""]]}, {"id": "1107.2724", "submitter": "Jairo Fuquene", "authors": "Jairo Fuquene and Brenda Betancourt", "title": "Heavy tailed priors: an alternative to non-informative priors in the\n  estimation of proportions on small areas", "comments": "Published paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the Cauchy and a new heavy tailed (Fuquene, Perez and Pericchi\n(2011)) priors to estimate proportions on small areas. Hierarchical models and\nthe Binomial likelihood in the exponential family form are used. We believe\nthat the heavy tailed priors in survey sampling settings could be more\neffective than the choice of noninformative priors to eliminate antipathy\ntowards methods that involve subjective elements or assumptions. To illustrate\nthe robust Bayesian approach, we apply this methodology in a popular example:\n\"the clement problem\". Finally, we recommend to use the Cauchy prior in absence\nor presence of outliers within the small areas and the Fuquene et al. (2011)\nprior when the outlier is a particular small area.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 04:07:56 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2012 23:06:28 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Fuquene", "Jairo", ""], ["Betancourt", "Brenda", ""]]}, {"id": "1107.2734", "submitter": "Zehua Chen", "authors": "Shan Luo and Zehua Chen", "title": "Sequential Lasso for feature selection with ultra-high dimensional\n  feature space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach, Sequential Lasso, for feature selection in\nlinear regression models with ultra-high dimensional feature spaces. We\ninvestigate in this article the asymptotic properties of Sequential Lasso and\nestablish its selection consistency. Like other sequential methods, the\nimplementation of Sequential Lasso is not limited by the dimensionality of the\nfeature space. It has advantages over other sequential methods. The simulation\nstudies comparing Sequential Lasso with other sequential methods are reported.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 06:21:43 GMT"}], "update_date": "2011-07-15", "authors_parsed": [["Luo", "Shan", ""], ["Chen", "Zehua", ""]]}, {"id": "1107.3036", "submitter": "Michael Eichler", "authors": "Michael Eichler", "title": "A note on global Markov properties for mixed graphs", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global Markov properties in mixed graphs are usually formulated in terms of\nthe path-oriented m-separation or by use of augmented graphs (similar to moral\ngraphs in the case of directed acyclic graphs). We provide an alternative\ncharacterization that can be easily implemented.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 10:29:07 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2011 13:41:39 GMT"}], "update_date": "2011-11-17", "authors_parsed": [["Eichler", "Michael", ""]]}, {"id": "1107.3133", "submitter": "JooSeuk Kim", "authors": "JooSeuk Kim and Clayton D. Scott", "title": "Robust Kernel Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for nonparametric density estimation that exhibits\nrobustness to contamination of the training sample. This method achieves\nrobustness by combining a traditional kernel density estimator (KDE) with ideas\nfrom classical $M$-estimation. We interpret the KDE based on a radial, positive\nsemi-definite kernel as a sample mean in the associated reproducing kernel\nHilbert space. Since the sample mean is sensitive to outliers, we estimate it\nrobustly via $M$-estimation, yielding a robust kernel density estimator (RKDE).\n  An RKDE can be computed efficiently via a kernelized iteratively re-weighted\nleast squares (IRWLS) algorithm. Necessary and sufficient conditions are given\nfor kernelized IRWLS to converge to the global minimizer of the $M$-estimator\nobjective function. The robustness of the RKDE is demonstrated with a\nrepresenter theorem, the influence function, and experimental results for\ndensity estimation and anomaly detection.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 19:05:48 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2011 03:18:45 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Kim", "JooSeuk", ""], ["Scott", "Clayton D.", ""]]}, {"id": "1107.3442", "submitter": "Weidong Liu", "authors": "Tony Cai and Weidong Liu", "title": "A Direct Estimation Approach to Sparse Linear Discriminant Analysis", "comments": "39 pages.To appear in Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers sparse linear discriminant analysis of high-dimensional\ndata. In contrast to the existing methods which are based on separate\nestimation of the precision matrix $\\O$ and the difference $\\de$ of the mean\nvectors, we introduce a simple and effective classifier by estimating the\nproduct $\\O\\de$ directly through constrained $\\ell_1$ minimization. The\nestimator can be implemented efficiently using linear programming and the\nresulting classifier is called the linear programming discriminant (LPD) rule.\n  The LPD rule is shown to have desirable theoretical and numerical properties.\nIt exploits the approximate sparsity of $\\O\\de$ and as a consequence allows\ncases where it can still perform well even when $\\O$ and/or $\\de$ cannot be\nestimated consistently. Asymptotic properties of the LPD rule are investigated\nand consistency and rate of convergence results are given. The LPD classifier\nhas superior finite sample performance and significant computational advantages\nover the existing methods that require separate estimation of $\\O$ and $\\de$.\nThe LPD rule is also applied to analyze real datasets from lung cancer and\nleukemia studies. The classifier performs favorably in comparison to existing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2011 14:09:54 GMT"}], "update_date": "2011-07-19", "authors_parsed": [["Cai", "Tony", ""], ["Liu", "Weidong", ""]]}, {"id": "1107.3618", "submitter": "Shuichi Kawano", "authors": "Hidetoshi Matsui, Toshihiro Misumi, Shuichi Kawano", "title": "Varying-coefficient modeling via regularized basis functions", "comments": "10 pages, 4 figures", "journal-ref": "Journal of Statistical Computation and Simulation 84 (2014)\n  2156-2165", "doi": "10.1080/00949655.2013.785548", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of constructing varying-coefficient models based on\nbasis expansions along with the technique of regularization. A crucial point in\nour modeling procedure is the selection of smoothing parameters in the\nregularization method. In order to choose the parameters objectively, we derive\nmodel selection criteria from the viewpoints of information-theoretic and\nBayesian approach. We demonstrate the effectiveness of proposed modeling\nstrategy through Monte Carlo simulations and analyzing a real data set.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 03:16:25 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Matsui", "Hidetoshi", ""], ["Misumi", "Toshihiro", ""], ["Kawano", "Shuichi", ""]]}, {"id": "1107.3904", "submitter": "Kaspar Rufibach", "authors": "Fadoua Balabdaoui and Hanna Jankowski and Kaspar Rufibach and Marios\n  Pavlides", "title": "Asymptotics of the discrete log-concave maximum likelihood estimator and\n  related applications", "comments": "21 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption of log-concavity is a flexible and appealing nonparametric\nshape constraint in distribution modelling. In this work, we study the\nlog-concave maximum likelihood estimator (MLE) of a probability mass function\n(pmf). We show that the MLE is strongly consistent and derive its pointwise\nasymptotic theory under both the well- and misspecified setting. Our asymptotic\nresults are used to calculate confidence intervals for the true log-concave\npmf. Both the MLE and the associated confidence intervals may be easily\ncomputed using the R package logcondiscr. We illustrate our theoretical results\nusing recent data from the H1N1 pandemic in Ontario, Canada.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2011 06:35:25 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2012 04:54:02 GMT"}, {"version": "v3", "created": "Sun, 7 Oct 2012 12:55:25 GMT"}, {"version": "v4", "created": "Sun, 14 Oct 2012 07:00:18 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Balabdaoui", "Fadoua", ""], ["Jankowski", "Hanna", ""], ["Rufibach", "Kaspar", ""], ["Pavlides", "Marios", ""]]}, {"id": "1107.3960", "submitter": "Jean-Michel Marin", "authors": "Damien Bousquet, Jean-Pierre Daur\\`es, Jean-Michel Marin", "title": "A new semi-parametric family of probability distributions for survival\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of survival analysis, Marshall and Olkin (1997) introduced\nfamilies of distributions by adding a scalar parameter to a given survival\nfunction, parameterized or not. In that paper, we generalize their approach. We\nshow how it is possible to add more than a single parameter to a given\ndistribution. We then introduce very flexible families of distributions for\nwhich we calculate some moments. Notably, we give some tractable expressions of\nthese moments when the given baseline distribution is Log-logistic. Finally, we\ndemonstrate how to generate sample from these new families.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2011 13:05:00 GMT"}], "update_date": "2011-07-21", "authors_parsed": [["Bousquet", "Damien", ""], ["Daur\u00e8s", "Jean-Pierre", ""], ["Marin", "Jean-Michel", ""]]}, {"id": "1107.4047", "submitter": "Eric Ford", "authors": "Eric B. Ford (UF), Althea V. Moorhead (UF), Dimitri Veras (UF, IoA)", "title": "A Bayesian Surrogate Model for Rapid Time Series Analysis and\n  Application to Exoplanet Observations", "comments": "25 pages, 4 figures, accepted to Bayesian Analysis\n  <http://ba.stat.cmu.edu>, special issue for Ninth Valencia International\n  Conference on Bayesian Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.EP astro-ph.IM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian surrogate model for the analysis of periodic or\nquasi-periodic time series data. We describe a computationally efficient\nimplementation that enables Bayesian model comparison. We apply this model to\nsimulated and real exoplanet observations. We discuss the results and\ndemonstrate some of the challenges for applying our surrogate model to\nrealistic exoplanet data sets. In particular, we find that analyses of real\nworld data should pay careful attention to the effects of uneven spacing of\nobservations and the choice of prior for the \"jitter\" parameter.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2011 17:47:43 GMT"}], "update_date": "2011-07-21", "authors_parsed": [["Ford", "Eric B.", "", "UF"], ["Moorhead", "Althea V.", "", "UF"], ["Veras", "Dimitri", "", "UF, IoA"]]}, {"id": "1107.4344", "submitter": "Guenther Walther", "authors": "Hock Peng Chan and Guenther Walther", "title": "Detection with the scan and the average likelihood ratio", "comments": null, "journal-ref": "Statistica Sinica 23 (2013), 409-428", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the performance of the scan (maximum likelihood ratio\nstatistic) and of the average likelihood ratio statistic in the problem of\ndetecting a deterministic signal with unknown spatial extent in the\nprototypical univariate sampled data model with white Gaussian noise. Our\nresults show that the scan statistic, a popular tool for detection problems, is\noptimal only for the detection of signals with the smallest spatial extent. For\nsignals with larger spatial extent the scan is suboptimal, and the power loss\ncan be considerable. In contrast, the average likelihood ratio statistic is\noptimal for the detection of signals on all scales except the smallest ones,\nwhere its performance is only slightly suboptimal. We give rigorous\nmathematical statements of these results as well as heuristic explanations\nwhich suggest that the essence of these findings applies to detection problems\nquite generally, such as the detection of clusters in models involving\ndensities or intensities or the detection of multivariate signals. We present a\nmodification of the average likelihood ratio that yields optimal detection of\nsignals with arbitrary spatial extent and which has the additional benefit of\nallowing for a fast computation of the statistic. In contrast, optimal\ndetection with the scan seems to require the use of scale-dependent critical\nvalues.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 19:27:52 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2014 19:29:47 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Chan", "Hock Peng", ""], ["Walther", "Guenther", ""]]}, {"id": "1107.4381", "submitter": "Johan Segers", "authors": "Oliver Grothe and Friedrich Schmid and Julius Schnieders and Johan\n  Segers", "title": "Measuring Association between Random Vectors", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper suggests five measures of association between two random vectors X\n= (X_1, ..., X_p) and Y = (Y_1, ..., Y_q). They are copula based and therefore\ninvariant with respect to the marginal distributions of the components X_i and\nY_j. The measures capture positive as well as negative association of X and Y.\nIn case p = q = 1 they reduce to Spearman's rho. Various properties of these\nnew measures are investigated. Nonparametric estimators, based on ranks, for\nthe measures are derived and their small sample behaviour is investigated by\nsimulation. The measures are applied to characterise strength and direction of\nassociation of bond and stock indices of five countries over time.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 21:06:55 GMT"}], "update_date": "2011-07-25", "authors_parsed": [["Grothe", "Oliver", ""], ["Schmid", "Friedrich", ""], ["Schnieders", "Julius", ""], ["Segers", "Johan", ""]]}, {"id": "1107.4390", "submitter": "Sergey Feldman", "authors": "Sergey Feldman, Bela A. Frigyik, Maya R. Gupta", "title": "Multi-Task Averaging", "comments": "totally redone paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-task learning approach to jointly estimate the means of\nmultiple independent data sets. The proposed multi-task averaging (MTA)\nalgorithm results in a convex combination of the single-task maximum likelihood\nestimates. We derive the optimal minimum risk estimator and the minimax\nestimator, and show that these estimators can be efficiently estimated.\nSimulations and real data experiments demonstrate that MTA estimators often\noutperform both single-task and James-Stein estimators.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 22:10:22 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2011 17:46:38 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2011 19:09:36 GMT"}, {"version": "v4", "created": "Fri, 24 Aug 2012 22:35:38 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Feldman", "Sergey", ""], ["Frigyik", "Bela A.", ""], ["Gupta", "Maya R.", ""]]}, {"id": "1107.4464", "submitter": "Christina  Steinkohl", "authors": "Richard A. Davis, Claudia Kl\\\"uppelberg, Christina Steinkohl", "title": "Max-stable processes for modelling extremes observed in space and time", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable processes have proved to be useful for the statistical modelling\nof spatial extremes. Several representations of max-stable random fields have\nbeen proposed in the literature. For statistical inference it is often assumed\nthat there is no temporal dependence, i.e., the observations at spatial\nlocations are independent in time. We use two representations of stationary\nmax-stable spatial random fields and extend the concepts to the space-time\ndomain. In a first approach, we extend the idea of constructing max-stable\nrandom fields as limits of normalized and rescaled pointwise maxima of\nindependent Gaussian random fields, which was introduced by Kabluchko,\nSchlather and de Haan [2009], who construct max-stable random fields associated\nto a class of variograms. We use a similar approach based on a well-known\nresult by H\\\"usler and Reiss and apply specific spatio-temporal covariance\nmodels for the underlying Gaussian random field, which satisfy weak regularity\nassumptions. Furthermore, we extend Smith's storm profile model to a space-time\nsetting and provide explicit expressions for the bivariate distribution\nfunctions.\n  The tail dependence coefficient is an important measure of extremal\ndependence. We show how the spatio-temporal covariance function underlying the\nGaussian random field can be interpreted in terms of the tail dependence\ncoefficient. Within this context, we examine different concepts for\nconstructing spatio-temporal covariance models and analyse several specific\nexamples, including Gneiting's class of nonseparable stationary covariance\nfunctions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 09:29:57 GMT"}], "update_date": "2011-07-25", "authors_parsed": [["Davis", "Richard A.", ""], ["Kl\u00fcppelberg", "Claudia", ""], ["Steinkohl", "Christina", ""]]}, {"id": "1107.4861", "submitter": "Heng Lian", "authors": "Heng Lian", "title": "Semiparametric Bayesian Information Criterion for Model Selection in\n  Ultra-high Dimensional Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For linear models with a diverging number of parameters, it has recently been\nshown that modified versions of Bayesian information criterion (BIC) can\nidentify the true model consistently. However, in many cases there is little\njustification that the effects of the covariates are actually linear. Thus a\nsemiparametric model such as the additive model studied here, is a viable\nalternative. We demonstrate that theoretical results on the consistency of\nBIC-type criterion can be extended to this more challenging situation, with\ndimension diverging exponentially fast with sample size. Besides, the noise\nassumptions are relaxed in our theoretical studies. These efforts significantly\nenlarge the applicability of the criterion to a more general class of models.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 08:00:13 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Lian", "Heng", ""]]}, {"id": "1107.4976", "submitter": "Artin Armagan", "authors": "Artin Armagan, David B. Dunson and Merlise Clyde", "title": "Generalized Beta Mixtures of Gaussians", "comments": "Advances in Neural Information Processing Systems 24 edited by J.\n  Shawe-Taylor and R.S. Zemel and P. Bartlett and F. Pereira and K.Q.\n  Weinberger (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a rich variety of shrinkage priors have been proposed that\nhave great promise in addressing massive regression problems. In general, these\nnew priors can be expressed as scale mixtures of normals, but have more complex\nforms and better properties than traditional Cauchy and double exponential\npriors. We first propose a new class of normal scale mixtures through a novel\ngeneralized beta distribution that encompasses many interesting priors as\nspecial cases. This encompassing framework should prove useful in comparing\ncompeting priors, considering properties and revealing close connections. We\nthen develop a class of variational Bayes approximations through the new\nhierarchy presented that will scale more efficiently to the types of truly\nmassive data sets that are now encountered routinely.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 15:21:06 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2012 21:22:21 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Armagan", "Artin", ""], ["Dunson", "David B.", ""], ["Clyde", "Merlise", ""]]}, {"id": "1107.5239", "submitter": "Emily Fox", "authors": "Emily B. Fox and Mike West", "title": "Autoregressive Models for Variance Matrices: Stationary Inverse Wishart\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and explore a new class of stationary time series models for\nvariance matrices based on a constructive definition exploiting inverse Wishart\ndistribution theory. The main class of models explored is a novel class of\nstationary, first-order autoregressive (AR) processes on the cone of positive\nsemi-definite matrices. Aspects of the theory and structure of these new models\nfor multivariate \"volatility\" processes are described in detail and\nexemplified. We then develop approaches to model fitting via Bayesian\nsimulation-based computations, creating a custom filtering method that relies\non an efficient innovations sampler. An example is then provided in analysis of\na multivariate electroencephalogram (EEG) time series in neurological studies.\nWe conclude by discussing potential further developments of higher-order AR\nmodels and a number of connections with prior approaches.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2011 15:19:27 GMT"}], "update_date": "2011-07-27", "authors_parsed": [["Fox", "Emily B.", ""], ["West", "Mike", ""]]}, {"id": "1107.5253", "submitter": "Caterina May", "authors": "Caterina May and Chiara Tommasi", "title": "An adaptive sequential optimum design for model selection and parameter\n  estimation in non-linear nested models", "comments": "This paper has been withdrawn by the author because it has been\n  substantially modified", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn by the author because it has been substantially\nmodified.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2011 16:09:44 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2011 11:31:47 GMT"}], "update_date": "2011-12-21", "authors_parsed": [["May", "Caterina", ""], ["Tommasi", "Chiara", ""]]}, {"id": "1107.5508", "submitter": "Matthew Sperrin", "authors": "Matthew Sperrin", "title": "Proximity penalty priors for Bayesian mixture models", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using mixture models it may be the case that the modeller has a-priori\nbeliefs or desires about what the components of the mixture should represent.\nFor example, if a mixture of normal densities is to be fitted to some data, it\nmay be desirable for components to focus on capturing differences in location\nrather than scale. We introduce a framework called proximity penalty priors\n(PPPs) that allows this preference to be made explicit in the prior\ninformation. The approach is scale-free and imposes minimal restrictions on the\nposterior; in particular no arbitrary thresholds need to be set. We show the\ntheoretical validity of the approach, and demonstrate the effects of using PPPs\non posterior distributions with simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 15:30:56 GMT"}], "update_date": "2011-07-28", "authors_parsed": [["Sperrin", "Matthew", ""]]}, {"id": "1107.5524", "submitter": "Cyrille Dubarry", "authors": "Cyrille Dubarry and Randal Douc", "title": "Particle approximation improvement of the joint smoothing distribution\n  with on-the-fly variance estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle smoothers are widely used algorithms allowing to approximate the\nsmoothing distribution in hidden Markov models. Existing algorithms often\nsuffer from slow computational time or degeneracy. We propose in this paper a\nway to improve any of them with a linear complexity in the number of particles.\nWhen iteratively applied to the degenerated Filter-Smoother, this method leads\nto an algorithm which turns out to outperform existing linear particle\nsmoothers for a fixed computational time. Moreover, the associated\napproximation satisfies a central limit theorem with a close-to-optimal\nasymptotic variance, which be easily estimated by only one run of the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 16:30:55 GMT"}], "update_date": "2011-07-28", "authors_parsed": [["Dubarry", "Cyrille", ""], ["Douc", "Randal", ""]]}, {"id": "1107.5592", "submitter": "Richard Davis", "authors": "Richard A. Davis, Thomas Mikosch, and Ivor Cribben", "title": "Estimating Extremal Dependence in Univariate and Multivariate Time\n  Series via the Extremogram", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Davis and Mikosch [7] introduced the extremogram as a flexible quantitative\ntool for measuring various types of extremal dependence in a stationary time\nseries. There we showed some standard statistical properties of the sample\nextremogram. A major difficulty was the construction of credible confidence\nbands for the extremogram. In this paper, we employ the stationary bootstrap to\novercome this problem. Moreover, we introduce the cross extremogram as a\nmeasure of extremal serial dependence between two or more time series. We also\nstudy the extremogram for return times between extremal events. The use of the\nstationary bootstrap for the extremogram and the resulting interpretations are\nillustrated in several univariate and multivariate financial time series\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 20:54:26 GMT"}], "update_date": "2011-07-29", "authors_parsed": [["Davis", "Richard A.", ""], ["Mikosch", "Thomas", ""], ["Cribben", "Ivor", ""]]}, {"id": "1107.5658", "submitter": "Gilles Fa\\\"{y}", "authors": "Gilles Fa\\\"y, Jacques Delabrouille, G\\'erard Kerkyacharian, Dominique\n  Picard", "title": "Testing the isotropy of high energy cosmic rays using spherical needlets", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS619 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 1040-1073", "doi": "10.1214/12-AOAS619", "report-no": "IMS-AOAS-AOAS619", "categories": "stat.AP astro-ph.HE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many decades, ultrahigh energy charged particles of unknown origin that\ncan be observed from the ground have been a puzzle for particle physicists and\nastrophysicists. As an attempt to discriminate among several possible\nproduction scenarios, astrophysicists try to test the statistical isotropy of\nthe directions of arrival of these cosmic rays. At the highest energies, they\nare supposed to point toward their sources with good accuracy. However, the\nobservations are so rare that testing the distribution of such samples of\ndirectional data on the sphere is nontrivial. In this paper, we choose a\nnonparametric framework that makes weak hypotheses on the alternative\ndistributions and allows in turn to detect various and possibly unexpected\nforms of anisotropy. We explore two particular procedures. Both are derived\nfrom fitting the empirical distribution with wavelet expansions of densities.\nWe use the wavelet frame introduced by [SIAM J. Math. Anal. 38 (2006b) 574-594\n(electronic)], the so-called needlets. The expansions are truncated at scale\nindices no larger than some ${J^{\\star}}$, and the $L^p$ distances between\nthose estimates and the null density are computed. One family of tests (called\nMultiple) is based on the idea of testing the distance from the null for each\nchoice of $J=1,\\ldots,{J^{\\star}}$, whereas the so-called PlugIn approach is\nbased on the single full ${J^{\\star}}$ expansion, but with thresholded wavelet\ncoefficients. We describe the practical implementation of these two procedures\nand compare them to other methods in the literature. As alternatives to\nisotropy, we consider both very simple toy models and more realistic\nnonisotropic models based on Physics-inspired simulations. The Monte Carlo\nstudy shows good performance of the Multiple test, even at moderate sample\nsize, for a wide sample of alternative hypotheses and for different choices of\nthe parameter ${J^{\\star}}$. On the 69 most energetic events published by the\nPierre Auger Collaboration, the needlet-based procedures suggest statistical\nevidence for anisotropy. Using several values for the parameters of the\nmethods, our procedures yield $p$-values below 1%, but with uncontrolled\nmultiplicity issues. The flexibility of this method and the possibility to\nmodify it to take into account a large variety of extensions of the problem\nmake it an interesting option for future investigation of the origin of\nultrahigh energy cosmic rays.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2011 09:33:46 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2012 00:02:22 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2013 13:24:00 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Fa\u00ff", "Gilles", ""], ["Delabrouille", "Jacques", ""], ["Kerkyacharian", "G\u00e9rard", ""], ["Picard", "Dominique", ""]]}, {"id": "1107.5735", "submitter": "Can Ozan Tan Dr.", "authors": "Michael A. Cohen and Can Ozan Tan", "title": "Multiplicative Propagation of Error During Recursive Wavelet Estimation", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Wavelet coefficients are estimated recursively at progressively coarser\nscales recursively. As a result, the estimation is prone to multiplicative\npropagation of truncation errors due to quantization and round-off at each\nstage. Yet, the influence of this propagation on wavelet filter output has not\nbeen explored systematically. Through numerical error analysis of a simple,\ngeneric sub-band coding scheme with a half-band low pass finite\nimpulse-response filter for down sampling, we show that truncation error in\nestimated wavelet filter coefficients can quickly reach unacceptable levels,\nand may render the results unreliable especially at coarser scales.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2011 15:16:47 GMT"}], "update_date": "2011-07-29", "authors_parsed": [["Cohen", "Michael A.", ""], ["Tan", "Can Ozan", ""]]}, {"id": "1107.6043", "submitter": "Zhijian Wang Dr.", "authors": "Bin Xu, Zhijian Wang", "title": "Measurement and Application of Entropy Production Rate in Human Subject\n  Social Interaction Systems", "comments": "4 pages, 4 figures, Keyword: entropy production rate, experimental\n  economics, experimental social dynamics, Edgeworth price cycle, mixed\n  strategy Nash equilibrium, velocity, minimax randomization, JEL: C91, C70", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME nlin.AO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper illustrates the measurement and the applications of the\nobservable, entropy production rate (EPR), in human subject social interaction\nsystems. To this end, we show (1) how to test the minimax randomization model\nwith experimental economics' 2$\\times$2 games data and with the Wimbledon\nTennis data; (2) how to identify the Edgeworth price cycle in experimental\nmarket data; and (3) the relationship within EPR and motion in data. As a\nresult, in human subject social interaction systems, EPR can be measured\npractically and can be employed to test models and to search for facts\nefficiently.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2011 19:30:00 GMT"}], "update_date": "2011-08-01", "authors_parsed": [["Xu", "Bin", ""], ["Wang", "Zhijian", ""]]}]