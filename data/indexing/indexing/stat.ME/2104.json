[{"id": "2104.00108", "submitter": "Jamie Yap", "authors": "Jamie Yap, John Dziak, Raju Maiti, Kevin Lynch, James R. McKay, Bibhas\n  Chakraborty, Inbal Nahum-Shani", "title": "Planning SMARTs: Sample size estimation for comparing dynamic treatment\n  regimens using longitudinal count outcomes with excess zeros", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many health domains such as substance-use, outcomes are often counts with\nan excessive number of zeros (EZ) - count data having zero counts at a rate\nsignificantly higher than that expected of a standard count distribution (e.g.,\nPoisson). However, an important gap exists in sample size estimation\nmethodology for planning sequential multiple assignment randomized trials\n(SMARTs) for comparing dynamic treatment regimens (DTRs) using longitudinal\ncount data. DTRs, also known as treatment algorithms or adaptive interventions,\nmimic the individualized and evolving nature of patient care through the\nspecification of decision rules guiding the type, timing and modality of\ndelivery, and dosage of treatments to address the unique and changing needs of\nindividuals. To close this gap, we develop a Monte Carlo-based approach to\nsample size estimation. A SMART for engaging alcohol and cocaine-dependent\npatients in treatment is used as motivation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:47:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Yap", "Jamie", ""], ["Dziak", "John", ""], ["Maiti", "Raju", ""], ["Lynch", "Kevin", ""], ["McKay", "James R.", ""], ["Chakraborty", "Bibhas", ""], ["Nahum-Shani", "Inbal", ""]]}, {"id": "2104.00151", "submitter": "Lam Ho", "authors": "Lam Si Tung Ho, Edward Susko", "title": "Ancestral state reconstruction with large numbers of sequences and\n  edge-length estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-based methods are widely considered the best approaches for\nreconstructing ancestral states. Although much effort has been made to study\nproperties of these methods, previous works often assume that both the tree\ntopology and edge lengths are known. In some scenarios the tree topology might\nbe reasonably well known for the taxa under study. When sequence length is much\nsmaller than the number of species, however, edge lengths are not likely to be\naccurately estimated. We study the consistency of the maximum likelihood and\nempirical Bayes estimators of ancestral state of discrete traits in such\nsettings under a star tree. We prove that the likelihood-based reconstruction\nis consistent under symmetric models but can be inconsistent under\nnon-symmetric models. We show, however, that a simple consistent estimator for\nthe ancestral states is available under non-symmetric models. The results\nillustrate that likelihood methods can unexpectedly have undesirable properties\nas the number of sequences considered get very large. Broader implications of\nthe results are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 22:41:42 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Ho", "Lam Si Tung", ""], ["Susko", "Edward", ""]]}, {"id": "2104.00242", "submitter": "Huijuan Zhou", "authors": "Huijuan Zhou, Xianyang Zhang, Kejun He, and Jun Chen", "title": "LinDA: Linear Models for Differential Abundance Analysis of Microbiome\n  Compositional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One fundamental statistical task in microbiome data analysis is differential\nabundance analysis, which aims to identify microbial taxa whose abundance\ncovaries with a variable of interest. Although the main interest is on the\nchange in the absolute abundance, i.e., the number of microbial cells per unit\narea/volume at the ecological site such as the human gut, the data from a\nsequencing experiment reflects only the taxa relative abundances in a sample.\nThus, microbiome data are compositional in nature. Analysis of such\ncompositional data is challenging since the change in the absolute abundance of\none taxon will lead to changes in the relative abundances of other taxa, making\nfalse positive control difficult. Here we present a simple, yet robust and\nhighly scalable approach to tackle the compositional effects in differential\nabundance analysis. The method only requires the application of established\nstatistical tools. It fits linear regression models on the centered log-ratio\ntransformed data, identifies a bias term due to the transformation and\ncompositional effect, and corrects the bias using the mode of the regression\ncoefficients. Due to the algorithmic simplicity, our method is 100-1000 times\nfaster than the state-of-the-art method ANCOM-BC. Under mild assumptions, we\nprove its asymptotic FDR control property, making it the first differential\nabundance method that enjoys a theoretical FDR control guarantee. The proposed\nmethod is very flexible and can be extended to mixed-effect models for the\nanalysis of correlated microbiome data. Using comprehensive simulations and\nreal data applications, we demonstrate that our method has overall the best\nperformance in terms of FDR control and power among the competitors. We\nimplemented the proposed method in the R package LinDA\n(https://github.com/zhouhj1994/LinDA).\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 04:01:44 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhou", "Huijuan", ""], ["Zhang", "Xianyang", ""], ["He", "Kejun", ""], ["Chen", "Jun", ""]]}, {"id": "2104.00245", "submitter": "Zhe Zhang", "authors": "Zhe Zhang and Linjun Zhang", "title": "High-Dimensional Differentially-Private EM Algorithm: Methods and\n  Near-Optimal Statistical Guarantees", "comments": "45 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a general framework to design differentially\nprivate expectation-maximization (EM) algorithms in high-dimensional latent\nvariable models, based on the noisy iterative hard-thresholding. We derive the\nstatistical guarantees of the proposed framework and apply it to three specific\nmodels: Gaussian mixture, mixture of regression, and regression with missing\ncovariates. In each model, we establish the near-optimal rate of convergence\nwith differential privacy constraints, and show the proposed algorithm is\nminimax rate optimal up to logarithm factors. The technical tools developed for\nthe high-dimensional setting are then extended to the classic low-dimensional\nlatent variable models, and we propose a near rate-optimal EM algorithm with\ndifferential privacy guarantees in this setting. Simulation studies and real\ndata analysis are conducted to support our results.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 04:08:34 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Zhe", ""], ["Zhang", "Linjun", ""]]}, {"id": "2104.00262", "submitter": "Michael Grabinski", "authors": "Maike Torm\\\"ahlen, Galiya Klinkova and Michael Grabinski", "title": "Statistical significance revisited", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": "10.20944/preprints202103.0398.v1", "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical significance measures the reliability of a result obtained from a\nrandom experiment. We investigate the number of repetitions needed for a\nstatistical result to have a certain significance. In the first step, we\nconsider binomially distributed variables in the example of medication testing\nwith fixed placebo efficacy, asking how many experiments are needed in order to\nachieve a significance of 95 %. In the next step, we take the probability\ndistribution of the placebo efficacy into account, which to the best of our\nknowledge has not been done so far. Depending on the specifics, we show that in\norder to obtain identical significance, it may be necessary to perform twice as\nmany experiments than in a setting where the placebo distribution is neglected.\nWe proceed by considering more general probability distributions and close with\ncomments on some erroneous assumptions on probability distributions which lead,\nfor instance, to a trivial explanation of the fat tail.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 05:38:46 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 08:35:20 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Torm\u00e4hlen", "Maike", ""], ["Klinkova", "Galiya", ""], ["Grabinski", "Michael", ""]]}, {"id": "2104.00271", "submitter": "Raffaele Mattera", "authors": "Roy Cerqueti, Massimiliano Giacalone and Raffaele Mattera", "title": "Model-based fuzzy time series clustering of conditional higher moments", "comments": null, "journal-ref": "International Journal of Approximate Reasoning (2021)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a new time series clustering procedure allowing for\nheteroskedasticity, non-normality and model's non-linearity. At this aim, we\nfollow a fuzzy approach. Specifically, considering a Dynamic Conditional Score\n(DCS) model, we propose to cluster time series according to their estimated\nconditional moments via the Autocorrelation-based fuzzy C-means (A-FCM)\nalgorithm. The DCS parametric modelling is appealing because of its generality\nand computational feasibility. The usefulness of the proposed procedure is\nillustrated using an experiment with simulated data and several empirical\napplications with financial time series assuming both linear and nonlinear\nmodels' specification and under several assumptions about time series density\nfunction.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 06:11:24 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Cerqueti", "Roy", ""], ["Giacalone", "Massimiliano", ""], ["Mattera", "Raffaele", ""]]}, {"id": "2104.00301", "submitter": "Nuutti Hyv\\\"onen", "authors": "Tapio Helin, Nuutti Hyv\\\"onen, Juha-Pekka Puska", "title": "Edge-promoting adaptive Bayesian experimental design for X-ray imaging", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers sequential edge-promoting Bayesian experimental design\nfor (discretized) linear inverse problems, exemplified by X-ray tomography. The\nprocess of computing a total variation type reconstruction of the absorption\ninside the imaged body via lagged diffusivity iteration is interpreted in the\nBayesian framework. Assuming a Gaussian additive noise model, this leads to an\napproximate Gaussian posterior with a covariance structure that contains\ninformation on the location of edges in the posterior mean. The next projection\ngeometry is then chosen through A-optimal Bayesian design, which corresponds to\nminimizing the trace of the updated posterior covariance matrix that accounts\nfor the new projection. Two and three-dimensional numerical examples based on\nsimulated data demonstrate the functionality of the introduced approach.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:12:50 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Helin", "Tapio", ""], ["Hyv\u00f6nen", "Nuutti", ""], ["Puska", "Juha-Pekka", ""]]}, {"id": "2104.00510", "submitter": "Karthik Bharath", "authors": "Shariq Mohammed, Karthik Bharath, Sebastian Kurtek, Arvind Rao and\n  Veerabhadran Baladandayuthapani", "title": "RADIOHEAD: Radiogenomic Analysis Incorporating Tumor Heterogeneity in\n  Imaging Through Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent technological advancements have enabled detailed investigation of\nassociations between the molecular architecture and tumor heterogeneity,\nthrough multi-source integration of radiological imaging and genomic\n(radiogenomic) data. In this paper, we integrate and harness radiogenomic data\nin patients with lower grade gliomas (LGG), a type of brain cancer, in order to\ndevelop a regression framework called RADIOHEAD (RADIOgenomic analysis\nincorporating tumor HEterogeneity in imAging through Densities) to identify\nradiogenomic associations. Imaging data is represented through voxel intensity\nprobability density functions of tumor sub-regions obtained from multimodal\nmagnetic resonance imaging, and genomic data through molecular signatures in\nthe form of pathway enrichment scores corresponding to their gene expression\nprofiles. Employing a Riemannian-geometric framework for principal component\nanalysis on the set of probability densities functions, we map each probability\ndensity to a vector of principal component scores, which are then included as\npredictors in a Bayesian regression model with the pathway enrichment scores as\nthe response. Variable selection compatible with the grouping structure amongst\nthe predictors induced through the tumor sub-regions is carried out under a\ngroup spike-and-slab prior. A Bayesian false discovery rate mechanism is then\nused to infer significant associations based on the posterior distribution of\nthe regression coefficients. Our analyses reveal several pathways relevant to\nLGG etiology (such as synaptic transmission, nerve impulse and neurotransmitter\npathways), to have significant associations with the corresponding\nimaging-based predictors.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:08:13 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 08:37:40 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Mohammed", "Shariq", ""], ["Bharath", "Karthik", ""], ["Kurtek", "Sebastian", ""], ["Rao", "Arvind", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "2104.00529", "submitter": "Hisashi Kobayashi", "authors": "Hisashi Kobayashi", "title": "Stochastic Modeling of an Infectious Disease, Part III-B: Analysis of\n  the Time-Nonhomogeneous BDI Process and Simulation Experiments of both BD and\n  BDI Processes", "comments": "31 pages, 66 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Section 1, we revisit the partial differential equation (PDE) for the\nprobability generating function (PGF) of the time-nonhomogeneous BDI\n(birth-and-death-with-immigration) process and derive a closed form solution.\nTo the best of our knowledge, this is a new mathematical result. We state this\nresult as Proposition 1. We state as Corollary 1 that the negative binomial\ndistribution of the time-homogeneous BDI process discussed in Part I extends to\nthe general time-nonhomogeneous case, provided that the ratio of the\nimmigration rate to the birth rate is a constant. In section 1.2, we take up\nthe heuristic approach discussed by Bartlett and Bailey (1964), and carry it\nout to completion by arriving at the solution obtained above,.\n  In Section 2, we present the results of our extensive simulation experiments\nof the time-nonhomogeneous BD process that was analyzed in Part III-A and\nconfirm our analytic results. In Section 3, we undertake similar simulation\nexperiments for the BDI process that is analyzed in Section 1.\n  As we discuss in Section 4, our stochastic model now seems more promising and\npowerful than has been heretofore expected. In Appendix B, a closed form\nsolution for the M(t)/M(t)/infinity queue is obtained, as a special case of\nthis BDI process model.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:43:16 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kobayashi", "Hisashi", ""]]}, {"id": "2104.00645", "submitter": "Tui Nolan", "authors": "Tui H. Nolan, Jeff Goldsmith and David Ruppert", "title": "Bayesian Functional Principal Components Analysis via Variational\n  Message Passing", "comments": "43 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional principal components analysis is a popular tool for inference on\nfunctional data. Standard approaches rely on an eigendecomposition of a\nsmoothed covariance surface in order to extract the orthonormal functions\nrepresenting the major modes of variation. This approach can be a\ncomputationally intensive procedure, especially in the presence of large\ndatasets with irregular observations. In this article, we develop a Bayesian\napproach, which aims to determine the Karhunen-Lo\\`eve decomposition directly\nwithout the need to smooth and estimate a covariance surface. More\nspecifically, we develop a variational Bayesian algorithm via message passing\nover a factor graph, which is more commonly referred to as variational message\npassing. Message passing algorithms are a powerful tool for compartmentalizing\nthe algebra and coding required for inference in hierarchical statistical\nmodels. Recently, there has been much focus on formulating variational\ninference algorithms in the message passing framework because it removes the\nneed for rederiving approximate posterior density functions if there is a\nchange to the model. Instead, model changes are handled by changing specific\ncomputational units, known as fragments, within the factor graph. We extend the\nnotion of variational message passing to functional principal components\nanalysis. Indeed, this is the first article to address a functional data model\nvia variational message passing. Our approach introduces two new fragments that\nare necessary for Bayesian functional principal components analysis. We present\nthe computational details, a set of simulations for assessing accuracy and\nspeed and an application to United States temperature data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:42:15 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Nolan", "Tui H.", ""], ["Goldsmith", "Jeff", ""], ["Ruppert", "David", ""]]}, {"id": "2104.00673", "submitter": "Stephen Bates", "authors": "Stephen Bates and Trevor Hastie and Robert Tibshirani", "title": "Cross-validation: what does it estimate and how well does it do it?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-validation is a widely-used technique to estimate prediction error, but\nits behavior is complex and not fully understood. Ideally, one would like to\nthink that cross-validation estimates the prediction error for the model at\nhand, fit to the training data. We prove that this is not the case for the\nlinear model fit by ordinary least squares; rather it estimates the average\nprediction error of models fit on other unseen training sets drawn from the\nsame population. We further show that this phenomenon occurs for most popular\nestimates of prediction error, including data splitting, bootstrapping, and\nMallow's Cp. Next, the standard confidence intervals for prediction error\nderived from cross-validation may have coverage far below the desired level.\nBecause each data point is used for both training and testing, there are\ncorrelations among the measured accuracies for each fold, and so the usual\nestimate of variance is too small. We introduce a nested cross-validation\nscheme to estimate this variance more accurately, and show empirically that\nthis modification leads to intervals with approximately correct coverage in\nmany examples where traditional cross-validation intervals fail. Lastly, our\nanalysis also shows that when producing confidence intervals for prediction\naccuracy with simple data splitting, one should not re-fit the model on the\ncombined data, since this invalidates the confidence intervals.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:58:54 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:51:23 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bates", "Stephen", ""], ["Hastie", "Trevor", ""], ["Tibshirani", "Robert", ""]]}, {"id": "2104.00718", "submitter": "Tom Edinburgh", "authors": "Tom Edinburgh and Stephen J. Eglen and Ari Ercole", "title": "Causality indices for bivariate time series data: a comparative review\n  of performance", "comments": "AAM, accepted for publication in AIP Chaos on 01/06/2021 (DOI not yet\n  available). 19 pages, 8 figures, 5 tables (including supplementary\n  materials): updated with CODECHECK", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inferring nonlinear and asymmetric causal relationships between multivariate\nlongitudinal data is a challenging task with wide-ranging application areas\nincluding clinical medicine, mathematical biology, economics and environmental\nresearch. A number of methods for inferring causal relationships within complex\ndynamic and stochastic systems have been proposed but there is not a unified\nconsistent definition of causality in this context. We evaluate the performance\nof ten prominent bivariate causality indices for time series data, across four\nsimulated model systems that have different coupling schemes and\ncharacteristics. In further experiments, we show that these methods may not\nalways be invariant to real-world relevant transformations (data availability,\nstandardisation and scaling, rounding error, missing data and noisy data). We\nrecommend transfer entropy and nonlinear Granger causality as likely to be\nparticularly robust indices for estimating bivariate causal relationships in\nreal-world applications. Finally, we provide flexible open-access Python code\nfor computation of these methods and for the model simulations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 18:53:48 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 15:22:05 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 15:06:02 GMT"}, {"version": "v4", "created": "Wed, 14 Jul 2021 17:51:38 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Edinburgh", "Tom", ""], ["Eglen", "Stephen J.", ""], ["Ercole", "Ari", ""]]}, {"id": "2104.00780", "submitter": "Tianyu Zhang", "authors": "Tianyu Zhang and Noah Simon", "title": "An Online Projection Estimator for Nonparametric Regression in\n  Reproducing Kernel Hilbert Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of nonparametric regression is to recover an underlying regression\nfunction from noisy observations, under the assumption that the regression\nfunction belongs to a pre-specified infinite dimensional function space. In the\nonline setting, when the observations come in a stream, it is generally\ncomputationally infeasible to refit the whole model repeatedly. There are as of\nyet no methods that are both computationally efficient and statistically\nrate-optimal. In this paper, we propose an estimator for online nonparametric\nregression. Notably, our estimator is an empirical risk minimizer (ERM) in a\ndeterministic linear space, which is quite different from existing methods\nusing random features and functional stochastic gradient. Our theoretical\nanalysis shows that this estimator obtains rate-optimal generalization error\nwhen the regression function is known to live in a reproducing kernel Hilbert\nspace. We also show, theoretically and empirically, that the computational\nexpense of our estimator is much lower than other rate-optimal estimators\nproposed for this online setting.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 21:54:54 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zhang", "Tianyu", ""], ["Simon", "Noah", ""]]}, {"id": "2104.00846", "submitter": "Tianyu Zhang", "authors": "Tianyu Zhang and Noah Simon", "title": "A Sieve Stochastic Gradient Descent Estimator for Online Nonparametric\n  Regression in Sobolev ellipsoids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of regression is to recover an unknown underlying function that best\nlinks a set of predictors to an outcome from noisy observations. In\nnon-parametric regression, one assumes that the regression function belongs to\na pre-specified infinite dimensional function space (the hypothesis space). In\nthe online setting, when the observations come in a stream, it is\ncomputationally-preferable to iteratively update an estimate rather than\nrefitting an entire model repeatedly. Inspired by nonparametric sieve\nestimation and stochastic approximation methods, we propose a sieve stochastic\ngradient descent estimator (Sieve-SGD) when the hypothesis space is a Sobolev\nellipsoid. We show that Sieve-SGD has rate-optimal MSE under a set of simple\nand direct conditions. We also show that the Sieve-SGD estimator can be\nconstructed with low time expense, and requires almost minimal memory usage\namong all statistically rate-optimal estimators, under some conditions on the\ndistribution of the predictors.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 01:52:47 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zhang", "Tianyu", ""], ["Simon", "Noah", ""]]}, {"id": "2104.00997", "submitter": "Landan Zhang", "authors": "Landan Zhang, Michael K. Pitt and Robert Kohn", "title": "Dynamic models using score copula innovations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a new class of observation driven dynamic models. The\ntime evolving parameters are driven by innovations of copula form. The\nresulting models can be made strictly stationary and the innovation term is\ntypically chosen to be Gaussian. The innovations are formed by applying a\ncopula approach for the conditional score function which has close connections\nthe existing literature on GAS models. This new method provides a unified\nframework for observation-driven models allowing the likelihood to be\nexplicitly computed using the prediction decomposition. The approach may be\nused for multiple lag structures and for multivariate models. Strict\nstationarity can be easily imposed upon the models making the invariant\nproperties simple to ascertain. This property also has advantages for\nspecifying the initial conditions needed for maximum likelihood estimation. One\nstep and multi-period forecasting is straight-forward and the forecasting\ndensity is either in closed form or a simple mixture over a univariate\ncomponent. The approach is very general and the illustrations focus on\nvolatility models and duration models. We illustrate the performance of the\nmodelling approach for both univariate and multivariate volatility models.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 11:47:24 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zhang", "Landan", ""], ["Pitt", "Michael K.", ""], ["Kohn", "Robert", ""]]}, {"id": "2104.01000", "submitter": "Zoe Guan", "authors": "Zoe Guan", "title": "A Proper Scoring Rule for Validation of Competing Risks Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scoring rules are used to evaluate the quality of predictions that take the\nform of probability distributions. A scoring rule is strictly proper if its\nexpected value is uniquely minimized by the true probability distribution. One\nof the most well-known and widely used strictly proper scoring rules is the\nlogarithmic scoring rule. We propose a version of the logarithmic scoring rule\nfor competing risks data and show that it remains strictly proper under\nnon-informative censoring.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 11:59:14 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Guan", "Zoe", ""]]}, {"id": "2104.01067", "submitter": "Lionel Truquet", "authors": "Zinsou Max Debaly and Lionel Truquet", "title": "Multivariate time series models for mixed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general approach for modeling the dynamic of multivariate time\nseries when the data are of mixed type (binary/count/continuous). Our method is\nquite flexible and conditionally on past values, each coordinate at time $t$\ncan have a distribution compatible with a standard univariate time series model\nsuch as GARCH, ARMA, INGARCH or logistic models whereas past values of the\nother coordinates play the role of exogenous covariates in the dynamic. The\nsimultaneous dependence in the multivariate time series can be modeled with a\ncopula. Additional exogenous covariates are also allowed in the dynamic. We\nfirst study usual stability properties of these models and then show that\nautoregressive parameters can be consistently estimated equation-by-equation\nusing a pseudo-maximum likelihood method, leading to a fast implementation even\nwhen the number of time series is large. Moreover, we prove consistency results\nwhen a parametric copula model is fitted to the time series and in the case of\nGaussian copulas, we show that the likelihood estimator of the correlation\nmatrix is strongly consistent. We carefully check all our assumptions for two\nprototypical examples: a GARCH/INGARCH model and logistic/log-linear INGARCH\nmodel. Our results are illustrated with numerical experiments as well as two\nreal data sets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 14:33:41 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Debaly", "Zinsou Max", ""], ["Truquet", "Lionel", ""]]}, {"id": "2104.01115", "submitter": "Jason Wang", "authors": "Jason Wang and Robert E. Weiss", "title": "Local and Global Topics in Text Modeling of Web Pages Nested in Web\n  Sites", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Topic models are popular models for analyzing a collection of text documents.\nThe models assert that documents are distributions over latent topics and\nlatent topics are distributions over words. A nested document collection is\nwhere documents are nested inside a higher order structure such as stories in a\nbook, articles in a journal, or web pages in a web site. In a single collection\nof documents, topics are global, or shared across all documents. For web pages\nnested in web sites, topic frequencies likely vary between web sites. Within a\nweb site, topic frequencies almost certainly vary between web pages. A\nhierarchical prior for topic frequencies models this hierarchical structure and\nspecifies a global topic distribution. Web site topic distributions vary around\nthe global topic distribution and web page topic distributions vary around the\nweb site topic distribution. In a nested collection of web pages, some topics\nare likely unique to a single web site. Local topics in a nested collection of\nweb pages are topics unique to one web site. For US local health department web\nsites, brief inspection of the text shows local geographic and news topics\nspecific to each department that are not present in others. Topic models that\nignore the nesting may identify local topics, but do not label topics as local\nnor do they explicitly identify the web site owner of the local topic. For web\npages nested inside web sites, local topic models explicitly label local topics\nand identifies the owning web site. This identification can be used to adjust\ninferences about global topics. In the US public health web site data, topic\ncoverage is defined at the web site level after removing local topic words from\npages. Hierarchical local topic models can be used to identify local topics,\nadjust inferences about if web sites cover particular health topics, and study\nhow well health topics are covered.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 23:16:46 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Wang", "Jason", ""], ["Weiss", "Robert E.", ""]]}, {"id": "2104.01133", "submitter": "Jonathan Cardoso-Silva", "authors": "Pedro Henrique da Costa Avelar, Luis C. Lamb, Sophia Tsoka, Jonathan\n  Cardoso-Silva", "title": "Weekly Bayesian modelling strategy to predict deaths by COVID-19: a\n  model and case study for the state of Santa Catarina, Brazil", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Background: The novel coronavirus pandemic has affected Brazil's Santa\nCatarina State (SC) severely. At the time of writing (24 March 2021), over\n764,000 cases and over 9,800 deaths by COVID-19 have been confirmed, hospitals\nwere fully occupied with local news reporting at least 397 people in the\nwaiting list for an ICU bed. In an attempt to better inform local policy\nmaking, we applied an existing Bayesian algorithm to model the spread of the\npandemic in the seven geographic macro-regions of the state. Here we propose\nchanges to extend the model and improve its forecasting capabilities.\n  Methods: Our four proposed variations of the original method allow accessing\ndata of daily reported infections and take into account under-reporting of\ncases more explicitly. Two of the proposed versions also attempt to model the\ndelay in test reporting. We simulated weekly forecasting of deaths from the\nperiod from 31/05/2020 until 31/01/2021. First week data were used as a\ncold-start to the algorithm, after which weekly calibrations of the model were\nable to converge in fewer iterations. Google Mobility data were used as\ncovariates to the model, as well as to estimate of the susceptible population\nat each simulated run.\n  Findings: The changes made the model significantly less reactive and more\nrapid in adapting to scenarios after a peak in deaths is observed. Assuming\nthat the cases are under-reported greatly benefited the model in its stability,\nand modelling retroactively-added data (due to the \"hot\" nature of the data\nused) had a negligible impact in performance.\n  Interpretation: Although not as reliable as death statistics, case\nstatistics, when modelled in conjunction with an overestimate parameter,\nprovide a good alternative for improving the forecasting of models, especially\nin long-range predictions and after the peak of an infection wave.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:18:41 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Avelar", "Pedro Henrique da Costa", ""], ["Lamb", "Luis C.", ""], ["Tsoka", "Sophia", ""], ["Cardoso-Silva", "Jonathan", ""]]}, {"id": "2104.01165", "submitter": "Marcos Matabuena", "authors": "Marcos Matabuena and Alex Petersen", "title": "Distributional data analysis with accelerometer data in a NHANES\n  database with nonparametric survey regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accelerometers enable an objective measurement of physical activity levels\namong groups of individuals in free-living environments, providing\nhigh-resolution detail about physical activity changes at different time\nscales. Current approaches used in the literature for analyzing such data\ntypically employ summary measures such as total inactivity time or\ncompositional metrics. However, at the conceptual level, these methods have the\npotential disadvantage of discarding important information from recorded data\nwhen calculating these summaries and metrics since these typically depend on\ncut-offs related to intensity exercise zones that are chosen subjectively or\neven arbitrarily. Much of the data collected in these studies follow complex\nsurvey designs, making application of standard statistical tools such as\nnon-parametric regression models inappropriate and the requirement of specific\nestimation procedures according to particular sampling-design is mandatory.\nWith functional data or other complex objects, barely literature exist that\nhandles complex sampling designs in the statistical analysis. This paper aims\ntwo-fold; first, we introduce a new functional representation of accelerometer\ndata of a distributional nature to build a complete individualized profile of\neach subject's physical activity levels. Second, using the NHANES accelerometer\ndata (2003-2006), we show the potential advantages of this new representation\nto predict patients' outcomes over $68$ years of age. A critical component in\nour statistical modeling is that we extend non-parametric functional models\nused: kernel smoother and kernel ridge regression, to handle the specific\neffect of complex sampling design in order to provide reliable conclusions\nabout the influence of physical activity in distinct analysis performed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 17:30:39 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Matabuena", "Marcos", ""], ["Petersen", "Alex", ""]]}, {"id": "2104.01346", "submitter": "Ruth Heller", "authors": "Ruth Heller and Abba Krieger and Saharon Rosset", "title": "Optimal multiple testing and design in clinical trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A central goal in designing clinical trials is to find the test that\nmaximizes power (or equivalently minimizes required sample size) for finding a\ntrue research hypothesis subject to the constraint of type I error. When there\nis more than one test, such as in clinical trials with multiple endpoints, the\nissues of optimal design and optimal policies become more complex. In this\npaper we address the question of how such optimal tests should be defined and\nhow they can be found. We review different notions of power and how they relate\nto study goals, and also consider the requirements of type I error control and\nthe nature of the policies. This leads us to formulate the optimal policy\nproblem as an explicit optimization problem with objective and constraints\nwhich describe its specific desiderata. We describe a complete solution for\nderiving optimal policies for two hypotheses, which have desired monotonicity\nproperties, and are computationally simple. For some of the optimization\nformulations this yields optimal policies that are identical to existing\npolicies, such as Hommel's procedure or the procedure of Bittman et al. (2009),\nwhile for others it yields completely novel and more powerful policies than\nexisting ones. We demonstrate the nature of our novel policies and their\nimproved power extensively in simulation and on the APEX study (Cohen et al.,\n2016).\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 08:54:13 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Heller", "Ruth", ""], ["Krieger", "Abba", ""], ["Rosset", "Saharon", ""]]}, {"id": "2104.01603", "submitter": "Konstantinos Vamvourellis", "authors": "Konstantinos Vamvourellis, Kostas Kalogeropoulos and Irini Moustaki", "title": "Generalised Bayesian Structural Equation Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a generalised framework for Bayesian Structural Equation Modelling\n(SEM) that can be applied to a variety of data types. The introduced framework\nfocuses on the approximate zero approach, according to which a hypothesised\nstructure is formulated with approximate rather than exact zero. It extends\npreviously suggested models by \\citeA{MA12} and can handle continuous, binary,\nand ordinal data. Moreover, we propose a novel model assessment paradigm aiming\nto address shortcomings of posterior predictive $p-$values, which provide the\ndefault metric of fit for Bayesian SEM. The introduced model assessment\nprocedure monitors the out-of-sample predictive performance of the model in\nquestion, and draws from a list of principles to answer whether the\nhypothesised theory is supported by the data. We incorporate scoring rules and\ncross-validation to supplement existing model assessment metrics for Bayesian\nSEM. The methodology is illustrated in continuous and categorical data examples\nvia simulation experiments as well as real-world applications on the `Big-5'\npersonality scale and the Fagerstrom test for nicotine dependence.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 12:44:05 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Vamvourellis", "Konstantinos", ""], ["Kalogeropoulos", "Kostas", ""], ["Moustaki", "Irini", ""]]}, {"id": "2104.01673", "submitter": "Xinwei Deng", "authors": "C. Devon Lin, Peter Chien, Xinwei Deng", "title": "Efficient Experimental Design for Regularized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regularized linear models, such as Lasso, have attracted great attention in\nstatistical learning and data science. However, there is sporadic work on\nconstructing efficient data collection for regularized linear models. In this\nwork, we propose an experimental design approach, using nearly orthogonal Latin\nhypercube designs, to enhance the variable selection accuracy of the\nregularized linear models. Systematic methods for constructing such designs are\npresented. The effectiveness of the proposed method is illustrated with several\nexamples.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 19:34:38 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Lin", "C. Devon", ""], ["Chien", "Peter", ""], ["Deng", "Xinwei", ""]]}, {"id": "2104.01707", "submitter": "Aaron Molstad", "authors": "Piotr M. Suder and Aaron J. Molstad", "title": "Scalable algorithms for semiparametric accelerated failure time models\n  in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semiparametric accelerated failure time (AFT) models are a useful alternative\nto Cox proportional hazards models, especially when the assumption of constant\nhazard ratios is untenable. However, rank-based criteria for fitting AFT models\nare often non-differentiable, which poses a computational challenge in\nhigh-dimensional settings. In this article, we propose a new alternating\ndirection method of multipliers algorithm for fitting semiparametric AFT models\nby minimizing a penalized rank-based loss function. Our algorithm scales well\nin both the number of subjects and number of predictors; and can easily\naccommodate a wide range of popular penalties. To improve the selection of\ntuning parameters, we propose a new criterion which avoids some common problems\nin cross-validation with censored responses. Through extensive simulation\nstudies, we show that our algorithm and software is much faster than existing\nmethods (which can only be applied to special cases), and we show that\nestimators which minimize a penalized rank-based criterion often outperform\nalternative estimators which minimize penalized weighted least squares\ncriteria. Application to nine cancer datasets further demonstrates that\nrank-based estimators of semiparametric AFT models are competitive with\nestimators assuming proportional hazards model in high-dimensional settings,\nwhereas weighted least squares estimators are often not. A software package\nimplementing the algorithm, along with a set of auxiliary functions, is\navailable for download at github.com/ajmolstad/penAFT.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 22:22:33 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Suder", "Piotr M.", ""], ["Molstad", "Aaron J.", ""]]}, {"id": "2104.01822", "submitter": "Solon Karapanagiotis", "authors": "Solon Karapanagiotis, Umberto Benedetto, Sach Mukherjee, Paul D. W.\n  Kirk, Paul J. Newcombe", "title": "Tailored Bayes: a risk modelling framework under unequal\n  misclassification costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Risk prediction models are a crucial tool in healthcare. Risk prediction\nmodels with a binary outcome (i.e., binary classification models) are often\nconstructed using methodology which assumes the costs of different\nclassification errors are equal. In many healthcare applications this\nassumption is not valid, and the differences between misclassification costs\ncan be quite large. For instance, in a diagnostic setting, the cost of\nmisdiagnosing a person with a life-threatening disease as healthy may be larger\nthan the cost of misdiagnosing a healthy person as a patient. In this work, we\npresent Tailored Bayes (TB), a novel Bayesian inference framework which\n\"tailors\" model fitting to optimise predictive performance with respect to\nunbalanced misclassification costs. We use simulation studies to showcase when\nTB is expected to outperform standard Bayesian methods in the context of\nlogistic regression. We then apply TB to three real-world applications, a\ncardiac surgery, a breast cancer prognostication task and a breast cancer\ntumour classification task, and demonstrate the improvement in predictive\nperformance over standard methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 09:37:07 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Karapanagiotis", "Solon", ""], ["Benedetto", "Umberto", ""], ["Mukherjee", "Sach", ""], ["Kirk", "Paul D. W.", ""], ["Newcombe", "Paul J.", ""]]}, {"id": "2104.01863", "submitter": "Matteo Barigozzi", "authors": "Matteo Barigozzi and Matteo Farn\\`e", "title": "An algebraic estimator for large spectral density matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new estimator of high-dimensional spectral density matrices,\ncalled UNshrunk ALgebraic Spectral Estimator (UNALSE), under the assumption of\nan underlying low rank plus sparse structure, as typically assumed in dynamic\nfactor models. The UNALSE is computed by minimizing a quadratic loss under a\nnuclear norm plus $l_1$ norm constraint to control the latent rank and the\nresidual sparsity pattern. The loss function requires as input the classical\nsmoothed periodogram estimator and two threshold parameters, the choice of\nwhich is thoroughly discussed. We prove consistency of UNALSE as both the\ndimension $p$ and the sample size $T$ diverge to infinity, as well as algebraic\nconsistency, i.e., the recovery of latent rank and residual sparsity pattern\nwith probability one. The finite sample properties of UNALSE are studied by\nmeans of an extended simulation exercise as well as an empirical analysis of US\nmacroeconomic data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 11:57:20 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Barigozzi", "Matteo", ""], ["Farn\u00e8", "Matteo", ""]]}, {"id": "2104.01885", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "Conformal testing in a binary model situation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal testing is a way of testing the IID assumption based on conformal\nprediction. The topic of this note is computational evaluation of the\nperformance of conformal testing in a model situation in which IID binary\nobservations generated from a Bernoulli distribution are followed by IID binary\nobservations generated from another Bernoulli distribution, with the parameters\nof the distributions and changepoint unknown. Existing conformal test\nmartingales can be used for this task and work well in simple cases, but their\nefficiency can be improved greatly.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 12:51:39 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "2104.01920", "submitter": "Michele Lambardi Di San Miniato", "authors": "Michele Lambardi di San Miniato (1), Nicola Sartori (1) ((1)\n  University of Padova)", "title": "Adjusted composite likelihood for robust Bayesian meta-analysis", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A composite likelihood is a non-genuine likelihood function that allows to\nmake inference on limited aspects of a model, such as marginal or conditional\ndistributions. Composite likelihoods are not proper likelihoods and need\ntherefore calibration for their use in inference, from both a frequentist and a\nBayesian perspective. The maximizer to the composite likelihood can serve as an\nestimator and its variance is assessed by means of a suitably defined sandwich\nmatrix. In the Bayesian setting, the composite likelihood can be adjusted by\nmeans of magnitude and curvature methods. Magnitude methods imply raising the\nlikelihood to a constant, while curvature methods imply evaluating the\nlikelihood at a different point by translating, rescaling and rotating the\nparameter vector. Some authors argue that curvature methods are more reliable\nin general, but others proved that magnitude methods are sufficient to recover,\nfor instance, the null distribution of a test statistic. We propose a simple\ncalibration for the marginal posterior distribution of a scalar parameter of\ninterest which is invariant to monotonic and smooth transformations. This can\nbe enough for instance in medical statistics, where a single scalar effect\nmeasure is often the target.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 14:02:00 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Miniato", "Michele Lambardi di San", ""], ["Sartori", "Nicola", ""]]}, {"id": "2104.01921", "submitter": "Alan Mishler", "authors": "Alan Mishler, Niccol\\`o Dalmasso", "title": "When the Oracle Misleads: Modeling the Consequences of Using Observable\n  Rather than Potential Outcomes in Risk Assessment Instruments", "comments": "6 pages, 3 figures. Presented at the workshop \"'Do the right thing':\n  machine learning and causal inference for improved decision making,\" NeurIPS\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Risk Assessment Instruments (RAIs) are widely used to forecast adverse\noutcomes in domains such as healthcare and criminal justice. RAIs are commonly\ntrained on observational data and are optimized to predict observable outcomes\nrather than potential outcomes, which are the outcomes that would occur absent\na particular intervention. Examples of relevant potential outcomes include\nwhether a patient's condition would worsen without treatment or whether a\ndefendant would recidivate if released pretrial. We illustrate how RAIs which\nare trained to predict observable outcomes can lead to worse decision making,\ncausing precisely the types of harm they are intended to prevent. This can\noccur even when the predictors are Bayes-optimal and there is no unmeasured\nconfounding.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 14:02:07 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Mishler", "Alan", ""], ["Dalmasso", "Niccol\u00f2", ""]]}, {"id": "2104.01958", "submitter": "Donald Ebeigbe", "authors": "Donald Ebeigbe, Tyrus Berry, Michael M. Norton, Andrew J. Whalen, Dan\n  Simon, Timothy Sauer, Steven J. Schiff", "title": "A Generalized Unscented Transformation for Probability Distributions", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The unscented transform uses a weighted set of samples called sigma points to\npropagate the means and covariances of nonlinear transformations of random\nvariables. However, unscented transforms developed using either the Gaussian\nassumption or a minimum set of sigma points typically fall short when the\nrandom variable is not Gaussian distributed and the nonlinearities are\nsubstantial. In this paper, we develop the generalized unscented transform\n(GenUT), which uses adaptable sigma points that can be positively constrained,\nand accurately approximates the mean, covariance, and skewness of an\nindependent random vector of most probability distributions, while being able\nto partially approximate the kurtosis. For correlated random vectors, the GenUT\ncan accurately approximate the mean and covariance. In addition to its superior\naccuracy in propagating means and covariances, the GenUT uses the same order of\ncalculations as most unscented transforms that guarantee third-order accuracy,\nwhich makes it applicable to a wide variety of applications, including the\nassimilation of observations in the modeling of the coronavirus (SARS-CoV-2)\ncausing COVID-19.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 15:18:33 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ebeigbe", "Donald", ""], ["Berry", "Tyrus", ""], ["Norton", "Michael M.", ""], ["Whalen", "Andrew J.", ""], ["Simon", "Dan", ""], ["Sauer", "Timothy", ""], ["Schiff", "Steven J.", ""]]}, {"id": "2104.02037", "submitter": "Ayao Ehara", "authors": "Ayao Ehara and Serge Guillas", "title": "An adaptive strategy for sequential designs of multilevel computer\n  experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Investigating uncertainties in computer simulations can be prohibitive in\nterms of computational costs, since the simulator needs to be run over a large\nnumber of input values. Building an emulator, i.e. a statistical surrogate\nmodel of the simulator, using a small design of experiments, greatly alleviates\nthe computational burden to carry out such investigations. Nevertheless, this\ncan still be above the computational budget for many studies. Two major\napproaches have been used are to reduce the budget needed to build the\nemulator: efficient design of experiments, such as sequential designs, and\ncombining training data of different degrees of sophistication in a so-called\nmulti-fidelity method, or multilevel in case these fidelities are ordered\ntypically for increasing resolutions. We present here a novel method that\ncombines both approaches, the multilevel adaptive sequential design of computer\nexperiments (MLASCE) in the framework of Gaussian process (GP) emulators. We\nmake use of reproducing kernel Hilbert spaces as a new tool for our GP\napproximations of the increments across levels. This dual strategy allows us to\nallocate efficiently limited computational resource over simulations of\ndifferent levels of fidelity and build the GP emulator. The allocation of\ncomputational resources is shown to be the solution of a simple optimization\nproblem in a special case where we theoretically prove the validity of our\napproach. Our proposed method is compared with other existing models of\nmulti-fidelity Gaussian process emulation. Gains of orders of magnitudes in\naccuracy for medium-size computing budgets are demonstrated in numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:42:48 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ehara", "Ayao", ""], ["Guillas", "Serge", ""]]}, {"id": "2104.02105", "submitter": "Olha Bodnar", "authors": "Olha Bodnar and Taras Bodnar", "title": "Objective Bayesian meta-analysis based on generalized multivariate\n  random effects model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective Bayesian inference procedures are derived for the parameters of the\nmultivariate random effects model generalized to elliptically contoured\ndistributions. The posterior for the overall mean vector and the between-study\ncovariance matrix is deduced by assigning two noninformative priors to the\nmodel parameter, namely the Berger and Bernardo reference prior and the\nJeffreys prior, whose analytical expressions are obtained under weak\ndistributional assumptions. It is shown that the only condition needed for the\nposterior to be proper is that the sample size is larger than the dimension of\nthe data-generating model, independently of the class of elliptically contoured\ndistributions used in the definition of the generalized multivariate random\neffects model. The theoretical findings of the paper are applied to real data\nconsisting of ten studies about the effectiveness of hypertension treatment for\nreducing blood pressure where the treatment effects on both the systolic blood\npressure and diastolic blood pressure are investigated.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:19:49 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Bodnar", "Olha", ""], ["Bodnar", "Taras", ""]]}, {"id": "2104.02126", "submitter": "Qingyan Xiang", "authors": "Qingyan Xiang (1), Ronald J. Bosch (2), Judith J. Lok (3) ((1)\n  Department of Biostatistics, Boston University, (2) Center for Biostatistics\n  in AIDS Research, Harvard T.H. Chan School of Public Health, (3) Department\n  of Mathematics and Statistics, Boston University)", "title": "The survival-incorporated median versus the median in the survivors or\n  in the always-survivors: What are we measuring? And why?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many clinical studies evaluate the benefit of treatment based on both\nsurvival and other ordinal/continuous clinical outcomes, such as neurocognitive\nscores or quality-of-life scores. In these studies, there are situations when\nthe clinical outcomes are truncated by death, where subjects die before their\nclinical outcome is measured. Treating outcomes as \"missing\" or \"censored\" due\nto death can be misleading for treatment effect evaluation. We show that if we\nuse the median in the survivors or in the always-survivors to summarize\nclinical outcomes, we may conclude a trade-off exists between the probability\nof survival and good clinical outcomes, even in settings where both the\nprobability of survival and the probability of any good clinical outcome are\nbetter for one treatment. Therefore, we advocate not always treating death as a\nmechanism through which clinical outcomes are missing, but rather as part of\nthe outcome measure. To account for the survival status, we describe the\nsurvival-incorporated median as an alternative summary measure for outcomes in\nthe presence of death. The survival-incorporated median is the threshold such\nthat 50\\% of the population is alive with an outcome above that threshold. We\nuse conceptual examples to show that the survival-incorporated median provides\na simple and useful summary measure to inform clinical practice.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 19:50:17 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 19:44:33 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xiang", "Qingyan", ""], ["Bosch", "Ronald J.", ""], ["Lok", "Judith J.", ""]]}, {"id": "2104.02134", "submitter": "Mattias Villani", "authors": "Mattias Villani, Matias Quiroz, Robert Kohn and Robert Salomone", "title": "Spectral Subsampling MCMC for Stationary Multivariate Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral subsampling MCMC was recently proposed to speed up Markov chain\nMonte Carlo (MCMC) for long stationary univariate time series by subsampling\nperiodogram observations in the frequency domain. This article extends the\napproach to stationary multivariate time series. It also proposes a\nmultivariate generalisation of the autoregressive tempered fractionally\ndifferentiated moving average model (ARTFIMA) and establishes some of its\nproperties. The new model is shown to provide a better fit compared to\nmultivariate autoregressive moving average models for three real world\nexamples. We demonstrate that spectral subsampling may provide up to two orders\nof magnitude faster estimation, while retaining MCMC sampling efficiency and\naccuracy, compared to spectral methods using the full dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 20:20:59 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Villani", "Mattias", ""], ["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Salomone", "Robert", ""]]}, {"id": "2104.02143", "submitter": "Chenchen Ma", "authors": "Chenchen Ma and Gongjun Xu", "title": "Learning Latent and Hierarchical Structures in Cognitive Diagnosis\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive Diagnosis Models (CDMs) are a special family of discrete latent\nvariable models that are widely used in modern educational, psychological,\nsocial and biological sciences. A key component of CDMs is a binary $Q$-matrix\ncharacterizing the dependence structure between the items and the latent\nattributes. Additionally, researchers also assume in many applications certain\nhierarchical structures among the latent attributes to characterize their\ndependence. In most CDM applications, the attribute-attribute hierarchical\nstructures, the item-attribute $Q$-matrix, the item-level diagnostic model, as\nwell as the number of latent attributes, need to be fully or partially\npre-specified, which however may be subjective and misspecified as noted by\nmany recent studies. This paper considers the problem of jointly learning these\nlatent and hierarchical structures in CDMs from observed data with minimal\nmodel assumptions. Specifically, a penalized likelihood approach is proposed to\nselect the number of attributes and estimate the latent and hierarchical\nstructures simultaneously. An efficient expectation-maximization (EM) algorithm\nand a latent structure recovery algorithm are developed, and statistical\nconsistency theory is also established under mild conditions. The good\nperformance of the proposed method is illustrated by simulation studies and a\nreal data application in educational assessment.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 20:33:02 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ma", "Chenchen", ""], ["Xu", "Gongjun", ""]]}, {"id": "2104.02210", "submitter": "Bowen Yi", "authors": "Alexey Bobtsov, Bowen Yi, Romeo Ortega, Alessandro Astolfi", "title": "Generation of new exciting regressors for consistent on-line estimation\n  of unknown parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of parameter estimation from a standard vector linear regression\nequation in the absence of sufficient excitation in the regressor is addressed.\nThe first step to solve the problem consists in transforming this equation into\na set of scalar ones using the well-known dynamic regressor extension and\nmixing technique. Then a novel procedure to generate new scalar exciting\nregressors is proposed.} The superior performance of a classical gradient\nestimator using this new regressor, instead of the original one, is illustrated\nwith comprehensive simulations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 00:59:32 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 13:03:22 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bobtsov", "Alexey", ""], ["Yi", "Bowen", ""], ["Ortega", "Romeo", ""], ["Astolfi", "Alessandro", ""]]}, {"id": "2104.02289", "submitter": "Prateek Bansal", "authors": "Krishna Murthy Gurumurthy, Zili Li, Kara M. Kockelman, and Prateek\n  Bansal", "title": "Modelling Animal-Vehicle Collision Counts across Large Networks Using a\n  Bayesian Hierarchical Model with Time-Varying Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animal-vehicle collisions (AVCs) are common around the world and result in\nconsiderable loss of animal and human life, as well as significant property\ndamage and regular insurance claims. Understanding their occurrence in relation\nto various contributing factors and being able to identify locations of high\nrisk are valuable to AVC prevention, yielding economic, social and\nenvironmental cost savings. However, many challenges exist in the study of AVC\ndatasets. These include seasonality of animal activity, unknown exposure (i.e.,\nthe number of animal crossings), very low AVC counts across most sections of\nextensive roadway networks, and computational burdens that come with discrete\nresponse analysis using large datasets. To overcome these challenges, a\nBayesian hierarchical model is proposed where the exposure is modeled with\nnonparametric Dirichlet process, and the number of segment-level AVCs is\nassumed to follow a Binomial distribution. A P\\'olya-Gamma augmented Gibbs\nsampler is derived to estimate the proposed model. By using the AVC data of\nmultiple years across about 100,000 segments of state-controlled highways in\nTexas, U.S., it is demonstrated that the model is scalable to large datasets,\nwith a preponderance of zeros and clear monthly seasonality in counts, while\nidentifying high-risk locations (for application of design treatments, like\nseparated animal crossings with fencing) and key explanatory factors based on\nsegment-specific factors (such as changes in speed limit) can be done within\nthe modelling framework, which provide useful information for policy-making\npurposes.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 05:05:11 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 02:53:25 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gurumurthy", "Krishna Murthy", ""], ["Li", "Zili", ""], ["Kockelman", "Kara M.", ""], ["Bansal", "Prateek", ""]]}, {"id": "2104.02320", "submitter": "Lei Shi", "authors": "Lei Shi, Chen Shen, Libin Jin, Qi Shi, Zhen Wang, Marko Jusup and\n  Stefano Boccaletti", "title": "Inferring Network Structures via Signal Lasso", "comments": "11 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the connectivity structure of networked systems from data is an\nextremely important task in many areas of science. Most of real-world networks\nexhibit sparsely connected topologies, with links between nodes that in some\ncases may be even associated to a binary state (0 or 1, denoting respectively\nthe absence or the existence of a connection). Such un-weighted topologies are\nelusive to classical reconstruction methods such as Lasso or Compressed Sensing\ntechniques. We here introduce a novel approach called signal Lasso, where the\nestimation of the signal parameter is subjected to 0 or 1 values. The\ntheoretical properties and algorithm of proposed method are studied in detail.\nApplications of the method are illustrated to an evolutionary game and\nsynchronization dynamics in several synthetic and empirical networks, where we\nshow that the novel strategy is reliable and robust, and outperform the\nclassical approaches in terms of accuracy and mean square errors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 06:51:50 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Shi", "Lei", ""], ["Shen", "Chen", ""], ["Jin", "Libin", ""], ["Shi", "Qi", ""], ["Wang", "Zhen", ""], ["Jusup", "Marko", ""], ["Boccaletti", "Stefano", ""]]}, {"id": "2104.02383", "submitter": "Augustin Kelava", "authors": "Augustin Kelava, Pascal Kilian, Judith Glaesser, Samuel Merk, Holger\n  Brandt", "title": "Forecasting intra-individual changes of affective states taking into\n  account inter-individual differences using intensive longitudinal data from a\n  university student drop out study in math", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The longitudinal process that leads to university student drop out in STEM\nsubjects can be described by referring to a) inter-individual differences\n(e.g., cognitive abilities) as well as b) intra-individual changes (e.g.,\naffective states), c) (unobserved) heterogeneity of trajectories, and d)\ntime-dependent variables. Large dynamic latent variable model frameworks for\nintensive longitudinal data (ILD) have been proposed which are (partially)\ncapable of simultaneously separating the complex data structures (e.g., DLCA;\nAsparouhov, Hamaker, & Muth\\'en, 2017; DSEM; Asparouhov, Hamaker, & Muth\\'en,\n2018; NDLC-SEM, Kelava & Brandt, 2019). From a methodological perspective,\nforecasting in dynamic frameworks allowing for real-time inferences on latent\nor observed variables based on ongoing data collection has not been an\nextensive research topic. From a practical perspective, there has been no\nempirical study on student drop out in math that integrates ILD, dynamic\nframeworks, and forecasting of critical states of the individuals allowing for\nreal-time interventions. In this paper, we show how Bayesian forecasting of\nmultivariate intra-individual variables and time-dependent class membership of\nindividuals (affective states) can be performed in these dynamic frameworks. To\nillustrate our approach, we use an empirical example where we apply forecasting\nmethodology to ILD from a large university student drop out study in math with\nmultivariate observations collected over 50 measurement occasions from multiple\nstudents (N = 122). More specifically, we forecast emotions and behavior\nrelated to drop out. This allows us to model (i) just-in-time interventions,\n(ii) detection of heterogeneity in trajectories, and (iii) prediction of\nemerging dynamic states (e.g. critical stress levels or pre-decisional states).\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 09:19:42 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kelava", "Augustin", ""], ["Kilian", "Pascal", ""], ["Glaesser", "Judith", ""], ["Merk", "Samuel", ""], ["Brandt", "Holger", ""]]}, {"id": "2104.02419", "submitter": "Magnus M\\\"unch", "authors": "Magnus M. M\\\"unch, Mark A. van de Wiel, Aad W. van der Vaart and Carel\n  F.W. Peeters", "title": "Semi-supervised empirical Bayes group-regularized factor regression", "comments": "19 pages, 5 figures, submitted to Biometrical Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The features in high dimensional biomedical prediction problems are often\nwell described with lower dimensional manifolds. An example is genes that are\norganised in smaller functional networks. The outcome can then be described\nwith the factor regression model. A benefit of the factor model is that is\nallows for straightforward inclusion of unlabeled observations in the\nestimation of the model, i.e., semi-supervised learning. In addition, the high\ndimensional features in biomedical prediction problems are often well\ncharacterised. Examples are genes, for which annotation is available, and\nmetabolites with $p$-values from a previous study available. In this paper, the\nextra information on the features is included in the prior model for the\nfeatures. The extra information is weighted and included in the estimation\nthrough empirical Bayes, with Variational approximations to speed up the\ncomputation. The method is demonstrated in simulations and two applications.\nOne application considers influenza vaccine efficacy prediction based on\nmicroarray data. The second application predictions oral cancer metastatsis\nfrom RNAseq data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 10:52:01 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["M\u00fcnch", "Magnus M.", ""], ["van de Wiel", "Mark A.", ""], ["van der Vaart", "Aad W.", ""], ["Peeters", "Carel F. W.", ""]]}, {"id": "2104.02422", "submitter": "Matteo Farn\\`e Dr.", "authors": "Matteo Farn\\`e and Angela Montanari", "title": "Large factor model estimation by nuclear norm plus $l_1$ norm\n  penalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides a comprehensive estimation framework via nuclear norm\nplus $l_1$ norm penalization for high-dimensional approximate factor models\nwith a sparse residual covariance. The underlying assumptions allow for\nnon-pervasive latent eigenvalues and a prominent residual covariance pattern.\nIn that context, existing approaches based on principal components may lead to\nmisestimate the latent rank, due to the numerical instability of sample\neigenvalues. On the contrary, the proposed optimization problem retrieves the\nlatent covariance structure and exactly recovers the latent rank and the\nresidual sparsity pattern. Conditioning on them, the asymptotic rates of the\nsubsequent ordinary least squares estimates of loadings and factor scores are\nprovided, the recovered latent eigenvalues are shown to be maximally\nconcentrated and the estimates of factor scores via Bartlett's and Thompson's\nmethods are proved to be the most precise given the data. The validity of\noutlined results is highlighted in an exhaustive simulation study and in a real\nfinancial data example.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 10:56:09 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Farn\u00e8", "Matteo", ""], ["Montanari", "Angela", ""]]}, {"id": "2104.02444", "submitter": "Alberto Caimo", "authors": "Alberto Caimo and Lampros Bouranis and Robert Krause and Nial Friel", "title": "Statistical Network Analysis with Bergm", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in computational methods for intractable models have made\nnetwork data increasingly amenable to statistical analysis. Exponential random\ngraph models (ERGMs) emerged as one of the main families of models capable of\ncapturing the complex dependence structure of network data in a wide range of\napplied contexts. The Bergm package for R has become a popular package to carry\nout Bayesian parameter inference, missing data imputation, model selection and\ngoodness-of-fit diagnostics for ERGMs. Over the last few years, the package has\nbeen considerably improved in terms of efficiency by adopting some of the\nstate-of-the-art Bayesian computational methods for doubly-intractable\ndistributions. Recently, version 5 of the package has been made available on\nCRAN having undergone a substantial makeover, which has made it more accessible\nand easy to use for practitioners. New functions include data augmentation\nprocedures based on the approximate exchange algorithm for dealing with missing\ndata, adjusted pseudo-likelihood and pseudo-posterior procedures, which allow\nfor fast approximate inference of the ERGM parameter posterior and model\nevidence for networks on several thousands nodes.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 11:59:53 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Caimo", "Alberto", ""], ["Bouranis", "Lampros", ""], ["Krause", "Robert", ""], ["Friel", "Nial", ""]]}, {"id": "2104.02456", "submitter": "Tomoya Wakayama", "authors": "Tomoya Wakayama, Shonosuke Sugasawa", "title": "Locally Adaptive Smoothing for Functional Data", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite increasing accessibility to function data, effective methods for\nflexibly estimating underlying trend structures are still scarce. We thereby\ndevelop locally adaptive smoothing methods for both functional time series and\nspatial data by extending trend filtering, a powerful nonparametric trend\nestimation technique for scalar data. We formulate the functional version of\ntrend filtering by introducing $L_2$-norm of the differences of adjacent trend\nfunctions. Through orthonormal basis expansion, we simplify the objective\nfunction to squared loss for coefficient vectors with grouped fused lasso\npenalty, and develop an efficient iteration algorithm for optimization. The\ntuning parameter in the proposed method is selected via cross validation. We\nalso consider an extension of the proposed algorithm to spatial functional\ndata. The proposed methods are demonstrated by simulation studies and an\napplication to two real world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 12:23:13 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 13:40:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wakayama", "Tomoya", ""], ["Sugasawa", "Shonosuke", ""]]}, {"id": "2104.02507", "submitter": "Subhodh Kotekal", "authors": "Subhodh Kotekal", "title": "Statistical Limits of Sparse Mixture Detection", "comments": "70 pages; minor typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting a general sparse mixture and obtain an\nexplicit characterization of the phase transition under some conditions,\ngeneralizing the univariate results of Cai and Wu. Additionally, we provide a\nsufficient condition for the adaptive optimality of a Higher Criticism type\ntesting statistic formulated by Gao and Ma. In the course of establishing these\nresults, we offer a unified perspective through the large deviations theory.\nThe phase transition and adaptive optimality we establish are direct\nconsequences of the large deviation principle of the normalized log-likelihood\nratios between the null and the signal distributions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 13:39:16 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 23:15:15 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 00:43:50 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Kotekal", "Subhodh", ""]]}, {"id": "2104.02640", "submitter": "TrungTin Nguyen", "authors": "TrungTin Nguyen, Hien Duy Nguyen, Faicel Chamroukhi and Florence\n  Forbes", "title": "A non-asymptotic penalization criterion for model selection in mixture\n  of experts models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of experts (MoE) is a popular class of models in statistics and\nmachine learning that has sustained attention over the years, due to its\nflexibility and effectiveness. We consider the Gaussian-gated localized MoE\n(GLoME) regression model for modeling heterogeneous data. This model poses\nchallenging questions with respect to the statistical estimation and model\nselection problems, including feature selection, both from the computational\nand theoretical points of view. We study the problem of estimating the number\nof components of the GLoME model, in a penalized maximum likelihood estimation\nframework. We provide a lower bound on the penalty that ensures a weak oracle\ninequality is satisfied by our estimator. To support our theoretical result, we\nperform numerical experiments on simulated and real data, which illustrate the\nperformance of our finite-sample oracle inequality.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:24:55 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Nguyen", "TrungTin", ""], ["Nguyen", "Hien Duy", ""], ["Chamroukhi", "Faicel", ""], ["Forbes", "Florence", ""]]}, {"id": "2104.02665", "submitter": "Michelle Zhou", "authors": "Qian M. Zhou, Xuan Wang, Yingye Zheng, and Tianxi Cai", "title": "A new weighting method when not all the events are selected as cases in\n  a nested case-control study", "comments": "27 pages,3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested case-control (NCC) is a sampling method widely used for developing and\nevaluating risk models with expensive biomarkers on large prospective cohort\nstudies. The biomarker values are typically obtained on a sub-cohort,\nconsisting of all the events and a subset of non-events. However, when the\nnumber of events is not small, it might not be affordable to measure the\nbiomarkers on all of them. Due to the costs and limited availability of\nbio-specimens, only a subset of events is selected to the sub-cohort as cases.\nFor these \"untypical\" NCC studies, we propose a new weighting method for the\ninverse probability weighted (IPW) estimation. We also design a perturbation\nmethod to estimate the variance of the IPW estimator with our new weights. It\naccounts for between-subject correlations induced by the sampling processes for\nboth cases and controls through perturbing their sampling indicator variables,\nand thus, captures all the variations. Furthermore, we demonstrate,\nanalytically and numerically, that when cases consist of only a subset of\nevents, our new weight produces more efficient IPW estimators than the weight\nproposed in Samuelsen (1997) for a standard NCC design. We illustrate the\nestimating procedure with a study that aims to evaluate a biomarker-based risk\nprediction model using the Framingham cohort study.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:53:59 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhou", "Qian M.", ""], ["Wang", "Xuan", ""], ["Zheng", "Yingye", ""], ["Cai", "Tianxi", ""]]}, {"id": "2104.02698", "submitter": "Anindya Roy", "authors": "Anindya Roy and Tucker S. McElroy", "title": "Constrained Parameterization of Reduced Rank and Co-integrated Vector\n  Autoregression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper provides a parametrization of Vector Autoregression (VAR) that\nenables one to look at the parameters associated with unit root dynamics and\nthose associated with stable dynamics separately. The task is achieved via a\nnovel factorization of the VAR polynomial that partitions the polynomial\nspectrum into unit root and stable and zero roots via polynomial factors. The\nproposed factorization adds to the literature of spectral factorization of\nmatrix polynomials. The main benefit is that using the parameterization,\nactions could be taken to model the dynamics due to a particular class of\nroots, e.g. unit roots or zero roots, without changing the properties of the\ndynamics due to other roots. For example, using the parameterization one is\nable to estimate cointegrating space with appropriate rank that maintains the\nroot structure of the original VAR processes or one can estimate a reduced rank\ncausal VAR process maintaining the constraints of causality. In essence, this\nparameterization provides the practitioner an option to perform estimation of\nVAR processes with constrained root structure (e.g., conintegrated VAR or\nreduced rank VAR) such that the estimated model maintains the assumed root\nstructure.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:46:40 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Roy", "Anindya", ""], ["McElroy", "Tucker S.", ""]]}, {"id": "2104.02717", "submitter": "Jonathan P Williams", "authors": "Jonathan P Williams", "title": "Discussion of \"A Gibbs sampler for a class of random convex polytopes\"", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An exciting new algorithmic breakthrough has been advanced for how to carry\nout inferences in a Dempster-Shafer (DS) formulation of a categorical data\ngenerating model. The developed sampling mechanism, which draws on theory for\ndirected graphs, is a clever and remarkable achievement, as this has been an\nopen problem for many decades. In this discussion, I comment on important\ncontributions, central questions, and prevailing matters of the article.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:27:36 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Williams", "Jonathan P", ""]]}, {"id": "2104.02769", "submitter": "Liangyuan Hu", "authors": "Liangyuan Hu and Jung-Yi Joyce Lin and Jiayi Ji", "title": "Variable selection with missing data in both covariates and outcomes:\n  Imputation and machine learning", "comments": "29 pages, 17 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The missing data issue is ubiquitous in health studies. Variable selection in\nthe presence of both missing covariates and outcomes is an important\nstatistical research topic but has been less studied. Existing literature\nfocuses on parametric regression techniques that provide direct parameter\nestimates of the regression model. Flexible nonparametric machine learning\nmethods considerably mitigate the reliance on the parametric assumptions, but\ndo not provide as naturally defined variable importance measure as the\ncovariate effect native to parametric models. We investigate a general variable\nselection approach when both the covariates and outcomes can be missing at\nrandom and have general missing data patterns. This approach exploits the\nflexibility of machine learning modeling techniques and bootstrap imputation,\nwhich is amenable to nonparametric methods in which the covariate effects are\nnot directly available. We conduct expansive simulations investigating the\npractical operating characteristics of the proposed variable selection\napproach, when combined with four tree-based machine learning methods, XGBoost,\nRandom Forests, Bayesian Additive Regression Trees (BART) and Conditional\nRandom Forests, and two commonly used parametric methods, lasso and backward\nstepwise selection. Numeric results suggest that when combined with bootstrap\nimputation, XGBoost and BART have the overall best variable selection\nperformance with respect to the $F_1$ score and Type I error across various\nsettings. In general, there is no significant difference in the variable\nselection performance due to imputation methods. We further demonstrate the\nmethods via a case study of risk factors for 3-year incidence of metabolic\nsyndrome with data from the Study of Women's Health Across the Nation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 20:18:29 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 21:02:21 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Hu", "Liangyuan", ""], ["Lin", "Jung-Yi Joyce", ""], ["Ji", "Jiayi", ""]]}, {"id": "2104.02810", "submitter": "Michael Weylandt", "authors": "Michael Weylandt and George Michailidis and T. Mitchell Roddenberry", "title": "Sparse Partial Least Squares for Coarse Noisy Graph Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI eess.SP stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Graph signal processing (GSP) provides a powerful framework for analyzing\nsignals arising in a variety of domains. In many applications of GSP, multiple\nnetwork structures are available, each of which captures different aspects of\nthe same underlying phenomenon. To integrate these different data sources,\ngraph alignment techniques attempt to find the best correspondence between\nvertices of two graphs. We consider a generalization of this problem, where\nthere is no natural one-to-one mapping between vertices, but where there is\ncorrespondence between the community structures of each graph. Because we seek\nto learn structure at this higher community level, we refer to this problem as\n\"coarse\" graph alignment. To this end, we propose a novel regularized partial\nleast squares method which both incorporates the observed graph structures and\nimposes sparsity in order to reflect the underlying block community structure.\nWe provide efficient algorithms for our method and demonstrate its\neffectiveness in simulations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:52:15 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Weylandt", "Michael", ""], ["Michailidis", "George", ""], ["Roddenberry", "T. Mitchell", ""]]}, {"id": "2104.02827", "submitter": "Matthew Singh", "authors": "Matthew F. Singh, Chong Wang, Michael W. Cole, and ShiNung Ching", "title": "Efficient state and parameter estimation for high-dimensional nonlinear\n  system identification with application to MEG brain network modeling", "comments": "Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY q-bio.NC q-bio.QM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  System identification poses a significant bottleneck to characterizing and\ncontrolling complex systems. This challenge is greatest when both the system\nstates and parameters are not directly accessible leading to a dual-estimation\nproblem. Current approaches to such problems are limited in their ability to\nscale with many-parameter systems as often occurs in networks. In the current\nwork, we present a new, computationally efficient approach to treat large\ndual-estimation problems. Our approach consists of directly integrating\npseudo-optimal state estimation (the Extended Kalman Filter) into a\ndual-optimization objective, leaving a differentiable cost/error function of\nonly in terms of the unknown system parameters which we solve using numerical\ngradient/Hessian methods. Intuitively, our approach consists of solving for the\nparameters that generate the most accurate state estimator (Extended Kalman\nFilter). We demonstrate that our approach is at least as accurate in state and\nparameter estimation as joint Kalman Filters (Extended/Unscented), despite\nlower complexity. We demonstrate the utility of our approach by inverting\nanatomically-detailed individualized brain models from human\nmagnetoencephalography (MEG) data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 23:25:26 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Singh", "Matthew F.", ""], ["Wang", "Chong", ""], ["Cole", "Michael W.", ""], ["Ching", "ShiNung", ""]]}, {"id": "2104.02888", "submitter": "Daniel Ahfock", "authors": "Daniel Ahfock, Saumyadipta Pyne, Geoffrey J. McLachlan", "title": "Data-fusion using factor analysis and low-rank matrix completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Data-fusion involves the integration of multiple related datasets. The\nstatistical file-matching problem is a canonical data-fusion problem in\nmultivariate analysis, where the objective is to characterise the joint\ndistribution of a set of variables when only strict subsets of marginal\ndistributions have been observed. Estimation of the covariance matrix of the\nfull set of variables is challenging given the missing-data pattern. Factor\nanalysis models use lower-dimensional latent variables in the data-generating\nprocess, and this introduces low-rank components in the complete-data matrix\nand the population covariance matrix. The low-rank structure of the factor\nanalysis model can be exploited to estimate the full covariance matrix from\nincomplete data via low-rank matrix completion. We prove the identifiability of\nthe factor analysis model in the statistical file-matching problem under\nconditions on the number of factors and the number of shared variables over the\nobserved marginal subsets. Additionally, we provide an EM algorithm for\nparameter estimation. On several real datasets, the factor model gives smaller\nreconstruction errors in file-matching problems than the common approaches for\nlow-rank matrix completion.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 03:25:23 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Ahfock", "Daniel", ""], ["Pyne", "Saumyadipta", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "2104.03083", "submitter": "Alessandro Casa", "authors": "Alessandro Casa, Charles Bouveyron, Elena Erosheva, Giovanna Menardi", "title": "Co-clustering of time-dependent data via Shape Invariant Model", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multivariate time-dependent data, where multiple features are observed over\ntime for a set of individuals, are increasingly widespread in many application\ndomains. To model these data we need to account for relations among both time\ninstants and variables and, at the same time, for subjects heterogeneity. We\npropose a new co-clustering methodology for clustering individuals and\nvariables simultaneously that is designed to handle both functional and\nlongitudinal data. Our approach borrows some concepts from the curve\nregistration framework by embedding the Shape Invariant Model in the Latent\nBlock Model, estimated via a suitable modification of the SEM-Gibbs algorithm.\nThe resulting procedure allows for several user-defined specifications of the\nnotion of cluster that could be chosen on substantive grounds and provides\nparsimonious summaries of complex longitudinal or functional data by\npartitioning data matrices into homogeneous blocks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 12:10:01 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Casa", "Alessandro", ""], ["Bouveyron", "Charles", ""], ["Erosheva", "Elena", ""], ["Menardi", "Giovanna", ""]]}, {"id": "2104.03087", "submitter": "Xiaoyu Hu", "authors": "Xiaoyu Hu and Fang Yao", "title": "Dynamic Principal Subspaces in High Dimensions", "comments": "33 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Principal component analysis (PCA) is a versatile tool to reduce the\ndimensionality which has wide applications in statistics and machine learning\ncommunity. It is particularly useful to model data in high-dimensional\nscenarios where the number of variables $p$ is comparable to, or much larger\nthan the sample size $n$. Despite extensive literature on this topic,\nresearchers have focused on modeling static principal eigenvectors or\nsubspaces, which are not suitable for stochastic processes that are dynamic in\nnature. To characterize the change in the whole course of high-dimensional data\ncollection, we propose a unified framework to directly estimate dynamic\nprincipal subspaces spanned by leading eigenvectors of covariance matrices. We\nformulate an optimization problem by combining the local linear smoothing and\nregularization penalty together with the orthogonality constraint, which can be\neffectively solved by the proximal gradient method for manifold optimization.\nWe show that our method is suitable for high-dimensional data observed under\nboth common and irregular designs. In addition, theoretical properties of the\nestimators are investigated under $l_q (0 \\leq q \\leq 1)$ sparsity. Extensive\nexperiments demonstrate the effectiveness of the proposed method in both\nsimulated and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 12:23:02 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 14:16:19 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Hu", "Xiaoyu", ""], ["Yao", "Fang", ""]]}, {"id": "2104.03088", "submitter": "Lewis Crawford Mr", "authors": "Stephane Doyen, Hugh Taylor, Peter Nicholas, Lewis Crawford, Isabella\n  Young, Michael Sughrue", "title": "Hollow-tree Super: a directional and scalable approach for feature\n  importance in boosted tree models", "comments": "28 pages, 1 table, 7 figures, PDF format - Submitted to PLOSONE\n  pending review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current limitations in boosted tree modelling prevent the effective scaling\nto datasets with a large feature number, particularly when investigating the\nmagnitude and directionality of various features on classification. We present\na novel methodology, Hollow-tree Super (HOTS), to resolve and visualize feature\nimportance in boosted tree models involving a large number of features.\nFurther, HOTS allows for investigation of the directionality and magnitude\nvarious features have on classification. Using the Iris dataset, we first\ncompare HOTS to Gini Importance, Partial Dependence Plots, and Permutation\nImportance, and demonstrate how HOTS resolves the weaknesses present in these\nmethods. We then show how HOTS can be utilized in high dimensional\nneuroscientific data, by taking 60 Schizophrenic subjects and applying the\nmethod to determine which brain regions were most important for classification\nof schizophrenia as determined by the PANSS. HOTS effectively replicated and\nsupported the findings of Gini importance, Partial Dependence Plots and\nPermutation importance within the Iris dataset. When applied to the\nschizophrenic brain dataset, HOTS was able to resolve the top 10 most important\nfeatures for classification, as well as their directionality for classification\nand magnitude compared to other features. Cross-validation supported that these\nsame 10 features were consistently used in the decision-making process across\nmultiple trees, and these features were localised primarily to the occipital\nand parietal cortices, commonly disturbed brain regions in those with\nSchizophrenia. It is imperative that a methodology is developed that is able to\nhandle the demands of working with large datasets that contain a large number\nof features. HOTS represents a unique way to investigate both the\ndirectionality and magnitude of feature importance when working at scale with\nboosted-tree modelling.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 12:24:56 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Doyen", "Stephane", ""], ["Taylor", "Hugh", ""], ["Nicholas", "Peter", ""], ["Crawford", "Lewis", ""], ["Young", "Isabella", ""], ["Sughrue", "Michael", ""]]}, {"id": "2104.03167", "submitter": "Sonja Petrovic", "authors": "Elizabeth Gross, Sonja Petrovi\\'c, Despina Stasi", "title": "Random graphs with node and block effects: models, goodness-of-fit\n  tests, and applications to biological networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many popular models from the networks literature can be viewed through a\ncommon lens. We describe it here and call the class of models log-linear ERGMs.\nIt includes degree-based models, stochastic blockmodels, and combinations of\nthese. Given the interest in combined node and block effects in network\nformation mechanisms, we introduce a general directed relative of the\ndegree-corrected stochastic blockmodel: an exponential family model we call\n$p_1$-SBM. It is a generalization of several well-known variants of the\nblockmodel.\n  We study the problem of testing model fit for the log-linear ERGM class.\n  The model fitting approach we take, through the use of quick estimation\nalgorithms borrowed from the contingency table literature and effective\nsampling methods rooted in graph theory and algebraic statistics, results in an\nexact test whose $p$-value can be approximated efficiently in networks of\nmoderate sizes.\n  We showcase the performance of the method on two data sets from biology: the\nconnectome of \\emph{C. elegans} and the interactome of \\emph{Arabidopsis\nthaliana}. These two networks, a neuronal network and a protein-protein\ninteraction network, have been popular examples in the network science\nliterature, but a model-based approach to studying them has been missing thus\nfar.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:56:36 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Gross", "Elizabeth", ""], ["Petrovi\u0107", "Sonja", ""], ["Stasi", "Despina", ""]]}, {"id": "2104.03193", "submitter": "Mai Ngoc Bui Miss", "authors": "Mai Ngoc Bui, Yvo Pokern and Petros Dellaportas", "title": "Inference for partially observed Riemannian Ornstein--Uhlenbeck\n  diffusions of covariance matrices", "comments": "35 pages, 13 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a generalization of the Ornstein--Uhlenbeck processes on the\ncone of covariance matrices endowed with the Log-Euclidean and the\nAffine-Invariant metrics. Our development exploits the Riemannian geometric\nstructure of symmetric positive definite matrices viewed as a differential\nmanifold. We then provide Bayesian inference for discretely observed diffusion\nprocesses of covariance matrices based on an MCMC algorithm built with the help\nof a novel diffusion bridge sampler accounting for the geometric structure. Our\nproposed algorithm is illustrated with a real data financial application.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:34:26 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Bui", "Mai Ngoc", ""], ["Pokern", "Yvo", ""], ["Dellaportas", "Petros", ""]]}, {"id": "2104.03194", "submitter": "Anna Gottard", "authors": "Anna Gottard and Agnese Panzera", "title": "Graphical models for circular variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are a key class of probabilistic models for studying the\nconditional independence structure of a set of random variables. Circular\nvariables are special variables, characterized by periodicity, arising in\nseveral contexts and fields. However, models for studying the\ndependence/independence structure of circular variables are under-explored.\nThis paper analyses three multivariate circular distributions, the von Mises,\nthe Wrapped Normal and the Inverse Stereographic distributions, focusing on\ntheir properties concerning conditional independence. For each one of these\ndistributions, we discuss the main properties related to conditional\nindependence and introduce suitable classes of graphical models. The usefulness\nof the proposed models is shown by modelling the conditional independence among\ndihedral angles characterizing the three-dimensional structure of some\nproteins.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:36:39 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Gottard", "Anna", ""], ["Panzera", "Agnese", ""]]}, {"id": "2104.03200", "submitter": "Friedrich Teuscher Dr.", "authors": "Friedrich Teuscher", "title": "The quantification of Simpsons paradox and other contributions to\n  contingency table theory", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of contingency tables is a powerful statistical tool used in\nexperiments with categorical variables. This study improves parts of the theory\nunderlying the use of contingency tables. Specifically, the linkage\ndisequilibrium parameter as a measure of two-way interactions applied to\nthree-way tables makes it possible to quantify Simpsons paradox by a simple\nformula. With tests on three-way interactions, there is only one that\ndetermines whether the partial interactions of all variables agree or whether\nthere is at least one variable whose partial interactions disagree. To date,\nthere has been no test available that determines whether the partial\ninteractions of a certain variable agree or disagree, and the presented work\ncloses this gap. This work reveals the relation of the multiplicative and the\nadditive measure of a three-way interaction. Another contribution addresses the\nquestion of which cells in a contingency table are fixed when the first- and\nsecond-order marginal totals are given. The proposed procedure not only detects\nfixed zero counts but also fixed positive counts. This impacts the\ndetermination of the degrees of freedom. Furthermore, limitations of methods\nthat simulate contingency tables with given pairwise associations are\naddressed.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:42:21 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Teuscher", "Friedrich", ""]]}, {"id": "2104.03394", "submitter": "Osama Almalik", "authors": "Osama Almalik and Edwin R. van den Heuvel", "title": "A bivariate likelihood approach for estimation of a pooled continuous\n  effect size from a heteroscedastic meta-analysis study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The DerSimonian-Laird (DL) weighted average method has been widely used for\nestimation of a pooled effect size from an aggregated data meta-analysis study.\nIt is mainly criticized for its underestimation of the standard error of the\npooled effect size in the presence of heterogeneous study effect sizes. The\nuncertainty in the estimation of the between-study variance is not accounted\nfor in the calculation of this standard error. Due to this negative property,\nmany alternative estimation approaches have been proposed in literature. One\napproach was developed by Hardy and Thompson (HT), who implemented a profile\nlikelihood approach instead of the moment-based approach of DL. Others have\nfurther extended the likelihood approach and proposed higher-order likelihood\ninferences (e.g., Bartlett-type corrections). Likelihood-based methods better\naddress the uncertainty in estimating the between-study variance than the DL\nmethod, but all these methods assume that the within-study standard deviations\nare known and equal to the observed standard error of the study effect sizes.\nHere we treat the observed standard errors as estimators for the within-study\nvariability and we propose a bivariate likelihood approach that jointly\nestimates the pooled effect size, the between-study variance, and the\npotentially heteroscedastic within-study variances. We study the performance of\nthe proposed method by means of simulation, and compare it to DL, HT, and the\nhigher-order likelihood methods. Our proposed approach appears to be less\nsensitive to the number of studies, and less biased in case of\nheteroscedasticity.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 21:01:18 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Almalik", "Osama", ""], ["Heuvel", "Edwin R. van den", ""]]}, {"id": "2104.03395", "submitter": "Michel Montoril PhD", "authors": "Michel H. Montoril, Leandro T. Correia, Helio S. Migon", "title": "Dynamic Weights in Gaussian Mixture Models: A Bayesian Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we consider a Gaussian mixture model where the mixture weight\nbehaves as an unknown function of time. To estimate the mixture weight\nfunction, we develop a Bayesian nonlinear dynamic approach for polynomial\nmodels. Two estimation methods that can be extended to other situations are\nconsidered. One of them, called here component-wise Metropolis-Hastings, is\nmore general and can be used for any situation where the observation and state\nequations are nonlinearly connected. The other method tends to be faster but\nmust be applied specifically to binary data (by using a probit link function).\nThis kind of Gaussian mixture model is capable of successfully capturing the\nfeatures of the data, as observed in numerical studies. It can be useful in\nstudies such as clustering, change-point and process control. We apply the\nproposed method an array Comparative Genomic Hybridization (aCGH) dataset from\nglioblastoma cancer studies, where we illustrate the ability of the new method\nto detect chromosome aberrations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 21:05:03 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 00:02:35 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 03:00:17 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Montoril", "Michel H.", ""], ["Correia", "Leandro T.", ""], ["Migon", "Helio S.", ""]]}, {"id": "2104.03397", "submitter": "Andrew McCormack", "authors": "Andrew McCormack and Peter Hoff", "title": "Equivariant Estimation of Fr\\'echet Means", "comments": "31 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fr\\'echet mean generalizes the concept of a mean to a metric space\nsetting. In this work we consider equivariant estimation of Fr\\'echet means for\nparametric models on metric spaces that are Riemannian manifolds. The geometry\nand symmetry of such a space is encoded by its isometry group. Estimators that\nare equivariant under the isometry group take into account the symmetry of the\nmetric space. For some models there exists an optimal equivariant estimator,\nwhich necessarily will perform as well or better than other common equivariant\nestimators, such as the maximum likelihood estimator or the sample Fr\\'echet\nmean. We derive the general form of this minimum risk equivariant estimator and\nin a few cases provide explicit expressions for it. In other models the\nisometry group is not large enough relative to the parametric family of\ndistributions for there to exist a minimum risk equivariant estimator. In such\ncases, we introduce an adaptive equivariant estimator that uses the data to\nselect a submodel for which there is an MRE. Simulations results show that the\nadaptive equivariant estimator performs favorably relative to alternative\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 21:06:06 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["McCormack", "Andrew", ""], ["Hoff", "Peter", ""]]}, {"id": "2104.03398", "submitter": "Dario Gasbarra", "authors": "Elja Arjas and Dario Gasbarra", "title": "Adaptive treatment allocation and selection in multi-arm clinical\n  trials: a Bayesian perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trials are an instrument for making informed decisions based on\nevidence from well-designed experiments. Here we consider adaptive designs\nmainly from the perspective of multi-arm Phase II clinical trials, in which one\nor more experimental treatments are compared to a control. Treatment allocation\nof individual trial participants is assumed to take place according to a fixed\nblock randomization, albeit with an important twist: The performance of each\ntreatment arm is assessed after every measured outcome, in terms of the\nposterior distribution of a corresponding model parameter. Different treatments\narms are then compared to each other, according to pre-defined criteria and\nusing the joint posterior as the basis for such assessment. If a treatment is\nfound to be sufficiently clearly inferior to the currently best candidate, it\ncan be closed off either temporarily or permanently from further participant\naccrual. The latter possibility provides a method for adaptive treatment\nselection, including early stopping of the trial. The main development in the\npaper is in terms of binary outcomes, but some extensions, notably for handling\ntime-to-event data, are discussed as well. The presentation is to a large\nextent comparative and expository.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 21:14:14 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Arjas", "Elja", ""], ["Gasbarra", "Dario", ""]]}, {"id": "2104.03436", "submitter": "David Frazier", "authors": "David T. Frazier, Christopher Drovandi, and David J. Nott", "title": "Synthetic Likelihood in Misspecified Models: Consequences and\n  Corrections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the behaviour of the synthetic likelihood (SL) method when the\nmodel generating the simulated data differs from the actual data generating\nprocess. One of the most common methods to obtain SL-based inferences is via\nthe Bayesian posterior distribution, with this method often referred to as\nBayesian synthetic likelihood (BSL). We demonstrate that when the model is\nmisspecified, the BSL posterior can be poorly behaved, placing significant\nposterior mass on values of the model parameters that do not represent the true\nfeatures observed in the data. Theoretical results demonstrate that in\nmisspecified models the BSL posterior can display a wide range of behaviours\ndepending on the level of model misspecification, including being\nasymptotically non-Gaussian. Our results suggest that a recently proposed\nrobust BSL approach can ameliorate this behavior and deliver reliable posterior\ninference under model misspecification. We document all theoretical results\nusing a simple running example.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 00:07:48 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Frazier", "David T.", ""], ["Drovandi", "Christopher", ""], ["Nott", "David J.", ""]]}, {"id": "2104.03446", "submitter": "Jiguo Cao", "authors": "Hua Liu, Jinhong You and Jiguo Cao", "title": "Functional L-Optimality Subsampling for Massive Data", "comments": "37 pages and 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Massive data bring the big challenges of memory and computation for analysis.\nThese challenges can be tackled by taking subsamples from the full data as a\nsurrogate. For functional data, it is common to collect multiple measurements\nover their domains, which require even more memory and computation time when\nthe sample size is large. The computation would be much more intensive when\nstatistical inference is required through bootstrap samples. To the best of our\nknowledge, this article is the first attempt to study the subsampling method\nfor the functional linear model. We propose an optimal subsampling method based\non the functional L-optimality criterion. When the response is a discrete or\ncategorical variable, we further extend our proposed functional L-optimality\nsubsampling (FLoS) method to the functional generalized linear model. We\nestablish the asymptotic properties of the estimators by the FLoS method. The\nfinite sample performance of our proposed FLoS method is investigated by\nextensive simulation studies. The FLoS method is further demonstrated by\nanalyzing two large-scale datasets: the global climate data and the kidney\ntransplant data. The analysis results on these data show that the FLoS method\nis much better than the uniform subsampling approach and can well approximate\nthe results based on the full data while dramatically reducing the computation\ntime and memory.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 00:41:35 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 04:20:48 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Hua", ""], ["You", "Jinhong", ""], ["Cao", "Jiguo", ""]]}, {"id": "2104.03464", "submitter": "Yufei Yi", "authors": "Yufei Yi, Matey Neykov", "title": "A New Perspective on Debiasing Linear Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an abstract procedure for debiasing constrained or\nregularized potentially high-dimensional linear models. It is elementary to\nshow that the proposed procedure can produce $\\frac{1}{\\sqrt{n}}$-confidence\nintervals for individual coordinates (or even bounded contrasts) in models with\nunknown covariance, provided that the covariance has bounded spectrum. While\nthe proof of the statistical guarantees of our procedure is simple, its\nimplementation requires more care due to the complexity of the optimization\nprograms we need to solve. We spend the bulk of this paper giving examples in\nwhich the proposed algorithm can be implemented in practice. One fairly general\nclass of instances which are amenable to applications of our procedure include\nconvex constrained least squares. We are able to translate the procedure to an\nabstract algorithm over this class of models, and we give concrete examples\nwhere efficient polynomial time methods for debiasing exist. Those include the\nconstrained version of LASSO, regression under monotone constraints, regression\nwith positive monotone constraints and non-negative least squares. In addition,\nwe show that our abstract procedure can be applied to efficiently debias SLOPE\nand square-root SLOPE, among other popular regularized procedures under certain\nassumptions. We provide thorough simulation results in support of our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 01:41:55 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Yi", "Yufei", ""], ["Neykov", "Matey", ""]]}, {"id": "2104.03802", "submitter": "Shuangning Li", "authors": "Yuchen Hu, Shuangning Li, Stefan Wager", "title": "Average Treatment Effects in the Presence of Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a definition for the average indirect effect of a binary treatment\nin the potential outcomes model for causal inference. Our definition is\nanalogous to the standard definition of the average direct effect, and can be\nexpressed without needing to compare outcomes across multiple randomized\nexperiments. We show that the proposed indirect effect satisfies a universal\ndecomposition theorem, whereby the sum of the average direct and indirect\neffects always corresponds to the average effect of a policy intervention. We\nalso consider a number of parametric models for interference considered by\napplied researchers, and find that our (non-parametrically defined) indirect\neffect remains a natural estimand when re-expressed in the context of these\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:28:16 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 09:11:13 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Hu", "Yuchen", ""], ["Li", "Shuangning", ""], ["Wager", "Stefan", ""]]}, {"id": "2104.03889", "submitter": "Lorenzo Pacchiardi", "authors": "Lorenzo Pacchiardi, Ritabrata Dutta", "title": "Generalized Bayesian Likelihood-Free Inference Using Scoring Rules\n  Estimators", "comments": "23 pages, plus references and appendices. 14 figures. Code available\n  at https://github.com/LoryPack/GenBayes_LikelihoodFree_ScoringRules. v3:\n  typos fixes and small additions. v2: added more theoretical and experimental\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for Bayesian Likelihood-Free Inference (LFI) based on\nGeneralized Bayesian Inference using scoring rules (SRs). SRs are used to\nevaluate probabilistic models given an observation; a proper SR is minimised in\nexpectation when the model corresponds to the data generating process for the\nobservations. Using a strictly proper SR, for which the above minimum is\nunique, ensures posterior consistency of our method. Further, we prove finite\nsample posterior consistency and outlier robustness of our posterior for the\nKernel and Energy Scores. As the likelihood function is intractable for LFI, we\nemploy consistent estimators of SRs using model simulations in a\npseudo-marginal MCMC; we show the target of such chain converges to the exact\nSR posterior by increasing the number of simulations. Furthermore, we note\npopular LFI techniques such as Bayesian Synthetic Likelihood (BSL) can be seen\nas special cases of our framework using only proper (but not strictly so) SR.\nWe empirically validate our consistency and outlier robustness results and show\nhow related approaches do not enjoy these properties. Practically, we use the\nEnergy and Kernel Scores, but our general framework sets the stage for\nextensions with other scoring rules.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:00:41 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 17:14:26 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 15:17:56 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Pacchiardi", "Lorenzo", ""], ["Dutta", "Ritabrata", ""]]}, {"id": "2104.03942", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Jukka Corander", "title": "Approximate Bayesian inference from noisy likelihoods with Gaussian\n  process emulated MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient approach for doing approximate Bayesian inference\nwhen only a limited number of noisy likelihood evaluations can be obtained due\nto computational constraints, which is becoming increasingly common for\napplications of complex models. Our main methodological innovation is to model\nthe log-likelihood function using a Gaussian process (GP) in a local fashion\nand apply this model to emulate the progression that an exact\nMetropolis-Hastings (MH) algorithm would take if it was applicable. New\nlog-likelihood evaluation locations are selected using sequential experimental\ndesign strategies such that each MH accept/reject decision is done within a\npre-specified error tolerance. The resulting approach is conceptually simple\nand sample-efficient as it takes full advantage of the GP model. It is also\nmore robust to violations of GP modelling assumptions and better suited for the\ntypical situation where the posterior is substantially more concentrated than\nthe prior, compared with various existing inference methods based on global GP\nsurrogate modelling. We discuss the probabilistic interpretations and central\ntheoretical aspects of our approach, and we then demonstrate the benefits of\nthe resulting algorithm in the context of likelihood-free inference for\nsimulator-based statistical models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:38:02 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Corander", "Jukka", ""]]}, {"id": "2104.04005", "submitter": "Amirhossein Karimi", "authors": "Amirhossein Karimi and Tryphon T. Georgiou", "title": "The Challenge of Small Data: Dynamic Mode Decomposition, Redux", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the setting and the assumptions that underlie the methodology of\nDynamic Mode Decomposition (DMD) in order to highlight caveats as well as\npotential measures of when the applicability is warranted.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:40:36 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 21:43:29 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Karimi", "Amirhossein", ""], ["Georgiou", "Tryphon T.", ""]]}, {"id": "2104.04157", "submitter": "Nicola Rennie", "authors": "Nicola Rennie, Catherine Cleophas, Adam M. Sykulski, Florian Dost", "title": "Detecting outlying demand in multi-leg bookings for transportation\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network effects complicate demand forecasting in general, and outlier\ndetection in particular. For example, in transportation networks, sudden\nincreases in demand for a specific destination will not only affect the legs\narriving at that destination, but also connected legs nearby in the network.\nNetwork effects are particularly relevant when transport service providers,\nsuch as railway or coach companies, offer many multi-leg itineraries. In this\npaper, we present a novel method for generating automated outlier alerts, to\nsupport analysts in adjusting demand forecasts accordingly for reliable\nplanning. To create such alerts, we propose a two-step method for detecting\noutlying demand from transportation network bookings. The first step clusters\nnetwork legs to appropriately partition and pool booking patterns. The second\nstep identifies outliers within each cluster to create a ranked alert list of\naffected legs. We show that this method outperforms analyses that independently\nconsider each leg in a network, especially in highly-connected networks where\nmost passengers book multi-leg itineraries. We illustrate the applicability on\nempirical data obtained from Deutsche Bahn and with a detailed simulation\nstudy. The latter demonstrates the robustness of the approach and quantifies\nthe potential revenue benefits of adjusting for outlying demand in networks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 13:40:45 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Rennie", "Nicola", ""], ["Cleophas", "Catherine", ""], ["Sykulski", "Adam M.", ""], ["Dost", "Florian", ""]]}, {"id": "2104.04190", "submitter": "Mohammad Hattab", "authors": "Mohammad W. Hattab", "title": "Measurement Errors in Semiparametric Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models that ignore measurement error in predictors may produce\nhighly biased estimates leading to erroneous inferences. It is well known that\nit is extremely difficult to take measurement error into account in Gaussian\nnonparametric regression. This problem becomes tremendously more difficult when\nconsidering other families such as logistic regression, Poisson and\nnegative-binomial. For the first time, we present a method aiming to correct\nfor measurement error when estimating regression functions flexibly covering\nvirtually all distributions and link functions regularly considered in\ngeneralized linear models. This approach depends on approximating the first and\nthe second moment of the response after integrating out the true unobserved\npredictors in a semiparametric generalized linear model. Unlike previous\nmethods, this method is not restricted to truncated splines and can utilize\nvarious basis functions. Through extensive simulation studies, we study the\nperformance of our method under many scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 04:48:52 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Hattab", "Mohammad W.", ""]]}, {"id": "2104.04432", "submitter": "Yajuan Si", "authors": "Yajuan Si, Roderick J. A. Little, Ya Mo and Nell Sedransk", "title": "A Case Study of Nonresponse Bias Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonresponse bias is a widely prevalent problem for data collections. We\ndevelop a ten-step exemplar to guide nonresponse bias analysis (NRBA) in\ncross-sectional studies, and apply these steps to the Early Childhood\nLongitudinal Study, Kindergarten Class of 2010-11. A key step is the\nconstruction of indices of nonresponse bias based on proxy pattern-mixture\nmodels for survey variables of interest. A novel feature is to characterize the\nstrength of evidence about nonresponse bias contained in these indices, based\non the strength of relationship of the characteristics in the nonresponse\nadjustment with the key survey variables. Our NRBA incorporates missing at\nrandom and missing not at random mechanisms, and all analyses can be done\nstraightforwardly with standard statistical software.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:30:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Si", "Yajuan", ""], ["Little", "Roderick J. A.", ""], ["Mo", "Ya", ""], ["Sedransk", "Nell", ""]]}, {"id": "2104.04514", "submitter": "Tin Nguyen", "authors": "Brian L. Trippe, Tin D. Nguyen, Tamara Broderick", "title": "Optimal transport couplings of Gibbs samplers on partitions for unbiased\n  estimation", "comments": "Brian Trippe and Tin Nguyen contributed equally. First presented at\n  3rd Symposium on Advances in Approximate Bayesian Inference, 2020. This\n  version fixes authoring order of first version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational couplings of Markov chains provide a practical route to\nunbiased Monte Carlo estimation that can utilize parallel computation. However,\nthese approaches depend crucially on chains meeting after a small number of\ntransitions. For models that assign data into groups, e.g. mixture models, the\nobvious approaches to couple Gibbs samplers fail to meet quickly. This failure\nowes to the so-called \"label-switching\" problem; semantically equivalent\nrelabelings of the groups contribute well-separated posterior modes that impede\nfast mixing and cause large meeting times. We here demonstrate how to avoid\nlabel switching by considering chains as exploring the space of partitions\nrather than labelings. Using a metric on this space, we employ an optimal\ntransport coupling of the Gibbs conditionals. This coupling outperforms\nalternative couplings that rely on labelings and, on a real dataset, provides\nestimates more precise than usual ergodic averages in the limited time regime.\nCode is available at github.com/tinnguyen96/coupling-Gibbs-partition.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 17:55:12 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 16:11:14 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Trippe", "Brian L.", ""], ["Nguyen", "Tin D.", ""], ["Broderick", "Tamara", ""]]}, {"id": "2104.04565", "submitter": "Ying Jin", "authors": "Ying Jin, Dominik Rothenh\\\"ausler", "title": "One estimator, many estimands: fine-grained quantification of\n  uncertainty using conditional inference", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical uncertainty has many components, such as measurement errors,\ntemporal variation, or sampling. Not all of these sources are relevant when\nconsidering a specific application, since practitioners might view some\nattributes of observations as fixed.\n  We study the statistical inference problem arising when data is drawn\nconditionally on some attributes. These attributes are assumed to be sampled\nfrom a super-population but viewed as fixed when conducting uncertainty\nquantification. The estimand is thus defined as the parameter of a conditional\ndistribution. We propose methods to construct conditionally valid p-values and\nconfidence intervals for these conditional estimands based on asymptotically\nlinear estimators.\n  In this setting, a given estimator is conditionally unbiased for potentially\nmany conditional estimands, which can be seen as parameters of different\npopulations. Testing different populations raises questions of multiple\ntesting. We discuss simple procedures that control novel conditional error\nrates. In addition, we introduce a bias correction technique that enables\ntransfer of estimators across conditional distributions arising from the same\nsuper-population. This can be used to infer parameters and estimators on future\ndatasets based on some new data.\n  The validity and applicability of the proposed methods are demonstrated on\nsimulated and real-world data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 18:49:03 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 03:33:57 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Jin", "Ying", ""], ["Rothenh\u00e4usler", "Dominik", ""]]}, {"id": "2104.04590", "submitter": "Jiaying Gu", "authors": "Christopher Dobronyi, Jiaying Gu, Kyoo il Kim", "title": "Identification of Dynamic Panel Logit Models with Fixed Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the identification problem for a class of dynamic panel logit\nmodels with fixed effects has a connection to the truncated moment problem in\nmathematics. We use this connection to show that the sharp identified set of\nthe structural parameters is characterized by a set of moment equality and\ninequality conditions. This result provides sharp bounds in models where moment\nequality conditions do not exist or do not point identify the parameters. We\nalso show that the sharp identifying content of the non-parametric latent\ndistribution of the fixed effects is characterized by a vector of its\ngeneralized moments, and that the number of moments grows linearly in T. This\nfinal result lets us point identify, or sharply bound, specific classes of\nfunctionals, without solving an optimization problem with respect to the latent\ndistribution.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 20:06:54 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 03:19:05 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dobronyi", "Christopher", ""], ["Gu", "Jiaying", ""], ["Kim", "Kyoo il", ""]]}, {"id": "2104.04628", "submitter": "Paromita Dubey", "authors": "Paromita Dubey and Hans-Georg M\\\"uller", "title": "Modeling Time-Varying Random Objects and Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Samples of dynamic or time-varying networks and other random object data such\nas time-varying probability distributions are increasingly encountered in\nmodern data analysis. Common methods for time-varying data such as functional\ndata analysis are infeasible when observations are time courses of networks or\nother complex non-Euclidean random objects that are elements of general metric\nspaces. In such spaces, only pairwise distances between the data objects are\navailable and a strong limitation is that one cannot carry out arithmetic\noperations due to the lack of an algebraic structure. We combat this complexity\nby a generalized notion of mean trajectory taking values in the object space.\nFor this, we adopt pointwise Fr\\'echet means and then construct pointwise\ndistance trajectories between the individual time courses and the estimated\nFr\\'echet mean trajectory, thus representing the time-varying objects and\nnetworks by functional data. Functional principal component analysis of these\ndistance trajectories can reveal interesting features of dynamic networks and\nobject time courses and is useful for downstream analysis. Our approach also\nmakes it possible to study the empirical dynamics of time-varying objects,\nincluding dynamic regression to the mean or explosive behavior over time. We\ndemonstrate desirable asymptotic properties of sample based estimators for\nsuitable population targets under mild assumptions. The utility of the proposed\nmethodology is illustrated with dynamic networks, time-varying distribution\ndata and longitudinal growth data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 22:36:47 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dubey", "Paromita", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "2104.04647", "submitter": "Peng Ding", "authors": "Fangzhou Su and Peng Ding", "title": "Model-assisted analyses of cluster-randomized experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cluster-randomized experiments are widely used due to their logistical\nconvenience and policy relevance. To analyze them properly, we must address the\nfact that the treatment is assigned at the cluster level instead of the\nindividual level. Standard analytic strategies are regressions based on\nindividual data, cluster averages, and cluster totals, which differ when the\ncluster sizes vary. These methods are often motivated by models with strong and\nunverifiable assumptions, and the choice among them can be subjective. Without\nany outcome modeling assumption, we evaluate these regression estimators and\nthe associated robust standard errors from a design-based perspective where\nonly the treatment assignment itself is random and controlled by the\nexperimenter. We demonstrate that regression based on cluster averages targets\na weighted average treatment effect, regression based on individual data is\nsuboptimal in terms of efficiency, and regression based on cluster totals is\nconsistent and more efficient with a large number of clusters. We highlight the\ncritical role of covariates in improving estimation efficiency, and illustrate\nthe efficiency gain via both simulation studies and data analysis. Moreover, we\nshow that the robust standard errors are convenient approximations to the true\nasymptotic standard errors under the design-based perspective. Our theory holds\neven when the outcome models are misspecified, so it is model-assisted rather\nthan model-based. We also extend the theory to a wider class of weighted\naverage treatment effects.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 23:58:42 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Su", "Fangzhou", ""], ["Ding", "Peng", ""]]}, {"id": "2104.04660", "submitter": "Nour Hawila", "authors": "Nour Hawila and Arthur Berg", "title": "Exact-corrected confidence interval for risk difference in\n  noninferiority binomial trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel confidence interval estimator is proposed for the risk difference in\nnoninferiority binomial trials. The confidence interval is consistent with an\nexact unconditional test that preserves the type-1 error, and has improved\npower, particularly for smaller sample sizes, compared to the confidence\ninterval by Chan & Zhang (1999). The improved performance of the proposed\nconfidence interval is theoretically justified and demonstrated with\nsimulations and examples. An R package is also distributed that implements the\nproposed methods along with other confidence interval estimators.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 01:16:39 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 16:16:25 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Hawila", "Nour", ""], ["Berg", "Arthur", ""]]}, {"id": "2104.04966", "submitter": "Yue Cui", "authors": "Solomon W. Harrar and Yue Cui", "title": "Nonparametric Method for Clustered Data in Pre-Post Factorial Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In repeated measures factorial designs involving clustered units, parametric\nmethods such as linear mixed effects models are used to handle within subject\ncorrelations. However, assumptions of these parametric models such as\ncontinuity and normality are usually hard to come by in many cases. The\nhomoscedasticity assumption is rather hard to verify in practice. Furthermore,\nthese assumptions may not even be realistic when data are measured in a\nnon-metric scale as commonly happens, for example, in Quality of Life outcomes.\nIn this article, nonparametric effect-size measures for clustered data in\nfactorial designs with pre-post measurements will be introduced. The\neffect-size measures provide intuitively-interpretable and informative\nprobabilistic comparisons of treatment and time effects. The dependence among\nobservations within a cluster can be arbitrary across treatment groups. The\neffect-size estimators along with their asymptotic properties for computing\nconfidence intervals and performing hypothesis tests will be discussed.\nANOVA-type statistics with $\\chi^2$ approximation that retain some of the\noptimal asymptotic behaviors in small samples are investigated. Within each\ntreatment group, we allow some clusters to involve observations measured on\nboth pre and post intervention periods (referred to as complete clusters),\nwhile others to contain observations from either pre or post intervention\nperiod only (referred to as incomplete clusters). Our methods are shown to be,\nparticularly effective in the presence of multiple forms of clustering. The\ndeveloped nonparametric methods are illustrated with data from a three-arm\nRandomized Trial of Indoor Wood Smoke reduction. The study considered two\nactive treatments to improve asthma symptoms of kids living in homes that use\nwood stove for heating.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 09:11:35 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Harrar", "Solomon W.", ""], ["Cui", "Yue", ""]]}, {"id": "2104.05021", "submitter": "Soham Sarkar", "authors": "Soham Sarkar and Victor M. Panaretos", "title": "CovNet: Covariance Networks for Functional Data on Multidimensional\n  Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Covariance estimation is ubiquitous in functional data analysis. Yet, the\ncase of functional observations over multidimensional domains introduces\ncomputational and statistical challenges, rendering the standard methods\neffectively inapplicable. To address this problem, we introduce Covariance\nNetworks (CovNet) as a modeling and estimation tool. The CovNet model is\nuniversal -- it can be used to approximate any covariance up to desired\nprecision. Moreover, the model can be fitted efficiently to the data and its\nneural network architecture allows us to employ modern computational tools in\nthe implementation. The CovNet model also admits a closed-form\neigen-decomposition, which can be computed efficiently, without constructing\nthe covariance itself. This facilitates easy storage and subsequent\nmanipulation in the context of the CovNet. Moreover, we establish consistency\nof the proposed estimator and derive its rate of convergence. The usefulness of\nthe proposed method is demonstrated by means of an extensive simulation study.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 14:40:41 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sarkar", "Soham", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "2104.05076", "submitter": "Daoji Li", "authors": "Ruipeng Dong, Daoji Li, Zemin Zheng", "title": "Parallel integrative learning for large-scale multi-response regression\n  with incomplete outcomes", "comments": "32 pages", "journal-ref": "Computational Statistics and Data Analysis, 2021", "doi": "10.1016/j.csda.2021.107243", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-task learning is increasingly used to investigate the association\nstructure between multiple responses and a single set of predictor variables in\nmany applications. In the era of big data, the coexistence of incomplete\noutcomes, large number of responses, and high dimensionality in predictors\nposes unprecedented challenges in estimation, prediction, and computation. In\nthis paper, we propose a scalable and computationally efficient procedure,\ncalled PEER, for large-scale multi-response regression with incomplete\noutcomes, where both the numbers of responses and predictors can be\nhigh-dimensional. Motivated by sparse factor regression, we convert the\nmulti-response regression into a set of univariate-response regressions, which\ncan be efficiently implemented in parallel. Under some mild regularity\nconditions, we show that PEER enjoys nice sampling properties including\nconsistency in estimation, prediction, and variable selection. Extensive\nsimulation studies show that our proposal compares favorably with several\nexisting methods in estimation accuracy, variable selection, and computation\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 19:01:24 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dong", "Ruipeng", ""], ["Li", "Daoji", ""], ["Zheng", "Zemin", ""]]}, {"id": "2104.05110", "submitter": "Brieuc Lehmann", "authors": "Brieuc Lehmann and Simon White", "title": "Bayesian exponential random graph models for populations of networks", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The collection of data on populations of networks is becoming increasingly\ncommon, where each data point can be seen as a realisation of a network-valued\nrandom variable. A canonical example is that of brain networks: a typical\nneuroimaging study collects one or more brain scans across multiple\nindividuals, each of which can be modelled as a network with nodes\ncorresponding to distinct brain regions and edges corresponding to structural\nor functional connections between these regions. Most statistical network\nmodels, however, were originally proposed to describe a single underlying\nrelational structure, although recent years have seen a drive to extend these\nmodels to populations of networks. Here, we propose one such extension: a\nmultilevel framework for populations of networks based on exponential random\ngraph models. By pooling information across the individual networks, this\nframework provides a principled approach to characterise the relational\nstructure for an entire population. To perform inference, we devise a novel\nexchange-within-Gibbs MCMC algorithm that generates samples from the\ndoubly-intractable posterior. To illustrate our framework, we use it to assess\ngroup-level variations in networks derived from fMRI scans, enabling the\ninference of age-related differences in the topological structure of the\nbrain's functional connectivity.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 21:15:50 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lehmann", "Brieuc", ""], ["White", "Simon", ""]]}, {"id": "2104.05134", "submitter": "Kai Xu", "authors": "Kai Xu, Tor Erlend Fjelde, Charles Sutton, Hong Ge", "title": "Couplings for Multinomial Hamiltonian Monte Carlo", "comments": "Published in AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is a popular sampling method in Bayesian\ninference. Recently, Heng & Jacob (2019) studied Metropolis HMC with couplings\nfor unbiased Monte Carlo estimation, establishing a generic parallelizable\nscheme for HMC. However, in practice a different HMC method, multinomial HMC,\nis considered as the go-to method, e.g. as part of the no-U-turn sampler. In\nmultinomial HMC, proposed states are not limited to end-points as in Metropolis\nHMC; instead points along the entire trajectory can be proposed. In this paper,\nwe establish couplings for multinomial HMC, based on optimal transport for\nmultinomial sampling in its transition. We prove an upper bound for the meeting\ntime - the time it takes for the coupled chains to meet - based on the notion\nof local contractivity. We evaluate our methods using three targets: 1,000\ndimensional Gaussians, logistic regression and log-Gaussian Cox point\nprocesses. Compared to Heng & Jacob (2019), coupled multinomial HMC generally\nattains a smaller meeting time, and is more robust to choices of step sizes and\ntrajectory lengths, which allows re-use of existing adaptation methods for HMC.\nThese improvements together paves the way for a wider and more practical use of\ncoupled HMC methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 23:06:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xu", "Kai", ""], ["Fjelde", "Tor Erlend", ""], ["Sutton", "Charles", ""], ["Ge", "Hong", ""]]}, {"id": "2104.05150", "submitter": "Le Bao", "authors": "Ben Sheng, Changcheng Li, Le Bao, Runze Li", "title": "Probabilistic HIV Recency Classification -- A Logistic Regression\n  without Labeled Individual Level Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate HIV incidence estimation based on individual recent infection status\n(recent vs long-term infection) is important for monitoring the epidemic,\ntargeting interventions to those at greatest risk of new infection, and\nevaluating existing programs of prevention and treatment. Starting from 2015,\nthe Population-based HIV Impact Assessment (PHIA) individual-level surveys are\nimplemented in the most-affected countries in sub-Saharan Africa. PHIA is a\nnationally-representative HIV-focused survey that combines household visits\nwith key questions and cutting-edge technologies such as biomarker tests for\nHIV antibody and HIV viral load which offer the unique opportunity of\ndistinguishing between recent infection and long-term infection, and providing\nrelevant HIV information by age, gender, and location. In this article, we\npropose a semi-supervised logistic regression model for estimating individual\nlevel HIV recency status. It incorporates information from multiple data\nsources -- the PHIA survey where the true HIV recency status is unknown, and\nthe cohort studies provided in the literature where the relationship between\nHIV recency status and the covariates are presented in the form of a\ncontingency table. It also utilizes the national level HIV incidence estimates\nfrom the epidemiology model. Applying the proposed model to Malawi PHIA data,\nwe demonstrate that our approach is more accurate for the individual level\nestimation and more appropriate for estimating HIV recency rates at aggregated\nlevels than the current practice -- the binary classification tree (BCT).\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 01:19:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sheng", "Ben", ""], ["Li", "Changcheng", ""], ["Bao", "Le", ""], ["Li", "Runze", ""]]}, {"id": "2104.05184", "submitter": "Leying Guan", "authors": "Leying Guan", "title": "A smoothed and probabilistic PARAFAC model with covariates", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis and clustering of multivariate time-series data attract growing\ninterest in immunological and clinical studies. In such applications,\nresearchers are interested in clustering subjects based on potentially\nhigh-dimensional longitudinal features, and in investigating how clinical\ncovariates may affect the clustering results. These studies are often\nchallenging due to high dimensionality, as well as the sparse and irregular\nnature of sample collection along the time dimension. We propose a smoothed\nprobabilistic PARAFAC model with covariates (SPACO) to tackle these two\nproblems while utilizing auxiliary covariates of interest. We provide intensive\nsimulations to test different aspects of SPACO and demonstrate its use on\nimmunological data sets from two recent cohorts of SARs-CoV-2 patients.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 03:38:33 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 23:54:32 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Guan", "Leying", ""]]}, {"id": "2104.05192", "submitter": "Yutao Liu", "authors": "Yutao Liu, Andrew Gelman, Qixuan Chen", "title": "Inference from Non-Random Samples Using Bayesian Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference from non-random samples in data-rich settings where\nhigh-dimensional auxiliary information is available both in the sample and the\ntarget population, with survey inference being a special case. We propose a\nregularized prediction approach that predicts the outcomes in the population\nusing a large number of auxiliary variables such that the ignorability\nassumption is reasonable while the Bayesian framework is straightforward for\nquantification of uncertainty. Besides the auxiliary variables, inspired by\nLittle & An (2004), we also extend the approach by estimating the propensity\nscore for a unit to be included in the sample and also including it as a\npredictor in the machine learning models. We show through simulation studies\nthat the regularized predictions using soft Bayesian additive regression trees\nyield valid inference for the population means and coverage rates close to the\nnominal levels. We demonstrate the application of the proposed methods using\ntwo different real data applications, one in a survey and one in an\nepidemiology study.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 04:08:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Liu", "Yutao", ""], ["Gelman", "Andrew", ""], ["Chen", "Qixuan", ""]]}, {"id": "2104.05414", "submitter": "Tasnim Hamza", "authors": "Tasnim Hamza, Toshi A. Furukawa, Nicola Orsini, Andrea Cipriani,\n  Cynthia Iglesias, Georgia Salanti", "title": "A dose-effect network meta-analysis model: an application in\n  antidepressants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network meta-analysis (NMA) has been used to answer a range of clinical\nquestions about the preferable intervention for a given condition. Although the\neffectiveness and safety of pharmacological agents depend on the dose\nadministered, NMA applications typically ignore the role that drugs dosage play\non the results. This leads to more heterogeneity in the network. In this paper\nwe present a suite of network meta-analysis models that incorporates the\ndose-effect relationship (DE-NMA) using restricted cubic splines (RCS). We\nextend the model into a dose-effect network meta-regression to account for\nstudy-level covariates and for groups of agents in a class-effect DE-NMA model.\nWe apply the models to a network of aggregate data about the efficacy of 21\nantidepressants and placebo for depression. We found that all antidepressants\nare more efficacious than placebo after a certain dose. We also identify the\ndose level in which each antidepressant effect exceeds that of placebo and\nestimate the dose beyond the effect of the antidepressants no longer increases.\nThe DE-NMA model with RCS takes a flexible approach to modelling the\ndose-effect relationship in multiple interventions, so decision-makers can use\nthem to inform treatment choice.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 12:48:48 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 16:05:01 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Hamza", "Tasnim", ""], ["Furukawa", "Toshi A.", ""], ["Orsini", "Nicola", ""], ["Cipriani", "Andrea", ""], ["Iglesias", "Cynthia", ""], ["Salanti", "Georgia", ""]]}, {"id": "2104.05513", "submitter": "Larry Han", "authors": "Larry Han, Xuan Wang, Tianxi Cai", "title": "On the Evaluation of Surrogate Markers in Real World Data Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shortcomings of randomized clinical trials are pronounced in urgent health\ncrises, when rapid identification of effective treatments is critical.\nLeveraging short-term surrogates in real-world data (RWD) can guide\npolicymakers evaluating new treatments. In this paper, we develop novel\nestimators for the proportion of treatment effect (PTE) on the true outcome\nexplained by a surrogate in RWD settings. We propose inverse probability\nweighted and doubly robust (DR) estimators of an optimal transformation of the\nsurrogate and PTE by semi-nonparametrically modeling the relationship between\nthe true outcome and surrogate given baseline covariates. We show that our\nestimators are consistent and asymptotically normal, and the DR estimator is\nconsistent when either the propensity score model or outcome regression model\nis correctly specified. We compare our proposed estimators to existing\nestimators and show a reduction in bias and gains in efficiency through\nsimulations. We illustrate the utility of our method in obtaining an\ninterpretable PTE by conducting a cross-trial comparison of two biologic\ntherapies for ulcerative colitis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:44:18 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Han", "Larry", ""], ["Wang", "Xuan", ""], ["Cai", "Tianxi", ""]]}, {"id": "2104.05620", "submitter": "Wojciech Zieli\\'nski", "authors": "Wojciech Zieli\\'nski", "title": "The shortest confidence interval for Poisson mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of the shortest confidence interval for Poisson mean is shown.\nThe method of obtaining such an interval is presented as well.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:14:30 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zieli\u0144ski", "Wojciech", ""]]}, {"id": "2104.05751", "submitter": "Jorge Sicacha-Parada", "authors": "Jorge Sicacha-Parada and Diego Pavon-Jordan and Ingelin Steinsland and\n  Roel May and B{\\aa}rd Stokke and Ingar Jostein {\\O}ien", "title": "A spatial modeling framework for monitoring surveys with different\n  sampling protocols with a case study for bird populations in mid-Scandinavia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying species abundance is the basis for spatial ecology and\nbiodiversity conservation. Abundance data are mostly collected through\nprofessional surveys as part of monitoring programs, often at a national level.\nThese surveys rarely follow the same sampling protocol in different countries,\nwhich represents a challenge for producing abundance maps based on the\ninformation available for more than one country. We here present a novel\nsolution for this methodological challenge with a case study concerning bird\nabundance in mid-Scandinavia. We use data from bird monitoring programs in\nNorway and Sweden. Each census collects abundance data following two different\nsampling protocols that each contain two different sampling methods. We propose\na modeling framework that assumes a common Gaussian Random Field driving both\nthe observed and true abundance with either a linear or a relaxed linear\nassociation between them. Thus, the models in this framework can integrate all\nsources of information involving count of organisms to produce one estimate for\nthe expected abundance, its uncertainty and the covariate effects. Bayesian\ninference is performed using INLA and the SPDE approach for spatial modeling.\nWe also present the results of a simulation study based on the census data from\nmid-Scandinavia to assess the performance of the models under misspecification.\nFinally, maps of the total expected abundance of the bird species present in\nour study region in mid-Scandinavia were produced. We found that the framework\nallows for consistent integration of data from surveys with different sampling\nprotocols. Further, the simulation study showed that models with a relaxed\nlinear specification are less sensitive to misspecification, compared to the\nmodel that assumes linear association between counts. Relaxed linear\nspecifications improved goodness-of-fit, but not the predictive power of the\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:24:37 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Sicacha-Parada", "Jorge", ""], ["Pavon-Jordan", "Diego", ""], ["Steinsland", "Ingelin", ""], ["May", "Roel", ""], ["Stokke", "B\u00e5rd", ""], ["\u00d8ien", "Ingar Jostein", ""]]}, {"id": "2104.05762", "submitter": "Alexander D'Amour", "authors": "Alexander D'Amour and Alexander Franks", "title": "Deconfounding Scores: Feature Representations for Causal Effect\n  Estimation with Weak Overlap", "comments": "A previous version of this paper was presented at the NeurIPS 2019\n  Causal ML workshop (https://tripods.cis.cornell.edu/neurips19_causalml/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key condition for obtaining reliable estimates of the causal effect of a\ntreatment is overlap (a.k.a. positivity): the distributions of the features\nused to perform causal adjustment cannot be too different in the treated and\ncontrol groups. In cases where overlap is poor, causal effect estimators can\nbecome brittle, especially when they incorporate weighting. To address this\nproblem, a number of proposals (including confounder selection or dimension\nreduction methods) incorporate feature representations to induce better overlap\nbetween the treated and control groups. A key concern in these proposals is\nthat the representation may introduce confounding bias into the effect\nestimator. In this paper, we introduce deconfounding scores, which are feature\nrepresentations that induce better overlap without biasing the target of\nestimation. We show that deconfounding scores satisfy a zero-covariance\ncondition that is identifiable in observed data. As a proof of concept, we\ncharacterize a family of deconfounding scores in a simplified setting with\nGaussian covariates, and show that in some simple simulations, these scores can\nbe used to construct estimators with good finite-sample properties. In\nparticular, we show that this technique could be an attractive alternative to\nstandard regularizations that are often applied to IPW and balancing weights.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:50:11 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["D'Amour", "Alexander", ""], ["Franks", "Alexander", ""]]}, {"id": "2104.05905", "submitter": "Sarah Robertson", "authors": "Sarah E. Robertson, Jon A. Steingrimsson, Nina R. Joyce, Elizabeth A.\n  Stuart, Issa J. Dahabreh", "title": "Center-specific causal inference with multicenter trials: reinterpreting\n  trial evidence in the context of each participating center", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multicenter randomized trials, when effect modifiers have a different\ndistribution across centers, comparisons between treatment groups that average\nover centers may not apply to any of the populations underlying the individual\ncenters. Here, we describe methods for reinterpreting the evidence produced by\na multicenter trial in the context of the population underlying each center. We\ndescribe how to identify center-specific effects under identifiability\nconditions that are largely supported by the study design and when associations\nbetween center membership and the outcome may be present, given baseline\ncovariates and treatment (\"center-outcome associations\"). We then consider an\nadditional condition of no center-outcome associations given baseline\ncovariates and treatment. We show that this condition can be assessed using the\ntrial data; when it holds, center-specific treatment effects can be estimated\nusing analyses that completely pool information across centers. We propose\nmethods for estimating center-specific average treatment effects, when\ncenter-outcome associations may be present and when they are absent, and\ndescribe approaches for assessing whether center-specific treatment effects are\nhomogeneous. We evaluate the performance of the methods in a simulation study\nand illustrate their implementation using data from the Hepatitis C Antiviral\nLong-Term Treatment Against Cirrhosis trial.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 02:43:03 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 17:33:10 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Robertson", "Sarah E.", ""], ["Steingrimsson", "Jon A.", ""], ["Joyce", "Nina R.", ""], ["Stuart", "Elizabeth A.", ""], ["Dahabreh", "Issa J.", ""]]}, {"id": "2104.06180", "submitter": "Achmad Choiruddin", "authors": "Achmad Choiruddin, Aisah, Finola Trisnisa, Nur Iriawan", "title": "Quantifying effect of geological factor on distribution of earthquake\n  occurrences by inhomogeneous Cox processes", "comments": "18 pages", "journal-ref": null, "doi": "10.1007/s00024-021-02713-2", "report-no": null, "categories": "physics.geo-ph stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Research on the earthquake occurrences using a statistical methodology based\non point processes has mainly focused on the data featuring earthquake catalogs\nwhich ignores the effect of environmental variables. In this paper, we\nintroduce inhomogeneous versions of the Cox process models which are able to\nquantify the effect of geological factors such as subduction zone, fault, and\nvolcano on the major earthquake distribution in Sulawesi and Maluku, Indonesia.\nIn particular, we compare Thomas, Cauchy, variance-Gamma, and log-Gaussian Cox\nmodels and consider parametric intensity and pair correlation functions to\nenhance model interpretability. We perform model selection using the Akaike\ninformation criterion (AIC) and envelopes test. We conclude that the nearest\ndistances to the subduction zone and volcano give a significant impact on the\nrisk of earthquake occurrence in Sulawesi and Maluku. Furthermore, the Cauchy\nand variance-Gamma cluster models fit well the major earthquake distribution in\nSulawesi and Maluku.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 03:54:31 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Choiruddin", "Achmad", ""], ["Aisah", "", ""], ["Trisnisa", "Finola", ""], ["Iriawan", "Nur", ""]]}, {"id": "2104.06296", "submitter": "Konstantinos Fokianos", "authors": "Mirko Armillotta and Konstantinos Fokianos", "title": "Poisson Network Autoregression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider network autoregressive models for count data with a non-random\ntime-varying neighborhood structure. The main methodological contribution is\nthe development of conditions that guarantee stability and valid statistical\ninference. We consider both cases of fixed and increasing network dimension and\nwe show that quasi-likelihood inference provides consistent and asymptotically\nnormally distributed estimators. The work is complemented by simulation results\nand a data example.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 15:39:23 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Armillotta", "Mirko", ""], ["Fokianos", "Konstantinos", ""]]}, {"id": "2104.06384", "submitter": "Philippe Gagnon", "authors": "Sebastian M Schmon and Philippe Gagnon", "title": "Optimal scaling of random walk Metropolis algorithms using Bayesian\n  large-sample asymptotics", "comments": "Both authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional limit theorems have been shown to be useful to derive tuning\nrules for finding the optimal scaling in random walk Metropolis algorithms. The\nassumptions under which weak convergence results are proved are however\nrestrictive; the target density is typically assumed to be of a product form.\nUsers may thus doubt the validity of such tuning rules in practical\napplications. In this paper, we shed some light on optimal scaling problems\nfrom a different perspective, namely a large-sample one. This allows to prove\nweak convergence results under realistic assumptions and to propose novel\nparameter-dimension-dependent tuning guidelines. The proposed guidelines are\nconsistent with previous ones when the target density is close to having a\nproduct form, but significantly different otherwise.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:39:50 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 23:27:22 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Schmon", "Sebastian M", ""], ["Gagnon", "Philippe", ""]]}, {"id": "2104.06389", "submitter": "Minjie Wang", "authors": "Minjie Wang, Genevera I. Allen", "title": "Thresholded Graphical Lasso Adjusts for Latent Variables: Application to\n  Functional Neural Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroscience, researchers seek to uncover the connectivity of neurons from\nlarge-scale neural recordings or imaging; often people employ graphical model\nselection and estimation techniques for this purpose. But, existing\ntechnologies can only record from a small subset of neurons leading to a\nchallenging problem of graph selection in the presence of extensive latent\nvariables. Chandrasekaran et al. (2012) proposed a convex program to address\nthis problem that poses challenges from both a computational and statistical\nperspective. To solve this problem, we propose an incredibly simple solution:\napply a hard thresholding operator to existing graph selection methods.\nConceptually simple and computationally attractive, we demonstrate that\nthresholding the graphical Lasso, neighborhood selection, or CLIME estimators\nhave superior theoretical properties in terms of graph selection consistency as\nwell as stronger empirical results than existing approaches for the latent\nvariable graphical model problem. We also demonstrate the applicability of our\napproach through a neuroscience case study on calcium-imaging data to estimate\nfunctional neural connections.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:50:26 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wang", "Minjie", ""], ["Allen", "Genevera I.", ""]]}, {"id": "2104.06487", "submitter": "Chiwoo Park", "authors": "Chiwoo Park", "title": "Gaussian Process Model for Estimating Piecewise Continuous Regression\n  Functions", "comments": "11 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Gaussian process (GP) model for estimating piecewise\ncontinuous regression functions. In scientific and engineering applications of\nregression analysis, the underlying regression functions are piecewise\ncontinuous in that data follow different continuous regression models for\ndifferent regions of the data with possible discontinuities between the\nregions. However, many conventional GP regression approaches are not designed\nfor piecewise regression analysis. We propose a new GP modeling approach for\nestimating an unknown piecewise continuous regression function. The new GP\nmodel seeks for a local GP estimate of an unknown regression function at each\ntest location, using local data neighboring to the test location. To\naccommodate the possibilities of the local data from different regions, the\nlocal data is partitioned into two sides by a local linear boundary, and only\nthe local data belonging to the same side as the test location is used for the\nregression estimate. This local split works very well when the input regions\nare bounded by smooth boundaries, so the local linear approximation of the\nsmooth boundaries works well. We estimate the local linear boundary jointly\nwith the other hyperparameters of the GP model, using the maximum likelihood\napproach. Its computation time is as low as the local GP's time. The superior\nnumerical performance of the proposed approach over the conventional GP\nmodeling approaches is shown using various simulated piecewise regression\nfunctions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 20:01:43 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Park", "Chiwoo", ""]]}, {"id": "2104.06581", "submitter": "Jose R. Zubizarreta", "authors": "Ambarish Chattopadhyay, Jose R. Zubizarreta", "title": "On the implied weights of linear regression for causal inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we derive and analyze the implied weights of linear regression\nmethods for causal inference. We obtain new closed-form, finite-sample\nexpressions of the weights for various types of estimators based on\nmultivariate linear regression models. In finite samples, we show that the\nimplied weights have minimum variance, exactly balance the means of the\ncovariates (or transformations thereof) included in the model, and produce\nestimators that may not be sample bounded. Furthermore, depending on the\nspecification of the regression model, we show that the implied weights may\ndistort the structure of the sample in such a way that the resulting estimator\nis biased for the average treatment effect for a given target population. In\nlarge samples, we demonstrate that, under certain functional form assumptions,\nthe implied weights are consistent estimators of the true inverse probability\nweights. We examine doubly robust properties of regression estimators from the\nperspective of their implied weights. We also derive and analyze the implied\nweights of weighted least squares regression. The equivalence between\nminimizing regression residuals and optimizing for certain weights allows us to\nbridge ideas from the regression modeling and causal inference literatures. As\na result, we propose a set of regression diagnostics for causal inference. We\ndiscuss the connection of the implied weights to existing matching and\nweighting approaches. As special cases, we analyze the implied weights in\ncommon settings such as multi-valued treatments, regression after matching, and\ntwo-stage least squares regression with instrumental variables.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 01:57:12 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 00:48:47 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chattopadhyay", "Ambarish", ""], ["Zubizarreta", "Jose R.", ""]]}, {"id": "2104.06667", "submitter": "Abhishek Chakrabortty", "authors": "Yuqian Zhang, Abhishek Chakrabortty and Jelena Bradic", "title": "Double Robust Semi-Supervised Inference for the Mean: Selection Bias\n  under MAR Labeling with Decaying Overlap", "comments": "37 pages; (Supplement: 43 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised (SS) inference has received much attention in recent years.\nApart from a moderate-sized labeled data, L, the SS setting is characterized by\nan additional, much larger sized, unlabeled data, U. The setting of |U| >> |L|,\nmakes SS inference unique and different from the standard missing data\nproblems, owing to natural violation of the so-called 'positivity' or 'overlap'\nassumption. However, most of the SS literature implicitly assumes L and U to be\nequally distributed, i.e., no selection bias in the labeling. Inferential\nchallenges in missing at random (MAR) type labeling allowing for selection\nbias, are inevitably exacerbated by the decaying nature of the propensity score\n(PS). We address this gap for a prototype problem, the estimation of the\nresponse's mean. We propose a double robust SS (DRSS) mean estimator and give a\ncomplete characterization of its asymptotic properties. The proposed estimator\nis consistent as long as either the outcome or the PS model is correctly\nspecified. When both models are correctly specified, we provide inference\nresults with a non-standard consistency rate that depends on the smaller size\n|L|. The results are also extended to causal inference with imbalanced\ntreatment groups. Further, we provide several novel choices of models and\nestimators of the decaying PS, including a novel offset logistic model and a\nstratified labeling model. We present their properties under both high and low\ndimensional settings. These may be of independent interest. Lastly, we present\nextensive simulations and also a real data application.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 07:27:27 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhang", "Yuqian", ""], ["Chakrabortty", "Abhishek", ""], ["Bradic", "Jelena", ""]]}, {"id": "2104.06872", "submitter": "Claudia Czado", "authors": "Claudia Czado and Ingrid Van Keilegom", "title": "Dependent censoring based on copulas", "comments": "33 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consider a survival time T that is subject to random right censoring, and\nsuppose that T is stochastically dependent on the censoring time C. We are\ninterested in the marginal distribution of T. This situation is often\nencountered in practice. Consider for instance the case where T is the time to\ndeath of a patient suffering from a certain disease. Then, the censoring time C\nis for instance the time until the person leaves the study or the time until\nhe/she dies from another disease. If the reason for leaving the study is\nrelated to the health condition of the patient or if he/she dies from a disease\nthat has similar risk factors as the disease of interest, then T and C are\nlikely dependent. In this paper we propose a new model that takes this\ndependence into account. The model is based on a parametric copula for the\nrelationship between T and C, and on parametric marginal distributions for T\nand C. Unlike most other papers in the literature, we do not assume that the\nparameter defining the copula function is known. We give sufficient conditions\non these parametric copula and marginals under which the bivariate distribution\nof (T;C) is identifed. These sufficient conditions are then checked for a wide\nrange of common copulas and marginal distributions. We also study the\nestimation of the model, and carry out extensive simulations and the analysis\nof data on pancreas cancer to illustrate the proposed model and estimation\nprocedure.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:14:56 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Czado", "Claudia", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "2104.06911", "submitter": "Zijian Guo", "authors": "Zijian Guo", "title": "Post-selection Problems for Causal Inference with Invalid Instruments: A\n  Solution Using Searching and Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable method is among the most commonly used causal inference\napproaches for analyzing observational studies with unmeasured confounders.\nDespite its popularity, the instruments' invalidity is a major concern for\npractical applications and a fast-growing area of research is inference for the\ncausal effect with possibly invalid instruments. In this paper, we construct\nuniformly valid confidence intervals for the causal effect when the instruments\nare possibly invalid. We illustrate the post-selection problem of existing\ninference methods relying on instrument selection. Our proposal is to search\nfor the value of the treatment effect such that a sufficient amount of\ncandidate instruments are taken as valid. We further devise a novel sampling\nmethod, which, together with searching, lead to a more precise confidence\ninterval. Our proposed searching and sampling confidence intervals are shown to\nbe uniformly valid under the finite-sample majority and plurality rules. We\ncompare our proposed methods with existing inference methods over a large set\nof simulation studies and apply them to study the effect of the triglyceride\nlevel on the glucose level over a mouse data set.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 15:03:22 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Guo", "Zijian", ""]]}, {"id": "2104.07084", "submitter": "Hussein Hazimeh", "authors": "Hussein Hazimeh, Rahul Mazumder, Peter Radchenko", "title": "Grouped Variable Selection with Discrete Optimization: Computational and\n  Statistical Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithmic framework for grouped variable selection that is\nbased on discrete mathematical optimization. While there exist several\nappealing approaches based on convex relaxations and nonconvex heuristics, we\nfocus on optimal solutions for the $\\ell_0$-regularized formulation, a problem\nthat is relatively unexplored due to computational challenges. Our methodology\ncovers both high-dimensional linear regression and nonparametric sparse\nadditive modeling with smooth components. Our algorithmic framework consists of\napproximate and exact algorithms. The approximate algorithms are based on\ncoordinate descent and local search, with runtimes comparable to popular sparse\nlearning algorithms. Our exact algorithm is based on a standalone\nbranch-and-bound (BnB) framework, which can solve the associated mixed integer\nprogramming (MIP) problem to certified optimality. By exploiting the problem\nstructure, our custom BnB algorithm can solve to optimality problem instances\nwith $5 \\times 10^6$ features in minutes to hours -- over $1000$ times larger\nthan what is currently possible using state-of-the-art commercial MIP solvers.\nWe also explore statistical properties of the $\\ell_0$-based estimators. We\ndemonstrate, theoretically and empirically, that our proposed estimators have\nan edge over popular group-sparse estimators in terms of statistical\nperformance in various regimes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:21:59 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hazimeh", "Hussein", ""], ["Mazumder", "Rahul", ""], ["Radchenko", "Peter", ""]]}, {"id": "2104.07113", "submitter": "Bingkai Wang", "authors": "Bingkai Wang, Brian S. Caffo, Xi Luo, Chin-Fu Liu, Andreia V. Faria,\n  Michael I. Miller, Yi Zhao", "title": "Regularized regression on compositional trees with application to MRI\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A compositional tree refers to a tree structure on a set of random variables\nwhere each random variable is a node and composition occurs at each non-leaf\nnode of the tree. As a generalization of compositional data, compositional\ntrees handle more complex relationships among random variables and appear in\nmany disciplines, such as brain imaging, genomics and finance. We consider the\nproblem of sparse regression on data that are associated with a compositional\ntree and propose a transformation-free tree-based regularized regression method\nfor component selection. The regularization penalty is designed based on the\ntree structure and encourages a sparse tree representation. We prove that our\nproposed estimator for regression coefficients is both consistent and model\nselection consistent. In the simulation study, our method shows higher accuracy\nthan competing methods under different scenarios. By analyzing a brain imaging\ndata set from studies of Alzheimer's disease, our method identifies meaningful\nassociations between memory declination and volume of brain regions that are\nconsistent with current understanding.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 20:27:37 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 18:42:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Bingkai", ""], ["Caffo", "Brian S.", ""], ["Luo", "Xi", ""], ["Liu", "Chin-Fu", ""], ["Faria", "Andreia V.", ""], ["Miller", "Michael I.", ""], ["Zhao", "Yi", ""]]}, {"id": "2104.07172", "submitter": "Carlos Erwin Rodr\\'iguez Dr.", "authors": "Carlos E. Rodr\\'iguez and Rams\\'es H. Mena", "title": "COVID-19 Clinical footprint to infer about mortality", "comments": "23 pages and 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information of 1.6 million patients identified as SARS-CoV-2 positive in\nMexico is used to understand the relationship between comorbidities, symptoms,\nhospitalizations and deaths due to the COVID-19 disease. Using the presence or\nabsence of these latter variables a clinical footprint for each patient is\ncreated. The risk, expected mortality and the prediction of death outcomes,\namong other relevant quantities, are obtained and analyzed by means of a\nmultivariate Bernoulli distribution. The proposal considers all possible\nfootprint combinations resulting in a robust model suitable for Bayesian\ninference.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 00:17:41 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Rodr\u00edguez", "Carlos E.", ""], ["Mena", "Rams\u00e9s H.", ""]]}, {"id": "2104.07261", "submitter": "Wanchuang Zhu", "authors": "Wanchuang Zhu, Yingkai Jiang, Jun S. Liu, Ke Deng", "title": "Partition-Mallows Model and Its Inference for Rank Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning how to aggregate ranking lists has been an active research area for\nmany years and its advances have played a vital role in many applications\nranging from bioinformatics to internet commerce. The problem of discerning\nreliability of rankers based only on the rank data is of great interest to many\npractitioners, but has received less attention from researchers. By dividing\nthe ranked entities into two disjoint groups, i.e., relevant and\nirrelevant/background ones, and incorporating the Mallows model for the\nrelative ranking of relevant entities, we propose a framework for rank\naggregation that can not only distinguish quality differences among the rankers\nbut also provide the detailed ranking information for relevant entities.\nTheoretical properties of the proposed approach are established, and its\nadvantages over existing approaches are demonstrated via simulation studies and\nreal-data applications. Extensions of the proposed method to handle partial\nranking lists and conduct covariate-assisted rank aggregation are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 06:18:56 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Zhu", "Wanchuang", ""], ["Jiang", "Yingkai", ""], ["Liu", "Jun S.", ""], ["Deng", "Ke", ""]]}, {"id": "2104.07262", "submitter": "Neelesh Upadhye Dr", "authors": "Aastha M. Sathe and N. S. Upadhye", "title": "Estimation of the Parameters of Vector Autoregressive (VAR) Time Series\n  Model with Symmetric Stable Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we propose the fractional lower order covariance method\n(FLOC) for estimating the parameters of vector autoregressive process (VAR) of\norder $p$, $p\\geq 1$ with symmetric stable noise. Further, we show the\nefficiency, accuracy, and simplicity of our methods through Monte-Carlo\nsimulation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 06:19:25 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Sathe", "Aastha M.", ""], ["Upadhye", "N. S.", ""]]}, {"id": "2104.07266", "submitter": "Thomas P Quinn", "authors": "Thomas P Quinn, Elliott Gordon-Rodriguez, Ionas Erb", "title": "A Critique of Differential Abundance Analysis, and Advocacy for an\n  Alternative", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It is largely taken for granted that differential abundance analysis is, by\ndefault, the best first step when analyzing genomic data. We argue that this is\nnot necessarily the case. In this article, we identify key limitations that are\nintrinsic to differential abundance analysis: it is (a) dependent on\nunverifiable assumptions, (b) an unreliable construct, and (c) overly\nreductionist. We formulate an alternative framework called ratio-based\nbiomarker analysis which does not suffer from the identified limitations.\nMoreover, ratio-based biomarkers are highly flexible. Beyond replacing DAA,\nthey can also be used for many other bespoke analyses, including dimension\nreduction and multi-omics data integration.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 06:33:25 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 21:23:10 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Quinn", "Thomas P", ""], ["Gordon-Rodriguez", "Elliott", ""], ["Erb", "Ionas", ""]]}, {"id": "2104.07328", "submitter": "Miles Lopes", "authors": "Junwen Yao and Miles E. Lopes", "title": "Rates of Bootstrap Approximation for Eigenvalues in High-Dimensional PCA", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the context of principal components analysis (PCA), the bootstrap is\ncommonly applied to solve a variety of inference problems, such as constructing\nconfidence intervals for the eigenvalues of the population covariance matrix\n$\\Sigma$. However, when the data are high-dimensional, there are relatively few\ntheoretical guarantees that quantify the performance of the bootstrap. Our aim\nin this paper is to analyze how well the bootstrap can approximate the joint\ndistribution of the leading eigenvalues of the sample covariance matrix\n$\\hat\\Sigma$, and we establish non-asymptotic rates of approximation with\nrespect to the multivariate Kolmogorov metric. Under certain assumptions, we\nshow that the bootstrap can achieve the dimension-free rate of\n${\\tt{r}}(\\Sigma)/\\sqrt n$ up to logarithmic factors, where ${\\tt{r}}(\\Sigma)$\nis the effective rank of $\\Sigma$, and $n$ is the sample size. From a\nmethodological standpoint, our work also illustrates that applying a\ntransformation to the eigenvalues of $\\hat\\Sigma$ before bootstrapping is an\nimportant consideration in high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 09:28:23 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yao", "Junwen", ""], ["Lopes", "Miles E.", ""]]}, {"id": "2104.07359", "submitter": "Takuo Matsubara", "authors": "Takuo Matsubara, Jeremias Knoblauch, Fran\\c{c}ois-Xavier Briol, Chris.\n  J. Oates", "title": "Robust Generalised Bayesian Inference for Intractable Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalised Bayesian inference updates prior beliefs using a loss function,\nrather than a likelihood, and can therefore be used to confer robustness\nagainst possible misspecification of the likelihood. Here we consider\ngeneralised Bayesian inference with a Stein discrepancy as a loss function,\nmotivated by applications in which the likelihood contains an intractable\nnormalisation constant. In this context, the Stein discrepancy circumvents\nevaluation of the normalisation constant and produces generalised posteriors\nthat are either closed form or accessible using standard Markov chain Monte\nCarlo. On a theoretical level, we show consistency, asymptotic normality, and\nbias-robustness of the generalised posterior, highlighting how these properties\nare impacted by the choice of Stein discrepancy. Then, we provide numerical\nexperiments on a range of intractable distributions, including applications to\nkernel-based exponential family models and non-Gaussian graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:31:22 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Matsubara", "Takuo", ""], ["Knoblauch", "Jeremias", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2104.07386", "submitter": "Michael Von Maltitz", "authors": "A. J. van der Merwe, M. J. von Maltitz, J. H. Meyer", "title": "Reference and Probability-Matching Priors for the Parameters of a\n  Univariate Student $t$-Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper reference and probability-matching priors are derived for the\nunivariate Student $t$-distribution. These priors generally lead to procedures\nwith properties frequentists can relate to while still retaining Bayes\nvalidity. The priors are tested by performing simulation studies. The focus is\non the relative mean squared error from the posterior median ($MSE(\\nu)/\\nu$)\nand on the frequentist coverage of the 95\\% credibility intervals for a sample\nsize of $n=30$. Average interval lengths of the credibility intervals as well\nas the modes of the interval lengths based on 2000 simulations are also\nconsidered. The performance of the priors are also tested on real data, namely\ndaily logarithmic returns of IBM stocks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:31:25 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["van der Merwe", "A. J.", ""], ["von Maltitz", "M. J.", ""], ["Meyer", "J. H.", ""]]}, {"id": "2104.07537", "submitter": "Augusto Fasano", "authors": "Augusto Fasano and Giovanni Rebaudo", "title": "Variational Inference for the Smoothing Distribution in Dynamic Probit\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Fasano, Rebaudo, Durante and Petrone (2019) provided closed-form\nexpressions for the filtering, predictive and smoothing distributions of\nmultivariate dynamic probit models, leveraging on unified skew-normal\ndistribution properties. This allows to develop algorithms to draw independent\nand identically distributed samples from such distributions, as well as\nsequential Monte Carlo procedures for the filtering and predictive\ndistributions, allowing to overcome computational bottlenecks that may arise\nfor large sample sizes. In this paper, we briefly review the above-mentioned\nclosed-form expressions, mainly focusing on the smoothing distribution of the\nunivariate dynamic probit. We develop a variational Bayes approach, extending\nthe partially factorized mean-field variational approximation introduced by\nFasano, Durante and Zanella (2019) for the static binary probit model to the\ndynamic setting. Results are shown for a financial application.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:46:00 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 13:44:18 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Fasano", "Augusto", ""], ["Rebaudo", "Giovanni", ""]]}, {"id": "2104.07552", "submitter": "Mariya Naumova", "authors": "Vladimir Gurvich, Mariya Naumova", "title": "Logical contradictions in the One-way ANOVA and Tukey-Kramer multiple\n  comparisons tests with more than two groups of observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that the One-way ANOVA and Tukey-Kramer (TK) tests agree on any\nsample with two groups. This result is based on a simple identity connecting\nthe Fisher-Snedecor and studentized probabilistic distributions and is proven\nwithout any additional assumptions; in particular, the standard ANOVA\nassumptions (independence, normality, and homoscedasticity (INAH)) are not\nneeded. In contrast, it is known that for a sample with k > 2 groups of\nobservations, even under the INAH assumptions, with the same significance level\n$\\alpha$, the above two tests may give opposite results: (i) ANOVA rejects its\nnull hypothesis $H_0^{A}: \\mu_1 = \\ldots = \\mu_k$, while the TK one,\n$H_0^{TK}(i,j): \\mu_i = \\mu_j$, is not rejected for any pair $i, j \\in \\{1,\n\\ldots, k\\}$; (ii) the TK test rejects $H_0^{TK}(i,j)$ for a pair $(i, j)$\n(with $i \\neq j$) while ANOVA does not reject $H_0^{A}$. We construct two large\ninfinite pseudo-random families of samples of both types satisfying INAH: in\ncase (i) for any $k \\geq 3$ and in case (ii) for some larger $k$. Furthermore,\nin case (ii) ANOVA, being restricted to the pair of groups $(i,j)$, may reject\nequality $\\mu_i = \\mu_j$ with the same $\\alpha$. This is an obvious\ncontradiction, since $\\mu_1 = \\ldots = \\mu_k$ implies $\\mu_i = \\mu_j$ for all\n$i, j \\in \\{1, \\ldots, k\\}.$ Similar contradictory examples are constructed for\nthe Multivariable Linear Regression (MLR). However, for these constructions it\nseems difficult to verify the Gauss-Markov assumptions, which are standardly\nrequired for MLR.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:06:57 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 22:26:09 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Gurvich", "Vladimir", ""], ["Naumova", "Mariya", ""]]}, {"id": "2104.07575", "submitter": "David Mori\\~na Prof.", "authors": "David Mori\\~na, Amanda Fern\\'andez-Fontelo, Alejandra Caba\\~na,\n  Argimiro Arratia and Pedro Puig", "title": "Bayesian Synthetic Likelihood Estimation for Underreported\n  Non-Stationary Time Series: Covid-19 Incidence in Spain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The problem of dealing with misreported data is very common in a wide range\nof contexts for different reasons. The current situation caused by the Covid-19\nworldwide pandemic is a clear example, where the data provided by official\nsources were not always reliable due to data collection issues and to the high\nproportion of asymptomatic cases. In this work, we explore the performance of\nBayesian Synthetic Likelihood to estimate the parameters of a model capable of\ndealing with misreported information and to reconstruct the most likely\nevolution of the phenomenon. The performance of the proposed methodology is\nevaluated through a comprehensive simulation study and illustrated by\nreconstructing the weekly Covid-19 incidence in each Spanish Autonomous\nCommunity in 2020.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:29:23 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Mori\u00f1a", "David", ""], ["Fern\u00e1ndez-Fontelo", "Amanda", ""], ["Caba\u00f1a", "Alejandra", ""], ["Arratia", "Argimiro", ""], ["Puig", "Pedro", ""]]}, {"id": "2104.07580", "submitter": "Aubain Nzokem PhD", "authors": "A.H.Nzokem", "title": "Fitting Infinitely divisible distribution: Case of Gamma-Variance Model", "comments": "23 pages, 39 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper examines the Fractional Fourier Transform (FRFT) based technique as\na tool for obtaining probability density function and its derivatives, and\nmainly for fitting stochastic model with the fundamental probabilistic\nrelationships of infinite divisibility. The probability density functions are\ncomputed and the distributional proprieties such as leptokurtosis, peakedness,\nand asymmetry are reviewed for Variance-Gamma (VG) model and Compound Poisson\nwith Normal Compounding model. The first and second derivatives of probability\ndensity function of the VG model are also computed in order to build the Fisher\ninformation matrix for the Maximum likelihood method. The VG model has been\nincreasingly used as an alternative to the Classical Lognormal Model (CLM) in\nmodelling asset price. The VG model with fives parameters was estimated by the\nFRFT. The data comes from the daily SPY ETF price data. The Kolmogorov-Smirnov\n(KS) goodness-of-fit shows that the VG model fits better the empirical\ncumulative distribution than the CLM. The best VG model comes from the FRFT\nestimation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:35:54 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 10:57:20 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Nzokem", "A. H.", ""]]}, {"id": "2104.07723", "submitter": "Beste Hamiye Beyaztas", "authors": "Beste Hamiye Beyaztas, Soutir Bandyopadhyay, Abhijit Mandal", "title": "A robust specification test in linear panel data models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The presence of outlying observations may adversely affect statistical\ntesting procedures that result in unstable test statistics and unreliable\ninferences depending on the distortion in parameter estimates. In spite of the\nfact that the adverse effects of outliers in panel data models, there are only\na few robust testing procedures available for model specification. In this\npaper, a new weighted likelihood based robust specification test is proposed to\ndetermine the appropriate approach in panel data including individual-specific\ncomponents. The proposed test has been shown to have the same asymptotic\ndistribution as that of most commonly used Hausman's specification test under\nnull hypothesis of random effects specification. The finite sample properties\nof the robust testing procedure are illustrated by means of Monte Carlo\nsimulations and an economic-growth data from the member countries of the\nOrganisation for Economic Co-operation and Development. Our records reveal that\nthe robust specification test exhibit improved performance in terms of size and\npower of the test in the presence of contamination.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 19:12:36 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Beyaztas", "Beste Hamiye", ""], ["Bandyopadhyay", "Soutir", ""], ["Mandal", "Abhijit", ""]]}, {"id": "2104.07752", "submitter": "Fabrizio Leisen", "authors": "Patrizia Berti, Emanuela Dreassi, Fabrizio Leisen, Luca Pratelli,\n  Pietro Rigo", "title": "New perspectives on knockoffs construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\Lambda$ be the collection of all probability distributions for\n$(X,\\widetilde{X})$, where $X$ is a fixed random vector and $\\widetilde{X}$\nranges over all possible knockoff copies of $X$ (in the sense of\n\\cite{CFJL18}). Three topics are developed in this note: (i) A new\ncharacterization of $\\Lambda$ is proved; (ii) A certain subclass of $\\Lambda$,\ndefined in terms of copulas, is introduced; (iii) The (meaningful) special case\nwhere the components of $X$ are conditionally independent is treated in depth.\nIn real problems, after observing $X=x$, each of points (i)-(ii)-(iii) may be\nuseful to generate a value $\\widetilde{x}$ for $\\widetilde{X}$ conditionally on\n$X=x$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:20:00 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Berti", "Patrizia", ""], ["Dreassi", "Emanuela", ""], ["Leisen", "Fabrizio", ""], ["Pratelli", "Luca", ""], ["Rigo", "Pietro", ""]]}, {"id": "2104.07773", "submitter": "Emma Jingfei Zhang", "authors": "Biao Cai, Jingfei Zhang and Will Wei Sun", "title": "Heterogeneous Tensor Mixture Models in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of jointly modeling and clustering populations of\ntensors by introducing a flexible high-dimensional tensor mixture model with\nheterogeneous covariances. The proposed mixture model exploits the intrinsic\nstructures of tensor data, and is assumed to have means that are low-rank and\ninternally sparse as well as heterogeneous covariances that are separable and\nconditionally sparse. We develop an efficient high-dimensional\nexpectation-conditional-maximization (HECM) algorithm that breaks the\nchallenging optimization in the M-step into several simpler conditional\noptimization problems, each of which is convex, admits regularization and has\nclosed-form updating formulas. We show that the proposed HECM algorithm, with\nan appropriate initialization, converges geometrically to a neighborhood that\nis within statistical precision of the true parameter. Such a theoretical\nanalysis is highly nontrivial due to the dual non-convexity arising from both\nthe EM-type estimation and the non-convex objective function in the M-step. The\nefficacy of our proposed method is demonstrated through simulation studies and\nan application to an autism spectrum disorder study, where our analysis\nidentifies important brain regions for diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:06:16 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Cai", "Biao", ""], ["Zhang", "Jingfei", ""], ["Sun", "Will Wei", ""]]}, {"id": "2104.07806", "submitter": "Jacques Balayla", "authors": "Jacques Balayla", "title": "The SIR-P Model: An Illustration of the Screening Paradox", "comments": "arXiv admin note: text overlap with arXiv:2011.06032", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In previous work by this author, the screening paradox - the loss of\npredictive power of screening tests over time $t$ - was mathematically\nformalized using Bayesian theory. Where $J$ is Youden's statistic, $b$ is the\nspecificity of the screening test and $\\phi$ is the prevalence of disease, the\nratio of positive predictive values at subsequent time $k$, $\\rho(\\phi_{k})$,\nover the original $\\rho(\\phi_{0})$ at $t_0$ is given by:\n  $\\zeta(\\phi_{0},k) = \\frac{\\rho(\\phi_{k})}{\\rho(\\phi_{0})}\n=\\frac{\\phi_k(1-b)+J\\phi_0\\phi_k}{\\phi_0(1-b)+J\\phi_0\\phi_k}$\n  Herein, we modify the traditional Kermack-McKendrick SIR Model to include the\nfluctuation of the positive predictive value $\\rho(\\phi)$ (PPV) of a screening\ntest over time as a function of the prevalence threshold $\\phi_e$. We term this\nmodified model the SIR-P model. Where a = sensitivity, b = specificity, $S$ =\nnumber susceptible, $I$ = number infected, $R$ = number recovered/dead, $\\beta$\n= infectious rate, $\\gamma$ = recovery rate, and $N$ is the total number in the\npopulation, the predictive value $\\rho(\\phi,t)$ over time $t$ is given by:\n  $\\rho(\\phi,t) = \\frac{a[\\frac{\\beta IS}{N}-\\gamma I]}{ a[\\frac{\\beta\nIS}{N}-\\gamma I]+(1-b)(1-[\\frac{\\beta IS}{N}-\\gamma I])}$\n  Otherwise stated:\n  $\\rho(\\phi,t) = \\frac{a\\frac{dI}{dt}}{\na\\frac{dI}{dt}+(1-b)(1-\\frac{dI}{dt})}$ where $\\frac{dI}{dt}$ is the\nfluctuation of infected individuals over time $t$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 22:31:32 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Balayla", "Jacques", ""]]}, {"id": "2104.07822", "submitter": "Bo Zhang", "authors": "Shuxiao Chen, Bo Zhang", "title": "Estimating and Improving Dynamic Treatment Regimes With a Time-Varying\n  Instrumental Variable", "comments": "67 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating dynamic treatment regimes (DTRs) from retrospective observational\ndata is challenging as some degree of unmeasured confounding is often expected.\nIn this work, we develop a framework of estimating properly defined \"optimal\"\nDTRs with a time-varying instrumental variable (IV) when unmeasured covariates\nconfound the treatment and outcome, rendering the potential outcome\ndistributions only partially identified. We derive a novel Bellman equation\nunder partial identification, use it to define a generic class of estimands\n(termed IV-optimal DTRs), and study the associated estimation problem. We then\nextend the IV-optimality framework to tackle the policy improvement problem,\ndelivering IV-improved DTRs that are guaranteed to perform no worse and\npotentially better than a pre-specified baseline DTR. Importantly, our\nIV-improvement framework opens up the possibility of strictly improving upon\nDTRs that are optimal under the no unmeasured confounding assumption (NUCA). We\ndemonstrate via extensive simulations the superior performance of IV-optimal\nand IV-improved DTRs over the DTRs that are optimal only under the NUCA. In a\nreal data example, we embed retrospective observational registry data into a\nnatural, two-stage experiment with noncompliance using a time-varying IV and\nestimate useful IV-optimal DTRs that assign mothers to high-level or low-level\nneonatal intensive care units based on their prognostic variables.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 23:44:39 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chen", "Shuxiao", ""], ["Zhang", "Bo", ""]]}, {"id": "2104.07834", "submitter": "Harlan Campbell", "authors": "Harlan Campbell and Paul Gustafson", "title": "re:Linde et al. (2021) -- The Bayes factor, HDI-ROPE and frequentist\n  equivalence testing are actually all equivalent", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Following an extensive simulation study comparing the operating\ncharacteristics of three different procedures used for establishing equivalence\n(the frequentist `TOST\", the Bayesian \"HDI-ROPE\", and the Bayes factor interval\nnull procedure), Linde et al. (2021) conclude with the recommendation that\n\"researchers rely more on the Bayes factor interval null approach for\nquantifying evidence for equivalence.\" We redo the simulation study of Linde et\nal. (2021) in its entirety but with the different procedures calibrated to have\nthe same predetermined maximum type 1 error rate. Our results suggest that the\nBayes Factor, HDI-ROPE, and frequentist equivalence testing are all essentially\nequivalent when it comes to predicting equivalence.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 00:53:41 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Campbell", "Harlan", ""], ["Gustafson", "Paul", ""]]}, {"id": "2104.07870", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro, Wanli Qiao, Lin Zheng", "title": "Estimation of the Global Mode of a Density: Minimaxity, Adaptation, and\n  Computational Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of the global mode of a density under some decay\nrate condition around the global mode. We show that the maximum of a histogram,\nwith proper choice of bandwidth, achieves the minimax rate that we establish\nfor the setting that we consider. This is based on knowledge of the decay rate.\nAddressing the situation where the decay rate is unknown, we propose a\nmultiscale variant consisting in the recursive refinement of a histogram, which\nis shown to be minimax adaptive. These methods run in linear time, and we prove\nin an appendix that this is best possible: There is no estimation procedure\nthat runs in sublinear time that achieves the minimax rate.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 03:08:24 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Qiao", "Wanli", ""], ["Zheng", "Lin", ""]]}, {"id": "2104.07985", "submitter": "Georgia Papacharalampous", "authors": "Georgia Papacharalampous, Andreas Langousis", "title": "Probabilistic water demand forecasting using quantile regression\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine and statistical learning algorithms can be reliably automated and\napplied at scale. Therefore, they can constitute a considerable asset for\ndesigning practical forecasting systems, such as those related to urban water\ndemand. Quantile regression algorithms are statistical and machine learning\nalgorithms that can provide probabilistic forecasts in a straightforward way,\nand have not been applied so far for urban water demand forecasting. In this\nwork, we aim to fill this gap by automating and extensively comparing several\nquantile-regression-based practical systems for probabilistic one-day ahead\nurban water demand forecasting. For designing the practical systems, we use\nfive individual algorithms (i.e., the quantile regression, linear boosting,\ngeneralized random forest, gradient boosting machine and quantile regression\nneural network algorithms), their mean combiner and their median combiner. The\ncomparison is conducted by exploiting a large urban water flow dataset, as well\nas several types of hydrometeorological time series (which are considered as\nexogenous predictor variables in the forecasting setting). The results mostly\nfavour the practical systems designed using the linear boosting algorithm,\nprobably due to the presence of trends in the urban water flow time series. The\nforecasts of the mean and median combiners are also found to be skilful in\ngeneral terms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 09:17:00 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Papacharalampous", "Georgia", ""], ["Langousis", "Andreas", ""]]}, {"id": "2104.08141", "submitter": "Ikuo Fukuda", "authors": "Ikuo Fukuda and Kei Moritsugu", "title": "Analysis of multiple data sequences with different distributions:\n  defining common principal component axes by ergodic sequence generation and\n  multiple reweighting composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.bio-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Principal component analysis (PCA) defines a reduced space described by PC\naxes for a given multidimensional-data sequence to capture the variations of\nthe data. In practice, we need multiple data sequences that accurately obey\nindividual probability distributions and for a fair comparison of the sequences\nwe need PC axes that are common for the multiple sequences but properly capture\nthese multiple distributions. For these requirements, we present individual\nergodic samplings for these sequences and provide special reweighting for\nrecovering the target distributions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 14:43:30 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Fukuda", "Ikuo", ""], ["Moritsugu", "Kei", ""]]}, {"id": "2104.08156", "submitter": "Eliane Maalouf", "authors": "Eliane Maalouf, David Ginsbourger and Niklas Linde", "title": "Fast ABC with joint generative modelling and subset simulation", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for solving inverse-problems with\nhigh-dimensional inputs and an expensive forward mapping. It leverages joint\ndeep generative modelling to transfer the original problem spaces to a lower\ndimensional latent space. By jointly modelling input and output variables and\nendowing the latent with a prior distribution, the fitted probabilistic model\nindirectly gives access to the approximate conditional distributions of\ninterest. Since model error and observational noise with unknown distributions\nare common in practice, we resort to likelihood-free inference with Approximate\nBayesian Computation (ABC). Our method calls on ABC by Subset Simulation to\nexplore the regions of the latent space with dissimilarities between generated\nand observed outputs below prescribed thresholds. We diagnose the diversity of\napproximate posterior solutions by monitoring the probability content of these\nregions as a function of the threshold. We further analyze the curvature of the\nresulting diagnostic curve to propose an adequate ABC threshold. When applied\nto a cross-borehole tomography example from geophysics, our approach delivers\npromising performance without using prior knowledge of the forward nor of the\nnoise distribution.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:03:23 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Maalouf", "Eliane", ""], ["Ginsbourger", "David", ""], ["Linde", "Niklas", ""]]}, {"id": "2104.08157", "submitter": "Robin Tu", "authors": "Robin Tu, Alexander H. Foss, Sihai D. Zhao", "title": "Capturing patterns of variation unique to a specific dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Capturing patterns of variation present in a dataset is important in\nexploratory data analysis and unsupervised learning. Contrastive dimension\nreduction methods, such as contrastive principal component analysis (cPCA),\nfind patterns unique to a target dataset of interest by contrasting with a\ncarefully chosen background dataset representing unwanted or uninteresting\nvariation. However, such methods typically require a tuning parameter that\ngoverns the level of contrast, and it is unclear how to choose this parameter\nobjectively. Furthermore, it is frequently of interest to contrast against\nmultiple backgrounds, which is difficult to accomplish with existing methods.\nWe propose unique component analysis (UCA), a tuning-free method that\nidentifies low-dimensional representations of a target dataset relative to one\nor more comparison datasets. It is computationally efficient even with large\nnumbers of features. We show in several experiments that UCA with a single\nbackground dataset achieves similar results compared to cPCA with various\ntuning parameters, and that UCA with multiple individual background datasets is\nsuperior to both cPCA with any single background data and cPCA with a pooled\nbackground dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:07:32 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Tu", "Robin", ""], ["Foss", "Alexander H.", ""], ["Zhao", "Sihai D.", ""]]}, {"id": "2104.08279", "submitter": "Stephen Bates", "authors": "Stephen Bates, Emmanuel Cand\\`es, Lihua Lei, Yaniv Romano, Matteo\n  Sesia", "title": "Testing for Outliers with Conformal p-values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the construction of p-values for nonparametric outlier\ndetection, taking a multiple-testing perspective. The goal is to test whether\nnew independent samples belong to the same distribution as a reference data set\nor are outliers. We propose a solution based on conformal inference, a broadly\napplicable framework which yields p-values that are marginally valid but\nmutually dependent for different test points. We prove these p-values are\npositively dependent and enable exact false discovery rate control, although in\na relatively weak marginal sense. We then introduce a new method to compute\np-values that are both valid conditionally on the training data and independent\nof each other for different test points; this paves the way to stronger type-I\nerror guarantees. Our results depart from classical conformal inference as we\nleverage concentration inequalities rather than combinatorial arguments to\nestablish our finite-sample guarantees. Furthermore, our techniques also yield\na uniform confidence bound for the false positive rate of any outlier detection\nalgorithm, as a function of the threshold applied to its raw statistics.\nFinally, the relevance of our results is demonstrated by numerical experiments\non real and simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:59:21 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 16:31:16 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bates", "Stephen", ""], ["Cand\u00e8s", "Emmanuel", ""], ["Lei", "Lihua", ""], ["Romano", "Yaniv", ""], ["Sesia", "Matteo", ""]]}, {"id": "2104.08300", "submitter": "Daniel Scharfstein", "authors": "Daniel O. Scharfstein, Razieh Nabi, Edward H. Kennedy, Ming-Yueh\n  Huang, Matteo Bonvini, Marcela Smid", "title": "Semiparametric Sensitivity Analysis: Unmeasured Confounding In\n  Observational Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Establishing cause-effect relationships from observational data often relies\non untestable assumptions. It is crucial to know whether, and to what extent,\nthe conclusions drawn from non-experimental studies are robust to potential\nunmeasured confounding. In this paper, we focus on the average causal effect\n(ACE) as our target of inference. We build on the work of Franks et al.\n(2019)and Robins (2000) by specifying non-identified sensitivity parameters\nthat govern a contrast between the conditional (on measured covariates)\ndistributions of the outcome under treatment (control) between treated and\nuntreated individuals. We use semiparametric theory to derive the\nnon-parametric efficient influence function of the ACE, for fixed sensitivity\nparameters. We use this influence function to construct a one-step\nbias-corrected estimator of the ACE. Our estimator depends on semiparametric\nmodels for the distribution of the observed data; importantly, these models do\nnot impose any restrictions on the values of sensitivity analysis parameters.\nWe establish sufficient conditions ensuring that our estimator has root-n\nasymptotics. We use our methodology to evaluate the causal effect of smoking\nduring pregnancy on birth weight. We also evaluate the performance of\nestimation procedure in a simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:12:32 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Scharfstein", "Daniel O.", ""], ["Nabi", "Razieh", ""], ["Kennedy", "Edward H.", ""], ["Huang", "Ming-Yueh", ""], ["Bonvini", "Matteo", ""], ["Smid", "Marcela", ""]]}, {"id": "2104.08302", "submitter": "Louis H. Y. Chen", "authors": "Louis H. Y. Chen", "title": "Stein's method of normal approximation: Some recollections and\n  reflections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a short exposition of Stein's method of normal approximation\nfrom my personal perspective. It focuses mainly on the characterization of the\nnormal distribution and the construction of Stein identities. Through examples,\nit provides glimpses into the many approaches to constructing Stein identities\nand the diverse applications of Stein's method to mathematical problems. It\nalso includes anecdotes of historical interest, including how Stein discovered\nhis method and how I found an unpublished proof of his of the Berry-Esseen\ntheorem.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:15:52 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Louis H. Y.", ""]]}, {"id": "2104.08344", "submitter": "Shuxi Zeng", "authors": "Shuxi Zeng, Elizabeth C.Lange, Elizabeth A.Archie, Fernando A.Campos,\n  Susan C.Alberts, Fan Li", "title": "Causal Mediation Analysis for Longitudinal Mediators and Survival\n  Outcomes", "comments": "30 pages, 6 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2007.01796", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis studies how the treatment effect of an exposure on\noutcomes is mediated through intermediate variables. Although many applications\ninvolve longitudinal data, the existing methods are not directly applicable to\nsettings where the mediators are measured on irregular time grids. In this\npaper, we propose a causal mediation method that accommodates longitudinal\nmediators on arbitrary time grids and survival outcomes simultaneously. We take\na functional data analysis perspective and view longitudinal mediators as\nrealizations of underlying smooth stochastic processes. We define causal\nestimands of direct and indirect effects accordingly and provide corresponding\nidentification assumptions. We employ a functional principal component analysis\napproach to estimate the mediator process, and propose a Cox hazard model for\nthe survival outcome that flexibly adjusts the mediator process. We then derive\na g-computation formula to express the causal estimands using the model\ncoefficients. The proposed method is applied to a longitudinal data set from\nthe Amboseli Baboon Research Project to investigate the causal relationships\nbetween early adversity, adult physiological stress responses, and survival\namong wild female baboons. We find that adversity experienced in early life has\na significant direct effect on females' life expectancy and survival\nprobability, but find little evidence that these effects were mediated by\nmarkers of the stress response in adulthood. We further developed a sensitivity\nanalysis method to assess the impact of potential violation to the key\nassumption of sequential ignorability.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 19:49:25 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zeng", "Shuxi", ""], ["Lange", "Elizabeth C.", ""], ["Archie", "Elizabeth A.", ""], ["Campos", "Fernando A.", ""], ["Alberts", "Susan C.", ""], ["Li", "Fan", ""]]}, {"id": "2104.08402", "submitter": "Emil Alfred Edgar Mendoza", "authors": "Emil Mendoza, Fabian Dunker, Marco Reale", "title": "Regularized Maximum Likelihood Estimation for the Random Coefficients\n  Model", "comments": "23 Pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The random coefficients model $Y_i={\\beta_0}_i+{\\beta_1}_i\n{X_1}_i+{\\beta_2}_i {X_2}_i+\\ldots+{\\beta_d}_i {X_d}_i$, with $\\mathbf{X}_i$,\n$Y_i$, $\\mathbf{\\beta}_i$ i.i.d, and $\\mathbf{\\beta}_i$ independent of $X_i$ is\noften used to capture unobserved heterogeneity in a population. We propose a\nquasi-maximum likelihood method to estimate the joint density distribution of\nthe random coefficient model. This method implicitly involves the inversion of\nthe Radon transformation in order to reconstruct the joint distribution, and\nhence is an inverse problem. Nonparametric estimation for the joint density of\n$\\mathbf{\\beta}_i=({\\beta_0}_i,\\ldots, {\\beta_d}_i)$ based on kernel methods or\nFourier inversion have been proposed in recent years. Most of these methods\nassume a heavy tailed design density $f_\\mathbf{X}$. To add stability to the\nsolution, we apply regularization methods. We analyze the convergence of the\nmethod without assuming heavy tails for $f_\\mathbf{X}$ and illustrate\nperformance by applying the method on simulated and real data. To add stability\nto the solution, we apply a Tikhonov-type regularization method.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 23:09:55 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mendoza", "Emil", ""], ["Dunker", "Fabian", ""], ["Reale", "Marco", ""]]}, {"id": "2104.08408", "submitter": "Yue Wang", "authors": "Yue Wang, Ali Shojaie, Timothy W. Randolph, and Jing Ma", "title": "Generalized Matrix Decomposition Regression: Estimation and Inference\n  for Two-way Structured Data", "comments": "41 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper studies high-dimensional regression with two-way structured data.\nTo estimate the high-dimensional coefficient vector, we propose the generalized\nmatrix decomposition regression (GMDR) to efficiently leverage any auxiliary\ninformation on row and column structures. The GMDR extends the principal\ncomponent regression (PCR) to two-way structured data, but unlike PCR, the GMDR\nselects the components that are most predictive of the outcome, leading to more\naccurate prediction. For inference on regression coefficients of individual\nvariables, we propose the generalized matrix decomposition inference (GMDI), a\ngeneral high-dimensional inferential framework for a large family of estimators\nthat include the proposed GMDR estimator. GMDI provides more flexibility for\nmodeling relevant auxiliary row and column structures. As a result, GMDI does\nnot require the true regression coefficients to be sparse; it also allows\ndependent and heteroscedastic observations. We study the theoretical properties\nof GMDI in terms of both the type-I error rate and power and demonstrate the\neffectiveness of GMDR and GMDI on simulation studies and an application to\nhuman microbiome data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 23:53:14 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Yue", ""], ["Shojaie", "Ali", ""], ["Randolph", "Timothy W.", ""], ["Ma", "Jing", ""]]}, {"id": "2104.08525", "submitter": "Sangita Das", "authors": "Sangita Das and Suchandan Kayal", "title": "On comparison of the second-order statistics from independent and\n  interdependent exponentiated location-scale distributed random variables", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider two batches of independent or interdependent exponentiated\nlocation-scale distributed heterogeneous random variables. This article\ninvestigates ordering results for the second-order statistics from these\nbatches when a vector of parameters is switched to another vector of parameters\nin the specified model. Sufficient conditions for the usual stochastic order\nand the hazard rate order are derived. Some applications of the established\nresults are presented.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 12:28:50 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Das", "Sangita", ""], ["Kayal", "Suchandan", ""]]}, {"id": "2104.08595", "submitter": "Haim Bar", "authors": "Haim Bar and James Booth and Martin T. Wells", "title": "Mixed Effect Modeling and Variable Selection for Quantile Regression", "comments": "arXiv admin note: text overlap with arXiv:1910.11479", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the estimating equations for quantile regression (QR) can be\nsolved using an EM algorithm in which the M-step is computed via weighted least\nsquares, with weights computed at the E-step as the expectation of independent\ngeneralized inverse-Gaussian variables. This fact is exploited here to extend\nQR to allow for random effects in the linear predictor. Convergence of the\nalgorithm in this setting is established by showing that it is a generalized\nalternating minimization (GAM) procedure. Another modification of the EM\nalgorithm also allows us to adapt a recently proposed method for variable\nselection in mean regression models to the QR setting. Simulations show the\nresulting method significantly outperforms variable selection in QR models\nusing the lasso penalty. Applications to real data include a frailty QR\nanalysis of hospital stays, and variable selection for age at onset of lung\ncancer and for riboflavin production rate using high-dimensional gene\nexpression arrays for prediction.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 16:45:50 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bar", "Haim", ""], ["Booth", "James", ""], ["Wells", "Martin T.", ""]]}, {"id": "2104.08611", "submitter": "Sangita Das", "authors": "Sangita Das and Suchandan Kayal", "title": "Some new ordering results on stochastic comparisons of second largest\n  order statistics from independent and interdependent heterogeneous\n  distributions", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The second-largest order statistic is of special importance in reliability\ntheory since it represents the time to failure of a $2$-out-of-$n$ system.\nConsider two $2$-out-of-$n$ systems with heterogeneous random lifetimes. The\nlifetimes are assumed to follow heterogeneous general exponentiated\nlocation-scale models. In this communication, the usual stochastic and reversed\nhazard rate orders between the systems' lifetimes are established under two\ncases. For the case of independent random lifetimes, the usual stochastic order\nand the reversed hazard rate order between the second-largest order statistics\nare obtained by using the concept of vector majorization and related orders.\nFor the dependent case, the conditions under which the usual stochastic order\nbetween the second-largest order statistics holds are investigated. To\nillustrate the theoretical findings, some special cases of the exponentiated\nlocation-scale model are considered.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 18:26:10 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Das", "Sangita", ""], ["Kayal", "Suchandan", ""]]}, {"id": "2104.08687", "submitter": "Dan M. Kluger", "authors": "Dan M. Kluger and Art B. Owen", "title": "A central limit theorem for the Benjamini-Hochberg false discovery\n  proportion under a factor model", "comments": "Main changes in version 2: i) restated Corollary 1 in a way that is\n  clearer and easier to use, ii) removed a regularity condition for our\n  theorems (in particular we removed Condition 2 from version 1), and iii) we\n  added a couple of remarks (namely, Remark 1 and 6 in version 2). Throughout\n  the text we also fixed typos, improved clarity, and added a some additional\n  commentary and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Benjamini-Hochberg (BH) procedure remains widely popular despite having\nlimited theoretical guarantees in the commonly encountered scenario of\ncorrelated test statistics. Of particular concern is the possibility that the\nmethod could exhibit bursty behavior, meaning that it might typically yield no\nfalse discoveries while occasionally yielding both a large number of false\ndiscoveries and a false discovery proportion (FDP) that far exceeds its own\nwell controlled mean. In this paper, we investigate which test statistic\ncorrelation structures lead to bursty behavior and which ones lead to well\ncontrolled FDPs. To this end, we develop a central limit theorem for the FDP in\na multiple testing setup where the test statistic correlations can be either\nshort-range or long-range as well as either weak or strong. The theorem and our\nsimulations from a data-driven factor model suggest that the BH procedure\nexhibits severe burstiness when the test statistics have many strong,\nlong-range correlations, but does not otherwise.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 02:52:35 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 00:56:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kluger", "Dan M.", ""], ["Owen", "Art B.", ""]]}, {"id": "2104.08784", "submitter": "Seong-Hee Kim", "authors": "A.B. Dieker, Seong-Hee Kim", "title": "Efficient Fully Sequential Indifference-Zone Procedures Using Properties\n  of Multidimensional Brownian Motion Exiting a Sphere", "comments": "37 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a ranking and selection (R&S) problem with the goal to select a\nsystem with the largest or smallest expected performance measure among a number\nof simulated systems with a pre-specified probability of correct selection.\nFully sequential procedures take one observation from each survived system and\neliminate inferior systems when there is clear statistical evidence that they\nare inferior. Most fully sequential procedures make elimination decisions based\non sample performances of each possible pair of survived systems and exploit\nthe bound crossing properties of a univariate Brownian motion. In this paper,\nwe present new fully sequential procedures with elimination decisions that are\nbased on sample performances of all competing systems. Using properties of a\nmultidimensional Brownian motion exiting a sphere, we derive heuristics that\naim to achieve a given target probability of correct selection. We show that in\npractice the new procedures significantly outperform a widely used fully\nsequential procedure. Compared to BIZ, a recent fully-sequential procedure that\nuses statistics inspired by Bayes posterior probabilities, our procedures have\nbetter performance under difficult mean or variance configurations but similar\nperformance under easy mean configurations.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 09:25:01 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dieker", "A. B.", ""], ["Kim", "Seong-Hee", ""]]}, {"id": "2104.08959", "submitter": "TrungTin Nguyen", "authors": "TrungTin Nguyen, Faicel Chamroukhi, Hien Duy Nguyen, Florence Forbes", "title": "Non-asymptotic model selection in block-diagonal mixture of polynomial\n  experts models", "comments": "Corrected typos. Extended results from arXiv:2104.02640. arXiv admin\n  note: substantial text overlap with arXiv:2104.02640", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection, via penalized likelihood type criteria, is a standard task\nin many statistical inference and machine learning problems. Progress has led\nto deriving criteria with asymptotic consistency results and an increasing\nemphasis on introducing non-asymptotic criteria. We focus on the problem of\nmodeling non-linear relationships in regression data with potential hidden\ngraph-structured interactions between the high-dimensional predictors, within\nthe mixture of experts modeling framework. In order to deal with such a complex\nsituation, we investigate a block-diagonal localized mixture of polynomial\nexperts (BLoMPE) regression model, which is constructed upon an inverse\nregression and block-diagonal structures of the Gaussian expert covariance\nmatrices. We introduce a penalized maximum likelihood selection criterion to\nestimate the unknown conditional density of the regression model. This model\nselection criterion allows us to handle the challenging problem of inferring\nthe number of mixture components, the degree of polynomial mean functions, and\nthe hidden block-diagonal structures of the covariance matrices, which reduces\nthe number of parameters to be estimated and leads to a trade-off between\ncomplexity and sparsity in the model. In particular, we provide a strong\ntheoretical guarantee: a finite-sample oracle inequality satisfied by the\npenalized maximum likelihood estimator with a Jensen-Kullback-Leibler type\nloss, to support the introduced non-asymptotic model selection criterion. The\npenalty shape of this criterion depends on the complexity of the considered\nrandom subcollection of BLoMPE models, including the relevant graph structures,\nthe degree of polynomial mean functions, and the number of mixture components.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 21:32:20 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 21:05:06 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Nguyen", "TrungTin", ""], ["Chamroukhi", "Faicel", ""], ["Nguyen", "Hien Duy", ""], ["Forbes", "Florence", ""]]}, {"id": "2104.08970", "submitter": "Sihai Zhao", "authors": "Yihe Wang and Sihai Dave Zhao", "title": "Linear shrinkage for predicting responses in large-scale multivariate\n  linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a new prediction method for multivariate linear regression\nproblems where the number of features is less than the sample size but the\nnumber of outcomes is extremely large. Many popular procedures, such as\npenalized regression procedures, require parameter tuning that is\ncomputationally untenable in such large-scale problems. We take a different\napproach, motivated by ideas from simultaneous estimation problems, that\nperforms linear shrinkage on ordinary least squares parameter estimates. Our\napproach is extremely computationally efficient and tuning-free. We show that\nit can asymptotically outperform ordinary least squares without any structural\nassumptions on the true regression coefficients and illustrate its good\nperformance in simulations and an analysis of single-cell RNA-seq data.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 22:21:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Yihe", ""], ["Zhao", "Sihai Dave", ""]]}, {"id": "2104.09090", "submitter": "Wenxian Zhou", "authors": "Wenxian Zhou, Giorgos Bakoyannis, Ying Zhang, and Constantin T.\n  Yiannoutsos", "title": "Semiparametric Marginal Regression for Clustered Competing Risks Data\n  with Missing Cause of Failure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustered competing risks data are commonly encountered in multicenter\nstudies. The analysis of such data is often complicated due to informative\ncluster size, a situation where the outcomes under study are associated with\nthe size of the cluster. In addition, cause of failure is frequently\nincompletely observed in real-world settings. To the best of our knowledge,\nthere is no methodology for population-averaged analysis with clustered\ncompeting risks data with informative cluster size and missing causes of\nfailure. To address this problem, we consider the semiparametric marginal\nproportional cause-specific hazards model and propose a maximum partial\npseudolikelihood estimator under a missing at random assumption. To make the\nlatter assumption more plausible in practice, we allow for auxiliary variables\nthat may be related to the probability of missingness. The proposed method does\nnot impose assumptions regarding the within-cluster dependence and allows for\ninformative cluster size. The asymptotic properties of the proposed estimators\nfor both regression coefficients and infinite-dimensional parameters, such as\nthe marginal cumulative incidence functions, are rigorously established.\nSimulation studies show that the proposed method performs well and that methods\nthat ignore the within-cluster dependence and the informative cluster size lead\nto invalid inferences. The proposed method is applied to competing risks data\nfrom a large multicenter HIV study in sub-Saharan Africa where a significant\nportion of causes of failure is missing.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 07:22:06 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 18:30:48 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Zhou", "Wenxian", ""], ["Bakoyannis", "Giorgos", ""], ["Zhang", "Ying", ""], ["Yiannoutsos", "Constantin T.", ""]]}, {"id": "2104.09237", "submitter": "Nathan Sandholtz", "authors": "Nathan Sandholtz, Yohsuke Miyamoto, Luke Bornn, Maurice Smith", "title": "Inverse Bayesian Optimization: Learning Human Search Strategies in a\n  Sequential Optimization Task", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG math.OC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is a popular algorithm for sequential optimization of a\nlatent objective function when sampling from the objective is costly. The\nsearch path of the algorithm is governed by the acquisition function, which\ndefines the agent's search strategy. Conceptually, the acquisition function\ncharacterizes how the optimizer balances exploration and exploitation when\nsearching for the optimum of the latent objective. In this paper, we explore\nthe inverse problem of Bayesian optimization; we seek to estimate the agent's\nlatent acquisition function based on observed search paths. We introduce a\nprobabilistic solution framework for the inverse problem which provides a\nprincipled framework to quantify both the variability with which the agent\nperforms the optimization task as well as the uncertainty around their\nestimated acquisition function.\n  We illustrate our methods by analyzing human behavior from an experiment\nwhich was designed to force subjects to balance exploration and exploitation in\nsearch of an invisible target location. We find that while most subjects\ndemonstrate clear trends in their search behavior, there is significant\nvariation around these trends from round to round. A wide range of search\nstrategies are exhibited across the subjects in our study, but upper confidence\nbound acquisition functions offer the best fit for the majority of subjects.\nFinally, some subjects do not map well to any of the acquisition functions we\ninitially consider; these subjects tend to exhibit exploration preferences\nbeyond that of standard acquisition functions to capture. Guided by the model\ndiscrepancies, we augment the candidate acquisition functions to yield a\nsuperior fit to the human behavior in this task.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:40:34 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sandholtz", "Nathan", ""], ["Miyamoto", "Yohsuke", ""], ["Bornn", "Luke", ""], ["Smith", "Maurice", ""]]}, {"id": "2104.09282", "submitter": "Ben Van Calster", "authors": "Michael Edlinger, Maarten van Smeden, Hannes F Alber, Maria\n  Wanitschek, Ben Van Calster", "title": "Risk prediction models for discrete ordinal outcomes: calibration and\n  the impact of the proportional odds assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Calibration is a vital aspect of the performance of risk prediction models,\nbut research in the context of ordinal outcomes is scarce. This study compared\ncalibration measures for risk models predicting a discrete ordinal outcome, and\ninvestigated the impact of the proportional odds assumption on calibration and\noverfitting. We studied the multinomial, cumulative, adjacent category,\ncontinuation ratio, and stereotype logit/logistic models. To assess\ncalibration, we investigated calibration intercepts and slopes, calibration\nplots, and the estimated calibration index. Using large sample simulations, we\nstudied the performance of models for risk estimation under various conditions,\nassuming that the true model has either a multinomial logistic form or a\ncumulative logit proportional odds form. Small sample simulations were used to\ncompare the tendency for overfitting between models. As a case study, we\ndeveloped models to diagnose the degree of coronary artery disease (five\ncategories) in symptomatic patients. When the true model was multinomial\nlogistic, proportional odds models often yielded poor risk estimates, with\ncalibration slopes deviating considerably from unity even on large model\ndevelopment datasets. The stereotype logistic model improved the calibration\nslope, but still provided biased risk estimates for individual patients. When\nthe true model had a cumulative logit proportional odds form, multinomial\nlogistic regression provided biased risk estimates, although these biases were\nmodest. Non-proportional odds models, however, required more parameters to be\nestimated from the data, and hence suffered more from overfitting. Despite\nlarger sample size requirements, we generally recommend multinomial logistic\nregression for risk prediction modeling of discrete ordinal outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 13:21:32 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Edlinger", "Michael", ""], ["van Smeden", "Maarten", ""], ["Alber", "Hannes F", ""], ["Wanitschek", "Maria", ""], ["Van Calster", "Ben", ""]]}, {"id": "2104.09323", "submitter": "Tobias Hatt", "authors": "Tobias Hatt, Stefan Feuerriegel", "title": "Sequential Deconfounding for Causal Inference with Unobserved\n  Confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using observational data to estimate the effect of a treatment is a powerful\ntool for decision-making when randomized experiments are infeasible or costly.\nHowever, observational data often yields biased estimates of treatment effects,\nsince treatment assignment can be confounded by unobserved variables. A remedy\nis offered by deconfounding methods that adjust for such unobserved\nconfounders. In this paper, we develop the Sequential Deconfounder, a method\nthat enables estimating individualized treatment effects over time in presence\nof unobserved confounders. This is the first deconfounding method that can be\nused in a general sequential setting (i.e., with one or more treatments\nassigned at each timestep). The Sequential Deconfounder uses a novel Gaussian\nprocess latent variable model to infer substitutes for the unobserved\nconfounders, which are then used in conjunction with an outcome model to\nestimate treatment effects over time. We prove that using our method yields\nunbiased estimates of individualized treatment responses over time. Using\nsimulated and real medical data, we demonstrate the efficacy of our method in\ndeconfounding the estimation of treatment responses over time.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 09:56:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hatt", "Tobias", ""], ["Feuerriegel", "Stefan", ""]]}, {"id": "2104.09358", "submitter": "Arun Kuchibhotla", "authors": "Arun K. Kuchibhotla, Richard A. Berk", "title": "Nested Conformal Prediction Sets for Classification with Applications to\n  Probation Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk assessments to help inform criminal justice decisions have been used in\nthe United States since the 1920s. Over the past several years, statistical\nlearning risk algorithms have been introduced amid much controversy about\nfairness, transparency and accuracy. In this paper, we focus on accuracy for a\nlarge department of probation and parole that is considering a major revision\nof its current, statistical learning risk methods. Because the content of each\noffender's supervision is substantially shaped by a forecast of subsequent\nconduct, forecasts have real consequences. Here we consider the probability\nthat risk forecasts are correct. We augment standard statistical learning\nestimates of forecasting uncertainty (i.e., confusion tables) with uncertainty\nestimates from nested conformal prediction sets. In a demonstration of concept\nusing data from the department of probation and parole, we show that the\nstandard uncertainty measures and uncertainty measures from nested conformal\nprediction sets can differ dramatically in concept and output. We also provide\na modification of nested conformal called the localized conformal method to\nmatch confusion tables more closely when possible. A strong case can be made\nfavoring the nested and localized conformal approach. As best we can tell, our\nformulation of such comparisons and consequent recommendations is novel.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:42:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kuchibhotla", "Arun K.", ""], ["Berk", "Richard A.", ""]]}, {"id": "2104.09371", "submitter": "Aniruddha Rajendra Rao", "authors": "Aniruddha Rajendra Rao and Matthew Reimherr", "title": "Non-linear Functional Modeling using Neural Networks", "comments": "3 figures, 8 tables (including supplementary material), 13 pages\n  (including supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new class of non-linear models for functional data based on\nneural networks. Deep learning has been very successful in non-linear modeling,\nbut there has been little work done in the functional data setting. We propose\ntwo variations of our framework: a functional neural network with continuous\nhidden layers, called the Functional Direct Neural Network (FDNN), and a second\nversion that utilizes basis expansions and continuous hidden layers, called the\nFunctional Basis Neural Network (FBNN). Both are designed explicitly to exploit\nthe structure inherent in functional data. To fit these models we derive a\nfunctional gradient based optimization algorithm. The effectiveness of the\nproposed methods in handling complex functional models is demonstrated by\ncomprehensive simulation studies and real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:59:55 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rao", "Aniruddha Rajendra", ""], ["Reimherr", "Matthew", ""]]}, {"id": "2104.09418", "submitter": "Laya Ghodrati", "authors": "Laya Ghodrati and Victor M. Panaretos", "title": "Distribution-on-Distribution Regression via Optimal Transport Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a framework for performing regression when both covariate and\nresponse are probability distributions on a compact interval\n$\\Omega\\subset\\mathbb{R}$. Our regression model is based on the theory of\noptimal transportation and links the conditional Fr\\'echet mean of the response\ndistribution to the covariate distribution via an optimal transport map. We\ndefine a Fr\\'echet-least-squares estimator of this regression map, and\nestablish its consistency and rate of convergence to the true map, under both\nfull and partial observation of the regression pairs. Computation of the\nestimator is shown to reduce to an isotonic regression problem, and thus our\nregression model can be implemented with ease. We illustrate our methodology\nusing real and simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:10:08 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ghodrati", "Laya", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "2104.09452", "submitter": "Vincent Pisztora", "authors": "Vincent Pisztora, Yanglan Ou, Xiaolei Huang, Francesca Chiaromonte,\n  Jia Li", "title": "Epsilon Consistent Mixup: An Adaptive Consistency-Interpolation Tradeoff", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose $\\epsilon$-Consistent Mixup ($\\epsilon$mu).\n$\\epsilon$mu is a data-based structural regularization technique that combines\nMixup's linear interpolation with consistency regularization in the Mixup\ndirection, by compelling a simple adaptive tradeoff between the two. This\nlearnable combination of consistency and interpolation induces a more flexible\nstructure on the evolution of the response across the feature space and is\nshown to improve semi-supervised classification accuracy on the SVHN and\nCIFAR10 benchmark datasets, yielding the largest gains in the most challenging\nlow label-availability scenarios. Empirical studies comparing $\\epsilon$mu and\nMixup are presented and provide insight into the mechanisms behind\n$\\epsilon$mu's effectiveness. In particular, $\\epsilon$mu is found to produce\nmore accurate synthetic labels and more confident predictions than Mixup.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:10:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Pisztora", "Vincent", ""], ["Ou", "Yanglan", ""], ["Huang", "Xiaolei", ""], ["Chiaromonte", "Francesca", ""], ["Li", "Jia", ""]]}, {"id": "2104.09474", "submitter": "Richard Warr", "authors": "Katherine A. Batterton, Christine M. Schubert, Richard L. Warr", "title": "Exact Confidence Intervals for Linear Combinations of Multinomial\n  Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Linear combinations of multinomial probabilities, such as those resulting\nfrom contingency tables, are of use when evaluating classification system\nperformance. While large sample inference methods for these combinations exist,\nsmall sample methods exist only for regions on the multinomial parameter space\ninstead of the linear combinations. However, in medical classification problems\nit is common to have small samples necessitating a small sample confidence\ninterval on linear combinations of multinomial probabilities. Therefore, in\nthis paper we derive an exact confidence interval, through the use of fiducial\ninference, for linear combinations of multinomial probabilities. Simulation\ndemonstrates the presented interval's adherence to exact coverage.\nAdditionally, an adjustment to the exact interval is provided, giving shorter\nlengths while still achieving better coverage than large sample methods.\nComputational efficiencies in estimation of the exact interval are achieved\nthrough the application of a fast Fourier transform and combining a numerical\nsolver and stochastic optimizer to find solutions. The exact confidence\ninterval presented in this paper allows for comparisons between diagnostic\nmethods previously unavailable, demonstrated through an example of diagnosing\nchronic allograph nephropathy in post kidney transplant patients.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:34:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Batterton", "Katherine A.", ""], ["Schubert", "Christine M.", ""], ["Warr", "Richard L.", ""]]}, {"id": "2104.09549", "submitter": "Michael Shvartsman", "authors": "Lucy Owen, Jonathan Browder, Benjamin Letham, Gideon Stocek, Chase\n  Tymms and Michael Shvartsman", "title": "Adaptive Nonparametric Psychophysics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce a new set of models and adaptive psychometric testing methods\nfor multidimensional psychophysics. In contrast to traditional adaptive\nstaircase methods like PEST and QUEST, the method is multi-dimensional and does\nnot require a grid over contextual dimensions, retaining sub-exponential\nscaling in the number of stimulus dimensions. In contrast to more recent\nmulti-dimensional adaptive methods, our underlying model does not require a\nparametric assumption about the interaction between intensity and the\nadditional dimensions. In addition, we introduce a new active sampling policy\nthat explicitly targets psychometric detection threshold estimation and does so\nsubstantially faster than policies that attempt to estimate the full\npsychometric function (though it still provides estimates of the function,\nalbeit with lower accuracy). Finally, we introduce AEPsych, a user-friendly\nopen-source package for nonparametric psychophysics that makes these\ntechnically-challenging methods accessible to the broader community.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 18:19:59 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Owen", "Lucy", ""], ["Browder", "Jonathan", ""], ["Letham", "Benjamin", ""], ["Stocek", "Gideon", ""], ["Tymms", "Chase", ""], ["Shvartsman", "Michael", ""]]}, {"id": "2104.09668", "submitter": "Andrew White", "authors": "Rainier Barrett, Mehrad Ansari, Gourab Ghoshal, Andrew D White", "title": "Simulation-Based Inference with Approximately Correct Parameters via\n  Maximum Entropy", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the input parameters of simulators from observations is a crucial\nchallenge with applications from epidemiology to molecular dynamics. Here we\nshow a simple approach in the regime of sparse data and approximately correct\nmodels, which is common when trying to use an existing model to infer latent\nvariables with observed data. This approach is based on the principle of\nmaximum entropy and provably makes the smallest change in the latent joint\ndistribution to accommodate new data. This simple method requires no likelihood\nor simulator derivatives and its fit is insensitive to prior strength, removing\nthe need to balance observed data fit with prior belief. We demonstrate this\nMaxEnt approach and compare with other likelihood-free inference methods across\nthree systems; a linear simulator with Gaussian noise, a point particle moving\nin a gravitational field, and finally a compartmental mode of epidemic spread.\nWe demonstrate that our method compares favorably, and in some cases exceeds\nthe performance of other methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 22:17:38 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Barrett", "Rainier", ""], ["Ansari", "Mehrad", ""], ["Ghoshal", "Gourab", ""], ["White", "Andrew D", ""]]}, {"id": "2104.09730", "submitter": "Joshua Warren", "authors": "Joshua L. Warren, Howard H. Chang, Lauren K. Warren, Matthew J.\n  Strickland, Lyndsey A. Darrow, James A. Mulholland", "title": "Critical Window Variable Selection for Mixtures: Estimating the Impact\n  of Multiple Air Pollutants on Stillbirth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the role of time-varying pollution mixtures on human health is\ncritical as people are simultaneously exposed to multiple pollutants during\ntheir lives. For vulnerable sub-populations who have well-defined exposure\nperiods (e.g., pregnant women), questions regarding critical windows of\nexposure to these mixtures are important for mitigating harm. We extend\nCritical Window Variable Selection (CWVS) to the multipollutant setting by\nintroducing CWVS for Mixtures (CWVSmix), a hierarchical Bayesian method that\ncombines smoothed variable selection and temporally correlated weight\nparameters to (i) identify critical windows of exposure to mixtures of\ntime-varying pollutants, (ii) estimate the time-varying relative importance of\neach individual pollutant and their first order interactions within the\nmixture, and (iii) quantify the impact of the mixtures on health. Through\nsimulation, we show that CWVSmix offers the best balance of performance in each\nof these categories in comparison to competing methods. Using these approaches,\nwe investigate the impact of exposure to multiple ambient air pollutants on the\nrisk of stillbirth in New Jersey, 2005-2014. We find consistent elevated risk\nin gestational weeks 2, 16-17, and 20 for non-Hispanic Black mothers, with\npollution mixtures dominated by ammonium (weeks 2, 17, 20), nitrate (weeks 2,\n17), nitrogen oxides (weeks 2, 16), PM2.5 (week 2), and sulfate (week 20). The\nmethod is available in the R package CWVSmix.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 02:56:59 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Warren", "Joshua L.", ""], ["Chang", "Howard H.", ""], ["Warren", "Lauren K.", ""], ["Strickland", "Matthew J.", ""], ["Darrow", "Lyndsey A.", ""], ["Mulholland", "James A.", ""]]}, {"id": "2104.09783", "submitter": "Martin Hartmann", "authors": "Petri Toiviainen and Martin Hartmann", "title": "Analyzing multidimensional movement interaction with generalized\n  cross-wavelet transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans can synchronize with musical events whilst coordinating their\nmovements with others. Interpersonal entrainment phenomena, such as dance,\ninvolve multiple body parts and movement directions. Along with being\nmultidimensional, dance movement interaction is plurifrequential, since it can\noccur at different frequencies simultaneously. Moreover, it is prone to\nnonstationarity, due to, for instance, displacements around the dance floor.\nVarious methodological approaches have been adopted to study entrainment, but\nonly spectrogram-based techniques allow for an integral analysis thereof. This\narticle proposes an alternative approach based upon the cross-wavelet\ntransform, a technique for nonstationary and plurifrequential analysis of\nunivariate interaction. The presented approach generalizes the cross-wavelet\ntransform to multidimensional signals. It allows to identify, for different\nfrequencies of movement, interaction estimates of interaction and\nleader-follower dynamics across body parts and movement directions. Further,\nthe generalized cross-wavelet transform can be used to quantify the\nfrequency-wise contribution of individual body parts and movement directions to\noverall synchrony. The article provides a thorough mathematical description of\nthe method and includes proofs of its invariance under translation, rotation,\nand reflection. Finally, its properties and performance are illustrated via\nexamples using simulated data and behavioral data collected through a mirror\ngame task and a free dance movement task.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 06:45:16 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Toiviainen", "Petri", ""], ["Hartmann", "Martin", ""]]}, {"id": "2104.09812", "submitter": "Linh Nghiem", "authors": "Linh Nghiem, Francis K.C.Hui, Samuel Mueller, A.H.Welsh", "title": "Screening methods for linear errors-in-variables models in high\n  dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Microarray studies, in order to identify genes associated with an outcome of\ninterest, usually produce noisy measurements for a large number of gene\nexpression features from a small number of subjects. One common approach to\nanalyzing such high-dimensional data is to use linear errors-in-variables\nmodels; however, current methods for fitting such models are computationally\nexpensive. In this paper, we present two efficient screening procedures, namely\ncorrected penalized marginal screening and corrected sure independence\nscreening, to reduce the number of variables for final model building. Both\nscreening procedures are based on fitting corrected marginal regression models\nrelating the outcome to each contaminated covariate separately, which can be\ncomputed efficiently even with a large number of features. Under mild\nconditions, we show that these procedures achieve screening consistency and\nreduce the number of features considerably, even when the number of covariates\ngrows exponentially with the sample size. Additionally, if the true covariates\nare weakly correlated, corrected penalized marginal screening can achieve full\nvariable selection consistency. Through simulation studies and an analysis of\ngene expression data for bone mineral density of Norwegian women, we\ndemonstrate that the two new screening procedures make estimation of linear\nerrors-in-variables models computationally scalable in high dimensional\nsettings, and improve finite sample estimation and selection performance\ncompared with estimators that do not employ a screening stage.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:56:15 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Nghiem", "Linh", ""], ["Hui", "Francis K. C.", ""], ["Mueller", "Samuel", ""], ["Welsh", "A. H.", ""]]}, {"id": "2104.09821", "submitter": "Mahdi Mahdizadeh", "authors": "M. Mahdizadeha and Ehsan Zamanzade", "title": "Using a rank-based design in estimating prevalence of breast cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is highly important for governments and health organizations to monitor\nthe prevalence of breast cancer as a leading source of cancer-related death\namong women. However, the accurate diagnosis of this disease is expensive,\nespecially in developing countries. This article concerns a cost-efficient\nmethod for estimating prevalence of breast cancer, when diagnosis is based on a\ncomprehensive biopsy procedure. Multistage ranked set sampling (MSRSS) is\nutilized to develop a proportion estimator. This design employs some visually\nassessed cytological covariates, which are pertinent to determination of breast\ncancer, so as to provide the experimenter with a more informative sample.\nTheoretical properties of the proposed estimator are explored. Evidence from\nnumerical studies is reported. The developed procedure can be substantially\nmore efficient than its competitor in simple random sampling (SRS). In some\nsituations, the proportion estimation in MSRSS needs around 76% fewer\nobservations than that in SRS, given a precision level. Thus, using MSRSS may\nlead to a considerable reduction in cost with respect to SRS. In many medical\nstudies, e.g. diagnosing breast cancer based on a full biopsy procedure, exact\nquantification is difficult (costly and/or time-consuming), but the potential\nsample units can be ranked fairly accurately without actual measurements. In\nthis setup, multistage ranked set sampling is an appropriate design for\ndeveloping cost-efficient statistical methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 08:18:57 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Mahdizadeha", "M.", ""], ["Zamanzade", "Ehsan", ""]]}, {"id": "2104.09838", "submitter": "Linh Nghiem", "authors": "Linh Nghiem, Francis K.C. Hui, Samuel Mueller, A.H.Welsh", "title": "Sparse Sliced Inverse Regression via Cholesky Matrix Penalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new sparse sliced inverse regression estimator called Cholesky\nmatrix penalization and its adaptive version for achieving sparsity in\nestimating the dimensions of the central subspace. The new estimators use the\nCholesky decomposition of the covariance matrix of the covariates and include a\nregularization term in the objective function to achieve sparsity in a\ncomputationally efficient manner. We establish the theoretical values of the\ntuning parameters that achieve estimation and variable selection consistency\nfor the central subspace. Furthermore, we propose a new projection information\ncriterion to select the tuning parameter for our proposed estimators and prove\nthat the new criterion facilitates selection consistency. The Cholesky matrix\npenalization estimator inherits the strength of the Matrix Lasso and the Lasso\nsliced inverse regression estimator; it has superior performance in numerical\nstudies and can be adapted to other sufficient dimension methods in the\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 08:54:49 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Nghiem", "Linh", ""], ["Hui", "Francis K. C.", ""], ["Mueller", "Samuel", ""], ["Welsh", "A. H.", ""]]}, {"id": "2104.09890", "submitter": "Salvatore Greco", "authors": "Diogo Cunha Ferreira and Jos\\`e RUi Figueira and Salvatore Greco and\n  Rui Marques", "title": "Data Envelopment Analysis models with imperfect knowledge of input and\n  output values: An application to Portuguese public hospitals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the technical efficiency of a set of observations requires that the\nassociated data composed of inputs and outputs are perfectly known. If this is\nnot the case, then biased estimates will likely be obtained. Data Envelopment\nAnalysis (DEA) is one of the most extensively used mathematical models to\nestimate efficiency. It constructs a piecewise linear frontier against which\nall observations are compared. Since the frontier is empirically defined, any\ndeviation resulting from low data quality (imperfect knowledge of data or IKD)\nmay lead to efficiency under/overestimation. In this study, we model IKD and,\nthen, apply the so-called Hit \\& Run procedure to randomly generate admissible\nobservations, following some prespecified probability density functions. Sets\nused to model IKD limit the domain of data associated with each observation.\nAny point belonging to that domain is a candidate to figure out as the\nobservation for efficiency assessment. Hence, this sampling procedure must run\na sizable number of times (infinite, in theory) in such a way that it populates\nthe whole sets. The DEA technique is used during the execution of each\niteration to estimate bootstrapped efficiency scores for each observation. We\nuse some scenarios to show that the proposed routine can outperform some of the\navailable alternatives. We also explain how efficiency estimations can be used\nfor statistical inference. An empirical case study based on the Portuguese\npublic hospitals database (2013-2016) was addressed using the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 10:53:24 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ferreira", "Diogo Cunha", ""], ["Figueira", "Jos\u00e8 RUi", ""], ["Greco", "Salvatore", ""], ["Marques", "Rui", ""]]}, {"id": "2104.10021", "submitter": "Ziyi Li", "authors": "Ziyi Li, Yijuan Huang, Dattatraya Patil, Martin G. Sanda", "title": "Covariate adjustment in continuous biomarker assessment", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Continuous biomarkers are common for disease screening and diagnosis. To\nreach a dichotomous clinical decision, a threshold would be imposed to\ndistinguish subjects with disease from non-diseased individuals. Among various\nperformance metrics for a continuous biomarker, specificity at a controlled\nsensitivity level (or vice versa) is often desirable for clinical utility since\nit directly targets where the clinical test is intended to operate. Covariates,\nsuch as age, race, and sample collection, could impact the controlled\nsensitivity level in subpopulations and may also confound the association\nbetween biomarker and disease status. Therefore, covariate adjustment is\nimportant in such biomarker evaluation. In this paper, we suggest to adopt a\nparsimonious quantile regression model for the diseased population, locally at\nthe controlled sensitivity level, and assess specificity with\ncovariate-specific control of the sensitivity. Variance estimates are obtained\nfrom a sample-based approach and bootstrap. Furthermore, our proposed local\nmodel extends readily to a global one for covariate adjustment for the receiver\noperating characteristic (ROC) curve over the sensitivity continuum. We\ndemonstrate computational efficiency of this proposed method and restore the\ninherent monotonicity in the estimated covariate-adjusted ROC curve. The\nasymptotic properties of the proposed estimators are established. Simulation\nstudies show favorable performance of the proposal. Finally, we illustrate our\nmethod in biomarker evaluation for aggressive prostate cancer.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:05:19 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Ziyi", ""], ["Huang", "Yijuan", ""], ["Patil", "Dattatraya", ""], ["Sanda", "Martin G.", ""]]}, {"id": "2104.10070", "submitter": "Natalie Klein", "authors": "Natalie Klein, Joshua H. Siegle, Tobias Teichert, Robert E. Kass", "title": "Cross-population coupling of neural activity based on Gaussian process\n  current source densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Because LFPs arise from multiple sources in different spatial locations, they\ndo not easily reveal coordinated activity across neural populations on a\ntrial-to-trial basis. As we show here, however, once disparate source signals\nare decoupled, their trial-to-trial fluctuations become more accessible, and\ncross-population correlations become more apparent. To decouple sources we\nintroduce a general framework for estimation of current source densities\n(CSDs). In this framework, the set of LFPs result from noise being added to the\ntransform of the CSD by a biophysical forward model, while the CSD is\nconsidered to be the sum of a zero-mean, stationary, spatiotemporal Gaussian\nprocess, having fast and slow components, and a mean function, which is the sum\nof multiple time-varying functions distributed across space, each varying\nacross trials. We derived biophysical forward models relevant to the data we\nanalyzed. In simulation studies this approach improved identification of source\nsignals compared to existing CSD estimation methods. Using data recorded from\nprimate auditory cortex, we analyzed trial-to-trial fluctuations in both\nsteady-state and task-evoked signals. We found cortical layer-specific phase\ncoupling between two probes and showed that the same analysis applied directly\nto LFPs did not recover these patterns. We also found task-evoked CSDs to be\ncorrelated across probes, at specific cortical depths. Using data from\nNeuropixels probes in mouse visual areas, we again found evidence for\ndepth-specific phase coupling of areas V1 and LM based on the CSDs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:46:56 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Klein", "Natalie", ""], ["Siegle", "Joshua H.", ""], ["Teichert", "Tobias", ""], ["Kass", "Robert E.", ""]]}, {"id": "2104.10099", "submitter": "Jack Storror Carter", "authors": "Jack Storror Carter, David Rossell, Jim Q. Smith", "title": "Partial Correlation Graphical LASSO", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard likelihood penalties to learn Gaussian graphical models are based on\nregularising the off-diagonal entries of the precision matrix. Such methods,\nand their Bayesian counterparts, are not invariant to scalar multiplication of\nthe variables, unless one standardises the observed data to unit sample\nvariances. We show that such standardisation can have a strong effect on\ninference and introduce a new family of penalties based on partial\ncorrelations. We show that the latter, as well as the maximum likelihood, $L_0$\nand logarithmic penalties are scale invariant. We illustrate the use of one\nsuch penalty, the partial correlation graphical LASSO, which sets an $L_{1}$\npenalty on partial correlations. The associated optimization problem is no\nlonger convex, but is conditionally convex. We show via simulated examples and\nin two real datasets that, besides being scale invariant, there can be\nimportant gains in terms of inference.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:32:46 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Carter", "Jack Storror", ""], ["Rossell", "David", ""], ["Smith", "Jim Q.", ""]]}, {"id": "2104.10125", "submitter": "Alexander Bond", "authors": "A. J. Bond, C. B. Beggs", "title": "Bisecting for selecting: using a Laplacian eigenmaps clustering approach\n  to create the new European football Super League", "comments": "24 pages, 9 Figures, 3 Tables, 1 Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We use European football performance data to select teams to form the\nproposed European football Super League, using only unsupervised techniques. We\nfirst used random forest regression to select important variables predicting\ngoal difference, which we used to calculate the Euclidian distances between\nteams. Creating a Laplacian eigenmap, we bisected the Fielder vector to\nidentify the five major European football leagues' natural clusters. Our\nresults showed how an unsupervised approach could successfully identify four\nclusters based on five basic performance metrics: shots, shots on target, shots\nconceded, possession, and pass success. The top two clusters identify those\nteams who dominate their respective leagues and are the best candidates to\ncreate the most competitive elite super league.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:12:31 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Bond", "A. J.", ""], ["Beggs", "C. B.", ""]]}, {"id": "2104.10150", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal", "title": "Bayesian subset selection and variable importance for interpretable\n  prediction and classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Subset selection is a valuable tool for interpretable learning, scientific\ndiscovery, and data compression. However, classical subset selection is often\neschewed due to selection instability, computational bottlenecks, and lack of\npost-selection inference. We address these challenges from a Bayesian\nperspective. Given any Bayesian predictive model $\\mathcal{M}$, we elicit\npredictively-competitive subsets using linear decision analysis. The approach\nis customizable for (local) prediction or classification and provides\ninterpretable summaries of $\\mathcal{M}$. A key quantity is the acceptable\nfamily of subsets, which leverages the predictive distribution from\n$\\mathcal{M}$ to identify subsets that offer nearly-optimal prediction. The\nacceptable family spawns new (co-) variable importance metrics based on whether\nvariables (co-) appear in all, some, or no acceptable subsets. Crucially, the\nlinear coefficients for any subset inherit regularization and predictive\nuncertainty quantification via $\\mathcal{M}$. The proposed approach exhibits\nexcellent prediction, interval estimation, and variable selection for simulated\ndata, including $p=400 > n$. These tools are applied to a large education\ndataset with highly correlated covariates, where the acceptable family is\nespecially useful. Our analysis provides unique insights into the combination\nof environmental, socioeconomic, and demographic factors that predict\neducational outcomes, and features highly competitive prediction with\nremarkable stability.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:48:34 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kowal", "Daniel R.", ""]]}, {"id": "2104.10190", "submitter": "Tim G. J. Rudner", "authors": "Tim G. J. Rudner and Vitchyr H. Pong and Rowan McAllister and Yarin\n  Gal and Sergey Levine", "title": "Outcome-Driven Reinforcement Learning via Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While reinforcement learning algorithms provide automated acquisition of\noptimal policies, practical application of such methods requires a number of\ndesign decisions, such as manually designing reward functions that not only\ndefine the task, but also provide sufficient shaping to accomplish it. In this\npaper, we discuss a new perspective on reinforcement learning, recasting it as\nthe problem of inferring actions that achieve desired outcomes, rather than a\nproblem of maximizing rewards. To solve the resulting outcome-directed\ninference problem, we establish a novel variational inference formulation that\nallows us to derive a well-shaped reward function which can be learned directly\nfrom environment interactions. From the corresponding variational objective, we\nalso derive a new probabilistic Bellman backup operator reminiscent of the\nstandard Bellman backup operator and use it to develop an off-policy algorithm\nto solve goal-directed tasks. We empirically demonstrate that this method\neliminates the need to design reward functions and leads to effective\ngoal-directed behaviors.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 18:16:21 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Rudner", "Tim G. J.", ""], ["Pong", "Vitchyr H.", ""], ["McAllister", "Rowan", ""], ["Gal", "Yarin", ""], ["Levine", "Sergey", ""]]}, {"id": "2104.10222", "submitter": "Qingying Zong", "authors": "Qingying Zong and Jonathan R. Bradley", "title": "Constrained Bayesian Hierarchical Models for Gaussian Data: A Model\n  Selection Criterion Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the setting where there are B>1 candidate statistical models, and\none is interested in model selection. Two common approaches to solve this\nproblem are to select a single model or to combine the candidate models through\nmodel averaging. Instead, we select a subset of the combined parameter space\nassociated with the models. Specifically, a model averaging perspective is used\nto increase the parameter space, and a model selection criterion is used to\nselect a subset of this expanded parameter space. We account for the\nvariability of the criterion by adapting Yekutieli (2012)'s method to Bayesian\nmodel averaging (BMA). Yekutieli (2012)'s method treats model selection as a\ntruncation problem. We truncate the joint support of the data and the parameter\nspace to only include small values of the covariance penalized error (CPE)\ncriterion. The CPE is a general expression that contains several information\ncriteria as special cases. Simulation results show that as long as the\ntruncated set does not have near zero probability, we tend to obtain lower mean\nsquared error than BMA. Additional theoretical results are provided that\nprovide the foundation for these observations. We apply our approach to a\ndataset consisting of American Community Survey (ACS) period estimates to\nillustrate that this perspective can lead to improvements of a single model.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 19:42:46 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zong", "Qingying", ""], ["Bradley", "Jonathan R.", ""]]}, {"id": "2104.10298", "submitter": "Olivia Bernstein", "authors": "Olivia M. Bernstein, Brian G. Vegetabile, Christian R. Salazar, Joshua\n  D. Grill, and Daniel L. Gillen", "title": "Adjustment for Biased Sampling Using NHANES Derived Propensity Weights", "comments": "20 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Consent-to-Contact (C2C) registry at the University of California, Irvine\ncollects data from community participants to aid in the recruitment to clinical\nresearch studies. Self-selection into the C2C likely leads to bias due in part\nto enrollees having more years of education relative to the US general\npopulation. Salazar et al. (2020) recently used the C2C to examine associations\nof race/ethnicity with participant willingness to be contacted about research\nstudies. To address questions about generalizability of estimated associations\nwe estimate propensity for self-selection into the convenience sample weights\nusing data from the National Health and Nutrition Examination Survey (NHANES).\nWe create a combined dataset of C2C and NHANES subjects and compare different\napproaches (logistic regression, covariate balancing propensity score, entropy\nbalancing, and random forest) for estimating the probability of membership in\nC2C relative to NHANES. We propose methods to estimate the variance of\nparameter estimates that account for uncertainty that arises from estimating\npropensity weights. Simulation studies explore the impact of propensity weight\nestimation on uncertainty. We demonstrate the approach by repeating the\nanalysis by Salazar et al. with the deduced propensity weights for the C2C\nsubjects and contrast the results of the two analyses. This method can be\nimplemented using our estweight package in R available on GitHub.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 01:10:18 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Bernstein", "Olivia M.", ""], ["Vegetabile", "Brian G.", ""], ["Salazar", "Christian R.", ""], ["Grill", "Joshua D.", ""], ["Gillen", "Daniel L.", ""]]}, {"id": "2104.10302", "submitter": "Kristen Hunter", "authors": "Kristen B. Hunter, Kristen Koenig, Marie-Ab\\`ele Bind", "title": "Conceptualizing experimental controls using the potential outcomes\n  framework", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The goal of a well-controlled study is to remove unwanted variation when\nestimating the causal effect of the intervention of interest. Experiments\nconducted in the basic sciences frequently achieve this goal using experimental\ncontrols, such as \"negative\" and \"positive\" controls, which are measurements\ndesigned to detect systematic sources of unwanted variation. Here, we introduce\nclear, mathematically precise definitions of experimental controls using\npotential outcomes. Our definitions provide a unifying statistical framework\nfor fundamental concepts of experimental design from the biological and other\nbasic sciences. These controls are defined in terms of whether assumptions are\nbeing made about a specific treatment level, outcome, or contrast between\noutcomes. We discuss experimental controls as tools for researchers to wield in\ndesigning experiments and detecting potential design flaws, including using\ncontrols to diagnose unintended factors that influence the outcome of interest,\nassess measurement error, and identify important subpopulations. We believe\nthat experimental controls are powerful tools for reproducible research that\nare possibly underutilized by statisticians, epidemiologists, and social\nscience researchers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 01:17:44 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Hunter", "Kristen B.", ""], ["Koenig", "Kristen", ""], ["Bind", "Marie-Ab\u00e8le", ""]]}, {"id": "2104.10335", "submitter": "Arkaprava Roy", "authors": "Arkaprava Roy, Shubhashis Ghosal", "title": "Optimal Bayesian Smoothing of Functional Observations over a Large Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern contexts, some types of data are observed in high-resolution,\nessentially continuously in time. Such data units are best described as taking\nvalues in a space of functions. Subject units carrying the observations may\nhave intrinsic relations among themselves, and are best described by the nodes\nof a large graph. It is often sensible to think that the underlying signals in\nthese functional observations vary smoothly over the graph, in that neighboring\nnodes have similar underlying signals. This qualitative information allows\nborrowing of strength over neighboring nodes and consequently leads to more\naccurate inference. In this paper, we consider a model with Gaussian functional\nobservations and adopt a Bayesian approach to smoothing over the nodes of the\ngraph. We characterize the minimax rate of estimation in terms of the\nregularity of the signals and their variation across nodes quantified in terms\nof the graph Laplacian. We show that an appropriate prior constructed from the\ngraph Laplacian can attain the minimax bound, while using a mixture prior, the\nminimax rate up to a logarithmic factor can be attained simultaneously for all\npossible values of functional and graphical smoothness. We also show that in\nthe fixed smoothness setting, an optimal sized credible region has arbitrarily\nhigh frequentist coverage. A simulation experiment demonstrates that the method\nperforms better than potential competing methods like the random forest. The\nmethod is also applied to a dataset on daily temperatures measured at several\nweather stations in the US state of North Carolina.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 03:22:22 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 03:01:31 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 21:36:01 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Roy", "Arkaprava", ""], ["Ghosal", "Shubhashis", ""]]}, {"id": "2104.10436", "submitter": "Silvia Columbu", "authors": "Silvia Columbu, Paolo Frumento, Matteo Bottai", "title": "Modeling sign concordance of quantile regression residuals with multiple\n  outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression permits describing how quantiles of a scalar response\nvariable depend on a set of predictors. Because a unique definition of\nmultivariate quantiles is lacking, extending quantile regression to\nmultivariate responses is somewhat complicated. In this paper, we describe a\nsimple approach based on a two-step procedure: in the first step, quantile\nregression is applied to each response separately; in the second step, the\njoint distribution of the signs of the residuals is modeled through multinomial\nregression. The described approach does not require a multidimensional\ndefinition of quantiles, and can be used to capture important features of a\nmultivariate response and assess the effects of covariates on the correlation\nstructure. We apply the proposed method to analyze two different datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 10:01:55 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Columbu", "Silvia", ""], ["Frumento", "Paolo", ""], ["Bottai", "Matteo", ""]]}, {"id": "2104.10554", "submitter": "Hengrui Cai", "authors": "Hengrui Cai, Wenbin Lu, Rui Song", "title": "Calibrated Optimal Decision Making with Multiple Data Sources and\n  Limited Outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimal decision-making problem in a primary sample of\ninterest with multiple auxiliary sources available. The outcome of interest is\nlimited in the sense that it is only observed in the primary sample. In\nreality, such multiple data sources may belong to different populations and\nthus cannot be combined directly. This paper proposes a novel calibrated\noptimal decision rule (CODR) to address the limited outcome, by leveraging the\nshared pattern in multiple data sources. Under a mild and testable assumption\nthat the conditional means of intermediate outcomes in different samples are\nequal given baseline covariates and the treatment information, we can show that\nthe calibrated mean outcome of interest under the CODR is unbiased and more\nefficient than using the primary sample solely. Extensive experiments on\nsimulated datasets demonstrate empirical validity and improvement of the\nproposed CODR, followed by a real application on the MIMIC-III as the primary\nsample with auxiliary data from eICU.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:24:17 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cai", "Hengrui", ""], ["Lu", "Wenbin", ""], ["Song", "Rui", ""]]}, {"id": "2104.10573", "submitter": "Hengrui Cai", "authors": "Hengrui Cai, Rui Song, Wenbin Lu", "title": "GEAR: On Optimal Decision Making with Auxiliary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized optimal decision making, finding the optimal decision rule (ODR)\nbased on individual characteristics, has attracted increasing attention\nrecently in many fields, such as education, economics, and medicine. Current\nODR methods usually require the primary outcome of interest in samples for\nassessing treatment effects, namely the experimental sample. However, in many\nstudies, treatments may have a long-term effect, and as such the primary\noutcome of interest cannot be observed in the experimental sample due to the\nlimited duration of experiments, which makes the estimation of ODR impossible.\nThis paper is inspired to address this challenge by making use of an auxiliary\nsample to facilitate the estimation of ODR in the experimental sample. We\npropose an auGmented inverse propensity weighted Experimental and Auxiliary\nsample-based decision Rule (GEAR) by maximizing the augmented inverse\npropensity weighted value estimator over a class of decision rules using the\nexperimental sample, with the primary outcome being imputed based on the\nauxiliary sample. The asymptotic properties of the proposed GEAR estimators and\ntheir associated value estimators are established. Simulation studies are\nconducted to demonstrate its empirical validity with a real AIDS application.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:59:25 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cai", "Hengrui", ""], ["Song", "Rui", ""], ["Lu", "Wenbin", ""]]}, {"id": "2104.10618", "submitter": "Qingyuan Zhao", "authors": "Yao Zhang, Qingyuan Zhao", "title": "Multiple conditional randomization tests", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for (multiple) conditional randomization tests\nthat incorporate several important ideas in the recent literature. We establish\na general sufficient condition on the construction of multiple conditional\nrandomization tests under which their p-values are \"independent\", in the sense\nthat their joint distribution stochastically dominates the product of uniform\ndistributions under the null. Conceptually, we argue that randomization should\nbe understood as the mode of inference precisely based on randomization. We\nshow that under a change of perspective, many existing statistical methods,\nincluding permutation tests for (conditional) independence and conformal\nprediction, are special cases of the general conditional randomization test.\nThe versatility of our framework is further illustrated with an example\nconcerning lagged treatment effects in stepped-wedge randomized trials.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 16:25:43 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 15:41:46 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhang", "Yao", ""], ["Zhao", "Qingyuan", ""]]}, {"id": "2104.10633", "submitter": "Wing Hung Wong", "authors": "Wing Hung Wong", "title": "A calculus for causal inference with instrumental variables", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Under a general structural equation framework for causal inference, we\nprovide a definition of the causal effect of a variable X on another variable\nY, and propose an approach to estimate this causal effect via the use of\ninstrumental variables.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 16:59:12 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 06:02:33 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wong", "Wing Hung", ""]]}, {"id": "2104.10673", "submitter": "Tobias Fissler", "authors": "Tobias Fissler and Yannick Hoga", "title": "Backtesting Systemic Risk Forecasts using Multi-Objective Elicitability", "comments": "61 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.MF stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Backtesting risk measure forecasts requires identifiability (for model\nvalidation) and elicitability (for model comparison). The systemic risk\nmeasures CoVaR (conditional value-at-risk), CoES (conditional expected\nshortfall) and MES (marginal expected shortfall), measuring the risk of a\nposition $Y$ given that a reference position $X$ is in distress, fail to be\nidentifiable and elicitable. We establish the joint identifiability of CoVaR,\nMES and (CoVaR, CoES) together with the value-at-risk (VaR) of the reference\nposition $X$, but show that an analogue result for elicitability fails. The\nnovel notion of multi-objective elicitability however, relying on multivariate\nscores equipped with an order, leads to a positive result when using the\nlexicographic order on $\\mathbb{R}^2$. We establish comparative backtests of\nDiebold--Mariano type for superior systemic risk forecasts and comparable VaR\nforecasts, accompanied by a traffic-light approach. We demonstrate the\nviability of these backtesting approaches in simulations and in an empirical\napplication to DAX 30 and S&P 500 returns.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:02:06 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 12:12:20 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 11:44:53 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fissler", "Tobias", ""], ["Hoga", "Yannick", ""]]}, {"id": "2104.10735", "submitter": "Robert Bassett", "authors": "Robert Bassett, Jacob Foster, Kay L. Gemba, Paul Leary, Kevin B. Smith", "title": "The Maximal Eigengap Estimator for Acoustic Vector-Sensor Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the maximal eigengap estimator for finding the\ndirection of arrival of a wideband acoustic signal using a single\nvector-sensor. We show that in this setting narrowband cross-spectral density\nmatrices can be combined in an optimal weighting that approximately maximizes\nsignal-to-noise ratio across a wide frequency band. The signal subspace\nresulting from this optimal combination of narrowband power matrices defines\nthe maximal eigengap estimator. We discuss the advantages of the maximal\neigengap estimator over competing methods, and demonstrate its utility in a\nreal-data application using signals collected in 2019 from an acoustic\nvector-sensor deployed in the Monterey Bay.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 19:43:23 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 21:52:57 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Bassett", "Robert", ""], ["Foster", "Jacob", ""], ["Gemba", "Kay L.", ""], ["Leary", "Paul", ""], ["Smith", "Kevin B.", ""]]}, {"id": "2104.10750", "submitter": "Ksheera Sagar K N", "authors": "Ksheera Sagar K. N, Sayantan Banerjee, Jyotishka Datta, Anindya Bhadra", "title": "Precision Matrix Estimation under the Horseshoe-like Prior-Penalty Dual", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of precision matrix estimation in a multivariate Gaussian model\nis fundamental to network estimation. Although there exist both Bayesian and\nfrequentist approaches to this, it is difficult to obtain good Bayesian and\nfrequentist properties under the same prior-penalty dual, complicating\njustification. It is well known, for example, that the Bayesian version of the\npopular lasso estimator has poor posterior concentration properties. To bridge\nthis gap for the precision matrix estimation problem, our contribution is a\nnovel prior-penalty dual that closely approximates the popular graphical\nhorseshoe prior and penalty, and performs well in both Bayesian and frequentist\nsenses. A chief difficulty with the horseshoe prior is a lack of closed form\nexpression of the density function, which we overcome in this article, allowing\nus to directly study the penalty function. In terms of theory, we establish\nposterior convergence rate of the precision matrix that matches the oracle\nrate, in addition to the frequentist consistency of the maximum a posteriori\nestimator. In addition, our results also provide theoretical justifications for\npreviously developed approaches that have been unexplored so far, e.g. for the\ngraphical horseshoe prior. Computationally efficient Expectation Conditional\nMaximization and Markov chain Monte Carlo algorithms are developed respectively\nfor the penalized likelihood and fully Bayesian estimation problems, using the\nsame latent variable framework. In numerical experiments, the horseshoe-based\napproaches echo their superior theoretical properties by comprehensively\noutperforming the competing methods. A protein-protein interaction network\nestimation in B-cell lymphoma is considered to validate the proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:30:38 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["N", "Ksheera Sagar K.", ""], ["Banerjee", "Sayantan", ""], ["Datta", "Jyotishka", ""], ["Bhadra", "Anindya", ""]]}, {"id": "2104.10770", "submitter": "Zeyu Wei", "authors": "Zeyu Wei and Yen-Chi Chen", "title": "Skeleton Clustering: Dimension-Free Density-based Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a density-based clustering method called skeleton clustering\nthat can detect clusters in multivariate and even high-dimensional data with\nirregular shapes. To bypass the curse of dimensionality, we propose surrogate\ndensity measures that are less dependent on the dimension but have intuitive\ngeometric interpretations. The clustering framework constructs a concise\nrepresentation of the given data as an intermediate step and can be thought of\nas a combination of prototype methods, density-based clustering, and\nhierarchical clustering. We show by theoretical analysis and empirical studies\nthat the skeleton clustering leads to reliable clusters in multivariate and\nhigh-dimensional scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 21:25:02 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Wei", "Zeyu", ""], ["Chen", "Yen-Chi", ""]]}, {"id": "2104.10784", "submitter": "Alejandro Schuler", "authors": "Alejandro Schuler", "title": "Designing efficient randomized trials: power and sample size calculation\n  when using semiparametric efficient estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trials enroll a large number of subjects in order to attain power, making\nthem expensive and time-consuming. Sample size calculations are often performed\nwith the assumption of an unadjusted analysis, even if the trial analysis plan\nspecifies a more efficient estimator (e.g. ANCOVA). This leads to conservative\nestimates of required sample sizes and an opportunity for savings. Here we show\nthat a relatively simple formula can be used to estimate the power of any\ntwo-arm, single-timepoint trial analyzed with a semiparametric efficient\nestimator, regardless of the domain of the outcome or kind of treatment effect\n(e.g. odds ratio, mean difference). Since an efficient estimator attains the\nminimum possible asymptotic variance, this allows for the design of trials that\nare as small as possible while still attaining design power and control of type\nI error. The required sample size calculation is parsimonious and requires the\nanalyst to provide only a small number of population parameters. We verify in\nsimulation that the large-sample properties of trials designed this way attain\ntheir nominal values. Lastly, we demonstrate how to use this formula in the\n\"design\" (and subsequent reanalysis) of a real clinical trial and show that\nfewer subjects are required to attain the same design power when a\nsemiparametric efficient estimator is accounted for at the design stage.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 22:33:32 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 21:19:24 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Schuler", "Alejandro", ""]]}, {"id": "2104.10906", "submitter": "Francisco Javier Rubio", "authors": "Danilo Alvares and Francisco Javier Rubio", "title": "A tractable Bayesian joint model for longitudinal and survival data", "comments": "To appear in Statistics in Medicine. Software available at\n  https://github.com/daniloalvares/Tractable-BJM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a numerically tractable formulation of Bayesian joint models for\nlongitudinal and survival data. The longitudinal process is modelled using\ngeneralised linear mixed models, while the survival process is modelled using a\nparametric general hazard structure. The two processes are linked by sharing\nfixed and random effects, separating the effects that play a role at the time\nscale from those that affect the hazard scale. This strategy allows for the\ninclusion of non-linear and time-dependent effects while avoiding the need for\nnumerical integration, which facilitates the implementation of the proposed\njoint model. We explore the use of flexible parametric distributions for\nmodelling the baseline hazard function which can capture the basic shapes of\ninterest in practice. We discuss prior elicitation based on the interpretation\nof the parameters. We present an extensive simulation study, where we analyse\nthe inferential properties of the proposed models, and illustrate the trade-off\nbetween flexibility, sample size, and censoring. We also apply our proposal to\ntwo real data applications in order to demonstrate the adaptability of our\nformulation both in univariate time-to-event data and in a competing risks\nframework. The methodology is implemented in rstan.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 07:36:40 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Alvares", "Danilo", ""], ["Rubio", "Francisco Javier", ""]]}, {"id": "2104.11039", "submitter": "Lisa Steyer", "authors": "Lisa Steyer, Almond St\\\"ocker, Sonja Greven", "title": "Elastic analysis of irregularly or sparsely sampled curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We provide statistical analysis methods for samples of curves when the image\nbut not the parametrisation of the curves is of interest. A parametrisation\ninvariant analysis can be based on the elastic distance of the curves modulo\nwarping, but existing methods have limitations in common realistic settings\nwhere curves are irregularly and potentially sparsely observed. We provide\nmethods and algorithms to approximate the elastic distance for such curves via\ninterpreting them as polygons. Moreover, we propose to use spline curves for\nmodelling smooth or polygonal Fr\\'echet means of open or closed curves with\nrespect to the elastic distance and show identifiability of the spline model\nmodulo warping. We illustrate the use of our methods for elastic mean and\ndistance computation by application to two datasets. The first application\nclusters sparsely sampled GPS tracks based on the elastic distance and computes\nsmooth means for each cluster to find new paths on Tempelhof field in Berlin.\nThe second classifies irregularly sampled handwritten spirals of Parkinson's\npatients and controls based on the elastic distance to a mean spiral curve\ncomputed using our approach. All developed methods are implemented in the\n\\texttt{R}-package \\texttt{elasdics} and evaluated in simulations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 13:13:12 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Steyer", "Lisa", ""], ["St\u00f6cker", "Almond", ""], ["Greven", "Sonja", ""]]}, {"id": "2104.11283", "submitter": "Yu Yang", "authors": "Hongcheng Liu and Yu Yang", "title": "A Dimension-Insensitive Algorithm for Stochastic Zeroth-Order\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns a convex, stochastic zeroth-order optimization (S-ZOO)\nproblem, where the objective is to minimize the expectation of a cost function\nand its gradient is not accessible directly. To solve this problem, traditional\noptimization techniques mostly yield query complexities that grow polynomially\nwith dimensionality, i.e., the number of function evaluations is a polynomial\nfunction of the number of decision variables. Consequently, these methods may\nnot perform well in solving massive-dimensional problems arising in many modern\napplications. Although more recent methods can be provably\ndimension-insensitive, almost all of them work with arguably more stringent\nconditions such as everywhere sparse or compressible gradient. Thus, prior to\nthis research, it was unknown whether dimension-insensitive S-ZOO is possible\nwithout such conditions. In this paper, we give an affirmative answer to this\nquestion by proposing a sparsity-inducing stochastic gradient-free (SI-SGF)\nalgorithm. It is proved to achieve dimension-insensitive query complexity in\nboth convex and strongly convex cases when neither gradient sparsity nor\ngradient compressibility is satisfied. Our numerical results demonstrate the\nstrong potential of the proposed SI-SGF compared with existing alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 18:56:17 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 01:45:14 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Hongcheng", ""], ["Yang", "Yu", ""]]}, {"id": "2104.11355", "submitter": "Salil Koner", "authors": "Salil Koner, So Young Park, Ana-Maria Staicu", "title": "PROFIT: Projection-based Test in Longitudinal Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many modern applications, a dependent functional response is observed for\neach subject over repeated time, leading to longitudinal functional data. In\nthis paper, we propose a novel statistical procedure to test whether the mean\nfunction varies over time. Our approach relies on reducing the dimension of the\nresponse using data-driven orthogonal projections and it employs a\nlikelihood-based hypothesis testing. We investigate the methodology\ntheoretically and discuss a computationally efficient implementation. The\nproposed test maintains the type I error rate, and shows excellent power to\ndetect departures from the null hypothesis in finite sample simulation studies.\nWe apply our method to the longitudinal diffusion tensor imaging study of\nmultiple sclerosis (MS) patients to formally assess whether the brain's health\ntissue, as summarized by fractional anisotropy (FA) profile, degrades over time\nduring the study period.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 00:03:15 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Koner", "Salil", ""], ["Park", "So Young", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "2104.11358", "submitter": "Giovanni Motta", "authors": "Giovanni Motta", "title": "Joint Mean-Vector and Var-Matrix estimation for Locally Stationary\n  VAR(1) processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the last two decades, locally stationary processes have been widely\nstudied in the time series literature. In this paper we consider the\nlocally-stationary vector-auto-regression model of order one, or LS-VAR(1), and\nestimate its parameters by weighted least squares. The LS-VAR(1) we consider\nallows for a smoothly time-varying non-diagonal VAR matrix, as well as for a\nsmoothly time-varying non-zero mean. The weighting scheme is based on kernel\nsmoothers. The time-varying mean and the time-varying VAR matrix are estimated\njointly, and the definition of the local-linear weighting matrix is provided in\nclosed-from. The quality of the estimated curves is illustrated through\nsimulation results.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 00:37:17 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Motta", "Giovanni", ""]]}, {"id": "2104.11426", "submitter": "Kyubaek Yoon", "authors": "Kyubaek Yoon, Hojun You, Wei-Ying Wu, Chae Young Lim, Jongeun Choi,\n  Connor Boss, Ahmed Ramadan, John M. Popovich Jr., Jacek Cholewicki, N. Peter\n  Reeves, Clark J. Radcliffe", "title": "Regularized Nonlinear Regression for Simultaneously Selecting and\n  Estimating Key Model Parameters", "comments": "13 pages, 4 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In system identification, estimating parameters of a model using limited\nobservations results in poor identifiability. To cope with this issue, we\npropose a new method to simultaneously select and estimate sensitive parameters\nas key model parameters and fix the remaining parameters to a set of typical\nvalues. Our method is formulated as a nonlinear least squares estimator with\nL1-regularization on the deviation of parameters from a set of typical values.\nFirst, we provide consistency and oracle properties of the proposed estimator\nas a theoretical foundation. Second, we provide a novel approach based on\nLevenberg-Marquardt optimization to numerically find the solution to the\nformulated problem. Third, to show the effectiveness, we present an application\nidentifying a biomechanical parametric model of a head position tracking task\nfor 10 human subjects from limited data. In a simulation study, the variances\nof estimated parameters are decreased by 96.1% as compared to that of the\nestimated parameters without L1-regularization. In an experimental study, our\nmethod improves the model interpretation by reducing the number of parameters\nto be estimated while maintaining variance accounted for (VAF) at above 82.5%.\nMoreover, the variances of estimated parameters are reduced by 71.1% as\ncompared to that of the estimated parameters without L1-regularization. Our\nmethod is 54 times faster than the standard simplex-based optimization to solve\nthe regularized nonlinear regression.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 06:17:57 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Yoon", "Kyubaek", ""], ["You", "Hojun", ""], ["Wu", "Wei-Ying", ""], ["Lim", "Chae Young", ""], ["Choi", "Jongeun", ""], ["Boss", "Connor", ""], ["Ramadan", "Ahmed", ""], ["Popovich", "John M.", "Jr."], ["Cholewicki", "Jacek", ""], ["Reeves", "N. Peter", ""], ["Radcliffe", "Clark J.", ""]]}, {"id": "2104.11531", "submitter": "Jouni Kuha", "authors": "Jouni Kuha, Siliang Zhang and Fiona Steele", "title": "Latent variable models for multivariate dyadic data with zero inflation:\n  Analysis of intergenerational exchanges of family support", "comments": "25 pages, 1 figure and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the help and support that is exchanged between family members\nof different generations is of increasing importance, with research questions\nin sociology and social policy focusing on both predictors of the levels of\nhelp given and received, and on reciprocity between them. We propose general\nlatent variable models for analysing such data, when helping tendencies in each\ndirection are measured by multiple binary indicators of specific types of help.\nThe model combines two continuous latent variables, which represent the helping\ntendencies, with two binary latent class variables which allow for high\nproportions of responses where no help of any kind is given or received. This\ndefines a multivariate version of a zero inflation model. The main part of the\nmodels is estimated using MCMC methods, with a bespoke data augmentation\nalgorithm. We apply the models to analyse exchanges of help between adult\nindividuals and their non-coresident parents, using survey data from the UK\nHousehold Longitudinal Study.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 10:47:54 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Kuha", "Jouni", ""], ["Zhang", "Siliang", ""], ["Steele", "Fiona", ""]]}, {"id": "2104.11643", "submitter": "Fabrizio Leisen", "authors": "Patrizia Berti, Emanuela Dreassi, Fabrizio Leisen, Pietro Rigo, Luca\n  Pratelli", "title": "Bayesian predictive inference without a prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $(X_n:n\\ge 1)$ be a sequence of random observations. Let\n$\\sigma_n(\\cdot)=P\\bigl(X_{n+1}\\in\\cdot\\mid X_1,\\ldots,X_n\\bigr)$ be the $n$-th\npredictive distribution and $\\sigma_0(\\cdot)=P(X_1\\in\\cdot)$ the marginal\ndistribution of $X_1$. In a Bayesian framework, to make predictions on $(X_n)$,\none only needs the collection $\\sigma=(\\sigma_n:n\\ge 0)$. Because of the\nIonescu-Tulcea theorem, $\\sigma$ can be assigned directly, without passing\nthrough the usual prior/posterior scheme. One main advantage is that no prior\nprobability has to be selected. In this paper, $\\sigma$ is subjected to two\nrequirements: (i) The resulting sequence $(X_n)$ is conditionally identically\ndistributed, in the sense of Berti, Pratelli and Rigo (2004); (ii) Each\n$\\sigma_{n+1}$ is a simple recursive update of $\\sigma_n$. Various new $\\sigma$\nsatisfying (i)-(ii) are introduced and investigated. For such $\\sigma$, the\nasymptotics of $\\sigma_n$, as $n\\rightarrow\\infty$, is determined. In some\ncases, the probability distribution of $(X_n)$ is also evaluated.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:51:07 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 13:02:43 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Berti", "Patrizia", ""], ["Dreassi", "Emanuela", ""], ["Leisen", "Fabrizio", ""], ["Rigo", "Pietro", ""], ["Pratelli", "Luca", ""]]}, {"id": "2104.11651", "submitter": "Kristen Hunter", "authors": "Kristen B. Hunter, Mark E. Glickman, Luis F. Campos", "title": "Inferring medication adherence from time-varying health measures", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Medication adherence is a problem of widespread concern in clinical care.\nPoor adherence is a particular problem for patients with chronic diseases\nrequiring long-term medication because poor adherence can result in less\nsuccessful treatment outcomes and even preventable deaths. Existing methods to\ncollect information about patient adherence are resource-intensive or do not\nsuccessfully detect low-adherers with high accuracy. Acknowledging that health\nmeasures recorded at clinic visits are more reliably recorded than a patient's\nadherence, we have developed an approach to infer medication adherence rates\nbased on longitudinally recorded health measures that are likely impacted by\ntime-varying adherence behaviors. Our framework permits the inclusion of\nbaseline health characteristics and socio-demographic data. We employ a modular\ninferential approach. First, we fit a two-component model on a training set of\npatients who have detailed adherence data obtained from electronic medication\nmonitoring. One model component predicts adherence behaviors only from baseline\nhealth and socio-demographic information, and the other predicts longitudinal\nhealth measures given the adherence and baseline health measures. Posterior\ndraws of relevant model parameters are simulated from this model using Markov\nchain Monte Carlo methods. Second, we develop an approach to infer medication\nadherence from the time-varying health measures using a Sequential Monte Carlo\nalgorithm applied to a new set of patients for whom no adherence data are\navailable. We apply and evaluate the method on a cohort of hypertensive\npatients, using baseline health comorbidities, socio-demographic measures, and\nblood pressure measured over time to infer patients' adherence to\nantihypertensive medication.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 15:00:30 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Hunter", "Kristen B.", ""], ["Glickman", "Mark E.", ""], ["Campos", "Luis F.", ""]]}, {"id": "2104.11722", "submitter": "Filippo Carone Fabiani", "authors": "Filippo Carone Fabiani", "title": "Asymptotic Incidence Rate estimation of SARS-COVID-19 via a Polya\n  process scheme: a comparative analysis in Italy and European Countries", "comments": "22 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During an ongoing epidemic, especially in the case of a new agent, data are\npartial and sparse, also affected by external factors as for example climatic\neffects or preparedness and response capability of healthcare structures.\nDespite that we showed how, under some universality assumptions, it is possible\nto extract strategic insights by modelling the pandemic trough a probabilistic\nPolya urn scheme. In the Polya framework, we provide both the distribution of\ninfected cases and the asymptotic estimation of the incidence rate, showing\nthat data are consistent with a general underlying process at different scales.\nUsing European confirmed cases and diagnostic test data on COVID-19 , we\nprovided an extensive comparison among European countries and between Europe\nand Italy at regional scale, for both the two big waves of infection. We\nglobally estimated an incidence rate in accordance with previous studies. On\nthe other hand, this quantity could play a crucial role as a proxy variable for\nan unbiased estimation of the real incidence rate, including symptomatic and\nasymptomatic cases.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 17:16:33 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Fabiani", "Filippo Carone", ""]]}, {"id": "2104.11766", "submitter": "Russell Bowater", "authors": "Russell J. Bowater", "title": "A very short guide to IOI: A general framework for statistical inference\n  summarised", "comments": "Subject to revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated organic inference (IOI) is discussed in a concise and informal way\nwith the aim that the reader is given the gist of what this approach to\nstatistical inference is about as well as given pointers to further reading.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 18:10:24 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bowater", "Russell J.", ""]]}, {"id": "2104.11900", "submitter": "Salvatore Daniele Tomarchio Ph.D", "authors": "Salvatore D. Tomarchio, Paul D. McNicholas and Antonio Punzo", "title": "Matrix Normal Cluster-Weighted Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finite mixtures of regressions with fixed covariates are a commonly used\nmodel-based clustering methodology to deal with regression data. However, they\nassume assignment independence, i.e. the allocation of data points to the\nclusters is made independently of the distribution of the covariates. In order\nto take into account the latter aspect, finite mixtures of regressions with\nrandom covariates, also known as cluster-weighted models (CWMs), have been\nproposed in the univariate and multivariate literature. In this paper, the CWM\nis extended to matrix data, e.g. those data where a set of variables are\nsimultaneously observed at different time points or locations. Specifically,\nthe cluster-specific marginal distribution of the covariates, and the\ncluster-specific conditional distribution of the responses given the\ncovariates, are assumed to be matrix normal. Maximum likelihood parameter\nestimates are derived using an ECM algorithm. Parameter recovery,\nclassification assessment and the capability of the BIC to detect the\nunderlying groups are analyzed on simulated data. Finally, two real data\napplications concerning educational indicators and the Italian non-life\ninsurance market are presented.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 07:48:58 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Tomarchio", "Salvatore D.", ""], ["McNicholas", "Paul D.", ""], ["Punzo", "Antonio", ""]]}, {"id": "2104.11963", "submitter": "Chaofan Huang", "authors": "Chaofan Huang, V. Roshan Joseph, Douglas M. Ray", "title": "Constrained Minimum Energy Designs", "comments": "Submitted to Statistics and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space-filling designs are important in computer experiments, which are\ncritical for building a cheap surrogate model that adequately approximates an\nexpensive computer code. Many design construction techniques in the existing\nliterature are only applicable for rectangular bounded space, but in real world\napplications, the input space can often be non-rectangular because of\nconstraints on the input variables. One solution to generate designs in a\nconstrained space is to first generate uniformly distributed samples in the\nfeasible region, and then use them as the candidate set to construct the\ndesigns. Sequentially Constrained Monte Carlo (SCMC) is the state-of-the-art\ntechnique for candidate generation, but it still requires large number of\nconstraint evaluations, which is problematic especially when the constraints\nare expensive to evaluate. Thus, to reduce constraint evaluations and improve\nefficiency, we propose the Constrained Minimum Energy Design (CoMinED) that\nutilizes recent advances in deterministic sampling methods. Extensive\nsimulation results on 15 benchmark problems with dimensions ranging from 2 to\n13 are provided for demonstrating the improved performance of CoMinED over the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 14:28:35 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Chaofan", ""], ["Joseph", "V. Roshan", ""], ["Ray", "Douglas M.", ""]]}, {"id": "2104.11965", "submitter": "Giuseppe Pandolfo", "authors": "Giuseppe Pandolfo", "title": "The GLD-plot: A depth-based plot to investigate unimodality of\n  directional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graphical tool for investigating unimodality of hyperspherical data is\nproposed. It is based on the notion of statistical data depth function for\ndirectional data which extends the univariate concept of rank. Firstly a local\nversion of distance-based depths for directional data based on aims at\nanalyzing the local structure of hyperspherical data is proposed. Then such\nnotion is compared to the global version of data depth by means of a\ntwo-dimensional scatterplot, i.e. the GLD-plot. The proposal is illustrated on\nsimulated and real data examples.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 14:38:50 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Pandolfo", "Giuseppe", ""]]}, {"id": "2104.12022", "submitter": "Reza Mehrizi", "authors": "Reza Valiollahi Mehrizi and Shojaeddin Chenouri", "title": "Valid Post-Detection Inference for Change Points Identified Using Trend\n  Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many research works and methods about change point detection in the\nliterature. However, there are only a few that provide inference for such\nchange points after being estimated. This work mainly focuses on a statistical\nanalysis of change points estimated by the PRUTF algorithm, which incorporates\ntrend filtering to determine change points in piecewise polynomial signals.\nThis paper develops a methodology to perform statistical inference, such as\ncomputing p-values and constructing confidence intervals in the newly developed\npost-selection inference framework. Our work concerns both cases of known and\nunknown error variance. As pointed out in the post-selection inference\nliterature, the length of such confidence intervals are undesirably long. To\nresolve this shortcoming, we also provide two novel strategies, global\npost-detection, and local post-detection which are based on the intrinsic\nproperties of change points. We run our proposed methods on real as well as\nsimulated data to evaluate their performances.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 21:00:00 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mehrizi", "Reza Valiollahi", ""], ["Chenouri", "Shojaeddin", ""]]}, {"id": "2104.12031", "submitter": "Yuetian Luo", "authors": "Yuetian Luo, Anru R. Zhang", "title": "Low-rank Tensor Estimation via Riemannian Gauss-Newton: Statistical\n  Optimality and Second-Order Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the estimation of a low Tucker rank tensor from a\nnumber of noisy linear measurements. The general problem covers many specific\nexamples arising from applications, including tensor regression, tensor\ncompletion, and tensor PCA/SVD. We propose a Riemannian Gauss-Newton (RGN)\nmethod with fast implementations for low Tucker rank tensor estimation.\nDifferent from the generic (super)linear convergence guarantee of RGN in the\nliterature, we prove the first quadratic convergence guarantee of RGN for\nlow-rank tensor estimation under some mild conditions. A deterministic\nestimation error lower bound, which matches the upper bound, is provided that\ndemonstrates the statistical optimality of RGN. The merit of RGN is illustrated\nthrough two machine learning applications: tensor regression and tensor SVD.\nFinally, we provide the simulation results to corroborate our theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 22:24:14 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 04:01:36 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Luo", "Yuetian", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2104.12060", "submitter": "Ruoyang Zhang", "authors": "Ruoyang Zhang, Yisha Yao, Malay Ghosh", "title": "Contraction of a quasi-Bayesian model with shrinkage priors in precision\n  matrix estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Currently several Bayesian approaches are available to estimate large sparse\nprecision matrices, including Bayesian graphical Lasso (Wang, 2012), Bayesian\nstructure learning (Banerjee and Ghosal, 2015), and graphical horseshoe (Li et\nal., 2019). Although these methods have exhibited nice empirical performances,\nin general they are computationally expensive. Moreover, we have limited\nknowledge about the theoretical properties, e.g., posterior contraction rate,\nof graphical Bayesian Lasso and graphical horseshoe. In this paper, we propose\na new method that integrates some commonly used continuous shrinkage priors\ninto a quasi-Bayesian framework featured by a pseudo-likelihood. Under mild\nconditions, we establish an optimal posterior contraction rate for the proposed\nmethod. Compared to existing approaches, our method has two main advantages.\nFirst, our method is computationally more efficient while achieving similar\nerror rate; second, our framework is more amenable to theoretical analysis.\nExtensive simulation experiments and the analysis on a real data set are\nsupportive of our theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 04:51:36 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhang", "Ruoyang", ""], ["Yao", "Yisha", ""], ["Ghosh", "Malay", ""]]}, {"id": "2104.12119", "submitter": "Christos Merkatas", "authors": "Christos Merkatas and Simo S\\\"arkk\\\"a", "title": "System identification using Bayesian neural networks with nonparametric\n  noise models", "comments": "Submitted to Statistics and Computing; figure and typos corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System identification is of special interest in science and engineering. This\narticle is concerned with a system identification problem arising in stochastic\ndynamic systems, where the aim is to estimating the parameters of a system\nalong with its unknown noise processes. In particular, we propose a Bayesian\nnonparametric approach for system identification in discrete time nonlinear\nrandom dynamical systems assuming only the order of the Markov process is\nknown. The proposed method replaces the assumption of Gaussian distributed\nerror components with a highly flexible family of probability density functions\nbased on Bayesian nonparametric priors. Additionally, the functional form of\nthe system is estimated by leveraging Bayesian neural networks which also leads\nto flexible uncertainty quantification. Asymptotically on the number of hidden\nneurons, the proposed model converges to full nonparametric Bayesian regression\nmodel. A Gibbs sampler for posterior inference is proposed and its\neffectiveness is illustrated in simulated and real time series.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 09:49:50 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 17:24:40 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Merkatas", "Christos", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2104.12127", "submitter": "Gu{\\dh}mundur Stef\\'an Gu{\\dh}mundsson", "authors": "Christian Brownlees and Gu{\\dh}mundur Stef\\'an Gu{\\dh}mundsson", "title": "Performance of Empirical Risk Minimization for Linear Regression with\n  Dependent Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper establishes bounds on the performance of empirical risk\nminimization for large-dimensional linear regression. We generalize existing\nresults by allowing the data to be dependent and heavy-tailed. The analysis\ncovers both the cases of identically and heterogeneously distributed\nobservations. Our analysis is nonparametric in the sense that the relationship\nbetween the regressand and the regressors is assumed to be unknown. The main\nresults of this paper indicate that the empirical risk minimizer achieves the\noptimal performance (up to a logarithmic factor) in a dependent data setting.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 10:56:04 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 10:45:32 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 23:44:24 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Brownlees", "Christian", ""], ["Gu\u00f0mundsson", "Gu\u00f0mundur Stef\u00e1n", ""]]}, {"id": "2104.12208", "submitter": "Matteo Farn\\`e Dr.", "authors": "Matteo Farn\\`e and Angelos Vouldis", "title": "Robust selection of predictors and conditional outlier detection in a\n  perturbed large-dimensional regression context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a fast methodology, called ROBOUT, to identify outliers\nin a response variable conditional on a set of linearly related predictors,\nretrieved from a large granular dataset. ROBOUT is shown to be effective and\nparticularly versatile compared to existing methods in the presence of a number\nof data idiosyncratic features. ROBOUT is able to identify observations with\noutlying conditional variance when the dataset contains element-wise sparse\nvariables, and the set of predictors contains multivariate outliers. Existing\nintegrated methodologies like SPARSE-LTS and RLARS are systematically\nsub-optimal under those conditions. ROBOUT entails a robust selection stage of\nthe statistically relevant predictors (by using a Huber or a quantile loss),\nthe estimation of a robust regression model based on the selected predictors\n(by LTS, GS or MM), and a criterion to identify conditional outliers based on a\nrobust measure of the residuals' dispersion. We conduct a comprehensive\nsimulation study in which the different variants of the proposed algorithm are\ntested under an exhaustive set of different perturbation scenarios. The\nmethodology is also applied to a granular supervisory banking dataset collected\nby the European Central Bank.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 17:08:41 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Farn\u00e8", "Matteo", ""], ["Vouldis", "Angelos", ""]]}, {"id": "2104.12219", "submitter": "Andrew Miller", "authors": "Andrew C. Miller, Nicholas J. Foti, Emily B. Fox", "title": "Breiman's two cultures: You don't have to choose sides", "comments": "Commentary to appear in a special issue of Observational Studies,\n  discussing Leo Breiman's paper \"Statistical Modeling: The Two Cultures\"\n  (https://doi.org/10.1214/ss/1009213726)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breiman's classic paper casts data analysis as a choice between two cultures:\ndata modelers and algorithmic modelers. Stated broadly, data modelers use\nsimple, interpretable models with well-understood theoretical properties to\nanalyze data. Algorithmic modelers prioritize predictive accuracy and use more\nflexible function approximations to analyze data. This dichotomy overlooks a\nthird set of models $-$ mechanistic models derived from scientific theories\n(e.g., ODE/SDE simulators). Mechanistic models encode application-specific\nscientific knowledge about the data. And while these categories represent\nextreme points in model space, modern computational and algorithmic tools\nenable us to interpolate between these points, producing flexible,\ninterpretable, and scientifically-informed hybrids that can enjoy accurate and\nrobust predictions, and resolve issues with data analysis that Breiman\ndescribes, such as the Rashomon effect and Occam's dilemma. Challenges still\nremain in finding an appropriate point in model space, with many choices on how\nto compose model components and the degree to which each component informs\ninferences.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 17:58:46 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Miller", "Andrew C.", ""], ["Foti", "Nicholas J.", ""], ["Fox", "Emily B.", ""]]}, {"id": "2104.12222", "submitter": "Geng Zhao", "authors": "Hannah Li, Geng Zhao, Ramesh Johari, Gabriel Y. Weintraub", "title": "Interference, Bias, and Variance in Two-Sided Marketplace\n  Experimentation: Guidance for Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-sided marketplace platforms often run experiments to test the effect of\nan intervention before launching it platform-wide. A typical approach is to\nrandomize individuals into the treatment group, which receives the\nintervention, and the control group, which does not. The platform then compares\nthe performance in the two groups to estimate the effect if the intervention\nwere launched to everyone. We focus on two common experiment types, where the\nplatform randomizes individuals either on the supply side or on the demand\nside. The resulting estimates of the treatment effect in these experiments are\ntypically biased: because individuals in the market compete with each other,\nindividuals in the treatment group affect those in the control group and vice\nversa, creating interference. We develop a simple tractable market model to\nstudy bias and variance in these experiments with interference. We focus on two\nchoices available to the platform: (1) Which side of the platform should it\nrandomize on (supply or demand)? (2) What proportion of individuals should be\nallocated to treatment? We find that both choices affect the bias and variance\nof the resulting estimators but in different ways. The bias-optimal choice of\nexperiment type depends on the relative amounts of supply and demand in the\nmarket, and we discuss how a platform can use market data to select the\nexperiment type. Importantly, we find in many circumstances, choosing the\nbias-optimal experiment type has little effect on variance. On the other hand,\nthe choice of treatment proportion can induce a bias-variance tradeoff, where\nthe bias-minimizing proportion increases variance. We discuss how a platform\ncan navigate this tradeoff and best choose the treatment proportion, using a\ncombination of modeling as well as contextual knowledge about the market, the\nrisk of the intervention, and reasonable effect sizes of the intervention.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 18:11:09 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Li", "Hannah", ""], ["Zhao", "Geng", ""], ["Johari", "Ramesh", ""], ["Weintraub", "Gabriel Y.", ""]]}, {"id": "2104.12231", "submitter": "Andrew Miller", "authors": "Andrew C. Miller, Leon A. Gatys, Joseph Futoma, Emily B. Fox", "title": "Model-based metrics: Sample-efficient estimates of predictive model\n  subpopulation performance", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models $-$ now commonly developed to screen, diagnose, or\npredict health conditions $-$ are evaluated with a variety of performance\nmetrics. An important first step in assessing the practical utility of a model\nis to evaluate its average performance over an entire population of interest.\nIn many settings, it is also critical that the model makes good predictions\nwithin predefined subpopulations. For instance, showing that a model is fair or\nequitable requires evaluating the model's performance in different demographic\nsubgroups. However, subpopulation performance metrics are typically computed\nusing only data from that subgroup, resulting in higher variance estimates for\nsmaller groups. We devise a procedure to measure subpopulation performance that\ncan be more sample-efficient than the typical subsample estimates. We propose\nusing an evaluation model $-$ a model that describes the conditional\ndistribution of the predictive model score $-$ to form model-based metric (MBM)\nestimates. Our procedure incorporates model checking and validation, and we\npropose a computationally efficient approximation of the traditional\nnonparametric bootstrap to form confidence intervals. We evaluate MBMs on two\nmain tasks: a semi-synthetic setting where ground truth metrics are available\nand a real-world hospital readmission prediction task. We find that MBMs\nconsistently produce more accurate and lower variance estimates of model\nperformance for small subpopulations.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 19:06:34 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Miller", "Andrew C.", ""], ["Gatys", "Leon A.", ""], ["Futoma", "Joseph", ""], ["Fox", "Emily B.", ""]]}, {"id": "2104.12248", "submitter": "Felipe Elorrieta", "authors": "Felipe Elorrieta, Susana Eyheramendy, Wilfredo Palma and Cesar Ojeda", "title": "Novel bivariate autoregressive model for predicting and forecasting\n  irregularly observed time series", "comments": "14 pages, 7 figures, 6 tables, 2 apendices. Accepted for publication\n  in the Monthly Notices of the Royal Astronomical Society (MNRAS)", "journal-ref": null, "doi": "10.1093/mnras/stab1216", "report-no": null, "categories": "astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several disciplines it is common to find time series measured at irregular\nobservational times. In particular, in astronomy there are a large number of\nsurveys that gather information over irregular time gaps and in more than one\npassband. Some examples are Pan-STARRS, ZTF and also the LSST. However, current\ncommonly used time series models that estimate the time dependency in\nastronomical light curves consider the information of each band separately\n(e.g, CIAR, IAR and CARMA models) disregarding the dependency that might exist\nbetween different passbands. In this paper we propose a novel bivariate model\nfor irregularly sampled time series, called the bivariate irregular\nautoregressive (BIAR) model. The BIAR model assumes an autoregressive structure\non each time series, it is stationary, and it allows to estimate the\nautocorrelation, the cross-correlation and the contemporary correlation between\ntwo unequally spaced time series. We implemented the BIAR model on light\ncurves, in the g and r bands, obtained from the ZTF alerts processed by the\nALeRCE broker. We show that if the light curves of the two bands are highly\ncorrelated, the model has more accurate forecast and prediction using the\nbivariate model than a similar method that uses only univariate information.\nFurther, the estimated parameters of the BIAR are useful to characterize\nLongPeriod Variable Stars and to distinguish between classes of stochastic\nobjects, providing promising features that can be used for classification\npurposes\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 20:24:16 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Elorrieta", "Felipe", ""], ["Eyheramendy", "Susana", ""], ["Palma", "Wilfredo", ""], ["Ojeda", "Cesar", ""]]}, {"id": "2104.12260", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban", "title": "Consistency of invariance-based randomization tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Invariance-based randomization tests -- such as permutation tests -- are an\nimportant and widely used class of statistical methods. They allow drawing\ninferences with few assumptions on the data distribution. Most work focuses on\ntheir type I error control properties, while their consistency properties are\nmuch less understood.\n  We develop a general framework and a set of results on the consistency of\ninvariance-based randomization tests in signal-plus-noise models. Our framework\nis grounded in the deep mathematical area of representation theory. We allow\nthe transforms to be general compact topological groups, such as rotation\ngroups. Moreover, we allow actions by general linear group representations.\n  We apply our framework to a number of fundamental and highly important\nproblems in statistics, including sparse vector detection, testing for low-rank\nmatrices in noise, sparse detection in linear regression, symmetric submatrix\ndetection, and two-sample testing. Perhaps surprisingly, we find that\nrandomization tests can adapt to problem structure and detect signals at the\nsame rate as tests with full knowledge of the noise distribution.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 21:21:36 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Dobriban", "Edgar", ""]]}, {"id": "2104.12301", "submitter": "Ping He", "authors": "Zhen-Wei Li, Ping He", "title": "Data-Based Optimal Bandwidth for Kernel Density Estimation of\n  Statistical Samples", "comments": "7 pages, 8 figures", "journal-ref": "Commun. Theor. Phys. 70 (2018) 728-734", "doi": "10.1088/0253-6102/70/6/728", "report-no": null, "categories": "stat.ME astro-ph.IM physics.comp-ph", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  It is a common practice to evaluate probability density function or matter\nspatial density function from statistical samples. Kernel density estimation is\na frequently used method, but to select an optimal bandwidth of kernel\nestimation, which is completely based on data samples, is a long-term issue\nthat has not been well settled so far. There exist analytic formulae of optimal\nkernel bandwidth, but they cannot be applied directly to data samples, since\nthey depend on the unknown underlying density functions from which the samples\nare drawn. In this work, we devise an approach to pick out the totally\ndata-based optimal bandwidth. First, we derive correction formulae for the\nanalytic formulae of optimal bandwidth to compute the roughness of the sample's\ndensity function. Then substitute the correction formulae into the analytic\nformulae for optimal bandwidth, and through iteration, we obtain the sample's\noptimal bandwidth. Compared with analytic formulae, our approach gives very\ngood results, with relative differences from the analytic formulae being only\n2%-3% for a sample size larger than 10^4. This approach can also be generalized\neasily to cases of variable kernel estimations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 01:18:25 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Li", "Zhen-Wei", ""], ["He", "Ping", ""]]}, {"id": "2104.12409", "submitter": "El Hadji Deme", "authors": "El Hadji Mamadou Sall, El Hadji Deme and Abdou Ka Diongue", "title": "Modeling Risk via Realized HYGARCH Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose the realized Hyperbolic GARCH model for the\njoint-dynamics of lowfrequency returns and realized measures that generalizes\nthe realized GARCH model of Hansen et al.(2012) as well as the FLoGARCH model\nintroduced by Vander Elst (2015). This model is sufficiently flexible to\ncapture both long memory and asymmetries related to leverage effects. In\naddition, we will study the strictly and weak stationarity conditions of the\nmodel. To evaluate its performance, experimental simulations, using the Monte\nCarlo method, are made to forecast the Value at Risk (VaR) and the Expected\nShortfall (ES). These simulation studies show that for ES and VaR forecasting,\nthe realized Hyperbolic GARCH (RHYGARCH-GG) model with Gaussian-Gaussian errors\nprovide more adequate estimates than the realized Hyperbolic GARCH model with\nstudent- Gaussian errors.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:07:15 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sall", "El Hadji Mamadou", ""], ["Deme", "El Hadji", ""], ["Diongue", "Abdou Ka", ""]]}, {"id": "2104.12431", "submitter": "Han Lin Shang", "authors": "Ufuk Beyaztas, Han Lin Shang, Zaher Mundher Yaseen", "title": "A functional autoregressive model based on exogenous hydrometeorological\n  variables for river flow prediction", "comments": "42 pages, 13 figures, to appear at the Journal of Hydrology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this research, a functional time series model was introduced to predict\nfuture realizations of river flow time series. The proposed model was\nconstructed based on a functional time series's correlated lags and the\nessential exogenous climate variables. Rainfall, temperature, and evaporation\nvariables were hypothesized to have substantial functionality in river flow\nsimulation. Because an actual time series model is unspecified and the input\nvariables' significance for the learning process is unknown in practice, it was\nemployed a variable selection procedure to determine only the significant\nvariables for the model. A nonparametric bootstrap model was also proposed to\ninvestigate predictions' uncertainty and construct pointwise prediction\nintervals for the river flow curve time series. Historical datasets at three\nmeteorological stations (Mosul, Baghdad, and Kut) located in the semi-arid\nregion, Iraq, were used for model development. The prediction performance of\nthe proposed model was validated against existing functional and traditional\ntime series models. The numerical analyses revealed that the proposed model\nprovides competitive or even better performance than the benchmark models.\nAlso, the incorporated exogenous climate variables have substantially improved\nthe modeling predictability performance. Overall, the proposed model indicated\na reliable methodology for modeling river flow within the semi-arid region.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:42:00 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Beyaztas", "Ufuk", ""], ["Shang", "Han Lin", ""], ["Yaseen", "Zaher Mundher", ""]]}, {"id": "2104.12587", "submitter": "Chris Oates", "authors": "Junyang Wang, Jon Cockayne, Oksana Chkrebtii, T. J. Sullivan, Chris.\n  J. Oates", "title": "Bayesian Numerical Methods for Nonlinear Partial Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The numerical solution of differential equations can be formulated as an\ninference problem to which formal statistical approaches can be applied.\nHowever, nonlinear partial differential equations (PDEs) pose substantial\nchallenges from an inferential perspective, most notably the absence of\nexplicit conditioning formula. This paper extends earlier work on linear PDEs\nto a general class of initial value problems specified by nonlinear PDEs,\nmotivated by problems for which evaluations of the right-hand-side, initial\nconditions, or boundary conditions of the PDE have a high computational cost.\nThe proposed method can be viewed as exact Bayesian inference under an\napproximate likelihood, which is based on discretisation of the nonlinear\ndifferential operator. Proof-of-concept experimental results demonstrate that\nmeaningful probabilistic uncertainty quantification for the unknown solution of\nthe PDE can be performed, while controlling the number of times the\nright-hand-side, initial and boundary conditions are evaluated. A suitable\nprior model for the solution of the PDE is identified using novel theoretical\nanalysis of the sample path properties of Mat\\'{e}rn processes, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 14:02:10 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 13:26:26 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wang", "Junyang", ""], ["Cockayne", "Jon", ""], ["Chkrebtii", "Oksana", ""], ["Sullivan", "T. J.", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2104.12588", "submitter": "Dursun Bulutoglu A", "authors": "Dursun A. Bulutoglu, Kenneth J. Ryan", "title": "Algorithms for finding generalized minimum aberration designs", "comments": "15 pages. arXiv admin note: text overlap with arXiv:1501.02281", "journal-ref": "Journal of Complexity 31 (4) (2015) 577-589", "doi": "10.1016/j.jco.2014.12.001", "report-no": null, "categories": "stat.CO math.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical design of experiments is widely used in scientific and industrial\ninvestigations. A generalized minimum aberration (GMA) orthogonal array is\noptimum under the well-established, so-called GMA criterion, and such an array\ncan extract as much information as possible at a fixed cost. Finding GMA arrays\nis an open (yet fundamental) problem in design of experiments because\nconstructing such arrays becomes intractable as the number of runs and factors\nincrease. We develop two directed enumeration algorithms that call the integer\nprogramming with isomorphism pruning algorithm of Margot (2007) for the purpose\nof finding GMA arrays. Our results include 16 GMA arrays that were not\npreviously in the literature, along with documentation of the efficiencies that\nmade the required calculations possible within a reasonable budget of computer\ntime. We also validate heuristic algorithms against a GMA array catalog, by\nshowing that they quickly output near GMA arrays, and then use the heuristics\nto find near GMA arrays when enumeration is computationally burdensome.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 12:42:43 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bulutoglu", "Dursun A.", ""], ["Ryan", "Kenneth J.", ""]]}, {"id": "2104.12597", "submitter": "Benedikt M. P\\\"otscher", "authors": "Benedikt M. P\\\"otscher and David Preinerstorfer", "title": "Valid Heteroskedasticity Robust Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tests based on heteroskedasticity robust standard errors are an important\ntechnique in econometric practice. Choosing the right critical value, however,\nis not all that simple: Conventional critical values based on asymptotics often\nlead to severe size distortions; and so do existing adjustments including the\nbootstrap. To avoid these issues, we suggest to use smallest size-controlling\ncritical values, the generic existence of which we prove in this article.\nFurthermore, sufficient and often also necessary conditions for their existence\nare given that are easy to check. Granted their existence, these critical\nvalues are the canonical choice: larger critical values result in unnecessary\npower loss, whereas smaller critical values lead to over-rejections under the\nnull hypothesis, make spurious discoveries more likely, and thus are invalid.\nWe suggest algorithms to numerically determine the proposed critical values and\nprovide implementations in accompanying software. Finally, we numerically study\nthe behavior of the proposed testing procedures, including their power\nproperties.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:01:40 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""], ["Preinerstorfer", "David", ""]]}, {"id": "2104.12639", "submitter": "Imke Mayer", "authors": "Imke Mayer, Julie Josse, Traumabase Group", "title": "Transporting treatment effects with incomplete attributes", "comments": "preprint, 47 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The simultaneous availability of experimental and observational data to\nestimate a treatment effect is both an opportunity and a statistical challenge:\nCombining the information gathered from both data is a promising avenue to\nbuild upon the internal validity of randomized controlled trials (RCTs) and a\ngreater external validity of observational data, but it raises methodological\nissues, especially due to different sampling designs inducing distributional\nshifts. We focus on the aim of transporting a causal effect estimated on an RCT\nonto a target population described by a set of covariates. Available methods\nsuch as inverse propensity weighting are not designed to handle missing values,\nwhich are however common in both data. In addition to coupling the assumptions\nfor causal identifiability and for the missing values mechanism and to defining\nappropriate strategies, one has to consider the specific structure of the data\nwith two sources and treatment and outcome only available in the RCT. We study\ndifferent approaches and their underlying assumptions on the data generating\nprocesses and distribution of missing values and suggest several adapted\nmethods, in particular multiple imputation strategies. These methods are\nassessed in an extensive simulation study and practical guidelines are provided\nfor different scenarios. This work is motivated by the analysis of a large\nregistry of over 20,000 major trauma patients and a multi-centered RCT studying\nthe effect of tranexamic acid administration on mortality. The analysis\nillustrates how the missing values handling can impact the conclusion about the\neffect transported from the RCT to the target population.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:09:58 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mayer", "Imke", ""], ["Josse", "Julie", ""], ["Group", "Traumabase", ""]]}, {"id": "2104.12688", "submitter": "Hein Putter", "authors": "Hein Putter, Dirk-Jan Eikema, Liesbeth C. de Wreede, Eoin McGrath,\n  Isabel Sanchez-Ortega, Riccardo Saccardi, John A. Snowden, Erik W. van Zwet", "title": "Benchmarking survival outcomes: A funnel plot for survival data", "comments": "24 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benchmarking is commonly used in many healthcare settings to monitor clinical\nperformance, with the aim of increasing cost-effectiveness and safe care of\npatients. The funnel plot is a popular tool in visualizing the performance of a\nhealthcare center in relation to other centers and to a target, taking into\naccount statistical uncertainty. In this paper we develop methodology for\nconstructing funnel plots for survival data. The method takes into account\ncensoring and can deal with differences in censoring distributions across\ncenters. Practical issues in implementing the methodology are discussed,\nparticularly in the setting of benchmarking clinical outcomes for hematopoietic\nstem cell transplantation. A simulation study is performed to assess the\nperformance of the funnel plots under several scenarios. Our methodology is\nillustrated using data from the EBMT benchmarking project.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 16:28:08 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Putter", "Hein", ""], ["Eikema", "Dirk-Jan", ""], ["de Wreede", "Liesbeth C.", ""], ["McGrath", "Eoin", ""], ["Sanchez-Ortega", "Isabel", ""], ["Saccardi", "Riccardo", ""], ["Snowden", "John A.", ""], ["van Zwet", "Erik W.", ""]]}, {"id": "2104.12832", "submitter": "Sean van der Merwe", "authors": "Sean van der Merwe", "title": "On Determining the Distribution of a Goodness-of-Fit Test Statistic", "comments": "Bayes, Distribution, Gamma, GPD, Hypothesis Testing, Objective Bayes,\n  p-value, Predictive Posterior, Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of goodness-of-fit testing for a model that has at\nleast one unknown parameter that cannot be eliminated by transformation.\nExamples of such problems can be as simple as testing whether a sample consists\nof independent Gamma observations, or whether a sample consists of independent\nGeneralised Pareto observations given a threshold. Over time the approach to\ndetermining the distribution of a test statistic for such a problem has moved\ntowards on-the-fly calculation post observing a sample. Modern approaches\ninclude the parametric bootstrap and posterior predictive checks. We argue that\nthese approaches are merely approximations to integrating over the posterior\npredictive distribution that flows naturally from a given model. Further, we\nattempt to demonstrate that shortcomings which may be present in the parametric\nbootstrap, especially in small samples, can be reduced through the use of\nobjective Bayes techniques, in order to more reliably produce a test with the\ncorrect size.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 19:13:44 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["van der Merwe", "Sean", ""]]}, {"id": "2104.12909", "submitter": "Yusuke Narita", "authors": "Yusuke Narita and Kohei Yata", "title": "Algorithm is Experiment: Machine Learning, Market Design, and Policy\n  Eligibility Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Algorithms produce a growing portion of decisions and recommendations both in\npolicy and business. Such algorithmic decisions are natural experiments\n(conditionally quasi-randomly assigned instruments) since the algorithms make\ndecisions based only on observable input variables. We use this observation to\ndevelop a treatment-effect estimator for a class of stochastic and\ndeterministic decision-making algorithms. Our estimator is shown to be\nconsistent and asymptotically normal for well-defined causal effects. A key\nspecial case of our estimator is a multidimensional regression discontinuity\ndesign. We apply our estimator to evaluate the effect of the Coronavirus Aid,\nRelief, and Economic Security (CARES) Act, where more than \\$175 billion worth\nof relief funding is allocated to hospitals via an algorithmic rule. Our\nestimates suggest that the relief funding has little effect on COVID-19-related\nhospital activity levels. Naive OLS and IV estimates exhibit substantial\nselection bias.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 23:18:34 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 12:37:09 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Narita", "Yusuke", ""], ["Yata", "Kohei", ""]]}, {"id": "2104.12947", "submitter": "Emily Roberts", "authors": "Emily Roberts, Michael Elliott, Jeremy M. G. Taylor", "title": "Incorporating baseline covariates to validate surrogate endpoints with a\n  constant biomarker under control arm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A surrogate endpoint S in a clinical trial is an outcome that may be measured\nearlier or more easily than the true outcome of interest T. In this work, we\nextend causal inference approaches to validate such a surrogate using potential\noutcomes. The causal association paradigm assesses the relationship of the\ntreatment effect on the surrogate with the treatment effect on the true\nendpoint. Using the principal surrogacy criteria, we utilize the joint\nconditional distribution of the potential outcomes T, given the potential\noutcomes S. In particular, our setting of interest allows us to assume the\nsurrogate under the placebo, S(0), is zero-valued, and we incorporate baseline\ncovariates in the setting of normally-distributed endpoints. We develop\nBayesian methods to incorporate conditional independence and other modeling\nassumptions and explore their impact on the assessment of surrogacy. We\ndemonstrate our approach via simulation and data that mimics an ongoing study\nof a muscular dystrophy gene therapy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 02:36:01 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Roberts", "Emily", ""], ["Elliott", "Michael", ""], ["Taylor", "Jeremy M. G.", ""]]}, {"id": "2104.13020", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Simple yet Sharp Sensitivity Analysis for Unmeasured Confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for assessing the sensitivity of the true causal effect\nto unmeasured confounding. The method requires the analyst to specify two\nintuitive parameters. Otherwise, the method is assumption-free. The method\nreturns an interval that contains the true causal effect. Moreover, the bounds\nof the interval are sharp, i.e. attainable. We show experimentally that our\nbounds can be sharper than those obtained by the method of Ding and VanderWeele\n(2016). Finally, we extend our method to bound the natural direct and indirect\neffects when there are measured mediators and unmeasured exposure-outcome\nconfounding.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 07:43:52 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 11:44:25 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "2104.13028", "submitter": "Thomas Alexander Gerds", "authors": "Helene C. W. Rytgaard, Claus T. Ekstr{\\o}m, Lars V. Kessing, Thomas A.\n  Gerds", "title": "Ranking of average treatment effects with generalized random forests for\n  time-to-event outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a data-adaptive estimation procedure for estimation\nof average treatment effects in a time-to-event setting based on generalized\nrandom forests. In these kinds of settings, the definition of causal effect\nparameters are complicated by competing risks; here we distinguish between\ntreatment effects on the crude and the net probabilities, respectively. To\nhandle right-censoring, and to switch between crude and net probabilities, we\npropose a two-step procedure for estimation, applying inverse probability\nweighting to construct time-point specific weighted outcomes as input for the\nforest. The forest adaptively handles confounding of the treatment assigned by\napplying a splitting rule that targets a causal parameter. We demonstrate that\nour method is effective for a causal search through a list of treatments to be\nranked according to the magnitude of their effect. We further apply our method\nto a dataset from the Danish health registries where it is of interest to\ndiscover drugs with an unexpected protective effect against relapse of severe\ndepression.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 07:56:51 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Rytgaard", "Helene C. W.", ""], ["Ekstr\u00f8m", "Claus T.", ""], ["Kessing", "Lars V.", ""], ["Gerds", "Thomas A.", ""]]}, {"id": "2104.13469", "submitter": "Jae-Kwang Kim", "authors": "Hengfang Wang, Jae Kwang Kim", "title": "Propensity Score Estimation Using Density Ratio Model under Item\n  Nonresponse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Missing data is frequently encountered in practice. Propensity score\nestimation is a popular tool for handling such missingness. The propensity\nscore is often developed using the model for the response probability, which\ncan be subject to model misspecification. In this paper, we consider an\nalternative approach of estimating the inverse of the propensity scores using\nthe density ratio function. By partitioning the sample into two groups based on\nthe response status of the elements, we can apply the density ratio function\nestimation method and obtain the inverse propensity scores for nonresponse\nadjustment. Density ratio estimation can be obtained by applying the so-called\nmaximum entropy method, which uses the Kullback-Leibler divergence measure\nunder calibration constraints. By including the covariates for the outcome\nregression models only into the density ratio model, we can achieve efficient\npropensity score estimation. We further extend the proposed approach to the\nmultivariate missing case. Some limited simulation studies are presented to\ncompare with the existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 20:48:26 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Wang", "Hengfang", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "2104.13520", "submitter": "Erniel Barrios", "authors": "Paolo Victor T. Redondo, Joseph Ryan G. Lansangan and Erniel B.\n  Barrios", "title": "Estimation of Poisson Autoregressive Model for Multiple Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A Poisson autoregressive (PAR) model accounting for discreteness and\nautocorrelation of count time series data is typically estimated in the\nstate-space modelling framework through extended Kalman filter. However,\nbecause of the complex dependencies in count time series, estimation becomes\nmore challenging. PAR is viewed as an additive model and estimated using a\nhybrid of cubic smoothing splines and maximum likelihood estimation (MLE) in\nthe backfitting framework. Simulation studies show that this estimation method\nis comparable or better than PAR estimated in the state-space context,\nespecially with larger count values. However, as [2] formulated PAR for\nstationary counts, both estimation procedures underestimate parameters in\nnearly nonstationary models. The flexibility of the additive model has two\nbenefits though: robust estimation in the presence of temporary structural\nchange, and; viability to integrate PAR model into a more complex model\nstructure. We further generalized the PAR(p) model into multiple time series of\ncounts and illustrated with indicators in the financial markets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 01:27:52 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Redondo", "Paolo Victor T.", ""], ["Lansangan", "Joseph Ryan G.", ""], ["Barrios", "Erniel B.", ""]]}, {"id": "2104.13524", "submitter": "Erniel Barrios", "authors": "Erniel B. Barrios, John D. Eustaquio, Rouselle F. Lavado", "title": "Statistical Inference in a Spatial-Temporal Stochastic Frontier Model", "comments": "Earlier version of the paper first appeared as Discussion Paper No.\n  2010-08 in Philippine Institute for Devel-opment Studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The stochastic frontier model with heterogeneous technical efficiency\nexplained by exoge-nous variables is augmented with a spatial-temporal\ncomponent, a generalization relaxing the panel independence assumption in a\npanel data. The estimation procedure takes advantage of additivity in the\nmodel, computational advantages over maximum likelihood estimation of\nparameters is exhibited. The spatial-temporal component can improve estimates\nof technical efficiency in a production frontier that is usually biased\ndownwards. We present a test to veri-fy model assumptions that facilitates\nestimation of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 01:41:00 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Barrios", "Erniel B.", ""], ["Eustaquio", "John D.", ""], ["Lavado", "Rouselle F.", ""]]}, {"id": "2104.13588", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami and Tomoko Matsui", "title": "Improved log-Gaussian approximation for over-dispersed Poisson\n  regression: application to spatial analysis of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the era of open data, Poisson and other count regression models are\nincreasingly important. Provided this, we develop a closed-form inference for\nan over-dispersed Poisson regression, especially for (over-dispersed) Bayesian\nPoisson wherein the exact inference is unobtainable. The approach is derived\nvia mode-based log-Gaussian approximation. Unlike closed-form alternatives, it\nremains accurate even for zero-inflated count data. Besides, our approach has\nno arbitrary parameter that must be determined a priori. Monte Carlo\nexperiments demonstrate that the estimation error of the proposed method is a\nconsiderably smaller estimation error than the closed-form alternatives and as\nsmall as the usual Poisson regressions. We obtained similar results in the case\nof Poisson additive mixed modeling considering spatial or group effects. The\ndeveloped method was applied for analyzing COVID-19 data in Japan. This result\nsuggests that influences of pedestrian density, age, and other factors on the\nnumber of cases change over periods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 06:43:04 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 10:24:04 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Murakami", "Daisuke", ""], ["Matsui", "Tomoko", ""]]}, {"id": "2104.13693", "submitter": "Giovanni Ballarin", "authors": "Giovanni Ballarin", "title": "On the Limiting Distribution of Sieve VAR($\\infty$) Estimators in Small\n  Samples", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a finite order vector autoregressive model is fitted to VAR($\\infty$)\ndata the asymptotic distribution of statistics obtained via smooth functions of\nleast-squares estimates requires care. L\\\"utkepohl and Poskitt (1991) provide a\nclosed-form expression for the limiting distribution of (structural) impulse\nresponses for sieve VAR models based on the Delta method. Yet, numerical\nsimulations have shown that confidence intervals built in such way appear\noverly conservative. In this note I argue that these results stem naturally\nfrom the limit arguments used in L\\\"utkepohl and Poskitt (1991), that they\nmanifest when sieve inference is improperly applied, and that they can be\n\"remedied\" by either using bootstrap resampling or, simply, by using standard\n(non-sieve) asymptotics.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 10:39:28 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Ballarin", "Giovanni", ""]]}, {"id": "2104.13730", "submitter": "Scott Mueller", "authors": "Scott Mueller, Ang Li, Judea Pearl", "title": "Causes of Effects: Learning individual responses from population data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of individualization is recognized as crucial in almost every\nfield. Identifying causes of effects in specific events is likewise essential\nfor accurate decision making. However, such estimates invoke counterfactual\nrelationships, and are therefore indeterminable from population data. For\nexample, the probability of benefiting from a treatment concerns an individual\nhaving a favorable outcome if treated and an unfavorable outcome if untreated.\nExperiments conditioning on fine-grained features are fundamentally inadequate\nbecause we can't test both possibilities for an individual. Tian and Pearl\nprovided bounds on this and other probabilities of causation using a\ncombination of experimental and observational data. Even though those bounds\nwere proven tight, narrower bounds, sometimes significantly so, can be achieved\nwhen structural information is available in the form of a causal model. This\nhas the power to solve central problems, such as explainable AI, legal\nresponsibility, and personalized medicine, all of which demand counterfactual\nlogic. We analyze and expand on existing research by applying bounds to the\nprobability of necessity and sufficiency (PNS) along with graphical criteria\nand practical applications.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 12:38:11 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 06:48:55 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mueller", "Scott", ""], ["Li", "Ang", ""], ["Pearl", "Judea", ""]]}, {"id": "2104.13775", "submitter": "Marco Doretti", "authors": "Martina Raggi, Elena Stanghellini, Marco Doretti", "title": "Path Analysis for Binary Random Variables", "comments": "43 pages, 7 figures. A version of this paper is forthcoming in\n  Sociological Methods & Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The decomposition of the overall effect of a treatment into direct and\nindirect effects is here investigated with reference to a recursive system of\nbinary random variables. We show how, for the single mediator context, the\nmarginal effect measured on the log odds scale can be written as the sum of the\nindirect and direct effects plus a residual term that vanishes under some\nspecific conditions. We then extend our definitions to situations involving\nmultiple mediators and address research questions concerning the decomposition\nof the total effect when some mediators on the pathway from the treatment to\nthe outcome are marginalized over. Connections to the counterfactual\ndefinitions of the effects are also made. Data coming from an encouragement\ndesign on students' attitude to visit museums in Florence, Italy, are\nreanalyzed. The estimates of the defined quantities are reported together with\ntheir standard errors to compute p-values and form confidence intervals.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:04:26 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Raggi", "Martina", ""], ["Stanghellini", "Elena", ""], ["Doretti", "Marco", ""]]}, {"id": "2104.13832", "submitter": "Francesco Denti", "authors": "Francesco Denti, Diego Doimo, Alessandro Laio, Antonietta Mira", "title": "Distributional Results for Model-Based Intrinsic Dimension Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern datasets are characterized by a large number of features that may\nconceal complex dependency structures. To deal with this type of data,\ndimensionality reduction techniques are essential. Numerous dimensionality\nreduction methods rely on the concept of intrinsic dimension, a measure of the\ncomplexity of the dataset. In this article, we first review the TWO-NN model, a\nlikelihood-based intrinsic dimension estimator recently introduced in the\nliterature. The TWO-NN estimator is based on the statistical properties of the\nratio of the distances between a point and its first two nearest neighbors,\nassuming that the points are a realization from an homogeneous Poisson point\nprocess. We extend the TWO-NN theoretical framework by providing novel\ndistributional results of consecutive and generic ratios of distances. These\ndistributional results are then employed to derive intrinsic dimension\nestimators, called Cride and Gride. These novel estimators are more robust to\nnoisy measurements than the TWO-NN and allow the study of the evolution of the\nintrinsic dimension as a function of the scale used to analyze the dataset. We\ndiscuss the properties of the different estimators with the help of simulation\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:37:23 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 17:43:45 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Denti", "Francesco", ""], ["Doimo", "Diego", ""], ["Laio", "Alessandro", ""], ["Mira", "Antonietta", ""]]}, {"id": "2104.13871", "submitter": "Yachong Yang", "authors": "Yachong Yang, Arun Kumar Kuchibhotla", "title": "Finite-sample Efficient Conformal Prediction", "comments": "46 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction is a generic methodology for finite-sample valid\ndistribution-free prediction. This technique has garnered a lot of attention in\nthe literature partly because it can be applied with any machine learning\nalgorithm that provides point predictions to yield valid prediction regions. Of\ncourse, the efficiency (width/volume) of the resulting prediction region\ndepends on the performance of the machine learning algorithm. In this paper, we\nconsider the problem of obtaining the smallest conformal prediction region\ngiven a family of machine learning algorithms. We provide two general-purpose\nselection algorithms and consider coverage as well as width properties of the\nfinal prediction region. The first selection method yields the smallest width\nprediction region among the family of conformal prediction regions for all\nsample sizes, but only has an approximate coverage guarantee. The second\nselection method has a finite sample coverage guarantee but only attains close\nto the smallest width. The approximate optimal width property of the second\nmethod is quantified via an oracle inequality. Asymptotic oracle inequalities\nare also considered when the family of algorithms is given by ridge regression\nwith different penalty parameters.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:36:05 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 06:35:30 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yang", "Yachong", ""], ["Kuchibhotla", "Arun Kumar", ""]]}, {"id": "2104.14008", "submitter": "Zhi Zhao", "authors": "Zhi Zhao, Marco Banterle, Leonardo Bottolo, Sylvia Richardson, Alex\n  Lewin, Manuela Zucknick", "title": "BayesSUR: An R package for high-dimensional multivariate Bayesian\n  variable and covariance selection in linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In molecular biology, advances in high-throughput technologies have made it\npossible to study complex multivariate phenotypes and their simultaneous\nassociations with high-dimensional genomic and other omics data, a problem that\ncan be studied with high-dimensional multi-response regression, where the\nresponse variables are potentially highly correlated. To this purpose, we\nrecently introduced several multivariate Bayesian variable and covariance\nselection models, e.g., Bayesian estimation methods for sparse seemingly\nunrelated regression for variable and covariance selection. Several variable\nselection priors have been implemented in this context, in particular the\nhotspot detection prior for latent variable inclusion indicators, which results\nin sparse variable selection for associations between predictors and multiple\nphenotypes. We also propose an alternative, which uses a Markov random field\n(MRF) prior for incorporating prior knowledge about the dependence structure of\nthe inclusion indicators. Inference of Bayesian seemingly unrelated regression\n(SUR) by Markov chain Monte Carlo methods is made computationally feasible by\nfactorisation of the covariance matrix amongst the response variables. In this\npaper we present BayesSUR, an R package, which allows the user to easily\nspecify and run a range of different Bayesian SUR models, which have been\nimplemented in C++ for computational efficiency. The R package allows the\nspecification of the models in a modular way, where the user chooses the priors\nfor variable selection and for covariance selection separately. We demonstrate\nthe performance of sparse SUR models with the hotspot prior and spike-and-slab\nMRF prior on synthetic and real data sets representing eQTL or mQTL studies and\nin vitro anti-cancer drug screening studies as examples for typical\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 20:29:49 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Zhao", "Zhi", ""], ["Banterle", "Marco", ""], ["Bottolo", "Leonardo", ""], ["Richardson", "Sylvia", ""], ["Lewin", "Alex", ""], ["Zucknick", "Manuela", ""]]}, {"id": "2104.14016", "submitter": "Jonathan Bartlett", "authors": "Jonathan W. Bartlett", "title": "Reference based multiple imputation -- what is the right variance and\n  how to estimate it", "comments": "17 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reference based multiple imputation methods have become popular for handling\nmissing data in randomised clinical trials. Rubin's variance estimator is well\nknown to be biased compared to the reference based imputation estimator's true\nrepeated sampling variance. Somewhat surprisingly given the increasingly\npopularity of these methods, there has been relatively little debate in the\nliterature as to whether Rubin's variance estimator or alternative (smaller)\nvariance estimators targeting the repeated sampling variance are more\nappropriate. We review the arguments made on both sides of this debate, and\nconclude that the repeated sampling variance is more appropriate. We review\ndifferent approaches for estimating the frequentist variance, and suggest a\nrecent proposal for combining bootstrapping with multiple imputation as a\nwidely applicable general solution. At the same time, in light of the\nconsequences of reference based assumptions for frequentist variance, we\nbelieve further scrutiny of these methods is warranted to determine whether the\nthe strength of their assumptions are generally justifiable.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 20:46:37 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Bartlett", "Jonathan W.", ""]]}, {"id": "2104.14054", "submitter": "David Frazier", "authors": "David T. Frazier, Ruben Loaiza-Maya, Gael M. Martin and Bonsoo Koo", "title": "Loss-Based Variational Bayes Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for Bayesian prediction that caters for models with a\nlarge number of parameters and is robust to model misspecification. Given a\nclass of high-dimensional (but parametric) predictive models, this new approach\nconstructs a posterior predictive using a variational approximation to a\nloss-based, or Gibbs, posterior that is directly focused on predictive\naccuracy. The theoretical behavior of the new prediction approach is analyzed\nand a form of optimality demonstrated. Applications to both simulated and\nempirical data using high-dimensional Bayesian neural network and\nautoregressive mixture models demonstrate that the approach provides more\naccurate results than various alternatives, including misspecified\nlikelihood-based predictions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 00:36:08 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Frazier", "David T.", ""], ["Loaiza-Maya", "Ruben", ""], ["Martin", "Gael M.", ""], ["Koo", "Bonsoo", ""]]}, {"id": "2104.14091", "submitter": "Manjari Das", "authors": "Manjari Das and Edward H. Kennedy", "title": "Doubly robust capture-recapture methods for estimating population size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimation of population size using incomplete lists (also called the\ncapture-recapture problem) has a long history across many biological and social\nsciences. For example, human rights and other groups often construct partial\nand overlapping lists of victims of armed conflicts, with the hope of using\nthis information to estimate the total number of victims. Earlier statistical\nmethods for this setup either use potentially restrictive parametric\nassumptions, or else rely on typically suboptimal plug-in-type nonparametric\nestimators; however, both approaches can lead to substantial bias, the former\nvia model misspecification and the latter via smoothing. Under an identifying\nassumption that two lists are conditionally independent given measured\ncovariate information, we make several contributions. First we derive the\nnonparametric efficiency bound for estimating the capture probability, which\nindicates the best possible performance of any estimator, and sheds light on\nthe statistical limits of capture-recapture methods. Then we present a new\nestimator, and study its finite-sample properties, showing that it has a double\nrobustness property new to capture-recapture, and that it is near-optimal in a\nnon-asymptotic sense, under relatively mild nonparametric conditions. Next, we\ngive a method for constructing confidence intervals for total population size\nfrom generic capture probability estimators, and prove non-asymptotic\nnear-validity. Finally, we study our methods in simulations, and apply them to\nestimate the number of killings and disappearances attributable to different\ngroups in Peru during its internal armed conflict between 1980 and 2000.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 03:24:47 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Das", "Manjari", ""], ["Kennedy", "Edward H.", ""]]}, {"id": "2104.14243", "submitter": "Jonathan Rathjens", "authors": "Jonathan Rathjens, Arthur Kolbe, J\\\"urgen H\\\"olzer, Katja Ickstadt and\n  Nadja Klein", "title": "Bivariate Analysis of Birth Weight and Gestational Age Depending on\n  Environmental Exposures: Bayesian Distributional Regression with Copulas", "comments": "25 pages, 7 figures (some of them composed from several pdf files)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we analyze perinatal data with birth weight (BW) as\nprimarily interesting response variable. Gestational age (GA) is usually an\nimportant covariate and included in polynomial form. However, in opposition to\nthis univariate regression, bivariate modeling of BW and GA is recommended to\ndistinguish effects on each, on both, and between them. Rather than a\nparametric bivariate distribution, we apply conditional copula regression,\nwhere marginal distributions of BW and GA (not necessarily of the same form)\ncan be estimated independently, and where the dependence structure is modeled\nconditional on the covariates separately from these marginals. In the resulting\ndistributional regression models, all parameters of the two marginals and the\ncopula parameter are observation-specific. Besides biometric and obstetric\ninformation, data on drinking water contamination and maternal smoking are\nincluded as environmental covariates. While the Gaussian distribution is\nsuitable for BW, the skewed GA data are better modeled by the three-parametric\nDagum distribution. The Clayton copula performs better than the Gumbel and the\nsymmetric Gaussian copula, indicating lower tail dependence (stronger\ndependence when both variables are low), although this non-linear dependence\nbetween BW and GA is surprisingly weak and only influenced by Cesarean section.\nA non-linear trend of BW on GA is detected by a classical univariate model that\nis polynomial with respect to the effect of GA. Linear effects on BW mean are\nsimilar in both models, while our distributional copula regression also reveals\ncovariates' effects on all other parameters.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 10:20:23 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Rathjens", "Jonathan", ""], ["Kolbe", "Arthur", ""], ["H\u00f6lzer", "J\u00fcrgen", ""], ["Ickstadt", "Katja", ""], ["Klein", "Nadja", ""]]}, {"id": "2104.14412", "submitter": "Erniel Barrios", "authors": "Erniel B. Barrios, Paolo Victor T. Redondo", "title": "Nonparametric Test for Volatility in Clustered Multiple Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Contagion arising from clustering of multiple time series like those in the\nstock market indicators can further complicate the nature of volatility,\nrendering a parametric test (relying on asymptotic distribution) to suffer from\nissues on size and power. We propose a test on volatility based on the\nbootstrap method for multiple time series, intended to account for possible\npresence of contagion effect. While the test is fairly robust to distributional\nassumptions, it depends on the nature of volatility. The test is correctly\nsized even in cases where the time series are almost nonstationary. The test is\nalso powerful specially when the time series are stationary in mean and that\nvolatility are contained only in fewer clusters. We illustrate the method in\nglobal stock prices data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 01:35:14 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Barrios", "Erniel B.", ""], ["Redondo", "Paolo Victor T.", ""]]}, {"id": "2104.14423", "submitter": "Rom\\'an Salmer\\'on", "authors": "Rom\\'an Salmer\\'on G\\'omez and Catalina Garc\\'ia Garc\\'ia and Jos\\'e\n  Garc\\'ia P\\'erez", "title": "The Raise Regression: Justification, properties and application", "comments": "25 pages, 2 figures, 9 tables; working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multicollinearity produces an inflation in the variance of the Ordinary Least\nSquares estimators due to the correlation between two or more independent\nvariables (including the constant term). A widely applied solution is to\nestimate with penalized estimators (such as the ridge estimator, the Liu\nestimator, etc.) which exchange the mean square error by the bias. Although the\nvariance diminishes with these procedures, all seems to indicate that the\ninference is lost and also the goodness of fit. Alternatively, the raise\nregression (\\cite{Garcia2011} and \\cite{Salmeron2017}) allows the mitigation of\nthe problems generated by multicollinearity but without losing the inference\nand keeping the coefficient of determination. This paper completely formalizes\nthe raise estimator summarizing all the previous contributions: its mean square\nerror, the variance inflation factor, the condition number, the adequate\nselection of the variable to be raised, the successive raising and the relation\nbetween the raise and the ridge estimator. As a novelty, it is also presented\nthe estimation method, the relation between the raise and the residualization,\nit is analyzed the norm of the estimator and the behaviour of the individual\nand joint significance test and the behaviour of the mean square error and the\ncoefficient of variation. The usefulness of the raise regression as alternative\nto mitigate the multicollinearity is illustrated with two empirical\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 15:41:12 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["G\u00f3mez", "Rom\u00e1n Salmer\u00f3n", ""], ["Garc\u00eda", "Catalina Garc\u00eda", ""], ["P\u00e9rez", "Jos\u00e9 Garc\u00eda", ""]]}, {"id": "2104.14483", "submitter": "Michael Crowther", "authors": "Jonathan Broomfield, Caroline E. Weibull, Michael J. Crowther", "title": "Assessing and relaxing the Markov assumption in the illness-death model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-state survival analysis considers several potential events of interest\nalong a disease pathway. Such analyses are crucial to model complex patient\ntrajectories and are increasingly being used in epidemiological and health\neconomic settings. Multi-state models often make the Markov assumption, whereby\nan individual's future trajectory is dependent only upon their present state,\nnot their past. In reality, there may be transitional dependence upon either\nprevious events and/or more than one timescale, for example time since entry to\nthe current or previous state(s). The aim of this study was to develop an\nillness-death Weibull model allowing for multiple timescales to impact the\nfuture risk of death. Following this, we evaluated the performance of the\nmultiple timescale model against a Markov illness-death model in a set of\nplausible simulation scenarios when the Markov assumption was violated. Guided\nby a study in breast cancer, data were simulated from Weibull baseline\ndistributions, with hazard functions dependent on single and multiple\ntimescales. Markov and non-Markov models were fitted to account for/ignore the\nunderlying data structure. Ignoring the presence of multiple timescales led to\nbias in underlying transition rates between states and associated covariate\neffects, while transition probabilities and lengths of stay were fairly\nrobustly estimated. Further work may be needed to evaluate different estimands\nor more complex multi-state models. Software implementations in Stata are also\ndescribed for simulating and estimating multiple timescale multi-state models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:47:56 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Broomfield", "Jonathan", ""], ["Weibull", "Caroline E.", ""], ["Crowther", "Michael J.", ""]]}, {"id": "2104.14525", "submitter": "Hongyuan Cao Prof", "authors": "Hongyuan Cao and Wei Biao Wu", "title": "Testing and estimation of clustered signals", "comments": null, "journal-ref": "Bernoulli, 2021", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a change-point detection method for large scale multiple testing\nproblems with data having clustered signals. Unlike the classic change-point\nsetup, the signals can vary in size within a cluster. The clustering structure\non the signals enables us to effectively delineate the boundaries between\nsignal and non-signal segments. New test statistics are proposed for\nobservations from one and/or multiple realizations. Their asymptotic\ndistributions are derived. We also study the associated variance estimation\nproblem. We allow the variances to be heteroscedastic in the multiple\nrealization case, which substantially expands the applicability of the proposed\nmethod. Simulation studies demonstrate that the proposed approach has a\nfavorable performance. Our procedure is applied to {an array based Comparative\nGenomic Hybridization (aCGH)} dataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:43:37 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Cao", "Hongyuan", ""], ["Wu", "Wei Biao", ""]]}, {"id": "2104.14601", "submitter": "Stephane Robin", "authors": "Tristan Mary-Huard, Sarmistha Das, Indranil Mukhopadhyay and\n  St\\'ephane Robin", "title": "Querying multiple sets of $p$-values through composed hypothesis testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Combining the results of different experiments to exhibit complex\npatterns or to improve statistical power is a typical aim of data integration.\nThe starting point of the statistical analysis often comes as sets of\n$p$-values resulting from previous analyses, that need to be combined in a\nflexible way to explore complex hypotheses, while guaranteeing a low proportion\nof false discoveries.\n  Results: We introduce the generic concept of {\\sl composed hypothesis}, which\ncorresponds to an arbitrary complex combination of simple hypotheses. We\nrephrase the problem of testing a composed hypothesis as a classification task,\nand show that finding items for which the composed null hypothesis is rejected\nboils down to fitting a mixture model and classify the items according to their\nposterior probabilities. We show that inference can be efficiently performed\nand provide a thorough classification rule to control for type I error. The\nperformance and the usefulness of the approach are illustrated on simulations\nand on two different applications combining data from different types. The\nmethod is scalable, does not require any parameter tuning, and provided\nvaluable biological insight on the considered application cases.\n  Availability: We implement the QCH methodology in the \\texttt{qch} R package\nhosted on CRAN.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 18:40:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Mary-Huard", "Tristan", ""], ["Das", "Sarmistha", ""], ["Mukhopadhyay", "Indranil", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "2104.14620", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Pierre Lafaye de Micheaux, Simos G.\n  Meintanis, Thomas Verdebout", "title": "Nonparametric tests of independence for circular data based on\n  trigonometric moments", "comments": "15 pages, 2 figures, 1 table. Supplementary material: 11 pages, 3\n  figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce nonparametric tests of independence for bivariate circular data\nbased on trigonometric moments. Our contributions lie in (i) proposing\nnonparametric tests that are locally and asymptotically optimal against\nbivariate cosine von Mises alternatives and (ii) extending these tests, via the\nempirical characteristic function, to obtain consistent tests against broader\nsets of alternatives, eventually being omnibus. We thus provide a collection of\ntrigonometric-based tests of varying generality and known optimalities. The\nlarge-sample behaviours of the tests under the null and alternative hypotheses\nare obtained, while simulations show that the new tests are competitive against\nprevious proposals. Two data applications in astronomy and forest science\nillustrate the usage of the tests.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 19:11:58 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["de Micheaux", "Pierre Lafaye", ""], ["Meintanis", "Simos G.", ""], ["Verdebout", "Thomas", ""]]}, {"id": "2104.14664", "submitter": "Derek Hansen", "authors": "Dobrislav Dobrev, Derek Hansen, Pawel Szerszen", "title": "A Randomized Missing Data Approach to Robust Filtering and Forecasting", "comments": "34 pages; 6 figures; 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We put forward a simple new randomized missing data (RMD) approach to robust\nfiltering of state-space models, motivated by the idea that the inclusion of\nonly a small fraction of available highly precise measurements can still\nextract most of the attainable efficiency gains for filtering latent states,\nestimating model parameters, and producing out-of-sample forecasts. In our\ngeneral RMD framework we develop two alternative implementations: endogenous\n(RMD-N) and exogenous (RMD-X) randomization of missing data. A degree of\nrobustness to outliers and model misspecification is achieved by purposely\nrandomizing over the utilized subset of seemingly highly precise but possibly\nmisspecified or outlier contaminated data measurements in their original time\nseries order, while treating the rest as if missing. Time-series dependence is\nthus fully preserved and all available measurements can get utilized subject to\na degree of downweighting depending on the loss function of interest. The\narising robustness-efficiency trade-off is controlled by varying the fraction\nof randomly utilized measurements or the incurred relative efficiency loss. As\nan empirical illustration, we show consistently attractive performance of our\nRMD framework in popular state space models for extracting inflation trends\nalong with model extensions that more directly reflect inflation targeting by\ncentral banks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 21:17:32 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 15:40:19 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 21:37:29 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Dobrev", "Dobrislav", ""], ["Hansen", "Derek", ""], ["Szerszen", "Pawel", ""]]}, {"id": "2104.14676", "submitter": "Robin Dunn", "authors": "Robin Dunn, Aaditya Ramdas, Sivaraman Balakrishnan, Larry Wasserman", "title": "Gaussian Universal Likelihood Ratio Testing", "comments": "46 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood ratio test (LRT) based on the asymptotic chi-squared\ndistribution of the log likelihood is one of the fundamental tools of\nstatistical inference. A recent universal LRT approach based on sample\nsplitting provides valid hypothesis tests and confidence sets in any setting\nfor which we can compute the split likelihood ratio statistic (or, more\ngenerally, an upper bound on the null maximum likelihood). The universal LRT is\nvalid in finite samples and without regularity conditions. This test empowers\nstatisticians to construct tests in settings for which no valid hypothesis test\npreviously existed. For the simple but fundamental case of testing the\npopulation mean of d-dimensional Gaussian data, the usual LRT itself applies\nand thus serves as a perfect test bed to compare against the universal LRT.\nThis work presents the first in-depth exploration of the size, power, and\nrelationships between several universal LRT variants. We show that a repeated\nsubsampling approach is the best choice in terms of size and power. We observe\nreasonable performance even in a high-dimensional setting, where the expected\nsquared radius of the best universal LRT confidence set is approximately 3/2\ntimes the squared radius of the standard LRT-based set. We illustrate the\nbenefits of the universal LRT through testing a non-convex doughnut-shaped null\nhypothesis, where a universal inference procedure can have higher power than a\nstandard approach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 22:10:01 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Dunn", "Robin", ""], ["Ramdas", "Aaditya", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "2104.14695", "submitter": "Tae Kim", "authors": "Tae Hyun Kim, Dan Nicolae", "title": "Dynamic Gene Coexpression Analysis with Correlation Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In many transcriptomic studies, the correlation of genes might fluctuate with\nquantitative factors such as genetic ancestry. We propose a method that models\nthe covariance between two variables to vary against a continuous covariate.\nFor the bivariate case, the proposed score test statistic is computationally\nsimple and robust to model misspecification of the covariance term.\nSubsequently, the method is expanded to test relationships between one highly\nconnected gene, such as a transcription factor, and several other genes for a\nmore global investigation of the dynamic of the coexpression network.\nSimulations show that the proposed method has higher statistical power than\nalternatives, can be used in more diverse scenarios, and is computationally\ncheaper. We apply this method to African American subjects from GTEx to analyze\nthe dynamic behavior of their gene coexpression against genetic ancestry and to\nidentify transcription factors whose coexpression with their target genes\nchange with the genetic ancestry. The proposed method can be applied to a wide\narray of problems that require covariance modeling.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 23:17:58 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Kim", "Tae Hyun", ""], ["Nicolae", "Dan", ""]]}, {"id": "2104.14714", "submitter": "El Hadji Deme", "authors": "El Hadji Mamadou Sall, El Hadji Deme and Abdou K\\^a Diongue", "title": "Adaptive Realized Hyperbolic GARCH Process: Stability and Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an Adaptive Realized Hyperbolic GARCH (A-Realized\nHYGARCH) process to model the long memory of high-frequency time series with\npossible structural breaks. The structural change is modeled by allowing the\nintercept to follow the smooth and flexible function form introduced by Gallant\n(1984). In addition, stability conditions of the process are investigated. A\nMonte Carlo study is investigated in order to illustrate the performance of the\nA-Realized HYGARCH process compared to the Realized HYGARCH with or without\nstructural change.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 01:30:53 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Sall", "El Hadji Mamadou", ""], ["Deme", "El Hadji", ""], ["Diongue", "Abdou K\u00e2", ""]]}, {"id": "2104.14752", "submitter": "Xiudi Li", "authors": "Xiudi Li, Sijia Li and Alex Luedtke", "title": "Estimating the Efficiency Gain of Covariate-Adjusted Analyses in Future\n  Clinical Trials Using External Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for using existing data to estimate the\nefficiency gain from using a covariate-adjusted estimator of a marginal\ntreatment effect in a future randomized trial. We describe conditions under\nwhich it is possible to define a mapping from the distribution that generated\nthe existing external data to the relative efficiency of a covariate-adjusted\nestimator compared to an unadjusted estimator. Under conditions, these relative\nefficiencies approximate the ratio of sample size needed to achieve a desired\npower. We consider two situations where the outcome is either fully or\npartially observed and several treatment effect estimands that are of\nparticular interest in most trials. For each such estimand, we develop a\nsemiparametrically efficient estimator of the relative efficiency that allows\nfor the application of flexible statistical learning tools to estimate the\nnuisance functions and an analytic form of a corresponding Wald-type confidence\ninterval. We also propose a double bootstrap scheme for constructing confidence\nintervals. We demonstrate the performance of the proposed methods through\nsimulation studies and apply these methods to data to estimate the relative\nefficiency of using covariate adjustment in Covid-19 therapeutic trials.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 04:35:30 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Li", "Xiudi", ""], ["Li", "Sijia", ""], ["Luedtke", "Alex", ""]]}, {"id": "2104.14787", "submitter": "Stefan Schrunner", "authors": "Anna Jenul, Stefan Schrunner, J\\\"urgen Pilz, Oliver Tomic", "title": "A User-Guided Bayesian Framework for Ensemble Feature Selection in Life\n  Science Applications (UBayFS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training machine learning models on high-dimensional datasets is a\nchallenging task and requires measures to prevent overfitting and to keep model\ncomplexity low. Feature selection, which represents such a measure, plays a key\nrole in data preprocessing and may provide insights into the systematic\nvariation in the data. The latter aspect is crucial in domains that rely on\nmodel interpretability, such as life sciences. We propose UBayFS, an ensemble\nfeature selection technique, embedded in a Bayesian statistical framework. Our\napproach considers two sources of information: data and domain knowledge. We\nbuild an ensemble of elementary feature selectors that extract information from\nempirical data and aggregate this information to form a meta-model, which\ncompensates for inconsistencies between elementary feature selectors. The user\nguides UBayFS by weighting features and penalizing specific feature blocks or\ncombinations. The framework builds on a multinomial likelihood and a novel\nversion of constrained Dirichlet-type prior distribution, involving initial\nfeature weights and side constraints. In a quantitative evaluation, we\ndemonstrate that the presented framework allows for a balanced trade-off\nbetween user knowledge and data observations. A comparison with standard\nfeature selectors underlines that UBayFS achieves competitive performance,\nwhile providing additional flexibility to incorporate domain knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 06:51:33 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 16:21:17 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Jenul", "Anna", ""], ["Schrunner", "Stefan", ""], ["Pilz", "J\u00fcrgen", ""], ["Tomic", "Oliver", ""]]}, {"id": "2104.14821", "submitter": "Ayush Deva", "authors": "Ayush Deva, Siddhant Shingi, Avtansh Tiwari, Nayana Bannur, Sansiddh\n  Jain, Jerome White, Alpan Raval, Srujana Merugu", "title": "Interpretability of Epidemiological Models : The Curse of\n  Non-Identifiability", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interpretability of epidemiological models is a key consideration, especially\nwhen these models are used in a public health setting. Interpretability is\nstrongly linked to the identifiability of the underlying model parameters,\ni.e., the ability to estimate parameter values with high confidence given\nobservations. In this paper, we define three separate notions of\nidentifiability that explore the different roles played by the model\ndefinition, the loss function, the fitting methodology, and the quality and\nquantity of data. We define an epidemiological compartmental model framework in\nwhich we highlight these non-identifiability issues and their mitigation.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 08:11:11 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Deva", "Ayush", ""], ["Shingi", "Siddhant", ""], ["Tiwari", "Avtansh", ""], ["Bannur", "Nayana", ""], ["Jain", "Sansiddh", ""], ["White", "Jerome", ""], ["Raval", "Alpan", ""], ["Merugu", "Srujana", ""]]}, {"id": "2104.14827", "submitter": "Xiaoli Gao", "authors": "Xiaoli Gao and Ejaz Ahmed", "title": "Joint Linear Trend Recovery Using L1 Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the recovery of a joint piece-wise linear trend from a\ntime series using L1 regularization approach, called L1 trend filtering (Kim,\nKoh and Boyd, 2009). We provide some sufficient conditions under which a L1\ntrend filter can be well-behaved in terms of mean estimation and change point\ndetection. The result is two-fold: for the mean estimation, an almost optimal\nconsistent rate is obtained; for the change point detection, the slope change\nin direction can be recovered in a high probability. In addition, we show that\nthe weak irrepresentable condition, a necessary condition for LASSO model to be\nsign consistent (Zhao and Yu, 2006), is not necessary for the consistent change\npoint detection. The performance of the L1 trend filter is evaluated by some\nfinite sample simulations studies.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 08:20:06 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Gao", "Xiaoli", ""], ["Ahmed", "Ejaz", ""]]}, {"id": "2104.14923", "submitter": "Helen Barnett", "authors": "Helen Yvette Barnett, Matthew George, Donia Skanji, Gaelle\n  Saint-Hilary, Thomas Jaki, Pavel Mozgunov", "title": "A Comparison of Model-Free Phase I Dose Escalation Designs for\n  Dual-Agent Combination Therapies", "comments": "29 pages, 6 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is increasingly common for therapies in oncology to be given in\ncombination. In some cases, patients can benefit from the interaction between\ntwo drugs, although often at the risk of higher toxicity. A large number of\ndesigns to conduct phase I trials in this setting are available, where the\nobjective is to select the maximum tolerated dose combination (MTC). Recently,\na number of model-free (also called model-assisted) designs have provoked\ninterest, providing several practical advantages over the more conventional\napproaches of rule-based or model-based designs. In this paper, we demonstrate\na novel calibration procedure for model-free designs to determine their most\ndesirable parameters. Under the calibration procedure, we compare the behaviour\nof model-free designs to a model-based approach in a comprehensive simulation\nstudy, covering a number of clinically plausible scenarios. It is found that\nmodel-free designs are competitive with the model-based design in terms of the\nproportion of correct selections of the MTC. However, there are a number of\nscenarios in which model-free designs offer a safer alternative. This is also\nillustrated in the application of the designs to a case study using data from a\nphase I oncology trial.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:40:18 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Barnett", "Helen Yvette", ""], ["George", "Matthew", ""], ["Skanji", "Donia", ""], ["Saint-Hilary", "Gaelle", ""], ["Jaki", "Thomas", ""], ["Mozgunov", "Pavel", ""]]}, {"id": "2104.14952", "submitter": "Nathaniel Josephs", "authors": "Nathaniel Josephs, Wenrui Li, and Eric D. Kolaczyk", "title": "Network Recovery from Unlabeled Noisy Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing literature on the statistical analysis of multiple\nnetworks in which the network is the fundamental data object. However, most of\nthis work requires networks on a shared set of labeled vertices. In this work,\nwe consider the question of recovering a parent network based on noisy\nunlabeled samples. We identify a specific regime in the noisy network\nliterature for recovery that is asymptotically unbiased and computationally\ntractable based on a three-stage recovery procedure: first, we align the\nnetworks via a sequential pairwise graph matching procedure; next, we compute\nthe sample average of the aligned networks; finally, we obtain an estimate of\nthe parent by thresholding the sample average. Previous work on multiple\nunlabeled networks is only possible for trivial networks due to the complexity\nof brute-force computations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 12:34:22 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Josephs", "Nathaniel", ""], ["Li", "Wenrui", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "2104.14977", "submitter": "Yikun Zhang", "authors": "Yikun Zhang and Yen-Chi Chen", "title": "Linear Convergence of the Subspace Constrained Mean Shift Algorithm:\n  From Euclidean to Directional Data", "comments": "75 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies linear convergence of the subspace constrained mean shift\n(SCMS) algorithm, a well-known algorithm for identifying a density ridge\ndefined by a kernel density estimator. By arguing that the SCMS algorithm is a\nspecial variant of a subspace constrained gradient ascent (SCGA) algorithm with\nan adaptive step size, we derive linear convergence of such SCGA algorithm.\nWhile the existing research focuses mainly on density ridges in the Euclidean\nspace, we generalize density ridges and the SCMS algorithm to directional data.\nIn particular, we establish the stability theorem of density ridges with\ndirectional data and prove the linear convergence of our proposed directional\nSCMS algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 01:46:35 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhang", "Yikun", ""], ["Chen", "Yen-Chi", ""]]}, {"id": "2104.14987", "submitter": "Hossein Mohammadi", "authors": "Hossein Mohammadi, Peter Challenor, Marc Goodfellow", "title": "Emulating computationally expensive dynamical simulators using Gaussian\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Gaussian process (GP)-based methodology is proposed to emulate\ncomputationally expensive dynamical computer models or simulators. The method\nrelies on emulating the short-time numerical flow map of the model. The flow\nmap returns the solution of a dynamic system at an arbitrary time for a given\ninitial condition. The prediction of the flow map is performed via a GP whose\nkernel is estimated using random Fourier features. This gives a distribution\nover the flow map such that each realisation serves as an approximation to the\nflow map. A realisation is then employed in an iterative manner to perform\none-step ahead predictions and forecast the whole time series. Repeating this\nprocedure with multiple draws from the emulated flow map provides a probability\ndistribution over the time series. The mean and variance of that distribution\nare used as the model output prediction and a measure of the associated\nuncertainty, respectively. The proposed method is used to emulate several\ndynamic non-linear simulators including the well-known Lorenz attractor and van\nder Pol oscillator. The results show that our approach has a high prediction\nperformance in emulating such systems with an accurate representation of the\nprediction uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 09:45:44 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 18:56:02 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Mohammadi", "Hossein", ""], ["Challenor", "Peter", ""], ["Goodfellow", "Marc", ""]]}, {"id": "2104.15028", "submitter": "Claude Renaux", "authors": "Claude Renaux, Peter B\\\"uhlmann", "title": "Efficient Multiple Testing Adjustment for Hierarchical Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical inference in (generalized) regression problems is powerful for\nfinding significant groups or even single covariates, especially in\nhigh-dimensional settings where identifiability of the entire regression\nparameter vector may be ill-posed. The general method proceeds in a fully\ndata-driven and adaptive way from large to small groups or singletons of\ncovariates, depending on the signal strength and the correlation structure of\nthe design matrix. We propose a novel hierarchical multiple testing adjustment\nthat can be used in combination with any significance test for a group of\ncovariates to perform hierarchical inference. Our adjustment passes on the\nsignificance level of certain hypotheses that could not be rejected and is\nshown to guarantee strong control of the familywise error rate. Our method is\nat least as powerful as a so-called depth-wise hierarchical Bonferroni\nadjustment. It provides a substantial gain in power over other previously\nproposed inheritance hierarchical procedures if the underlying alternative\nhypotheses occur sparsely along a few branches in the tree-structured\nhierarchy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 14:30:17 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Renaux", "Claude", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2104.15140", "submitter": "Nabarun Deb", "authors": "Arnab Auddy, Nabarun Deb, and Sagnik Nandy", "title": "Exact Detection Thresholds for Chatterjee's Correlation", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Chatterjee (2021) introduced a new rank-based correlation\ncoefficient which can be used to test for independence between two random\nvariables. His test has already attracted much attention as it is\ndistribution-free, consistent against all fixed alternatives, asymptotically\nnormal under the null hypothesis of independence and computable in (near)\nlinear time; thereby making it appropriate for large-scale applications.\nHowever, not much is known about the power properties of this test beyond\nconsistency against fixed alternatives. In this paper, we bridge this gap by\nobtaining the asymptotic distribution of Chatterjee's correlation under any\nchanging sequence of alternatives \"converging\" to the null hypothesis (of\nindependence). We further obtain a general result that gives exact detection\nthresholds and limiting power for Chatterjee's test of independence under\nnatural nonparametric alternatives \"converging\" to the null. As applications of\nthis general result, we prove a non-standard $n^{-1/4}$ detection boundary for\nthis test and compute explicitly the limiting local power on the detection\nboundary, for popularly studied alternatives in literature such as mixture\nmodels, rotation models and noisy nonparametric regression. Moreover our\nconvergence results provide explicit finite sample bounds that depend on the\n\"distance\" between the null and the alternative. Our proof techniques rely on\nsecond order Poincar\\'{e} type inequalities and a non-asymptotic projection\ntheorem.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 17:58:50 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Auddy", "Arnab", ""], ["Deb", "Nabarun", ""], ["Nandy", "Sagnik", ""]]}]