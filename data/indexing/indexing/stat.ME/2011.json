[{"id": "2011.00119", "submitter": "Hui Zou", "authors": "Le Zhou, R. Dennis Cook and Hui Zou", "title": "Enveloped Huber Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huber regression (HR) is a popular robust alternative to the least squares\nregression when the error follows a heavy-tailed distribution. We propose a new\nmethod called the enveloped Huber regression (EHR) by considering the envelope\nassumption that there exists some subspace of the predictors that has no\nassociation with the response, which is referred to as the immaterial part.\nMore efficient estimation is achieved via the removal of the immaterial part.\nDifferent from the envelope least squares (ENV) model whose estimation is based\non maximum normal likelihood, the estimation of the EHR model is through\nGeneralized Method of Moments. The asymptotic normality of the EHR estimator is\nestablished, and it is shown that EHR is more efficient than HR. Moreover, EHR\nis more efficient than ENV when the error distribution is heavy-tailed, while\nmaintaining a small efficiency loss when the error distribution is normal.\nMoreover, our theory also covers the heteroscedastic case in which the error\nmay depend on the covariates. Extensive simulation studies confirm the messages\nfrom the asymptotic theory. EHR is further illustrated on a real dataset.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 22:03:18 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhou", "Le", ""], ["Cook", "R. Dennis", ""], ["Zou", "Hui", ""]]}, {"id": "2011.00138", "submitter": "Jun Li", "authors": "Jun Li, Juliane Manitz, Enrico Bertuzzo, Eric D. Kolaczyk", "title": "Sensor-based localization of epidemic sources on human mobility networks", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1008545", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the source detection problem in epidemiology, which is one of\nthe most important issues for control of epidemics. Mathematically, we\nreformulate the problem as one of identifying the relevant component in a\nmultivariate Gaussian mixture model. Focusing on the study of cholera and\ndiseases with similar modes of transmission, we calibrate the parameters of our\nmixture model using human mobility networks within a stochastic, spatially\nexplicit epidemiological model for waterborne disease. Furthermore, we adopt a\nBayesian perspective, so that prior information on source location can be\nincorporated (e.g., reflecting the impact of local conditions). Posterior-based\ninference is performed, which permits estimates in the form of either\nindividual locations or regions. Importantly, our estimator only requires\nfirst-arrival times of the epidemic by putative observers, typically located\nonly at a small proportion of nodes. The proposed method is demonstrated within\nthe context of the 2000-2002 cholera outbreak in the KwaZulu-Natal province of\nSouth Africa.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 23:11:51 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Jun", ""], ["Manitz", "Juliane", ""], ["Bertuzzo", "Enrico", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "2011.00159", "submitter": "Xiaowu Dai", "authors": "Xiaowu Dai and Michael I. Jordan", "title": "Learning Strategies in Decentralized Matching Markets under Uncertain\n  Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two-sided decentralized matching markets in which participants have\nuncertain preferences. We present a statistical model to learn the preferences.\nThe model incorporates uncertain state and the participants' competition on one\nside of the market. We derive an optimal strategy that maximizes the agent's\nexpected payoff and calibrate the uncertain state by taking the opportunity\ncosts into account. We discuss the sense in which the matching derived from the\nproposed strategy has a stability property. We also prove a fairness property\nthat asserts that there exists no justified envy according to the proposed\nstrategy. We provide numerical results to demonstrate the improved payoff,\nstability and fairness, compared to alternative methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 03:08:22 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Dai", "Xiaowu", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2011.00216", "submitter": "Thomas Berrett", "authors": "Thomas Berrett, L\\'aszl\\'o Gy\\\"orfi, Harro Walk", "title": "Strongly universally consistent nonparametric regression and\n  classification with privatised data", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit the classical problem of nonparametric regression,\nbut impose local differential privacy constraints. Under such constraints, the\nraw data $(X_1,Y_1),\\ldots,(X_n,Y_n)$, taking values in $\\mathbb{R}^d \\times\n\\mathbb{R}$, cannot be directly observed, and all estimators are functions of\nthe randomised output from a suitable privacy mechanism. The statistician is\nfree to choose the form of the privacy mechanism, and here we add Laplace\ndistributed noise to a discretisation of the location of a feature vector $X_i$\nand to the value of its response variable $Y_i$. Based on this randomised data,\nwe design a novel estimator of the regression function, which can be viewed as\na privatised version of the well-studied partitioning regression estimator. The\nmain result is that the estimator is strongly universally consistent. Our\nmethods and analysis also give rise to a strongly universally consistent binary\nclassification rule for locally differentially private data.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 09:00:43 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Berrett", "Thomas", ""], ["Gy\u00f6rfi", "L\u00e1szl\u00f3", ""], ["Walk", "Harro", ""]]}, {"id": "2011.00289", "submitter": "Edoardo Belli", "authors": "Edoardo Belli", "title": "Smoothly Adaptively Centered Ridge Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a focus on linear models with smooth functional covariates, we propose a\npenalization framework (SACR) based on the nonzero centered ridge, where the\ncenter of the penalty is optimally reweighted in a supervised way, starting\nfrom the ordinary ridge solution as the initial centerfunction. In particular,\nwe introduce a convex formulation that jointly estimates the model's\ncoefficients and the weight function, with a roughness penalty on the\ncenterfunction and constraints on the weights in order to recover a possibly\nsmooth and/or sparse solution. This allows for a non-iterative and continuous\nvariable selection mechanism, as the weight function can either inflate or\ndeflate the initial center, in order to target the penalty towards a suitable\ncenter, with the objective to reduce the unwanted shrinkage on the nonzero\ncoefficients, instead of uniformly shrinking the whole coefficient function. As\nempirical evidence of the interpretability and predictive power of our method,\nwe provide a simulation study and two real world spectroscopy applications with\nboth classification and regression.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 15:04:23 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Belli", "Edoardo", ""]]}, {"id": "2011.00321", "submitter": "Fan Yin", "authors": "Fan Yin, Domarin Khago, Rachel W. Martin, Carter T. Butts", "title": "Bayesian Analysis of Static Light Scattering Data for Globular Proteins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static light scattering is a popular physical chemistry technique that\nenables calculation of physical attributes such as the radius of gyration and\nthe second virial coefficient for a macromolecule (e.g., a polymer or a\nprotein) in solution. The second virial coefficient is a physical quantity that\ncharacterizes the magnitude and sign of pairwise interactions between\nparticles, and hence is related to aggregation propensity, a property of\nconsiderable scientific and practical interest. Estimating the second virial\ncoefficient from experimental data is challenging due both to the degree of\nprecision required and the complexity of the error structure involved. In\ncontrast to conventional approaches based on heuristic OLS estimates, Bayesian\ninference for the second virial coefficient allows explicit modeling of error\nprocesses, incorporation of prior information, and the ability to directly test\ncompeting physical models. Here, we introduce a fully Bayesian model for static\nlight scattering experiments on small-particle systems, with joint inference\nfor concentration, index of refraction, oligomer size, and the second virial\ncoefficient. We apply our proposed model to study the aggregation behavior of\nhen egg-white lysozyme and human gammaS-crystallin using in-house experimental\ndata. Based on these observations, we also perform a simulation study on the\nprimary drivers of uncertainty in this family of experiments, showing in\nparticular the potential for improved monitoring and control of concentration\nto aid inference.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 17:32:05 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yin", "Fan", ""], ["Khago", "Domarin", ""], ["Martin", "Rachel W.", ""], ["Butts", "Carter T.", ""]]}, {"id": "2011.00360", "submitter": "Yajuan Si", "authors": "Yajuan Si", "title": "On the Use of Auxiliary Variables in Multilevel Regression and\n  Poststratification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilevel regression and poststratification (MRP) has been a popular\napproach for selection bias adjustment and subgroup estimation, with successful\nand widespread applications from social sciences to health sciences. We\ndemonstrate the capability of MRP to handle the methodological and\ncomputational issues in data integration and inferences of probability and\nnonprobability-based surveys, and the broad extensions in practical\napplications. Our development is motivated by the Adolescent Brain Cognitive\nDevelopment (ABCD) Study that has collected children across 21 U.S. geographic\nlocations for national representation but is subject to selection bias, a\ncommon problem of nonprobability samples. Though treated as the gold standard\nin public opinion research, MRP is a statistical technique that has assumptions\nand pitfalls, the validity of which prominently depends on the quality of\navailable auxiliary information. In this paper, we develop the statistical\nfoundation of how to incorporate auxiliary variables under MRP. We build up a\nsystematic framework under MRP for statistical data integration and inferences.\nOur simulation studies indicate the statistical validity of MRP with a tradeoff\nbetween robustness and efficiency and present the improvement over alternative\nmethods. We apply the approach to evaluate cognition performances of diverse\ngroups of children in the ABCD study and find that the adjustment of auxiliary\nvariables has a substantial effect on the inference results.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 20:58:25 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Si", "Yajuan", ""]]}, {"id": "2011.00373", "submitter": "Michael Pollmann", "authors": "Michael Pollmann", "title": "Causal Inference for Spatial Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a framework, estimators, and inference procedures for the analysis\nof causal effects in a setting with spatial treatments. Many events and\npolicies (treatments), such as opening of businesses, building of hospitals,\nand sources of pollution, occur at specific spatial locations, with researchers\ninterested in their effects on nearby individuals or businesses (outcome\nunits). However, the existing treatment effects literature primarily considers\ntreatments that could be assigned directly at the level of the outcome units,\npotentially with spillover effects. I approach the spatial treatment setting\nfrom a similar experimental perspective: What ideal experiment would we design\nto estimate the causal effects of spatial treatments? This perspective\nmotivates a comparison between individuals near realized treatment locations\nand individuals near unrealized candidate locations, which is distinct from\ncurrent empirical practice. Furthermore, I show how to find such candidate\nlocations and apply the proposed methods with observational data. I apply the\nproposed methods to study the causal effects of grocery stores on foot traffic\nto nearby businesses during COVID-19 lockdowns.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 22:09:26 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Pollmann", "Michael", ""]]}, {"id": "2011.00404", "submitter": "Tao Liu", "authors": "Tao Liu, Joseph W Hogan, Wanning Su, Yizhen Xu, Michael J Daniels,\n  Kantor Rami", "title": "Informed Pooled Testing with Quantitative Assays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooled testing is widely used for screening for viral or bacterial infections\nwith low prevalence when individual testing is not cost-efficient. Pooled\ntesting with qualitative assays that give binary results has been well-studied.\nHowever, characteristics of pooling with quantitative assays were mostly\ndemonstrated using simulations or empirical studies. We investigate properties\nof three pooling strategies with quantitative assays: traditional two-stage\nmini-pooling (MP) (Dorfman, 1943), mini-pooling with deconvolution algorithm\n(MPA) (May et al., 2010), and marker-assisted MPA (mMPA) (Liu et al., 2017).\nMPA and mMPA test individuals in a sequence after a positive pool and implement\na deconvolution algorithm to determine when testing can cease to ascertain all\nindividual statuses. mMPA uses information from other available markers to\ndetermine an optimal order for individual testings. We derive and compare the\ngeneral statistical properties of the three pooling methods. We show that with\na proper pool size, MP, MPA, and mMPA can be more cost-efficient than\nindividual testing, and mMPA is superior to MPA and MP. For diagnostic\naccuracy, mMPA and MPA have higher specificity and positive predictive value\nbut lower sensitivity and negative predictive value than MP and individual\ntesting. Included in this paper are applications to various simulations and an\napplication for HIV treatment monitoring.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 02:22:02 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Liu", "Tao", ""], ["Hogan", "Joseph W", ""], ["Su", "Wanning", ""], ["Xu", "Yizhen", ""], ["Daniels", "Michael J", ""], ["Rami", "Kantor", ""]]}, {"id": "2011.00415", "submitter": "Tim G. J. Rudner", "authors": "Tim G. J. Rudner, Dino Sejdinovic, Yarin Gal", "title": "Inter-domain Deep Gaussian Processes", "comments": "Published in Proceedings of the 37th International Conference on\n  Machine Learning (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-domain Gaussian processes (GPs) allow for high flexibility and low\ncomputational cost when performing approximate inference in GP models. They are\nparticularly suitable for modeling data exhibiting global structure but are\nlimited to stationary covariance functions and thus fail to model\nnon-stationary data effectively. We propose Inter-domain Deep Gaussian\nProcesses, an extension of inter-domain shallow GPs that combines the\nadvantages of inter-domain and deep Gaussian processes (DGPs), and demonstrate\nhow to leverage existing approximate inference methods to perform simple and\nscalable approximate inference using inter-domain features in DGPs. We assess\nthe performance of our method on a range of regression tasks and demonstrate\nthat it outperforms inter-domain shallow GPs and conventional DGPs on\nchallenging large-scale real-world datasets exhibiting both global structure as\nwell as a high-degree of non-stationarity.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 04:03:35 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Rudner", "Tim G. J.", ""], ["Sejdinovic", "Dino", ""], ["Gal", "Yarin", ""]]}, {"id": "2011.00442", "submitter": "Kin Yau Wong", "authors": "Hoi Min Ng, Binyan Jiang and Kin Yau Wong", "title": "Penalized estimation for single-index varying-coefficient models with\n  applications to integrative genomic analysis", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technological advances have made it possible to collect\nhigh-dimensional genomic data along with clinical data on a large number of\nsubjects. In the studies of chronic diseases such as cancer, it is of great\ninterest to integrate clinical and genomic data to build a comprehensive\nunderstanding of the disease mechanisms. Despite extensive studies on\nintegrative analysis, it remains an ongoing challenge to model the interaction\neffects between clinical and genomic variables, due to high-dimensionality of\nthe data and heterogeneity across data types. In this paper, we propose an\nintegrative approach that models interaction effects using a single-index\nvarying-coefficient model, where the effects of genomic features can be\nmodified by clinical variables. We propose a penalized approach for separate\nselection of main and interaction effects. We demonstrate the advantages of the\nproposed methods through extensive simulation studies and provide applications\nto a motivating cancer genomic study.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 07:18:34 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ng", "Hoi Min", ""], ["Jiang", "Binyan", ""], ["Wong", "Kin Yau", ""]]}, {"id": "2011.00515", "submitter": "Tim G. J. Rudner", "authors": "Tim G. J. Rudner, Oscar Key, Yarin Gal, Tom Rainforth", "title": "On Signal-to-Noise Ratio Issues in Variational Inference for Deep\n  Gaussian Processes", "comments": "Published in Proceedings of the 38th International Conference on\n  Machine Learning (ICML 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the gradient estimates used in training Deep Gaussian Processes\n(DGPs) with importance-weighted variational inference are susceptible to\nsignal-to-noise ratio (SNR) issues. Specifically, we show both theoretically\nand via an extensive empirical evaluation that the SNR of the gradient\nestimates for the latent variable's variational parameters decreases as the\nnumber of importance samples increases. As a result, these gradient estimates\ndegrade to pure noise if the number of importance samples is too large. To\naddress this pathology, we show how doubly reparameterized gradient estimators,\noriginally proposed for training variational autoencoders, can be adapted to\nthe DGP setting and that the resultant estimators completely remedy the SNR\nissue, thereby providing more reliable training. Finally, we demonstrate that\nour fix can lead to consistent improvements in the predictive performance of\nDGP models.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 14:38:02 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 12:14:08 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Rudner", "Tim G. J.", ""], ["Key", "Oscar", ""], ["Gal", "Yarin", ""], ["Rainforth", "Tom", ""]]}, {"id": "2011.00544", "submitter": "Vartan Choulakian", "authors": "Vartan Choulakian", "title": "Comments on \"correspondence analysis makes you blind\"", "comments": "20 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collins' (2002) statement \"correspondence analysis makes you blind\" followed\nafter his seriation like description of a brand attribute count data set\nanalyzed by Whitlark and Smith (2001), who applied correspondence analysis. In\nthis essay we comment on Collins' statement within taxicab correspondence\nanalysis framework by simultaneously decomposing the covariance matrix and its\nassociated density matrix, thus interpreting two interrelated maps for\ncontingency tables : TCov map and TCA map.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 15:59:51 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Choulakian", "Vartan", ""]]}, {"id": "2011.00591", "submitter": "Alex Diana", "authors": "Alex Diana, Eleni Matechou, Jim Griffin, Todd Arnold, Richard\n  Griffiths, John Pickering, Simone Tenan, Stefano Volponi", "title": "A general modelling framework for open wildlife populations based on the\n  Polya Tree prior", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wildlife monitoring for open populations can be performed using a number of\ndifferent survey methods. Each survey method gives rise to a type of data and,\nin the last five decades, a large number of associated statistical models have\nbeen developed for analysing these data. Although these models have been\nparameterised and fitted using different approaches, they have all been\ndesigned to model the pattern with which individuals enter and exit the\npopulation and to estimate the population size. However, existing approaches\nrely on a predefined model structure and complexity, either by assuming that\nparameters are specific to sampling occasions, or by employing parametric\ncurves. Instead, we propose a novel Bayesian nonparametric framework for\nmodelling entry and exit patterns based on the Polya Tree (PT) prior for\ndensities. Our Bayesian non-parametric approach avoids overfitting when\ninferring entry and exit patterns while simultaneously allowing more\nflexibility than is possible using parametric curves. We apply our new\nframework to capture-recapture, count and ring-recovery data and we introduce\nthe replicated PT prior for defining classes of models for these data.\nAdditionally, we define the Hierarchical Logistic PT prior for jointly\nmodelling related data and we consider the Optional PT prior for modelling long\ntime series of data. We demonstrate our new approach using five different case\nstudies on birds, amphibians and insects.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 18:46:44 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Diana", "Alex", ""], ["Matechou", "Eleni", ""], ["Griffin", "Jim", ""], ["Arnold", "Todd", ""], ["Griffiths", "Richard", ""], ["Pickering", "John", ""], ["Tenan", "Simone", ""], ["Volponi", "Stefano", ""]]}, {"id": "2011.00641", "submitter": "Chandler Squires", "authors": "Chandler Squires, Sara Magliacane, Kristjan Greenewald, Dmitriy Katz,\n  Murat Kocaoglu, Karthikeyan Shanmugam", "title": "Active Structure Learning of Causal DAGs via Directed Clique Tree", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing body of work has begun to study intervention design for efficient\nstructure learning of causal directed acyclic graphs (DAGs). A typical setting\nis a causally sufficient setting, i.e. a system with no latent confounders,\nselection bias, or feedback, when the essential graph of the observational\nequivalence class (EC) is given as an input and interventions are assumed to be\nnoiseless. Most existing works focus on worst-case or average-case lower bounds\nfor the number of interventions required to orient a DAG. These worst-case\nlower bounds only establish that the largest clique in the essential graph\ncould make it difficult to learn the true DAG. In this work, we develop a\nuniversal lower bound for single-node interventions that establishes that the\nlargest clique is always a fundamental impediment to structure learning.\nSpecifically, we present a decomposition of a DAG into independently orientable\ncomponents through directed clique trees and use it to prove that the number of\nsingle-node interventions necessary to orient any DAG in an EC is at least the\nsum of half the size of the largest cliques in each chain component of the\nessential graph. Moreover, we present a two-phase intervention design algorithm\nthat, under certain conditions on the chordal skeleton, matches the optimal\nnumber of interventions up to a multiplicative logarithmic factor in the number\nof maximal cliques. We show via synthetic experiments that our algorithm can\nscale to much larger graphs than most of the related work and achieves better\nworst-case performance than other scalable approaches. A code base to recreate\nthese results can be found at https://github.com/csquires/dct-policy\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 23:11:17 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Squires", "Chandler", ""], ["Magliacane", "Sara", ""], ["Greenewald", "Kristjan", ""], ["Katz", "Dmitriy", ""], ["Kocaoglu", "Murat", ""], ["Shanmugam", "Karthikeyan", ""]]}, {"id": "2011.00647", "submitter": "Jiangzhou Wang", "authors": "Jiangzhou Wang, Jingfei Zhang, Binghui Liu, Ji Zhu, and Jianhua Guo", "title": "Fast Network Community Detection with Profile-Pseudo Likelihood Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model is one of the most studied network models for\ncommunity detection. It is well-known that most algorithms proposed for fitting\nthe stochastic block model likelihood function cannot scale to large-scale\nnetworks. One prominent work that overcomes this computational challenge is\nAmini et al.(2013), which proposed a fast pseudo-likelihood approach for\nfitting stochastic block models to large sparse networks. However, this\napproach does not have convergence guarantee, and is not well suited for small-\nor medium- scale networks. In this article, we propose a novel likelihood based\napproach that decouples row and column labels in the likelihood function, which\nenables a fast alternating maximization; the new method is computationally\nefficient, performs well for both small and large scale networks, and has\nprovable convergence guarantee. We show that our method provides strongly\nconsistent estimates of the communities in a stochastic block model. As\ndemonstrated in simulation studies, the proposed method outperforms the\npseudo-likelihood approach in terms of both estimation accuracy and computation\nefficiency, especially for large sparse networks. We further consider\nextensions of our proposed method to handle networks with degree heterogeneity\nand bipartite properties.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 23:40:26 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 10:46:50 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Wang", "Jiangzhou", ""], ["Zhang", "Jingfei", ""], ["Liu", "Binghui", ""], ["Zhu", "Ji", ""], ["Guo", "Jianhua", ""]]}, {"id": "2011.00680", "submitter": "Jiaxin Zhang", "authors": "Jiaxin Zhang", "title": "Modern Monte Carlo Methods for Efficient Uncertainty Quantification and\n  Propagation: A Survey", "comments": "This review paper has been accepted by WIREs Computational Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification (UQ) includes the characterization, integration,\nand propagation of uncertainties that result from stochastic variations and a\nlack of knowledge or data in the natural world. Monte Carlo (MC) method is a\nsampling-based approach that has widely used for quantification and propagation\nof uncertainties. However, the standard MC method is often time-consuming if\nthe simulation-based model is computationally intensive. This article gives an\noverview of modern MC methods to address the existing challenges of the\nstandard MC in the context of UQ. Specifically, multilevel Monte Carlo (MLMC)\nextending the concept of control variates achieves a significant reduction of\nthe computational cost by performing most evaluations with low accuracy and\ncorresponding low cost, and relatively few evaluations at high accuracy and\ncorrespondingly high cost. Multifidelity Monte Carlo (MFMC) accelerates the\nconvergence of standard Monte Carlo by generalizing the control variates with\ndifferent models having varying fidelities and varying computational costs.\nMultimodel Monte Carlo method (MMMC), having a different setting of MLMC and\nMFMC, aims to address the issue of uncertainty quantification and propagation\nwhen data for characterizing probability distributions are limited. Multimodel\ninference combined with importance sampling is proposed for quantifying and\nefficiently propagating the uncertainties resulting from small datasets. All of\nthese three modern MC methods achieve a significant improvement of\ncomputational efficiency for probabilistic UQ, particularly uncertainty\npropagation. An algorithm summary and the corresponding code implementation are\nprovided for each of the modern Monte Carlo methods. The extension and\napplication of these methods are discussed in detail.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 02:02:35 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhang", "Jiaxin", ""]]}, {"id": "2011.00725", "submitter": "Fei Gao", "authors": "Fei Gao, David V. Glidden, James P. Hughes, Deborah Donnell", "title": "Sample Size Calculation for Active-Arm Trial with Counterfactual\n  Incidence Based on Recency Assay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has seen tremendous progress in the development of biomedical\nagents that are effective as pre-exposure prophylaxis (PrEP) for HIV\nprevention. To expand the choice of products and delivery methods, new\nmedications and delivery methods are under development. Future trials of\nnon-inferiority, given the high efficacy of ARV-based PrEP products as they\nbecome current or future standard of care, would require a large number of\nparticipants and long follow-up time that may not be feasible. This motivates\nthe construction of a counterfactual estimate that approximates incidence for a\nrandomized concurrent control group receiving no PrEP. We propose an approach\nthat is to enroll a cohort of prospective PrEP users and augment screening for\nHIV with laboratory markers of duration of HIV infection to indicate recent\ninfections. We discuss the assumptions under which these data would yield an\nestimate of the counterfactual HIV incidence and develop sample size and power\ncalculations for comparisons to incidence observed on an investigational PrEP\nagent.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 04:16:29 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Gao", "Fei", ""], ["Glidden", "David V.", ""], ["Hughes", "James P.", ""], ["Donnell", "Deborah", ""]]}, {"id": "2011.00901", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Hadi Nekoei, Aydin Ghojogh, Fakhri Karray, Mark\n  Crowley", "title": "Sampling Algorithms, from Survey Sampling to Monte Carlo Methods:\n  Tutorial and Literature Review", "comments": "The first three authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.comp-ph physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a tutorial and literature review on sampling algorithms. We\nhave two main types of sampling in statistics. The first type is survey\nsampling which draws samples from a set or population. The second type is\nsampling from probability distribution where we have a probability density or\nmass function. In this paper, we cover both types of sampling. First, we review\nsome required background on mean squared error, variance, bias, maximum\nlikelihood estimation, Bernoulli, Binomial, and Hypergeometric distributions,\nthe Horvitz-Thompson estimator, and the Markov property. Then, we explain the\ntheory of simple random sampling, bootstrapping, stratified sampling, and\ncluster sampling. We also briefly introduce multistage sampling, network\nsampling, and snowball sampling. Afterwards, we switch to sampling from\ndistribution. We explain sampling from cumulative distribution function, Monte\nCarlo approximation, simple Monte Carlo methods, and Markov Chain Monte Carlo\n(MCMC) methods. For simple Monte Carlo methods, whose iterations are\nindependent, we cover importance sampling and rejection sampling. For MCMC\nmethods, we cover Metropolis algorithm, Metropolis-Hastings algorithm, Gibbs\nsampling, and slice sampling. Then, we explain the random walk behaviour of\nMonte Carlo methods and more efficient Monte Carlo methods, including\nHamiltonian (or hybrid) Monte Carlo, Adler's overrelaxation, and ordered\noverrelaxation. Finally, we summarize the characteristics, pros, and cons of\nsampling methods compared to each other. This paper can be useful for different\nfields of statistics, machine learning, reinforcement learning, and\ncomputational physics.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 11:27:23 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Nekoei", "Hadi", ""], ["Ghojogh", "Aydin", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2011.00947", "submitter": "Colin Griesbach", "authors": "Colin Griesbach and Benjamin S\\\"afken and Elisabeth Waldmann", "title": "Gradient Boosting for Linear Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting from the field of statistical learning is widely known as a\npowerful framework for estimation and selection of predictor effects in various\nregression models by adapting concepts from classification theory. Current\nboosting approaches also offer methods accounting for random effects and thus\nenable prediction of mixed models for longitudinal and clustered data. However,\nthese approaches include several flaws resulting in unbalanced effect selection\nwith falsely induced shrinkage and a low convergence rate on the one hand and\nbiased estimates of the random effects on the other hand. We therefore propose\na new boosting algorithm which explicitly accounts for the random structure by\nexcluding it from the selection procedure, properly correcting the random\neffects estimates and in addition providing likelihood-based estimation of the\nrandom effects variance structure. The new algorithm offers an organic and\nunbiased fitting approach, which is shown via simulations and data examples.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 13:04:55 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Griesbach", "Colin", ""], ["S\u00e4fken", "Benjamin", ""], ["Waldmann", "Elisabeth", ""]]}, {"id": "2011.00959", "submitter": "Xiaoyu Hu", "authors": "Xiaoyu Hu and Fang Yao", "title": "Sparse Functional Principal Component Analysis in High Dimensions", "comments": "27 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional principal component analysis (FPCA) is a fundamental tool and has\nattracted increasing attention in recent decades, while existing methods are\nrestricted to data with a single or finite number of random functions (much\nsmaller than the sample size $n$). In this work, we focus on high-dimensional\nfunctional processes where the number of random functions $p$ is comparable to,\nor even much larger than $n$. Such data are ubiquitous in various fields such\nas neuroimaging analysis, and cannot be properly modeled by existing methods.\nWe propose a new algorithm, called sparse FPCA, which is able to model\nprincipal eigenfunctions effectively under sensible sparsity regimes. While\nsparsity assumptions are standard in multivariate statistics, they have not\nbeen investigated in the complex context where not only is $p$ large, but also\neach variable itself is an intrinsically infinite-dimensional process. The\nsparsity structure motivates a thresholding rule that is easy to compute\nwithout nonparametric smoothing by exploiting the relationship between\nunivariate orthonormal basis expansions and multivariate Kahunen-Lo\\`eve (K-L)\nrepresentations. We investigate the theoretical properties of the resulting\nestimators, and illustrate the performance with simulated and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 13:25:38 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 12:14:55 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Hu", "Xiaoyu", ""], ["Yao", "Fang", ""]]}, {"id": "2011.01048", "submitter": "Edoardo Belli", "authors": "Edoardo Belli, Simone Vantini", "title": "Ridge regression with adaptive additive rectangles and other piecewise\n  functional templates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an $L_{2}$-based penalization algorithm for functional linear\nregression models, where the coefficient function is shrunk towards a\ndata-driven shape template $\\gamma$, which is constrained to belong to a class\nof piecewise functions by restricting its basis expansion. In particular, we\nfocus on the case where $\\gamma$ can be expressed as a sum of $q$ rectangles\nthat are adaptively positioned with respect to the regression error. As the\nproblem of finding the optimal knot placement of a piecewise function is\nnonconvex, the proposed parametrization allows to reduce the number of\nvariables in the global optimization scheme, resulting in a fitting algorithm\nthat alternates between approximating a suitable template and solving a convex\nridge-like problem. The predictive power and interpretability of our method is\nshown on multiple simulations and two real world case studies.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 15:28:54 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Belli", "Edoardo", ""], ["Vantini", "Simone", ""]]}, {"id": "2011.01058", "submitter": "Peng Chen", "authors": "Peng Chen and Keyi Wu and Omar Ghattas", "title": "Bayesian inference of heterogeneous epidemic models: Application to\n  COVID-19 spread accounting for long-term care facilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a high dimensional Bayesian inference framework for learning\nheterogeneous dynamics of a COVID-19 model, with a specific application to the\ndynamics and severity of COVID-19 inside and outside long-term care (LTC)\nfacilities. We develop a heterogeneous compartmental model that accounts for\nthe heterogeneity of the time-varying spread and severity of COVID-19 inside\nand outside LTC facilities, which is characterized by time-dependent stochastic\nprocesses and time-independent parameters in $\\sim$1500 dimensions after\ndiscretization. To infer these parameters, we use reported data on the number\nof confirmed, hospitalized, and deceased cases with suitable post-processing in\nboth a deterministic inversion approach with appropriate regularization as a\nfirst step, followed by Bayesian inversion with proper prior distributions. To\naddress the curse of dimensionality and the ill-posedness of the\nhigh-dimensional inference problem, we propose use of a dimension-independent\nprojected Stein variational gradient descent method, and demonstrate the\nintrinsic low-dimensionality of the inverse problem. We present inference\nresults with quantified uncertainties for both New Jersey and Texas, which\nexperienced different epidemic phases and patterns. Moreover, we also present\nforecasting and validation results based on the empirical posterior samples of\nour inference for the future trajectory of COVID-19.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 15:40:17 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Chen", "Peng", ""], ["Wu", "Keyi", ""], ["Ghattas", "Omar", ""]]}, {"id": "2011.01106", "submitter": "Haiyan Zheng", "authors": "Haiyan Zheng, Thomas Jaki, James M. S. Wason", "title": "Bayesian sample size determination using commensurate priors to leverage\n  pre-experimental data", "comments": "A submitted paper formatted with the double spacing, including 34\n  pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper develops Bayesian sample size formulae for experiments comparing\ntwo groups. We assume the experimental data will be analysed in the Bayesian\nframework, where pre-experimental information from multiple sources can be\nrepresented into robust priors. In particular, such robust priors account for\npreliminary belief about the pairwise commensurability between parameters that\nunderpin the historical and new experiments, to permit flexible borrowing of\ninformation. Averaged over the probability space of the new experimental data,\nappropriate sample sizes are found according to criteria that control certain\naspects of the posterior distribution, such as the coverage probability or\nlength of a defined density region. Our Bayesian methodology can be applied to\ncircumstances where the common variance in the new experiment is known or\nunknown. Exact solutions are available based on most of the criteria considered\nfor Bayesian sample size determination, while a search procedure is described\nin cases for which there are no closed-form expressions. We illustrate the\napplication of our Bayesian sample size formulae in the setting of designing a\nclinical trial. Hypothetical data examples, motivated by a rare-disease trial\nwith elicitation of expert prior opinion, and a comprehensive performance\nevaluation of the proposed methodology are presented.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:39:33 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zheng", "Haiyan", ""], ["Jaki", "Thomas", ""], ["Wason", "James M. S.", ""]]}, {"id": "2011.01107", "submitter": "Huijuan Zhou", "authors": "Huijuan Zhou, Xianyang Zhang and Jun Chen", "title": "Covariate Adaptive Family-wise Error Rate Control for Genome-Wide\n  Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The family-wise error rate (FWER) has been widely used in genome-wide\nassociation studies. With the increasing availability of functional genomics\ndata, it is possible to increase the detection power by leveraging these\ngenomic functional annotations. Previous efforts to accommodate covariates in\nmultiple testing focus on the false discovery rate control while\ncovariate-adaptive FWER-controlling procedures remain under-developed. Here we\npropose a novel covariate-adaptive FWER-controlling procedure that incorporates\nexternal covariates which are potentially informative of either the statistical\npower or the prior null probability. An efficient algorithm is developed to\nimplement the proposed method. We prove its asymptotic validity and obtain the\nrate of convergence through a perturbation-type argument. Our numerical studies\nshow that the new procedure is more powerful than competing methods and\nmaintains robustness across different settings. We apply the proposed approach\nto the UK Biobank data and analyze 27 traits with 9 million single-nucleotide\npolymorphisms tested for associations. Seventy-five genomic annotations are\nused as covariates. Our approach detects more genome-wide significant loci than\nother methods in 21 out of the 27 traits.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:40:45 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 20:52:04 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Zhou", "Huijuan", ""], ["Zhang", "Xianyang", ""], ["Chen", "Jun", ""]]}, {"id": "2011.01343", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani", "title": "p-value peeking and estimating extrema", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pervasive issue in statistical hypothesis testing is that the reported\n$p$-values are biased downward by data \"peeking\" -- the practice of reporting\nonly progressively extreme values of the test statistic as more data samples\nare collected. We develop principled mechanisms to estimate such running\nextrema of test statistics, which directly address the effect of peeking in\nsome general scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 22:00:57 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Balsubramani", "Akshay", ""]]}, {"id": "2011.01379", "submitter": "Angeliki Papana", "authors": "Angeliki Papana, Elsa Siggiridou, Dimitris Kugiumtzis", "title": "Detecting direct causality in multivariate time series: A comparative\n  study", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME nlin.CD stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of Granger causality is increasingly being applied for the\ncharacterization of directional interactions in different applications. A\nmultivariate framework for estimating Granger causality is essential in order\nto account for all the available information from multivariate time series.\nHowever, the inclusion of non-informative or non-significant variables creates\nestimation problems related to the 'curse of dimensionality'. To deal with this\nissue, direct causality measures using variable selection and dimension\nreduction techniques have been introduced. In this comparative work, the\nperformance of an ensemble of bivariate and multivariate causality measures in\nthe time domain is assessed, focusing on dimension reduction causality\nmeasures. In particular, different types of high-dimensional coupled discrete\nsystems are used (involving up to 100 variables) and the robustness of the\ncausality measures to time series length and different noise types is examined.\nThe results of the simulation study highlight the superiority of the dimension\nreduction measures, especially for high-dimensional systems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 23:21:57 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Papana", "Angeliki", ""], ["Siggiridou", "Elsa", ""], ["Kugiumtzis", "Dimitris", ""]]}, {"id": "2011.01388", "submitter": "Roland Matsouaka", "authors": "Roland A. Matsouaka, Yunji Zhou", "title": "A framework for causal inference in the presence of extreme inverse\n  probability weights: the role of overlap weights", "comments": "73 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider recent progress in estimating the average\ntreatment effect when extreme inverse probability weights are present and focus\non methods that account for a possible violation of the positivity assumption.\nThese methods aim at estimating the treatment effect on the subpopulation of\npatients for whom there is a clinical equipoise. We propose a systematic\napproach to determine their related causal estimands and develop new insights\ninto the properties of the weights targeting such a subpopulation. Then, we\nexamine the roles of overlap weights, matching weights, Shannon's entropy\nweights, and beta weights. This helps us characterize and compare their\nunderlying estimators, analytically and via simulations, in terms of the\naccuracy, precision, and root mean squared error. Moreover, we study the\nasymptotic behaviors of their augmented estimators (that mimic doubly robust\nestimators), which lead to improved estimations when either the propensity or\nthe regression models are correctly specified. Based on the analytical and\nsimulation results, we conclude that overall overlap weights are preferable to\nmatching weights, especially when there is moderate or extreme violations of\nthe positivity assumption. Finally, we illustrate the methods using a real data\nexample marked by extreme inverse probability weights.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 00:01:03 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Matsouaka", "Roland A.", ""], ["Zhou", "Yunji", ""]]}, {"id": "2011.01493", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Daisuke Murakami", "title": "Spatially Clustered Regression", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial regression or geographically weighted regression models have been\nwidely adopted to capture the effects of auxiliary information on a response\nvariable of interest over a region. In contrast, relationships between response\nand auxiliary variables are expected to exhibit complex spatial patterns in\nmany applications. This paper proposes a new approach for spatial regression,\ncalled spatially clustered regression, to estimate possibly clustered spatial\npatterns of the relationships. We combine K-means-based clustering formulation\nand penalty function motivated from a spatial process known as Potts model for\nencouraging similar clustering in neighboring locations. We provide a simple\niterative algorithm to fit the proposed method, scalable for large spatial\ndatasets. Through simulation studies, the proposed method demonstrates its\nsuperior performance to existing methods even under the true structure does not\nadmit spatial clustering. Finally, the proposed method is applied to crime\nevent data in Tokyo and produces interpretable results for spatial patterns.\nThe R code is available at https://github.com/sshonosuke/SCR.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 06:00:02 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 06:52:26 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Murakami", "Daisuke", ""]]}, {"id": "2011.01567", "submitter": "Sida Chen", "authors": "Sida Chen and B\\\"arbel Finkenst\\\"adt Rand", "title": "Bayesian inference for spline-based hidden Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  B-spline-based hidden Markov models (HMMs), where the emission densities are\nspecified as mixtures of normalized B-spline basis functions, offer a more\nflexible modelling approach to data than conventional parametric HMMs. We\nintroduce a fully Bayesian framework for inference in these nonparametric\nmodels where the number of states may be unknown along with other model\nparameters. We propose the use of a trans-dimensional Markov chain inference\nalgorithm to identify a parsimonious knot configuration of the B-splines while\nmodel selection regarding the number of states can be performed within a\nparallel sampling framework. The feasibility and efficiency of our proposed\nmethodology is shown in a simulation study. Its explorative use for real data\nis demonstrated for activity acceleration data in animals, i.e.\nwhitetip-sharks. The flexibility of a Bayesian approach allows us to extend the\nmodelling framework in a straightforward way and we demonstrate this by\ndeveloping a hierarchical conditional HMM to analyse human accelerator activity\ndata to focus on studying small movements and/or inactivity during sleep.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 08:47:27 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Chen", "Sida", ""], ["Rand", "B\u00e4rbel Finkenst\u00e4dt", ""]]}, {"id": "2011.01650", "submitter": "Elena Tuzhilina", "authors": "Elena Tuzhilina, Leonardo Tozzi, Trevor Hastie", "title": "Canonical Correlation Analysis in high dimensions with structured\n  regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Canonical correlation analysis (CCA) is a technique for measuring the\nassociation between two multivariate data matrices. A regularized modification\nof canonical correlation analysis (RCCA) which imposes an $\\ell_2$ penalty on\nthe CCA coefficients is widely used in applications with high-dimensional data.\nOne limitation of such regularization is that it ignores any data structure,\ntreating all the features equally, which can be ill-suited for some\napplications. In this paper we introduce several approaches to regularizing CCA\nthat take the underlying data structure into account. In particular, the\nproposed group regularized canonical correlation analysis (GRCCA) is useful\nwhen the variables are correlated in groups. We illustrate some computational\nstrategies to avoid excessive computations with regularized CCA in high\ndimensions. We demonstrate the application of these methods in our motivating\napplication from neuroscience, as well as in a small simulation example.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 12:01:15 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 03:19:09 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 07:40:59 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Tuzhilina", "Elena", ""], ["Tozzi", "Leonardo", ""], ["Hastie", "Trevor", ""]]}, {"id": "2011.01661", "submitter": "Subhadip Maji", "authors": "Indranil Basu and Subhadip Maji", "title": "Multicollinearity Correction and Combined Feature Effect in Shapley\n  Values", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model interpretability is one of the most intriguing problems in most of the\nMachine Learning models, particularly for those that are mathematically\nsophisticated. Computing Shapley Values are arguably the best approach so far\nto find the importance of each feature in a model, at the row level. In other\nwords, Shapley values represent the importance of a feature for a particular\nrow, especially for Classification or Regression problems. One of the biggest\nlimitations of Shapley vales is that, Shapley value calculations assume all the\nfeatures are uncorrelated (independent of each other), this assumption is often\nincorrect. To address this problem, we present a unified framework to calculate\nShapley values with correlated features. To be more specific, we do an\nadjustment (Matrix formulation) of the features while calculating Independent\nShapley values for the rows. Moreover, we have given a Mathematical proof\nagainst the said adjustments. With these adjustments, Shapley values\n(Importance) for the features become independent of the correlations existing\nbetween them. We have also enhanced this adjustment concept for more than\nfeatures. As the Shapley values are additive, to calculate combined effect of\ntwo features, we just have to add their individual Shapley values. This is\nagain not right if one or more of the features (used in the combination) are\ncorrelated with the other features (not in the combination). We have addressed\nthis problem also by extending the correlation adjustment for one feature to\nmultiple features in the said combination for which Shapley values are\ndetermined. Our implementation of this method proves that our method is\ncomputationally efficient also, compared to original Shapley method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 12:28:42 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Basu", "Indranil", ""], ["Maji", "Subhadip", ""]]}, {"id": "2011.01698", "submitter": "Saralees Nadarajah", "authors": "H. Kwong, S. Nadarajah", "title": "A new robust class of skew elliptical distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new robust class of multivariate skew distributions is introduced.\nPractical aspects such as parameter estimation method of the proposed class are\ndiscussed, we show that the proposed class can be fitted under a reasonable\ntime frame. Our study shows that the class of distributions is capable to model\nmultivariate skewness structure and does not suffer from the curse of\ndimensionality as heavily as other distributions of similar complexity do, such\nas the class of canonical skew distributions. We also derive a nested form of\nthe proposed class which appears to be the most flexible class of multivariate\nskew distributions in literature that has a closed-form density function.\nNumerical examples on two data sets, i) a data set containing daily river flow\ndata recorded in the UK; and ii) a data set containing biomedical variables of\nathletes collected by the Australian Institute of Sports (AIS), are\ndemonstrated. These examples further support the practicality of the proposed\nclass on moderate dimensional data sets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 13:36:05 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 15:59:45 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kwong", "H.", ""], ["Nadarajah", "S.", ""]]}, {"id": "2011.01704", "submitter": "Fabian Guignard", "authors": "Fabian Guignard, Federico Amato and Mikhail Kanevski", "title": "Uncertainty Quantification in Extreme Learning Machine: Analytical\n  Developments, Variance Estimates and Confidence Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification is crucial to assess prediction quality of a\nmachine learning model. In the case of Extreme Learning Machines (ELM), most\nmethods proposed in the literature make strong assumptions on the data, ignore\nthe randomness of input weights or neglect the bias contribution in confidence\ninterval estimations. This paper presents novel estimations that overcome these\nconstraints and improve the understanding of ELM variability. Analytical\nderivations are provided under general assumptions, supporting the\nidentification and the interpretation of the contribution of different\nvariability sources. Under both homoskedasticity and heteroskedasticity,\nseveral variance estimates are proposed, investigated, and numerically tested,\nshowing their effectiveness in replicating the expected variance behaviours.\nFinally, the feasibility of confidence intervals estimation is discussed by\nadopting a critical approach, hence raising the awareness of ELM users\nconcerning some of their pitfalls. The paper is accompanied with a scikit-learn\ncompatible Python library enabling efficient computation of all estimates\ndiscussed herein.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 13:45:59 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Guignard", "Fabian", ""], ["Amato", "Federico", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "2011.01725", "submitter": "Vincent Valton PhD", "authors": "Vincent Valton, Toby Wise, Oliver J. Robinson", "title": "Recommendations for Bayesian hierarchical model specifications for\n  case-control studies in mental health", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2020 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical model fitting has become commonplace for case-control studies of\ncognition and behaviour in mental health. However, these techniques require us\nto formalise assumptions about the data-generating process at the group level,\nwhich may not be known. Specifically, researchers typically must choose whether\nto assume all subjects are drawn from a common population, or to model them as\nderiving from separate populations. These assumptions have profound\nimplications for computational psychiatry, as they affect the resulting\ninference (latent parameter recovery) and may conflate or mask true group-level\ndifferences. To test these assumptions we ran systematic simulations on\nsynthetic multi-group behavioural data from a commonly used multi-armed bandit\ntask (reinforcement learning task). We then examined recovery of group\ndifferences in latent parameter space under the two commonly used generative\nmodelling assumptions: (1) modelling groups under a common shared group-level\nprior (assuming all participants are generated from a common distribution, and\nare likely to share common characteristics); (2) modelling separate groups\nbased on symptomatology or diagnostic labels, resulting in separate group-level\npriors. We evaluated the robustness of these approaches to variations in data\nquality and prior specifications on a variety of metrics. We found that fitting\ngroups separately (assumptions 2), provided the most accurate and robust\ninference across all conditions. Our results suggest that when dealing with\ndata from multiple clinical groups, researchers should analyse patient and\ncontrol groups separately as it provides the most accurate and robust recovery\nof the parameters of interest.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 14:19:59 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Valton", "Vincent", ""], ["Wise", "Toby", ""], ["Robinson", "Oliver J.", ""]]}, {"id": "2011.01808", "submitter": "Daniel Simpson", "authors": "Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob\n  Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian B\\\"urkner,\n  Martin Modr\\'ak", "title": "Bayesian Workflow", "comments": "77 pages, 35 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Bayesian approach to data analysis provides a powerful way to handle\nuncertainty in all observations, model parameters, and model structure using\nprobability theory. Probabilistic programming languages make it easier to\nspecify and fit Bayesian models, but this still leaves us with many options\nregarding constructing, evaluating, and using these models, along with many\nremaining challenges in computation. Using Bayesian inference to solve\nreal-world problems requires not only statistical skills, subject matter\nknowledge, and programming, but also awareness of the decisions made in the\nprocess of data analysis. All of these aspects can be understood as part of a\ntangled workflow of applied Bayesian statistics. Beyond inference, the workflow\nalso includes iterative model building, model checking, validation and\ntroubleshooting of computational problems, model understanding, and model\ncomparison. We review all these aspects of workflow in the context of several\nexamples, keeping in mind that in practice we will be fitting many models for\nany given problem, even if only a subset of them will ultimately be relevant\nfor our conclusions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 15:59:50 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Gelman", "Andrew", ""], ["Vehtari", "Aki", ""], ["Simpson", "Daniel", ""], ["Margossian", "Charles C.", ""], ["Carpenter", "Bob", ""], ["Yao", "Yuling", ""], ["Kennedy", "Lauren", ""], ["Gabry", "Jonah", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Modr\u00e1k", "Martin", ""]]}, {"id": "2011.01831", "submitter": "Israel Mart\\'inez Hern\\'andez", "authors": "Israel Mart\\'inez-Hern\\'andez, Jes\\'us Gonzalo, Graciela\n  Gonz\\'alez-Far\\'ias", "title": "Nonparametric Estimation of Functional Dynamic Factor Model", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many phenomena, data are collected on a large scale, resulting in\nhigh-dimensional and high-frequency data. In this context, functional data\nanalysis (FDA) is attracting interest. FDA deals with data that are defined on\nan intrinsically infinite-dimensional space. These data are called functional\ndata. However, the infinite-dimensional data might be driven by a small number\nof latent variables. Hence, factor models are relevant for functional data. In\nthis paper, we study functional factor models for time-dependent functional\ndata. We propose nonparametric estimators under stationary and nonstationary\nprocesses. We obtain estimators that consider the time-dependence property.\nSpecifically, we use the information contained on the covariances at different\nlags. We show that the proposed estimators are consistent. Through Monte Carlo\nsimulations, we find that our methodology outperforms the common estimators\nbased on functional principal components. We also apply our methodology to\nmonthly yield curves. In general, the suitable integration of time-dependent\ninformation improves the estimation of the latent factors.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 16:44:19 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Mart\u00ednez-Hern\u00e1ndez", "Israel", ""], ["Gonzalo", "Jes\u00fas", ""], ["Gonz\u00e1lez-Far\u00edas", "Graciela", ""]]}, {"id": "2011.01857", "submitter": "Yi Yu", "authors": "Yi Yu", "title": "A review on minimax rates in change point detection and localisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews recent developments in fundamental limits and optimal\nalgorithms for change point analysis. We focus on minimax optimal rates in\nchange point detection and localisation, in both parametric and nonparametric\nmodels. We start with the univariate mean change point analysis problem and\nreview the state-of-the-art results in the literature. We then move on to more\ncomplex data types and investigate general principles behind the optimal\nprocedures that lead to minimax rate-optimal results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:19:45 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Yu", "Yi", ""]]}, {"id": "2011.01979", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald, Dmitriy Katz-Rogozhnikov, Karthik Shanmugam", "title": "High-Dimensional Feature Selection for Sample Efficient Treatment Effect\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of causal treatment effects from observational data is a\nfundamental problem in causal inference. To avoid bias, the effect estimator\nmust control for all confounders. Hence practitioners often collect data for as\nmany covariates as possible to raise the chances of including the relevant\nconfounders. While this addresses the bias, this has the side effect of\nsignificantly increasing the number of data samples required to accurately\nestimate the effect due to the increased dimensionality. In this work, we\nconsider the setting where out of a large number of covariates $X$ that satisfy\nstrong ignorability, an unknown sparse subset $S$ is sufficient to include to\nachieve zero bias, i.e. $c$-equivalent to $X$. We propose a common objective\nfunction involving outcomes across treatment cohorts with nonconvex joint\nsparsity regularization that is guaranteed to recover $S$ with high probability\nunder a linear outcome model for $Y$ and subgaussian covariates for each of the\ntreatment cohort. This improves the effect estimation sample complexity so that\nit scales with the cardinality of the sparse subset $S$ and $\\log |X|$, as\nopposed to the cardinality of the full set $X$. We validate our approach with\nexperiments on treatment effect estimation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 19:54:16 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Katz-Rogozhnikov", "Dmitriy", ""], ["Shanmugam", "Karthik", ""]]}, {"id": "2011.02304", "submitter": "Pengcheng Zeng", "authors": "Lin Tang, Pengcheng Zeng, Jian Qing Shi, Won-Seok Kim", "title": "Joint Curve Registration and Classification with Two-level Functional\n  Models", "comments": "27 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many classification techniques when the data are curves or functions have\nbeen recently proposed. However, the presence of misaligned problems in the\ncurves can influence the performance of most of them. In this paper, we propose\na model-based approach for simultaneous curve registration and classification.\nThe method is proposed to perform curve classification based on a functional\nlogistic regression model that relies on both scalar variables and functional\nvariables, and to align curves simultaneously via a data registration model.\nEM-based algorithms are developed to perform maximum likelihood inference of\nthe proposed models. We establish the identifiability results for curve\nregistration model and investigate the asymptotic properties of the proposed\nestimation procedures. Simulation studies are conducted to demonstrate the\nfinite sample performance of the proposed models. An application of the hyoid\nbone movement data from stroke patients reveals the effectiveness of the new\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 14:05:35 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Tang", "Lin", ""], ["Zeng", "Pengcheng", ""], ["Shi", "Jian Qing", ""], ["Kim", "Won-Seok", ""]]}, {"id": "2011.02688", "submitter": "Gerhard Tutz", "authors": "Gerhard Tutz, Ingrid Mauerer", "title": "Heterogeneity in General Multinomial Choice Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different voters behave differently, different governments make different\ndecisions, or different organizations are ruled differently. Many research\nquestions important to political scientists concern choice behavior, which\ninvolves dealing with nominal-scale dependent variables. Drawing on the\nprinciple of maximum random utility, we propose a flexible and general\nheterogeneous multinomial logit model for studying differences in choice\nbehavior. The model systematically accounts for heterogeneity that is not\ncaptured by classical models, indicates the strength of heterogeneity, and\npermits examining which explanatory variables cause heterogeneity. As the\nproposed approach allows incorporating theoretical expectations about\nheterogeneity into the analysis of nominal dependent variables, it can be\napplied to a wide range of research problems. Our empirical example uses data\non multiparty elections to demonstrate the benefits of the model in the study\nof heterogeneity in spatial voting.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 07:19:17 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Tutz", "Gerhard", ""], ["Mauerer", "Ingrid", ""]]}, {"id": "2011.02784", "submitter": "Euloge Clovis Kenne Pagui", "authors": "Euloge Clovis Kenne Pagui and Alessandra Salvan and Nicola Sartori", "title": "Accurate inference in negative binomial regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negative binomial regression is commonly employed to analyze overdispersed\ncount data. With small to moderate sample sizes, the maximum likelihood\nestimator of the dispersion parameter may be subject to a significant bias,\nthat in turn affects inference on mean parameters. This paper proposes\ninference for negative binomial regression based on adjustments of the score\nfunction aimed at mean and median bias reduction. The resulting estimating\nequations are similar to those available for improved inference in generalized\nlinear models and, in particular, can be solved using a suitable extension of\niterative weighted least squares. Simulation studies show a remarkable\nperformance of the new methods, which are also found to solve in many cases\nnumerical problems of maximum likelihood estimates. The methods are illustrated\nand evaluated using two case studies: an Ames salmonella assay data set and\ndata on epileptic seizures. Inference based on adjusted scores turns out to be\ngenerally preferable to explicit bias correction.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 12:31:47 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Pagui", "Euloge Clovis Kenne", ""], ["Salvan", "Alessandra", ""], ["Sartori", "Nicola", ""]]}, {"id": "2011.02832", "submitter": "Stella Biderman", "authors": "Stella Biderman and Walter J. Scheirer", "title": "Pitfalls in Machine Learning Research: Reexamining the Development Cycle", "comments": "NeurIPS \"I Can't Believe It's Not Better!\" Workshop", "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has the potential to fuel further advances in data science,\nbut it is greatly hindered by an ad hoc design process, poor data hygiene, and\na lack of statistical rigor in model evaluation. Recently, these issues have\nbegun to attract more attention as they have caused public and embarrassing\nissues in research and development. Drawing from our experience as machine\nlearning researchers, we follow the machine learning process from algorithm\ndesign to data collection to model evaluation, drawing attention to common\npitfalls and providing practical recommendations for improvements. At each\nstep, case studies are introduced to highlight how these pitfalls occur in\npractice, and where things could be improved.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 17:58:18 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Biderman", "Stella", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "2011.03065", "submitter": "Qinglong Tian", "authors": "Qinglong Tian, Daniel J. Nordman, William Q. Meeker", "title": "Methods to Compute Prediction Intervals: A Review and New Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews two main types of prediction interval methods under a\nparametric framework. First, we describe methods based on an (approximate)\npivotal quantity. Examples include the plug-in, pivotal, and calibration\nmethods. Then we describe methods based on a predictive distribution (sometimes\nderived based on the likelihood). Examples include Bayesian, fiducial, and\ndirect-bootstrap methods. Several examples involving continuous distributions\nalong with simulation studies to evaluate coverage probability properties are\nprovided. We provide specific connections among different prediction interval\nmethods for the (log-)location-scale family of distributions. This paper also\ndiscusses general prediction interval methods for discrete data, using the\nbinomial and Poisson distributions as examples. We also overview methods for\ndependent data, with application to time series, spatial data, and Markov\nrandom fields, for example.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 19:11:05 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Tian", "Qinglong", ""], ["Nordman", "Daniel J.", ""], ["Meeker", "William Q.", ""]]}, {"id": "2011.03121", "submitter": "Naoki Awaya", "authors": "Naoki Awaya, Li Ma", "title": "Hidden Markov P\\'olya trees for high-dimensional distributions", "comments": "50 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The P\\'olya tree (PT) process is a general-purpose Bayesian nonparametric\nmodel that has found wide application in a range of inference problems. The PT\nhas a simple analytic form and the resulting posterior computation boils down\nto straight-forward beta-binomial conjugate updates along a partition tree over\nthe sample space. Recent development in PT models shows that performance of\nthese models can be substantially improved by (i) incorporating latent state\nvariables that characterize local features of the underlying distributions and\n(ii) allowing the partition tree to adapt to the structure of the underlying\ndistribution. Despite these advances, however, some important limitations of\nthe PT that remain include---(i) the sensitivity in the posterior inference\nwith respect to the choice of the partition points, and (ii) the lack of\ncomputational scalability to multivariate problems beyond a small number\n($<10$) of dimensions. We consider a modeling strategy for PT models that\nincorporates a very flexible prior on the partition tree along with latent\nstates that can be first-order dependent (i.e., following a Markov process),\nand introduce a hybrid algorithm that combines sequential Monte Carlo (SMC) and\nrecursive message passing for posterior inference that can readily accommodate\nPT models with or without latent states as well as flexible partition points in\nproblems up to 100 dimensions. Moreover, we investigate the large sample\nproperties of the tree structures and latent states under the posterior model.\nWe carry out extensive numerical experiments in the context of density\nestimation and two-sample testing, which show that flexible partitioning can\nsubstantially improve the performance of PT models in both inference tasks. We\ndemonstrate an application to a flow cytometry data set with 19 dimensions and\nover 200,000 observations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 22:14:03 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 04:23:23 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Awaya", "Naoki", ""], ["Ma", "Li", ""]]}, {"id": "2011.03127", "submitter": "Chandler Squires", "authors": "Chandler Squires, Dennis Shen, Anish Agarwal, Devavrat Shah, Caroline\n  Uhler", "title": "Causal Imputation via Synthetic Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of determining the effect of a compound on a specific\ncell type. To answer this question, researchers traditionally need to run an\nexperiment applying the drug of interest to that cell type. This approach is\nnot scalable: given a large number of different actions (compounds) and a large\nnumber of different contexts (cell types), it is infeasible to run an\nexperiment for every action-context pair. In such cases, one would ideally like\nto predict the outcome for every pair while only having to perform experiments\non a small subset of pairs. This task, which we label \"causal imputation\", is a\ngeneralization of the causal transportability problem. To address this\nchallenge, we extend the recently introduced synthetic interventions (SI)\nestimator to handle more general data sparsity patterns. We prove that, under a\nlatent factor model, our estimator provides valid estimates for the causal\nimputation task. We motivate this model by establishing a connection to the\nlinear structural causal model literature. Finally, we consider the prominent\nCMAP dataset in predicting the effects of compounds on gene expression across\ncell types. We find that our estimator outperforms standard baselines, thus\nconfirming its utility in biological applications.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 22:39:13 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 20:54:28 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Squires", "Chandler", ""], ["Shen", "Dennis", ""], ["Agarwal", "Anish", ""], ["Shah", "Devavrat", ""], ["Uhler", "Caroline", ""]]}, {"id": "2011.03140", "submitter": "Colin Lewis-Beck", "authors": "Colin Lewis-Beck, Qinglong Tian, William Q. Meeker", "title": "Prediction of Future Failures for Heterogeneous Reliability Field Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces methods for constructing prediction bounds or\nintervals for the number of future failures from heterogeneous reliability\nfield data. We focus on within-sample prediction where early data from a\nfailure-time process is used to predict future failures from the same process.\nEarly data from high-reliability products, however, often have limited\ninformation due to some combination of small sample sizes, censoring, and\ntruncation. In such cases, we use a Bayesian hierarchical model to model\njointly multiple lifetime distributions arising from different subpopulations\nof similar products. By borrowing information across subpopulations, our method\nenables stable estimation and the computation of corresponding prediction\nintervals, even in cases where there are few observed failures. Three\napplications are provided to illustrate this methodology, and a simulation\nstudy is used to validate the coverage performance of the prediction intervals.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 23:40:46 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 17:34:59 GMT"}, {"version": "v3", "created": "Sun, 11 Apr 2021 15:16:21 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lewis-Beck", "Colin", ""], ["Tian", "Qinglong", ""], ["Meeker", "William Q.", ""]]}, {"id": "2011.03219", "submitter": "Martin Bladt", "authors": "Martin Bladt and Jorge Yslas", "title": "Inhomogeneous Markov Survival Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new regression models in survival analysis based on homogeneous\nand inhomogeneous phase-type distributions. The intensity function in this\nsetting plays the role of the hazard function. For unidimensional intensity\nmatrices, we recover the proportional hazard and accelerated failure time\nmodels, among others. However, when considering higher dimensions, the proposed\nmethods are only asymptotically equivalent to their classical counterparts and\nenjoy greater flexibility in the body of the distribution. For their\nestimation, the latent path representation of semi-Markov models is exploited.\nConsequently, an adapted EM algorithm is provided and the likelihood is shown\nto increase at each iteration. We provide several examples of practical\nsignificance and outline relevant extensions. The practical feasibility of the\nmodels is illustrated on simulated and real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 07:56:39 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 18:28:51 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Bladt", "Martin", ""], ["Yslas", "Jorge", ""]]}, {"id": "2011.03270", "submitter": "Helen Barnett", "authors": "Helen Yvette Barnett, Sofia S Villar, Helena Geys and Thomas Jaki", "title": "A Novel Statistical Test for Treatment Differences in Clinical Trials\n  using a Response Adaptive Forward Looking Gittins Index Rule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most common objective for response adaptive clinical trials is to seek to\nensure that patients within a trial have a high chance of receiving the best\ntreatment available by altering the chance of allocation on the basis of\naccumulating data. Approaches which yield good patient benefit properties\nsuffer from low power from a frequentist perspective when testing for a\ntreatment difference at the end of the study due to the high imbalance in\ntreatment allocations. In this work we develop an alternative pairwise test for\ntreatment difference on the basis of allocation probabilities of the\ncovariate-adjusted response-adaptive randomization with forward looking Gittins\nindex rule (CARA-FLGI). The performance of the novel test is evaluated in\nsimulations for two-armed studies and then its applications to multi-armed\nstudies is illustrated. The proposed test has markedly improved power over the\ntraditional Fisher exact test when this class of non-myopic response adaptation\nis used. We also find that the test's power is close to the power of a Fisher\nexact test under equal randomization.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 10:29:45 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Barnett", "Helen Yvette", ""], ["Villar", "Sofia S", ""], ["Geys", "Helena", ""], ["Jaki", "Thomas", ""]]}, {"id": "2011.03282", "submitter": "Fran\\c{c}ois Bachoc", "authors": "A. Fradi and Y. Feunteun and C. Samir and M. Baklouti and F. Bachoc\n  and J-M. Loubes", "title": "Bayesian Regression and Classification Using Gaussian Process Priors\n  Indexed by Probability Density Functions", "comments": null, "journal-ref": "Information Sciences Volume 548, 16 February 2021, Pages 56-68", "doi": "10.1016/j.ins.2020.09.027", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the notion of Gaussian processes indexed by\nprobability density functions for extending the Mat\\'ern family of covariance\nfunctions. We use some tools from information geometry to improve the\nefficiency and the computational aspects of the Bayesian learning model. We\nparticularly show how a Bayesian inference with a Gaussian process prior\n(covariance parameters estimation and prediction) can be put into action on the\nspace of probability density functions. Our framework has the capacity of\nclassifiying and infering on data observations that lie on nonlinear subspaces.\nExtensive experiments on multiple synthetic, semi-synthetic and real data\ndemonstrate the effectiveness and the efficiency of the proposed methods in\ncomparison with current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 11:02:05 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Fradi", "A.", ""], ["Feunteun", "Y.", ""], ["Samir", "C.", ""], ["Baklouti", "M.", ""], ["Bachoc", "F.", ""], ["Loubes", "J-M.", ""]]}, {"id": "2011.03482", "submitter": "Michael Genin", "authors": "Camille Fr\\'event and Mohamed-Salem Ahmed and Matthieu Marbac and\n  Micha\\\"el Genin", "title": "Detecting spatial clusters in functional data: new scan statistic\n  approaches", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed two scan statistics for detecting clusters of functional\ndata indexed in space. The first method is based on an adaptation of a\nfunctional analysis of variance and the second one is based on a\ndistribution-free spatial scan statistic for univariate data. In a simulation\nstudy, the distribution-free method always performed better than a\nnonparametric functional scan statistic, and the adaptation of the anova also\nperformed better for data with a normal or a quasi-normal distribution. Our\nmethods can detect smaller spatial clusters than the nonparametric method.\nLastly, we used our scan statistics for functional data to search for spatial\nclusters of abnormal unemployment rates in France over the period 1998-2013\n(divided into quarters).\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 17:22:32 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 20:43:40 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Fr\u00e9vent", "Camille", ""], ["Ahmed", "Mohamed-Salem", ""], ["Marbac", "Matthieu", ""], ["Genin", "Micha\u00ebl", ""]]}, {"id": "2011.03515", "submitter": "Paul Parker", "authors": "Paul A. Parker and Scott H. Holan", "title": "A Bayesian Functional Data Model for Surveys Collected under Informative\n  Sampling with Application to Mortality Estimation using NHANES", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data are often extremely high-dimensional and exhibit strong\ndependence structures but can often prove valuable for both prediction and\ninference. The literature on functional data analysis is well developed;\nhowever, there has been very little work involving functional data in complex\nsurvey settings. Motivated by physical activity monitor data from the National\nHealth and Nutrition Examination Survey (NHANES), we develop a Bayesian model\nfor functional covariates that can properly account for the survey design. Our\napproach is intended for non-Gaussian data and can be applied in multivariate\nsettings. In addition, we make use of a variety of Bayesian modeling techniques\nto ensure that the model is fit in a computationally efficient manner. We\nillustrate the value of our approach through an empirical simulation study as\nwell as an example of mortality estimation using NHANES data.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 18:46:17 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Parker", "Paul A.", ""], ["Holan", "Scott H.", ""]]}, {"id": "2011.03567", "submitter": "Michael Lindon", "authors": "Michael Lindon, Alan Malek", "title": "Sequential Testing of Multinomial Hypotheses with Applications to\n  Detecting Implementation Errors and Missing Data in Randomized Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simply randomized designs are one of the most common controlled experiments\nused to study causal effects. Failure of the assignment mechanism, to provide\nproper randomization of units across treatments, or the data collection\nmechanism, when data is missing not at random, can render subsequent analysis\ninvalid if not properly identified. In this paper we demonstrate that such\npractical implementation errors can often be identified, fortunately, through\nconsideration of the total unit counts resulting in each treatment group. Based\non this observation, we introduce a sequential hypothesis test constructed from\nBayesian multinomial-Dirichlet families for detecting practical implementation\nerrors in simply randomized experiments. By establishing a Martingale property\nof the posterior odds under the null hypothesis, frequentist Type-I error is\ncontrolled under both optional stopping and continuation via maximal\ninequalities, preventing practitioners from potentially inflating false\npositive probabilities through continuous monitoring. In contrast to other\nstatistical tests that are performed once all data collection is completed, the\nproposed test is sequential - frequently rejecting the null during the process\nof data collection itself, saving further units from entering an\nimproperly-executed experiment. We illustrate the utility of this test in the\ncontext of online controlled experiments (OCEs), where the assignment is\nautomated through code and data collected through complex processing pipelines,\noften in the presence of unintended bugs and logical errors. Confidence\nsequences possessing desired sequential frequentist coverage probabilities are\nprovided and their connection to the Bayesian support interval is examined. The\ndifferences between pure Bayesian and sequential frequentist testing procedures\nare finally discussed through a conditional frequentist testing perspective.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 19:17:38 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lindon", "Michael", ""], ["Malek", "Alan", ""]]}, {"id": "2011.03593", "submitter": "Ting Ye", "authors": "Ting Ye, Ashkan Ertefaie, James Flory, Sean Hennessy, Dylan S. Small", "title": "Controlling for Unmeasured Confounding in the Presence of Time:\n  Instrumental Variable for Trend", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmeasured confounding is a key threat to reliable causal inference based on\nobservational studies. We propose a new method called instrumental variable for\ntrend that explicitly leverages exogenous randomness in the exposure trend to\nestimate the average and conditional average treatment effect in the presence\nof unmeasured confounding. Specifically, we use an instrumental variable for\ntrend, a variable that (i) is associated with trend in exposure; (ii) is\nindependent of the potential exposures, potential trends in outcome and\nindividual treatment effect; and (iii) has no direct effect on the trend in\noutcome and does not modify the individual treatment effect. We develop the\nidentification assumptions using the potential outcomes framework and we\npropose two measures of weak identification. In addition, we present a Wald\nestimator and a class of multiply robust and efficient semiparametric\nestimators, with provable consistency and asymptotic normality. Furthermore, we\npropose a two-sample summary-data Wald estimator to facilitate investigations\nof delayed treatment effect. We demonstrate our results in simulated and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 20:49:40 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 16:27:28 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Ye", "Ting", ""], ["Ertefaie", "Ashkan", ""], ["Flory", "James", ""], ["Hennessy", "Sean", ""], ["Small", "Dylan S.", ""]]}, {"id": "2011.03598", "submitter": "Linjun Zhang", "authors": "Linjun Zhang, Rong Ma, T. Tony Cai and Hongzhe Li", "title": "Estimation, Confidence Intervals, and Large-Scale Hypotheses Testing for\n  High-Dimensional Mixed Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the high-dimensional mixed linear regression (MLR) where\nthe output variable comes from one of the two linear regression models with an\nunknown mixing proportion and an unknown covariance structure of the random\ncovariates. Building upon a high-dimensional EM algorithm, we propose an\niterative procedure for estimating the two regression vectors and establish\ntheir rates of convergence. Based on the iterative estimators, we further\nconstruct debiased estimators and establish their asymptotic normality. For\nindividual coordinates, confidence intervals centered at the debiased\nestimators are constructed.\n  Furthermore, a large-scale multiple testing procedure is proposed for testing\nthe regression coefficients and is shown to control the false discovery rate\n(FDR) asymptotically. Simulation studies are carried out to examine the\nnumerical performance of the proposed methods and their superiority over\nexisting methods. The proposed methods are further illustrated through an\nanalysis of a dataset of multiplex image cytometry, which investigates the\ninteraction networks among the cellular phenotypes that include the expression\nlevels of 20 epitopes or combinations of markers.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 21:17:41 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Zhang", "Linjun", ""], ["Ma", "Rong", ""], ["Cai", "T. Tony", ""], ["Li", "Hongzhe", ""]]}, {"id": "2011.03599", "submitter": "Samuel Tickle PhD", "authors": "S. O. Tickle and I. A. Eckley and P. Fearnhead", "title": "A computationally efficient, high-dimensional multiple changepoint\n  procedure with application to global terrorism incidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting changepoints in datasets with many variates is a data science\nchallenge of increasing importance. Motivated by the problem of detecting\nchanges in the incidence of terrorism from a global terrorism database, we\npropose a novel approach to multiple changepoint detection in multivariate time\nseries. Our method, which we call SUBSET, is a model-based approach which uses\na penalised likelihood to detect changes for a wide class of parametric\nsettings. We provide theory that guides the choice of penalties to use for\nSUBSET, and that shows it has high power to detect changes regardless of\nwhether only a few variates or many variates change. Empirical results show\nthat SUBSET out-performs many existing approaches for detecting changes in mean\nin Gaussian data; additionally, unlike these alternative methods, it can be\neasily extended to non-Gaussian settings such as are appropriate for modelling\ncounts of terrorist events.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 21:21:33 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 18:36:02 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tickle", "S. O.", ""], ["Eckley", "I. A.", ""], ["Fearnhead", "P.", ""]]}, {"id": "2011.03610", "submitter": "Chandler Squires", "authors": "Chandler Squires, Joshua Amaniampong, Caroline Uhler", "title": "Efficient Permutation Discovery in Causal DAGs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning a directed acyclic graph (DAG) up to Markov\nequivalence is equivalent to the problem of finding a permutation of the\nvariables that induces the sparsest graph. Without additional assumptions, this\ntask is known to be NP-hard. Building on the minimum degree algorithm for\nsparse Cholesky decomposition, but utilizing DAG-specific problem structure, we\nintroduce an efficient algorithm for finding such sparse permutations. We show\nthat on jointly Gaussian distributions, our method with depth $w$ runs in\n$O(p^{w+3})$ time. We compare our method with $w = 1$ to algorithms for finding\nsparse elimination orderings of undirected graphs, and show that taking\nadvantage of DAG-specific problem structure leads to a significant improvement\nin the discovered permutation. We also compare our algorithm to provably\nconsistent causal structure learning algorithms, such as the PC algorithm, GES,\nand GSP, and show that our method achieves comparable performance with a\nshorter runtime. Thus, our method can be used on its own for causal structure\ndiscovery. Finally, we show that there exist dense graphs on which our method\nachieves almost perfect performance, so that unlike most existing causal\nstructure learning algorithms, the situations in which our algorithm achieves\nboth good performance and good runtime are not limited to sparse graphs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 21:56:41 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Squires", "Chandler", ""], ["Amaniampong", "Joshua", ""], ["Uhler", "Caroline", ""]]}, {"id": "2011.03668", "submitter": "Guenther Walther", "authors": "Guenther Walther, Alnur Ali, Xinyue Shen and Stephen Boyd", "title": "Confidence bands for a log-concave density", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for inference about a log-concave distribution:\nInstead of using the method of maximum likelihood, we propose to incorporate\nthe log-concavity constraint in an appropriate nonparametric confidence set for\nthe cdf $F$. This approach has the advantage that it automatically provides a\nmeasure of statistical uncertainty and it thus overcomes a marked limitation of\nthe maximum likelihood estimate. In particular, we show how to construct\nconfidence bands for the density that have a finite sample guaranteed\nconfidence level. The nonparametric confidence set for $F$ which we introduce\nhere has attractive computational and statistical properties: It allows to\nbring modern tools from optimization to bear on this problem via difference of\nconvex programming, and it results in optimal statistical inference. We show\nthat the width of the resulting confidence bands converges at nearly the\nparametric $n^{-\\frac{1}{2}}$ rate when the log density is $k$-affine.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 03:15:27 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Walther", "Guenther", ""], ["Ali", "Alnur", ""], ["Shen", "Xinyue", ""], ["Boyd", "Stephen", ""]]}, {"id": "2011.03718", "submitter": "Ryan Chen", "authors": "Ryan Chen, Javier Cabrera", "title": "Bootstrap Confidence Intervals Using the Likelihood Ratio Test in\n  Changepoint Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims to evaluate the performance of power in the likelihood ratio\ntest for changepoint detection by bootstrap sampling, and proposes a hypothesis\ntest based on bootstrapped confidence interval lengths. Assuming i.i.d normally\ndistributed errors, and using the bootstrap method, the changepoint sampling\ndistribution is estimated. Furthermore, this study describes a method to\nestimate a data set with no changepoint to form the null sampling distribution.\nWith the null sampling distribution, and the distribution of the estimated\nchangepoint, critical values and power calculations can be made, over the\nlengths of confidence intervals.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 07:49:53 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Chen", "Ryan", ""], ["Cabrera", "Javier", ""]]}, {"id": "2011.03872", "submitter": "Gabriel Martos Venturini", "authors": "Miguel de Carvalho, Gabriel Martos", "title": "Modeling Interval Trendlines: Symbolic Singular Spectrum Analysis for\n  Interval Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this article we propose an extension of singular spectrum analysis for\ninterval-valued time series. The proposed methods can be used to decompose and\nforecast the dynamics governing a set-valued stochastic process. The resulting\ncomponents on which the interval time series is decomposed can be understood as\ninterval trendlines, cycles, or noise. Forecasting can be conducted through a\nlinear recurrent method, and we devised generalizations of the decomposition\nmethod for the multivariate setting. The performance of the proposed methods is\nshowcased in a simulation study. We apply the proposed methods so to track the\ndynamics governing the Argentina Stock Market (MERVAL) in real time, in a case\nstudy that covers the most recent period of turbulence that led to discussions\nof the government of Argentina with the International Monetary Fund.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 00:16:38 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["de Carvalho", "Miguel", ""], ["Martos", "Gabriel", ""]]}, {"id": "2011.03900", "submitter": "Yichen Wang", "authors": "T. Tony Cai, Yichen Wang, Linjun Zhang", "title": "The Cost of Privacy in Generalized Linear Models: Algorithms and Minimax\n  Lower Bounds", "comments": "56 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose differentially private algorithms for parameter estimation in both\nlow-dimensional and high-dimensional sparse generalized linear models (GLMs) by\nconstructing private versions of projected gradient descent. We show that the\nproposed algorithms are nearly rate-optimal by characterizing their statistical\nperformance and establishing privacy-constrained minimax lower bounds for GLMs.\nThe lower bounds are obtained via a novel technique, which is based on Stein's\nLemma and generalizes the tracing attack technique for privacy-constrained\nlower bounds. This lower bound argument can be of independent interest as it is\napplicable to general parametric models. Simulated and real data experiments\nare conducted to demonstrate the numerical performance of our algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 04:27:21 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 00:30:30 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Cai", "T. Tony", ""], ["Wang", "Yichen", ""], ["Zhang", "Linjun", ""]]}, {"id": "2011.03996", "submitter": "Marcelo Medeiros", "authors": "Jianqing Fan, Ricardo P. Masini, Marcelo C. Medeiros", "title": "Do We Exploit all Information for Counterfactual Analysis? Benefits of\n  Factor Models and Idiosyncratic Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The measurement of treatment (intervention) effects on a single (or just a\nfew) treated unit(s) based on counterfactuals constructed from artificial\ncontrols has become a popular practice in applied statistics and economics\nsince the proposal of the synthetic control method. In high-dimensional\nsetting, we often use principal component or (weakly) sparse regression to\nestimate counterfactuals. Do we use enough data information? To better estimate\nthe effects of price changes on the sales in our case study, we propose a\ngeneral framework on counterfactual analysis for high dimensional dependent\ndata. The framework includes both principal component regression and sparse\nlinear regression as specific cases. It uses both factor and idiosyncratic\ncomponents as predictors for improved counterfactual analysis, resulting a\nmethod called Factor-Adjusted Regularized Method for Treatment (FarmTreat)\nevaluation. We demonstrate convincingly that using either factors or sparse\nregression is inadequate for counterfactual analysis in many applications and\nthe case for information gain can be made through the use of idiosyncratic\ncomponents. We also develop theory and methods to formally answer the question\nif common factors are adequate for estimating counterfactuals. Furthermore, we\nconsider a simple resampling approach to conduct inference on the treatment\neffect as well as bootstrap test to access the relevance of the idiosyncratic\ncomponents. We apply the proposed method to evaluate the effects of price\nchanges on the sales of a set of products based on a novel large panel of sale\ndata from a major retail chain in Brazil and demonstrate the benefits of using\nadditional idiosyncratic components in the treatment effect evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 15:07:48 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 12:27:40 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Fan", "Jianqing", ""], ["Masini", "Ricardo P.", ""], ["Medeiros", "Marcelo C.", ""]]}, {"id": "2011.04067", "submitter": "Ge Zhao", "authors": "Ge Zhao, Yanyuan Ma, Huazhen Lin, Yi Li", "title": "Semiparametric regression of mean residual life with censoring and\n  covariate dimension reduction", "comments": "73 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of semiparametric regression models of mean residual\nlife for censored outcome data. The models, which enable us to estimate the\nexpected remaining survival time and generalize commonly used mean residual\nlife models, also conduct covariate dimension reduction. Using the geometric\napproaches in semiparametrics literature and the martingale properties with\nsurvival data, we propose a flexible inference procedure that relaxes the\nparametric assumptions on the dependence of mean residual life on covariates\nand how long a patient has lived. We show that the estimators for the covariate\neffects are root-$n$ consistent, asymptotically normal, and semiparametrically\nefficient. With the unspecified mean residual life function, we provide a\nnonparametric estimator for predicting the residual life of a given subject,\nand establish the root-$n$ consistency and asymptotic normality for this\nestimator. Numerical experiments are conducted to illustrate the feasibility of\nthe proposed estimators. We apply the method to analyze a national kidney\ntransplantation dataset to further demonstrate the utility of the work.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 20:11:54 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Zhao", "Ge", ""], ["Ma", "Yanyuan", ""], ["Lin", "Huazhen", ""], ["Li", "Yi", ""]]}, {"id": "2011.04116", "submitter": "Colin Daly", "authors": "Colin Daly", "title": "An Embedded Model Estimator for Non-Stationary Random Functions using\n  Multiple Secondary Variables", "comments": "29 pages; 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for non-stationary spatial modelling using multiple secondary\nvariables is developed. It combines Geostatistics with Quantile Random Forests\nto give a new interpolation and stochastic simulation algorithm. This paper\nintroduces the method and shows that it has consistency results that are\nsimilar in nature to those applying to geostatistical modelling and to Quantile\nRandom Forests. The method allows for embedding of simpler interpolation\ntechniques, such as Kriging, to further condition the model. The algorithm\nworks by estimating a conditional distribution for the target variable at each\ntarget location. The family of such distributions is called the envelope of the\ntarget variable. From this, it is possible to obtain spatial estimates,\nquantiles and uncertainty. An algorithm to produce conditional simulations from\nthe envelope is also developed. As they sample from the envelope, realizations\nare therefore locally influenced by relative changes of importance of secondary\nvariables, trends and variability.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 00:14:24 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 20:15:18 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 13:17:19 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Daly", "Colin", ""]]}, {"id": "2011.04135", "submitter": "Guanyu Hu", "authors": "Junxian Geng, Guanyu Hu", "title": "Mixture of Finite Mixtures Model for Basket Trial", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the recent paradigm shift from cytotoxic drugs to new generation of\ntarget therapy and immuno-oncology therapy during oncology drug developments,\npatients with various cancer (sub)types may be eligible to participate in a\nbasket trial if they have the same molecular target. Bayesian hierarchical\nmodeling (BHM) are widely used in basket trial data analysis, where they\nadaptively borrow information among different cohorts (subtypes) rather than\nfully pool the data together or doing stratified analysis based on each cohort.\nThose approaches, however, may have the risk of over shrinkage estimation\nbecause of the invalidated exchangeable assumption. We propose a two-step\nprocedure to find the balance between pooled and stratified analysis. In the\nfirst step, we treat it as a clustering problem by grouping cohorts into\nclusters that share the similar treatment effect. In the second step, we use\nshrinkage estimator from BHM to estimate treatment effects for cohorts within\neach cluster under exchangeable assumption. For clustering part, we adapt the\nmixture of finite mixtures (MFM) approach to have consistent estimate of the\nnumber of clusters. We investigate the performance of our proposed method in\nsimulation studies and apply this method to Vemurafenib basket trial data\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 01:43:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Geng", "Junxian", ""], ["Hu", "Guanyu", ""]]}, {"id": "2011.04147", "submitter": "Ruiqi Liu", "authors": "Ruiqi Liu, Kexuan Li, Zuofeng Shang", "title": "A Computationally Efficient Classification Algorithm in Posterior Drift\n  Model: Phase Transition and Minimax Adaptivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In massive data analysis, training and testing data often come from very\ndifferent sources, and their probability distributions are not necessarily\nidentical. A feature example is nonparametric classification in posterior drift\nmodel where the conditional distributions of the label given the covariates are\npossibly different. In this paper, we derive minimax rate of the excess risk\nfor nonparametric classification in posterior drift model in the setting that\nboth training and testing data have smooth distributions, extending a recent\nwork by Cai and Wei (2019) who only impose smoothness condition on the\ndistribution of testing data. The minimax rate demonstrates a phase transition\ncharacterized by the mutual relationship between the smoothness orders of the\ntraining and testing data distributions. We also propose a computationally\nefficient and data-driven nearest neighbor classifier which achieves the\nminimax excess risk (up to a logarithm factor). Simulation studies and a\nreal-world application are conducted to demonstrate our approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 02:12:24 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Liu", "Ruiqi", ""], ["Li", "Kexuan", ""], ["Shang", "Zuofeng", ""]]}, {"id": "2011.04155", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Xibin Zhang", "title": "Bayesian bandwidth estimation for local linear fitting in nonparametric\n  regression models", "comments": "25 pages, 6 figures, to appear at Studies in Nonlinear Dynamics &\n  Econometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian sampling approach to bandwidth estimation for\nthe local linear estimator of the regression function in a nonparametric\nregression model. In the Bayesian sampling approach, the error density is\napproximated by a location-mixture density of Gaussian densities with means the\nindividual errors and variance a constant parameter. This mixture density has\nthe form of a kernel density estimator of errors and is referred to as the\nkernel-form error density (c.f., Zhang et al., 2014). While Zhang et al. (2014)\nuse the local constant (also known as the Nadaraya- Watson) estimator to\nestimate the regression function, we extend this to the local linear estimator,\nwhich produces more accurate estimation. The proposed investigation is\nmotivated by the lack of data-driven methods for simultaneously choosing\nbandwidths in the local linear estimator of the regression function and\nkernel-form error density. Treating bandwidths as parameters, we derive an\napproximate (pseudo) likelihood and a posterior. A simulation study shows that\nthe proposed bandwidth estimation outperforms the rule-of-thumb and\ncross-validation methods under the criterion of integrated squared errors. The\nproposed bandwidth estimation method is validated through a nonparametric\nregression model involving firm ownership concentration, and a model involving\nstate-price density estimation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 02:26:34 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Shang", "Han Lin", ""], ["Zhang", "Xibin", ""]]}, {"id": "2011.04168", "submitter": "Maryclare Griffin", "authors": "Maryclare Griffin, Gennady Samorodnitsky, and David S. Matteson", "title": "Likelihood Inference for Possibly Non-Stationary Processes via Adaptive\n  Overdifferencing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make a simple observation that facilitates valid likelihood-based\ninference for the parameters of the popular ARFIMA or FARIMA model without\nrequiring stationarity by allowing the upper bound $\\bar{d}$ for the memory\nparameter $d$ to exceed $0.5$. We observe that estimating the parameters of a\nsingle non-stationary ARFIMA model is equivalent to estimating the parameters\nof a sequence of stationary ARFIMA models. This enables improved inference\nbecause many standard methods perform poorly when estimates are close to the\nboundary of the parameter space. It also allows us to leverage the wealth of\nlikelihood approximations that have been introduced for estimating the\nparameters of a stationary process. We explore how estimation of the memory\nparameter $d$ depends on the upper bound $\\bar{d}$ and introduce adaptive\nprocedures for choosing $\\bar{d}$. Via simulations, we examine the performance\nof our adaptive procedures for estimating the memory parameter when the true\nvalue is as large as $2.5$. Our adaptive procedures estimate the memory\nparameter well, can be used to obtain confidence intervals for the memory\nparameter that achieve nominal coverage rates, and perform favorably relative\nto existing alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 03:28:22 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Griffin", "Maryclare", ""], ["Samorodnitsky", "Gennady", ""], ["Matteson", "David S.", ""]]}, {"id": "2011.04207", "submitter": "Wei Xie", "authors": "Wei Xie, Barry L. Nelson, Russell R. Barton", "title": "Statistical Uncertainty Analysis for Stochastic Simulation", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When we use simulation to evaluate the performance of a stochastic system,\nthe simulation often contains input distributions estimated from real-world\ndata; therefore, there is both simulation and input uncertainty in the\nperformance estimates. Ignoring either source of uncertainty underestimates the\noverall statistical error. Simulation uncertainty can be reduced by additional\ncomputation (e.g., more replications). Input uncertainty can be reduced by\ncollecting more real-world data, when feasible. This paper proposes an approach\nto quantify overall statistical uncertainty when the simulation is driven by\nindependent parametric input distributions; specifically, we produce a\nconfidence interval that accounts for both simulation and input uncertainty by\nusing a metamodel-assisted bootstrapping approach. The input uncertainty is\nmeasured via bootstrapping, an equation-based stochastic kriging metamodel\npropagates the input uncertainty to the output mean, and both simulation and\nmetamodel uncertainty are derived using properties of the metamodel. A variance\ndecomposition is proposed to estimate the relative contribution of input to\noverall uncertainty; this information indicates whether the overall uncertainty\ncan be significantly reduced through additional simulation alone. Asymptotic\nanalysis provides theoretical support for our approach, while an empirical\nstudy demonstrates that it has good finite-sample performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 05:59:07 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Xie", "Wei", ""], ["Nelson", "Barry L.", ""], ["Barton", "Russell R.", ""]]}, {"id": "2011.04216", "submitter": "Amit Sharma", "authors": "Amit Sharma, Emre Kiciman", "title": "DoWhy: An End-to-End Library for Causal Inference", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.MS econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to efficient statistical estimators of a treatment's effect,\nsuccessful application of causal inference requires specifying assumptions\nabout the mechanisms underlying observed data and testing whether they are\nvalid, and to what extent. However, most libraries for causal inference focus\nonly on the task of providing powerful statistical estimators. We describe\nDoWhy, an open-source Python library that is built with causal assumptions as\nits first-class citizens, based on the formal framework of causal graphs to\nspecify and test causal assumptions. DoWhy presents an API for the four steps\ncommon to any causal analysis---1) modeling the data using a causal graph and\nstructural assumptions, 2) identifying whether the desired effect is estimable\nunder the causal model, 3) estimating the effect using statistical estimators,\nand finally 4) refuting the obtained estimate through robustness checks and\nsensitivity analyses. In particular, DoWhy implements a number of robustness\nchecks including placebo tests, bootstrap tests, and tests for unoberved\nconfounding. DoWhy is an extensible library that supports interoperability with\nother implementations, such as EconML and CausalML for the the estimation step.\nThe library is available at https://github.com/microsoft/dowhy\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 06:22:11 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Sharma", "Amit", ""], ["Kiciman", "Emre", ""]]}, {"id": "2011.04315", "submitter": "Elias Raninen", "authors": "Elias Raninen and Esa Ollila", "title": "Coupled regularized sample covariance matrix estimator for multiple\n  classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of covariance matrices of multiple classes with limited\ntraining data is a difficult problem. The sample covariance matrix (SCM) is\nknown to perform poorly when the number of variables is large compared to the\navailable number of samples. In order to reduce the mean squared error (MSE) of\nthe SCM, regularized (shrinkage) SCM estimators are often used. In this work,\nwe consider regularized SCM (RSCM) estimators for multiclass problems that\ncouple together two different target matrices for regularization: the pooled\n(average) SCM of the classes and the scaled identity matrix. Regularization\ntoward the pooled SCM is beneficial when the population covariances are\nsimilar, whereas regularization toward the identity matrix guarantees that the\nestimators are positive definite. We derive the MSE optimal tuning parameters\nfor the estimators as well as propose a method for their estimation under the\nassumption that the class populations follow (unspecified) elliptical\ndistributions with finite fourth-order moments. The MSE performance of the\nproposed coupled RSCMs are evaluated with simulations and in a regularized\ndiscriminant analysis (RDA) classification set-up on real data. The results\nbased on three different real data sets indicate comparable performance to\ncross-validation but with a significant speed-up in computation time.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 10:39:53 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Raninen", "Elias", ""], ["Ollila", "Esa", ""]]}, {"id": "2011.04369", "submitter": "Steffen Betsch", "authors": "Steffen Betsch, Bruno Ebner, Franz Nestmann", "title": "Characterizations of non-normalized discrete probability distributions\n  and their application in statistics", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From the distributional characterizations that lie at the heart of Stein's\nmethod we derive explicit formulae for the mass functions of discrete\nprobability laws that identify those distributions. These identities are\napplied to develop tools for the solution of statistical problems. Our\ncharacterizations, and hence the applications built on them, do not require any\nknowledge about normalization constants of the probability laws. We discuss\nseveral examples where this lack of feasibility of the normalization constant\nis a built-in feature. To demonstrate that our statistical methods are sound,\nwe provide comparative simulation studies for the testing of fit to the Poisson\ndistribution and for parameter estimation of the negative binomial family when\nboth parameters are unknown. We also consider the problem of parameter\nestimation for discrete exponential-polynomial models which generally are\nnon-normalized.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 12:08:12 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Betsch", "Steffen", ""], ["Ebner", "Bruno", ""], ["Nestmann", "Franz", ""]]}, {"id": "2011.04464", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Jason L. Williams, Lennart Svensson,\n  Yuxuan Xia", "title": "A Poisson multi-Bernoulli mixture filter for coexisting point and\n  extended targets", "comments": "Matlab files can be found at\n  https://github.com/Agarciafernandez/Coexisting-point-extended-target-PMBM-filter\n  and\n  https://github.com/yuhsuansia/Coexisting-point-extended-target-PMBM-filter. A\n  relevant multi-object tracking course can be found at\n  https://www.youtube.com/channel/UCa2-fpj6AV8T6JK1uTRuFpw", "journal-ref": "in IEEE Transactions on Signal Processing, vol. 69, pp. 2600-2610,\n  2021", "doi": "10.1109/TSP.2021.3072006", "report-no": null, "categories": "stat.ME cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a Poisson multi-Bernoulli mixture (PMBM) filter for\ncoexisting point and extended targets, i.e., for scenarios where there may be\nsimultaneous point and extended targets. The PMBM filter provides a recursion\nto compute the multi-target filtering posterior based on probabilistic\ninformation on data associations, and single-target predictions and updates. In\nthis paper, we first derive the PMBM filter update for a generalised\nmeasurement model, which can include measurements originated from point and\nextended targets. Second, we propose a single-target space that accommodates\nboth point and extended targets and derive the filtering recursion that\npropagates Gaussian densities for point targets and gamma Gaussian inverse\nWishart densities for extended targets. As a computationally efficient\napproximation of the PMBM filter, we also develop a Poisson multi-Bernoulli\n(PMB) filter for coexisting point and extended targets. The resulting filters\nare analysed via numerical simulations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 14:41:40 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 06:28:22 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Williams", "Jason L.", ""], ["Svensson", "Lennart", ""], ["Xia", "Yuxuan", ""]]}, {"id": "2011.04470", "submitter": "Soumendu Sundar Mukherjee", "authors": "Abhinav Chakraborty and Soumendu Sundar Mukherjee and Arijit\n  Chakrabarti", "title": "High dimensional PCA: a new model selection criterion", "comments": "37 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random sample from a multivariate population, estimating the number\nof large eigenvalues of the population covariance matrix is an important\nproblem in Statistics with wide applications in many areas. In the context of\nPrincipal Component Analysis (PCA), the linear combinations of the original\nvariables having the largest amounts of variation are determined by this\nnumber. In this paper, we study the high dimensional asymptotic regime where\nthe number of variables grows at the same rate as the number of observations,\nand use the spiked covariance model proposed in Johnstone (2001), under which\nthe problem reduces to model selection. Our focus is on the Akaike Information\nCriterion (AIC) which is known to be strongly consistent from the work of Bai\net al. (2018). However, Bai et al. (2018) requires a certain \"gap condition\"\nensuring the dominant eigenvalues to be above a threshold strictly larger than\nthe BBP threshold (Baik et al. (2005), both quantities depending on the\nlimiting ratio of the number of variables and observations. It is well-known\nthat, below the BBP threshold, a spiked covariance structure becomes\nindistinguishable from one with no spikes. Thus the strong consistency of AIC\nrequires some extra signal strength.\n  In this paper, we investigate whether consistency continues to hold even if\nthe \"gap\" is made smaller. We show that strong consistency under arbitrarily\nsmall gap is achievable if we alter the penalty term of AIC suitably depending\non the target gap. Furthermore, another intuitive alteration of the penalty can\nindeed make the gap exactly zero, although we can only achieve weak consistency\nin this case. We compare the two newly-proposed estimators with other existing\nestimators in the literature via extensive simulation studies, and show, by\nsuitably calibrating our proposals, that a significant improvement in terms of\nmean-squared error is achievable.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 14:42:31 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Chakraborty", "Abhinav", ""], ["Mukherjee", "Soumendu Sundar", ""], ["Chakrabarti", "Arijit", ""]]}, {"id": "2011.04486", "submitter": "Emma Simpson", "authors": "Emma S. Simpson, Thomas Opitz, Jennifer L. Wadsworth", "title": "High-dimensional modeling of spatial and spatio-temporal conditional\n  extremes using INLA and the SPDE approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conditional extremes framework allows for event-based stochastic modeling\nof dependent extremes, and has recently been extended to spatial and\nspatio-temporal settings. After standardizing the marginal distributions and\napplying an appropriate linear normalization, certain non-stationary Gaussian\nprocesses can be used as asymptotically-motivated models for the process\nconditioned on threshold exceedances at a fixed reference location and time. In\nthis work, we adopt a Bayesian perspective by implementing estimation through\nthe integrated nested Laplace approximation (INLA), allowing for novel and\nflexible semi-parametric specifications of the Gaussian mean function. By using\nGauss-Markov approximations of the Mat\\'ern covariance function (known as the\nStochastic Partial Differential Equation approach) at a latent stage of the\nmodel, likelihood-based inference becomes feasible even with thousands of\nobserved locations. We explain how constraints on the spatial and\nspatio-temporal Gaussian processes, arising from the conditioning mechanism,\ncan be implemented through the latent variable approach without losing the\ncomputationally convenient Markov property. We discuss tools for the comparison\nof models via their posterior distributions, and illustrate the flexibility of\nthe approach with gridded Red Sea surface temperature data at over 6,000\nobserved locations. Posterior sampling is exploited to study the probability\ndistribution of cluster functionals of spatial and spatio-temporal extreme\nepisodes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:11:47 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 09:42:27 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Simpson", "Emma S.", ""], ["Opitz", "Thomas", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "2011.04493", "submitter": "Justin Krometis", "authors": "Nathan E. Glatt-Holtz, Justin A. Krometis, Cecilia F. Mondaini", "title": "On the accept-reject mechanism for Metropolis-Hastings algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops a powerful and versatile framework for determining\nacceptance ratios in Metropolis-Hastings type Markov kernels widely used in\nstatistical sampling problems. Our approach allows us to derive new classes of\nkernels which unify random walk or diffusion-type sampling methods with more\ncomplicated \"extended phase space\" algorithms based around ideas from\nHamiltonian dynamics. Our starting point is an abstract result developed in the\ngenerality of measurable state spaces that addresses proposal kernels that\npossess a certain involution structure. Note that, while this underlying\nproposal structure suggests a scope which includes Hamiltonian-type kernels, we\ndemonstrate that our abstract result is, in an appropriate sense, equivalent to\nan earlier general state space setting developed in [Tierney, Annals of Applied\nProbability, 1998] where the connection to Hamiltonian methods was more\nobscure. Altogether, the theoretical unity and reach of our main result\nprovides a basis for deriving novel sampling algorithms while laying bare\nimportant relationships between existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:24:50 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 19:54:45 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Glatt-Holtz", "Nathan E.", ""], ["Krometis", "Justin A.", ""], ["Mondaini", "Cecilia F.", ""]]}, {"id": "2011.04504", "submitter": "Wang Miao", "authors": "Wang Miao, Wenjie Hu, Elizabeth L. Ogburn, and Xiaohua Zhou", "title": "Identifying effects of multiple treatments in the presence of unmeasured\n  confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of treatment effects in the presence of unmeasured confounding\nis a persistent problem in the social, biological, and medical sciences. The\nproblem of unmeasured confounding in settings with multiple treatments is most\ncommon in statistical genetics and bioinformatics settings, where researchers\nhave developed many successful statistical strategies without engaging deeply\nwith the causal aspects of the problem. Recently there have been a number of\nattempts to bridge the gap between these statistical approaches and causal\ninference, but these attempts have either been shown to be flawed or have\nrelied on fully parametric assumptions. In this paper, we propose two\nstrategies for identifying and estimating causal effects of multiple treatments\nin the presence of unmeasured confounding. The auxiliary variables approach\nleverages auxiliary variables that are not causally associated with the\noutcome; in the case of a univariate confounder, our method only requires one\nauxiliary variable, unlike existing instrumental variable methods that would\nrequire as many instruments as there are treatments. An alternative null\ntreatments approach relies on the assumption that at least half of the\nconfounded treatments have no causal effect on the outcome, but does not\nrequire a priori knowledge of which treatments are null. Our identification\nstrategies do not impose parametric assumptions on the outcome model and do not\nrest on estimation of the confounder. This work extends and generalizes\nexisting work on unmeasured confounding with a single treatment, and provides a\nnonparametric extension of models commonly used in bioinformatics.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:32:41 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 12:09:21 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 13:56:52 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Miao", "Wang", ""], ["Hu", "Wenjie", ""], ["Ogburn", "Elizabeth L.", ""], ["Zhou", "Xiaohua", ""]]}, {"id": "2011.04519", "submitter": "Jaco Visagie", "authors": "E. Bothma, J. S. Allison, M. Cockeran and I. J. H. Visagie", "title": "Kaplan-Meier based tests for exponentiality in the presence of censoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we test the composite hypothesis that lifetimes follow an\nexponential distribution based on observed randomly right censored data.\nTesting this hypothesis is complicated by the presence of this censoring, due\nto the fact that not all lifetimes are observed. To account for this\ncomplication, we propose modifications to tests based on the empirical\ncharacteristic function and Laplace transform. In the full sample case these\nempirical functions can be expressed as integrals with respect to the empirical\ndistribution function of the lifetimes. We propose replacing this estimate of\nthe distribution function by the Kaplan-Meier estimate. The resulting test\nstatistics can be expressed in easily calculable forms in terms of summations\nof functionals of the observed data. Additionally, a general framework for\ngoodness-of-fit testing, in the presence of random right censoring, is\noutlined. A Monte Carlo study is performed, the results of which indicate that\nthe newly modified tests generally outperform the existing tests. A practical\napplication, concerning initial remission times of leukemia patients, is\ndiscussed along with some concluding remarks and avenues for future research.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:54:04 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Bothma", "E.", ""], ["Allison", "J. S.", ""], ["Cockeran", "M.", ""], ["Visagie", "I. J. H.", ""]]}, {"id": "2011.04532", "submitter": "Louis Raynal", "authors": "Louis Raynal, Sixing Chen, Antonietta Mira, Jukka-Pekka Onnela", "title": "Scalable Approximate Bayesian Computation for Growing Network Models via\n  Extrapolated and Sampled Summaries", "comments": "28 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a simulation-based likelihood-free\nmethod applicable to both model selection and parameter estimation. ABC\nparameter estimation requires the ability to forward simulate datasets from a\ncandidate model, but because the sizes of the observed and simulated datasets\nusually need to match, this can be computationally expensive. Additionally,\nsince ABC inference is based on comparisons of summary statistics computed on\nthe observed and simulated data, using computationally expensive summary\nstatistics can lead to further losses in efficiency. ABC has recently been\napplied to the family of mechanistic network models, an area that has\ntraditionally lacked tools for inference and model choice. Mechanistic models\nof network growth repeatedly add nodes to a network until it reaches the size\nof the observed network, which may be of the order of millions of nodes. With\nABC, this process can quickly become computationally prohibitive due to the\nresource intensive nature of network simulations and evaluation of summary\nstatistics. We propose two methodological developments to enable the use of ABC\nfor inference in models for large growing networks. First, to save time needed\nfor forward simulating model realizations, we propose a procedure to\nextrapolate (via both least squares and Gaussian processes) summary statistics\nfrom small to large networks. Second, to reduce computation time for evaluating\nsummary statistics, we use sample-based rather than census-based summary\nstatistics. We show that the ABC posterior obtained through this approach,\nwhich adds two additional layers of approximation to the standard ABC, is\nsimilar to a classic ABC posterior. Although we deal with growing network\nmodels, both extrapolated summaries and sampled summaries are expected to be\nrelevant in other ABC settings where the data are generated incrementally.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 16:13:47 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Raynal", "Louis", ""], ["Chen", "Sixing", ""], ["Mira", "Antonietta", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "2011.04538", "submitter": "Hao Chen Dr.", "authors": "Hao Chen, Lanshan Han and Alvin Lim", "title": "Estimating Linear Mixed Effects Models with Truncated Normally\n  Distributed Random Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linear Mixed Effects (LME) models have been widely applied in clustered data\nanalysis in many areas including marketing research, clinical trials, and\nbiomedical studies. Inference can be conducted using maximum likelihood\napproach if assuming Normal distributions on the random effects. However, in\nmany applications of economy, business and medicine, it is often essential to\nimpose constraints on the regression parameters after taking their real-world\ninterpretations into account. Therefore, in this paper we extend the classical\n(unconstrained) LME models to allow for sign constraints on its overall\ncoefficients. We propose to assume a symmetric doubly truncated Normal (SDTN)\ndistribution on the random effects instead of the unconstrained Normal\ndistribution which is often found in classical literature. With the\naforementioned change, difficulty has dramatically increased as the exact\ndistribution of the dependent variable becomes analytically intractable. We\nthen develop likelihood-based approaches to estimate the unknown model\nparameters utilizing the approximation of its exact distribution. Simulation\nstudies have shown that the proposed constrained model not only improves\nreal-world interpretations of results, but also achieves satisfactory\nperformance on model fits as compared to the existing model.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 16:17:35 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 16:41:06 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 14:22:47 GMT"}, {"version": "v4", "created": "Thu, 15 Jul 2021 14:43:44 GMT"}, {"version": "v5", "created": "Sat, 17 Jul 2021 23:29:36 GMT"}, {"version": "v6", "created": "Wed, 21 Jul 2021 00:15:39 GMT"}, {"version": "v7", "created": "Thu, 22 Jul 2021 21:01:52 GMT"}, {"version": "v8", "created": "Thu, 29 Jul 2021 02:02:51 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chen", "Hao", ""], ["Han", "Lanshan", ""], ["Lim", "Alvin", ""]]}, {"id": "2011.04766", "submitter": "Werner Brannath", "authors": "Werner Brannath, Charlie Hillner, Kornelius Rohmeyer", "title": "A liberal type I error rate for studies in precision medicine", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new multiple type I error criterion for clinical trials with\nmultiple populations. Such trials are of interest in precision medicine where\nthe goal is to develop treatments that are targeted to specific sub-populations\ndefined by genetic and/or clinical biomarkers. The new criterion is based on\nthe observation that not all type I errors are relevant to all patients in the\noverall population. If disjoint sub-populations are considered, no multiplicity\nadjustment appears necessary, since a claim in one sub-population does not\naffect patients in the other ones. For intersecting sub-populations we suggest\nto control the average multiple type error rate, i.e. the probably that a\nrandomly selected patient will be exposed to an inefficient treatment. We call\nthis the population-wise error rate, exemplify it by a number of examples and\nillustrate how to control it with an adjustment of critical boundaries or\nadjusted p-values. We furthermore define corresponding simultaneous confidence\nintervals. We finally illustrate the power gain achieved by passing from\nfamily-wise to population-wise error rate control with two simple examples and\na recently suggest multiple testing approach for umbrella trials.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 21:10:14 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 20:52:40 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Brannath", "Werner", ""], ["Hillner", "Charlie", ""], ["Rohmeyer", "Kornelius", ""]]}, {"id": "2011.04826", "submitter": "Mattew Cefalu", "authors": "Matthew Cefalu, Brian G. Vegetabile, Michael Dworsky, Christine\n  Eibner, and Federico Girosi", "title": "Reducing bias in difference-in-differences models using entropy\n  balancing", "comments": "20 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper illustrates the use of entropy balancing in\ndifference-in-differences analyses when pre-intervention outcome trends suggest\na possible violation of the parallel trends assumption. We describe a set of\nassumptions under which weighting to balance intervention and comparison groups\non pre-intervention outcome trends leads to consistent\ndifference-in-differences estimates even when pre-intervention outcome trends\nare not parallel. Simulated results verify that entropy balancing of\npre-intervention outcomes trends can remove bias when the parallel trends\nassumption is not directly satisfied, and thus may enable researchers to use\ndifference-in-differences designs in a wider range of observational settings\nthan previously acknowledged.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 23:21:17 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Cefalu", "Matthew", ""], ["Vegetabile", "Brian G.", ""], ["Dworsky", "Michael", ""], ["Eibner", "Christine", ""], ["Girosi", "Federico", ""]]}, {"id": "2011.04833", "submitter": "Johan Steen", "authors": "Johan Steen, Pawel Morzywolek, Wim Van Biesen, Johan Decruyenaere,\n  Stijn Vansteelandt", "title": "Handling time-dependent exposures and confounders when estimating\n  attributable fractions -- bridging the gap between multistate and\n  counterfactual modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The population-attributable fraction (PAF) expresses the percentage of events\nthat could have been prevented by eradicating a certain exposure in a certain\npopulation. It can be strongly time-dependent because either exposure incidence\nor excess risk may change over time. Competing events may moreover hinder the\noutcome of interest from being observed. Occurrence of either of these events\nmay, in turn, prevent the exposure of interest. Estimation approaches thus need\nto carefully account for the timing of potential events in such highly dynamic\nsettings. The use of multistate models (MSMs) has been widely encouraged to\nmeet this need so as to eliminate preventable yet common types of bias, such as\nimmortal time bias. However, certain MSM based proposals for PAF estimation\nfail to fully eliminate such biases. In addition, assessing whether patients\ndie from rather than with a certain exposure, not only requires adequate\nmodeling of the timing of events, but also of the confounding factors affecting\nthese events and their timing. While proposed MSM approaches for confounding\nadjustment may be sufficient to accommodate imbalances between infected and\nuninfected patients present at baseline, these proposals generally fail to\nadequately tackle time-dependent confounding. For this, a class of generalized\nmethods (g-methods) which includes inverse probability (IP) weighting can be\nused. Because the connection between MSMs and g-methods is not readily\napparent, we here provide a detailed mapping between MSM and IP of censoring\nweighting approaches for estimating PAFs. In particular, we illustrate that the\nconnection between these two approaches can be made more apparent by means of a\nweighting-based characterization of MSM approaches that aids to both pinpoint\ncurrent shortcomings of MSM based proposals and to enhance intuition into\nsimple modifications to overcome these limitations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 23:41:08 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Steen", "Johan", ""], ["Morzywolek", "Pawel", ""], ["Van Biesen", "Wim", ""], ["Decruyenaere", "Johan", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2011.04854", "submitter": "Richard Creswell", "authors": "Richard Creswell, Ben Lambert, Chon Lok Lei, Martin Robinson, David\n  Gavaghan", "title": "Using flexible noise models to avoid noise model misspecification in\n  inference of differential equation time series models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modelling time series, it is common to decompose observed variation into\na \"signal\" process, the process of interest, and \"noise\", representing nuisance\nfactors that obfuscate the signal. To separate signal from noise, assumptions\nmust be made about both parts of the system. If the signal process is\nincorrectly specified, our predictions using this model may generalise poorly;\nsimilarly, if the noise process is incorrectly specified, we can attribute too\nmuch or too little observed variation to the signal. With little justification,\nindependent Gaussian noise is typically chosen, which defines a statistical\nmodel that is simple to implement but often misstates system uncertainty and\nmay underestimate error autocorrelation. There are a range of alternative noise\nprocesses available but, in practice, none of these may be entirely\nappropriate, as actual noise may be better characterised as a time-varying\nmixture of these various types. Here, we consider systems where the signal is\nmodelled with ordinary differential equations and present classes of flexible\nnoise processes that adapt to a system's characteristics. Our noise models\ninclude a multivariate normal kernel where Gaussian processes allow for\nnon-stationary persistence and variance, and nonparametric Bayesian models that\npartition time series into distinct blocks of separate noise structures. Across\nthe scenarios we consider, these noise processes faithfully reproduce true\nsystem uncertainty: that is, parameter estimate uncertainty when doing\ninference using the correct noise model. The models themselves and the methods\nfor fitting them are scalable to large datasets and could help to ensure more\nappropriate quantification of uncertainty in a host of time series models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 01:44:22 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Creswell", "Richard", ""], ["Lambert", "Ben", ""], ["Lei", "Chon Lok", ""], ["Robinson", "Martin", ""], ["Gavaghan", "David", ""]]}, {"id": "2011.05067", "submitter": "Manuele Leonelli", "authors": "Miguel de Carvalho, Manuele Leonelli, Alex Rossi", "title": "Tracking change-points in multivariate extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we devise a statistical method for tracking and modeling\nchange-points on the dependence structure of multivariate extremes. The methods\nare motivated by and illustrated on a case study on crypto-assets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 12:17:38 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["de Carvalho", "Miguel", ""], ["Leonelli", "Manuele", ""], ["Rossi", "Alex", ""]]}, {"id": "2011.05068", "submitter": "Ilmun Kim", "authors": "Ilmun Kim, Aaditya Ramdas", "title": "Dimension-agnostic inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classical asymptotic theory for statistical inference usually involves\ncalibrating a statistic by fixing the dimension $d$ while letting the sample\nsize $n$ increase to infinity. Recently, much effort has been dedicated towards\nunderstanding how these methods behave in high-dimensional settings, where\n$d_n$ and $n$ both increase to infinity together at some prescribed relative\nrate. This often leads to different inference procedures, depending on the\nassumptions about the dimensionality, leaving the practitioner in a bind: given\na dataset with 100 samples in 20 dimensions, should they calibrate by assuming\n$n \\gg d$, or $d_n/n \\approx 0.2$? This paper considers the goal of\ndimension-agnostic inference -- developing methods whose validity does not\ndepend on any assumption on $d_n$. We introduce a new, generic approach that\nuses variational representations of existing test statistics along with sample\nsplitting and self-normalization to produce a new test statistic with a\nGaussian limiting distribution. The resulting statistic can be viewed as a\ncareful modification of degenerate U-statistics, dropping diagonal blocks and\nretaining off-diagonals. We exemplify our technique for a handful of classical\nproblems including one-sample mean and covariance testing. Our tests are shown\nto have minimax rate-optimal power against appropriate local alternatives, and\nwithout explicitly targeting the high-dimensional setting their power is\noptimal up to a $\\sqrt 2$ factor. A hidden advantage is that our proofs are\nsimple and transparent. We end by describing several fruitful open directions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 12:21:34 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 23:50:28 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Kim", "Ilmun", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2011.05195", "submitter": "Xinhe Wang", "authors": "Xinhe Wang, Tingyu Wang, Hanzhong Liu", "title": "Rerandomization in stratified randomized experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stratification and rerandomization are two well-known methods used in\nrandomized experiments for balancing the baseline covariates. Renowned scholars\nin experimental design have recommended combining these two methods; however,\nlimited studies have addressed the statistical properties of this combination.\nThis paper proposes two rerandomization methods to be used in stratified\nrandomized experiments, based on the overall and stratum-specific Mahalanobis\ndistances. The first method is applicable for nearly arbitrary numbers of\nstrata, strata sizes, and stratum-specific proportions of the treated units.\nThe second method, which is generally more efficient than the first method, is\nsuitable for situations in which the number of strata is fixed with their sizes\ntending to infinity. Under the randomization inference framework, we obtain\nthese methods' asymptotic distributions and the formulas of variance reduction\nwhen compared to stratified randomization. Our analysis does not require any\nmodeling assumption regarding the potential outcomes. Moreover, we provide\nasymptotically conservative variance estimators and confidence intervals for\nthe average treatment effect. The advantages of the proposed methods are\nexhibited through an extensive simulation study and a real-data example.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:50:11 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Wang", "Xinhe", ""], ["Wang", "Tingyu", ""], ["Liu", "Hanzhong", ""]]}, {"id": "2011.05223", "submitter": "Srilakshmi Pattabiraman", "authors": "Ryan Gabrys, Srilakshmi Pattabiraman, Vishal Rana, Jo\\~ao Ribeiro,\n  Mahdi Cheraghchi, Venkatesan Guruswami and Olgica Milenkovic", "title": "AC-DC: Amplification Curve Diagnostics for Covid-19 Group Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first part of the paper presents a review of the gold-standard testing\nprotocol for Covid-19, real-time, reverse transcriptase PCR, and its properties\nand associated measurement data such as amplification curves that can guide the\ndevelopment of appropriate and accurate adaptive group testing protocols. The\nsecond part of the paper is concerned with examining various off-the-shelf\ngroup testing methods for Covid-19 and identifying their strengths and\nweaknesses for the application at hand. The third part of the paper contains a\ncollection of new analytical results for adaptive semiquantitative group\ntesting with probabilistic and combinatorial priors, including performance\nbounds, algorithmic solutions, and noisy testing protocols. The probabilistic\nsetting is of special importance as it is designed to be simple to implement by\nnonexperts and handle heavy hitters. The worst-case paradigm extends and\nimproves upon prior work on semiquantitative group testing with and without\nspecialized PCR noise models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 16:33:58 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 22:46:53 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gabrys", "Ryan", ""], ["Pattabiraman", "Srilakshmi", ""], ["Rana", "Vishal", ""], ["Ribeiro", "Jo\u00e3o", ""], ["Cheraghchi", "Mahdi", ""], ["Guruswami", "Venkatesan", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "2011.05245", "submitter": "Emma Jingfei Zhang", "authors": "Jingfei Zhang and Yi Li", "title": "Gaussian Graphical Regression Models with High Dimensional Responses and\n  Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Though Gaussian graphical models have been widely used in many scientific\nfields, limited progress has been made to link graph structures to external\ncovariates because of substantial challenges in theory and computation. We\npropose a Gaussian graphical regression model, which regresses both the mean\nand the precision matrix of a Gaussian graphical model on covariates. In the\ncontext of co-expression quantitative trait locus (QTL) studies, our framework\nfacilitates estimation of both population- and subject-level gene regulatory\nnetworks, and detection of how subject-level networks vary with genetic\nvariants and clinical conditions. Our framework accommodates high dimensional\nresponses and covariates, and encourages covariate effects on both the mean and\nthe precision matrix to be sparse. In particular for the precision matrix, we\nstipulate simultaneous sparsity, i.e., group sparsity and element-wise\nsparsity, on effective covariates and their effects on network edges,\nrespectively. We establish variable selection consistency first under the case\nwith known mean parameters and then a more challenging case with unknown means\ndepending on external covariates, and show in both cases that the convergence\nrate of the estimated precision parameters is faster than that obtained by\nlasso or group lasso, a desirable property for the sparse group lasso\nestimation. The utility and efficacy of our proposed method is demonstrated\nthrough simulation studies and an application to a co-expression QTL study with\nbrain cancer patients.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 17:12:44 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Zhang", "Jingfei", ""], ["Li", "Yi", ""]]}, {"id": "2011.05482", "submitter": "Olanrewaju Akande", "authors": "Olanrewaju Akande and Jerome P. Reiter", "title": "Multiple Imputation for Nonignorable Item Nonresponse in Complex Surveys\n  Using Auxiliary Margin", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline a framework for multiple imputation of nonignorable item\nnonresponse when the marginal distributions of some of the variables with\nmissing values are known. In particular, our framework ensures that (i) the\ncompleted datasets result in design-based estimates of totals that are\nplausible, given the margins, and (ii) the completed datasets maintain\nassociations across variables as posited in the imputation models. To do so, we\npropose an additive nonignorable model for nonresponse, coupled with a\nrejection sampling step. The rejection sampling step favors completed datasets\nthat result in design-based estimates that are plausible given the known\nmargins. We illustrate the framework using simulations with stratified\nsampling.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 00:34:35 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Akande", "Olanrewaju", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "2011.05493", "submitter": "Muxuan Liang", "authors": "Muxuan Liang, Xiang Zhong, Jaeyoung Park", "title": "Learning a high-dimensional classification rule using auxiliary outcomes", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlated outcomes are common in many practical problems. Based on a\ndecomposition of estimation bias into two types, within-subspace and\nagainst-subspace, we develop a robust approach to estimating the classification\nrule for the outcome of interest with the presence of auxiliary outcomes in\nhigh-dimensional settings. The proposed method includes a pooled estimation\nstep using all outcomes to gain efficiency, and a subsequent calibration step\nusing only the outcome of interest to correct both types of biases. We show\nthat when the pooled estimator has a low estimation error and a sparse\nagainst-subspace bias, the calibrated estimator can achieve a lower estimation\nerror than that when using only the single outcome of interest. An inference\nprocedure for the calibrated estimator is also provided. Simulations and a real\ndata analysis are conducted to justify the superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 01:14:33 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Liang", "Muxuan", ""], ["Zhong", "Xiang", ""], ["Park", "Jaeyoung", ""]]}, {"id": "2011.05601", "submitter": "Katherine Tsai", "authors": "Katherine Tsai, Mladen Kolar, Oluwasanmi Koyejo", "title": "A Nonconvex Framework for Structured Dynamic Covariance Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible yet interpretable model for high-dimensional data with\ntime-varying second order statistics, motivated and applied to functional\nneuroimaging data. Motivated by the neuroscience literature, we factorize the\ncovariances into sparse spatial and smooth temporal components. While this\nfactorization results in both parsimony and domain interpretability, the\nresulting estimation problem is nonconvex. To this end, we design a two-stage\noptimization scheme with a carefully tailored spectral initialization, combined\nwith iteratively refined alternating projected gradient descent. We prove a\nlinear convergence rate up to a nontrivial statistical error for the proposed\ndescent scheme and establish sample complexity guarantees for the estimator. We\nfurther quantify the statistical error for the multivariate Gaussian case.\nEmpirical results using simulated and real brain imaging data illustrate that\nour approach outperforms existing baselines.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 07:09:44 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 19:42:15 GMT"}, {"version": "v3", "created": "Sun, 18 Jul 2021 01:46:08 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Tsai", "Katherine", ""], ["Kolar", "Mladen", ""], ["Koyejo", "Oluwasanmi", ""]]}, {"id": "2011.05794", "submitter": "Daniel Andr\\'es D\\'iaz Pach\\'on", "authors": "Daniel Andr\\'es D\\'iaz-Pach\\'on and Juan Pablo S\\'aenz and J. Sunil\n  Rao and Jean-Eudes Dazard", "title": "Mode hunting through active information", "comments": "12 pages", "journal-ref": "Applied Stochastic Models in Business and Industry (35)2, pp.\n  376-393, 2019", "doi": "10.1002/asmb.2430", "report-no": null, "categories": "physics.data-an cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new method to find modes based on active information. We develop\nan algorithm that, when applied to the whole space, will say whether there are\nany modes present \\textit{and} where they are; this algorithm will reduce the\ndimensionality without resorting to Principal Components; and more importantly,\npopulation-wise, will not detect modes when they are not present.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 01:55:29 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["D\u00edaz-Pach\u00f3n", "Daniel Andr\u00e9s", ""], ["S\u00e1enz", "Juan Pablo", ""], ["Rao", "J. Sunil", ""], ["Dazard", "Jean-Eudes", ""]]}, {"id": "2011.05826", "submitter": "Eli Ben-Michael", "authors": "Eli Ben-Michael, Avi Feller, Elizabeth A. Stuart", "title": "A trial emulation approach for policy evaluations with group-level\n  longitudinal data", "comments": "Forthcoming at Epidemiology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To limit the spread of the novel coronavirus, governments across the world\nimplemented extraordinary physical distancing policies, such as stay-at-home\norders, and numerous studies aim to estimate their effects. Many statistical\nand econometric methods, such as difference-in-differences, leverage repeated\nmeasurements and variation in timing to estimate policy effects, including in\nthe COVID-19 context. While these methods are less common in epidemiology,\nepidemiologic researchers are well accustomed to handling similar complexities\nin studies of individual-level interventions. \"Target trial emulation\"\nemphasizes the need to carefully design a non-experimental study in terms of\ninclusion and exclusion criteria, covariates, exposure definition, and outcome\nmeasurement -- and the timing of those variables. We argue that policy\nevaluations using group-level longitudinal (\"panel\") data need to take a\nsimilar careful approach to study design, which we refer to as \"policy trial\nemulation.\" This is especially important when intervention timing varies across\njurisdictions; the main idea is to construct target trials separately for each\n\"treatment cohort\" (states that implement the policy at the same time) and then\naggregate. We present a stylized analysis of the impact of state-level\nstay-at-home orders on total coronavirus cases. We argue that estimates from\npanel methods -- with the right data and careful modeling and diagnostics --\ncan help add to our understanding of many policies, though doing so is often\nchallenging.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 14:44:59 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Ben-Michael", "Eli", ""], ["Feller", "Avi", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "2011.05877", "submitter": "Yanbo Xu", "authors": "Yanbo Xu, Divyat Mahajan, Liz Manrao, Amit Sharma and Emre Kiciman", "title": "Split-Treatment Analysis to Rank Heterogeneous Causal Effects for\n  Prospective Interventions", "comments": "To be published in WSDM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For many kinds of interventions, such as a new advertisement, marketing\nintervention, or feature recommendation, it is important to target a specific\nsubset of people for maximizing its benefits at minimum cost or potential harm.\nHowever, a key challenge is that no data is available about the effect of such\na prospective intervention since it has not been deployed yet. In this work, we\npropose a split-treatment analysis that ranks the individuals most likely to be\npositively affected by a prospective intervention using past observational\ndata. Unlike standard causal inference methods, the split-treatment method does\nnot need any observations of the target treatments themselves. Instead it\nrelies on observations of a proxy treatment that is caused by the target\ntreatment. Under reasonable assumptions, we show that the ranking of\nheterogeneous causal effect based on the proxy treatment is the same as the\nranking based on the target treatment's effect. In the absence of any\ninterventional data for cross-validation, Split-Treatment uses sensitivity\nanalyses for unobserved confounding to select model parameters. We apply\nSplit-Treatment to both a simulated data and a large-scale, real-world\ntargeting task and validate our discovered rankings via a randomized experiment\nfor the latter.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 16:17:29 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Xu", "Yanbo", ""], ["Mahajan", "Divyat", ""], ["Manrao", "Liz", ""], ["Sharma", "Amit", ""], ["Kiciman", "Emre", ""]]}, {"id": "2011.05951", "submitter": "Gen Li", "authors": "Gen Li, Yan Li, Kun Chen", "title": "It's All Relative: New Regression Paradigm for Microbiome Compositional\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microbiome data are complex in nature, involving high dimensionality,\ncompositionally, zero inflation, and taxonomic hierarchy. Compositional data\nreside in a simplex that does not admit the standard Euclidean geometry. Most\nexisting compositional regression methods rely on transformations that are\ninadequate or even inappropriate in modeling data with excessive zeros and\ntaxonomic structure. We develop a novel relative-shift regression framework\nthat directly uses compositions as predictors. The new framework provides a\nparadigm shift for compositional regression and offers a superior biological\ninterpretation. New equi-sparsity and taxonomy-guided regularization methods\nand an efficient smoothing proximal gradient algorithm are developed to\nfacilitate feature aggregation and dimension reduction in regression. As a\nresult, the framework can automatically identify clinically relevant microbes\neven if they are important at different taxonomic levels. A unified\nfinite-sample prediction error bound is developed for the proposed regularized\nestimators. We demonstrate the efficacy of the proposed methods in extensive\nsimulation studies. The application to a preterm infant study reveals novel\ninsights of association between the gut microbiome and neurodevelopment.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 18:15:21 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Li", "Gen", ""], ["Li", "Yan", ""], ["Chen", "Kun", ""]]}, {"id": "2011.05988", "submitter": "HaiYing Wang", "authors": "HaiYing Wang and Jae Kwang Kim", "title": "Maximum sampled conditional likelihood for informative subsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subsampling is a computationally effective approach to extract information\nfrom massive data sets when computing resources are limited. After a subsample\nis taken from the full data, most available methods use an inverse probability\nweighted objective function to estimate the model parameters. This type of\nweighted estimator does not fully utilize information in the selected\nsubsample. In this paper, we propose to use the maximum sampled conditional\nlikelihood estimator (MSCLE) based on the sampled data. We established the\nasymptotic normality of the MSCLE and prove that its asymptotic variance\ncovariance matrix is the smallest among a class of asymptotically unbiased\nestimators, including the inverse probability weighted estimator. We further\ndiscuss the asymptotic results with the L-optimal subsampling probabilities and\nillustrate the estimation procedure with generalized linear models. Numerical\nexperiments are provided to evaluate the practical performance of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 16:01:17 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 01:38:16 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "HaiYing", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "2011.06031", "submitter": "Jiachen Chen", "authors": "Jiachen Chen, Xin Zhou, Fan Li, Donna Spiegelman", "title": "swdpwr: A SAS Macro and An R Package for Power Calculation in Stepped\n  Wedge Cluster Randomized Trials", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Background and objective: The stepped wedge cluster randomized trial is a\nstudy design increasingly used for public health intervention evaluations. Most\nprevious literature focuses on power calculations for this particular type of\ncluster randomized trials for continuous outcomes, along with an approximation\nto this approach for binary outcomes. Although not accurate for binary\noutcomes, it has been widely used. To improve the approximation for binary\noutcomes, two new methods for stepped wedge designs (SWDs) of binary outcomes\nhave recently been published. However, these new methods have not been\nimplemented in publicly available software. The objective of this paper is to\npresent power calculation software for SWDs in various settings for both\ncontinuous and binary outcomes.\n  Methods: We have developed a SAS macro %swdpwr and an R package swdpwr for\npower calculation in SWDs. Different scenarios including cross-sectional and\ncohort designs, binary and continuous outcomes, marginal and conditional\nmodels, three link functions, with and without time effects are accommodated in\nthis software.\n  Results: swdpwr provides an efficient tool to support investigators in the\ndesign and analysis of stepped wedge cluster randomized trails. swdpwr\naddresses the implementation gap between newly proposed methodology and their\napplication to obtain more accurate power calculations in SWDs.\n  Conclusions: This user-friendly software makes the new methods more\naccessible and incorporates as many variations as currently available, which\nwere not supported in other related packages. swdpwr is implemented under two\nplatforms: SAS and R, satisfying the needs of investigators from various\nbackgrounds.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 19:31:40 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 05:26:48 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chen", "Jiachen", ""], ["Zhou", "Xin", ""], ["Li", "Fan", ""], ["Spiegelman", "Donna", ""]]}, {"id": "2011.06032", "submitter": "Jacques Balayla", "authors": "Jacques Balayla", "title": "On the Formalism of The Screening Paradox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayes' Theorem imposes inevitable limitations on the accuracy of screening\ntests by tying the test's predictive value to the disease prevalence. The\naforementioned limitation is independent of the adequacy and make-up of the\ntest and thus implies inherent Bayesian limitations to the screening process\nitself. As per the WHO's $Wilson-Jungner$ criteria, one of the prerequisite\nsteps before undertaking screening is to ensure that a treatment for the\ncondition screened exists. However, in so doing, a paradox, henceforth termed\nthe screening paradox, ensues. If a disease process is screened for and\nsubsequently treated, its prevalence would drop in the population, which as per\nBayes' theorem, would make the tests' predictive value drop in return. Put\nanother way, a very powerful screening test would, by performing and succeeding\nat the very task it was developed to do, paradoxically reduce its ability to\ncorrectly identify individuals with the disease it screens for in the future.\nWhere $J$ is Youden's statistic (sensitivity [$a$] + specificity [$b$] - 1),\nand $\\phi$ is the prevalence, the ratio of positive predictive values at\nsubsequent time $k$, $\\rho(\\phi_{k})$, over the original $\\rho(\\phi_{0})$ at\n$t_0$ is given by:\n  $\\zeta(\\phi_{0},k) = \\frac{\\rho(\\phi_{k})}{\\rho(\\phi_{0})}\n=\\frac{\\phi_k(1-b)+J\\phi_0\\phi_k}{\\phi_0(1-b)+J\\phi_0\\phi_k}$\n  In this manuscript, we explore the mathematical model which formalizes said\nscreening paradox and explore its implications for population level screening\nprograms. In particular, we define the number of positive test iterations (PTI)\nneeded to reverse the effects of the paradox as follows:\n  $n_{i\\phi_e}=\\left\\lceil\\frac{ln\\left[\\frac{\\omega\\phi_e\\phi_k-\\omega\\phi_e}{\\omega\\phi_e\\phi_k-\\phi_k}\\right]}{2ln\\omega}\\right\\rceil$\n  where $\\omega$ is the square root of the positive likelihood ratio (LR+).\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 19:35:16 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Balayla", "Jacques", ""]]}, {"id": "2011.06042", "submitter": "Radoslav Paulen", "authors": "Anwesh Reddy Gottu Mukkula, Michal Mate\\'a\\v{s}, Miroslav Fikar,\n  Radoslav Paulen", "title": "Robust multi-stage model-based design of optimal experiments for\n  nonlinear estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.SY eess.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study approaches to robust model-based design of experiments in the\ncontext of maximum-likelihood estimation. These approaches provide\nrobustification of model-based methodologies for the design of optimal\nexperiments by accounting for the effect of the parametric uncertainty. We\nstudy the problem of robust optimal design of experiments in the framework of\nnonlinear least-squares parameter estimation using linearized confidence\nregions. We investigate several well-known robustification frameworks in this\nrespect and propose a novel methodology based on multi-stage robust\noptimization. The proposed methodology aims at problems, where the experiments\nare designed sequentially with a possibility of re-estimation in-between the\nexperiments. The multi-stage formalism aids in identifying experiments that are\nbetter conducted in the early phase of experimentation, where parameter\nknowledge is poor. We demonstrate the findings and effectiveness of the\nproposed methodology using four case studies of varying complexity.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 19:50:31 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Mukkula", "Anwesh Reddy Gottu", ""], ["Mate\u00e1\u0161", "Michal", ""], ["Fikar", "Miroslav", ""], ["Paulen", "Radoslav", ""]]}, {"id": "2011.06045", "submitter": "Konstantinos Perrakis", "authors": "Konstantinos Perrakis, Dimitris Karlis, Mario Cools, Davy Janssens", "title": "Bayesian inference for transportation origin-destination matrices: the\n  Poisson-inverse Gaussian and other Poisson mixtures", "comments": null, "journal-ref": null, "doi": "10.1111/rssa.12057", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present Poisson mixture approaches for origin-destination\n(OD) modeling in transportation analysis. We introduce covariate-based models\nwhich incorporate different transport modeling phases and also allow for direct\nprobabilistic inference on link traffic based on Bayesian predictions. Emphasis\nis placed on the Poisson-inverse Gaussian as an alternative to the\ncommonly-used Poisson-gamma and Poisson-lognormal models. We present a first\nfull Bayesian formulation and demonstrate that the Poisson-inverse Gaussian is\nparticularly suited for OD analysis due to desirable marginal and hierarchical\nproperties. In addition, the integrated nested Laplace approximation (INLA) is\nconsidered as an alternative to Markov chain Monte Carlo and the two\nmethodologies are compared under specific modeling assumptions. The case study\nis based on 2001 Belgian census data and focuses on a large,\nsparsely-distributed OD matrix containing trip information for 308 Flemish\nmunicipalities.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 19:57:00 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Perrakis", "Konstantinos", ""], ["Karlis", "Dimitris", ""], ["Cools", "Mario", ""], ["Janssens", "Davy", ""]]}, {"id": "2011.06061", "submitter": "James Long", "authors": "James P. Long and Ehsan Irajizad and James D. Doecke and Kim-Anh Do\n  and Min Jin Ha", "title": "A Framework for Mediation Analysis with Multiple Exposures, Multivariate\n  Mediators, and Non-Linear Response Models", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis seeks to identify and quantify the paths by which an\nexposure affects an outcome. Intermediate variables which are effected by the\nexposure and which effect the outcome are known as mediators. There exists\nextensive work on mediation analysis in the context of models with a single\nmediator and continuous and binary outcomes. However these methods are often\nnot suitable for multi-omic data that include highly interconnected variables\nmeasuring biological mechanisms and various types of outcome variables such as\ncensored survival responses. In this article, we develop a general framework\nfor causal mediation analysis with multiple exposures, multivariate mediators,\nand continuous, binary, and survival responses. We estimate mediation effects\non several scales including the mean difference, odds ratio, and restricted\nmean scale as appropriate for various outcome models. Our estimation method\navoids imposing constraints on model parameters such as the rare disease\nassumption while accommodating continuous exposures. We evaluate the framework\nand compare it to other methods in extensive simulation studies by assessing\nbias, type I error and power at a range of sample sizes, disease prevalences,\nand number of false mediators. Using Kidney Renal Clear Cell Carcinoma data\nfrom The Cancer Genome Atlas, we identify proteins which mediate the effect of\nmetabolic gene expression on survival. Software for implementing this unified\nframework is made available in an R package\n(https://github.com/longjp/mediateR).\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 20:42:17 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Long", "James P.", ""], ["Irajizad", "Ehsan", ""], ["Doecke", "James D.", ""], ["Do", "Kim-Anh", ""], ["Ha", "Min Jin", ""]]}, {"id": "2011.06080", "submitter": "Anna Malinovskaya", "authors": "Anna Malinovskaya, Philipp Otto and Torben Peters", "title": "Statistical learning for change point and anomaly detection in graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems which can be represented in the form of static and dynamic\ngraphs arise in different fields, e.g. communication, engineering and industry.\nOne of the interesting problems in analysing dynamic network structures is to\nmonitor changes in their development. Statistical learning, which encompasses\nboth methods based on artificial intelligence and traditional statistics, can\nbe used to progress in this research area. However, the majority of approaches\napply only one or the other framework. In this paper, we discuss the\npossibility of bringing together both disciplines in order to create enhanced\nnetwork monitoring procedures focussing on the example of combining statistical\nprocess control and deep learning algorithms. Together with the presentation of\nchange point and anomaly detection in network data, we propose to monitor the\nresponse times of ambulance services, applying jointly the control chart for\nquantile function values and a graph convolutional network.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 17:15:53 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Malinovskaya", "Anna", ""], ["Otto", "Philipp", ""], ["Peters", "Torben", ""]]}, {"id": "2011.06122", "submitter": "Michael Newton", "authors": "Peng Yu, Spencer S. Ericksen, Anthony Gitter, and Michael A. Newton", "title": "Bayes Optimal Informer Sets for Early-Stage Drug Discovery", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important experimental design problem in early-stage drug discovery is how\nto prioritize available compounds for testing when very little is known about\nthe target protein. Informer based ranking (IBR) methods address the\nprioritization problem when the compounds have provided bioactivity data on\nother potentially relevant targets. An IBR method selects an informer set of\ncompounds, and then prioritizes the remaining compounds on the basis of new\nbioactivity experiments performed with the informer set on the target. We\nformalize the problem as a two-stage decision problem and introduce the Bayes\nOptimal Informer SEt (BOISE) method for its solution. BOISE leverages a\nflexible model of the initial bioactivity data, a relevant loss function, and\neffective computational schemes to resolve the two-step design problem. We\nevaluate BOISE and compare it to other IBR strategies in two retrospective\nstudies, one on protein-kinase inhibition and the other on anti-cancer drug\nsensitivity. In both empirical settings BOISE exhibits better predictive\nperformance than available methods. It also behaves well with missing data,\nwhere methods that use matrix completion show worse predictive performance. We\nprovide an R implementation of BOISE at\nhttps://github.com/wiscstatman/esdd/BOISE\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 23:45:53 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Yu", "Peng", ""], ["Ericksen", "Spencer S.", ""], ["Gitter", "Anthony", ""], ["Newton", "Michael A.", ""]]}, {"id": "2011.06127", "submitter": "Hao Chen", "authors": "Hoseung Song and Hao Chen", "title": "Generalized Kernel Two-Sample Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel two-sample tests have been widely used for multivariate data in\ntesting equal distribution. However, existing tests based on mapping\ndistributions into a reproducing kernel Hilbert space do not work well for some\ncommon alternatives when the dimension of the data is moderate to high due to\nthe curse of dimensionality. We propose a new test statistic that makes use of\nan informative pattern under moderate and high dimensions and achieves\nsubstantial power improvements over existing kernel two-sample tests for a wide\nrange of alternatives. We also propose alternative testing procedures that\nmaintain high power with low computational cost, offering easy off-the-shelf\ntools for large datasets. We illustrate these new approaches through an\nanalysis of the New York City taxi data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 00:02:47 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Song", "Hoseung", ""], ["Chen", "Hao", ""]]}, {"id": "2011.06158", "submitter": "Jiafeng Chen", "authors": "Jiafeng Chen and Daniel L. Chen and Greg Lewis", "title": "Mostly Harmless Machine Learning: Learning Optimal Instruments in Linear\n  IV Models", "comments": "NeurIPS 2020 Workshop on Machine Learning for Economic Policy", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We offer straightforward theoretical results that justify incorporating\nmachine learning in the standard linear instrumental variable setting. The key\nidea is to use machine learning, combined with sample-splitting, to predict the\ntreatment variable from the instrument and any exogenous covariates, and then\nuse this predicted treatment and the covariates as technical instruments to\nrecover the coefficients in the second-stage. This allows the researcher to\nextract non-linear co-variation between the treatment and instrument that may\ndramatically improve estimation precision and robustness by boosting instrument\nstrength. Importantly, we constrain the machine-learned predictions to be\nlinear in the exogenous covariates, thus avoiding spurious identification\narising from non-linear relationships between the treatment and the covariates.\nWe show that this approach delivers consistent and asymptotically normal\nestimates under weak conditions and that it may be adapted to be\nsemiparametrically efficient (Chamberlain, 1992). Our method preserves standard\nintuitions and interpretations of linear instrumental variable methods,\nincluding under weak identification, and provides a simple, user-friendly\nupgrade to the applied economics toolbox. We illustrate our method with an\nexample in law and criminal justice, examining the causal effect of appellate\ncourt reversals on district court sentencing decisions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 01:55:11 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 04:26:23 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 18:26:37 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Jiafeng", ""], ["Chen", "Daniel L.", ""], ["Lewis", "Greg", ""]]}, {"id": "2011.06172", "submitter": "Han Du", "authors": "Han Du and Ge Jiang and Zijun Ke", "title": "A Bootstrap Based Between-Study Heterogeneity Test in Meta-Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis combines pertinent information from existing studies to provide\nan overall estimate of population parameters/effect sizes, as well as to\nquantify and explain the differences between studies. However, testing the\nbetween-study heterogeneity is one of the most troublesome topics in\nmeta-analysis research. Additionally, no methods have been proposed to test\nwhether the size of the heterogeneity is larger than a specific level. The\nexisting methods, such as the Q test and likelihood ratio (LR) tests, are\ncriticized for their failure to control the Type I error rate and/or failure to\nattain enough statistical power. Although better reference distribution\napproximations have been proposed in the literature, the expression is\ncomplicated and the application is limited. In this article, we propose\nbootstrap based heterogeneity tests combining the restricted maximum likelihood\n(REML) ratio test or Q test with bootstrap procedures, denoted as B-REML-LRT\nand B-Q respectively. Simulation studies were conducted to examine and compare\nthe performance of the proposed methods with the regular LR tests, the regular\nQ test, and the improved Q test in both the random-effects meta-analysis and\nmixed-effects meta-analysis. Based on the results of Type I error rates and\nstatistical power, B-Q is recommended. An R package \\mathtt{boot.heterogeneity}\nis provided to facilitate the implementation of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 02:44:00 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Du", "Han", ""], ["Jiang", "Ge", ""], ["Ke", "Zijun", ""]]}, {"id": "2011.06241", "submitter": "Jiaqi Li", "authors": "Liya Fu, Jiaqi Li, You-Gan Wang", "title": "Robust approach for variable selection with high dimensional Logitudinal\n  data analysis", "comments": "32 pages, 7 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new robust smooth-threshold estimating equation to\nselect important variables and automatically estimate parameters for high\ndimensional longitudinal data. A novel working correlation matrix is proposed\nto capture correlations within the same subject. The proposed procedure works\nwell when the number of covariates p increases as the number of subjects n\nincreases. The proposed estimates are competitive with the estimates obtained\nwith the true correlation structure, especially when the data are contaminated.\nMoreover, the proposed method is robust against outliers in the response\nvariables and/or covariates. Furthermore, the oracle properties for robust\nsmooth-threshold estimating equations under \"large n, diverging p\" are\nestablished under some regularity conditions. Extensive simulation studies and\na yeast cell cycle data are used to evaluate the performance of the proposed\nmethod, and results show that our proposed method is competitive with existing\nrobust variable selection procedures.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 07:41:57 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 10:07:51 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Fu", "Liya", ""], ["Li", "Jiaqi", ""], ["Wang", "You-Gan", ""]]}, {"id": "2011.06333", "submitter": "Weichi Wu", "authors": "Subhra Sankar Dhar, Weichi Wu", "title": "Shift identification in time varying regression quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates whether time-varying quantile regression curves are\nthe same up to the horizontal shift or not. The errors and the covariates\ninvolved in the regression model are allowed to be locally stationary. We\nformalise this issue in a corresponding non-parametric hypothesis testing\nproblem, and develop a integrated-squared-norm based test (SIT) as well as a\nsimultaneous confidence band (SCB) approach. The asymptotic properties of SIT\nand SCB under null and local alternatives are derived. We then propose valid\nwild bootstrap algorithms to implement SIT and SCB. Furthermore, the usefulness\nof the proposed methodology is illustrated via analysing simulated and real\ndata related to Covid-19 outbreak and climate science.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 12:08:19 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Dhar", "Subhra Sankar", ""], ["Wu", "Weichi", ""]]}, {"id": "2011.06334", "submitter": "Antonio Remiro-Az\\'ocar Mr.", "authors": "Antonio Remiro-Az\\'ocar, Anna Heath, Gianluca Baio", "title": "Conflating marginal and conditional treatment effects: Comments on\n  'Assessing the performance of population adjustment methods for anchored\n  indirect comparisons: A simulation study'", "comments": "6 pages, submitted to Statistics in Medicine. Response to `Assessing\n  the performance of population adjustment methods for anchored indirect\n  comparisons: A simulation study' by Phillippo, Dias, Ades and Welton,\n  published in Statistics in Medicine (2020). Updated after Ph.D. proposal\n  defense/transfer viva comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this commentary, we highlight the importance of: (1) carefully considering\nand clarifying whether a marginal or conditional treatment effect is of\ninterest in a population-adjusted indirect treatment comparison; and (2)\ndeveloping distinct methodologies for estimating the different measures of\neffect. The appropriateness of each methodology depends on the preferred target\nof inference.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 12:08:28 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 17:44:54 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Remiro-Az\u00f3car", "Antonio", ""], ["Heath", "Anna", ""], ["Baio", "Gianluca", ""]]}, {"id": "2011.06416", "submitter": "Sami Stouli", "authors": "Richard Spady, Sami Stouli", "title": "Gaussian Transforms Modeling and the Estimation of Distributional\n  Regression Functions", "comments": "44 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conditional distribution functions are important statistical objects for the\nanalysis of a wide class of problems in econometrics and statistics. We propose\nflexible Gaussian representations for conditional distribution functions and\ngive a concave likelihood formulation for their global estimation. We obtain\nsolutions that satisfy the monotonicity property of conditional distribution\nfunctions, including under general misspecification and in finite samples. A\nLasso-type penalized version of the corresponding maximum likelihood estimator\nis given that expands the scope of our estimation analysis to models with\nsparsity. Inference and estimation results for conditional distribution,\nquantile and density functions implied by our representations are provided and\nillustrated with an empirical example and simulations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 14:34:57 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Spady", "Richard", ""], ["Stouli", "Sami", ""]]}, {"id": "2011.06436", "submitter": "Liliana Forzani", "authors": "R. Dennis Cook, Liliana Forzani", "title": "Fundamentals of path analysis in the social sciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.soc-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a recent series of diametrically opposed articles on the\nrelative value of statistical methods for the analysis of path diagrams in the\nsocial sciences, we discuss from a primarily theoretical perspective selected\nfundamental aspects of path modeling and analysis based on a common re\nreflexive setting. Since there is a paucity of technical support evident in the\ndebate, our aim is to connect it to mainline statistics literature and to\naddress selected foundational issues that may help move the discourse. We do\nnot intend to advocate for or against a particular method or analysis\nphilosophy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 15:10:31 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Cook", "R. Dennis", ""], ["Forzani", "Liliana", ""]]}, {"id": "2011.06444", "submitter": "Mario Beraha", "authors": "Mario Beraha, Raffaele Argiento, Jesper M{\\o}ller, Alessandra\n  Guglielmi", "title": "MCMC computations for Bayesian mixture models using repulsive point\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repulsive mixture models have recently gained popularity for Bayesian cluster\ndetection. Compared to more traditional mixture models, repulsive mixture\nmodels produce a smaller number of well separated clusters. The most commonly\nused methods for posterior inference either require to fix a priori the number\nof components or are based on reversible jump MCMC computation. We present a\ngeneral framework for mixture models, when the prior of the `cluster centres'\nis a finite repulsive point process depending on a hyperparameter, specified by\na density which may depend on an intractable normalizing constant. By\ninvestigating the posterior characterization of this class of mixture models,\nwe derive a MCMC algorithm which avoids the well-known difficulties associated\nto reversible jump MCMC computation. In particular, we use an ancillary\nvariable method, which eliminates the problem of having intractable normalizing\nconstants in the Hastings ratio. The ancillary variable method relies on a\nperfect simulation algorithm, and we demonstrate this is fast because the\nnumber of components is typically small. In several simulation studies and an\napplication on sociological data, we illustrate the advantage of our new\nmethodology over existing methods, and we compare the use of a determinantal or\na repulsive Gibbs point process prior model.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 15:39:01 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 10:03:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Beraha", "Mario", ""], ["Argiento", "Raffaele", ""], ["M\u00f8ller", "Jesper", ""], ["Guglielmi", "Alessandra", ""]]}, {"id": "2011.06557", "submitter": "Hayden Helm", "authors": "Hayden S. Helm, Ronak D. Mehta, Brandon Duderstadt, Weiwei Yang,\n  Christoper M. White, Ali Geisa, Joshua T. Vogelstein, Carey E. Priebe", "title": "A partition-based similarity for classification distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herein we define a measure of similarity between classification distributions\nthat is both principled from the perspective of statistical pattern recognition\nand useful from the perspective of machine learning practitioners. In\nparticular, we propose a novel similarity on classification distributions,\ndubbed task similarity, that quantifies how an optimally-transformed optimal\nrepresentation for a source distribution performs when applied to inference\nrelated to a target distribution. The definition of task similarity allows for\nnatural definitions of adversarial and orthogonal distributions. We highlight\nlimiting properties of representations induced by (universally) consistent\ndecision rules and demonstrate in simulation that an empirical estimate of task\nsimilarity is a function of the decision rule deployed for inference. We\ndemonstrate that for a given target distribution, both transfer efficiency and\nsemantic similarity of candidate source distributions correlate with empirical\ntask similarity.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 18:21:11 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Helm", "Hayden S.", ""], ["Mehta", "Ronak D.", ""], ["Duderstadt", "Brandon", ""], ["Yang", "Weiwei", ""], ["White", "Christoper M.", ""], ["Geisa", "Ali", ""], ["Vogelstein", "Joshua T.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2011.06629", "submitter": "Tommaso Rigon", "authors": "Alessandro Zito and Tommaso Rigon and Otso Ovaskainen and David Dunson", "title": "Bayesian nonparametric modelling of sequential discoveries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We aim at modelling the appearance of distinct tags in a sequence of labelled\nobjects. Common examples of this type of data include words in a corpus or\ndistinct species in a sample. These sequential discoveries are often summarised\nvia accumulation curves, which count the number of distinct entities observed\nin an increasingly large set of objects. We propose a novel Bayesian\nnonparametric method for species sampling modelling by directly specifying the\nprobability of a new discovery, therefore allowing for flexible specifications.\nThe asymptotic behavior and finite sample properties of such an approach are\nextensively studied. Interestingly, our enlarged class of sequential processes\nincludes highly tractable special cases. We present a subclass of models\ncharacterized by appealing theoretical and computational properties. Moreover,\ndue to strong connections with logistic regression models, the latter subclass\ncan naturally account for covariates. We finally test our proposal on both\nsynthetic and real data, with special emphasis on a large fungal biodiversity\nstudy in Finland.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 19:29:16 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Zito", "Alessandro", ""], ["Rigon", "Tommaso", ""], ["Ovaskainen", "Otso", ""], ["Dunson", "David", ""]]}, {"id": "2011.06663", "submitter": "Xu Shi", "authors": "Guanghao Zhang, Lauren J. Beesley, Bhramar Mukherjee, Xu Shi", "title": "Patient Recruitment Using Electronic Health Records Under Selection\n  Bias: a Two-phase Sampling Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electronic health records (EHRs) are increasingly recognized as a\ncost-effective resource for patient recruitment for health research. Suppose we\nwant to conduct a study to estimate the mean or mean difference of an expensive\noutcome in a target population. Inexpensive auxiliary covariates predictive of\nthe outcome may often be available in patients' health records, presenting an\nopportunity to recruit patients selectively and estimate the mean outcome\nefficiently. In this paper, we propose a two-phase sampling design that\nleverages available information on auxiliary covariates in EHR data. A key\nchallenge in using EHR data for multi-phase sampling is the potential selection\nbias, because EHR data are not necessarily representative of the target\npopulation. Extending existing literature on two-phase sampling designs, we\nderive an optimal two-phase sampling method that improves efficiency over\nrandom sampling while accounting for the potential selection bias in EHR data.\nWe demonstrate the efficiency gain of our sampling design by conducting finite\nsample simulation studies and an application study based on data from the\nMichigan Genomics Initiative.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 21:39:27 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Zhang", "Guanghao", ""], ["Beesley", "Lauren J.", ""], ["Mukherjee", "Bhramar", ""], ["Shi", "Xu", ""]]}, {"id": "2011.06682", "submitter": "Sanjeena Subedi", "authors": "Yuan Fang and Sanjeena Subedi", "title": "Clustering microbiome data using mixtures of logistic normal multinomial\n  models", "comments": "51 pages, 11 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discrete data such as counts of microbiome taxa resulting from\nnext-generation sequencing are routinely encountered in bioinformatics. Taxa\ncount data in microbiome studies are typically high-dimensional,\nover-dispersed, and can only reveal relative abundance therefore being treated\nas compositional. Analyzing compositional data presents many challenges because\nthey are restricted on a simplex. In a logistic normal multinomial model, the\nrelative abundance is mapped from a simplex to a latent variable that exists on\nthe real Euclidean space using the additive log-ratio transformation. While a\nlogistic normal multinomial approach brings in flexibility for modeling the\ndata, it comes with a heavy computational cost as the parameter estimation\ntypically relies on Bayesian techniques. In this paper, we develop a novel\nmixture of logistic normal multinomial models for clustering microbiome data.\nAdditionally, we utilize an efficient framework for parameter estimation using\nvariational Gaussian approximations (VGA). Adopting a variational Gaussian\napproximation for the posterior of the latent variable reduces the\ncomputational overhead substantially. The proposed method is illustrated on\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 23:16:24 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Fang", "Yuan", ""], ["Subedi", "Sanjeena", ""]]}, {"id": "2011.06695", "submitter": "Tymon Sloczynski", "authors": "Tymon S{\\l}oczy\\'nski", "title": "When Should We (Not) Interpret Linear IV Estimands as LATE?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper I revisit the interpretation of the linear instrumental\nvariables (IV) estimand as a weighted average of conditional local average\ntreatment effects (LATEs). I focus on a practically relevant situation in which\nadditional covariates are required for identification while the reduced-form\nand first-stage regressions implicitly restrict the effects of the instrument\nto be homogeneous, and are thus possibly misspecified. I show that the weights\non some conditional LATEs are negative and the IV estimand is no longer\ninterpretable as a causal effect under a weaker version of monotonicity, i.e.\nwhen there are compliers but no defiers at some covariate values and defiers\nbut no compliers elsewhere. The problem of negative weights disappears in the\noveridentified specification of Angrist and Imbens (1995) and in an alternative\nmethod, termed \"reordered IV,\" that I also develop. Even if all weights are\npositive, the IV estimand in the just identified specification is not\ninterpretable as the unconditional LATE parameter unless the groups with\ndifferent values of the instrument are roughly equal sized. I illustrate my\nfindings in an application to causal effects of college education using the\ncollege proximity instrument. The benchmark estimates suggest that college\nattendance yields earnings gains of about 60 log points, which is well outside\nthe range of estimates in the recent literature. I demonstrate that this result\nis driven by the existence of defiers and the presence of negative weights.\nCorrected estimates indicate that attending college causes earnings to be\nroughly 20% higher.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 00:04:30 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 18:36:43 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 15:56:11 GMT"}, {"version": "v4", "created": "Fri, 25 Jun 2021 17:48:48 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["S\u0142oczy\u0144ski", "Tymon", ""]]}, {"id": "2011.06706", "submitter": "Youngjoo Cho", "authors": "Youngjoo Cho, Annette M. Molinaro, Chen Hu, and Robert L. Strawderman", "title": "Regression Trees for Cumulative Incidence Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The use of cumulative incidence functions for characterizing the risk of one\ntype of event in the presence of others has become increasingly popular over\nthe past decade. The problems of modeling, estimation and inference have been\ntreated using parametric, nonparametric and semi-parametric methods. Efforts to\ndevelop suitable extensions of machine learning methods, such as regression\ntrees and related ensemble methods, have begun only recently. In this paper, we\ndevelop a novel approach to building regression trees for estimating cumulative\nincidence curves in a competing risks setting. The proposed methods employ\naugmented estimators of the Brier score risk as the primary basis for building\nand pruning trees. The proposed methods are easily implemented using the R\nstatistical software package. Simulation studies demonstrate the utility of our\napproach in the competing risks setting. Data from the Radiation Therapy\nOncology Group (trial 9410) is used to illustrate these new methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 00:37:12 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Cho", "Youngjoo", ""], ["Molinaro", "Annette M.", ""], ["Hu", "Chen", ""], ["Strawderman", "Robert L.", ""]]}, {"id": "2011.06765", "submitter": "Yisha Yao", "authors": "Yisha Yao and Cun-Hui Zhang", "title": "Adaptive Estimation In High-Dimensional Additive Models With\n  Multi-Resolution Group Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In additive models with many nonparametric components, a number of\nregularized estimators have been proposed and proven to attain various error\nbounds under different combinations of sparsity and fixed smoothness\nconditions. Some of these error bounds match minimax rates in the corresponding\nsettings. Some of the rate minimax methods are non-convex and computationally\ncostly. From these perspectives, the existing solutions to the high-dimensional\nadditive nonparametric regression problem are fragmented. In this paper, we\npropose a multi-resolution group Lasso (MR-GL) method in a unified approach to\nsimultaneously achieve or improve existing error bounds and provide new ones\nwithout the knowledge of the level of sparsity or the degree of smoothness of\nthe unknown functions. Such adaptive convergence rates are established when a\nprediction factor can be treated as a constant. Furthermore, we prove that the\nprediction factor, which can be bounded in terms of a restricted eigenvalue or\na compatibility coefficient, can be indeed treated as a constant for random\ndesigns under a nearly optimal sample size condition.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 05:21:08 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Yao", "Yisha", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "2011.06879", "submitter": "Jacob Leander", "authors": "Jacob Leander, Joachim Almquist, Anna Johnning, Julia Larsson, Mats\n  Jirstrand", "title": "NLMEModeling: A Wolfram Mathematica Package for Nonlinear Mixed Effects\n  Modeling of Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear mixed effects modeling is a powerful tool when analyzing data from\nseveral entities in an experiment. In this paper, we present NLMEModeling, a\npackage for mixed effects modeling in Wolfram Mathematica. NLMEModeling\nsupports mixed effects modeling of dynamical systems where the underlying\ndynamics are described by either ordinary or stochastic differential equations\ncombined with a flexible observation error model. Moreover, NLMEModeling is a\nuser-friendly package with functionality for model validation, visual\npredictive checks and simulation capabilities. The package is freely available\nand provides a flexible add-on to Wolfram Mathematica.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 12:28:45 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Leander", "Jacob", ""], ["Almquist", "Joachim", ""], ["Johnning", "Anna", ""], ["Larsson", "Julia", ""], ["Jirstrand", "Mats", ""]]}, {"id": "2011.06887", "submitter": "Sakshi Arya", "authors": "Anuj Abhishek and Sakshi Arya", "title": "Adaptive estimation of a function from its Exponential Radon Transform\n  in presence of noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we propose a locally adaptive strategy for estimating a\nfunction from its Exponential Radon Transform (ERT) data, without prior\nknowledge of the smoothness of functions that are to be estimated. We build a\nnon-parametric kernel type estimator and show that for a class of functions\ncomprising a wide Sobolev regularity scale, our proposed strategy follows the\nminimax optimal rate up to a $\\log{n}$ factor. We also show that there does not\nexist an optimal adaptive estimator on the Sobolev scale when the pointwise\nrisk is used and in fact the rate achieved by the proposed estimator is the\nadaptive rate of convergence.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 12:54:09 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Abhishek", "Anuj", ""], ["Arya", "Sakshi", ""]]}, {"id": "2011.06898", "submitter": "Gregor Zens", "authors": "Sylvia Fr\\\"uhwirth-Schnatter, Gregor Zens, Helga Wagner", "title": "Ultimate P\\'olya Gamma Samplers -- Efficient MCMC for possibly\n  imbalanced binary and categorical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Modeling binary and categorical data is one of the most commonly encountered\ntasks of applied statisticians and econometricians. While Bayesian methods in\nthis context have been available for decades now, they often require a high\nlevel of familiarity with Bayesian statistics or suffer from issues such as low\nsampling efficiency. To contribute to the accessibility of Bayesian models for\nbinary and categorical data, we introduce novel latent variable representations\nbased on P\\'olya Gamma random variables for a range of commonly encountered\ndiscrete choice models. From these latent variable representations, new Gibbs\nsampling algorithms for binary, binomial and multinomial logistic regression\nmodels are derived. All models allow for a conditionally Gaussian likelihood\nrepresentation, rendering extensions to more complex modeling frameworks such\nas state space models straight-forward. However, sampling efficiency may still\nbe an issue in these data augmentation based estimation frameworks. To\ncounteract this, MCMC boosting strategies are developed and discussed in\ndetail. The merits of our approach are illustrated through extensive\nsimulations and a real data application.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 13:29:53 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 06:45:57 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 04:14:53 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Fr\u00fchwirth-Schnatter", "Sylvia", ""], ["Zens", "Gregor", ""], ["Wagner", "Helga", ""]]}, {"id": "2011.06917", "submitter": "Bo Zhang", "authors": "Bo Zhang, Siyu Heng, Ting Ye, Dylan S. Small", "title": "Social Distancing and COVID-19: Randomization Inference for a Structured\n  Dose-Response Relationship", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Social distancing is widely acknowledged as an effective public health policy\ncombating the novel coronavirus. But extreme social distancing has costs and it\nis not clear how much social distancing is needed to achieve public health\neffects. In this article, we develop a design-based framework to make inference\nabout the dose-response relationship between social distancing and COVID-19\nrelated death toll and case numbers. We first discuss how to embed\nobservational data with a time-independent, continuous treatment dose into an\napproximate randomized experiment, and develop a randomization-based procedure\nthat tests if a structured dose-response relationship fits the data. We then\ngeneralize the design and testing procedure to accommodate a time-dependent,\ntreatment dose trajectory, and generalize a dose-response relationship to a\nlongitudinal setting. Finally, we apply the proposed design and testing\nprocedures to investigate the effect of social distancing during the phased\nreopening in the United States on public health outcomes using data compiled\nfrom sources including Unacast, the United States Census Bureau, and the County\nHealth Rankings and Roadmaps Program. We test a primary analysis hypothesis\nthat states the social distancing from April 27th to June 28th had no effect on\nthe COVID-19-related death toll from June 29th to August 2nd (p-value < 0.001)\nand conducted extensive secondary analyses that investigate the dose-response\nrelationship between social distancing and COVID-19 case numbers.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 18:59:10 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 00:11:03 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zhang", "Bo", ""], ["Heng", "Siyu", ""], ["Ye", "Ting", ""], ["Small", "Dylan S.", ""]]}, {"id": "2011.06927", "submitter": "Fr\\'ed\\'eric Semet J.", "authors": "Tifaout Almeftah, Luce Brotcorne, Diego Cattaruzza, Bernard Fortz,\n  Kaba Keita, Martine Labb\\'e, Maxime Ogier, Fr\\'ed\\'eric Semet", "title": "Group design in group testing for COVID-19 : A French case-study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group testing is a screening strategy that involves dividing a population\ninto several disjointed groups of subjects. In its simplest implementation,\neach group is tested with a single test in the first phase, while in the second\nphase only subjects in positive groups, if any, need to be tested again\nindividually. In this paper, we address the problem of group testing design,\nwhich aims to determine a partition into groups of a finite population in such\na way that cardinality constraints on the size of each group and a constraint\non the expected total number of tests are satisfied while minimizing a linear\ncombination of the expected number of false negative and false positive\nclassifications. First, we show that the properties and model introduced by\nAprahmian et al. can be extended to the group test design problem, which is\nthen modeled as a constrained shortest path problem on a specific graph. We\ndesign and implement an ad hoc algorithm to solve this problem. On instances\nbased on Sant\\'e Publique France data on Covid-19 screening tests, the results\nof the computational experiments are very promising.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 14:21:19 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Almeftah", "Tifaout", ""], ["Brotcorne", "Luce", ""], ["Cattaruzza", "Diego", ""], ["Fortz", "Bernard", ""], ["Keita", "Kaba", ""], ["Labb\u00e9", "Martine", ""], ["Ogier", "Maxime", ""], ["Semet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2011.06931", "submitter": "Peter Gr\\\"unwald", "authors": "J. ter Schure and M.F. Perez-Ortiz and A. Ly and P. Grunwald", "title": "The Safe Logrank Test: Error Control under Continuous Monitoring with\n  Unlimited Horizon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the safe logrank test, a version of the logrank test that\nprovides type-I error guarantees under optional stopping and optional\ncontinuation. The test is sequential without the need to specify a maximum\nsample size or stopping rule and allows for cumulative meta-analysis with\nType-I error control. The method can be extended to define anytime-valid\nconfidence intervals. All these properties are a virtue of the recently\ndeveloped martingale tests based on E-variables, of which the safe logrank test\nis an instance. We demonstrate the validity of the underlying nonnegative\nmartingale in a semiparametric setting of proportional hazards and show how to\nextend it to ties, Cox' regression and confidence sequences. Using a Gaussian\napproximation on the logrank statistic, we show that the safe logrank test\n(which itself is always exact) has a similar rejection region to\nO'Brien-Fleming alpha-spending but with the potential to achieve 100% power by\noptional continuation. Although our approach to study design requires a larger\nsample size, the expected sample size is competitive by optional stopping.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 14:23:32 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 21:42:44 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 12:28:47 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["ter Schure", "J.", ""], ["Perez-Ortiz", "M. F.", ""], ["Ly", "A.", ""], ["Grunwald", "P.", ""]]}, {"id": "2011.07030", "submitter": "Lucy D'Agostino McGowan", "authors": "Lucy D'Agostino McGowan and Robert A. Greevy, Jr", "title": "Contextualizing E-values for Interpretable Sensitivity to Unmeasured\n  Confounding Analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strength of evidence provided by epidemiological and observational\nstudies is inherently limited by the potential for unmeasured confounding.\nResearchers should present a quantified sensitivity to unmeasured confounding\nanalysis that is contextualized by the study's observed covariates. VanderWeele\nand Ding's E-value provides an easily calculated metric for the magnitude of\nthe hypothetical unmeasured confounding required to render the study's result\ninconclusive. We propose the Observed Covariate E-value to contextualize the\nsensitivity analysis' hypothetical E-value within the actual impact of observed\ncovariates, individually or within groups. We introduce a sensitivity analysis\nfigure that presents the Observed Covariate E-values, on the E-value scale,\nnext to their corresponding observed bias effects, on the original scale of the\nstudy results. This observed bias plot allows easy comparison of the\nhypothetical E-values, Observed Covariate E-values, and observed bias effects.\nWe illustrate the methods with a specific example and provide a supplemental\nappendix with modifiable code that teaches how to implement the method and\ncreate a publication quality figure.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 17:37:34 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["McGowan", "Lucy D'Agostino", ""], ["Greevy,", "Robert A.", "Jr"]]}, {"id": "2011.07047", "submitter": "Minge Xie", "authors": "Dungang Liu, Regina Y. Liu, Minge Xie", "title": "Nonparametric fusion learning: synthesize inferences from diverse\n  sources using depth confidence distribution", "comments": "47 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusion learning refers to synthesizing inferences from multiple sources or\nstudies to provide more effective inference and prediction than from any\nindividual source or study alone. Most existing methods for synthesizing\ninferences rely on parametric model assumptions, such as normality, which often\ndo not hold in practice. In this paper, we propose a general nonparametric\nfusion learning framework for synthesizing inferences of the target parameter\nfrom multiple sources. The main tool underlying the proposed framework is the\nnotion of depth confidence distribution (depth-CD), which is also developed in\nthis paper. Broadly speaking, a depth-CD is a data-driven nonparametric summary\ndistribution of inferential information for the target parameter. We show that\na depth-CD is a useful inferential tool and, moreover, is an omnibus form of\nconfidence regions (or p-values), whose contours of level sets shrink toward\nthe true parameter value. The proposed fusion learning approach combines\ndepth-CDs from the individual studies, with each depth-CD constructed by\nnonparametric bootstrap and data depth. This approach is shown to be efficient,\ngeneral and robust. Specifically, it achieves high-order accuracy and Bahadur\nefficiency under suitably chosen combining elements. It allows the model or\ninference structure to be different among individual studies. And it readily\nadapts to heterogeneous studies with a broad range of complex and irregular\nsettings. This property enables it to utilize indirect evidence from incomplete\nstudies to gain efficiency in the overall inference. The advantages of the\nproposed approach are demonstrated simulations and in a Federal Aviation\nAdministration (FAA) study of aircraft landing performance.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 18:33:38 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Liu", "Dungang", ""], ["Liu", "Regina Y.", ""], ["Xie", "Minge", ""]]}, {"id": "2011.07051", "submitter": "Francis DiTraglia", "authors": "Francis J. DiTraglia (1), Camilo Garcia-Jimeno (2), Rossa\n  O'Keeffe-O'Donovan (1), and Alejandro Sanchez-Becerra (3) ((1) Department of\n  Economics University of Oxford, (2) Federal Reserve Bank of Chicago, (3)\n  University of Pennsylvania)", "title": "Identifying Causal Effects in Experiments with Spillovers and\n  Non-compliance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper shows how to use a randomized saturation experimental design to\nidentify and estimate causal effects in the presence of spillovers--one\nperson's treatment may affect another's outcome--and one-sided\nnon-compliance--subjects can only be offered treatment, not compelled to take\nit up. Two distinct causal effects are of interest in this setting: direct\neffects quantify how a person's own treatment changes her outcome, while\nindirect effects quantify how her peers' treatments change her outcome. We\nconsider the case in which spillovers occur only within known groups, and\ntake-up decisions do not depend on peers' offers. In this setting we point\nidentify local average treatment effects, both direct and indirect, in a\nflexible random coefficients model that allows for both heterogenous treatment\neffects and endogeneous selection into treatment. We go on to propose a\nfeasible estimator that is consistent and asymptotically normal as the number\nand size of groups increases. We apply our estimator to data from a large-scale\njob placement services experiment, and find negative indirect treatment effects\non the likelihood of employment for those willing to take up the program. These\nnegative spillovers are offset by positive direct treatment effects from own\ntake-up.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 18:43:16 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 16:33:53 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["DiTraglia", "Francis J.", ""], ["Garcia-Jimeno", "Camilo", ""], ["O'Keeffe-O'Donovan", "Rossa", ""], ["Sanchez-Becerra", "Alejandro", ""]]}, {"id": "2011.07085", "submitter": "Francis DiTraglia", "authors": "Minsu Chang (1), and Francis J. DiTraglia (2) ((1) Department of\n  Economics Georgetown University, (2) Department of Economics University of\n  Oxford)", "title": "A Generalized Focused Information Criterion for GMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper proposes a criterion for simultaneous GMM model and moment\nselection: the generalized focused information criterion (GFIC). Rather than\nattempting to identify the \"true\" specification, the GFIC chooses from a set of\npotentially mis-specified moment conditions and parameter restrictions to\nminimize the mean-squared error (MSE) of a user-specified target parameter. The\nintent of the GFIC is to formalize a situation common in applied practice. An\napplied researcher begins with a set of fairly weak \"baseline\" assumptions,\nassumed to be correct, and must decide whether to impose any of a number of\nstronger, more controversial \"suspect\" assumptions that yield parameter\nrestrictions, additional moment conditions, or both. Provided that the baseline\nassumptions identify the model, we show how to construct an asymptotically\nunbiased estimator of the asymptotic MSE to select over these suspect\nassumptions: the GFIC. We go on to provide results for post-selection inference\nand model averaging that can be applied both to the GFIC and various\nalternative selection criteria. To illustrate how our criterion can be used in\npractice, we specialize the GFIC to the problem of selecting over exogeneity\nassumptions and lag lengths in a dynamic panel model, and show that it performs\nwell in simulations. We conclude by applying the GFIC to a dynamic panel data\nmodel for the price elasticity of cigarette demand.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 19:02:39 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chang", "Minsu", ""], ["DiTraglia", "Francis J.", ""]]}, {"id": "2011.07129", "submitter": "Jon Wakefield", "authors": "Katie Wilson and Jon Wakefield", "title": "A Probabilistic Model for Analyzing Summary Birth History Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BACKGROUND\n  There is an increasing demand for high quality subnational estimates of\nunder-five mortality. In low and middle income countries, where the burden of\nunder-five mortality is concentrated, vital registration is often lacking and\nhousehold surveys, which provide full birth history data, are often the most\nreliable source. Unfortunately, these data are spatially sparse and so data are\npulled from other sources to increase the available information. Summary birth\nhistories represent a large fraction of the available data, and provide numbers\nof births and deaths aggregated over time, along with the mother's age.\n  OBJECTIVE\n  Specialized methods are needed to leverage this information, and previously\nthe Brass method, and variants, have been used. We wish to develop a\nmodel-based approach that can propagate errors, and make the most efficient use\nof the data. Further, we strive to provide a method that does not have large\ncomputational overhead.\n  CONTRIBUTION\n  We describe a computationally efficient model-based approach which allows\nsummary birth history and full birth history data to be combined into analyses\nof under-five mortality in a natural way. The method is based on fertility and\nmortality models that allow direct smoothing over time and space, with the\npossibility for including relevant covariates that are associated with\nfertility and/or mortality. We first examine the behavior of the approach on\nsimulated data, before applying the model to survey and census data from\nMalawi.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 21:01:53 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wilson", "Katie", ""], ["Wakefield", "Jon", ""]]}, {"id": "2011.07131", "submitter": "Yuefeng Han", "authors": "Yuefeng Han, Cun-Hui Zhang and Rong Chen", "title": "Rank Determination in Tensor Factor Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Factor model is an appealing and effective analytic tool for high-dimensional\ntime series, with a wide range of applications in economics, finance and\nstatistics. One of the fundamental issues in using factor model for time series\nin practice is the determination of the number of factors to use. This paper\ndevelops two criteria for such a task for tensor factor models where the signal\npart of an observed time series in tensor form assumes a Tucker decomposition\nwith the core tensor as the factor tensor. The task is to determine the\ndimensions of the core tensor. One of the proposed criteria is similar to\ninformation based criteria of model selection, and the other is an extension of\nthe approaches based on the ratios of consecutive eigenvalues often used in\nfactor analysis for panel time series. The new criteria are designed to locate\nthe gap between the true smallest non-zero eigenvalue and the zero eigenvalues\nof a functional of the population version of the auto-cross-covariances of the\ntensor time series using their sample versions. As sample size and tensor\ndimension increase, such a gap increases under regularity conditions, resulting\nin consistency of the rank estimator. The criteria are built upon the existing\nnon-iterative and iterative estimation procedures of tensor factor model,\nyielding different performances. We provide sufficient conditions and\nconvergence rate for the consistency of the criteria as the sample size $T$ and\nthe dimensions of the observed tensor time series go to infinity. The results\ninclude the vector factor models as special cases, with an additional\nconvergence rates. The results also include the cases when there exist factors\nwith different signal strength. In addition, the convergence rates of the\neigenvalue estimators are established. Simulation studies provide promising\nfinite sample performance for the two criteria.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 21:04:47 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Han", "Yuefeng", ""], ["Zhang", "Cun-Hui", ""], ["Chen", "Rong", ""]]}, {"id": "2011.07186", "submitter": "Harlan Campbell", "authors": "Harlan Campbell, Valentijn M.T. de Jong, Lauren Maxwell, Thomas P.A.\n  Debray, Thomas Jaenisch, Paul Gustafson", "title": "Measurement Error in Meta-Analysis (MEMA) -- a Bayesian framework for\n  continuous outcome data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ideally, a meta-analysis will summarize data from several unbiased studies.\nHere we consider the less than ideal situation in which contributing studies\nmay be compromised by measurement error. Measurement error affects every study\ndesign, from randomized controlled trials to retrospective observational\nstudies. We outline a flexible Bayesian framework for continuous outcome data\nwhich allows one to obtain appropriate point and interval estimates with\nvarying degrees of prior knowledge about the magnitude of the measurement\nerror. We also demonstrate how, if individual-participant data (IPD) are\navailable, the Bayesian meta-analysis model can adjust for multiple\nparticipant-level covariates, measured with or without measurement error.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 01:43:30 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Campbell", "Harlan", ""], ["de Jong", "Valentijn M. T.", ""], ["Maxwell", "Lauren", ""], ["Debray", "Thomas P. A.", ""], ["Jaenisch", "Thomas", ""], ["Gustafson", "Paul", ""]]}, {"id": "2011.07234", "submitter": "Xinyu Li", "authors": "Xinyu Li, Wang Miao, Fang Lu and Xiao-Hua Zhou", "title": "Improving efficiency of inference in clinical trials with external\n  control data", "comments": "3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we are interested in the effect of a treatment in a clinical trial.\nThe efficiency of inference may be limited due to small sample size of the\nclinical trial. However, external control data are often available from\nhistorical studies. It is appealing to borrow strength from such data to\nimprove efficiency of inference in the clinical trial. Under an exchangeability\nassumption about the potential outcome mean, we show that the semiparametric\nefficiency bound for estimating the average treatment effect can be reduced by\nincorporating both external controls and the clinical trial data. We then\nderive a doubly robust and locally efficient estimator. We show that the\nimprovement in efficiency is prominent especially when the external control\ndataset has a large sample size and small variability. Our method allows for a\nrelaxed overlap assumption, and we illustrate with the case where the clinical\ntrial only contains an active treated group. We also develop a doubly robust\nand locally efficient approach that extrapolates the causal effect in the\nclinical trial to the overall population. Our results are also useful for trial\ndesign and data collection. We evaluate the finite-sample performance of the\nproposed estimators via simulation. In application to a clinical study\ncomparing the effect of a combination treatment on Helicobacter pylori\ninfection to that of the conventional triple therapy, our approach shows that\nthe combination treatment has efficacy advantages.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 07:31:09 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Li", "Xinyu", ""], ["Miao", "Wang", ""], ["Lu", "Fang", ""], ["Zhou", "Xiao-Hua", ""]]}, {"id": "2011.07420", "submitter": "Michael Newton", "authors": "Zihao Zheng, Aisha M. Mergaert, Irene M. Ong, Miriam A. Shelef, and\n  Michael A. Newton", "title": "MixTwice: large-scale hypothesis testing for peptide arrays by variance\n  mixing", "comments": null, "journal-ref": "Bioinformatics 2021", "doi": "10.1093/bioinformatics/btab162", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Peptide microarrays have emerged as a powerful technology in immunoproteomics\nas they provide a tool to measure the abundance of different antibodies in\npatient serum samples. The high dimensionality and small sample size of many\nexperiments challenge conventional statistical approaches, including those\naiming to control the false discovery rate (FDR). Motivated by limitations in\nreproducibility and power of current methods, we advance an empirical Bayesian\ntool that computes local false discovery rate statistics and local false sign\nrate statistics when provided with data on estimated effects and estimated\nstandard errors from all the measured peptides. As the name suggests, the\n\\verb+MixTwice+ tool involves the estimation of two mixing distributions, one\non underlying effects and one on underlying variance parameters. Constrained\noptimization techniques provide for model fitting of mixing distributions under\nweak shape constraints (unimodality of the effect distribution). Numerical\nexperiments show that \\verb+MixTwice+ can accurately estimate generative\nparameters and powerfully identify non-null peptides. In a peptide array study\nof rheumatoid arthritis (RA), \\verb+MixTwice+ recovers meaningful peptide\nmarkers in one case where the signal is weak, and has strong reproducibility\nproperties in one case where the signal is strong. \\verb+MixTwice+ is available\nas an R software package.\n\\href{https://github.com/wiscstatman/MixTwice}{https://github.com/wiscstatman/MixTwice}\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 00:18:13 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zheng", "Zihao", ""], ["Mergaert", "Aisha M.", ""], ["Ong", "Irene M.", ""], ["Shelef", "Miriam A.", ""], ["Newton", "Michael A.", ""]]}, {"id": "2011.07503", "submitter": "Alan Huang", "authors": "Alan Huang", "title": "On arbitrarily underdispersed Conway-Maxwell-Poisson distributions", "comments": "9 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Conway--Maxwell--Poisson distribution can be arbitrarily\nunderdispersed when parametrized via its mean. More precisely, if the mean\n$\\mu$ is an integer then the limiting distribution is a unit probability mass\nat $\\mu$. If the mean $\\mu$ is not an integer then the limiting distribution is\na shifted Bernoulli on the two values $\\floor{\\mu}$ and $\\ceil{\\mu}$ with\nprobabilities equal to the fractional parts of $\\mu$. In either case, the\nlimiting distribution is the most underdispersed discrete distribution possible\nfor any given mean. This is currently the only known generalization of the\nPoisson distribution exhibiting this property. Four practical implications are\ndiscussed, each adding to the claim that the (mean-parametrized)\nConway--Maxwell--Poisson distribution should be considered the default model\nfor underdispersed counts. We suggest that all future generalizations of the\nPoisson distribution be tested against this property.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 11:39:55 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Huang", "Alan", ""]]}, {"id": "2011.07518", "submitter": "Han Wang", "authors": "Han Wang, Changhu Wang, Linjie Wu, Ruibin Xi", "title": "A robust statistical method for Genome-wide association analysis of\n  human copy number variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conducting genome-wide association studies (GWAS) in copy number variation\n(CNV) level is a field where few people involves and little statistical\nprogresses have been achieved, traditional methods suffer from many problems\nsuch as batch effects, heterogeneity across genome, leading to low power or\nhigh false discovery rate. We develop a new robust method to find\ndisease-risking regions related to CNV's disproportionately distributed between\ncase and control samples, even if there are batch effects between them, our\ntest formula is robust to such effects. We propose a new empirical Bayes rule\nto deal with overfitting when estimating parameters during testing, this rule\ncan be extended to the field of model selection, it can be more efficient\ncompared with traditional methods when there are too much potential models to\nbe specified. We also give solid theoretical guarantees for our proposed\nmethod, and demonstrate the effectiveness by simulation and realdata analysis.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 13:05:00 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wang", "Han", ""], ["Wang", "Changhu", ""], ["Wu", "Linjie", ""], ["Xi", "Ruibin", ""]]}, {"id": "2011.07539", "submitter": "Niklas Hartung", "authors": "Niklas Hartung, Martin Wahl, Abhishake Rastogi, Wilhelm Huisinga", "title": "Nonparametric goodness-of-fit testing for parametric covariate models in\n  pharmacometric analyses", "comments": null, "journal-ref": null, "doi": "10.1002/psp4.12614", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The characterization of covariate effects on model parameters is a crucial\nstep during pharmacokinetic/pharmacodynamic analyses. While covariate selection\ncriteria have been studied extensively, the choice of the functional\nrelationship between covariates and parameters, however, has received much less\nattention. Often, a simple particular class of covariate-to-parameter\nrelationships (linear, exponential, etc.) is chosen ad hoc or based on domain\nknowledge, and a statistical evaluation is limited to the comparison of a small\nnumber of such classes. Goodness-of-fit testing against a nonparametric\nalternative provides a more rigorous approach to covariate model evaluation,\nbut no such test has been proposed so far. In this manuscript, we derive and\nevaluate nonparametric goodness-of-fit tests for parametric covariate models,\nthe null hypothesis, against a kernelized Tikhonov regularized alternative,\ntransferring concepts from statistical learning to the pharmacological setting.\nThe approach is evaluated in a simulation study on the estimation of the\nage-dependent maturation effect on the clearance of a monoclonal antibody.\nScenarios of varying data sparsity and residual error are considered. The\ngoodness-of-fit test correctly identified misspecified parametric models with\nhigh power for relevant scenarios. The case study provides proof-of-concept of\nthe feasibility of the proposed approach, which is envisioned to be beneficial\nfor applications that lack well-founded covariate models.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 14:36:47 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Hartung", "Niklas", ""], ["Wahl", "Martin", ""], ["Rastogi", "Abhishake", ""], ["Huisinga", "Wilhelm", ""]]}, {"id": "2011.07559", "submitter": "Mehrdad Naderi Dr", "authors": "Mehrdad Naderi, Elham Mirfarah, Matthew Bernhardt and Ding-Geng Chen", "title": "Semiparametric inference for the scale-mixture of normal partial linear\n  regression model with censored data", "comments": "17 page, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of censored data modeling, the classical linear regression\nmodel that assumes normally distributed random errors has received increasing\nattention in recent years, mainly for mathematical and computational\nconvenience. However, practical studies have often criticized this linear\nregression model due to its sensitivity to departure from the normality and\nfrom the partial nonlinearity. This paper proposes to solve these potential\nissues simultaneously in the context of the partial linear regression model by\nassuming that the random errors follow a scale-mixture of normal (SMN) family\nof distributions. The proposed method allows us to model data with great\nflexibility, accommodating heavy tails, and outliers. By implementing the\nB-spline function and using the convenient hierarchical representation of the\nSMN distributions, a computationally analytical EM-type algorithm is developed\nto perform maximum likelihood inference of the model parameters. Various\nsimulation studies are conducted to investigate the finite sample properties as\nwell as the robustness of the model in dealing with the heavy-tails distributed\ndatasets. Real-word data examples are finally analyzed for illustrating the\nusefulness of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 15:38:49 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Naderi", "Mehrdad", ""], ["Mirfarah", "Elham", ""], ["Bernhardt", "Matthew", ""], ["Chen", "Ding-Geng", ""]]}, {"id": "2011.07568", "submitter": "Zijian Guo", "authors": "Zijian Guo", "title": "Inference for High-dimensional Maximin Effects in Heterogeneous\n  Regression Models Using a Sampling Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneity is an important feature of modern data sets and a central task\nis to extract information from large-scale and heterogeneous data. In this\npaper, we consider multiple high-dimensional linear models and adopt the\ndefinition of maximin effect (Meinshausen, B{\\\"u}hlmann, AoS, 43(4),\n1801--1830) to summarize the information contained in this heterogeneous model.\nWe define the maximin effect for a targeted population whose covariate\ndistribution is possibly different from that of the observed data. We further\nintroduce a ridge-type maximin effect to simultaneously account for reward\noptimality and statistical stability. To identify the high-dimensional maximin\neffect, we estimate the regression covariance matrix by a debiased estimator\nand use it to construct the aggregation weights for the maximin effect. A main\nchallenge for statistical inference is that the estimated weights might have a\nmixture distribution and the resulted maximin effect estimator is not\nnecessarily asymptotic normal. To address this, we devise a novel sampling\napproach to construct the confidence interval for any linear contrast of\nhigh-dimensional maximin effects. The coverage and precision properties of the\nproposed confidence interval are studied. The proposed method is demonstrated\nover simulations and a genetic data set on yeast colony growth under different\nenvironments.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 16:15:10 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 10:50:51 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Guo", "Zijian", ""]]}, {"id": "2011.07664", "submitter": "Han Lin Shang", "authors": "Ufuk Beyaztas and Han Lin Shang", "title": "Robust bootstrap prediction intervals for univariate and multivariate\n  autoregressive time series models", "comments": "34 pages, 15 figures, to appear at Journal of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The bootstrap procedure has emerged as a general framework to construct\nprediction intervals for future observations in autoregressive time series\nmodels. Such models with outlying data points are standard in real data\napplications, especially in the field of econometrics. These outlying data\npoints tend to produce high forecast errors, which reduce the forecasting\nperformances of the existing bootstrap prediction intervals calculated based on\nnon-robust estimators. In the univariate and multivariate autoregressive time\nseries, we propose a robust bootstrap algorithm for constructing prediction\nintervals and forecast regions. The proposed procedure is based on the weighted\nlikelihood estimates and weighted residuals. Its finite sample properties are\nexamined via a series of Monte Carlo studies and two empirical data examples.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 00:12:55 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Beyaztas", "Ufuk", ""], ["Shang", "Han Lin", ""]]}, {"id": "2011.07677", "submitter": "Zhichao Jiang", "authors": "Zhichao Jiang, Kosuke Imai", "title": "Statistical Inference and Power Analysis for Direct and Spillover\n  Effects in Two-Stage Randomized Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-stage randomized experiments are becoming an increasingly popular\nexperimental design for causal inference when the outcome of one unit may be\naffected by the treatment assignments of other units in the same cluster. In\nthis paper, we provide a methodological framework for general tools of\nstatistical inference and power analysis for two-stage randomized experiments.\nUnder the randomization-based framework, we propose unbiased point estimators\nof direct and spillover effects, construct conservative variance estimators,\ndevelop hypothesis testing procedures, and derive sample size formulas. We also\nestablish the equivalence relationships between the randomization-based and\nregression-based methods. We theoretically compare the two-stage randomized\ndesign with the completely randomized and cluster randomized designs, which\nrepresent two limiting designs. Finally, we conduct simulation studies to\nevaluate the empirical performance of our sample size formulas. For empirical\nillustration, the proposed methodology is applied to the analysis of the data\nfrom a field experiment on a job placement assistance program.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 01:37:52 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 03:41:02 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Jiang", "Zhichao", ""], ["Imai", "Kosuke", ""]]}, {"id": "2011.07721", "submitter": "Sanjay Chaudhuri", "authors": "Sanjay Chaudhuri and Subhroshekhar Ghosh and David J. Nott and Kim Cuc\n  Pham", "title": "On a Variational Approximation based Empirical Likelihood ABC Method", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.01675", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientifically well-motivated statistical models in natural,\nengineering, and environmental sciences are specified through a generative\nprocess. However, in some cases, it may not be possible to write down the\nlikelihood for these models analytically. Approximate Bayesian computation\n(ABC) methods allow Bayesian inference in such situations. The procedures are\nnonetheless typically computationally intensive. Recently, computationally\nattractive empirical likelihood-based ABC methods have been suggested in the\nliterature. All of these methods rely on the availability of several suitable\nanalytically tractable estimating equations, and this is sometimes problematic.\nWe propose an easy-to-use empirical likelihood ABC method in this article.\nFirst, by using a variational approximation argument as a motivation, we show\nthat the target log-posterior can be approximated as a sum of an expected joint\nlog-likelihood and the differential entropy of the data generating density. The\nexpected log-likelihood is then estimated by an empirical likelihood where the\nonly inputs required are a choice of summary statistic, it's observed value,\nand the ability to simulate the chosen summary statistics for any parameter\nvalue under the model. The differential entropy is estimated from the simulated\nsummaries using traditional methods. Posterior consistency is established for\nthe method, and we discuss the bounds for the required number of simulated\nsummaries in detail. The performance of the proposed method is explored in\nvarious examples.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 21:24:26 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chaudhuri", "Sanjay", ""], ["Ghosh", "Subhroshekhar", ""], ["Nott", "David J.", ""], ["Pham", "Kim Cuc", ""]]}, {"id": "2011.07753", "submitter": "Iuri Ferreira Dr.", "authors": "Ferreira, Iuri Emmanuel de Paula and Zocchi, Silvio Sandoval", "title": "A family of smooth piecewise-linear models with probabilistic\n  interpretations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smooth piecewise-linear models cover a wide range of applications\nnowadays. Basically, there are two classes of them: models are transitional or\nhyperbolic according to their behaviour at the phase-transition zones. This\nstudy explored three different approaches to build smooth piecewise-linear\nmodels, and we analysed their inter-relationships by a unifying modelling\nframework. We conceived the smoothed phase-transition zones as domains where a\nmixture process takes place, which ensured probabilistic interpretations for\nboth hyperbolic and transitional models in the light of random thresholds. Many\npopular models found in the literature are special cases of our methodology.\nFurthermore, this study introduces novel regression models as alternatives,\nsuch as the Epanechnikov, Normal and Skewed-Normal Bent-Cables.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 07:09:46 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ferreira", "", ""], ["de Paula", "Iuri Emmanuel", ""], ["Zocchi", "", ""], ["Sandoval", "Silvio", ""]]}, {"id": "2011.07816", "submitter": "Yang Lu", "authors": "Christian Gourieroux and Yang Lu", "title": "SIR Model with Stochastic Transmission", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Susceptible-Infected-Recovered (SIR) model is the cornerstone of\nepidemiological models. However, this specification depends on two parameters\nonly, which implies a lack of flexibility and the difficulty to replicate the\nvolatile reproduction numbers observed in practice. We extend the classic SIR\nmodel by introducing nonlinear stochastic transmission, to get a stochastic SIR\nmodel. We derive its exact solution and discuss the condition for herd\nimmunity. The stochastic SIR model corresponds to a population of infinite\nsize. When the population size is finite, there is also sampling uncertainty.\nWe propose a state-space framework under which we analyze the relative\nmagnitudes of the observational and stochastic epidemiological uncertainties\nduring the evolution of the epidemic. We also emphasize the lack of robustness\nof the notion of herd immunity when the SIR model is time discretized.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 09:39:47 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Gourieroux", "Christian", ""], ["Lu", "Yang", ""]]}, {"id": "2011.07866", "submitter": "Benjamin Guedj", "authors": "Arthur Leroy and Pierre Latouche and Benjamin Guedj and Servane Gey", "title": "Cluster-Specific Predictions with Multi-Task Gaussian Processes", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A model involving Gaussian processes (GPs) is introduced to simultaneously\nhandle multi-task learning, clustering, and prediction for multiple functional\ndata. This procedure acts as a model-based clustering method for functional\ndata as well as a learning step for subsequent predictions for new tasks. The\nmodel is instantiated as a mixture of multi-task GPs with common mean\nprocesses. A variational EM algorithm is derived for dealing with the\noptimisation of the hyper-parameters along with the hyper-posteriors'\nestimation of latent variables and processes. We establish explicit formulas\nfor integrating the mean processes and the latent clustering variables within a\npredictive distribution, accounting for uncertainty on both aspects. This\ndistribution is defined as a mixture of cluster-specific GP predictions, which\nenhances the performances when dealing with group-structured data. The model\nhandles irregular grid of observations and offers different hypotheses on the\ncovariance structure for sharing additional information across tasks. The\nperformances on both clustering and prediction tasks are assessed through\nvarious simulated scenarios and real datasets. The overall algorithm, called\nMagmaClust, is publicly available as an R package.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 11:08:59 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 13:45:02 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Leroy", "Arthur", ""], ["Latouche", "Pierre", ""], ["Guedj", "Benjamin", ""], ["Gey", "Servane", ""]]}, {"id": "2011.07998", "submitter": "Marija Cupari\\'c", "authors": "Marija Cupari\\'c and Bojana Milo\\v{s}evi\\'c", "title": "New characterization based exponentiality tests for randomly censored\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the characterization based approach for the construction of\ngoodness of fit tests has become popular. Most of the proposed tests have been\ndesigned for complete i.i.d. samples. Here we present the adaptation of the\nrecently proposed exponentiality tests based on equidistribution-type\ncharacterizations for the case of randomly censored data. Their asymptotic\nproperties are provided. Besides, we present the results of wide empirical\npower study including the powers of several recent competitors. This study can\nbe used as a benchmark for future tests proposed for this kind of data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 14:46:14 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Cupari\u0107", "Marija", ""], ["Milo\u0161evi\u0107", "Bojana", ""]]}, {"id": "2011.08047", "submitter": "B\\'en\\'edicte Colnet", "authors": "B\\'en\\'edicte Colnet, Imke Mayer, Guanhua Chen, Awa Dieng, Ruohong Li,\n  Ga\\\"el Varoquaux, Jean-Philippe Vert, Julie Josse, Shu Yang", "title": "Causal inference methods for combining randomized trials and\n  observational studies: a review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing data availability, causal treatment effects can be evaluated\nacross different datasets, both randomized controlled trials (RCTs) and\nobservational studies. RCTs isolate the effect of the treatment from that of\nunwanted (confounding) co-occurring effects. But they may struggle with\ninclusion biases, and thus lack external validity. On the other hand, large\nobservational samples are often more representative of the target population\nbut can conflate confounding effects with the treatment of interest. In this\npaper, we review the growing literature on methods for causal inference on\ncombined RCTs and observational studies, striving for the best of both worlds.\nWe first discuss identification and estimation methods that improve\ngeneralizability of RCTs using the representativeness of observational data.\nClassical estimators include weighting, difference between conditional outcome\nmodels, and doubly robust estimators. We then discuss methods that combine RCTs\nand observational data to improve (conditional) average treatment effect\nestimation, handling possible unmeasured confounding in the observational data.\nWe also connect and contrast works developed in both the potential outcomes\nframework and the structural causal model framework. Finally, we compare the\nmain methods using a simulation study and real world data to analyze the effect\nof tranexamic acid on the mortality rate in major trauma patients. Code to\nimplement many of the methods is provided.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 15:57:43 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 12:21:48 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Colnet", "B\u00e9n\u00e9dicte", ""], ["Mayer", "Imke", ""], ["Chen", "Guanhua", ""], ["Dieng", "Awa", ""], ["Li", "Ruohong", ""], ["Varoquaux", "Ga\u00ebl", ""], ["Vert", "Jean-Philippe", ""], ["Josse", "Julie", ""], ["Yang", "Shu", ""]]}, {"id": "2011.08174", "submitter": "Davide Viviano Mr.", "authors": "Davide Viviano", "title": "Policy choice in experiments with unknown interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses experimental design to estimate welfare-maximizing\npolicies. We consider a setting where units are organized into large, finitely\nmany independent clusters and interact over unobserved dimensions within each\ncluster. The contribution of this paper is two-fold. First, we construct a test\nfor whether a welfare-improving treatment configuration exists and hence worth\nlearning by conducting a larger scale experiment. Second, we introduce an\nadaptive randomization procedure to estimate welfare-maximizing individual\ntreatment allocation rules valid under unobserved interference. We derive\nasymptotic properties of the marginal effects estimators and finite-sample\nregret guarantees of the policy. Finally, we illustrate the method's advantage\nin simulations calibrated to an existing experiment on information diffusion.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 18:58:54 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 18:57:10 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 18:58:15 GMT"}, {"version": "v4", "created": "Wed, 23 Jun 2021 17:42:04 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Viviano", "Davide", ""]]}, {"id": "2011.08269", "submitter": "Sophie Achard", "authors": "Sophie Achard, Jean-Francois Coeurjolly, Pierre Lafaye de Micheaux,\n  Jonas Richiardi", "title": "Robust correlation for aggregated data with spatial characteristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this paper is to study the robustness of computation of\ncorrelations under spatial constraints. The motivation of our paper is the\nspecific case of functional magnetic resonance (fMRI) brain imaging data, where\nvoxels are aggregated to compute correlations. In this paper we show that the\nway the spatial components are aggregating to compute correlation may have a\nstrong influence on the resulting estimations. We then propose various\nestimators which take into account this spatial structure.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 20:53:30 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Achard", "Sophie", ""], ["Coeurjolly", "Jean-Francois", ""], ["de Micheaux", "Pierre Lafaye", ""], ["Richiardi", "Jonas", ""]]}, {"id": "2011.08282", "submitter": "Deepak Nag Ayyala", "authors": "Deepak Nag Ayyala, Santu Ghosh and Daniel F. Linder", "title": "Covariance matrix testing in high dimension using random projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimation and hypothesis tests for the covariance matrix in high dimensions\nis a challenging problem as the traditional multivariate asymptotic theory is\nno longer valid. When the dimension is larger than or increasing with the\nsample size, standard likelihood based tests for the covariance matrix have\npoor performance. Existing high dimensional tests are either computationally\nexpensive or have very weak control of type I error. In this paper, we propose\na test procedure, CRAMP, for testing hypotheses involving one or more\ncovariance matrices using random projections. Projecting the high dimensional\ndata randomly into lower dimensional subspaces alleviates of the curse of\ndimensionality, allowing for the use of traditional multivariate tests. An\nextensive simulation study is performed to compare CRAMP against\nasymptotics-based high dimensional test procedures. An application of the\nproposed method to two gene expression data sets is presented.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 21:15:14 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Ayyala", "Deepak Nag", ""], ["Ghosh", "Santu", ""], ["Linder", "Daniel F.", ""]]}, {"id": "2011.08299", "submitter": "Harrison Wilde", "authors": "Harrison Wilde, Jack Jewson, Sebastian Vollmer and Chris Holmes", "title": "Foundations of Bayesian Learning from Synthetic Data", "comments": "43 pages (10 main text, 33 supplement), 32 figures (4 main text, 28\n  supplement)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is significant growth and interest in the use of synthetic data as an\nenabler for machine learning in environments where the release of real data is\nrestricted due to privacy or availability constraints. Despite a large number\nof methods for synthetic data generation, there are comparatively few results\non the statistical properties of models learnt on synthetic data, and fewer\nstill for situations where a researcher wishes to augment real data with\nanother party's synthesised data. We use a Bayesian paradigm to characterise\nthe updating of model parameters when learning in these settings, demonstrating\nthat caution should be taken when applying conventional learning algorithms\nwithout appropriate consideration of the synthetic data generating process and\nlearning task. Recent results from general Bayesian updating support a novel\nand robust approach to Bayesian synthetic-learning founded on decision theory\nthat outperforms standard approaches across repeated experiments on supervised\nlearning and inference problems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 21:49:17 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 15:01:22 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Wilde", "Harrison", ""], ["Jewson", "Jack", ""], ["Vollmer", "Sebastian", ""], ["Holmes", "Chris", ""]]}, {"id": "2011.08321", "submitter": "Moritz Schauer", "authors": "Denis Belomestny, Shota Gugushvili, Moritz Schauer, Peter Spreij", "title": "Nonparametric Bayesian volatility estimation for gamma-driven stochastic\n  differential equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a nonparametric Bayesian approach to estimation of the volatility\nfunction of a stochastic differential equation driven by a gamma process. The\nvolatility function is modelled a priori as piecewise constant, and we specify\na gamma prior on its values. This leads to a straightforward procedure for\nposterior inference via an MCMC procedure. We give theoretical performance\nguarantees (contraction rates for the posterior) for the Bayesian estimate in\nterms of the regularity of the unknown volatility function. We illustrate the\nmethod on synthetic and real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 22:47:42 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Belomestny", "Denis", ""], ["Gugushvili", "Shota", ""], ["Schauer", "Moritz", ""], ["Spreij", "Peter", ""]]}, {"id": "2011.08389", "submitter": "Reza Hosseini", "authors": "Alireza Hosseini and Reza Hosseini", "title": "Model selection for count timeseries with applications in forecasting\n  number of trips in bike-sharing systems and its volatility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Forecasting the number of trips in bike-sharing systems and its volatility\nover time is crucial for planning and optimizing such systems. This paper\ndevelops timeseries models to forecast hourly count timeseries data, and\nestimate its volatility. Such models need to take into account the complex\npatterns over various temporal scales including hourly, daily, weekly and\nannual as well as the temporal correlation. To capture this complex structure,\na large number of parameters are needed. Here a structural model selection\napproach is utilized to choose the parameters. This method explores the\nparameter space for a group of covariates at each step. These groups of\ncovariate are constructed to represent a particular structure in the model. The\nstatistical models utilized are extensions of Generalized Linear Models to\ntimeseries data. One challenge in using such models is the explosive behavior\nof the simulated values. To address this issue, we develop a technique which\nrelies on damping the simulated value, if it falls outside of an admissible\ninterval. The admissible interval is defined using measures of variability of\nthe left and right tails. A new definition of outliers is proposed based on\nthese variability measures. This new definition is shown to be useful in the\ncontext of asymmetric distributions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 02:55:26 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Hosseini", "Alireza", ""], ["Hosseini", "Reza", ""]]}, {"id": "2011.08411", "submitter": "Yifan Cui", "authors": "Yifan Cui, Hongming Pu, Xu Shi, Wang Miao, Eric Tchetgen Tchetgen", "title": "Semiparametric proximal causal inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skepticism about the assumption of no unmeasured confounding, also known as\nexchangeability, is often warranted in making causal inferences from\nobservational data; because exchangeability hinges on an investigator's ability\nto accurately measure covariates that capture all potential sources of\nconfounding. In practice, the most one can hope for is that covariate\nmeasurements are at best proxies of the true underlying confounding mechanism\noperating in a given observational study. In this paper, we consider the\nframework of proximal causal inference introduced by Tchetgen Tchetgen et al.\n(2020), which while explicitly acknowledging covariate measurements as\nimperfect proxies of confounding mechanisms, offers an opportunity to learn\nabout causal effects in settings where exchangeability on the basis of measured\ncovariates fails. We make a number of contributions to proximal inference\nincluding (i) an alternative set of conditions for nonparametric proximal\nidentification of the average treatment effect; (ii) general semiparametric\ntheory for proximal estimation of the average treatment effect including\nefficiency bounds for key semiparametric models of interest; (iii) a\ncharacterization of proximal doubly robust and locally efficient estimators of\nthe average treatment effect. Moreover, we provide analogous identification and\nefficiency results for the average treatment effect on the treated. Our\napproach is illustrated via simulation studies and a data application on\nevaluating the effectiveness of right heart catheterization in the intensive\ncare unit of critically ill patients.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 04:07:27 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Cui", "Yifan", ""], ["Pu", "Hongming", ""], ["Shi", "Xu", ""], ["Miao", "Wang", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "2011.08475", "submitter": "Sai Kumar Popuri", "authors": "Sai K. Popuri, Nagaraj K. Neerchal, Amita Mehta, and Ahmad Mousavi", "title": "Density Estimation using Entropy Maximization for Semi-continuous Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-continuous data comes from a distribution that is a mixture of the point\nmass at zero and a continuous distribution with support on the positive real\nline. A clear example is the daily rainfall data. In this paper, we present a\nnovel algorithm to estimate the density function for semi-continuous data using\nthe principle of maximum entropy. Unlike existing methods in the literature,\nour algorithm needs only the sample values of the constraint functions in the\nentropy maximization problem and does not need the entire sample. Using\nsimulations, we show that the estimate of the entropy produced by our algorithm\nhas significantly less bias compared to existing methods. An application to the\ndaily rainfall data is provided.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 06:57:33 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 22:26:14 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Popuri", "Sai K.", ""], ["Neerchal", "Nagaraj K.", ""], ["Mehta", "Amita", ""], ["Mousavi", "Ahmad", ""]]}, {"id": "2011.08521", "submitter": "Jie Wu", "authors": "Zemin Zheng, Yang Li, Jie Wu and Yuchen Wang", "title": "Sequential scaled sparse factor regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale association analysis between multivariate responses and\npredictors is of great practical importance, as exemplified by modern business\napplications including social media marketing and crisis management. Despite\nthe rapid methodological advances, how to obtain scalable estimators with free\ntuning of the regularization parameters remains unclear under general noise\ncovariance structures. In this paper, we develop a new methodology called\nsequential scaled sparse factor regression (SESS) based on a new viewpoint that\nthe problem of recovering a jointly low-rank and sparse regression coefficient\nmatrix can be decomposed into several univariate response sparse regressions\nthrough regular eigenvalue decomposition. It combines the strengths of\nsequential estimation and scaled sparse regression, thus sharing the\nscalability and the tuning free property for sparsity parameters inherited from\nthe two approaches. The stepwise convex formulation, sequential factor\nregression framework, and tuning insensitiveness make SESS highly scalable for\nbig data applications. Comprehensive theoretical justifications with new\ninsights into high-dimensional multi-response regressions are also provided. We\ndemonstrate the scalability and effectiveness of the proposed method by\nsimulation studies and stock short interest data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 09:16:32 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zheng", "Zemin", ""], ["Li", "Yang", ""], ["Wu", "Jie", ""], ["Wang", "Yuchen", ""]]}, {"id": "2011.08644", "submitter": "Sebastian Schmon", "authors": "Sebastian M Schmon, Patrick W Cannon, Jeremias Knoblauch", "title": "Generalized Posteriors in Approximate Bayesian Computation", "comments": "Accepted at Advances in Approximate Bayesian Inference, AABI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex simulators have become a ubiquitous tool in many scientific\ndisciplines, providing high-fidelity, implicit probabilistic models of natural\nand social phenomena. Unfortunately, they typically lack the tractability\nrequired for conventional statistical analysis. Approximate Bayesian\ncomputation (ABC) has emerged as a key method in simulation-based inference,\nwherein the true model likelihood and posterior are approximated using samples\nfrom the simulator. In this paper, we draw connections between ABC and\ngeneralized Bayesian inference (GBI). First, we re-interpret the accept/reject\nstep in ABC as an implicitly defined error model. We then argue that these\nimplicit error models will invariably be misspecified. While ABC posteriors are\noften treated as a necessary evil for approximating the standard Bayesian\nposterior, this allows us to re-interpret ABC as a potential robustification\nstrategy. This leads us to suggest the use of GBI within ABC, a use case we\nexplore empirically.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 14:08:59 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 18:16:36 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Schmon", "Sebastian M", ""], ["Cannon", "Patrick W", ""], ["Knoblauch", "Jeremias", ""]]}, {"id": "2011.08661", "submitter": "Yuhao Wang", "authors": "Yuhao Wang and Rajen D. Shah", "title": "Debiased Inverse Propensity Score Weighting for Estimation of Average\n  Treatment Effects with High-Dimensional Confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of average treatment effects given observational data\nwith high-dimensional pretreatment variables. Existing methods for this problem\ntypically assume some form of sparsity for the regression functions. In this\nwork, we introduce a debiased inverse propensity score weighting (DIPW) scheme\nfor average treatment effect estimation that delivers $\\sqrt{n}$-consistent\nestimates of the average treatment effect when the propensity score follows a\nsparse logistic regression model; the regression functions are permitted to be\narbitrarily complex. Our theoretical results quantify the price to pay for\npermitting the regression functions to be unestimable, which shows up as an\ninflation of the variance of the estimator compared to the semiparametric\nefficient variance by at most O(1) under mild conditions. Given the lack of\nassumptions on the regression functions, averages of transformed responses\nunder each treatment may also be estimated at the $\\sqrt{n}$ rate, and so for\nexample, the variances of the potential outcomes may be estimated. We show how\nconfidence intervals centred on our estimates may be constructed, and also\ndiscuss an extension of the method to estimating projections of the\nheterogeneous treatment effect function.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 14:42:17 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wang", "Yuhao", ""], ["Shah", "Rajen D.", ""]]}, {"id": "2011.08669", "submitter": "Li-Chun Zhang", "authors": "Li-Chun Zhang", "title": "Sampling designs for epidemic prevalence estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intuitively, sampling is likely to be more efficient for prevalence\nestimation, if the cases (or positives) have a relatively higher representation\nin the sample than in the population. In case the virus is transmitted via\npersonal contacts, contact tracing of the observed cases (but not noncases), to\nbe referred to as \\emph{adaptive network tracing}, can generate a higher yield\nof cases than random sampling from the population. The efficacy of relevant\ndesigns for cross-sectional and change estimation is investigated. The\navailability of these designs allows one unite tracing for combating the\nepidemic and sampling for estimating the prevalence in a single endeavour.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 11:13:06 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zhang", "Li-Chun", ""]]}, {"id": "2011.08708", "submitter": "Martina Sundqvist", "authors": "Martina Sundqvist, Julien Chiquet, Guillem Rigaill", "title": "Adjusting the adjusted Rand Index -- A multinomial story", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Adjusted Rand Index ($ARI$) is arguably one of the most popular measures\nfor cluster comparison. The adjustment of the $ARI$ is based on a\nhypergeometric distribution assumption which is unsatisfying from a modeling\nperspective as (i) it is not appropriate when the two clusterings are\ndependent, (ii) it forces the size of the clusters, and (iii) it ignores\nrandomness of the sampling. In this work, we present a new \"modified\" version\nof the Rand Index. First, we redefine the $MRI$ by only counting the pairs\nconsistent by similarity and ignoring the pairs consistent by difference,\nincreasing the interpretability of the score. Second, we base the adjusted\nversion, $MARI$, on a multinomial distribution instead of a hypergeometric\ndistribution. The multinomial model is advantageous as it does not force the\nsize of the clusters, properly models randomness, and is easily extended to the\ndependant case. We show that the $ARI$ is biased under the multinomial model\nand that the difference between the $ARI$ and $MARI$ can be large for small $n$\nbut essentially vanish for large $n$, where $n$ is the number of individuals.\nFinally, we provide an efficient algorithm to compute all these quantities\n($(A)RI$ and $M(A)RI$) by relying on a sparse representation of the contingency\ntable in our \\texttt{aricode} package. The space and time complexity is linear\nin the number of samples and importantly does not depend on the number of\nclusters as we do not explicitly compute the contingency table.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 15:31:53 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Sundqvist", "Martina", ""], ["Chiquet", "Julien", ""], ["Rigaill", "Guillem", ""]]}, {"id": "2011.08799", "submitter": "Wagner Barreto-Souza", "authors": "Luiza S.C. Piancastelli, Wagner Barreto-Souza and Hernando Ombao", "title": "Flexible Bivariate INGARCH Process With a Broad Range of Contemporaneous\n  Correlation", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel flexible bivariate conditional Poisson (BCP)\nINteger-valued Generalized AutoRegressive Conditional Heteroscedastic (INGARCH)\nmodel for correlated count time series data. Our proposed BCP-INGARCH model is\nmathematically tractable and has as the main advantage over existing bivariate\nINGARCH models its ability to capture a broad range (both negative and\npositive) of contemporaneous cross-correlation which is a non-trivial\nadvancement. Properties of stationarity and ergodicity for the BCP-INGARCH\nprocess are developed. Estimation of the parameters is performed through\nconditional maximum likelihood (CML) and finite sample behavior of the\nestimators are investigated through simulation studies. Asymptotic properties\nof the CML estimators are derived. Additional simulation studies compare and\ncontrast methods of obtaining standard errors of the parameter estimates, where\na bootstrap option is demonstrated to be advantageous. Hypothesis testing\nmethods for the presence of contemporaneous correlation between the time series\nare presented and evaluated. We apply our methodology to monthly counts of\nhepatitis cases at two nearby Brazilian cities, which are highly\ncross-correlated. The data analysis demonstrates the importance of considering\na bivariate model allowing for a wide range of contemporaneous correlation in\nreal-life applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 17:46:27 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Piancastelli", "Luiza S. C.", ""], ["Barreto-Souza", "Wagner", ""], ["Ombao", "Hernando", ""]]}, {"id": "2011.08896", "submitter": "Stephen Portnoy", "authors": "Stephen Portnoy and Joseph Haimberg", "title": "Canonical Regression Quantiles with application to CEO compensation and\n  predicting company performance", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In using multiple regression methods for prediction, one often considers the\nlinear combination of explanatory variables as an index. Seeking a single such\nindex when here are multiple responses is rather more complicated. One\nclassical approach is to use the coefficients from the leading canonical\ncorrelation. However, methods based on variances are unable to disaggregate\nresponses by quantile effects, lack robustness, and rely on normal assumptions\nfor inference. We develop here an alternative regression quantile approach and\napply it to an empirical study of the performance of large publicly held\ncompanies and CEO compensation. The initial results are very promising.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:30:21 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Portnoy", "Stephen", ""], ["Haimberg", "Joseph", ""]]}, {"id": "2011.08991", "submitter": "Tamara Fernandez", "authors": "Tamara Fern\\'andez, Wenkai Xu, Marc Ditzhaus and Arthur Gretton", "title": "A kernel test for quasi-independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider settings in which the data of interest correspond to pairs of\nordered times, e.g, the birth times of the first and second child, the times at\nwhich a new user creates an account and makes the first purchase on a website,\nand the entry and survival times of patients in a clinical trial. In these\nsettings, the two times are not independent (the second occurs after the\nfirst), yet it is still of interest to determine whether there exists\nsignificant dependence {\\em beyond} their ordering in time. We refer to this\nnotion as \"quasi-(in)dependence\". For instance, in a clinical trial, to avoid\nbiased selection, we might wish to verify that recruitment times are\nquasi-independent of survival times, where dependencies might arise due to\nseasonal effects. In this paper, we propose a nonparametric statistical test of\nquasi-independence. Our test considers a potentially infinite space of\nalternatives, making it suitable for complex data where the nature of the\npossible quasi-dependence is not known in advance. Standard parametric\napproaches are recovered as special cases, such as the classical conditional\nKendall's tau, and log-rank tests. The tests apply in the right-censored\nsetting: an essential feature in clinical trials, where patients can withdraw\nfrom the study. We provide an asymptotic analysis of our test-statistic, and\ndemonstrate in experiments that our test obtains better power than existing\napproaches, while being more computationally efficient.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 22:42:45 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Fern\u00e1ndez", "Tamara", ""], ["Xu", "Wenkai", ""], ["Ditzhaus", "Marc", ""], ["Gretton", "Arthur", ""]]}, {"id": "2011.09029", "submitter": "Zhaoxing Gao", "authors": "Zhaoxing Gao, Ruey S. Tsay", "title": "A Two-Way Transformed Factor Model for Matrix-Variate Time Series", "comments": "49 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for modeling high-dimensional matrix-variate time\nseries by a two-way transformation, where the transformed data consist of a\nmatrix-variate factor process, which is dynamically dependent, and three other\nblocks of white noises. Specifically, for a given $p_1\\times p_2$\nmatrix-variate time series, we seek common nonsingular transformations to\nproject the rows and columns onto another $p_1$ and $p_2$ directions according\nto the strength of the dynamic dependence of the series on the past values.\nConsequently, we treat the data as nonsingular linear row and column\ntransformations of dynamically dependent common factors and white noise\nidiosyncratic components. We propose a common orthonormal projection method to\nestimate the front and back loading matrices of the matrix-variate factors.\nUnder the setting that the largest eigenvalues of the covariance of the\nvectorized idiosyncratic term diverge for large $p_1$ and $p_2$, we introduce a\ntwo-way projected Principal Component Analysis (PCA) to estimate the associated\nloading matrices of the idiosyncratic terms to mitigate such diverging noise\neffects. A diagonal-path white noise testing procedure is proposed to estimate\nthe order of the factor matrix. %under the assumption that the idiosyncratic\nterm is a matrix-variate white noise process. Asymptotic properties of the\nproposed method are established for both fixed and diverging dimensions as the\nsample size increases to infinity. We use simulated and real examples to assess\nthe performance of the proposed method. We also compare our method with some\nexisting ones in the literature and find that the proposed approach not only\nprovides interpretable results but also performs well in out-of-sample\nforecasting.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 01:28:28 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Gao", "Zhaoxing", ""], ["Tsay", "Ruey S.", ""]]}, {"id": "2011.09033", "submitter": "David Kline", "authors": "David Kline, Zehang Li, Yue Chu, Jon Wakefield, William C. Miller,\n  Abigail Norris Turner, Samuel J Clark", "title": "Estimating Seroprevalence of SARS-CoV-2 in Ohio: A Bayesian Multilevel\n  Poststratification Approach with Multiple Diagnostic Tests", "comments": null, "journal-ref": "PNAS June 29, 2021 118 (26) e2023947118", "doi": "10.1073/pnas.2023947118", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Globally the SARS-CoV-2 coronavirus has infected more than 59 million people\nand killed more than 1.39 million. Designing and monitoring interventions to\nslow and stop the spread of the virus require knowledge of how many people have\nbeen and are currently infected, where they live, and how they interact. The\nfirst step is an accurate assessment of the population prevalence of past\ninfections. There are very few population-representative prevalence studies of\nthe SARS-CoV-2 coronavirus, and only two American states -- Indiana and\nConnecticut -- have reported probability-based sample surveys that characterize\nstate-wide prevalence of the SARS-CoV-2 coronavirus. One of the difficulties is\nthe fact that the tests to detect and characterize SARS-CoV-2 coronavirus\nantibodies are new, not well characterized, and generally function poorly.\nDuring July, 2020, a survey representing all adults in the State of Ohio in the\nUnited States collected biomarkers and information on protective behavior\nrelated to the SARS-CoV-2 coronavirus. Several features of the survey make it\ndifficult to estimate past prevalence: 1) a low response rate, 2) very low\nnumber of positive cases, and 3) the fact that multiple, poor quality\nserological tests were used to detect SARS-CoV-2 antibodies. We describe a new\nBayesian approach for analyzing the biomarker data that simultaneously\naddresses these challenges and characterizes the potential effect of selective\nresponse. The model does not require survey sample weights, accounts for\nmultiple, imperfect antibody test results, and characterizes uncertainty\nrelated to the sample survey and the multiple, imperfect, potentially\ncorrelated tests.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 01:50:04 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 15:27:06 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kline", "David", ""], ["Li", "Zehang", ""], ["Chu", "Yue", ""], ["Wakefield", "Jon", ""], ["Miller", "William C.", ""], ["Turner", "Abigail Norris", ""], ["Clark", "Samuel J", ""]]}, {"id": "2011.09070", "submitter": "Sudipta Bhattacharya", "authors": "Sudipta Bhattacharya and Jyotirmoy Dey", "title": "Assessing contribution of treatment phases through tipping point\n  analyses using rank preserving structural failure time models", "comments": "33 pages, 6 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In clinical trials, an experimental treatment is sometimes added on to a\nstandard of care or control therapy in multiple treatment phases (e.g.,\nconcomitant and maintenance phases) to improve patient outcomes. When the new\nregimen provides meaningful benefit over the control therapy in such cases, it\nproves difficult to separately assess the contribution of each phase to the\noverall effect observed. This article provides an approach for assessing the\nimportance of a specific treatment phase in such a situation through tipping\npoint analyses of a time-to-event endpoint using\nrank-preserving-structural-failure-time (RPSFT) modeling. A tipping-point\nanalysis is commonly used in situations where it is suspected that a\nstatistically significant difference between treatment arms could be a result\nof missing or unobserved data instead of a real treatment effect.\nRank-preserving-structural-failure-time modeling is an approach for causal\ninference that is typically used to adjust for treatment switching in clinical\ntrials with time to event endpoints. The methodology proposed in this article\nis an amalgamation of these two ideas to investigate the contribution of a\ntreatment phase of interest to the effect of a regimen comprising multiple\ntreatment phases. We provide two different variants of the method corresponding\nto two different effects of interest. We provide two different tipping point\nthresholds depending on inferential goals. The proposed approaches are\nmotivated and illustrated with data from a recently concluded, real-life phase\n3 cancer clinical trial. We then conclude with several considerations and\nrecommendations.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 03:39:43 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Bhattacharya", "Sudipta", ""], ["Dey", "Jyotirmoy", ""]]}, {"id": "2011.09248", "submitter": "Davide Biancalana", "authors": "Fabio Baione, Davide Biancalana, Paolo De Angelis", "title": "An application of Zero-One Inflated Beta regression models for\n  predicting health insurance reimbursement", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In actuarial practice the dependency between contract limitations\n(deductibles, copayments) and health care expenditures are measured by the\napplication of the Monte Carlo simulation technique. We propose, for the same\ngoal, an alternative approach based on Generalized Linear Model for Location,\nScale and Shape (GAMLSS). We focus on the estimate of the ratio between the\none-year reimbursement amount (after the effect of limitations) and the one\nyear expenditure (before the effect of limitations). We suggest a regressive\nmodel to investigate the relation between this response variable and a set of\ncovariates, such as limitations and other rating factors related to health\nrisk. In this way a dependency structure between reimbursement and limitations\nis provided. The density function of the ratio is a mixture distribution,\nindeed it can continuously assume values mass at 0 and 1, in addition to the\nprobability density within (0, 1) . This random variable does not belong to the\nexponential family, then an ordinary Generalized Linear Model is not suitable.\nGAMLSS introduces a probability structure compliant with the density of the\nresponse variable, in particular zero-one inflated beta density is assumed. The\nlatter is a mixture between a Bernoulli distribution and a Beta distribution.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 12:43:22 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Baione", "Fabio", ""], ["Biancalana", "Davide", ""], ["De Angelis", "Paolo", ""]]}, {"id": "2011.09362", "submitter": "Owais Sarwar", "authors": "Owais Sarwar and Benjamin Sauk and Nikolaos V. Sahinidis", "title": "A Discussion on Practical Considerations with Sparse Regression\n  Methodologies", "comments": "12 pages", "journal-ref": "Statist. Sci. Volume 35, Number 4 (2020), 593-601", "doi": "10.1214/20-STS806", "report-no": null, "categories": "cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse linear regression is a vast field and there are many different\nalgorithms available to build models. Two new papers published in Statistical\nScience study the comparative performance of several sparse regression\nmethodologies, including the lasso and subset selection. Comprehensive\nempirical analyses allow the researchers to demonstrate the relative merits of\neach estimator and provide guidance to practitioners. In this discussion, we\nsummarize and compare the two studies and we examine points of agreement and\ndivergence, aiming to provide clarity and value to users. The authors have\nstarted a highly constructive dialogue, our goal is to continue it.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 15:58:35 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 19:43:24 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Sarwar", "Owais", ""], ["Sauk", "Benjamin", ""], ["Sahinidis", "Nikolaos V.", ""]]}, {"id": "2011.09437", "submitter": "Haoxuan Wu", "authors": "Haoxuan Wu, David S. Matteson", "title": "Adaptive Bayesian Changepoint Analysis and Local Outlier Scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce global-local shrinkage priors into a Bayesian dynamic linear\nmodel to adaptively estimate both changepoints and local outliers in a novel\nmodel we call Adaptive Bayesian Changepoints with Outliers (ABCO). We utilize a\nstate-space approach to identify a dynamic signal in the presence of outliers\nand measurement error with stochastic volatility. We find that global state\nequation parameters are inadequate for most real applications and we include\nlocal parameters to track noise at each time-step. This setup provides a\nflexible framework to detect unspecified changepoints in complex series, such\nas those with large interruptions in local trends, with robustness to outliers\nand heteroskedastic noise. ABCO may also be used as a robust Bayesian trend\nfilter that can reconstruct interrupted time series. We detail the extension of\nour approach to time-varying parameter estimation within dynamic regression\nanalysis to identify structural breaks. Finally, we compare our algorithm\nagainst several alternatives to demonstrate its efficacy in diverse simulation\nscenarios and two empirical examples.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:14:58 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 22:13:31 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Wu", "Haoxuan", ""], ["Matteson", "David S.", ""]]}, {"id": "2011.09462", "submitter": "Tijana Zrnic", "authors": "Tijana Zrnic, Michael I. Jordan", "title": "Post-Selection Inference via Algorithmic Stability", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern approaches to data analysis make extensive use of data-driven model\nselection. The resulting dependencies between the selected model and data used\nfor inference invalidate statistical guarantees derived from classical\ntheories. The framework of post-selection inference (PoSI) has formalized this\nproblem and proposed corrections which ensure valid inferences. Yet, obtaining\ngeneral principles that enable computationally-efficient, powerful PoSI\nmethodology with formal guarantees remains a challenge. With this goal in mind,\nwe revisit the PoSI problem through the lens of algorithmic stability. Under an\nappropriate formulation of stability---one that captures closure under\npost-processing and compositionality properties---we show that stability\nparameters of a selection method alone suffice to provide non-trivial\ncorrections to classical z-test and t-test intervals. Then, for several popular\nmodel selection methods, including the LASSO, we show how stability can be\nachieved through simple, computationally efficient randomization schemes. Our\nalgorithms offer provable unconditional simultaneous coverage and are\ncomputationally efficient; in particular, they do not rely on MCMC sampling.\nImportantly, our proposal explicitly relates the magnitude of randomization to\nthe resulting confidence interval width, allowing the analyst to tune interval\nwidth to the loss in utility due to randomizing selection.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:40:25 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Zrnic", "Tijana", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2011.09537", "submitter": "Trang Nguyen", "authors": "Trang Quynh Nguyen, Ian Schmid, Elizabeth L. Ogburn, Elizabeth A.\n  Stuart", "title": "Clarifying causal mediation analysis for the applied researcher: Effect\n  identification via three assumptions and five potential outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis is complicated with multiple effect definitions\nthat require different sets of assumptions for identification. This paper\nprovides a systematic explanation of such assumptions. We define five potential\noutcome types whose means are involved in various effect definitions. We tackle\ntheir mean/distribution's identification, starting with the one that requires\nthe weakest assumptions and gradually building up to the one that requires the\nstrongest assumptions. This presentation shows clearly why an assumption is\nrequired for one estimand and not another, and provides a succinct table from\nwhich an applied researcher could pick out the assumptions required for\nidentifying the causal effects they target. Using a running example, the paper\nillustrates the assembling and consideration of identifying assumptions for a\nrange of causal contrasts. For several that are commonly encountered in the\nliterature, this exercise clarifies that identification requires weaker\nassumptions than those often stated in the literature. This attention to the\ndetails also draws attention to the differences in the positivity assumption\nfor different estimands, with practical implications. Clarity on the\nidentifying assumptions of these various estimands will help researchers\nconduct appropriate mediation analyses and interpret the results with\nappropriate caution given the plausibility of the assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 20:39:17 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Nguyen", "Trang Quynh", ""], ["Schmid", "Ian", ""], ["Ogburn", "Elizabeth L.", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "2011.09544", "submitter": "Alexander Foss", "authors": "Alexander H. Foss, Richard B. Lehoucq, W. Zachary Stuart, J. Derek\n  Tucker, Jonathan W. Berry", "title": "A Deterministic Hitting-Time Moment Approach to Seed-set Expansion over\n  a Graph", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce HITMIX, a new technique for network seed-set expansion, i.e.,\nthe problem of identifying a set of graph vertices related to a given seed-set\nof vertices. We use the moments of the graph's hitting-time distribution to\nquantify the relationship of each non-seed vertex to the seed-set. This\ninvolves a deterministic calculation for the hitting-time moments that is\nscalable in the number of graph edges and so avoids directly sampling a Markov\nchain over the graph. The moments are used to fit a mixture model to estimate\nthe probability that each non-seed vertex should be grouped with the seed set.\nThis membership probability enables us to sort the non-seeds and threshold in a\nstatistically-justified way. To the best of our knowledge, HITMIX is the first\nfull statistical model for seed-set expansion that can give vertex-level\nmembership probabilities. While HITMIX is a global method, its linear\ncomputation complexity in practice enables computations on large graphs. We\nhave a high-performance implementation, and we present computational results on\nstochastic blockmodels and a small-world network from the SNAP repository. The\nstate of the art in this problem is a collection of recently developed local\nmethods, and we show that distinct advantages in solution quality are available\nif our global method can be used. In practice, we expect to be able to run\nHITMIX if the graph can be stored in memory.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 20:54:10 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Foss", "Alexander H.", ""], ["Lehoucq", "Richard B.", ""], ["Stuart", "W. Zachary", ""], ["Tucker", "J. Derek", ""], ["Berry", "Jonathan W.", ""]]}, {"id": "2011.09558", "submitter": "Kelly Ramsay", "authors": "Kelly Ramsay, Shoja'eddin Chenouri", "title": "Robust multiple change-point detection for covariance matrices using\n  data depth", "comments": "27 pages before references, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, two robust, nonparametric methods for multiple change-point\ndetection in the covariance matrix of a multivariate sequence of observations\nare introduced. We demonstrate that changes in ranks generated from data depth\nfunctions can be used to detect certain types of changes in the covariance\nmatrix of a sequence of observations. In order to detect more than one change,\nthe first algorithm uses methods similar to that of wild-binary segmentation.\nThe second algorithm estimates change-points by maximizing a penalized version\nof the classical Kruskal Wallis ANOVA test statistic. We show that this\nobjective function can be maximized via the well-known PELT algorithm. Under\nmild, nonparametric assumptions both of these algorithms are shown to be\nconsistent for the correct number of change-points and the correct location(s)\nof the change-point(s). We demonstrate the efficacy of these methods with a\nsimulation study, where we compare our new methods to an competing method. We\nare able to estimate changes accurately when the data is heavy tailed or\nskewed. We are also able to detect second order change-points in a time series\nof multivariate financial returns, without first imposing a time series model\non the data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 22:07:20 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 17:40:46 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Ramsay", "Kelly", ""], ["Chenouri", "Shoja'eddin", ""]]}, {"id": "2011.09569", "submitter": "Xiang Zhou", "authors": "Xiang Zhou", "title": "Some Doubly and Multiply Robust Estimators of Controlled Direct Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This letter introduces several doubly, triply, and quadruply robust\nestimators of the controlled direct effect. Among them, the triply and\nquadruply robust estimators are locally semiparametric efficient, and well\nsuited to the use of data-adaptive methods for estimating their nuisance\nfunctions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 22:38:00 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Zhou", "Xiang", ""]]}, {"id": "2011.09682", "submitter": "Sabrina Enriquez", "authors": "Sabrina Enriquez, Fushing Hsieh", "title": "Categorical exploratory data analysis on goodness-of-fit issues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If the aphorism \"All models are wrong\"- George Box, continues to be true in\ndata analysis, particularly when analyzing real-world data, then we should\nannotate this wisdom with visible and explainable data-driven patterns. Such\nannotations can critically shed invaluable light on validity as well as\nlimitations of statistical modeling as a data analysis approach. In an effort\nto avoid holding our real data to potentially unattainable or even unrealistic\ntheoretical structures, we propose to utilize the data analysis paradigm called\nCategorical Exploratory Data Analysis (CEDA). We illustrate the merits of this\nproposal with two real-world data sets from the perspective of goodness-of-fit.\nIn both data sets, the Normal distribution's bell shape seemingly fits rather\nwell by first glance. We apply CEDA to bring out where and how each data fits\nor deviates from the model shape via several important distributional aspects.\nWe also demonstrate that CEDA affords a version of tree-based p-value, and\ncompare it with p-values based on traditional statistical approaches. Along our\ndata analysis, we invest computational efforts in making graphic display to\nilluminate the advantages of using CEDA as one primary way of data analysis in\nData Science education.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 06:11:06 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 01:41:15 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Enriquez", "Sabrina", ""], ["Hsieh", "Fushing", ""]]}, {"id": "2011.09734", "submitter": "Hanzhong Liu", "authors": "Hanzhong Liu, Fuyi Tu, Wei Ma", "title": "A general theory of regression adjustment for covariate-adaptive\n  randomization: OLS, Lasso, and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of estimating and inferring treatment effects in\nrandomized experiments. In practice, stratified randomization, or more\ngenerally, covariate-adaptive randomization, is routinely used in the design\nstage to balance the treatment allocations with respect to a few variables that\nare most relevant to the outcomes. Then, regression is performed in the\nanalysis stage to adjust the remaining imbalances to yield more efficient\ntreatment effect estimators. Building upon and unifying the recent results\nobtained for ordinary least squares adjusted estimators under\ncovariate-adaptive randomization, this paper presents a general theory of\nregression adjustment that allows for arbitrary model misspecification and the\npresence of a large number of baseline covariates. We exemplify the theory on\ntwo Lasso-adjusted treatment effect estimators, both of which are optimal in\ntheir respective classes. In addition, nonparametric consistent variance\nestimators are proposed to facilitate valid inferences, which work irrespective\nof the specific randomization methods used. The robustness and improved\nefficiency of the proposed estimators are demonstrated through a simulation\nstudy and a clinical trial example. This study sheds light on improving\ntreatment effect estimation efficiency by implementing machine learning methods\nin covariate-adaptive randomized experiments.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 09:19:56 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Liu", "Hanzhong", ""], ["Tu", "Fuyi", ""], ["Ma", "Wei", ""]]}, {"id": "2011.09745", "submitter": "Osama Idais", "authors": "Osama Idais and Rainer Schwabe", "title": "In- and Equivariance for Optimal Designs in Generalized Linear Models:\n  The Gamma Model", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give an overview over the usefulness of the concept of equivariance and\ninvariance in the design of experiments for generalized linear models. In\ncontrast to linear models here pairs of transformations have to be considered\nwhich act simultaneously on the experimental settings and on the location\nparameters in the linear component. Given the transformation of the\nexperimental settings the parameter transformations are not unique and may be\nnonlinear to make further use of the model structure. The general concepts and\nresults are illustrated by models with gamma distributed response. Locally\noptimal and maximin efficient design are obtained for the common D- and\nIMSE-criterion.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 09:48:27 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Idais", "Osama", ""], ["Schwabe", "Rainer", ""]]}, {"id": "2011.09794", "submitter": "Yi-Jheng Lin", "authors": "Yi-Jheng Lin, Che-Hao Yu, Tzu-Hsuan Liu, Cheng-Shang Chang, and\n  Wen-Tsuen Chen", "title": "Positively Correlated Samples Save Pooled Testing Costs", "comments": "14 pages, 8 figures, submitted for publication", "journal-ref": null, "doi": "10.1109/TNSE.2021.3081759", "report-no": null, "categories": "stat.ME cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The group testing approach that achieves significant cost reduction over the\nindividual testing approach has received a lot of interest lately for massive\ntesting of COVID-19. Many studies simply assume samples mixed in a group are\nindependent. However, this assumption may not be reasonable for a contagious\ndisease like COVID-19. Specifically, people within a family tend to infect each\nother and thus are likely to be positively correlated. By exploiting positive\ncorrelation, we make the following two main contributions. One is to provide a\nrigorous proof that further cost reduction can be achieved by using the Dorfman\ntwo-stage method when samples within a group are positively correlated. The\nother is to propose a hierarchical agglomerative algorithm for pooled testing\nwith a social graph, where an edge in the social graph connects frequent social\ncontacts between two persons. Such an algorithm leads to notable cost reduction\n(roughly 20%-35%) compared to random pooling when the Dorfman two-stage\nalgorithm is applied.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 12:39:54 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 07:58:32 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Lin", "Yi-Jheng", ""], ["Yu", "Che-Hao", ""], ["Liu", "Tzu-Hsuan", ""], ["Chang", "Cheng-Shang", ""], ["Chen", "Wen-Tsuen", ""]]}, {"id": "2011.09815", "submitter": "Lijing Lin", "authors": "Lijing Lin, Matthew Sperrin, David A. Jenkins, Glen P. Martin, Niels\n  Peek", "title": "A scoping review of causal methods enabling predictions under\n  hypothetical interventions", "comments": null, "journal-ref": "Diagnostic and Prognostic Research, 2021", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Aims: The methods with which prediction models are usually\ndeveloped mean that neither the parameters nor the predictions should be\ninterpreted causally. However, when prediction models are used to support\ndecision making, there is often a need for predicting outcomes under\nhypothetical interventions. We aimed to identify published methods for\ndeveloping and validating prediction models that enable risk estimation of\noutcomes under hypothetical interventions, utilizing causal inference: their\nmain methodological approaches, underlying assumptions, targeted estimands, and\npotential pitfalls and challenges with using the method, and unresolved\nmethodological challenges.\n  Methods: We systematically reviewed literature published by December 2019,\nconsidering papers in the health domain that used causal considerations to\nenable prediction models to be used for predictions under hypothetical\ninterventions.\n  Results: We identified 4919 papers through database searches and a further\n115 papers through manual searches, of which 13 were selected for inclusion,\nfrom both the statistical and the machine learning literature. Most of the\nidentified methods for causal inference from observational data were based on\nmarginal structural models and g-estimation.\n  Conclusions: There exist two broad methodological approaches for allowing\nprediction under hypothetical intervention into clinical prediction models: 1)\nenriching prediction models derived from observational studies with estimated\ncausal effects from clinical trials and meta-analyses; and 2) estimating\nprediction models and causal effects directly from observational data. These\nmethods require extending to dynamic treatment regimes, and consideration of\nmultiple interventions to operationalise a clinical decision support system.\nTechniques for validating 'causal prediction models' are still in their\ninfancy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 13:36:26 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 15:31:04 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lin", "Lijing", ""], ["Sperrin", "Matthew", ""], ["Jenkins", "David A.", ""], ["Martin", "Glen P.", ""], ["Peek", "Niels", ""]]}, {"id": "2011.09829", "submitter": "Ruoyu Wang", "authors": "Ruoyu Wang and Qihua Wang and Wang Miao and Xiaohua Zhou", "title": "Sharp bounds for variance of treatment effect estimators in the finite\n  population in the presence of covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the completely randomized experiment, the variances of treatment effect\nestimators in the finite population are usually not identifiable and hence not\nestimable. Although some estimable bounds of the variances have been\nestablished in the literature, few of them are derived in the presence of\ncovariates. In this paper, the difference-in-means estimator and the Wald\nestimator are considered in the completely randomized experiment with perfect\ncompliance and noncompliance, respectively. Sharp bounds for the variances of\nthese two estimators are established when covariates are available.\nFurthermore, consistent estimators for such bounds are obtained, which can be\nused to shorten the confidence intervals and improve power of tests.\nSimulations were conducted to evaluate the proposed methods. The proposed\nmethods are also illustrated with two real data analyses.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 14:08:57 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Wang", "Ruoyu", ""], ["Wang", "Qihua", ""], ["Miao", "Wang", ""], ["Zhou", "Xiaohua", ""]]}, {"id": "2011.09838", "submitter": "Matthew Norris", "authors": "Matthew Norris", "title": "Using Ordinal Data to Assess Distance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is some disagreement on whether Likert scale data should be treated as\nordinal or continuous. This paper treats Likert data as ordinal, uses\nnon-parametric hypothesis testing, and clustering to validate those variables\nthat have significant results from hypothesis testing.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 02:08:28 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Norris", "Matthew", ""]]}, {"id": "2011.09912", "submitter": "Uwe Aickelin", "authors": "Xuetong Wu, Hadi Akbarzadeh Khorshidi, Uwe Aickelin, Zobaida Edib,\n  Michelle Peate", "title": "Imputation techniques on missing values in breast cancer treatment and\n  fertility data", "comments": "Health Information Science and Systems, Volume 7, Issue 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clinical decision support using data mining techniques offers more\nintelligent way to reduce the decision error in the last few years. However,\nclinical datasets often suffer from high missingness, which adversely impacts\nthe quality of modelling if handled improperly. Imputing missing values\nprovides an opportunity to resolve the issue. Conventional imputation methods\nadopt simple statistical analysis, such as mean imputation or discarding\nmissing cases, which have many limitations and thus degrade the performance of\nlearning. This study examines a series of machine learning based imputation\nmethods and suggests an efficient approach to in preparing a good quality\nbreast cancer (BC) dataset, to find the relationship between BC treatment and\nchemotherapy-related amenorrhoea, where the performance is evaluated with the\naccuracy of the prediction.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 06:28:26 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Wu", "Xuetong", ""], ["Khorshidi", "Hadi Akbarzadeh", ""], ["Aickelin", "Uwe", ""], ["Edib", "Zobaida", ""], ["Peate", "Michelle", ""]]}, {"id": "2011.10157", "submitter": "Erin Craig", "authors": "Erin Craig, Donald A Redelmeier and Robert J Tibshirani", "title": "Finding and assessing treatment effect sweet spots in clinical trial\n  data", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying heterogeneous treatment effects (HTEs) in randomized controlled\ntrials is an important step toward understanding and acting on trial results.\nHowever, HTEs are often small and difficult to identify, and HTE modeling\nmethods which are very general can suffer from low power. We present a method\nthat exploits any existing relationship between illness severity and treatment\neffect, and identifies the \"sweet spot\", the contiguous range of illness\nseverity where the estimated treatment benefit is maximized. We further compute\na bias-corrected estimate of the conditional average treatment effect (CATE) in\nthe sweet spot, and a $p$-value. Because we identify a single sweet spot and\n$p$-value, we believe our method to be straightforward to interpret and\nactionable: results from our method can inform future clinical trials and help\nclinicians make personalized treatment recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 00:24:17 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 15:17:26 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Craig", "Erin", ""], ["Redelmeier", "Donald A", ""], ["Tibshirani", "Robert J", ""]]}, {"id": "2011.10195", "submitter": "Chen Yang", "authors": "Ning Sun, Chen Yang, Ri\\v{c}ardas Zitikis", "title": "Detecting systematic anomalies affecting systems when inputs are\n  stationary time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop an anomaly-detection method when systematic anomalies, possibly\nstatistically very similar to genuine inputs, are affecting control systems at\nthe input and/or output stages. The method allows anomaly-free inputs (i.e.,\nthose before contamination) to originate from a wide class of random sequences,\nthus opening up possibilities for diverse applications. To illustrate how the\nmethod works on data, and how to interpret its results and make decisions, we\nanalyze several actual time series, which are originally non-stationary but in\nthe process of analysis are converted into stationary. As a further\nillustration, we provide a controlled experiment with anomaly-free inputs\nfollowing an ARMA time series model under various contamination scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 04:00:06 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 16:33:12 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 19:02:46 GMT"}, {"version": "v4", "created": "Mon, 12 Jul 2021 19:23:13 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Sun", "Ning", ""], ["Yang", "Chen", ""], ["Zitikis", "Ri\u010dardas", ""]]}, {"id": "2011.10240", "submitter": "Jiaqi Gu", "authors": "Jiaqi Gu, Yiwei Fan, and Guosheng Yin", "title": "Reconstruct Kaplan--Meier Estimator as M-estimator and Its Confidence\n  Band", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kaplan--Meier (KM) estimator, which provides a nonparametric estimate of\na survival function for time-to-event data, has wide application in clinical\nstudies, engineering, economics and other fields. The theoretical properties of\nthe KM estimator including its consistency and asymptotic distribution have\nbeen extensively studied. We reconstruct the KM estimator as an M-estimator by\nmaximizing a quadratic M-function based on concordance, which can be computed\nusing the expectation--maximization (EM) algorithm. It is shown that the\nconvergent point of the EM algorithm coincides with the traditional KM\nestimator, offering a new interpretation of the KM estimator as an M-estimator.\nTheoretical properties including the large-sample variance and limiting\ndistribution of the KM estimator are established using M-estimation theory.\nSimulations and application on two real datasets demonstrate that the proposed\nM-estimator is exactly equivalent to the KM estimator, while the confidence\ninterval and band can be derived as well.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 07:18:09 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Gu", "Jiaqi", ""], ["Fan", "Yiwei", ""], ["Yin", "Guosheng", ""]]}, {"id": "2011.10398", "submitter": "Rowan Iskandar", "authors": "Rowan Iskandar", "title": "Probability bound analysis: A novel approach for quantifying parameter\n  uncertainty in decision-analytic modeling and cost-effectiveness analysis", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decisions about health interventions are often made using limited evidence.\nMathematical models used to inform such decisions often include uncertainty\nanalysis to account for the effect of uncertainty in the current evidence-base\non decisional-relevant quantities. However, current uncertainty quantification\nmethodologies require modelers to specify a precise probability distribution to\nrepresent the uncertainty of a model parameter. This study introduces a novel\napproach for propagating parameter uncertainty, probability bounds analysis\n(PBA), where the uncertainty about the unknown probability distribution of a\nmodel parameter is expressed in terms of an interval bounded by lower and upper\nbounds on the cumulative distribution function (p-box) and without assuming a\nparticular form of the distribution function. We give the formulas of the\np-boxes for common situations (given combinations of data on minimum, maximum,\nmedian, mean, or standard deviation), describe an approach to propagate p-boxes\ninto a black-box mathematical model, and introduce an approach for\ndecision-making based on the results of PBA. Then, we demonstrate an\napplication of PBA using a case study. In sum, this study will provide modelers\nwith tools to conduct parameter uncertainty quantification given constraints of\navailable data with the fewest number of assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 13:28:30 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Iskandar", "Rowan", ""]]}, {"id": "2011.10414", "submitter": "Ting Wang", "authors": "Ting Wang and Benjamin Graves and Yves Rosseel and Edgar C. Merkle", "title": "Computation and application of generalized linear mixed model\n  derivatives using lme4", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Maximum likelihood estimation of generalized linear mixed models(GLMMs) is\ndifficult due to marginalization of the random effects. Computing derivatives\nof a fitted GLMM's likelihood (with respect to model parameters) is also\ndifficult, especially because the derivatives are not by-products of popular\nestimation algorithms. In this paper, we describe GLMM derivatives along with a\nquadrature method to efficiently compute them, focusing on lme4 models with a\nsingle clustering variable. We describe how psychometric results related to IRT\nare helpful for obtaining these derivatives, as well as for verifying the\nderivatives' accuracies. After describing the derivative computation methods,\nwe illustrate the many possible uses of these derivatives, including robust\nstandard errors, score tests of fixed effect parameters, and likelihood ratio\ntests of non-nested models. The derivative computation methods and applications\ndescribed in the paper are all available in easily-obtained R packages.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 14:09:53 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Wang", "Ting", ""], ["Graves", "Benjamin", ""], ["Rosseel", "Yves", ""], ["Merkle", "Edgar C.", ""]]}, {"id": "2011.10522", "submitter": "James Dawber", "authors": "James Dawber, Nicola Salvati, Timo Schmid and Nikos Tzavidis", "title": "Scale estimation and data-driven tuning constant selection for\n  M-quantile regression", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  M-quantile regression is a general form of quantile-like regression which\nusually utilises the Huber influence function and corresponding tuning\nconstant. Estimation requires a nuisance scale parameter to ensure the\nM-quantile estimates are scale invariant, with several scale estimators having\npreviously been proposed. In this paper we assess these scale estimators and\nevaluate their suitability, as well as proposing a new scale estimator based on\nthe method of moments. Further, we present two approaches for estimating\ndata-driven tuning constant selection for M-quantile regression. The tuning\nconstants are obtained by i) minimising the estimated asymptotic variance of\nthe regression parameters and ii) utilising an inverse M-quantile function to\nreduce the effect of outlying observations. We investigate whether data-driven\ntuning constants, as opposed to the usual fixed constant, for instance, at\nc=1.345, can improve the efficiency of the estimators of M-quantile regression\nparameters. The performance of the data-driven tuning constant is investigated\nin different scenarios using model-based simulations. Finally, we illustrate\nthe proposed methods using a European Union Statistics on Income and Living\nConditions data set.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 17:28:45 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Dawber", "James", ""], ["Salvati", "Nicola", ""], ["Schmid", "Timo", ""], ["Tzavidis", "Nikos", ""]]}, {"id": "2011.10545", "submitter": "Evaldas Vaiciukynas Dr.", "authors": "Evaldas Vaiciukynas, Paulius Danenas, Vilius Kontrimas, Rimantas\n  Butleris", "title": "Meta-Learning for Time Series Forecasting Ensemble", "comments": "Submitted for review to Journal of Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amounts of historical data collected increase together with business\nintelligence applicability and demands for automatic forecasting of time\nseries. While no single time series modeling method is universal to all types\nof dynamics, forecasting using ensemble of several methods is often seen as a\ncompromise. Instead of fixing ensemble diversity and size we propose to\nadaptively predict these aspects using meta-learning. Meta-learning here\nconsiders two separate random forest regression models, built on 390 time\nseries features, to rank 22 univariate forecasting methods and to recommend\nensemble size. Forecasting ensemble is consequently formed from methods ranked\nas the best and forecasts are pooled using either simple or weighted average\n(with weight corresponding to reciprocal rank). Proposed approach was tested on\n12561 micro-economic time series (expanded to 38633 for various forecasting\nhorizons) of M4 competition where meta-learning outperformed Theta and Comb\nbenchmarks by relative forecasting errors for all data types and horizons. Best\noverall results were achieved by weighted pooling with symmetric mean absolute\npercentage error of 9.21% versus 11.05% obtained using Theta method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 18:35:02 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Vaiciukynas", "Evaldas", ""], ["Danenas", "Paulius", ""], ["Kontrimas", "Vilius", ""], ["Butleris", "Rimantas", ""]]}, {"id": "2011.10576", "submitter": "Graciela Boente Prof.", "authors": "Graciela Boente and Nadia Kudraszow", "title": "Robust smoothed canonical correlation analysis for functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides robust estimators for the first canonical correlation and\ndirections of random elements on Hilbert separable spaces by using robust\nassociation and scale measures combined with basis expansion and/or\npenalizations as a regularization tool. Under regularity conditions, the\nresulting estimators are consistent.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 15:36:53 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Boente", "Graciela", ""], ["Kudraszow", "Nadia", ""]]}, {"id": "2011.10715", "submitter": "Kyungmin Kim", "authors": "Seungjae Jung, Kyung-Min Kim, Hanock Kwak and Young-Jin Park", "title": "A Worrying Analysis of Probabilistic Time-series Models for Sales\n  Forecasting", "comments": "NeurIPS 2020 workshop (I Can't Believe It's Not Better,\n  ICBINB@NeurIPS 2020). All authors contributed equally to this research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic time-series models become popular in the forecasting field as\nthey help to make optimal decisions under uncertainty. Despite the growing\ninterest, a lack of thorough analysis hinders choosing what is worth applying\nfor the desired task. In this paper, we analyze the performance of three\nprominent probabilistic time-series models for sales forecasting. To remove the\nrole of random chance in architecture's performance, we make two experimental\nprinciples; 1) Large-scale dataset with various cross-validation sets. 2) A\nstandardized training and hyperparameter selection. The experimental results\nshow that a simple Multi-layer Perceptron and Linear Regression outperform the\nprobabilistic models on RMSE without any feature engineering. Overall, the\nprobabilistic models fail to achieve better performance on point estimation,\nsuch as RMSE and MAPE, than comparably simple baselines. We analyze and discuss\nthe performances of probabilistic time-series models.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 03:31:23 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Jung", "Seungjae", ""], ["Kim", "Kyung-Min", ""], ["Kwak", "Hanock", ""], ["Park", "Young-Jin", ""]]}, {"id": "2011.10720", "submitter": "Roland Matsouaka", "authors": "Roland A. Matsouaka and Adrian Coles", "title": "Robust statistical inference for the matched net benefit and the matched\n  win ratio using prioritized composite endpoints", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As alternatives to the time-to-first-event analysis of composite endpoints,\nthe {\\it net benefit} (NB) and the {\\it win ratio} (WR) -- which assess\ntreatment effects using prioritized component outcomes based on clinical\nimportance -- have been proposed. However, statistical inference of NB and WR\nrelies on a large-sample assumptions, which can lead to an invalid test\nstatistic and inadequate, unsatisfactory confidence intervals, especially when\nthe sample size is small or the proportion of wins is near 0 or 1.\n  In this paper, we develop a systematic approach to address these limitations\nin a paired-sample design. We first introduce a new test statistic under the\nnull hypothesis of no treatment difference. Then, we present the formula to\ncalculate the sample size. Finally, we develop the confidence interval\nestimations of these two estimators. To estimate the confidence intervals, we\nuse the {\\it method of variance estimates recovery} (MOVER), that combines two\nseparate individual-proportion confidence intervals into a hybrid interval for\nthe estimand of interest. We assess the performance of the proposed test\nstatistic and MOVER confidence interval estimations through simulation studies.\n  We demonstrate that the MOVER confidence intervals are as good as the\nlarge-sample confidence intervals when the sample is large and when the\nproportions of wins is bounded away from 0 and 1. Moreover, the MOVER intervals\noutperform their competitors when the sample is small or the proportions are at\nor near the boundaries 0 and 1. We illustrate the method (and its competitors)\nusing three examples from randomized clinical studies.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 04:54:38 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Matsouaka", "Roland A.", ""], ["Coles", "Adrian", ""]]}, {"id": "2011.10863", "submitter": "Mengyang Gu", "authors": "Mengyang Gu and Hanmo Li", "title": "Gaussian orthogonal latent factor processes for large incomplete\n  matrices of correlated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Gaussian orthogonal latent factor processes for modeling and\npredicting large correlated data. To handle the computational challenge, we\nfirst decompose the likelihood function of the Gaussian random field with a\nmulti-dimensional input domain into a product of densities at the orthogonal\ncomponents with lower-dimensional inputs. The continuous-time Kalman filter is\nimplemented to compute the likelihood function efficiently without making\napproximations. We also show that the posterior distribution of the factor\nprocesses is independent, as a consequence of prior independence of factor\nprocesses and orthogonal factor loading matrix. For studies with large sample\nsizes, we propose a flexible way to model the mean, and we derive the marginal\nposterior distribution to solve identifiability issues in sampling these\nparameters. Both simulated and real data applications confirm the outstanding\nperformance of this method.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 20:39:38 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 19:28:38 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Gu", "Mengyang", ""], ["Li", "Hanmo", ""]]}, {"id": "2011.11023", "submitter": "Silvia Noirjean", "authors": "Silvia Noirjean, Marco Mariani, Alessandra Mattei, Fabrizia Mealli", "title": "Exploiting network information to disentangle spillover effects in a\n  field experiment on teens' museum attendance", "comments": "Original article, 36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nudging youths to visit historical and artistic heritage is a key goal\npursued by cultural organizations. The field experiment we analyze is a\nclustered encouragement design (CED) conducted in Florence (Italy) and devised\nto assess how appropriate incentives assigned to high-school classes may induce\nteens to visit museums in their free time. In CEDs, where the focus is on\ncausal effects for individuals, interference between units is generally\nunavoidable. The presence of noncompliance and spillover effects makes causal\ninference particularly challenging. We propose to deal with these complications\nby creatively blending the principal stratification framework and causal\nmediation methods, and exploiting information on interpersonal networks. We\nformally define principal natural direct and indirect effects and principal\ncontrolled direct and indirect effects, and use them to disentangle spillovers\nfrom other causal channels. The key insights are that overall principal causal\neffects for sub-populations of units defined by the compliance behavior combine\nencouragement, treatment and spillovers effects. In this situation, a synthesis\nof the network information may be used as a possible mediator, such that the\npart of the effect that is channeled by it can be attributed to spillovers. A\nBayesian approach is used for inference, invoking latent ignorability\nassumptions on the mediator conditional on principal stratum membership.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 14:30:02 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Noirjean", "Silvia", ""], ["Mariani", "Marco", ""], ["Mattei", "Alessandra", ""], ["Mealli", "Fabrizia", ""]]}, {"id": "2011.11026", "submitter": "Yongqiang Tang Dr.", "authors": "Yongqiang Tang, Ronan Fitzpatrick", "title": "Sample size calculation for the Andersen-Gill model comparing rates of\n  recurrent events", "comments": "10", "journal-ref": "Statistics in Medicine 2019", "doi": "10.1002/sim.8335", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent events arise frequently in biomedical research, where the subject\nmay experience the same type of events more than once. The Andersen-Gill (AG)\nmodel has become increasingly popular in the analysis of recurrent events\nparticularly when the event rate is not constant over time. We propose a\nprocedure for calculating the power and sample size for the robust Wald test\nfrom the AG model in superiority, noninferiority, and equivalence clinical\ntrials. Its performance is demonstrated by numerical examples. Sample SAS code\nis provided in the Supplementary Material.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 14:40:54 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Tang", "Yongqiang", ""], ["Fitzpatrick", "Ronan", ""]]}, {"id": "2011.11123", "submitter": "Beste Hamiye Beyaztas", "authors": "Beste Hamiye Beyaztas and Soutir Bandyopadhyay", "title": "Data Driven Robust Estimation Methods for Fixed Effects Panel Data\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The panel data regression models have gained increasing attention in\ndifferent areas of research including but not limited to econometrics,\nenvironmental sciences, epidemiology, behavioral and social sciences. However,\nthe presence of outlying observations in panel data may often lead to biased\nand inefficient estimates of the model parameters resulting in unreliable\ninferences when the least squares (LS) method is applied. We propose extensions\nof the M-estimation approach with a data-driven selection of tuning parameters\nto achieve desirable level of robustness against outliers without loss of\nestimation efficiency. The consistency and asymptotic normality of the proposed\nestimators have also been proved under some mild regularity conditions. The\nfinite sample properties of the existing and proposed robust estimators have\nbeen examined through an extensive simulation study and an application to\nmacroeconomic data. Our findings reveal that the proposed methods often\nexhibits improved estimation and prediction performances in the presence of\noutliers and are consistent with the traditional LS method when there is no\ncontamination.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 22:13:11 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Beyaztas", "Beste Hamiye", ""], ["Bandyopadhyay", "Soutir", ""]]}, {"id": "2011.11177", "submitter": "Paul Roediger", "authors": "Paul A. Roediger", "title": "Gonogo: An R Implementation of Test Methods to Perform, Analyze and\n  Simulate Sensitivity Experiments", "comments": "This documentation is 58 pages in length and contains 31 figures, 40\n  tables and 2 flow diagrams. The subject of much of the paper, the gonogo.R\n  file, contains 118 functions plus 2 constants and is available online", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work provides documentation for a suite of R functions contained in\ngonogo.R. The functions provide sensitivity testing practitioners and\nresearchers with an ability to conduct, analyze and simulate various\nsensitivity experiments involving binary responses and a single stimulus level\n(e.g., drug dosage, drop height, velocity, etc.). Included are the modern Neyer\nand 3pod adaptive procedures, as well as the Bruceton and Langlie. The latter\ntwo benchmark procedures are capable of being performed according to\ngeneralized up-down transformed-response rules. Each procedure is designated\nphase-one of a three-phase experiment. The goal of phase-one is to achieve\noverlapping data. The two additional (and optional) refinement phases utilize\nthe D-optimal criteria and the Robbins-Monro-Joseph procedure. The goals of the\ntwo refinement phases are to situate testing in the vicinity of the median and\ntails of the latent response distribution, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 02:28:29 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Roediger", "Paul A.", ""]]}, {"id": "2011.11178", "submitter": "Fan Yin", "authors": "Fan Yin, Jieying Jiao, Guanyu Hu, Jun Yan", "title": "Bayesian Nonparametric Estimation for Point Processes with Spatial\n  Homogeneity: A Spatial Analysis of NBA Shot Locations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basketball shot location data provide valuable summary information regarding\nplayers to coaches, sports analysts, fans, statisticians, as well as players\nthemselves. Represented by spatial points, such data are naturally analyzed\nwith spatial point process models. We present a novel nonparametric Bayesian\nmethod for learning the underlying intensity surface built upon a combination\nof Dirichlet process and Markov random field. Our method has the advantage of\neffectively encouraging local spatial homogeneity when estimating a globally\nheterogeneous intensity surface. Posterior inferences are performed with an\nefficient Markov chain Monte Carlo (MCMC) algorithm. Simulation studies show\nthat the inferences are accurate and that the method is superior compared to\nthe competing methods. Application to the shot location data of $20$\nrepresentative NBA players in the 2017-2018 regular season offers interesting\ninsights about the shooting patterns of these players. A comparison against the\ncompeting method shows that the proposed method can effectively incorporate\nspatial contiguity into the estimation of intensity surfaces.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 02:32:49 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Yin", "Fan", ""], ["Jiao", "Jieying", ""], ["Hu", "Guanyu", ""], ["Yan", "Jun", ""]]}, {"id": "2011.11480", "submitter": "Emma Gerard", "authors": "Emma Gerard, Sarah Zohar, Hoai-Thu Thai, Christelle Lorenzato,\n  Marie-Karelle Riviere and Moreno Ursino", "title": "Bayesian dose-regimen assessment in early phase oncology incorporating\n  pharmacokinetics and pharmacodynamics", "comments": "16 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Phase I dose-finding trials in oncology seek to find the maximum tolerated\ndose (MTD) of a drug under a specific schedule. Evaluating drug-schedules aims\nat improving treatment safety while maintaining efficacy. However, while we can\nreasonably assume that toxicity increases with the dose for cytotoxic drugs,\nthe relationship between toxicity and multiple schedules remains elusive. We\nproposed a Bayesian dose-regimen assessment method (DRtox) using\npharmacokinetics/pharmacodynamics (PK/PD) information to estimate the maximum\ntolerated dose-regimen (MTD-regimen), at the end of the dose-escalation stage\nof a trial to be recommended for the next phase. We modeled the binary toxicity\nvia a PD endpoint and estimated the dose-regimen toxicity relationship through\nthe integration of a dose-regimen PD model and a PD toxicity model. For the\ndose-regimen PD model, we considered nonlinear mixed-effects models, and for\nthe PD toxicity model, we proposed the following two Bayesian approaches: a\nlogistic model and a hierarchical model. We evaluated the operating\ncharacteristics of the DRtox through simulation studies under various\nscenarios. The results showed that our method outperforms traditional\nmodel-based designs demonstrating a higher percentage of correctly selecting\nthe MTD-regimen. Moreover, the inclusion of PK/PD information in the DRtox\nhelped provide more precise estimates for the entire dose-regimen toxicity\ncurve; therefore the DRtox may recommend alternative untested regimens for\nexpansion cohorts. The DRtox should be applied at the end of the\ndose-escalation stage of an ongoing trial for patients with relapsed or\nrefractory acute myeloid leukemia (NCT03594955) once all toxicity and PK/PD\ndata are collected.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 15:41:46 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Gerard", "Emma", ""], ["Zohar", "Sarah", ""], ["Thai", "Hoai-Thu", ""], ["Lorenzato", "Christelle", ""], ["Riviere", "Marie-Karelle", ""], ["Ursino", "Moreno", ""]]}, {"id": "2011.11555", "submitter": "Cansu Alakus", "authors": "Cansu Alakus, Denis Larocque, Sebastien Jacquemont, Fanny Barlaam,\n  Charles-Olivier Martin, Kristian Agbogba, Sarah Lippe, Aurelie Labbe", "title": "Conditional canonical correlation estimation based on covariates with\n  random forests", "comments": "27 pages, 8 figures, 1 table", "journal-ref": null, "doi": "10.1093/bioinformatics/btab158", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigating the relationships between two sets of variables helps to\nunderstand their interactions and can be done with canonical correlation\nanalysis (CCA). However, the correlation between the two sets can sometimes\ndepend on a third set of covariates, often subject-related ones such as age,\ngender, or other clinical measures. In this case, applying CCA to the whole\npopulation is not optimal and methods to estimate conditional CCA, given the\ncovariates, can be useful. We propose a new method called Random Forest with\nCanonical Correlation Analysis (RFCCA) to estimate the conditional canonical\ncorrelations between two sets of variables given subject-related covariates.\nThe individual trees in the forest are built with a splitting rule specifically\ndesigned to partition the data to maximize the canonical correlation\nheterogeneity between child nodes. We also propose a significance test to\ndetect the global effect of the covariates on the relationship between two sets\nof variables. The performance of the proposed method and the global\nsignificance test is evaluated through simulation studies that show it provides\naccurate canonical correlation estimations and well-controlled Type-1 error. We\nalso show an application of the proposed method with EEG data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 17:09:46 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 22:55:03 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Alakus", "Cansu", ""], ["Larocque", "Denis", ""], ["Jacquemont", "Sebastien", ""], ["Barlaam", "Fanny", ""], ["Martin", "Charles-Olivier", ""], ["Agbogba", "Kristian", ""], ["Lippe", "Sarah", ""], ["Labbe", "Aurelie", ""]]}, {"id": "2011.11558", "submitter": "Jos\\'e Antonio Perusqu\\'ia Cort\\'es", "authors": "Jos\\'e A. Perusqu\\'ia and Jim E. Griffin and Cristiano Villa", "title": "On a Bayesian Approach to Malware Detection and Classification through\n  $n$-gram Profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and correctly classifying malicious executables has become one of\nthe major concerns in cyber security, especially because traditional detection\nsystems have become less effective with the increasing number and danger of\nthreats found nowadays. One way to differentiate benign from malicious\nexecutables is to leverage on their hexadecimal representation by creating a\nset of binary features that completely characterise each executable. In this\npaper we present a novel supervised learning Bayesian nonparametric approach\nfor binary matrices, that provides an effective probabilistic approach for\nmalware detection. Moreover, and due to the model's flexible assumptions, we\nare able to use it in a multi-class framework where the interest relies in\nclassifying malware into known families. Finally, a generalisation of the model\nwhich provides a deeper understanding of the behaviour across groups for each\nfeature is also developed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 17:12:34 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Perusqu\u00eda", "Jos\u00e9 A.", ""], ["Griffin", "Jim E.", ""], ["Villa", "Cristiano", ""]]}, {"id": "2011.11583", "submitter": "Geoffrey Johnson", "authors": "Geoffrey S Johnson", "title": "Tolerance and Prediction Intervals for Non-normal Models", "comments": "Clinical Trial Recruitment, Time on Treatment, Probability of\n  Success, Prediction Interval, Tolerance Interval", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prediction interval covers a future observation from a random process in\nrepeated sampling, and is typically constructed by identifying a pivotal\nquantity that is also an ancillary statistic. Analogously, a tolerance interval\ncovers a population percentile in repeated sampling and is often based on a\npivotal quantity. One approach we consider in non-normal models leverages a\nlink function resulting in a pivotal quantity that is approximately normally\ndistributed. In settings where this normal approximation does not hold we\nconsider a second approach for tolerance and prediction based on a confidence\ninterval for the mean. These methods are intuitive, simple to implement, have\nproper operating characteristics, and are computationally efficient compared to\nBayesian, re-sampling, and machine learning methods. This is demonstrated in\nthe context of multi-site clinical trial recruitment with staggered site\ninitiation, real-world time on treatment, and end-of-study success for a\nclinical endpoint.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 17:48:09 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 15:07:05 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 00:34:36 GMT"}, {"version": "v4", "created": "Mon, 7 Jun 2021 18:43:56 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Johnson", "Geoffrey S", ""]]}, {"id": "2011.11770", "submitter": "Giovanni Punzi", "authors": "Giovanni Punzi", "title": "Sensitivity optimization of multichannel searches for new signals", "comments": "6 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an hep-ex stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The frequentist definition of sensitivity of a search for new phenomena\nproposed in arXiv:0308063 has been utilized in a number of published\nexperimental searches. In most cases, the simple approximate formula for the\ncommon problem of Poisson counts with background has been deemed adequate for\nthe purpose. There are however many problems nowadays in which more complex\nanalysis is required, involving multiple channels. In this article, the same\napproach of arXiv:0308063 is applied to a multichannel Poisson problem, and a\nconvenient formula is derived in closed form, generalizing the known result for\nthe simple counting experiment. An explicit solution is also derived for the\ncommon case of a search for a Gaussian signal superimposed over a flat\nbackground.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 22:28:29 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Punzi", "Giovanni", ""]]}, {"id": "2011.11874", "submitter": "Sarah Reifeis", "authors": "Sarah A. Reifeis and Michael G. Hudgens", "title": "On variance of the treatment effect in the treated using inverse\n  probability weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the analysis of observational studies, inverse probability weighting (IPW)\nis commonly used to consistently estimate the average treatment effect (ATE) or\nthe average treatment effect in the treated (ATT). The variance of the IPW ATE\nestimator is often estimated by assuming the weights are known and then using\nthe so-called \"robust\" (Huber-White) sandwich estimator, which results in\nconservative standard error (SE) estimation. Here it is shown that using such\nan approach when estimating the variance of the IPW ATT estimator does not\nnecessarily result in conservative SE estimates. That is, assuming the weights\nare known, the robust sandwich estimator may be conservative or\nanti-conservative. Thus confidence intervals of the ATT using the robust SE\nestimate will not be valid in general. Instead, stacked estimating equations\nwhich account for the weight estimation can be used to compute a consistent,\nclosed-form variance estimator for the IPW ATT estimator. The two variance\nestimators are compared via simulation studies and in a data analysis of the\neffect of smoking on gene expression.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 03:29:59 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Reifeis", "Sarah A.", ""], ["Hudgens", "Michael G.", ""]]}, {"id": "2011.11939", "submitter": "Uri Keich", "authors": "Dong Luo, Yilun He, Kristen Emery, William Stafford Noble, Uri Keich", "title": "Competition-based control of the false discovery proportion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, Barber and Cand\\`es laid the theoretical foundation for a general\nframework for false discovery rate (FDR) control based on the notion of\n\"knockoffs.\" A closely related FDR control methodology has long been employed\nin the analysis of mass spectrometry data, referred to there as \"target-decoy\ncompetition\" (TDC). However, any approach that aims to control the FDR, which\nis defined as the expected value of the false discovery proportion (FDP),\nsuffers from a problem. Specifically, even when successfully controlling the\nFDR at level {\\alpha}, the FDP in the list of discoveries can significantly\nexceed {\\alpha}. We offer two new procedures to address this problem, both of\nwhich are compatible with the knockoff framework or TDC. FDP-SD rigorously\ncontrols the FDP in the competition setup by guaranteeing that the FDP is\nbounded by {\\alpha} at any desired confidence level. The complementary TDC-UB\nis designed to bound the FDP for any list of top-scoring target discoveries.\nCompared with the just-published general framework of Katsevich and Ramdas, our\nproposed procedures generally offer more power (FDP-SD) and tighter bounds\n(TDC-UB) and often substantially so in simulated as well as real data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 07:31:48 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 14:02:27 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Luo", "Dong", ""], ["He", "Yilun", ""], ["Emery", "Kristen", ""], ["Noble", "William Stafford", ""], ["Keich", "Uri", ""]]}, {"id": "2011.12036", "submitter": "Fabio Centofanti", "authors": "Fabio Centofanti, Antonio Lepore, Alessandra Menafoglio, Biagio\n  Palumbo, Simone Vantini", "title": "Adaptive Smoothing Spline Estimator for the Function-on-Function Linear\n  Regression Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an adaptive smoothing spline (AdaSS) estimator for\nthe function-on-function linear regression model where each value of the\nresponse, at any domain point, depends on the full trajectory of the predictor.\nThe AdaSS estimator is obtained by the optimization of an objective function\nwith two spatially adaptive penalties, based on initial estimates of the\npartial derivatives of the regression coefficient function. This allows the\nproposed estimator to adapt more easily to the true coefficient function over\nregions of large curvature and not to be undersmoothed over the remaining part\nof the domain. A novel evolutionary algorithm is developed ad hoc to obtain the\noptimization tuning parameters. Extensive Monte Carlo simulations have been\ncarried out to compare the AdaSS estimator with competitors that have already\nappeared in the literature before. The results show that our proposal mostly\noutperforms the competitor in terms of estimation and prediction accuracy.\nLastly, those advantages are illustrated also on two real-data benchmark\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:29:12 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Centofanti", "Fabio", ""], ["Lepore", "Antonio", ""], ["Menafoglio", "Alessandra", ""], ["Palumbo", "Biagio", ""], ["Vantini", "Simone", ""]]}, {"id": "2011.12044", "submitter": "Davide Risso", "authors": "Thi Kim Hue Nguyen, Koen Van den Berge, Monica Chiogna, Davide Risso", "title": "Structure learning for zero-inflated counts, with an application to\n  single-cell RNA sequencing data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of estimating the structure of a graph from observed data is of\ngrowing interest in the context of high-throughput genomic data, and\nsingle-cell RNA sequencing in particular. These, however, are challenging\napplications, since the data consist of high-dimensional counts with high\nvariance and over-abundance of zeros. Here, we present a general framework for\nlearning the structure of a graph from single-cell RNA-seq data, based on the\nzero-inflated negative binomial distribution. We demonstrate with simulations\nthat our approach is able to retrieve the structure of a graph in a variety of\nsettings and we show the utility of the approach on real data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:37:48 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Nguyen", "Thi Kim Hue", ""], ["Berge", "Koen Van den", ""], ["Chiogna", "Monica", ""], ["Risso", "Davide", ""]]}, {"id": "2011.12116", "submitter": "Caleb Bastian", "authors": "Caleb Deen Bastian, Herschel Rabitz", "title": "Uncertainty Quantification by Random Measures and Fields", "comments": "65 pages, 20 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a general framework for uncertainty quantification that is a\nmosaic of interconnected models. We define global first and second order\nstructural and correlative sensitivity analyses for random counting measures\nacting on risk functionals of input-output maps. These are the ANOVA\ndecomposition of the intensity measure and the decomposition of the random\nmeasure variance, each into subspaces. Orthogonal random measures furnish\nsensitivity distributions. We show that the random counting measure may be used\nto construct positive random fields, which admit decompositions of covariance\nand sensitivity indices and may be used to represent interacting particle\nsystems. The first and second order global sensitivity analyses conveyed\nthrough random counting measures elucidate and integrate different notions of\nuncertainty quantification, and the global sensitivity analysis of random\nfields conveys the proportionate functional contributions to covariance. This\nframework complements others when used in conjunction with for instance\nalgorithmic uncertainty and model selection uncertainty frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 18:31:28 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 22:11:08 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 16:12:52 GMT"}, {"version": "v4", "created": "Thu, 31 Dec 2020 20:06:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Bastian", "Caleb Deen", ""], ["Rabitz", "Herschel", ""]]}, {"id": "2011.12154", "submitter": "Florian Frommlet", "authors": "Malgorzata Bogdan, Florian Frommlet", "title": "Identifying important predictors in large data bases -- multiple testing\n  and model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is a chapter of the forthcoming Handbook of Multiple Testing. We\nconsider a variety of model selection strategies in a high-dimensional setting,\nwhere the number of potential predictors p is large compared to the number of\navailable observations n. In particular modifications of information criteria\nwhich are suitable in case of p > n are introduced and compared with a variety\nof penalized likelihood methods, in particular SLOPE and SLOBE. The focus is on\nmethods which control the FDR in terms of model identification. Theoretical\nresults are provided both with respect to model identification and prediction\nand various simulation results are presented which illustrate the performance\nof the different methods in different situations.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 15:10:33 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Bogdan", "Malgorzata", ""], ["Frommlet", "Florian", ""]]}, {"id": "2011.12182", "submitter": "Binhuan Wang", "authors": "Binhuan Wang, Lanqiu Yao, Jiyuan Hu, and Huilin Li", "title": "A New Algorithm for Convex Biclustering and Its Extension to the\n  Compositional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biclustering is a powerful data mining technique that allows simultaneously\nclustering rows (observations) and columns (features) in a matrix-format data\nset, which can provide results in a checkerboard-like pattern for visualization\nand exploratory analysis in a wide array of domains. Multiple biclustering\nalgorithms have been developed in the past two decades, among which the convex\nbiclustering can guarantee a global optimum by formulating in as a convex\noptimization problem. On the other hand, the application of biclustering has\nnot progressed in parallel with the algorithm techniques. For example,\nbiclustering for increasingly popular microbiome research data is under-applied\npossibly due to its compositional constraints for each sample. In this\nmanuscript, we propose a new convex biclustering algorithm, called the bi-ADMM,\nunder general setups based on the ADMM algorithm, which is free of extra\nsmoothing steps to visualize informative biclusters required by existing convex\nbiclustering algorithms. Furthermore, we tailor it to the algorithm named\nbiC-ADMM specifically to tackle compositional constraints confronted in\nmicrobiome data. The key step of our methods utilizes the Sylvester Equation to\nderive the ADMM algorithm, which is new to the clustering research. The\neffectiveness of the proposed methods is examined through a variety of\nnumerical experiments and a microbiome data application.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 16:02:02 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 17:02:41 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Binhuan", ""], ["Yao", "Lanqiu", ""], ["Hu", "Jiyuan", ""], ["Li", "Huilin", ""]]}, {"id": "2011.12215", "submitter": "Feng Ruan", "authors": "Keli Liu and Feng Ruan", "title": "A Self-Penalizing Objective Function for Scalable Interaction Detection", "comments": "34 pages; the Appendix can be found on the authors' personal websites\n  (the url is in the pdf)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of nonparametric variable selection with a focus on\ndiscovering interactions between variables. With $p$ variables there are\n$O(p^s)$ possible order-$s$ interactions making exhaustive search infeasible.\nIt is nonetheless possible to identify the variables involved in interactions\nwith only linear computation cost, $O(p)$. The trick is to maximize a class of\nparametrized nonparametric dependence measures which we call metric learning\nobjectives; the landscape of these nonconvex objective functions is sensitive\nto interactions but the objectives themselves do not explicitly model\ninteractions. Three properties make metric learning objectives highly\nattractive:\n  (a) The stationary points of the objective are automatically sparse (i.e.\nperforms selection) -- no explicit $\\ell_1$ penalization is needed.\n  (b) All stationary points of the objective exclude noise variables with high\nprobability.\n  (c) Guaranteed recovery of all signal variables without needing to reach the\nobjective's global maxima or special stationary points.\n  The second and third properties mean that all our theoretical results apply\nin the practical case where one uses gradient ascent to maximize the metric\nlearning objective. While not all metric learning objectives enjoy good\nstatistical power, we design an objective based on $\\ell_1$ kernels that does\nexhibit favorable power: it recovers (i) main effects with $n \\sim \\log p$\nsamples, (ii) hierarchical interactions with $n \\sim \\log p$ samples and (iii)\norder-$s$ pure interactions with $n \\sim p^{2(s-1)}\\log p$ samples.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 17:07:49 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 00:14:34 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Liu", "Keli", ""], ["Ruan", "Feng", ""]]}, {"id": "2011.12220", "submitter": "Lin Zheng", "authors": "Lin Zheng", "title": "Some Theory for Texture Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of texture segmentation in images, and provide some\ntheoretical guarantees for the prototypical approach which consists in\nextracting local features in the neighborhood of a pixel and then applying a\nclustering algorithm for grouping the pixel according to these features. On the\none hand, for stationary textures, which we model with Gaussian Markov random\nfields, we construct the feature for each pixel by calculating the sample\ncovariance matrix of its neighborhood patch and cluster the pixels by an\napplication of k-means to group the covariance matrices. We show that this\ngeneric method is consistent. On the other hand, for non-stationary fields, we\ninclude the location of the pixel as an additional feature and apply\nsingle-linkage clustering. We again show that this generic and emblematic\nmethod is consistent. We complement our theory with some numerical experiments\nperformed on both generated and natural textures.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 04:21:16 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Zheng", "Lin", ""]]}, {"id": "2011.12268", "submitter": "Giorgos Afendras", "authors": "Georgios Afendras, Marianthi Markatou and Albert Vexler", "title": "An AUK-based index for measuring and testing the joint dependence of a\n  random vector", "comments": "33 pages (plus 8 pages supplementary material), 7 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present an index of dependence that allows one to measure the joint or\nmutual dependence of a $d$-dimensional random vector with $d>2$. The index is\nbased on a $d$-dimensional Kendall process. We further propose a standardized\nversion of our index of dependence that is easy to interpret, and provide an\nalgorithm for its computation. We discuss tests of total independence based on\nconsistent estimates of the area under the Kendall curve. We evaluate the\nperformance of our procedures via simulation, and apply our methods to a real\ndata set.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 18:25:19 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 15:44:36 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Afendras", "Georgios", ""], ["Markatou", "Marianthi", ""], ["Vexler", "Albert", ""]]}, {"id": "2011.12345", "submitter": "Maria Josefsson", "authors": "Maria Josefsson, Michael J. Daniels, Sara Pudas", "title": "A Bayesian semi-parametric approach for inference on the population\n  partly conditional mean from longitudinal data with dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies of memory trajectories using longitudinal data often result in highly\nnon-representative samples due to selective study enrollment and attrition. An\nadditional bias comes from practice effects that result in improved or\nmaintained performance due to familiarity with test content or context. These\nchallenges may bias study findings and severely distort the ability to\ngeneralize to the target population. In this study we propose an approach for\nestimating the finite population mean of a longitudinal outcome conditioning on\nbeing alive at a specific time point. We develop a flexible Bayesian\nsemi-parametric predictive estimator for population inference when longitudinal\nauxiliary information is known for the target population. We evaluate\nsensitivity of the results to untestable assumptions and further compare our\napproach to other methods used for population inference in a simulation study.\nThe proposed approach is motivated by 15-year longitudinal data from the Betula\nlongitudinal cohort study. We apply our approach to estimate lifespan\ntrajectories in episodic memory, with the aim to generalize findings to a\ntarget population.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 19:54:42 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 23:42:33 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 08:43:09 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Josefsson", "Maria", ""], ["Daniels", "Michael J.", ""], ["Pudas", "Sara", ""]]}, {"id": "2011.12392", "submitter": "Gersende Fort", "authors": "Gersende Fort (IMT), Eric Moulines (X-DEP-MATHAPP), Hoi-To Wai", "title": "Geom-SPIDER-EM: Faster Variance Reduced Stochastic Expectation\n  Maximization for Nonconvex Finite-Sum Optimization", "comments": "Submitted to an International conference, with reviewing process", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Expectation Maximization (EM) algorithm is a key reference for inference\nin latent variable models; unfortunately, its computational cost is prohibitive\nin the large scale learning setting. In this paper, we propose an extension of\nthe Stochastic Path-Integrated Differential EstimatoR EM (SPIDER-EM) and derive\ncomplexity bounds for this novel algorithm, designed to solve smooth nonconvex\nfinite-sum optimization problems. We show that it reaches the same state of the\nart complexity bounds as SPIDER-EM; and provide conditions for a linear rate of\nconvergence. Numerical results support our findings.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 21:20:53 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Fort", "Gersende", "", "IMT"], ["Moulines", "Eric", "", "X-DEP-MATHAPP"], ["Wai", "Hoi-To", ""]]}, {"id": "2011.12397", "submitter": "James Tucker", "authors": "Xiao Zang, Sebastian Kurtek, Oksana Chkrebtii, J. Derek Tucker", "title": "Elastic $k$-means clustering of functional data for posterior\n  exploration, with an application to inference on acute respiratory infection\n  dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a new method for clustering of functional data using a $k$-means\nframework. We work within the elastic functional data analysis framework, which\nallows for decomposition of the overall variation in functional data into\namplitude and phase components. We use the amplitude component to partition\nfunctions into shape clusters using an automated approach. To select an\nappropriate number of clusters, we additionally propose a novel Bayesian\nInformation Criterion defined using a mixture model on principal components\nestimated using functional Principal Component Analysis. The proposed method is\nmotivated by the problem of posterior exploration, wherein samples obtained\nfrom Markov chain Monte Carlo algorithms are naturally represented as\nfunctions. We evaluate our approach using a simulated dataset, and apply it to\na study of acute respiratory infection dynamics in San Luis Potos\\'{i}, Mexico.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 21:27:10 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zang", "Xiao", ""], ["Kurtek", "Sebastian", ""], ["Chkrebtii", "Oksana", ""], ["Tucker", "J. Derek", ""]]}, {"id": "2011.12416", "submitter": "Nathaniel Josephs", "authors": "Li Chen, Nathaniel Josephs, Lizhen Lin, Jie Zhou, and Eric D. Kolaczyk", "title": "A spectral-based framework for hypothesis testing in populations of\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new spectral-based approach to hypothesis testing\nfor populations of networks. The primary goal is to develop a test to determine\nwhether two given samples of networks come from the same random model or\ndistribution. Our test statistic is based on the trace of the third order for a\ncentered and scaled adjacency matrix, which we prove converges to the standard\nnormal distribution as the number of nodes tends to infinity. The asymptotic\npower guarantee of the test is also provided. The proper interplay between the\nnumber of networks and the number of nodes for each network is explored in\ncharacterizing the theoretical properties of the proposed testing statistics.\nOur tests are applicable to both binary and weighted networks, operate under a\nvery general framework where the networks are allowed to be large and sparse,\nand can be extended to multiple-sample testing. We provide an extensive\nsimulation study to demonstrate the superior performance of our test over\nexisting methods and apply our test to three real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 21:58:27 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Chen", "Li", ""], ["Josephs", "Nathaniel", ""], ["Lin", "Lizhen", ""], ["Zhou", "Jie", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "2011.12509", "submitter": "Aniruddha Rajendra Rao", "authors": "Aniruddha Rajendra Rao, Matthew Reimherr", "title": "Modern Multiple Imputation with Functional Data", "comments": "7 figures (including supplementary material), 8 tables (including\n  supplementary material), 14 pages (including supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work considers the problem of fitting functional models with sparsely\nand irregularly sampled functional data. It overcomes the limitations of the\nstate-of-the-art methods, which face major challenges in the fitting of more\ncomplex non-linear models. Currently, many of these models cannot be\nconsistently estimated unless the number of observed points per curve grows\nsufficiently quickly with the sample size, whereas, we show numerically that a\nmodified approach with more modern multiple imputation methods can produce\nbetter estimates in general. We also propose a new imputation approach that\ncombines the ideas of {\\it MissForest} with {\\it Local Linear Forest} and\ncompare their performance with {\\it PACE} and several other multivariate\nmultiple imputation methods. This work is motivated by a longitudinal study on\nsmoking cessation, in which the Electronic Health Records (EHR) from Penn State\nPaTH to Health allow for the collection of a great deal of data, with highly\nvariable sampling. To illustrate our approach, we explore the relation between\nrelapse and diastolic blood pressure. We also consider a variety of simulation\nschemes with varying levels of sparsity to validate our methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 04:22:30 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Rao", "Aniruddha Rajendra", ""], ["Reimherr", "Matthew", ""]]}, {"id": "2011.12516", "submitter": "Ian Laga", "authors": "Ian Laga, Le Bao, and Xiaoyue Niu", "title": "Thirty Years of The Network Scale up Method", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2021.1935267", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the size of hard-to-reach populations is an important problem for\nmany fields. The Network Scale-up Method (NSUM) is a relatively new approach to\nestimate the size of these hard-to-reach populations by asking respondents the\nquestion, \"How many X's do you know,\" where X is the population of interest\n(e.g. \"How many female sex workers do you know?\"). The answers to these\nquestions form Aggregated Relational Data (ARD). The NSUM has been used to\nestimate the size of a variety of subpopulations, including female sex workers,\ndrug users, and even children who have been hospitalized for choking. Within\nthe Network Scale-up methodology, there are a multitude of estimators for the\nsize of the hidden population, including direct estimators, maximum likelihood\nestimators, and Bayesian estimators. In this article, we first provide an\nin-depth analysis of ARD properties and the techniques to collect the data.\nThen, we comprehensively review different estimation methods in terms of the\nassumptions behind each model, the relationships between the estimators, and\nthe practical considerations of implementing the methods. Finally, we provide a\nsummary of the dominant methods and an extensive list of the applications, and\ndiscuss the open problems and potential research directions in this area.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 05:02:05 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 01:08:27 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Laga", "Ian", ""], ["Bao", "Le", ""], ["Niu", "Xiaoyue", ""]]}, {"id": "2011.12560", "submitter": "Christophe Ley", "authors": "Sla{\\dj}ana Babi\\'c and Christophe Ley and Marko Palangeti\\'c", "title": "Elliptical Symmetry Tests in \\proglang{R}", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption of elliptical symmetry has an important role in many\ntheoretical developments and applications, hence it is of primary importance to\nbe able to test whether that assumption actually holds true or not. Various\ntests have been proposed in the literature for this problem. To the best of our\nknowledge, none of them has been implemented in R. The focus of this paper is\nthe implementation of several well-known tests for elliptical symmetry together\nwith some recent tests. We demonstrate the testing procedures with a real data\nexample.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 07:55:45 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 11:25:32 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Babi\u0107", "Sla\u0111ana", ""], ["Ley", "Christophe", ""], ["Palangeti\u0107", "Marko", ""]]}, {"id": "2011.12746", "submitter": "Asma Bahamyirou", "authors": "Asma Bahamyirou, Mireille E. Schnitzer, Edward H. Kennedy, Lucie Blais\n  and Yi Yang", "title": "Doubly Robust Adaptive LASSO for Effect Modifier Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effect modification occurs when the effect of the treatment on an outcome\ndiffers according to the level of a third variable (the effect modifier, EM). A\nnatural way to assess effect modification is by subgroup analysis or include\nthe interaction terms between the treatment and the covariates in an outcome\nregression. The latter, however, does not target a parameter of a marginal\nstructural model (MSM) unless a correctly specified outcome model is specified.\nOur aim is to develop a data-adaptive method to select effect modifying\nvariables in an MSM with a single time point exposure. A two-stage procedure is\nproposed. First, we estimate the conditional outcome expectation and propensity\nscore and plug these into a doubly robust loss function. Second, we use the\nadaptive LASSO to select the EMs and estimate MSM coefficients. Post-selection\ninference is then used to obtain coverage on the selected EMs. Simulations\nstudies are performed in order to verify the performance of the proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:02:54 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Bahamyirou", "Asma", ""], ["Schnitzer", "Mireille E.", ""], ["Kennedy", "Edward H.", ""], ["Blais", "Lucie", ""], ["Yang", "Yi", ""]]}, {"id": "2011.12751", "submitter": "Xiang Zhou", "authors": "Xiang Zhou", "title": "Semiparametric Estimation for Causal Mediation Analysis with Multiple\n  Causally Ordered Mediators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal mediation analysis concerns the pathways through which a treatment\naffects an outcome. While most of the mediation literature focuses on settings\nwith a single mediator, a flourishing line of research has considered settings\ninvolving multiple causally ordered mediators, under which a set of\npath-specific effects (PSEs) are often of interest. We consider estimation of\nPSEs for the general case where the treatment effect operates through\n$K(\\geq1)$ causally ordered, possibly multivariate mediators. We first define a\nset of PSEs that are identified under Pearl's nonparametric structural equation\nmodel. These PSEs are defined as contrasts between the expectations of\n$2^{K+1}$ potential outcomes, which are identified via what we call the\ngeneralized mediation functional (GMF). We introduce an array of\nregression-imputation, weighting, and \"hybrid\" estimators, and, in particular,\ntwo $K+2$-robust and locally semiparametric efficient estimators for the GMF.\nThe latter estimators are well suited to the use of data-adaptive methods for\nestimating their nuisance functions. We establish rate conditions required of\nthe nuisance functions for semiparametric efficiency. We also discuss how our\nframework applies to several causal and noncausal estimands that may be of\nparticular interest in empirical applications. The proposed estimators are\nillustrated with a simulation study and an empirical example.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:15:33 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zhou", "Xiang", ""]]}, {"id": "2011.12781", "submitter": "Won-Ki Seo", "authors": "Won-Ki Seo", "title": "Functional Principal Component Analysis of Cointegrated Functional Time\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional principal component analysis (FPCA) has played an important role\nin the development of functional time series analysis. This paper investigates\nhow FPCA can be used to analyze cointegrated functional time series and\nproposes a modification of FPCA as a novel statistical tool. Our modified FPCA\nnot only provides an asymptotically more efficient estimator of the\ncointegrating vectors, but also leads to novel FPCA-based tests for examining\nsome essential properties of cointegrated functional time series. As an\nempirical illustration, our methodology is applied to two empirical examples:\nage-specific employment rates and earning densities.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 14:43:30 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 15:38:29 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 15:24:21 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Seo", "Won-Ki", ""]]}, {"id": "2011.12858", "submitter": "Emil Aas Stoltenberg", "authors": "Emil Aas Stoltenberg", "title": "The standard cure model with a linear hazard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a mixture cure model with a linear hazard rate\nregression model for the event times. Cure models are statistical models for\nevent times that take into account that a fraction of the population might\nnever experience the event of interest, this fraction is said to be\n{`}cured{'}. The population survival function in a mixture cure model takes the\nform $S(t) = 1 - \\pi + \\pi\\exp(-\\int_0^t\\alpha(s)\\,d s)$, where $\\pi$ is the\nprobability of being susceptible to the event under study, and $\\alpha(s)$ is\nthe hazard rate of the susceptible fraction. We let both $\\pi$ and $\\alpha(s)$\ndepend on possibly different covariate vectors $X$ and $Z$. The probability\n$\\pi$ is taken to be the logistic function $\\pi(X^{\\prime}\\gamma) =\n1/\\{1+\\exp(-X^{\\prime}\\gamma)\\}$, while we model $\\alpha(s)$ by Aalen's linear\nhazard rate regression model. This model postulates that a susceptible\nindividual has hazard rate function $\\alpha(t;Z) = \\beta_0(t) + \\beta_1(t)Z_1 +\n\\cdots + Z_{q-1}\\beta_{q-1}(t)$ in terms of her covariate values\n$Z_1,\\ldots,Z_{q-1}$. The large-sample properties of our estimators are studied\nby way of parametric models that tend to a semiparametric model as a parameter\n$K \\to \\infty$. For each model in the sequence of parametric models, we assume\nthat the data generating mechanism is parametric, thus simplifying the\nderivation of the estimators, as well as the proofs of consistency and limiting\nnormality. Finally, we use contiguity techniques to switch back to assuming\nthat the data stem from the semiparametric model. This technique for deriving\nand studying estimators in non- and semiparametric settings has previously been\nstudied and employed in the high-frequency data literature, but seems to be\nnovel in survival analysis.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:28:31 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Stoltenberg", "Emil Aas", ""]]}, {"id": "2011.12873", "submitter": "Adam McCloskey", "authors": "Adam McCloskey", "title": "Hybrid Confidence Intervals for Informative Uniform Asymptotic Inference\n  After Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a new type of confidence interval for correct asymptotic inference\nafter using data to select a model of interest without assuming any model is\ncorrectly specified. This hybrid confidence interval is constructed by\ncombining techniques from the selective inference and post-selection inference\nliteratures to yield a short confidence interval across a wide range of data\nrealizations. I show that hybrid confidence intervals have correct asymptotic\ncoverage, uniformly over a large class of probability distributions. I\nillustrate the use of these confidence intervals in the problem of inference\nafter using the LASSO objective function to select a regression model of\ninterest and provide evidence of their desirable length properties in finite\nsamples via a set of Monte Carlo exercises that is calibrated to real-world\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:51:50 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["McCloskey", "Adam", ""]]}, {"id": "2011.12901", "submitter": "Daniel Taylor-Rodriguez", "authors": "Daniel Taylor-Rodriguez, David Lovitz, Nora Mattek, Chao-Yi Wu, Hiroko\n  Dodge, Jeffrey Kaye, Bruno M. Jedynak", "title": "Unstructured Primary Outcome in Randomized Controlled Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The primary outcome of Randomized clinical Trials (RCTs) are typically\ndichotomous, continuous, multivariate continuous, or time-to-event. However,\nwhat if this outcome is unstructured, e.g., a list of variables of mixed types,\nlongitudinal sequences, images, audio recordings, etc. When the outcome is\nunstructured it is unclear how to assess RCT success and how to compute sample\nsize. We show that kernel methods offer natural extensions to traditional\nbiostatistics methods. We demonstrate our approach with the measurements of\ncomputer usage in a cohort of aging participants, some of which will become\ncognitively impaired. Simulations as well as a real data experiment show the\nsuperiority of the proposed approach compared to the standard in this\nsituation: generalized mixed effect models.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:34:32 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Taylor-Rodriguez", "Daniel", ""], ["Lovitz", "David", ""], ["Mattek", "Nora", ""], ["Wu", "Chao-Yi", ""], ["Dodge", "Hiroko", ""], ["Kaye", "Jeffrey", ""], ["Jedynak", "Bruno M.", ""]]}, {"id": "2011.13027", "submitter": "Mohammad Arashi", "authors": "Mohammad Arashi, Najmeh Nakhaei Rad, Andriette Bekker, Wolf Dieter\n  Schubert", "title": "Protein Structure Parameterization via Mobius Distributions on the Torus", "comments": "23 pages, 13 Figures, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.BM q-bio.QM", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Proteins constitute a large group of macromolecules with a multitude of\nfunctions for all living organisms. Proteins achieve this by adopting distinct\nthree-dimensional structures encoded by the sequence of their constituent amino\nacids in one or more polypeptides. In this paper, the statistical modelling of\nthe protein backbone torsion angles is considered. Two new distributions are\nproposed for toroidal data by applying the M\\\"obius transformation to the\nbivariate von Mises distribution. Marginal and conditional distributions in\naddition to sine-skewed versions of the proposed models are also developed.\nThree big data sets consisting of bivariate information about protein domains\nare analysed to illustrate the strength of the flexible proposed models.\nFinally, a simulation study is done to evaluate the obtained maximum likelihood\nestimates and also to find the best method of generating samples from the\nproposed models to use as the proposal distributions in the Markov Chain Monte\nCarlo sampling method for predicting the 3D structure of proteins.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 21:20:29 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Arashi", "Mohammad", ""], ["Rad", "Najmeh Nakhaei", ""], ["Bekker", "Andriette", ""], ["Schubert", "Wolf Dieter", ""]]}, {"id": "2011.13057", "submitter": "Giles Hooker", "authors": "Zi Ye, Giles Hooker and Stephen P. Ellner", "title": "Generalized Single Index Models and Jensen Effects on Reproduction and\n  Survival", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental variability often has substantial impacts on natural\npopulations and communities through its effects on the performance of\nindividuals. Because organisms' responses to environmental conditions are often\nnonlinear (e.g., decreasing performance on both sides of an optimal\ntemperature), the mean response is often different from the response in the\nmean environment. Ye et. al. 2020, proposed testing for the presence of such\nvariance effects on individual or population growth rates by estimating the\n\"Jensen Effect\", the difference in average growth rates under varying versus\nfixed environments, in functional single index models for environmental effects\non growth. In this paper, we extend this analysis to effect of environmental\nvariance on reproduction and survival, which have count and binary outcomes. In\nthe standard generalized linear models used to analyze such data the direction\nof the Jensen Effect is tacitly assumed a priori by the model's link function.\nHere we extend the methods of Ye et. al. 2020 using a generalized single index\nmodel to test whether this assumed direction is contradicted by the data. We\nshow that our test has reasonable power under mild alternatives, but requires\nsample sizes that are larger than are often available. We demonstrate our\nmethods on a long-term time series of plant ground cover on the Idaho steppe.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 23:03:13 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Ye", "Zi", ""], ["Hooker", "Giles", ""], ["Ellner", "Stephen P.", ""]]}, {"id": "2011.13058", "submitter": "Trang Nguyen", "authors": "Trang Quynh Nguyen, Benjamin Ackerman, Ian Schmid, Stephen R. Cole,\n  Elizabeth A. Stuart", "title": "Sensitivity analyses for effect modifiers not observed in the target\n  population when generalizing treatment effects from a randomized controlled\n  trial: Assumptions, models, effect scales, data scenarios, and implementation\n  details", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0208795 10.1371/journal.pone.0208795\n  10.1371/journal.pone.0208795", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Randomized controlled trials are often used to inform policy and\npractice for broad populations. The average treatment effect (ATE) for a target\npopulation, however, may be different from the ATE observed in a trial if there\nare effect modifiers whose distribution in the target population is different\nthat from that in the trial. Methods exist to use trial data to estimate the\ntarget population ATE, provided the distributions of treatment effect modifiers\nare observed in both the trial and target population -- an assumption that may\nnot hold in practice.\n  Methods: The proposed sensitivity analyses address the situation where a\ntreatment effect modifier is observed in the trial but not the target\npopulation. These methods are based on an outcome model or the combination of\nsuch a model and weighting adjustment for observed differences between the\ntrial sample and target population. They accommodate several types of outcome\nmodels: linear models (including single time outcome and pre- and\npost-treatment outcomes) for additive effects, and models with log or logit\nlink for multiplicative effects. We clarify the methods' assumptions and\nprovide detailed implementation instructions.\n  Illustration: We illustrate the methods using an example generalizing the\neffects of an HIV treatment regimen from a randomized trial to a relevant\ntarget population.\n  Conclusion: These methods allow researchers and decision-makers to have more\nappropriate confidence when drawing conclusions about target population\neffects.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 23:07:19 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Nguyen", "Trang Quynh", ""], ["Ackerman", "Benjamin", ""], ["Schmid", "Ian", ""], ["Cole", "Stephen R.", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "2011.13161", "submitter": "Takahiro Hoshino", "authors": "Tomoki Toyabe, Yasuhiro Hasegawa, and Takahiro Hoshino", "title": "Positive-Unlabelled Survival Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider a novel framework of positive-unlabeled data in\nwhich as positive data survival times are observed for subjects who have events\nduring the observation time as positive data and as unlabeled data censoring\ntimes are observed but whether the event occurs or not are unknown for some\nsubjects. We consider two cases: (1) when censoring time is observed in\npositive data, and (2) when it is not observed. For both cases, we developed\nparametric models, nonparametric models, and machine learning models and the\nestimation strategies for these models. Simulation studies show that under this\ndata setup, traditional survival analysis may yield severely biased results,\nwhile the proposed estimation method can provide valid results.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 07:11:41 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Toyabe", "Tomoki", ""], ["Hasegawa", "Yasuhiro", ""], ["Hoshino", "Takahiro", ""]]}, {"id": "2011.13301", "submitter": "Koki Okajima", "authors": "Koki Okajima, Kenji Nagata, Masato Okada", "title": "Fast Bayesian Deconvolution using Simple Reversible Jump Moves", "comments": "11 pages, 9 figures", "journal-ref": "J. Phys. Soc. Jpn. 90, 034001 (2021)", "doi": "10.7566/JPSJ.90.034001", "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Markov chain Monte Carlo-based deconvolution method designed to\nestimate the number of peaks in spectral data, along with the optimal\nparameters of each radial basis function. Assuming cases where the number of\npeaks is unknown, and a sweep simulation on all candidate models is\ncomputationally unrealistic, the proposed method efficiently searches over the\nprobable candidates via trans-dimensional moves assisted by annealing effects\nfrom replica exchange Monte Carlo moves. Through simulation using synthetic\ndata, the proposed method demonstrates its advantages over conventional sweep\nsimulations, particularly in model selection problems. Application to a set of\nolivine reflectance spectral data with varying forsterite and fayalite mixture\nratios reproduced results obtained from previous mineralogical research,\nindicating that our method is applicable to deconvolution on real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 14:11:58 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Okajima", "Koki", ""], ["Nagata", "Kenji", ""], ["Okada", "Masato", ""]]}, {"id": "2011.13415", "submitter": "Susanne Strohmaier", "authors": "Odd O. Aalen, Mats J. Stensrud, Vanessa Didelez, Rhian Daniel, Kjetil\n  R{\\o}ysland and Susanne Strohmaier", "title": "Time-dependent mediators in survival analysis: Modelling direct and\n  indirect effects with the additive hazards model", "comments": null, "journal-ref": "Biometrical Journal. 2020; 62(3):532-549", "doi": "10.1002/bimj.201800263", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss causal mediation analyses for survival data and propose a new\napproach based on the additive hazards model. The emphasis is on a dynamic\npoint of view, that is, understanding how the direct and indirect effects\ndevelop over time. Hence, importantly, we allow for a time varying mediator. To\ndefine direct and indirect effects in such a longitudinal survival setting we\ntake an interventional approach (Didelez (2018)) where treatment is separated\ninto one aspect affecting the mediator and a different aspect affecting\nsurvival. In general, this leads to a version of the non-parametric g-formula\n(Robins (1986)). In the present paper, we demonstrate that combining the\ng-formula with the additive hazards model and a sequential linear model for the\nmediator process results in simple and interpretable expressions for direct and\nindirect effects in terms of relative survival as well as cumulative hazards.\nOur results generalise and formalise the method of dynamic path analysis (Fosen\net al. (2006), Strohmaier et al. (2015)). An application to data from a\nclinical trial on blood pressure medication is given.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 18:09:42 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 14:44:26 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Aalen", "Odd O.", ""], ["Stensrud", "Mats J.", ""], ["Didelez", "Vanessa", ""], ["Daniel", "Rhian", ""], ["R\u00f8ysland", "Kjetil", ""], ["Strohmaier", "Susanne", ""]]}, {"id": "2011.13624", "submitter": "Tengyao Wang", "authors": "Fengnan Gao and Tengyao Wang", "title": "Two-sample testing of high-dimensional linear regression coefficients\n  via complementary sketching", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new method for two-sample testing of high-dimensional linear\nregression coefficients without assuming that those coefficients are\nindividually estimable. The procedure works by first projecting the matrices of\ncovariates and response vectors along directions that are complementary in sign\nin a subset of the coordinates, a process which we call 'complementary\nsketching'. The resulting projected covariates and responses are aggregated to\nform two test statistics, which are shown to have essentially optimal\nasymptotic power under a Gaussian design when the difference between the two\nregression coefficients is sparse and dense respectively. Simulations confirm\nthat our methods perform well in a broad class of settings.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 09:31:52 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Gao", "Fengnan", ""], ["Wang", "Tengyao", ""]]}, {"id": "2011.13646", "submitter": "Zhihua Sun", "authors": "Zhihua Sun, Yi Liu, Kani Chen and Gang Li", "title": "Broken Adaptive Ridge Regression for Right-Censored Survival Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broken adaptive ridge (BAR) is a computationally scalable surrogate to\n$L_0$-penalized regression, which involves iteratively performing reweighted\n$L_2$ penalized regressions and enjoys some appealing properties of both $L_0$\nand $L_2$ penalized regressions while avoiding some of their limitations. In\nthis paper, we extend the BAR method to the semi-parametric accelerated failure\ntime (AFT) model for right-censored survival data. Specifically, we propose a\ncensored BAR (CBAR) estimator by applying the BAR algorithm to the Leurgan's\nsynthetic data and show that the resulting CBAR estimator is consistent for\nvariable selection, possesses an oracle property for parameter estimation {and\nenjoys a grouping property for highly correlation covariates}. Both low and\nhigh dimensional covariates are considered. The effectiveness of our method is\ndemonstrated and compared with some popular penalization methods using\nsimulations. Real data illustrations are provided on a diffuse large-B-cell\nlymphoma data and a glioblastoma multiforme data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 10:42:48 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Sun", "Zhihua", ""], ["Liu", "Yi", ""], ["Chen", "Kani", ""], ["Li", "Gang", ""]]}, {"id": "2011.13655", "submitter": "Payam Shahsavari Baboukani", "authors": "Payam Shahsavari Baboukani, Carina Graversen, Emina Alickovic, Jan\n  {\\O}stergaard", "title": "Estimating Conditional Transfer Entropy in Time Series using Mutual\n  Information and Non-linear Prediction", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": "10.3390/e22101124", "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new estimator to measure directed dependencies in time series.\nThe dimensionality of data is first reduced using a new non-uniform embedding\ntechnique, where the variables are ranked according to a weighted sum of the\namount of new information and improvement of the prediction accuracy provided\nby the variables. Then, using a greedy approach, the most informative subsets\nare selected in an iterative way. The algorithm terminates, when the highest\nranked variable is not able to significantly improve the accuracy of the\nprediction as compared to that obtained using the existing selected subsets. In\na simulation study, we compare our estimator to existing state-of-the-art\nmethods at different data lengths and directed dependencies strengths. It is\ndemonstrated that the proposed estimator has a significantly higher accuracy\nthan that of existing methods, especially for the difficult case, where the\ndata is highly correlated and coupled. Moreover, we show its false detection of\ndirected dependencies due to instantaneous couplings effect is lower than that\nof existing measures. We also show applicability of the proposed estimator on\nreal intracranial electroencephalography data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 10:51:23 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Baboukani", "Payam Shahsavari", ""], ["Graversen", "Carina", ""], ["Alickovic", "Emina", ""], ["\u00d8stergaard", "Jan", ""]]}, {"id": "2011.13694", "submitter": "Vincent Audigier", "authors": "Vincent Audigier, Nd\\`eye Niang", "title": "Clustering with missing data: which equivalent for Rubin's rules?", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation (MI) is a popular method for dealing with missing values.\nHowever, the suitable way for applying clustering after MI remains unclear: how\nto pool partitions? How to assess the clustering instability when data are\nincomplete? By answering both questions, this paper proposed a complete view of\nclustering with missing data using MI. The problem of partitions pooling is\nhere addressed using consensus clustering while, based on the bootstrap theory,\nwe explain how to assess the instability related to observed and missing data.\nThe new rules for pooling partitions and instability assessment are\ntheoretically argued and extensively studied by simulation. Partitions pooling\nimproves accuracy while measuring instability with missing data enlarges the\ndata analysis possibilities: it allows assessment of the dependence of the\nclustering to the imputation model, as well as a convenient way for choosing\nthe number of clusters when data are incomplete, as illustrated on a real data\nset.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 12:09:31 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Audigier", "Vincent", ""], ["Niang", "Nd\u00e8ye", ""]]}, {"id": "2011.13800", "submitter": "Adel Bedoui", "authors": "Adel Bedoui and Ori Rosen", "title": "Comparison of Bayesian Nonparametric Density Estimation Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a nonparametric Bayesian approach for Lindsey and\npenalized Gaussian mixtures methods. We compare these methods with the\nDirichlet process mixture model. Our approach is a Bayesian nonparametric\nmethod not based solely on a parametric family of probability distributions.\nThus, the fitted models are more robust to model misspecification. Also, with\nthe Bayesian approach, we have the entire posterior distribution of our\nparameter of interest; it can be summarized through credible intervals, mean,\nmedian, standard deviation, quantiles, etc. The Lindsey, penalized Gaussian\nmixtures, and Dirichlet process mixture methods are reviewed. The estimations\nare performed via Markov chain Monte Carlo (MCMC) methods. The penalized\nGaussian mixtures method is implemented via Hamiltonian Monte Carlo (HMC). We\nshow that under certain regularity conditions, and as n increases, the\nposterior distribution of the weights converges to a Normal distribution.\nSimulation results and data analysis are reported.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 15:53:36 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Bedoui", "Adel", ""], ["Rosen", "Ori", ""]]}, {"id": "2011.13884", "submitter": "Haeran Cho Dr", "authors": "Haeran Cho, Piotr Fryzlewicz", "title": "Multiple change point detection under serial dependence: Wild energy\n  maximisation and gappy Schwarz criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a methodology for detecting multiple change points in the mean of\nan otherwise stationary, autocorrelated, linear time series. It combines\nsolution path generation based on the wild energy maximisation principle, and\nan information criterion-based model selection strategy termed gappy Schwarz\ncriterion. The former is well-suited to separating shifts in the mean from\nfluctuations due to serial correlations, while the latter simultaneously\nestimates the dependence structure and the number of change points without\nperforming the difficult task of estimating the level of the noise as\nquantified e.g.\\ by the long-run variance. We provide modular investigation\ninto their theoretical properties and show that the combined methodology, named\nWEM.gSC, achieves consistency in estimating both the total number and the\nlocations of the change points. The good performance of WEM.gSC is demonstrated\nvia extensive simulation studies, and we further illustrate its usefulness by\napplying the methodology to London air quality data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 18:17:38 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 11:00:40 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 15:50:49 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Cho", "Haeran", ""], ["Fryzlewicz", "Piotr", ""]]}, {"id": "2011.13967", "submitter": "Zejian Liu", "authors": "Zejian Liu and Meng Li", "title": "Equivalence of Convergence Rates of Posterior Distributions and Bayes\n  Estimators for Functions and Nonparametric Functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the posterior contraction rates of a Bayesian method with Gaussian\nprocess priors in nonparametric regression and its plug-in property for\ndifferential operators. For a general class of kernels, we establish\nconvergence rates of the posterior measure of the regression function and its\nderivatives, which are both minimax optimal up to a logarithmic factor for\nfunctions in certain classes. Our calculation shows that the rate-optimal\nestimation of the regression function and its derivatives share the same choice\nof hyperparameter, indicating that the Bayes procedure remarkably adapts to the\norder of derivatives and enjoys a generalized plug-in property that extends\nreal-valued functionals to function-valued functionals. This leads to a\npractically simple method for estimating the regression function and its\nderivatives, whose finite sample performance is assessed using simulations.\n  Our proof shows that, under certain conditions, to any convergence rate of\nBayes estimators there corresponds the same convergence rate of the posterior\ndistributions (i.e., posterior contraction rate), and vice versa. This\nequivalence holds for a general class of Gaussian processes and covers the\nregression function and its derivative functionals, under both the $L_2$ and\n$L_{\\infty}$ norms. In addition to connecting these two fundamental large\nsample properties in Bayesian and non-Bayesian regimes, such equivalence\nenables a new routine to establish posterior contraction rates by calculating\nconvergence rates of nonparametric point estimators.\n  At the core of our argument is an operator-theoretic framework for kernel\nridge regression and equivalent kernel techniques. We derive a range of sharp\nnon-asymptotic bounds that are pivotal in establishing convergence rates of\nnonparametric point estimators and the equivalence theory, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 19:11:56 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Liu", "Zejian", ""], ["Li", "Meng", ""]]}, {"id": "2011.13993", "submitter": "Daren Wang", "authors": "Daren Wang, Zifeng Zhao, Rebecca Willett, Chun Yip Yau", "title": "Functional Autoregressive Processes in Reproducing Kernel Hilbert Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation and prediction of functional autoregressive~(FAR)\nprocesses, a statistical tool for modeling functional time series data. Due to\nthe infinite-dimensional nature of FAR processes, the existing literature\naddresses its inference via dimension reduction and theoretical results therein\nrequire the (unrealistic) assumption of fully observed functional time series.\nWe propose an alternative inference framework based on Reproducing Kernel\nHilbert Spaces~(RKHS). Specifically, a nuclear norm regularization method is\nproposed for estimating the transition operators of the FAR process directly\nfrom discrete samples of the functional time series. We derive a representer\ntheorem for the FAR process, which enables infinite-dimensional inference\nwithout dimension reduction. Sharp theoretical guarantees are established under\nthe (more realistic) assumption that we only have finite discrete samples of\nthe FAR process. Extensive numerical experiments and a real data application of\nenergy consumption prediction are further conducted to illustrate the promising\nperformance of the proposed approach compared to the state-of-the-art methods\nin the literature.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 21:16:48 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wang", "Daren", ""], ["Zhao", "Zifeng", ""], ["Willett", "Rebecca", ""], ["Yau", "Chun Yip", ""]]}, {"id": "2011.14185", "submitter": "Yang Ning", "authors": "Siyi Deng, Yang Ning, Jiwei Zhao, Heping Zhang", "title": "Optimal Semi-supervised Estimation and Inference for High-dimensional\n  Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There are many scenarios such as the electronic health records where the\noutcome is much more difficult to collect than the covariates. In this paper,\nwe consider the linear regression problem with such a data structure under the\nhigh dimensionality. Our goal is to investigate when and how the unlabeled data\ncan be exploited to improve the estimation and inference of the regression\nparameters in linear models, especially in light of the fact that such linear\nmodels may be misspecified in data analysis. In particular, we address the\nfollowing two important questions. (1) Can we use the labeled data as well as\nthe unlabeled data to construct a semi-supervised estimator such that its\nconvergence rate is faster than the supervised estimators? (2) Can we construct\nconfidence intervals or hypothesis tests that are guaranteed to be more\nefficient or powerful than the supervised estimators? To address the first\nquestion, we establish the minimax lower bound for parameter estimation in the\nsemi-supervised setting. We show that the upper bound from the supervised\nestimators that only use the labeled data cannot attain this lower bound. We\nclose this gap by proposing a new semi-supervised estimator which attains the\nlower bound. To address the second question, based on our proposed\nsemi-supervised estimator, we propose two additional estimators for\nsemi-supervised inference, the efficient estimator and the safe estimator. The\nformer is fully efficient if the unknown conditional mean function is estimated\nconsistently, but may not be more efficient than the supervised approach\notherwise. The latter usually does not aim to provide fully efficient\ninference, but is guaranteed to be no worse than the supervised approach, no\nmatter whether the linear model is correctly specified or the conditional mean\nfunction is consistently estimated.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 18:26:46 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Deng", "Siyi", ""], ["Ning", "Yang", ""], ["Zhao", "Jiwei", ""], ["Zhang", "Heping", ""]]}, {"id": "2011.14186", "submitter": "Chau-Wai Wong", "authors": "Ritesh Goenka, Shu-Jie Cao, Chau-Wai Wong, Ajit Rajwade, Dror Baron", "title": "Contact Tracing Enhances the Efficiency of COVID-19 Group Testing", "comments": "This version includes a supplemental document", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Group testing can save testing resources in the context of the ongoing\nCOVID-19 pandemic. In group testing, we are given $n$ samples, one per\nindividual, and arrange them into $m < n$ pooled samples, where each pool is\nobtained by mixing a subset of the $n$ individual samples. Infected individuals\nare then identified using a group testing algorithm. In this paper, we use side\ninformation (SI) collected from contact tracing (CT) within\nnon-adaptive/single-stage group testing algorithms. We generate data by\nincorporating CT SI and characteristics of disease spread between individuals.\nThese data are fed into two signal and measurement models for group testing,\nwhere numerical results show that our algorithms provide improved sensitivity\nand specificity. While Nikolopoulos et al. utilized family structure to improve\nnon-adaptive group testing, ours is the first work to explore and demonstrate\nhow CT SI can further improve group testing performance.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 18:32:14 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Goenka", "Ritesh", ""], ["Cao", "Shu-Jie", ""], ["Wong", "Chau-Wai", ""], ["Rajwade", "Ajit", ""], ["Baron", "Dror", ""]]}, {"id": "2011.14279", "submitter": "Lizhen Nie", "authors": "Lizhen Nie and Veronika Ro\\v{c}kov\\'a", "title": "Bayesian Bootstrap Spike-and-Slab LASSO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impracticality of posterior sampling has prevented the widespread\nadoption of spike-and-slab priors in high-dimensional applications. To\nalleviate the computational burden, optimization strategies have been proposed\nthat quickly find local posterior modes. Trading off uncertainty quantification\nfor computational speed, these strategies have enabled spike-and-slab\ndeployments at scales that would be previously unfeasible. We build on one\nrecent development in this strand of work: the Spike-and-Slab LASSO procedure\nof Ro\\v{c}kov\\'{a} and George (2018). Instead of optimization, however, we\nexplore multiple avenues for posterior sampling, some traditional and some new.\nIntrigued by the speed of Spike-and-Slab LASSO mode detection, we explore the\npossibility of sampling from an approximate posterior by performing MAP\noptimization on many independently perturbed datasets. To this end, we explore\nBayesian bootstrap ideas and introduce a new class of jittered Spike-and-Slab\nLASSO priors with random shrinkage targets. These priors are a key constituent\nof the Bayesian Bootstrap Spike-and-Slab LASSO (BB-SSL) method proposed here.\nBB-SSL turns fast optimization into approximate posterior sampling. Beyond its\nscalability, we show that BB-SSL has a strong theoretical support. Indeed, we\nfind that the induced pseudo-posteriors contract around the truth at a\nnear-optimal rate in sparse normal-means and in high-dimensional regression. We\ncompare our algorithm to the traditional Stochastic Search Variable Selection\n(under Laplace priors) as well as many state-of-the-art methods for shrinkage\npriors. We show, both in simulations and on real data, that our method fares\nsuperbly in these comparisons, often providing substantial computational gains.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 04:39:43 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 17:55:20 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Nie", "Lizhen", ""], ["Ro\u010dkov\u00e1", "Veronika", ""]]}, {"id": "2011.14423", "submitter": "Bryan Cai", "authors": "Bryan Cai, John P.A. Ioannidis, Eran Bendavid, Lu Tian", "title": "Exact Inference for Disease Prevalence Based on a Test with Unknown\n  Specificity and Sensitivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make informative public policy decisions in battling the ongoing COVID-19\npandemic, it is important to know the disease prevalence in a population. There\nare two intertwined difficulties in estimating this prevalence based on testing\nresults from a group of subjects. First, the test is prone to measurement error\nwith unknown sensitivity and specificity. Second, the prevalence tends to be\nlow at the initial stage of the pandemic and we may not be able to determine if\na positive test result is a false positive due to the imperfect specificity of\nthe test. The statistical inference based on large sample approximation or\nconventional bootstrap may not be sufficiently reliable and yield confidence\nintervals that do not cover the true prevalence at the nominal level. In this\npaper, we have proposed a set of 95% confidence intervals, whose validity is\nguaranteed and doesn't depend on the sample size in the unweighted setting. For\nthe weighted setting, the proposed inference is equivalent to a class of hybrid\nbootstrap methods, whose performance is also more robust to the sample size\nthan those based on asymptotic approximations. The methods are used to\nreanalyze data from a study investigating the antibody prevalence in Santa\nClara county, California, which was the motivating example of this research, in\naddition to several other seroprevalence studies where authors had tried to\ncorrect their estimates for test performance. Extensive simulation studies have\nbeen conducted to examine the finite-sample performance of the proposed\nconfidence intervals.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 19:15:24 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Cai", "Bryan", ""], ["Ioannidis", "John P. A.", ""], ["Bendavid", "Eran", ""], ["Tian", "Lu", ""]]}, {"id": "2011.14529", "submitter": "Wei Ling Katherine Tan", "authors": "W Katherine Tan, Patrick J Heagerty", "title": "Predictive case control designs for modification learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prediction models for clinical outcomes may be developed using a source\ndataset and additionally applied to new settings. Towards model external\nvalidation and model updating in the new setting, one procedure is model\nmodification learning that involves the dual goals of recalibrating overall\npredictions as well as revising individual feature effects. Modification\nlearning generally requires the collection of an adequate sample of true\noutcome labels from the new setting, which is frequently an expensive and\ntime-consuming process, as it involves abstraction by human clinical experts.\nTo reduce the abstraction burden for such new data collection, we propose a\nclass of designs based on original model scores and their associated outcome\npredictions. We provide mathematical justification that the general predictive\nscore sampling class results in valid samples for analysis. Then, we focus\nattention specifically on a stratified sampling procedure that we call\npredictive case control (PCC) sampling, which allows the dual modification\nlearning goals to be achieved at a smaller sample size compared to simple\nrandom sampling (SRS). PCC sampling intentionally over-represents subjects with\ninformative scores, where we suggest using the D-optimality and Binary Entropy\ninformation functions to summarize sample information. For design evaluation\nwithin the PCC class, we provide a computational framework to estimate and\nvisualize empirical response surfaces of the proposed information functions. We\ndemonstrate the benefit of using PCC designs for modification learning,\nrelative to SRS, through Monte Carlo simulation. Finally, using radiology\nreport data from the Lumbar Imaging with Reporting of Epidemiology (LIRE)\nstudy, we illustrate the application of PCC for new outcome label abstraction\nand subsequent modification learning across imaging modalities.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 03:40:28 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Tan", "W Katherine", ""], ["Heagerty", "Patrick J", ""]]}, {"id": "2011.14542", "submitter": "Kevin W. Lu", "authors": "Kevin W. Lu", "title": "Calibration for multivariate L\\'evy-driven Ornstein-Uhlenbeck processes\n  with applications to weak subordination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a multivariate L\\'evy-driven Ornstein-Uhlenbeck process where the\nstationary distribution or background driving L\\'evy process is from a\nparametric family. We derive the likelihood function assuming that the\ninnovation term is absolutely continuous. Two examples are studied in detail:\nthe process where the stationary distribution or background driving L\\'evy\nprocess is given by a weak variance alpha-gamma process, which is a\nmultivariate generalisation of the variance gamma process created using weak\nsubordination. In the former case, we give an explicit representation of the\nbackground driving L\\'evy process, leading to an innovation term with a\nmixed-type distribution, allowing for the exact simulation of the process, and\na separate likelihood function. In the latter case, we show the innovation term\nis absolutely continuous. The results of a simulation study demonstrate that\nmaximum likelihood numerically computed using Fourier inversion can be applied\nto accurately estimate the parameters in both cases.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 04:28:30 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 02:21:58 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Lu", "Kevin W.", ""]]}, {"id": "2011.14625", "submitter": "Asher Spector", "authors": "Asher Spector and Lucas Janson", "title": "Powerful Knockoffs via Minimizing Reconstructability", "comments": "72 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-X knockoffs allows analysts to perform feature selection using almost\nany machine learning algorithm while still provably controlling the expected\nproportion of false discoveries. To apply model-X knockoffs, one must construct\nsynthetic variables, called knockoffs, which effectively act as controls during\nfeature selection. The gold standard for constructing knockoffs has been to\nminimize the mean absolute correlation (MAC) between features and their\nknockoffs, but, surprisingly, we prove this procedure can be powerless in\nextremely easy settings, including Gaussian linear models with correlated\nexchangeable features. The key problem is that minimizing the MAC creates\nstrong joint dependencies between the features and knockoffs, which allow\nmachine learning algorithms to partially or fully reconstruct the effect of the\nfeatures on the response using the knockoffs. To improve the power of\nknockoffs, we propose generating knockoffs which minimize the\nreconstructability (MRC) of the features, and we demonstrate our proposal for\nGaussian features by showing it is computationally efficient, robust, and\npowerful. We also prove that certain MRC knockoffs minimize a natural\ndefinition of estimation error in Gaussian linear models. Furthermore, in an\nextensive set of simulations, we find many settings with correlated features in\nwhich MRC knockoffs dramatically outperform MAC-minimizing knockoffs and no\nsettings in which MAC-minimizing knockoffs outperform MRC knockoffs by more\nthan a very slight margin. We implement our methods and a host of others from\nthe knockoffs literature in a new open source python package knockpy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:55:19 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 03:40:28 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Spector", "Asher", ""], ["Janson", "Lucas", ""]]}, {"id": "2011.14650", "submitter": "Maximilian Aigner", "authors": "Maximilian Aigner, Val\\'erie Chavez-Demoulin", "title": "A competing risks interpretation of Hawkes processes", "comments": "14 pages, 2 figures; submitted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a construction of the Hawkes process as a piecewise competing risks\nmodel. We argue that the most natural interpretation of the self-excitation\nkernel is the hazard function of a defective random variable. This establishes\na link between desired qualitative features of the process and a parametric\nform for the kernel, which we illustrate using examples from the literature.\nTwo families of cure rate models taken from the survival analysis literature\nare proposed as new models for the self-excitation kernel. Finally, we show\nthat the competing risks viewpoint leads to a general simulation algorithm\nwhich avoids inverting the compensator of the point process or performing an\naccept-reject step, and is therefore fast and quite general.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 09:42:34 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 07:28:41 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Aigner", "Maximilian", ""], ["Chavez-Demoulin", "Val\u00e9rie", ""]]}, {"id": "2011.14735", "submitter": "Roland Gerard Gera M.sc.", "authors": "Roland Gerard Gera and Tim Friede", "title": "Blinded sample size re-calculation in multiple composite population\n  designs with normal data and baseline adjustments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing interest in subpopulation analysis has led to the development\nof various new trial designs and analysis methods in the fields of personalized\nmedicine and targeted therapies. In this paper, subpopulations are defined in\nterms of an accumulation of disjoint population subsets and will therefore be\ncalled composite populations. The proposed trial design is applicable to any\nset of composite populations, considering normally distributed endpoints and\nrandom baseline covariates. Treatment effects for composite populations are\ntested by combining $p$-values, calculated on the subset levels, using the\ninverse normal combination function to generate test statistics for those\ncomposite populations. The family-wise type I error rate for simultaneous\ntesting is controlled in the strong sense by the application of the closed\ntesting procedure. Critical values for intersection hypothesis tests are\nderived using multivariate normal distributions, reflecting the joint\ndistribution of composite population test statistics under the null hypothesis.\nFor sample size calculation and sample size re-calculation multivariate normal\ndistributions are derived which describe the joint distribution of composite\npopulation test statistics under an assumed alternative hypothesis. Simulations\ndemonstrate the strong control of the family-wise type I error rate in fixed\ndesigns and re-calculation designs with blinded sample size re-calculation. The\ntarget power after sample size re-calculation is typically met or close to\nbeing met.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 12:28:32 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Gera", "Roland Gerard", ""], ["Friede", "Tim", ""]]}, {"id": "2011.14762", "submitter": "Benjamin Eltzner", "authors": "Benjamin Eltzner", "title": "Testing for Uniqueness of Estimators", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniqueness of the population value of an estimated descriptor is a standard\nassumption in asymptotic theory. However, m-estimation problems often allow for\nlocal minima of the sample estimating function, which may stem from multiple\nglobal minima of the underlying population estimating function. In the present\narticle, we provide tools to systematically determine for a given sample\nwhether the underlying population estimating function may have multiple global\nminima. To achieve this goal, we develop asymptotic theory for non-unique\nminimizers and introduce asymptotic tests using the bootstrap. We discuss three\napplications of our tests to data, each of which presents a typical scenario in\nwhich non-uniqueness of descriptors may occur. These model scenarios are the\nmean on a non-euclidean space, non-linear regression and Gaussian mixture\nclustering.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:18:39 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Eltzner", "Benjamin", ""]]}, {"id": "2011.14817", "submitter": "Christophe Ley", "authors": "Sla{\\dj}ana Babi\\'c and Christophe Ley and Lorenzo Ricci and David\n  Veredas", "title": "TailCoR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economic and financial crises are characterised by unusually large events.\nThese tail events co-move because of linear and/or nonlinear dependencies. We\nintroduce TailCoR, a metric that combines (and disentangles) these linear and\nnon-linear dependencies. TailCoR between two variables is based on the tail\ninter quantile range of a simple projection. It is dimension-free, it performs\nwell in small samples, and no optimisations are needed.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 20:58:58 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Babi\u0107", "Sla\u0111ana", ""], ["Ley", "Christophe", ""], ["Ricci", "Lorenzo", ""], ["Veredas", "David", ""]]}, {"id": "2011.14850", "submitter": "Lingxiao Wang", "authors": "Lingxiao Wang, Barry I. Graubard, Hormuzd A. Katki, and Yan Li", "title": "Efficient and Robust Propensity-Score-Based Methods for Population\n  Inference using Epidemiologic Cohorts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most epidemiologic cohorts are composed of volunteers who do not represent\nthe general population. To enable population inference from cohorts, we and\nothers have proposed utilizing probability survey samples as external\nreferences to develop a propensity score (PS) for membership in the cohort\nversus survey. Herein we develop a unified framework for PS-based weighting\n(such as inverse PS weighting (IPSW)) and matching methods (such as\nkernel-weighting (KW) method). We identify a fundamental Strong Exchangeability\nAssumption (SEA) underlying existing PS-based matching methods whose failure\ninvalidates inference even if the PS-model is correctly specified. We relax the\nSEA to a Weak Exchangeability Assumption (WEA) for the matching method. Also,\nwe propose IPSW.S and KW.S methods that reduce the variance of PS-based\nestimators by scaling the survey weights used in the PS estimation. We prove\nconsistency of the IPSW.S and KW.S estimators of population means and\nprevalences under WEA, and provide asymptotic variances and consistent variance\nestimators. In simulations, the KW.S and IPSW.S estimators had smallest MSE. In\nour data example, the original KW estimates had large bias, whereas the KW.S\nestimates had the smallest MSE.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:44:18 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wang", "Lingxiao", ""], ["Graubard", "Barry I.", ""], ["Katki", "Hormuzd A.", ""], ["Li", "Yan", ""]]}, {"id": "2011.14865", "submitter": "Hana \\v{S}inkovec", "authors": "Hana \\v{S}inkovec and Angelika Geroldinger and Georg Heinze and Rok\n  Blagus", "title": "Tuning in ridge logistic regression to solve separation", "comments": "45 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separation in logistic regression is a common problem causing failure of the\niterative estimation process when finding maximum likelihood estimates. Firth's\ncorrection (FC) was proposed as a solution, providing estimates also in\npresence of separation. In this paper we evaluate whether ridge regression (RR)\ncould be considered instead, specifically, if it could reduce the mean squared\nerror (MSE) of coefficient estimates in comparison to FC. In RR the tuning\nparameter determining the penalty strength is usually obtained by minimizing\nsome measure of the out-of-sample prediction error or information criterion.\nHowever, in presence of separation tuning these measures can yield an optimized\nvalue of zero (no shrinkage), and hence cannot provide a universal solution. We\nderive a new bootstrap based tuning criterion $B$ that always leads to\nshrinkage. Moreover, we demonstrate how valid inference can be obtained by\ncombining resampled profile penalized likelihood functions. Our approach is\nillustrated in an example from oncology and its performance is compared to FC\nin a simulation study. Our simulations showed that in analyses of small and\nsparse datasets and with many correlated covariates $B$-tuned RR can yield\ncoefficient estimates with MSE smaller than FC and confidence intervals that\napproximately achieve nominal coverage probabilities.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 15:03:08 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["\u0160inkovec", "Hana", ""], ["Geroldinger", "Angelika", ""], ["Heinze", "Georg", ""], ["Blagus", "Rok", ""]]}, {"id": "2011.14990", "submitter": "Vivek Gopalakrishnan", "authors": "Vivek Gopalakrishnan, Jaewon Chung, Eric Bridgeford, Benjamin D.\n  Pedigo, Jes\\'us Arroyo, Lucy Upchurch, G. Allan Johnson, Nian Wang, Youngser\n  Park, Carey E. Priebe, Joshua T. Vogelstein", "title": "Multiscale Comparative Connectomics", "comments": "29 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A connectome is a map of the structural and/or functional connections in the\nbrain. This information-rich representation has the potential to transform our\nunderstanding of the relationship between patterns in brain connectivity and\nneurological processes, disorders, and diseases. However, existing\ncomputational techniques used to analyze connectomes are often insufficient for\ninterrogating multi-subject connectomics datasets. Several methods are either\nsolely designed to analyze single connectomes, or leverage heuristic graph\ninvariants that ignore the complete topology of connections between brain\nregions. To enable more rigorous comparative connectomics analysis, we\nintroduce robust and interpretable statistical methods motivated by recent\ntheoretical advances in random graph models. These methods enable simultaneous\nanalysis of multiple connectomes across different scales of network topology,\nfacilitating the discovery of hierarchical brain structures that vary in\nrelation with phenotypic profiles. We validated these methods through extensive\nsimulation studies, as well as synthetic and real-data experiments. Using a set\nof high-resolution connectomes obtained from genetically distinct mouse strains\n(including the BTBR mouse -- a standard model of autism -- and three behavioral\nwild-types), we show that these methods uncover valuable latent information in\nmulti-subject connectomics data and yield novel insights into the connective\ncorrelates of neurological phenotypes.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 16:58:25 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 20:31:55 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 06:24:43 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Gopalakrishnan", "Vivek", ""], ["Chung", "Jaewon", ""], ["Bridgeford", "Eric", ""], ["Pedigo", "Benjamin D.", ""], ["Arroyo", "Jes\u00fas", ""], ["Upchurch", "Lucy", ""], ["Johnson", "G. Allan", ""], ["Wang", "Nian", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "2011.14996", "submitter": "Emily C Hector", "authors": "Emily C. Hector and Peter X.-K. Song", "title": "Joint integrative analysis of multiple data sources with correlated\n  vector outcomes", "comments": "26 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed quadratic inference function framework to jointly\nestimate regression parameters from multiple potentially heterogeneous data\nsources with correlated vector outcomes. The primary goal of this joint\nintegrative analysis is to estimate covariate effects on all outcomes through a\nmarginal regression model in a statistically and computationally efficient way.\nWe develop a data integration procedure for statistical estimation and\ninference of regression parameters that is implemented in a fully distributed\nand parallelized computational scheme. To overcome computational and modeling\nchallenges arising from the high-dimensional likelihood of the correlated\nvector outcomes, we propose to analyze each data source using Qu, Lindsay and\nLi (2000)'s quadratic inference functions, and then to jointly reestimate\nparameters from each data source by accounting for correlation between data\nsources using a combined meta-estimator in a similar spirit to Hansen (1982)'s\ngeneralised method of moments. We show both theoretically and numerically that\nthe proposed method yields efficiency improvements and is computationally fast.\nWe illustrate the proposed methodology with the joint integrative analysis of\nthe association between smoking and metabolites in a large multi-cohort study\nand provide an R package for ease of implementation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:03:04 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Hector", "Emily C.", ""], ["Song", "Peter X. -K.", ""]]}, {"id": "2011.14999", "submitter": "Ryan Giordano", "authors": "Tamara Broderick, Ryan Giordano, and Rachael Meager", "title": "An Automatic Finite-Sample Robustness Metric: Can Dropping a Little Data\n  Change Conclusions?", "comments": "71 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to assess the sensitivity of econometric analyses to the\nremoval of a small fraction of the sample. Analyzing all possible data subsets\nof a certain size is computationally prohibitive, so we provide a finite-sample\nmetric to approximately compute the number (or fraction) of observations that\nhas the greatest influence on a given result when dropped. We call our\nresulting metric the Approximate Maximum Influence Perturbation. Our\napproximation is automatically computable and works for common estimators\n(including OLS, IV, GMM, MLE, and variational Bayes). We provide explicit\nfinite-sample error bounds on our approximation for linear and instrumental\nvariables regressions. At minimal computational cost, our metric provides an\nexact finite-sample lower bound on sensitivity for any estimator, so any\nnon-robustness our metric finds is conclusive. We demonstrate that the\nApproximate Maximum Influence Perturbation is driven by a low signal-to-noise\nratio in the inference problem, is not reflected in standard errors, does not\ndisappear asymptotically, and is not a product of misspecification. Several\nempirical applications show that even 2-parameter linear regression analyses of\nrandomized trials can be highly sensitive. While we find some applications are\nrobust, in others the sign of a treatment effect can be changed by dropping\nless than 1% of the sample even when standard errors are small.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:05:48 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 14:11:11 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Broderick", "Tamara", ""], ["Giordano", "Ryan", ""], ["Meager", "Rachael", ""]]}, {"id": "2011.15004", "submitter": "Erik van Zwet", "authors": "Erik van Zwet and Simon Schwab and Stephen Senn", "title": "The statistical properties of RCTs and a proposal for shrinkage", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We abstract the concept of a randomized controlled trial (RCT) as a triple\n(beta,b,s), where beta is the primary efficacy parameter, b the estimate and s\nthe standard error (s>0). The parameter beta is either a difference of means, a\nlog odds ratio or a log hazard ratio. If we assume that b is unbiased and\nnormally distributed, then we can estimate the full joint distribution of\n(beta,b,s) from a sample of pairs (b_i,s_i). We have collected 23,747 such\npairs from the Cochrane database to do so. Here, we report the estimated\ndistribution of the signal-to-noise ratio beta/s and the achieved power. We\nestimate the median achieved power to be 0.13. We also consider the\nexaggeration ratio which is the factor by which the magnitude of beta is\noverestimated. We find that if the estimate is just significant at the 5%\nlevel, we would expect it to overestimate the true effect by a factor of 1.7.\nThis exaggeration is sometimes referred to as the winner's curse and it is\nundoubtedly to a considerable extent responsible for disappointing replication\nresults. For this reason, we believe it is important to shrink the unbiased\nestimator, and we propose a method for doing so.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:09:20 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["van Zwet", "Erik", ""], ["Schwab", "Simon", ""], ["Senn", "Stephen", ""]]}, {"id": "2011.15037", "submitter": "Erik van Zwet", "authors": "Erik van Zwet and Andrew Gelman", "title": "A proposal for informative default priors scaled by the standard error\n  of estimates", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  If we have an unbiased estimate of some parameter of interest, then its\nabsolute value is positively biased for the absolute value of the parameter.\nThis bias is large when the signal-to-noise ratio (SNR) is small, and it\nbecomes even larger when we condition on statistical significance; the winner's\ncurse. This is a frequentist motivation for regularization. To determine a\nsuitable amount of shrinkage, we propose to estimate the distribution of the\nSNR from a large collection or corpus of similar studies and use this as a\nprior distribution. The wider the scope of the corpus, the less informative the\nprior, but a wider scope does not necessarily result in a more diffuse prior.\nWe show that the estimation of the prior simplifies if we require that\nposterior inference is equivariant under linear transformations of the data. We\ndemonstrate our approach with corpora of 86 replication studies from psychology\nand 178 phase 3 clinical trials. Our suggestion is not intended to be a\nreplacement for a prior based on full information about a particular problem;\nrather, it represents a familywise choice that should yield better long-term\nproperties than the current default uniform prior, which has led to systematic\noverestimates of effect sizes and a replication crisis when these inflated\nestimates have not shown up in later studies.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:39:43 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["van Zwet", "Erik", ""], ["Gelman", "Andrew", ""]]}, {"id": "2011.15099", "submitter": "Roy Adams", "authors": "Roy Adams, Suchi Saria, Michael Rosenblum", "title": "The Impact of Time Series Length and Discretization on Longitudinal\n  Causal Estimation Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of observational time series data to assess the impact of multi-time\npoint interventions is becoming increasingly common as more health and activity\ndata are collected and digitized via wearables, social media, and electronic\nhealth records. Such time series may involve hundreds or thousands of\nirregularly sampled observations. One common analysis approach is to simplify\nsuch time series by first discretizing them into sequences before applying a\ndiscrete-time estimation method that adjusts for time-dependent confounding. In\ncertain settings, this discretization results in sequences with many time\npoints; however, the empirical properties of longitudinal causal estimators\nhave not been systematically compared on long sequences. We compare three\nrepresentative longitudinal causal estimation methods on simulated and real\nclinical data. Our simulations and analyses assume a Markov structure and that\nlongitudinal treatments/exposures are binary-valued and have at most a single\njump point. We identify sources of bias that arise from temporally discretizing\nthe data and provide practical guidance for discretizing data and choosing\nbetween methods when working with long sequences. Additionally, we compare\nthese estimators on real electronic health record data, evaluating the impact\nof early treatment for patients with a life-threatening complication of\ninfection called sepsis.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:32:19 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Adams", "Roy", ""], ["Saria", "Suchi", ""], ["Rosenblum", "Michael", ""]]}]