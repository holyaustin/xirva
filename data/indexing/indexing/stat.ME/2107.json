[{"id": "2107.00074", "submitter": "Daniel Gervini", "authors": "Daniel Gervini", "title": "Spatial kriging for replicated temporal point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a kriging method for spatial prediction of temporal\nintensity functions, for situations where a temporal point process is observed\nat different spatial locations. Assuming that several replications of the\nprocesses are available at the spatial sites, this method avoids assumptions\nlike isotropy, which are not valid in many applications. As part of the\nderivations, new nonparametric estimators for the mean and covariance functions\nof temporal point processes are introduced, and their properties are studied\ntheoretically and by simulation. The method is applied to the analysis of bike\ndemand patterns in the Divvy bicycle sharing system of the city of Chicago.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 19:49:55 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Gervini", "Daniel", ""]]}, {"id": "2107.00109", "submitter": "Qiang Sun", "authors": "Qiang Sun, Rui Mao, Wen-Xin Zhou", "title": "Adaptive Capped Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes the capped least squares regression with an adaptive\nresistance parameter, hence the name, adaptive capped least squares regression.\nThe key observation is, by taking the resistant parameter to be data dependent,\nthe proposed estimator achieves full asymptotic efficiency without losing the\nresistance property: it achieves the maximum breakdown point asymptotically.\nComputationally, we formulate the proposed regression problem as a quadratic\nmixed integer programming problem, which becomes computationally expensive when\nthe sample size gets large. The data-dependent resistant parameter, however,\nmakes the loss function more convex-like for larger-scale problems. This makes\na fast randomly initialized gradient descent algorithm possible for global\noptimization. Numerical examples indicate the superiority of the proposed\nestimator compared with classical methods. Three data applications to cancer\ncell lines, stationary background recovery in video surveillance, and blind\nimage inpainting showcase its broad applicability.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 21:28:56 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Sun", "Qiang", ""], ["Mao", "Rui", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "2107.00118", "submitter": "Qiang Sun", "authors": "Qiang Sun", "title": "Do we need to estimate the variance in robust mean estimation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies robust mean estimators for distributions with only finite\nvariances. We propose a new loss function that is a function of the mean\nparameter and a robustification parameter. By simultaneously optimizing the\nempirical loss with respect to both parameters, we show that the resulting\nestimator for the robustification parameter can automatically adapt to the data\nand the unknown variance. Thus the resulting mean estimator can achieve\nnear-optimal finite-sample performance. Compared with prior work, our method is\ncomputationally efficient and user-friendly. It does not need cross-validation\nto tune the robustification parameter.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 22:00:00 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Sun", "Qiang", ""]]}, {"id": "2107.00122", "submitter": "Rachael C Aikens", "authors": "Rachael C. Aikens and Michael Baiocchi", "title": "Assignment-Control Plots: A Visual Companion for Causal Inference Study\n  Design", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important step for any causal inference study design is understanding the\ndistribution of the treated and control subjects in terms of measured baseline\ncovariates. However, not all baseline variation is equally important. In the\nobservational context, balancing on baseline variation summarized in a\npropensity score can help reduce bias due to self-selection. In both\nobservational and experimental studies, controlling baseline variation\nassociated with the expected outcomes can help increase the precision of causal\neffect estimates. We propose a set of visualizations which decompose the space\nof measured covariates into the different types of baseline variation important\nto the study design. These ``assignment-control plots'' and variations thereof\nvisually illustrate core concepts of causal inference and suggest new\ndirections for methodological research on study design. As a practical\ndemonstration, we illustrate one application of assignment-control plots to a\nstudy of cardiothoracic surgery. While the family of visualization tools for\nstudies of causality is relatively sparse, simple visual tools can be an asset\nto education, application, and methods development.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 22:08:21 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Aikens", "Rachael C.", ""], ["Baiocchi", "Michael", ""]]}, {"id": "2107.00153", "submitter": "Min Xu", "authors": "Harry Crane and Min Xu", "title": "Root and community inference on the latent growth process of a network\n  using noisy attachment models", "comments": "52 pages; 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the PAPER (Preferential Attachment Plus Erd\\H{o}s--R\\'{e}nyi)\nmodel for random networks, where we let a random network G be the union of a\npreferential attachment (PA) tree T and additional Erd\\H{o}s--R\\'{e}nyi (ER)\nrandom edges. The PA tree component captures the fact that real world networks\noften have an underlying growth/recruitment process where vertices and edges\nare added sequentially, while the ER component can be regarded as random noise.\nGiven only a single snapshot of the final network G, we study the problem of\nconstructing confidence sets for the early history, in particular the root\nnode, of the unobserved growth process; the root node can be patient zero in a\ndisease infection network or the source of fake news in a social media network.\nWe propose an inference algorithm based on Gibbs sampling that scales to\nnetworks with millions of nodes and provide theoretical analysis showing that\nthe expected size of the confidence set is small so long as the noise level of\nthe ER edges is not too large. We also propose variations of the model in which\nmultiple growth processes occur simultaneously, reflecting the growth of\nmultiple communities, and we use these models to provide a new approach\ncommunity detection.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 00:11:04 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 23:30:49 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Crane", "Harry", ""], ["Xu", "Min", ""]]}, {"id": "2107.00219", "submitter": "Brian Liu", "authors": "Brian Liu and Miaolan Xie and Madeleine Udell", "title": "ControlBurn: Feature Selection by Sparse Forests", "comments": "15 pages", "journal-ref": null, "doi": "10.1145/3447548.3467387", "report-no": null, "categories": "cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tree ensembles distribute feature importance evenly amongst groups of\ncorrelated features. The average feature ranking of the correlated group is\nsuppressed, which reduces interpretability and complicates feature selection.\nIn this paper we present ControlBurn, a feature selection algorithm that uses a\nweighted LASSO-based feature selection method to prune unnecessary features\nfrom tree ensembles, just as low-intensity fire reduces overgrown vegetation.\nLike the linear LASSO, ControlBurn assigns all the feature importance of a\ncorrelated group of features to a single feature. Moreover, the algorithm is\nefficient and only requires a single training iteration to run, unlike\niterative wrapper-based feature selection methods. We show that ControlBurn\nperforms substantially better than feature selection methods with comparable\ncomputational costs on datasets with correlated features.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 05:14:51 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Liu", "Brian", ""], ["Xie", "Miaolan", ""], ["Udell", "Madeleine", ""]]}, {"id": "2107.00224", "submitter": "Sven Knoth", "authors": "Sven Knoth and William H. Woodall and V\\'ictor G. Tercero-G\\'omez", "title": "The Case against Generally Weighted Moving Average (GWMA) Control Charts", "comments": "8 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue against the use of generally weighted moving average (GWMA) control\ncharts. Our primary reasons are the following: 1) There is no recursive formula\nfor the GWMA control chart statistic, so all previous data must be stored and\nused in the calculation of each chart statistic. 2) The Markovian property does\nnot apply to the GWMA statistics, so computer simulation must be used to\ndetermine control limits and the statistical performance. 3) An appropriately\ndesigned, and much simpler, exponentially weighted moving average (EWMA) chart\nprovides as good or better statistical performance. 4) In some cases the GWMA\nchart gives more weight to past data values than to current values.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 05:42:44 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Knoth", "Sven", ""], ["Woodall", "William H.", ""], ["Tercero-G\u00f3mez", "V\u00edctor G.", ""]]}, {"id": "2107.00248", "submitter": "David Choi", "authors": "David Choi", "title": "Randomization-only Inference in Experiments with Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In experiments that study social phenomena, such as peer influence or herd\nimmunity, the treatment of one unit may influence the outcomes of others. Such\n\"interference between units\" violates traditional approaches for causal\ninference, so that additional assumptions are required to model the underlying\nsocial mechanism. We propose an approach that requires no such assumptions,\nallowing for interference that is both unmodeled and strong, with confidence\nintervals found using only the randomization of treatment. Additionally, the\napproach allows for the usage of regression, matching, or weighting, as may\nbest fit the application at hand. Inference is done by bounding the\ndistribution of the estimation error over all possible values of the unknown\ncounterfactual, using an integer program. Examples are shown using a vaccine\ntrial and two experiments investigating social influence.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 06:59:14 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 18:00:00 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Choi", "David", ""]]}, {"id": "2107.00375", "submitter": "Michael Schweinberger", "authors": "Michael Schweinberger, Rashmi P. Bomiriya, Sergii Babkin", "title": "A semiparametric Bayesian approach to epidemics, with application to the\n  spread of the coronavirus MERS in South Korea in 2015", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider incomplete observations of stochastic processes governing the\nspread of infectious diseases through finite populations by way of contact. We\npropose a flexible semiparametric modeling framework with at least three\nadvantages. First, it enables researchers to study the structure of a\npopulation contact network and its impact on the spread of infectious diseases.\nSecond, it can accommodate short- and long-tailed degree distributions and\ndetect potential superspreaders, who represent an important public health\nconcern. Third, it addresses the important issue of incomplete data. Starting\nfrom first principles, we show when the incomplete-data generating process is\nignorable for the purpose of Bayesian inference for the parameters of the\npopulation model. We demonstrate the semiparametric modeling framework by\nsimulations and an application to the partially observed MERS epidemic in South\nKorea in 2015. We conclude with an extended discussion of open questions and\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 11:22:55 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Schweinberger", "Michael", ""], ["Bomiriya", "Rashmi P.", ""], ["Babkin", "Sergii", ""]]}, {"id": "2107.00470", "submitter": "Noemi Corsini", "authors": "Noemi Corsini and Cinzia Viroli", "title": "Dealing with overdispersion in multivariate count data", "comments": "21 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of overdispersion in multivariate count data is a challenging\nissue. Nowadays, it covers a central role mainly due to the relevance of modern\ntechnologies data, such as Next Generation Sequencing and textual data from the\nweb or digital collections. This work presents a comprehensive analysis of the\nlikelihood-based models for extra-variation data proposed in the scientific\nliterature. Particular attention will be paid to the models feasible for\nhigh-dimensional data. A new approach together with its parametric-estimation\nprocedure is proposed. It is a deeper version of the Dirichlet-Multinomial\ndistribution and it leads to important results allowing to get a better\napproximation of the observed variability. A significative comparison of these\nmodels is made through two different simulation studies that both confirm that\nthe new model considered in this work allows to achieve the best results.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 14:19:40 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Corsini", "Noemi", ""], ["Viroli", "Cinzia", ""]]}, {"id": "2107.00502", "submitter": "Naomi Elizabeth Hannaford", "authors": "Naomi E. Hannaford, Sarah E. Heaps, Tom M. W. Nye, Thomas P. Curtis,\n  Ben Allen, Andrew Golightly, Darren J. Wilkinson", "title": "A sparse Bayesian hierarchical vector autoregressive model for microbial\n  dynamics in a wastewater treatment plant", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proper function of a wastewater treatment plant (WWTP) relies on maintaining\na delicate balance between a multitude of competing microorganisms. Gaining a\ndetailed understanding of the complex network of interactions therein is\nessential to maximising not only current operational efficiencies, but also for\nthe effective design of new treatment technologies. Metagenomics offers an\ninsight into these dynamic systems through the analysis of the microbial DNA\nsequences present. Unique taxa are inferred through sequence clustering to form\noperational taxonomic units (OTUs), with per-taxa abundance estimates obtained\nfrom corresponding sequence counts. The data in this study comprise weekly OTU\ncounts from an activated sludge (AS) tank of a WWTP. To model the OTU dynamics,\nwe develop a Bayesian hierarchical vector autoregressive model, which is a\nlinear approximation to the commonly used generalised Lotka-Volterra (gLV)\nmodel. To tackle the high dimensionality and sparsity of the data, they are\nfirst clustered into 12 \"bins\" using a seasonal phase-based approach. The\nautoregressive coefficient matrix is assumed to be sparse, so we explore\ndifferent shrinkage priors by analysing simulated data sets before selecting\nthe regularised horseshoe prior for the biological application. We find that\nammonia and chemical oxygen demand have a positive relationship with several\nbins and pH has a positive relationship with one bin. These results are\nsupported by findings in the biological literature. We identify several\nnegative interactions, which suggests OTUs in different bins may be competing\nfor resources and that these relationships are complex. We also identify two\npositive interactions. Although simpler than a gLV model, our vector\nautoregression offers valuable insight into the microbial dynamics of the WWTP.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 14:49:19 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Hannaford", "Naomi E.", ""], ["Heaps", "Sarah E.", ""], ["Nye", "Tom M. W.", ""], ["Curtis", "Thomas P.", ""], ["Allen", "Ben", ""], ["Golightly", "Andrew", ""], ["Wilkinson", "Darren J.", ""]]}, {"id": "2107.00527", "submitter": "Jacopo Diquigiovanni", "authors": "Jacopo Diquigiovanni, Matteo Fontana, Simone Vantini", "title": "Distribution-Free Prediction Bands for Multivariate Functional Time\n  Series: an Application to the Italian Gas Market", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Uncertainty quantification in forecasting represents a topic of great\nimportance in statistics, especially when dealing with complex data\ncharacterized by non-trivial dependence structure. Pushed by novel works\nconcerning distribution-free prediction, we propose a scalable procedure that\noutputs closed-form simultaneous prediction bands for multivariate functional\nresponse variables in a time series setting, which is able to guarantee\nperformance bounds in terms of unconditional coverage and asymptotic exactness,\nboth under some conditions. After evaluating its performance on synthetic data,\nthe method is used to build multivariate prediction bands for daily demand and\noffer curves in the Italian gas market. The prediction framework thus obtained\nallows traders to directly evaluate the impact of their own offers/bids on the\nmarket, providing an intriguing tool for the business practice.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 15:13:16 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Diquigiovanni", "Jacopo", ""], ["Fontana", "Matteo", ""], ["Vantini", "Simone", ""]]}, {"id": "2107.00633", "submitter": "Mohamed Chaouch", "authors": "Kilani Ghoudi, Na\\^amane La\\\"ib, Mohamed Chaouch", "title": "Joint parametric specification checking of conditional mean and\n  volatility in time series models with martingale difference innovations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using cumulative residual processes, we propose joint goodness-of-fit tests\nfor conditional means and variances functions in the context of nonlinear time\nseries with martingale difference innovations. The main challenge comes from\nthe fact the cumulative residual process no longer admits, under the null\nhypothesis, a distribution-free limit. To obtain a practical solution one\neither transforms the process in order to achieve a distribution-free limit or\napproximates the non-distribution free limit using a numerical or a re-sampling\ntechnique. Here the three solutions will be considered.It is shown that the\nproposed tests have nontrivial power against a class of root-n local\nalternatives, and are suitable when the conditioning information set is\ninfinite-dimensional, which allows including models like autoregressive\nconditional heteroscedastic stochastic models with dependent innovations. The\napproach presented assumes only certain conditions on the first- and\nsecond-order conditional moments, without imposing any autoregression model.\nThe test procedures introduced are compared with each other and with other\ncompetitors in terms of their power using a simulation study and a real data\napplication. These simulations have shown that the statistical powers of tests\nbased on re-sampling or numerical approximation of the original statistics are\nin general slightly better than those based on a martingale transformation of\nthe original process.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:48:13 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ghoudi", "Kilani", ""], ["La\u00efb", "Na\u00e2mane", ""], ["Chaouch", "Mohamed", ""]]}, {"id": "2107.00728", "submitter": "Xiaoping Shi", "authors": "Xiaoping Shi", "title": "Two edge-count tests and relevance analysis in k high-dimensional\n  samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the task of relevance analysis, the conventional Tukey's test may be\napplied to the set of all pairwise comparisons. However, there were few studies\nthat discuss both nonparametric k-sample comparisons and relevance analysis in\nhigh dimensions. Our aim is to capture the degree of relevance between combined\nsamples and provide additional insights and advantages in high-dimensional\nk-sample comparisons. Our solution is to extend a graph-based two-sample\ncomparison and investigate its availability for large and unequal sample sizes.\nWe propose two distribution-free test statistics based on between-sample edge\ncounts and measure the degree of relevance by standardized counts. The\nasymptotic permutation null distributions of the proposed statistics are\nderived, and the power gain is proved when the sample sizes are smaller than\nthe square root of the dimension. We also discuss different edge costs in the\ngraph to compare the parameters of the distributions. Simulation comparisons\nand real data analysis of tumors and images further convince the value of our\nproposed method. Software implementing the relevance analysis is available in\nthe R package Relevance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 20:09:56 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Shi", "Xiaoping", ""]]}, {"id": "2107.00744", "submitter": "Phillip Shreeves", "authors": "Phillip Shreeves, Jeffrey L. Andrews, Xinchen Deng, Ramie Ali-Adeeb,\n  Andrew Jirasek", "title": "Nonnegative Matrix Factorization with Group and Basis Restrictions", "comments": "11 pages (introduction to conclusion; 17 total). 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a popular method used to reduce\ndimensionality in data sets whose elements are nonnegative. It does so by\ndecomposing the data set of interest, $\\mathbf{X}$, into two lower rank\nnonnegative matrices multiplied together ($\\mathbf{X} \\approx \\mathbf{WH}$).\nThese two matrices can be described as the latent factors, represented in the\nrows of $\\mathbf{H}$, and the scores of the observations on these factors that\nare found in the rows of $\\mathbf{W}$. This paper provides an extension of this\nmethod which allows one to specify prior knowledge of the data, including both\ngroup information and possible underlying factors. This is done by further\ndecomposing the matrix, $\\mathbf{H}$, into matrices $\\mathbf{A}$ and\n$\\mathbf{S}$ multiplied together. These matrices represent an 'auxiliary'\nmatrix and a semi-constrained factor matrix respectively. This method and its\nupdating criterion are proposed, followed by its application on both simulated\nand real world examples.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 21:07:34 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Shreeves", "Phillip", ""], ["Andrews", "Jeffrey L.", ""], ["Deng", "Xinchen", ""], ["Ali-Adeeb", "Ramie", ""], ["Jirasek", "Andrew", ""]]}, {"id": "2107.00776", "submitter": "Gang Li", "authors": "Ning Li, Yi Liu, Shanpeng Li, Robert M. Elashoff, Gang Li", "title": "A Flexible Joint Model for Multiple Longitudinal Biomarkers and A\n  Time-to-Event Outcome: With Applications to Dynamic Prediction Using Highly\n  Correlated Biomarkers", "comments": "Accepted for publication in biometrical journal: 13 pages, 3 figures.\n  arXiv admin note: substantial text overlap with arXiv:math/0602240 by other\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In biomedical studies it is common to collect data on multiple biomarkers\nduring study follow-up for dynamic prediction of a time-to-event clinical\noutcome. The biomarkers are typically intermittently measured, missing at some\nevent times, and may be subject to high biological variations, which cannot be\nreadily used as time-dependent covariates in a standard time-to-event model.\nMoreover, they can be highly correlated if they are from in the same biological\npathway. To address these issues, we propose a flexible joint model framework\nthat models the multiple biomarkers with a shared latent reduced rank\nlongitudinal principal component model and correlates the latent process to the\nevent time by the Cox model for dynamic prediction of the event time. The\nproposed joint model for highly correlated biomarkers is more flexible than\nsome existing methods since the latent trajectory shared by the multiple\nbiomarkers does not require specification of a priori parametric time trend and\nis determined by data. We derive an Expectation-Maximization (EM) algorithm for\nparameter estimation, study large sample properties of the estimators, and\nadapt the developed method to make dynamic prediction of the time-to-event\noutcome. Bootstrap is used for standard error estimation and inference. The\nproposed method is evaluated using simulations and illustrated on a lung\ntransplant data to predict chronic lung allograft dysfunction (CLAD) using\nchemokines measured in bronchoalveolar lavage fluid of the patients.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 20:31:49 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Li", "Ning", ""], ["Liu", "Yi", ""], ["Li", "Shanpeng", ""], ["Elashoff", "Robert M.", ""], ["Li", "Gang", ""]]}, {"id": "2107.00815", "submitter": "Bo Zhang", "authors": "Kan Chen, Siyu Heng, Qi Long, Bo Zhang", "title": "Testing Randomization and Relaxed Randomization Assumptions: A\n  Clustering With Side-information Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One central goal of design of observational studies is to embed\nnon-experimental data into an approximate randomized controlled trial using\nstatistical matching. Researchers then make the randomization assumption in\ntheir downstream, outcome analysis. For matched pair design, the randomization\nassumption states that the treatment assignment across all matched pairs are\nindependent, and that the probability of the first subject in each pair\nreceiving treatment and the other control is the same as the first receiving\ncontrol and the other treatment. In this article, we develop a novel framework\nfor testing the randomization assumption based on solving a clustering problem\nwith side-information using modern statistical learning tools. Our testing\nframework is nonparametric, finite-sample exact, and distinct from previous\nproposals in that it can be used to test a relaxed version of the randomization\nassumption called the biased randomization assumption. One important by-product\nof our testing framework is a quantity called residual sensitivity value (RSV),\nwhich quantifies the level of minimal residual confounding due to observed\ncovariates not being well matched. We advocate taking into account RSV in the\ndownstream primary analysis. The proposed methodology is illustrated by\nre-examining a famous observational study concerning the effect of right heart\ncatheterization (RHC) in the initial care of critically ill patients.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 03:42:44 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 23:32:07 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chen", "Kan", ""], ["Heng", "Siyu", ""], ["Long", "Qi", ""], ["Zhang", "Bo", ""]]}, {"id": "2107.00856", "submitter": "Ye Wang", "authors": "Licheng Liu, Ye Wang, and Yiqing Xu", "title": "A Practical Guide to Counterfactual Estimators for Causal Inference with\n  Time-Series Cross-Sectional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a unified framework of counterfactual estimation for\ntime-series cross-sectional data, which estimates the average treatment effect\non the treated by directly imputing treated counterfactuals. Examples include\nthe fixed effects counterfactual estimator, interactive fixed effects\ncounterfactual estimator, and matrix completion estimator. These estimators\nprovide more reliable causal estimates than conventional twoway fixed effects\nmodels when treatment effects are heterogeneous or unobserved time-varying\nconfounders exist. Under this framework, we propose a new dynamic treatment\neffects plot, as well as several diagnostic tests, to help researchers gauge\nthe validity of the identifying assumptions. We illustrate these methods with\ntwo political economy examples and develop an open-source package, fect, in\nboth R and Stata to facilitate implementation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 06:08:42 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Liu", "Licheng", ""], ["Wang", "Ye", ""], ["Xu", "Yiqing", ""]]}, {"id": "2107.00928", "submitter": "Shosei Sakaguchi", "authors": "Shosei Sakaguchi", "title": "Partial Identification and Inference in Duration Models with Endogenous\n  Censoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies identification and inference in transformation models with\nendogenous censoring. Many kinds of duration models, such as the accelerated\nfailure time model, proportional hazard model, and mixed proportional hazard\nmodel, can be viewed as transformation models. We allow the censoring of a\nduration outcome to be arbitrarily correlated with observed covariates and\nunobserved heterogeneity. We impose no parametric restrictions on either the\ntransformation function or the distribution function of the unobserved\nheterogeneity. In this setting, we develop bounds on the regression parameters\nand the transformation function, which are characterized by conditional moment\ninequalities involving U-statistics. We provide inference methods for them by\nconstructing an inference approach for conditional moment inequality models in\nwhich the sample analogs of moments are U-statistics. We apply the proposed\ninference methods to evaluate the effect of heart transplants on patients'\nsurvival time using data from the Stanford Heart Transplant Study.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 09:37:40 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Sakaguchi", "Shosei", ""]]}, {"id": "2107.00960", "submitter": "Alexander McNeil", "authors": "Martin Bladt and Alexander J. McNeil", "title": "Time series models with infinite-order partial copula dependence", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stationary and ergodic time series can be constructed using an s-vine\ndecomposition based on sets of bivariate copula functions. The extension of\nsuch processes to infinite copula sequences is considered and shown to yield a\nrich class of models that generalizes Gaussian ARMA and ARFIMA processes to\nallow both non-Gaussian marginal behaviour and a non-Gaussian description of\nthe serial partial dependence structure. Extensions of classical causal and\ninvertible representations of linear processes to general s-vine processes are\nproposed and investigated. A practical and parsimonious method for\nparameterizing s-vine processes using the Kendall partial autocorrelation\nfunction is developed. The potential of the resulting models to give improved\nstatistical fits in many applications is indicated with an example using\nmacroeconomic data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 10:46:23 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Bladt", "Martin", ""], ["McNeil", "Alexander J.", ""]]}, {"id": "2107.00975", "submitter": "Giovanni Saraceno", "authors": "Giovanni Saraceno, Fatemah Alqallaf, Claudio Agostinelli", "title": "A Robust Seemingly Unrelated Regressions For Row-Wise And Cell-Wise\n  Contamination", "comments": "18 pages, 6 figures and 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Seemingly Unrelated Regressions (SUR) model is a wide used estimation\nprocedure in econometrics, insurance and finance, where very often, the\nregression model contains more than one equation. Unknown parameters,\nregression coefficients and covariances among the errors terms, are estimated\nusing algorithms based on Generalized Least Squares or Maximum Likelihood, and\nthe method, as a whole, is very sensitive to outliers. To overcome this problem\nM-estimators and S-estimators are proposed in the literature together with fast\nalgorithms. However, these procedures are only able to cope with row-wise\noutliers in the error terms, while their performance becomes very poor in the\npresence of cell-wise outliers and as the number of equations increases. A new\nrobust approach is proposed which is able to perform well under both\ncontamination types as well as it is fast to compute. Illustrations based on\nMonte Carlo simulations and a real data example are provided.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 11:13:16 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Saraceno", "Giovanni", ""], ["Alqallaf", "Fatemah", ""], ["Agostinelli", "Claudio", ""]]}, {"id": "2107.01070", "submitter": "Fernando Hartwig", "authors": "Fernando Pires Hartwig, Linbo Wang, George Davey Smith, Neil Martin\n  Davies", "title": "Homogeneity in the instrument-treatment association is not sufficient\n  for the Wald estimand to equal the average causal effect when the exposure is\n  continuous", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background: Interpreting results of instrumental variable (IV) analysis as\nwell-defined causal estimands requires further assumptions in addition to the\ncore IV assumptions of relevance, independence, and the exclusion restriction.\nOne such assumption is additive homogeneity in the instrument-exposure\nassociation, which is postulated to render the conventional Wald estimand as\nequal to the average causal effect (ACE). Methods: We used theoretical\narguments and an illustrative example to assess whether instrument-exposure\nadditive homogeneity identifies the ACE when the exposure is continuous and the\ninstrument is either binary or continuous. Results: Instrument-exposure\nadditive homogeneity is insufficient to identify the ACE when the instrument is\nbinary, the exposure is continuous and the effect of the exposure on the\noutcome is non-linear on the additive scale. If the exposure is binary, the\nexposure-outcome effect is necessarily additive linear, so the homogeneity\ncondition is sufficient. For a continuous instrument, the instrument-exposure\nadditive homogeneity is sufficient regardless of the exposure-outcome effect\nbeing linear or not. Conclusions: For binary instruments, additive homogeneity\nin the instrument-exposure association identifies the ACE if the exposure is\nalso binary. Otherwise, additional assumptions (such as additive linearity of\nthe exposure-outcome effect) are required.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 13:23:11 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Hartwig", "Fernando Pires", ""], ["Wang", "Linbo", ""], ["Smith", "George Davey", ""], ["Davies", "Neil Martin", ""]]}, {"id": "2107.01103", "submitter": "Subhabrata Majumdar", "authors": "Subhabrata Majumdar, Snigdhansu Chatterjee", "title": "Generalized Multivariate Signs for Nonparametric Hypothesis Testing in\n  High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data, where the dimension of the feature space is much\nlarger than sample size, arise in a number of statistical applications. In this\ncontext, we construct the generalized multivariate sign transformation, defined\nas a vector divided by its norm. For different choices of the norm function,\nthe resulting transformed vector adapts to certain geometrical features of the\ndata distribution. Building up on this idea, we obtain one-sample and\ntwo-sample testing procedures for mean vectors of high-dimensional data using\nthese generalized sign vectors. These tests are based on U-statistics using\nkernel inner products, do not require prohibitive assumptions, and are amenable\nto a fast randomization-based implementation. Through experiments in a number\nof data settings, we show that tests using generalized signs display higher\npower than existing tests, while maintaining nominal type-I error rates.\nFinally, we provide example applications on the MNIST and Minnesota Twin\nStudies genomic data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 14:31:44 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Majumdar", "Subhabrata", ""], ["Chatterjee", "Snigdhansu", ""]]}, {"id": "2107.01144", "submitter": "Antonio Elias", "authors": "A. El\\'ias, J. M. Morales and S. Pineda", "title": "Depth-based Outlier Detection for Grouped Smart Meters: a Functional\n  Data Analysis Toolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart metering infrastructures collect data almost continuously in the form\nof fine-grained long time series. These massive time series often have common\ndaily patterns that are repeated between similar days or seasons and shared\nbetween grouped meters. Within this context, we propose a method to highlight\nindividuals with abnormal daily dependency patterns, which we term evolution\noutliers. To this end, we approach the problem from the standpoint of\nFunctional Data Analysis (FDA), by treating each daily record as a function or\ncurve. We then focus on the morphological aspects of the observed curves, such\nas daily magnitude, daily shape, derivatives, and inter-day evolution. The\nproposed method for evolution outliers relies on the concept of functional\ndepth, which has been a cornerstone in the literature of FDA to build shape and\nmagnitude outlier detection methods. In conjunction with our evolution outlier\nproposal, these methods provide an outlier detection toolbox for smart meter\ndata that covers a wide palette of functional outliers classes. We illustrate\nthe outlier identification ability of this toolbox using actual smart metering\ndata corresponding to photovoltaic energy generation and circuit voltage\nrecords.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:42:29 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["El\u00edas", "A.", ""], ["Morales", "J. M.", ""], ["Pineda", "S.", ""]]}, {"id": "2107.01246", "submitter": "Gabriel Hassler", "authors": "Gabriel W. Hassler, Brigida Gallone, Leandro Aristide, William L.\n  Allen, Max R. Tolkoff, Andrew J. Holbrook, Guy Baele, Philippe Lemey and Marc\n  A. Suchard", "title": "Principled, practical, flexible, fast: a new approach to phylogenetic\n  factor analysis", "comments": "27 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological phenotypes are products of complex evolutionary processes in which\nselective forces influence multiple biological trait measurements in unknown\nways. Phylogenetic factor analysis disentangles these relationships across the\nevolutionary history of a group of organisms. Scientists seeking to employ this\nmodeling framework confront numerous modeling and implementation decisions, the\ndetails of which pose computational and replicability challenges. General and\nimpactful community employment requires a data scientific analysis plan that\nbalances flexibility, speed and ease of use, while minimizing model and\nalgorithm tuning. Even in the presence of non-trivial phylogenetic model\nconstraints, we show that one may analytically address latent factor\nuncertainty in a way that (a) aids model flexibility, (b) accelerates\ncomputation (by as much as 500-fold) and (c) decreases required tuning. We\nfurther present practical guidance on inference and modeling decisions as well\nas diagnosing and solving common problems in these analyses. We codify this\nanalysis plan in an automated pipeline that distills the potentially\noverwhelming array of modeling decisions into a small handful of (typically\nbinary) choices. We demonstrate the utility of these methods and analysis plan\nin four real-world problems of varying scales.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 19:40:45 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Hassler", "Gabriel W.", ""], ["Gallone", "Brigida", ""], ["Aristide", "Leandro", ""], ["Allen", "William L.", ""], ["Tolkoff", "Max R.", ""], ["Holbrook", "Andrew J.", ""], ["Baele", "Guy", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "2107.01266", "submitter": "Kan Chen", "authors": "Kan Chen, Zhiqi Bu, Shiyun Xu", "title": "Asymptotic Statistical Analysis of Sparse Group LASSO via Approximate\n  Message Passing Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sparse Group LASSO (SGL) is a regularized model for high-dimensional linear\nregression problems with grouped covariates. SGL applies $l_1$ and $l_2$\npenalties on the individual predictors and group predictors, respectively, to\nguarantee sparse effects both on the inter-group and within-group levels. In\nthis paper, we apply the approximate message passing (AMP) algorithm to\nefficiently solve the SGL problem under Gaussian random designs. We further use\nthe recently developed state evolution analysis of AMP to derive an\nasymptotically exact characterization of SGL solution. This allows us to\nconduct multiple fine-grained statistical analyses of SGL, through which we\ninvestigate the effects of the group information and $\\gamma$ (proportion of\n$\\ell_1$ penalty). With the lens of various performance measures, we show that\nSGL with small $\\gamma$ benefits significantly from the group information and\ncan outperform other SGL (including LASSO) or regularized models which does not\nexploit the group information, in terms of the recovery rate of signal, false\ndiscovery rate and mean squared error.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 20:48:54 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chen", "Kan", ""], ["Bu", "Zhiqi", ""], ["Xu", "Shiyun", ""]]}, {"id": "2107.01271", "submitter": "Nicolas Meyer", "authors": "Nicolas Meyer and Erik-Andr\\'e Sauleau", "title": "Bayesian two-interval test", "comments": "49 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The null hypothesis test (NHT) is widely used for validating scientific\nhypotheses but is actually highly criticized. Although Bayesian tests overcome\nseveral criticisms, some limits remain. We propose a Bayesian two-interval test\n(2IT) in which two hypotheses on an effect being present or absent are\nexpressed as prespecified joint or disjoint intervals and their posterior\nprobabilities are computed. The same formalism can be applied for superiority,\nnon-inferiority, or equivalence tests. The 2IT was studied for three real\nexamples and three sets of simulations (comparison of a proportion and a mean\nto a reference and comparison of two proportions). Several scenarios were\ncreated (with different sample sizes), and simulations were conducted to\ncompute the probabilities of the parameter of interest being in the interval\ncorresponding to either hypothesis given the data generated under one of the\nhypotheses. Posterior estimates were obtained using conjugacy with a\nlow-informative prior. Bias was also estimated. The probability of accepting a\nhypothesis when that hypothesis is true progressively increases the sample\nsize, tending towards 1, while the probability of accepting the other\nhypothesis is always very low (less than 5%) and tends towards 0. The speed of\nconvergence varies with the gap between the hypotheses and with their width. In\nthe case of a mean, the bias is low and rapidly becomes negligible. We propose\na Bayesian test that follows a scientifically sound process, in which two\ninterval hypotheses are explicitly used and tested. The proposed test has\nalmost none of the limitations of the NHT and suggests new features, such as a\nrationale for serendipity or a justification for a \"trend in data\". The\nconceptual framework of the 2-IT also allows the calculation of a sample size\nand the use of sequential methods in numerous contexts.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 20:57:22 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Meyer", "Nicolas", ""], ["Sauleau", "Erik-Andr\u00e9", ""]]}, {"id": "2107.01306", "submitter": "Yunyi Shen", "authors": "Yunyi Shen and Claudia Solis-Lemus", "title": "The Effect of the Prior and the Experimental Design on the Inference of\n  the Precision Matrix in Gaussian Chain Graph Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Here, we investigate whether (and how) experimental design could aid in the\nestimation of the precision matrix in a Gaussian chain graph model, especially\nthe interplay between the design, the effect of the experiment and prior\nknowledge about the effect. We approximate the marginal posterior precision of\nthe precision matrix via Laplace approximation under different priors: a flat\nprior, the conjugate prior Normal-Wishart, the unconfounded prior Normal-Matrix\nGeneralized Inverse Gaussian (MGIG) and a general independent prior. We show\nthat the approximated posterior precision is not a function of the design\nmatrix for the cases of the Normal-Wishart and flat prior, but it is for the\ncases of the Normal-MGIG and the general independent prior. However, for the\nNormal-MGIG and the general independent prior, we find a sharp upper bound on\nthe approximated posterior precision that does not involve the design matrix\nwhich translates into a bound on the information that could be extracted from a\ngiven experiment. We confirm the theoretical findings via a simulation study\ncomparing the Stein's loss difference between random versus no experiment\n(design matrix equal to zero). Our findings provide practical advice for domain\nscientists conducting experiments to decode the relationships between a\nmultidimensional response and a set of predictors.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 23:57:43 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Shen", "Yunyi", ""], ["Solis-Lemus", "Claudia", ""]]}, {"id": "2107.01333", "submitter": "Shuyan Wang", "authors": "Shuyan Wang, Peter Spirtes", "title": "A Uniformly Consistent Estimator of non-Gaussian Causal Effects Under\n  the k-Triangle-Faithfulness Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kalisch and B\\\"{u}hlmann (2007) showed that for linear Gaussian models, under\nthe Causal Markov Assumption, the Strong Causal Faithfulness Assumption, and\nthe assumption of causal sufficiency, the PC algorithm is a uniformly\nconsistent estimator of the Markov Equivalence Class of the true causal DAG for\nlinear Gaussian models; it follows from this that for the identifiable causal\neffects in the Markov Equivalence Class, there are uniformly consistent\nestimators of causal effects as well. The $k$-Triangle-Faithfulness Assumption\nis a strictly weaker assumption that avoids some implausible implications of\nthe Strong Causal Faithfulness Assumption and also allows for uniformly\nconsistent estimates of Markov Equivalence Classes (in a weakened sense), and\nof identifiable causal effects. However, both of these assumptions are\nrestricted to linear Gaussian models. We propose the Generalized $k$-Triangle\nFaithfulness, which can be applied to any smooth distribution. In addition,\nunder the Generalized $k$-Triangle Faithfulness Assumption, we describe the\nEdge Estimation Algorithm that provides uniformly consistent estimates of\ncausal effects in some cases (and otherwise outputs \"can't tell\"), and the\n\\textit{Very Conservative }$SGS$ Algorithm that (in a slightly weaker sense) is\na uniformly consistent estimator of the Markov equivalence class of the true\nDAG.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 03:26:48 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wang", "Shuyan", ""], ["Spirtes", "Peter", ""]]}, {"id": "2107.01338", "submitter": "Shiv Shankar", "authors": "Shiv Shankar, Daniel Sheldon", "title": "Sibling Regression for Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Field observations form the basis of many scientific studies, especially in\necological and social sciences. Despite efforts to conduct such surveys in a\nstandardized way, observations can be prone to systematic measurement errors.\nThe removal of systematic variability introduced by the observation process, if\npossible, can greatly increase the value of this data. Existing non-parametric\ntechniques for correcting such errors assume linear additive noise models. This\nleads to biased estimates when applied to generalized linear models (GLM). We\npresent an approach based on residual functions to address this limitation. We\nthen demonstrate its effectiveness on synthetic data and show it reduces\nsystematic detection variability in moth surveys.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 04:07:11 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 15:37:01 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Shankar", "Shiv", ""], ["Sheldon", "Daniel", ""]]}, {"id": "2107.01388", "submitter": "Sreedevi E P", "authors": "Sreedevi E. P. and Sankaran P. G.", "title": "Proportional mean model for panel count data with multiple modes of\n  recurrence", "comments": "25 pages, 2 fgires", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Panel count data is common when the study subjects are exposed to recurrent\nevents, observed only at discrete time points. In this article, we consider the\nregression analysis of panel count data with multiple modes of recurrence. We\npropose a proportional mean model to estimate the effect of covariates on the\nunderlying counting process due to different modes of recurrence. The\nsimultaneous estimation of baseline cumulative mean functions and regression\nparameters of $(k>1)$ recurrence modes are studied in detail. Asymptotic\nproperties of the proposed estimators are also established. A Monte Carlo\nsimulation study is carried out to validate the finite sample behaviour of the\nproposed estimators. The methods are applied to a real data arising from skin\ncancer chemoprevention trial.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 09:18:06 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["P.", "Sreedevi E.", ""], ["G.", "Sankaran P.", ""]]}, {"id": "2107.01480", "submitter": "Sudipta Bhattacharya", "authors": "Sudipta Bhattacharya, Jyotirmoy Dey", "title": "Assessing contribution of treatment phases through tipping point\n  analyses via counterfactual elicitation using rank preserving structural\n  failure time models", "comments": "38 pages, 6 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:2011.09070", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article provides a novel approach to assess the importance of specific\ntreatment phases within a treatment regimen through tipping point analyses\n(TPA) of a time-to-event endpoint using rank-preserving-structural-failure-time\n(RPSFT) modelling. In oncology clinical research, an experimental treatment is\noften added to the standard of care therapy in multiple treatment phases to\nimprove patient outcomes. When the resulting new regimen provides a meaningful\nbenefit over standard of care, gaining insights into the contribution of each\ntreatment phase becomes important to properly guide clinical practice. New\nstatistical approaches are needed since traditional methods are inadequate in\nanswering such questions. RPSFT modelling is an approach for causal inference,\ntypically used to adjust for treatment switching in randomized clinical trials\nwith time-to-event endpoints. A tipping-point analysis is commonly used in\nsituations where a statistically significant treatment effect is suspected to\nbe an artifact of missing or unobserved data rather than a real treatment\ndifference. The methodology proposed in this article is an amalgamation of\nthese two ideas to investigate the contribution of a specific component of a\nregimen comprising multiple treatment phases. We provide different variants of\nthe method and construct indices of contribution of a treatment phase to the\noverall benefit of a regimen that facilitates interpretation of results. The\nproposed approaches are illustrated with findings from a recently concluded,\nreal-life phase 3 cancer clinical trial. We conclude with several\nconsiderations and recommendations for practical implementation of this new\nmethodology.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 19:07:26 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Bhattacharya", "Sudipta", ""], ["Dey", "Jyotirmoy", ""]]}, {"id": "2107.01497", "submitter": "Hailin Huang", "authors": "Hailin Huang", "title": "Novel Semi-parametric Tobit Additive Regression Models", "comments": "ICDATA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression method has been widely used to explore relationship between\ndependent and independent variables. In practice, data issues such as censoring\nand missing data often exist. When the response variable is (fixed) censored,\nTobit regression models have been widely employed to explore the relationship\nbetween the response variable and covariates. In this paper, we extend\nconventional parametric Tobit models to a novel semi-parametric regression\nmodel by replacing the linear components in Tobit models with nonparametric\nadditive components, which we refer as Tobit additive models, and propose a\nlikelihood based estimation method for Tobit additive models. %The proposed\nestimation method is computational efficient and easy to implement. Numerical\nexperiments are conducted to evaluate the finite sample performance. The\nestimation method works well in finite sample experiments, even when sample\nsize is relative small.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 21:24:29 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Huang", "Hailin", ""]]}, {"id": "2107.01513", "submitter": "Ashish Patel", "authors": "Ashish Patel, Francis J. Ditraglia, Verena Zuber, and Stephen Burgess", "title": "Selection of invalid instruments can improve estimation in Mendelian\n  randomization", "comments": "56 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian randomization (MR) is a widely-used method to identify causal links\nbetween a risk factor and disease. A fundamental part of any MR analysis is to\nchoose appropriate genetic variants as instrumental variables. Current practice\nusually involves selecting only those genetic variants that are deemed to\nsatisfy certain exclusion restrictions, in a bid to remove bias from unobserved\nconfounding. Many more genetic variants may violate these exclusion\nrestrictions due to unknown pleiotropic effects (i.e. direct effects on the\noutcome not via the exposure), but their inclusion could increase the precision\nof causal effect estimates at the cost of allowing some bias. We explore how to\noptimally tackle this bias-variance trade-off by carefully choosing from many\nweak and locally invalid instruments. Specifically, we study a focused\ninstrument selection approach for publicly available two-sample summary data on\ngenetic associations, whereby genetic variants are selected on the basis of how\nthey impact the asymptotic mean square error of causal effect estimates. We\nshow how different restrictions on the nature of pleiotropic effects have\nimportant implications for the quality of post-selection inferences. In\nparticular, a focused selection approach under systematic pleiotropy allows for\nconsistent model selection, but in practice can be susceptible to winner's\ncurse biases. Whereas a more general form of idiosyncratic pleiotropy allows\nonly conservative model selection, but offers uniformly valid confidence\nintervals. We propose a novel method to tighten honest confidence intervals\nthrough support restrictions on pleiotropy. We apply our results to several\nreal data examples which suggest that the optimal selection of instruments does\nnot only involve biologically-justified valid instruments, but additionally\nhundreds of potentially pleiotropic variants.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 00:27:49 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Patel", "Ashish", ""], ["Ditraglia", "Francis J.", ""], ["Zuber", "Verena", ""], ["Burgess", "Stephen", ""]]}, {"id": "2107.01537", "submitter": "Helene Charlotte Rytgaard", "authors": "Helene C. W. Rytgaard and Mark J. van der Laan", "title": "One-step TMLE to target cause-specific absolute risks and survival\n  curves", "comments": "21 pages (including appendix), 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers one-step targeted maximum likelihood estimation method\nfor general competing risks and survival analysis settings where event times\ntake place on the positive real line R+ and are subject to right-censoring. Our\ninterest is overall in the effects of baseline treatment decisions, static,\ndynamic or stochastic, possibly confounded by pre-treatment covariates. We\npoint out two overall contributions of our work. First, our method can be used\nto obtain simultaneous inference across all absolute risks in competing risks\nsettings. Second, we present a practical result for achieving inference for the\nfull survival curve, or a full absolute risk curve, across time by targeting\nover a fine enough grid of points. The one-step procedure is based on a\none-dimensional universal least favorable submodel for each cause-specific\nhazard that can be implemented in recursive steps along a corresponding\nuniversal least favorable submodel. We present a theorem for conditions to\nachieve weak convergence of the estimator for an infinite-dimensional target\nparameter. Our empirical study demonstrates the use of the methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 04:28:36 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Rytgaard", "Helene C. W.", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "2107.01644", "submitter": "Georgia Papadogeorgou", "authors": "Georgia Papadogeorgou", "title": "Discussion of the manuscript: Spatial+ a novel approach to spatial\n  confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  I congratulate Dupont, Wood and Augustin (DWA hereon) for providing an\neasy-to-implement method for estimation in the presence of spatial confounding,\nand for addressing some of the complicated aspects on the topic. The method\nregresses the covariate of interest on spatial basis functions and uses the\nresiduals of this model in an outcome regression. The authors show that, if the\ncovariate is not completely spatial, this approach leads to consistent\nestimation of the conditional association between the exposure and the outcome.\nBelow I discuss conceptual and operational issues that are fundamental to\ninference in spatial settings: (i) the target quantity and its\ninterpretability, (ii) the non-spatial aspect of covariates and their relative\nspatial scales, and (iii) the impact of spatial smoothing. While DWA provide\nsome insights on these issues, I believe that the audience might benefit from a\ndeeper discussion. In what follows, I focus on the setting where a researcher\nis interested in interpreting the relationship between a given covariate and an\noutcome. I refer to the covariate of interest as the exposure to differentiate\nit from the rest.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 14:22:47 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Papadogeorgou", "Georgia", ""]]}, {"id": "2107.01688", "submitter": "Ryan Martin", "authors": "Pei-Shien Wu and Ryan Martin", "title": "Calibrating generalized predictive distributions", "comments": "33 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In prediction problems, it is common to model the data-generating process and\nthen use a model-based procedure, such as a Bayesian predictive distribution,\nto quantify uncertainty about the next observation. However, if the posited\nmodel is misspecified, then its predictions may not be calibrated -- that is,\nthe predictive distribution's quantiles may not be nominal frequentist\nprediction upper limits, even asymptotically. Rather than abandoning the\ncomfort of a model-based formulation for a more complicated non-model-based\napproach, here we propose a strategy in which the data itself helps determine\nif the assumed model-based solution should be adjusted to account for model\nmisspecification. This is achieved through a generalized Bayes formulation\nwhere a learning rate parameter is tuned, via the proposed generalized\npredictive calibration (GPrC) algorithm, to make the predictive distribution\ncalibrated, even under model misspecification. Extensive numerical experiments\nare presented, under a variety of settings, demonstrating the proposed GPrC\nalgorithm's validity, efficiency, and robustness.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 17:19:24 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wu", "Pei-Shien", ""], ["Martin", "Ryan", ""]]}, {"id": "2107.01742", "submitter": "Gordon J. Ross", "authors": "Gordon J. Ross", "title": "Nonparametric Detection of Multiple Location-Scale Change Points via\n  Wild Binary Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While parametric multiple change point detection has been widely studied,\nless attention has been given to the nonparametric task of detecting multiple\nchange points in a sequence of observations when their distribution is unknown.\nMost existing work on this topic is either based on penalized cost functions\nwhich can suffer from false positive detections, or on binary segmentation\nwhich can fail to detect certain configurations of change points. We introduce\na new approach to change point detection which adapts the recently proposed\nWild Binary Segmentation (WBS) procedure to a nonparametric setting. Our\napproach is based on the use of rank based test statistics which are especially\npowerful at detecting changes in location and/or scale. We show via simulation\nthat the resulting nonparametric WBS procedure has favorable performance\ncompared to existing methods, particularly when it comes to detecting changes\nin scale. We apply our procedure to study a problem in stylometry involving\nchange points in an author's writing style, and provide a full implementation\nof our algorithm in an associated R package.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 22:17:14 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Ross", "Gordon J.", ""]]}, {"id": "2107.01773", "submitter": "Jin Liu", "authors": "Jin Liu", "title": "Extending Latent Basis Growth Model to Explore Joint Development in the\n  Framework of Individual Measurement Occasions", "comments": "Draft version 1.2, 07/08/2021. This paper has not been peer reviewed.\n  Please do not copy or cite without author's permission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal processes in multiple domains are often theorized to be\nnonlinear, which poses unique statistical challenges. Empirical researchers\noften select a nonlinear longitudinal model by weighing how specific the model\nmust be in terms of the nature of the nonlinearity, whether the model is\ncomputationally efficient, and whether the model provides interpretable\ncoefficients. Latent basis growth models (LBGMs) are one method that can get\naround these tradeoffs: it does not require specification of any functional\nform; additionally, its estimation process is expeditious, and estimates are\nstraightforward to interpret. We propose a novel specification for LBGMs that\nallows for (1) unequally-spaced study waves and (2) individual measurement\noccasions around each wave. We then extend LBGMs to explore multiple repeated\noutcomes because longitudinal processes rarely unfold in isolation. We present\nthe proposed model by simulation studies and real-world data analyses. Our\nsimulation studies demonstrate that the proposed model can provide unbiased and\naccurate estimates with target coverage probabilities of a 95% confidence\ninterval for the parameters of interest. With the real-world analyses using\nlongitudinal reading and mathematics scores, we demonstrate that the proposed\nparallel LBGM can capture the underlying developmental patterns of these two\nabilities and that the novel specification of LBGMs is helpful in joint\ndevelopment where longitudinal processes have different time structures. We\nalso provide the corresponding code for the proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 03:43:54 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 20:49:55 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Liu", "Jin", ""]]}, {"id": "2107.01813", "submitter": "N. Balakrishna", "authors": "N. Balakrishna, Muhammed Anvar and Bovas Abraham", "title": "Zero-modified Count Time Series with Markovian Intensities", "comments": "31 pages including Tables and Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for analyzing count time series with inflation\nor deflation of zeros. In particular, zero-modified Poisson and zero-modified\nnegative binomial series with intensities generated by non-negative Markov\nsequences are studied in detail. Parameters of the model are estimated by the\nmethod of estimating equations which is facilitated by expressing the model in\na generalized state space form. The latent intensities required for estimation\nare extracted using generalized Kalman filter. The applications of proposed\nmodel and its estimation methods are illustrated using simulated and real data\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 06:35:15 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Balakrishna", "N.", ""], ["Anvar", "Muhammed", ""], ["Abraham", "Bovas", ""]]}, {"id": "2107.01850", "submitter": "Jiaqi Zhang", "authors": "Jiaqi Zhang, Chandler Squires, Caroline Uhler", "title": "Matching a Desired Causal State via Shift Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transforming a causal system from a given initial state to a desired target\nstate is an important task permeating multiple fields including control theory,\nbiology, and materials science. In causal models, such transformations can be\nachieved by performing a set of interventions. In this paper, we consider the\nproblem of identifying a shift intervention that matches the desired mean of a\nsystem through active learning. We define the Markov equivalence class that is\nidentifiable from shift interventions and propose two active learning\nstrategies that are guaranteed to exactly match a desired mean. We then derive\na worst-case lower bound for the number of interventions required and show that\nthese strategies are optimal for certain classes of graphs. In particular, we\nshow that our strategies may require exponentially fewer interventions than the\npreviously considered approaches, which optimize for structure learning in the\nunderlying causal graph. In line with our theoretical results, we also\ndemonstrate experimentally that our proposed active learning strategies require\nfewer interventions compared to several baselines.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 08:11:36 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhang", "Jiaqi", ""], ["Squires", "Chandler", ""], ["Uhler", "Caroline", ""]]}, {"id": "2107.01865", "submitter": "Motonori Oka", "authors": "Motonori Oka, Shun Saso, Kensuke Okada", "title": "Variational Bayesian Inference for the Polytomous-Attribute Saturated\n  Diagnostic Classification Model with Parallel Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a statistical tool to assist formative assessments in educational\nsettings, diagnostic classification models (DCMs) have been increasingly used\nto provide diagnostic information regarding examinees' attributes. DCMs often\nadopt dichotomous division such as mastery and non-mastery of attributes to\nexpress mastery states of attributes. However, many practical settings involve\ndifferent levels of mastery states rather than a simple dichotomy in a single\nattribute. Although this practical demand can be addressed by\npolytomous-attribute DCMs, their computational cost in a Markov chain Monte\nCarlo estimation impedes their large-scale applications due to the larger\nnumber of polytomous-attribute mastery patterns than that of binary-attribute\nones. This study considers a scalable Bayesian estimation method for\npolytomous-attribute DCMs and developed a variational Bayesian (VB) algorithm\nfor a polytomous-attribute saturated DCM -- a generalization of\npolytomous-attribute DCMs -- by building on the existing literature in VB for\nbinary-attribute DCMs and polytomous-attribute DCMs. Furthermore, we proposed\nthe configuration of parallel computing for the proposed VB algorithm to\nachieve better computational efficiency. Monte Carlo simulations revealed that\nour method exhibited the high performance in parameter recovery under a wide\nrange of conditions. An empirical example is used to demonstrate the utility of\nour method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 08:37:36 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Oka", "Motonori", ""], ["Saso", "Shun", ""], ["Okada", "Kensuke", ""]]}, {"id": "2107.01916", "submitter": "Christoph Muehlmann", "authors": "Christoph Muehlmann, Fran\\c{c}ois Bachoc, Klaus Nordhausen", "title": "Blind source separation for non-stationary random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regional data analysis is concerned with the analysis and modeling of\nmeasurements that are spatially separated by specifically accounting for\ntypical features of such data. Namely, measurements in close proximity tend to\nbe more similar than the ones further separated. This might hold also true for\ncross-dependencies when multivariate spatial data is considered. Often,\nscientists are interested in linear transformations of such data which are easy\nto interpret and might be used as dimension reduction. Recently, for that\npurpose spatial blind source separation (SBSS) was introduced which assumes\nthat the observed data are formed by a linear mixture of uncorrelated, weakly\nstationary random fields. However, in practical applications, it is well-known\nthat when the spatial domain increases in size the weak stationarity\nassumptions can be violated in the sense that the second order dependency is\nvarying over the domain which leads to non-stationary analysis. In our work we\nextend the SBSS model to adjust for these stationarity violations, present\nthree novel estimators and establish the identifiability and affine\nequivariance property of the unmixing matrix functionals defining these\nestimators. In an extensive simulation study, we investigate the performance of\nour estimators and also show their use in the analysis of a geochemical dataset\nwhich is derived from the GEMAS geochemical mapping project.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 10:19:01 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 08:30:51 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Muehlmann", "Christoph", ""], ["Bachoc", "Fran\u00e7ois", ""], ["Nordhausen", "Klaus", ""]]}, {"id": "2107.01942", "submitter": "Callum Murphy-Barltrop", "authors": "C. J. R. Murphy-Barltrop and J. L. Wadsworth and E. F. Eastoe", "title": "On the Estimation of Bivariate Return Curves for Extreme Values", "comments": "41 pages (without supplementary), 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the multivariate setting, defining extremal risk measures is important in\nmany contexts, such as finance, environmental planning and structural\nengineering. In this paper, we review the literature on extremal bivariate\nreturn curves, a risk measure that is the natural bivariate extension to a\nreturn level, and propose new estimation methods based on multivariate extreme\nvalue models that can account for both asymptotic dependence and asymptotic\nindependence. We identify gaps in the existing literature and propose novel\ntools for testing and validating return curves and comparing estimates from a\nrange of multivariate models. These tools are then used to compare a selection\nof models through simulation and case studies. We conclude with a discussion\nand list some of the challenges.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 11:17:45 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Murphy-Barltrop", "C. J. R.", ""], ["Wadsworth", "J. L.", ""], ["Eastoe", "E. F.", ""]]}, {"id": "2107.02013", "submitter": "Jie Ding", "authors": "Ganghua Wang and Jie Ding", "title": "Subset Privacy: Draw from an Obfuscated Urn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapidly increasing ability to collect and analyze personal data,\ndata privacy becomes an emerging concern. In this work, we develop a new\nstatistical notion of local privacy to protect each categorical data that will\nbe collected by untrusted entities. The proposed solution, named subset\nprivacy, privatizes the original data value by replacing it with a random\nsubset containing that value. We develop methods for the estimation of\ndistribution functions and independence testing from subset-private data with\ntheoretical guarantees. We also study different mechanisms to realize the\nsubset privacy and evaluation metrics to quantify the amount of privacy in\npractice. Experimental results on both simulated and real-world datasets\ndemonstrate the encouraging performance of the developed concepts and methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 16:01:27 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wang", "Ganghua", ""], ["Ding", "Jie", ""]]}, {"id": "2107.02085", "submitter": "Anand Dixit", "authors": "Anand Dixit and Vivekananda Roy", "title": "Analyzing Relevance Vector Machines using a single penalty approach", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relevance vector machine (RVM) is a popular sparse Bayesian learning model\ntypically used for prediction. Recently it has been shown that improper priors\nassumed on multiple penalty parameters in RVM may lead to an improper\nposterior. Currently in the literature, the sufficient conditions for posterior\npropriety of RVM do not allow improper priors over the multiple penalty\nparameters. In this article, we propose a single penalty relevance vector\nmachine (SPRVM) model in which multiple penalty parameters are replaced by a\nsingle penalty and we consider a semi Bayesian approach for fitting the SPRVM.\nThe necessary and sufficient conditions for posterior propriety of SPRVM are\nmore liberal than those of RVM and allow for several improper priors over the\npenalty parameter. Additionally, we also prove the geometric ergodicity of the\nGibbs sampler used to analyze the SPRVM model and hence can estimate the\nasymptotic standard errors associated with the Monte Carlo estimate of the\nmeans of the posterior predictive distribution. Such a Monte Carlo standard\nerror cannot be computed in the case of RVM, since the rate of convergence of\nthe Gibbs sampler used to analyze RVM is not known. The predictive performance\nof RVM and SPRVM is compared by analyzing three real life datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 15:26:09 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Dixit", "Anand", ""], ["Roy", "Vivekananda", ""]]}, {"id": "2107.02091", "submitter": "Kaushik Jana Dr.", "authors": "Soudeep Deb and Kaushik Jana", "title": "Nonparametric quantile regression for time series with replicated\n  observations and its application to climate data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a model-free nonparametric estimator of conditional\nquantile of a time series regression model where the covariate vector is\nrepeated many times for different values of the response. This type of data is\nabound in climate studies. To tackle such problems, our proposed method\nexploits the replicated nature of the data and improves on restrictive linear\nmodel structure of conventional quantile regression. Relevant asymptotic theory\nfor the nonparametric estimators of the mean and variance function of the model\nare derived under a very general framework. We provide a detailed simulation\nstudy which clearly demonstrates the gain in efficiency of the proposed method\nover other benchmark models, especially when the true data generating process\nentails nonlinear mean function and heteroskedastic pattern with time dependent\ncovariates. The predictive accuracy of the non-parametric method is remarkably\nhigh compared to other methods when attention is on the higher quantiles of the\nvariable of interest. Usefulness of the proposed method is then illustrated\nwith two climatological applications, one with a well-known tropical cyclone\nwind-speed data and the other with an air pollution data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 15:31:03 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 06:31:46 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Deb", "Soudeep", ""], ["Jana", "Kaushik", ""]]}, {"id": "2107.02146", "submitter": "Ali Mahzarnia", "authors": "Ali Mahzarnia and Jun Song", "title": "Multivariate functional group sparse regression: functional predictor\n  selection", "comments": "The R package that is developed for this paper is available at\n  GitHub. See https://github.com/Ali-Mahzarnia/MFSGrp", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.NC stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose methods for functional predictor selection and the\nestimation of smooth functional coefficients simultaneously in a\nscalar-on-function regression problem under high-dimensional multivariate\nfunctional data setting. In particular, we develop two methods for functional\ngroup-sparse regression under a generic Hilbert space of infinite dimension. We\nshow the convergence of algorithms and the consistency of the estimation and\nthe selection (oracle property) under infinite-dimensional Hilbert spaces.\nSimulation studies show the effectiveness of the methods in both the selection\nand the estimation of functional coefficients. The applications to the\nfunctional magnetic resonance imaging (fMRI) reveal the regions of the human\nbrain related to ADHD and IQ.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 17:11:28 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 15:03:06 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Mahzarnia", "Ali", ""], ["Song", "Jun", ""]]}, {"id": "2107.02150", "submitter": "Daniel McDonald", "authors": "Lei Ding, Gabriel E. Zentner, Daniel J. McDonald", "title": "Sufficient principal component regression for pattern discovery in\n  transcriptomic data", "comments": "26 pages, 9 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Methods for global measurement of transcript abundance such as microarrays\nand RNA-seq generate datasets in which the number of measured features far\nexceeds the number of observations. Extracting biologically meaningful and\nexperimentally tractable insights from such data therefore requires\nhigh-dimensional prediction. Existing sparse linear approaches to this\nchallenge have been stunningly successful, but some important issues remain.\nThese methods can fail to select the correct features, predict poorly relative\nto non-sparse alternatives, or ignore any unknown grouping structures for the\nfeatures. We propose a method called SuffPCR that yields improved predictions\nin high-dimensional tasks including regression and classification, especially\nin the typical context of omics with correlated features. SuffPCR first\nestimates sparse principal components and then estimates a linear model on the\nrecovered subspace. Because the estimated subspace is sparse in the features,\nthe resulting predictions will depend on only a small subset of genes. SuffPCR\nworks well on a variety of simulated and experimental transcriptomic data,\nperforming nearly optimally when the model assumptions are satisfied. We also\ndemonstrate near-optimal theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 17:23:45 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Ding", "Lei", ""], ["Zentner", "Gabriel E.", ""], ["McDonald", "Daniel J.", ""]]}, {"id": "2107.02283", "submitter": "Liao Zhu", "authors": "Liao Zhu, Ningning Sun, Martin T. Wells", "title": "Clustering Structure of Microstructure Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper builds the clustering model of measures of market microstructure\nfeatures which are popular in predicting the stock returns. In a 10-second time\nfrequency, we study the clustering structure of different measures to find out\nthe best ones for predicting. In this way, we can predict more accurately with\na limited number of predictors, which removes the noise and makes the model\nmore interpretable.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 21:40:08 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zhu", "Liao", ""], ["Sun", "Ningning", ""], ["Wells", "Martin T.", ""]]}, {"id": "2107.02291", "submitter": "Paramahansa Pramanik", "authors": "Paramahansa Pramanik and Alan M. Polansky", "title": "Optimal Estimation of Brownian Penalized Regression Coefficients", "comments": "27 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we introduce a new methodology to determine an optimal\ncoefficient of penalized functional regression. We assume the dependent,\nindependent variables and the regression coefficients are functions of time and\nerror dynamics follow a stochastic differential equation. First we construct\nour objective function as a time dependent residual sum of square and then\nminimize it with respect to regression coefficients subject to different error\ndynamics such as LASSO, group LASSO, fused LASSO and cubic smoothing spline.\nThen we use Feynman-type path integral approach to determine a\nSchr\\\"odinger-type equation which have the entire information of the system.\nUsing first order conditions with respect to these coefficients give us a\nclosed form solution of them.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 21:54:08 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Pramanik", "Paramahansa", ""], ["Polansky", "Alan M.", ""]]}, {"id": "2107.02324", "submitter": "Kei Hirose", "authors": "Kei Hirose, Kanta Miura and Atori Koie", "title": "Hierarchical clustered multiclass discriminant analysis via\n  cross-validation", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear discriminant analysis (LDA) is a well-known method for multiclass\nclassification and dimensionality reduction. However, in general, ordinary LDA\ndoes not achieve high prediction accuracy when observations in some classes are\ndifficult to be classified. This study proposes a novel cluster-based LDA\nmethod that significantly improves the prediction accuracy. We adopt\nhierarchical clustering, and the dissimilarity measure of two clusters is\ndefined by the cross-validation (CV) value. Therefore, clusters are constructed\nsuch that the misclassification error rate is minimized. Our approach involves\na heavy computational load because the CV value must be computed at each step\nof the hierarchical clustering algorithm. To address this issue, we develop a\nregression formulation for LDA and construct an efficient algorithm that\ncomputes an approximate value of the CV. The performance of the proposed method\nis investigated by applying it to both artificial and real datasets. Our\nproposed method provides high prediction accuracy with fast computation from\nboth numerical and theoretical viewpoints.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 23:57:35 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Hirose", "Kei", ""], ["Miura", "Kanta", ""], ["Koie", "Atori", ""]]}, {"id": "2107.02417", "submitter": "Erniel Barrios", "authors": "Ruby Anne E. Lemence, Erniel B. Barrios", "title": "Testing for the Presence of Structural Change and Spatial Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In a spatial-temporal model, structural change and/or spatial heterogeneity\ncan easily affect estimation of parameters. Following the spatial-temporal\nmodel in [1], we develop a nonparametric procedure for test-ing the presence of\nstructural change and spatial heterogeneity using bootstrap techniques and the\nforward search algorithm. The time series bootstrap can filter the effect of\ntemporary structural change in the con-struction of a confidence interval for\nthe temporal parameter. The forward search will also facilitate the\nconstruction of a robust confidence interval for the spatial parameter. These\nconfidence intervals are then used in deciding on the null hypothesis that\nthere is no structural change/spatial heterogeneity. Simulation studies\nillustrate the ability of the proposed test procedure in detecting presence of\nstructural change and spatial heterogeneity under certain conditions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 06:34:40 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Lemence", "Ruby Anne E.", ""], ["Barrios", "Erniel B.", ""]]}, {"id": "2107.02510", "submitter": "Changwoo Lee", "authors": "Changwoo J. Lee, Zhao Tang Luo, Huiyan Sang", "title": "T-LoHo: A Bayesian Regularization Model for Structured Sparsity and\n  Smoothness on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern complex data can be represented as a graph. In models dealing\nwith graph-structured data, multivariate parameters are not just sparse but\nhave structured sparsity and smoothness in the sense that both zero and\nnon-zero parameters tend to cluster together. We propose a new prior for high\ndimensional parameters with graphical relations, referred to as a Tree-based\nLow-rank Horseshoe(T-LoHo) model, that generalizes the popular univariate\nBayesian horseshoe shrinkage prior to the multivariate setting to detect\nstructured sparsity and smoothness simultaneously. The prior can be embedded in\nmany hierarchical high dimensional models. To illustrate its utility, we apply\nit to regularize a Bayesian high-dimensional regression problem where the\nregression coefficients are linked on a graph. The resulting clusters have\nflexible shapes and satisfy the cluster contiguity constraint with respect to\nthe graph. We design an efficient Markov chain Monte Carlo algorithm that\ndelivers full Bayesian inference with uncertainty measures for model parameters\nincluding the number of clusters. We offer theoretical investigations of the\nclustering effects and posterior concentration results. Finally, we illustrate\nthe performance of the model with simulation studies and real data applications\nsuch as anomaly detection in road networks. The results indicate substantial\nimprovements over other competing methods such as sparse fused lasso.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 10:10:03 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Lee", "Changwoo J.", ""], ["Luo", "Zhao Tang", ""], ["Sang", "Huiyan", ""]]}, {"id": "2107.02602", "submitter": "Yuan Liao", "authors": "Victor Chernozhukov, Christian Hansen, Yuan Liao, Yinchu Zhu", "title": "Inference for Low-Rank Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies inference in linear models whose parameter of interest is\na high-dimensional matrix. We focus on the case where the high-dimensional\nmatrix parameter is well-approximated by a ``spiked low-rank matrix'' whose\nrank grows slowly compared to its dimensions and whose nonzero singular values\ndiverge to infinity. We show that this framework covers a broad class of models\nof latent-variables which can accommodate matrix completion problems, factor\nmodels, varying coefficient models, principal components analysis with missing\ndata, and heterogeneous treatment effects. For inference, we propose a new\n``rotation-debiasing\" method for product parameters initially estimated using\nnuclear norm penalization. We present general high-level results under which\nour procedure provides asymptotically normal estimators. We then present\nlow-level conditions under which we verify the high-level conditions in a\ntreatment effects example.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 13:24:26 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Hansen", "Christian", ""], ["Liao", "Yuan", ""], ["Zhu", "Yinchu", ""]]}, {"id": "2107.02627", "submitter": "Pekka Korhonen", "authors": "Pekka Korhonen, Francis K.C. Hui, Jenni Niku, Sara Taskinen", "title": "Fast, universal estimation of latent variable models using extended\n  variational approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear latent variable models (GLLVMs) are a class of methods for\nanalyzing multi-response data which has garnered considerable popularity in\nrecent years, for example, in the analysis of multivariate abundance data in\necology. One of the main features of GLLVMs is their capacity to handle a\nvariety of responses types, such as (overdispersed) counts, binomial responses,\n(semi-)continuous, and proportions data. On the other hand, the introduction of\nunderlying latent variables presents some major computational challenges, as\nthe resulting marginal likelihood function involves an intractable integral for\nnon-normally distributed responses. This has spurred research into\napproximation methods to overcome this integral, with a recent and particularly\ncomputationally scalable one being that of variational approximations (VA).\nHowever, research into the use of VA of GLLVMs and related models has been\nhampered by the fact that closed-form approximations have only been obtained\nfor certain pairs of response distributions and link functions.\n  In this article, we propose an extended variational approximations (EVA)\napproach which widens the set of VA-applicable GLLVMs drastically. EVA draws\ninspiration from the underlying idea of Laplace approximations: by replacing\nthe complete-data likelihood function with its second order Taylor\napproximation about the mean of the variational distribution, we can obtain a\nclosed-form approximation to the marginal likelihood of the GLLVM for any\nresponse type and link function. Through simulation studies and an application\nto testate amoebae data set in ecology, we demonstrate how EVA results in a\nuniversal approach to fitting GLLVMs, which remains competitive in terms of\nestimation and inferential performance relative to both standard VA and a\nLaplace approximation approach, while being computationally more scalable than\nboth in practice.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 14:06:55 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Korhonen", "Pekka", ""], ["Hui", "Francis K. C.", ""], ["Niku", "Jenni", ""], ["Taskinen", "Sara", ""]]}, {"id": "2107.02667", "submitter": "Mike Pereira", "authors": "Annika Lang and Mike Pereira", "title": "Galerkin--Chebyshev approximation of Gaussian random fields on compact\n  Riemannian manifolds", "comments": "34 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new numerical approximation method for a class of Gaussian random fields on\ncompact Riemannian manifolds is introduced. This class of random fields is\ncharacterized by the Laplace--Beltrami operator on the manifold. A Galerkin\napproximation is combined with a polynomial approximation using Chebyshev\nseries. This so-called Galerkin--Chebyshev approximation scheme yields\nefficient and generic sampling algorithms for Gaussian random fields on\nmanifolds. Strong and weak orders of convergence for the Galerkin approximation\nand strong convergence orders for the Galerkin--Chebyshev approximation are\nshown and confirmed through numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 15:16:17 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Lang", "Annika", ""], ["Pereira", "Mike", ""]]}, {"id": "2107.02726", "submitter": "Qiang Sun", "authors": "Jiyu Luo, Qiang Sun, Wenxin Zhou", "title": "Distributed Adaptive Huber Regression", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Distributed data naturally arise in scenarios involving multiple sources of\nobservations, each stored at a different location. Directly pooling all the\ndata together is often prohibited due to limited bandwidth and storage, or due\nto privacy protocols. This paper introduces a new robust distributed algorithm\nfor fitting linear regressions when data are subject to heavy-tailed and/or\nasymmetric errors with finite second moments. The algorithm only communicates\ngradient information at each iteration and therefore is\ncommunication-efficient. Statistically, the resulting estimator achieves the\ncentralized nonasymptotic error bound as if all the data were pooled together\nand came from a distribution with sub-Gaussian tails. Under a finite\n$(2+\\delta)$-th moment condition, we derive a Berry-Esseen bound for the\ndistributed estimator, based on which we construct robust confidence intervals.\nNumerical studies further confirm that compared with extant distributed\nmethods, the proposed methods achieve near-optimal accuracy with low\nvariability and better coverage with tighter confidence width.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 16:50:51 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Luo", "Jiyu", ""], ["Sun", "Qiang", ""], ["Zhou", "Wenxin", ""]]}, {"id": "2107.02730", "submitter": "Qiang Sun", "authors": "Jianqing Fan, Wenyan Gong, Qiang Sun", "title": "A provable two-stage algorithm for penalized hazards regression", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  From an optimizer's perspective, achieving the global optimum for a general\nnonconvex problem is often provably NP-hard using the classical worst-case\nanalysis. In the case of Cox's proportional hazards model, by taking its\nstatistical model structures into account, we identify local strong convexity\nnear the global optimum, motivated by which we propose to use two convex\nprograms to optimize the folded-concave penalized Cox's proportional hazards\nregression. Theoretically, we investigate the statistical and computational\ntradeoffs of the proposed algorithm and establish the strong oracle property of\nthe resulting estimators. Numerical studies and real data analysis lend further\nsupport to our algorithm and theory.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 16:59:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Fan", "Jianqing", ""], ["Gong", "Wenyan", ""], ["Sun", "Qiang", ""]]}, {"id": "2107.02871", "submitter": "Nicholas Bussberg", "authors": "Nicholas W. Bussberg, Jacob Shields, Chunfeng Huang", "title": "Non-Homogeneity Estimation and Universal Kriging on the Sphere", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kriging is a widely recognized method for making spatial predictions. On the\nsphere, popular methods such as ordinary kriging assume that the spatial\nprocess is intrinsically homogeneous. However, intrinsic homogeneity is too\nstrict in many cases. This research uses intrinsic random function (IRF) theory\nto relax the homogeneity assumption. A key component of modeling IRF processes\nis estimating the degree of non-homogeneity. A graphical approach is proposed\nto accomplish this estimation. With the ability to estimate non-homogeneity, an\nIRF universal kriging procedure can be developed. Results from simulation\nstudies are provided to demonstrate the advantage of using IRF universal\nkriging as opposed to ordinary kriging when the underlying process is not\nintrinsically homogeneous.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 20:09:49 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Bussberg", "Nicholas W.", ""], ["Shields", "Jacob", ""], ["Huang", "Chunfeng", ""]]}, {"id": "2107.02891", "submitter": "Alexis Rosuel", "authors": "Alexis Rosuel (LIGM), Philippe Loubaton (LIGM), Pascal Vallet (IMS)", "title": "On the asymptotic distribution of the maximum sample spectral coherence\n  of Gaussian time series in the high dimensional regime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the asymptotic distribution of the maximum of a frequency\nsmoothed estimate of the spectral coherence of a M-variate complex Gaussian\ntime series with mutually independent components when the dimension M and the\nnumber of samples N both converge to infinity. If B denotes the smoothing span\nof the underlying smoothed periodogram estimator, a type I extreme value\nlimiting distribution is obtained under the rate assumptions M N $\\rightarrow$\n0 and M B $\\rightarrow$ c $\\in$ (0, +$\\infty$). This result is then exploited\nto build a statistic with controlled asymptotic level for testing independence\nbetween the M components of the observed time series. Numerical simulations\nsupport our results.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 08:07:49 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Rosuel", "Alexis", "", "LIGM"], ["Loubaton", "Philippe", "", "LIGM"], ["Vallet", "Pascal", "", "IMS"]]}, {"id": "2107.02903", "submitter": "Harsh Tripathi", "authors": "Harsh Tripathi, Mahendra Saha", "title": "An application of time truncated single acceptance sampling inspection\n  plan based on transmuted Rayleigh distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce single acceptance sampling inspection plan\n(SASIP) for transmuted Rayleigh (TR) distribution when the lifetime experiment\nis truncated at a prefixed time. Establish the proposed plan for different\nchoices of confidence level, acceptance number and ratio of true mean lifetime\nto specified mean lifetime. Minimum sample size necessary to ensure a certain\nspecified lifetime is obtained. Operating characteristic(OC) values and\nproducer's risk of proposed plan are presented. Two real life example has been\npresented to show the applicability of proposed SASIP.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 18:24:16 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Tripathi", "Harsh", ""], ["Saha", "Mahendra", ""]]}, {"id": "2107.02947", "submitter": "Mark Rubin", "authors": "Mark Rubin", "title": "When to adjust alpha during multiple testing: A consideration of\n  disjunction, conjunction, and individual testing", "comments": "Synthese (2021)", "journal-ref": null, "doi": "10.1007/s11229-021-03276-4", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Scientists often adjust their significance threshold (alpha level) during\nnull hypothesis significance testing in order to take into account multiple\ntesting and multiple comparisons. This alpha adjustment has become particularly\nrelevant in the context of the replication crisis in science. The present\narticle considers the conditions in which this alpha adjustment is appropriate\nand the conditions in which it is inappropriate. A distinction is drawn between\nthree types of multiple testing: disjunction testing, conjunction testing, and\nindividual testing. It is argued that alpha adjustment is only appropriate in\nthe case of disjunction testing, in which at least one test result must be\nsignificant in order to reject the associated joint null hypothesis. Alpha\nadjustment is inappropriate in the case of conjunction testing, in which all\nrelevant results must be significant in order to reject the joint null\nhypothesis. Alpha adjustment is also inappropriate in the case of individual\ntesting, in which each individual result must be significant in order to reject\neach associated individual null hypothesis. The conditions under which each of\nthese three types of multiple testing is warranted are examined. It is\nconcluded that researchers should not automatically (mindlessly) assume that\nalpha adjustment is necessary during multiple testing. Illustrations are\nprovided in relation to joint studywise hypotheses and joint multiway ANOVAwise\nhypotheses.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 23:46:48 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Rubin", "Mark", ""]]}, {"id": "2107.02990", "submitter": "Vathy Kamulete", "authors": "Vathy M. Kamulete", "title": "Test for non-negligible adverse shifts", "comments": "14 pages, 4 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Statistical tests for dataset shift are susceptible to false alarms: they are\nsensitive to minor differences where there is in fact adequate sample coverage\nand predictive performance. We propose instead a robust framework for tests of\ndataset shift based on outlier scores, D-SOS for short. D-SOS detects adverse\nshifts and can identify false alarms caused by benign ones. It posits that a\nnew (test) sample is not substantively worse than an old (training) sample, and\nnot that the two are equal. The key idea is to reduce observations to outlier\nscores and compare contamination rates. Beyond comparing distributions, users\ncan define what worse means in terms of predictive performance and other\nrelevant notions. We show how versatile and practical D-SOS is for a wide range\nof real and simulated datasets. Unlike tests of equal distribution and of\ngoodness-of-fit, the D-SOS tests are uniquely tailored to serve as robust\nperformance metrics to monitor model drift and dataset shift.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 03:07:40 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Kamulete", "Vathy M.", ""]]}, {"id": "2107.02999", "submitter": "Zeyu Wu", "authors": "Zeyu Wu, Cheng Wang, Weidong Liu", "title": "High dimensional precision matrix estimation under weak sparsity", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we estimate the high dimensional precision matrix under the\nweak sparsity condition where many entries are nearly zero. We study a\nLasso-type method for high dimensional precision matrix estimation and derive\ngeneral error bounds under the weak sparsity condition. The common\nirrepresentable condition is relaxed and the results are applicable to the weak\nsparse matrix. As applications, we study the precision matrix estimation for\nthe heavy-tailed data, the non-paranormal data, and the matrix data with the\nLasso-type method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 03:41:57 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wu", "Zeyu", ""], ["Wang", "Cheng", ""], ["Liu", "Weidong", ""]]}, {"id": "2107.03119", "submitter": "Sheng Dai", "authors": "Sheng Dai", "title": "Variable selection in convex quantile regression: L1-norm or L0-norm\n  regularization?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The curse of dimensionality is a recognized challenge in nonparametric\nestimation. This paper develops a new L0-norm regularization approach to the\nconvex quantile and expectile regressions for subset variable selection. We\nshow how to use mixed integer programming to solve the proposed L0-norm\nregularization approach in practice and build a link to the commonly used\nL1-norm regularization approach. A Monte Carlo study is performed to compare\nthe finite sample performances of the proposed L0-penalized convex quantile and\nexpectile regression approaches with the L1-norm regularization approaches. The\nproposed approach is further applied to benchmark the sustainable development\nperformance of the OECD countries and empirically analyze the accuracy in the\ndimensionality reduction of variables. The results from the simulation and\napplication illustrate that the proposed L0-norm regularization approach can\nmore effectively address the curse of dimensionality than the L1-norm\nregularization approach in multidimensional spaces.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:58:05 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Dai", "Sheng", ""]]}, {"id": "2107.03190", "submitter": "Juan Correa", "authors": "Juan D Correa, Sanghack Lee, Elias Bareinboim", "title": "Nested Counterfactual Identification from Arbitrary Surrogate\n  Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Ladder of Causation describes three qualitatively different types of\nactivities an agent may be interested in engaging in, namely, seeing\n(observational), doing (interventional), and imagining (counterfactual) (Pearl\nand Mackenzie, 2018). The inferential challenge imposed by the causal hierarchy\nis that data is collected by an agent observing or intervening in a system\n(layers 1 and 2), while its goal may be to understand what would have happened\nhad it taken a different course of action, contrary to what factually ended up\nhappening (layer 3). While there exists a solid understanding of the conditions\nunder which cross-layer inferences are allowed from observations to\ninterventions, the results are somewhat scarcer when targeting counterfactual\nquantities. In this paper, we study the identification of nested\ncounterfactuals from an arbitrary combination of observations and experiments.\nSpecifically, building on a more explicit definition of nested counterfactuals,\nwe prove the counterfactual unnesting theorem (CUT), which allows one to map\narbitrary nested counterfactuals to unnested ones. For instance, applications\nin mediation and fairness analysis usually evoke notions of direct, indirect,\nand spurious effects, which naturally require nesting. Second, we introduce a\nsufficient and necessary graphical condition for counterfactual identification\nfrom an arbitrary combination of observational and experimental distributions.\nLastly, we develop an efficient and complete algorithm for identifying nested\ncounterfactuals; failure of the algorithm returning an expression for a query\nimplies it is not identifiable.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 12:51:04 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Correa", "Juan D", ""], ["Lee", "Sanghack", ""], ["Bareinboim", "Elias", ""]]}, {"id": "2107.03230", "submitter": "Luka Grb\\v{c}i\\'c", "authors": "Luka Grb\\v{c}i\\'c, Sini\\v{s}a Dru\\v{z}eta, Goran Mau\\v{s}a, Tomislav\n  Lipi\\'c, Darija Vuki\\'c Lu\\v{s}i\\'c, Marta Alvir, Ivana Lu\\v{c}in, Ante\n  Sikirica, Davor Davidovi\\'c, Vanja Trava\\v{s}, Daniela Kalafatovi\\'c,\n  Kristina Pikelj, Hana Fajkovi\\'c, Toni Holjevi\\'c and Lado Kranj\\v{c}evi\\'c", "title": "Coastal water quality prediction based on machine learning with feature\n  interpretation and spatio-temporal analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Coastal water quality management is a public health concern, as poor coastal\nwater quality can harbor pathogens that are dangerous to human health.\nTourism-oriented countries need to actively monitor the condition of coastal\nwater at tourist popular sites during the summer season. In this study, routine\nmonitoring data of $Escherichia\\ Coli$ and enterococci across 15 public beaches\nin the city of Rijeka, Croatia, were used to build machine learning models for\npredicting their levels based on environmental parameters as well as to\ninvestigate their relationships with environmental stressors. Gradient Boosting\n(Catboost, Xgboost), Random Forests, Support Vector Regression and Artificial\nNeural Networks were trained with measurements from all sampling sites and used\nto predict $E.\\ Coli$ and enterococci values based on environmental features.\nThe evaluation of stability and generalizability with 10-fold cross validation\nanalysis of the machine learning models, showed that the Catboost algorithm\nperformed best with R$^2$ values of 0.71 and 0.68 for predicting $E.\\ Coli$ and\nenterococci, respectively, compared to other evaluated ML algorithms including\nXgboost, Random Forests, Support Vector Regression and Artificial Neural\nNetworks. We also use the SHapley Additive exPlanations technique to identify\nand interpret which features have the most predictive power. The results show\nthat site salinity measured is the most important feature for forecasting both\n$E.\\ Coli$ and enterococci levels. Finally, the spatial and temporal accuracy\nof both ML models were examined at sites with the lowest coastal water quality.\nThe spatial $E. Coli$ and enterococci models achieved strong R$^2$ values of\n0.85 and 0.83, while the temporal models achieved R$^2$ values of 0.74 and\n0.67. The temporal model also achieved moderate R$^2$ values of 0.44 and 0.46\nat a site with high coastal water quality.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 14:00:14 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 07:09:03 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Grb\u010di\u0107", "Luka", ""], ["Dru\u017eeta", "Sini\u0161a", ""], ["Mau\u0161a", "Goran", ""], ["Lipi\u0107", "Tomislav", ""], ["Lu\u0161i\u0107", "Darija Vuki\u0107", ""], ["Alvir", "Marta", ""], ["Lu\u010din", "Ivana", ""], ["Sikirica", "Ante", ""], ["Davidovi\u0107", "Davor", ""], ["Trava\u0161", "Vanja", ""], ["Kalafatovi\u0107", "Daniela", ""], ["Pikelj", "Kristina", ""], ["Fajkovi\u0107", "Hana", ""], ["Holjevi\u0107", "Toni", ""], ["Kranj\u010devi\u0107", "Lado", ""]]}, {"id": "2107.03325", "submitter": "David Kepplinger", "authors": "David Kepplinger", "title": "Robust Variable Selection and Estimation Via Adaptive Elastic Net\n  S-Estimators for Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heavy-tailed error distributions and predictors with anomalous values are\nubiquitous in high-dimensional regression problems and can seriously jeopardize\nthe validity of statistical analyses if not properly addressed. For more\nreliable estimation under these adverse conditions, we propose a new robust\nregularized estimator for simultaneous variable selection and coefficient\nestimation. This estimator, called adaptive PENSE, possesses the oracle\nproperty without prior knowledge of the scale of the residuals and without any\nmoment conditions on the error distribution. The proposed estimator gives\nreliable results even under very heavy-tailed error distributions and aberrant\ncontamination in the predictors or residuals. Importantly, even in these\nchallenging settings variable selection by adaptive PENSE remains stable.\nNumerical studies on simulated and real data sets highlight superior\nfinite-sample performance in a vast range of settings compared to other robust\nregularized estimators in the case of contaminated samples and competitiveness\ncompared to classical regularized estimators in clean samples.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 16:04:08 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 15:52:05 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Kepplinger", "David", ""]]}, {"id": "2107.03430", "submitter": "Kaixu Yang", "authors": "Kaixu Yang and Tapabrata Maiti", "title": "ENNS: Variable Selection, Regression, Classification and Deep Neural\n  Network for High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-dimensional, low sample-size (HDLSS) data problems have been a topic of\nimmense importance for the last couple of decades. There is a vast literature\nthat proposed a wide variety of approaches to deal with this situation, among\nwhich variable selection was a compelling idea. On the other hand, a deep\nneural network has been used to model complicated relationships and\ninteractions among responses and features, which is hard to capture using a\nlinear or an additive model. In this paper, we discuss the current status of\nvariable selection techniques with the neural network models. We show that the\nstage-wise algorithm with neural network suffers from disadvantages such as the\nvariables entering into the model later may not be consistent. We then propose\nan ensemble method to achieve better variable selection and prove that it has\nprobability tending to zero that a false variable is selected. Then, we discuss\nadditional regularization to deal with over-fitting and make better regression\nand classification. We study various statistical properties of our proposed\nmethod. Extensive simulations and real data examples are provided to support\nthe theory and methodology.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 18:26:04 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Yang", "Kaixu", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2107.03441", "submitter": "Nicholas Illenberger", "authors": "Nicholas Illenberger, Andrew J. Spieker, and Nandita Mitra", "title": "Identifying optimally cost-effective dynamic treatment regimes with a\n  Q-learning approach", "comments": "18 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health policy decisions regarding patient treatment strategies require\nconsideration of both treatment effectiveness and cost. Optimizing treatment\nrules with respect to effectiveness may result in prohibitively expensive\nstrategies; on the other hand, optimizing with respect to costs may result in\npoor patient outcomes. We propose a two-step approach for identifying an\noptimally cost-effective and interpretable dynamic treatment regime. First, we\ndevelop a combined Q-learning and policy-search approach to estimate an optimal\nlist-based regime under a constraint on expected treatment costs. Second, we\npropose an iterative procedure to select an optimally cost-effective regime\nfrom a set of candidate regimes corresponding to different cost constraints.\nOur approach can estimate optimal regimes in the presence of commonly\nencountered challenges including time-varying confounding and correlated\noutcomes. Through simulation studies, we illustrate the validity of estimated\noptimal treatment regimes and examine operating characteristics under flexible\nmodeling approaches.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 19:02:36 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 14:33:53 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Illenberger", "Nicholas", ""], ["Spieker", "Andrew J.", ""], ["Mitra", "Nandita", ""]]}, {"id": "2107.03459", "submitter": "Justin Rising", "authors": "Justin Rising", "title": "Uncertainty in Ranking", "comments": "23 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ranks estimated from data are uncertain and this poses a challenge in many\napplications. However, estimated ranks are deterministic functions of estimated\nparameters, so the uncertainty in the ranks must be determined by the\nuncertainty in the parameter estimates. We give a complete characterization of\nthis relationship in terms of the linear extensions of a partial order\ndetermined by interval estimates of the parameters of interest. We then use\nthis relationship to give a set estimator for the overall ranking, use its size\nto measure the uncertainty in a ranking, and give efficient algorithms for\nseveral questions of interest. We show that our set estimator is a valid\nconfidence set and describe its relationship to a joint confidence set for\nranks recently proposed by Klein, Wright \\& Wieczorek. We apply our methods to\nboth simulated and real data and make them available through the R package\nrankUncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 19:41:11 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 14:58:25 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 19:30:57 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Rising", "Justin", ""]]}, {"id": "2107.03494", "submitter": "Iain Carmichael", "authors": "Iain Carmichael", "title": "The folded concave Laplacian spectral penalty learns block diagonal\n  sparsity patterns with the strong oracle property", "comments": "First draft! 60 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structured sparsity is an important part of the modern statistical toolkit.\nWe say set of model parameters has block diagonal sparsity up to permutations\nif its elements can be viewed as the edges of a graph that has multiple\nconnected components. For example, a block diagonal correlation matrix with K\nblocks of variables corresponds to a graph with K connected components whose\nnodes are the variables and whose edges are the correlations. This type of\nsparsity captures clusters of model parameters. To learn block diagonal\nsparsity patterns we develop the folded concave Laplacian spectral penalty and\nprovide a majorization-minimization algorithm for the resulting non-convex\nproblem. We show this algorithm has the appealing computational and statistical\nguarantee of converging to the oracle estimator after two steps with high\nprobability, even in high-dimensional settings. The theory is then demonstrated\nin several classical problems including covariance estimation, linear\nregression, and logistic regression.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 21:45:01 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Carmichael", "Iain", ""]]}, {"id": "2107.03544", "submitter": "Tianchen Qian", "authors": "Tianchen Qian, Ashley E. Walton, Linda M. Collins, Predrag Klasnja,\n  Stephanie T. Lanza, Inbal Nahum-Shani, Mashifiqui Rabbi, Michael A. Russell,\n  Maureen A. Walton, Hyesun Yoo, Susan A. Murphy", "title": "The Micro-Randomized Trial for Developing Digital Interventions:\n  Experimental Design and Data Analysis Considerations", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.05880,\n  arXiv:2004.10241", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Just-in-time adaptive interventions (JITAIs) are time-varying adaptive\ninterventions that use frequent opportunities for the intervention to be\nadapted--weekly, daily, or even many times a day. The micro-randomized trial\n(MRT) has emerged for use in informing the construction of JITAIs. MRTs can be\nused to address research questions about whether and under what circumstances\nJITAI components are effective, with the ultimate objective of developing\neffective and efficient JITAI. The purpose of this article is to clarify why,\nwhen, and how to use MRTs; to highlight elements that must be considered when\ndesigning and implementing an MRT; and to review primary and secondary analyses\nmethods for MRTs. We briefly review key elements of JITAIs and discuss a\nvariety of considerations that go into planning and designing an MRT. We\nprovide a definition of causal excursion effects suitable for use in primary\nand secondary analyses of MRT data to inform JITAI development. We review the\nweighted and centered least-squares (WCLS) estimator which provides consistent\ncausal excursion effect estimators from MRT data. We describe how the WCLS\nestimator along with associated test statistics can be obtained using standard\nstatistical software such as R (R Core Team, 2019). Throughout we illustrate\nthe MRT design and analyses using the HeartSteps MRT, for developing a JITAI to\nincrease physical activity among sedentary individuals. We supplement the\nHeartSteps MRT with two other MRTs, SARA and BariFit, each of which highlights\ndifferent research questions that can be addressed using the MRT and\nexperimental design considerations that might arise.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 00:25:59 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 05:40:51 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Qian", "Tianchen", ""], ["Walton", "Ashley E.", ""], ["Collins", "Linda M.", ""], ["Klasnja", "Predrag", ""], ["Lanza", "Stephanie T.", ""], ["Nahum-Shani", "Inbal", ""], ["Rabbi", "Mashifiqui", ""], ["Russell", "Michael A.", ""], ["Walton", "Maureen A.", ""], ["Yoo", "Hyesun", ""], ["Murphy", "Susan A.", ""]]}, {"id": "2107.03584", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Runjing Liu, Michael I. Jordan, Tamara Broderick", "title": "Evaluating Sensitivity to the Stick-Breaking Prior in Bayesian\n  Nonparametrics", "comments": "65 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian models based on the Dirichlet process and other stick-breaking\npriors have been proposed as core ingredients for clustering, topic modeling,\nand other unsupervised learning tasks. Prior specification is, however,\nrelatively difficult for such models, given that their flexibility implies that\nthe consequences of prior choices are often relatively opaque. Moreover, these\nchoices can have a substantial effect on posterior inferences. Thus,\nconsiderations of robustness need to go hand in hand with nonparametric\nmodeling. In the current paper, we tackle this challenge by exploiting the fact\nthat variational Bayesian methods, in addition to having computational\nadvantages in fitting complex nonparametric models, also yield sensitivities\nwith respect to parametric and nonparametric aspects of Bayesian models. In\nparticular, we demonstrate how to assess the sensitivity of conclusions to the\nchoice of concentration parameter and stick-breaking distribution for\ninferences under Dirichlet process mixtures and related mixture models. We\nprovide both theoretical and empirical support for our variational approach to\nBayesian sensitivity analysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 03:40:18 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 16:44:03 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Giordano", "Ryan", ""], ["Liu", "Runjing", ""], ["Jordan", "Michael I.", ""], ["Broderick", "Tamara", ""]]}, {"id": "2107.03597", "submitter": "Wenyu Chen", "authors": "Wenyu Chen, Mathias Drton, Ali Shojaie", "title": "Causal Structural Learning Via Local Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning causal structures in sparse\nhigh-dimensional settings that may be subject to the presence of (potentially\nmany) unmeasured confounders, as well as selection bias. Based on the structure\nfound in common families of large random networks and examining the\nrepresentation of local structures in linear structural equation models (SEM),\nwe propose a new local notion of sparsity for consistent structure learning in\nthe presence of latent and selection variables, and develop a new version of\nthe Fast Causal Inference (FCI) algorithm with reduced computational and sample\ncomplexity, which we refer to as local FCI (lFCI). The new notion of sparsity\nallows the presence of highly connected hub nodes, which are common in\nreal-world networks, but problematic for existing methods. Our numerical\nexperiments indicate that the lFCI algorithm achieves state-of-the-art\nperformance across many classes of large random networks, and its performance\nis superior to that of existing methods for networks containing hub nodes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 04:23:39 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Chen", "Wenyu", ""], ["Drton", "Mathias", ""], ["Shojaie", "Ali", ""]]}, {"id": "2107.03674", "submitter": "Mikkel Bennedsen", "authors": "Mikkel Bennedsen, Asger Lunde, Neil Shephard, Almut E. D. Veraart", "title": "Inference and forecasting for continuous-time integer-valued trawl\n  processes and their use in financial economics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops likelihood-based methods for estimation, inference, model\nselection, and forecasting of continuous-time integer-valued trawl processes.\nThe full likelihood of integer-valued trawl processes is, in general, highly\nintractable, motivating the use of composite likelihood methods, where we\nconsider the pairwise likelihood in lieu of the full likelihood. Maximizing the\npairwise likelihood of the data yields an estimator of the parameter vector of\nthe model, and we prove consistency and asymptotic normality of this estimator.\nThe same methods allow us to develop probabilistic forecasting methods, which\ncan be used to construct the predictive distribution of integer-valued time\nseries. In a simulation study, we document good finite sample performance of\nthe likelihood-based estimator and the associated model selection procedure.\nLastly, the methods are illustrated in an application to modelling and\nforecasting financial bid-ask spread data, where we find that it is beneficial\nto carefully model both the marginal distribution and the autocorrelation\nstructure of the data. We argue that integer-valued trawl processes are\nespecially well-suited in such situations.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 08:27:42 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Bennedsen", "Mikkel", ""], ["Lunde", "Asger", ""], ["Shephard", "Neil", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "2107.03836", "submitter": "John Lazarsfeld", "authors": "John Lazarsfeld and Aaron Johnson", "title": "Consistency of the Maximal Information Coefficient Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Maximal Information Coefficient (MIC) of Reshef et al. (Science, 2011) is\na statistic for measuring dependence between variable pairs in large datasets.\nIn this note, we prove that MIC is a consistent estimator of the corresponding\npopulation statistic MIC$_*$. This corrects an error in an argument of Reshef\net al. (JMLR, 2016), which we describe.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 13:28:06 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Lazarsfeld", "John", ""], ["Johnson", "Aaron", ""]]}, {"id": "2107.03873", "submitter": "Mattia  Zorzi", "authors": "Lucia Falconi, Augusto Ferrante and Mattia Zorzi", "title": "A Robust Approach to ARMA Factor Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper deals with the dynamic factor analysis problem for an ARMA\nprocess. To robustly estimate the number of factors, we construct a confidence\nregion centered in a finite sample estimate of the underlying model which\ncontains the true model with a prescribed probability. In this confidence\nregion, the problem, formulated as a rank minimization of a suitable spectral\ndensity, is efficiently approximated via a trace norm convex relaxation. The\nlatter is addressed by resorting to the Lagrange duality theory, which allows\nto prove the existence of solutions. Finally, a numerical algorithm to solve\nthe dual problem is presented. The effectiveness of the proposed estimator is\nassessed through simulation studies both with synthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 14:27:23 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Falconi", "Lucia", ""], ["Ferrante", "Augusto", ""], ["Zorzi", "Mattia", ""]]}, {"id": "2107.03883", "submitter": "Philippe Lambert", "authors": "Philippe Lambert", "title": "Moment-based density and risk estimation from grouped summary statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data on a continuous variable are often summarized by means of histograms or\ndisplayed in tabular format: the range of data is partitioned into consecutive\ninterval classes and the number of observations falling within each class is\nprovided to the analyst. Computations can then be carried in a nonparametric\nway by assuming a uniform distribution of the variable within each partitioning\nclass, by concentrating all the observed values in the center, or by spreading\nthem to the extremities. Smoothing methods can also be applied to estimate the\nunderlying density or a parametric model can be fitted to these grouped data.\nFor insurance loss data, some additional information is often provided about\nthe observed values contained in each class, typically class-specific sample\nmoments such as the mean, the variance or even the skewness and the kurtosis.\nThe question is then how to include this additional information in the\nestimation procedure. The present paper proposes a method for performing\ndensity and quantile estimation based on such augmented information with an\nillustration on car insurance data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:00:31 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Lambert", "Philippe", ""]]}, {"id": "2107.03922", "submitter": "Beth Ann Griffin PhD", "authors": "Melody Y. Huang, Brian G. Vegetabile, Lane F. Burgette, Claude\n  Setodji, Beth Ann Griffin", "title": "Balancing Higher Moments Matters for Causal Estimation: Further Context\n  for the Results of Setodji et al. (2017)", "comments": "9 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We expand upon the simulation study of Setodji et al. (2017) which compared\nthree promising balancing methods when assessing the average treatment effect\non the treated for binary treatments: generalized boosted models (GBM),\ncovariate-balancing propensity scores (CBPS), and entropy balance (EB). The\nstudy showed that GBM can outperform CBPS and EB when there are likely to be\nnon-linear associations in both the treatment assignment and outcome models and\nCBPS and EB are fine-tuned to obtain balance only on first order moments. We\nexplore the potential benefit of using higher-order moments in the balancing\nconditions for CBPS and EB. Our findings showcase that CBPS and EB should, by\ndefault, include higher order moments and that focusing only on first moments\ncan result in substantial bias in both CBPS and EB estimated treatment effect\nestimates that could be avoided by the use of higher moments.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:54:41 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Huang", "Melody Y.", ""], ["Vegetabile", "Brian G.", ""], ["Burgette", "Lane F.", ""], ["Setodji", "Claude", ""], ["Griffin", "Beth Ann", ""]]}, {"id": "2107.03940", "submitter": "Yann Issartel", "authors": "Cristina Butucea and Yann Issartel", "title": "Locally differentially private estimation of nonlinear functionals of\n  discrete distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating non-linear functionals of discrete\ndistributions in the context of local differential privacy. The initial data\n$x_1,\\ldots,x_n \\in [K]$ are supposed i.i.d. and distributed according to an\nunknown discrete distribution $p = (p_1,\\ldots,p_K)$. Only $\\alpha$-locally\ndifferentially private (LDP) samples $z_1,...,z_n$ are publicly available,\nwhere the term 'local' means that each $z_i$ is produced using one individual\nattribute $x_i$. We exhibit privacy mechanisms (PM) that are interactive (i.e.\nthey are allowed to use already published confidential data) or\nnon-interactive. We describe the behavior of the quadratic risk for estimating\nthe power sum functional $F_{\\gamma} = \\sum_{k=1}^K p_k^{\\gamma}$, $\\gamma >0$\nas a function of $K, \\, n$ and $\\alpha$. In the non-interactive case, we study\ntwo plug-in type estimators of $F_{\\gamma}$, for all $\\gamma >0$, that are\nsimilar to the MLE analyzed by Jiao et al. (2017) in the multinomial model.\nHowever, due to the privacy constraint the rates we attain are slower and\nsimilar to those obtained in the Gaussian model by Collier et al. (2020). In\nthe interactive case, we introduce for all $\\gamma >1$ a two-step procedure\nwhich attains the faster parametric rate $(n \\alpha^2)^{-1/2}$ when $\\gamma\n\\geq 2$. We give lower bounds results over all $\\alpha$-LDP mechanisms and all\nestimators using the private samples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 16:11:10 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Butucea", "Cristina", ""], ["Issartel", "Yann", ""]]}, {"id": "2107.03975", "submitter": "Jorge Silva", "authors": "Jorge F. Silva", "title": "Compressibility Analysis of Asymptotically Mean Stationary Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work provides new results for the analysis of random sequences in terms\nof $\\ell_p$-compressibility. The results characterize the degree in which a\nrandom sequence can be approximated by its best $k$-sparse version under\ndifferent rates of significant coefficients (compressibility analysis). In\nparticular, the notion of strong $\\ell_p$-characterization is introduced to\ndenote a random sequence that has a well-defined asymptotic limit (sample-wise)\nof its best $k$-term approximation error when a fixed rate of significant\ncoefficients is considered (fixed-rate analysis). The main theorem of this work\nshows that the rich family of asymptotically mean stationary (AMS) processes\nhas a strong $\\ell_p$-characterization. Furthermore, we present results that\ncharacterize and analyze the $\\ell_p$-approximation error function for this\nfamily of processes. Adding ergodicity in the analysis of AMS processes, we\nintroduce a theorem demonstrating that the approximation error function is\nconstant and determined in closed-form by the stationary mean of the process.\nOur results and analyses contribute to the theory and understanding of\ndiscrete-time sparse processes and, on the technical side, confirm how\ninstrumental the point-wise ergodic theorem is to determine the compressibility\nexpression of discrete-time processes even when stationarity and ergodicity\nassumptions are relaxed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:06:21 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Silva", "Jorge F.", ""]]}, {"id": "2107.04118", "submitter": "Felipe Maia Polo", "authors": "Alex Akira Okuno and Felipe Maia Polo", "title": "OCDE: Odds Conditional Density Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conditional density estimation (CDE) models can be useful for many\nstatistical applications, especially because the full conditional density is\nestimated instead of traditional regression point estimates, revealing more\ninformation about the uncertainty of the random variable of interest. In this\npaper, we propose a new methodology called Odds Conditional Density Estimator\n(OCDE) to estimate conditional densities in a supervised learning scheme. The\nmain idea is that it is very difficult to estimate $p_{x,y}$ and $p_{x}$ in\norder to estimate the conditional density $p_{y|x}$, but by introducing an\ninstrumental distribution, we transform the CDE problem into a problem of odds\nestimation, or similarly, training a binary probabilistic classifier. We\ndemonstrate how OCDE works using simulated data and then test its performance\nagainst other known state-of-the-art CDE methods in real data. Overall, OCDE is\ncompetitive compared with these methods in real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 03:49:52 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Okuno", "Alex Akira", ""], ["Polo", "Felipe Maia", ""]]}, {"id": "2107.04208", "submitter": "Michael Bertolacci", "authors": "Noel Cressie and Michael Bertolacci and Andrew Zammit-Mangion", "title": "From Many to One: Consensus Inference in a MIP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph physics.geo-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Model Intercomparison Project (MIP) consists of teams who each estimate the\nsame underlying quantity (e.g., temperature projections to the year 2070), and\nthe spread of the estimates indicates their uncertainty. It recognizes that a\ncommunity of scientists will not agree completely but that there is value in\nlooking for a consensus and information in the range of disagreement. A simple\naverage of the teams' outputs gives a consensus estimate, but it does not\nrecognize that some outputs are more variable than others. Statistical analysis\nof variance (ANOVA) models offer a way to obtain a weighted consensus estimate\nof outputs with a variance that is the smallest possible and hence the tightest\npossible 'one-sigma' and 'two-sigma' intervals. Modulo dependence between MIP\noutputs, the ANOVA approach weights a team's output inversely proportional to\nits variation. When external verification data are available for evaluating the\nfidelity of each MIP output, ANOVA weights can also provide a prior\ndistribution for Bayesian Model Averaging to yield a consensus estimate. We use\na MIP of carbon dioxide flux inversions to illustrate the ANOVA-based weighting\nand subsequent consensus inferences.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 04:56:18 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Cressie", "Noel", ""], ["Bertolacci", "Michael", ""], ["Zammit-Mangion", "Andrew", ""]]}, {"id": "2107.04330", "submitter": "Salvatore Daniele Tomarchio Ph.D", "authors": "Salvatore D. Tomarchio, Antonio Punzo, Antonello Maruotti", "title": "Parsimonious Hidden Markov Models for Matrix-Variate Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hidden Markov models (HMMs) have been extensively used in the univariate and\nmultivariate literature. However, there has been an increased interest in the\nanalysis of matrix-variate data over the recent years. In this manuscript we\nintroduce HMMs for matrix-variate longitudinal data, by assuming a matrix\nnormal distribution in each hidden state. Such data are arranged in a four-way\narray. To address for possible overparameterization issues, we consider the\nspectral decomposition of the covariance matrices, leading to a total of 98\nHMMs. An expectation-conditional maximization algorithm is discussed for\nparameter estimation. The proposed models are firstly investigated on simulated\ndata, in terms of parameter recovery, computational times and model selection.\nThen, they are fitted to a four-way real data set concerning the unemployment\nrates of the Italian provinces, evaluated by gender and age classes, over the\nlast 16 years.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 09:49:26 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Tomarchio", "Salvatore D.", ""], ["Punzo", "Antonio", ""], ["Maruotti", "Antonello", ""]]}, {"id": "2107.04392", "submitter": "Camila Olarte Parra", "authors": "Camila Olarte Parra, Rhian M. Daniel, Jonathan W. Bartlett", "title": "Hypothetical estimands in clinical trials: a unification of causal\n  inference and missing data methods", "comments": "44 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ICH E9 addendum introduces the term intercurrent event to refer to events\nthat happen after randomisation and that can either preclude observation of the\noutcome of interest or affect its interpretation. It proposes five strategies\nfor handling intercurrent events to form an estimand but does not suggest\nstatistical methods for estimation. In this paper we focus on the hypothetical\nstrategy, where the treatment effect is defined under the hypothetical scenario\nin which the intercurrent event is prevented. For its estimation, we consider\ncausal inference and missing data methods. We establish that certain 'causal\ninference estimators' are identical to certain 'missing data estimators'. These\nlinks may help those familiar with one set of methods but not the other.\nMoreover, using potential outcome notation allows us to state more clearly the\nassumptions on which missing data methods rely to estimate hypothetical\nestimands. This helps to indicate whether estimating a hypothetical estimand is\nreasonable, and what data should be used in the analysis. We show that\nhypothetical estimands can be estimated by exploiting data after intercurrent\nevent occurrence, which is typically not used. We also present Monte Carlo\nsimulations that illustrate the implementation and performance of the methods\nin different settings.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 12:41:30 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Parra", "Camila Olarte", ""], ["Daniel", "Rhian M.", ""], ["Bartlett", "Jonathan W.", ""]]}, {"id": "2107.04496", "submitter": "Jizi Shangguan", "authors": "Jizi Shangguan", "title": "Joint Modeling of Longitudinal and Survival Data with Censored\n  Single-index Varying Coefficient Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical and biological research, longitudinal data and survival data types\nare commonly seen. Traditional statistical models mostly consider to deal with\neither of the data types, such as linear mixed models for longitudinal data,\nand the Cox models for survival data, while they do not adjust the association\nbetween these two different data types. It is desirable to have a joint\nmodeling approach which accomadates both data types and the dependency between\nthem. In this paper, we extend traditional single-index models to a new joint\nmodeling approach, by replacing the single-index component to a varying\ncoefficient component to deal with longitudinal outcomes, and accomadate the\nrandom censoring problem in survival analysis by nonparametric synthetic data\nregression for the link function. Numerical experiments are conducted to\nevaluate the finite sample performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 15:37:43 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Shangguan", "Jizi", ""]]}, {"id": "2107.04529", "submitter": "Ariel Caticha", "authors": "Ariel Caticha", "title": "Entropy, Information, and the Updating of Probabilities", "comments": "28 pages. Invited paper to appear in Entropy in the special volume\n  \"Statistical Foundations of Entropy\", ed. by P. Jizba and J. Korbel. arXiv\n  admin note: text overlap with arXiv:1412.5644", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.AI stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is a review of a particular approach to the method of maximum\nentropy as a general framework for inference. The discussion emphasizes the\npragmatic elements in the derivation. An epistemic notion of information is\ndefined in terms of its relation to the Bayesian beliefs of ideally rational\nagents. The method of updating from a prior to a posterior probability\ndistribution is designed through an eliminative induction process. The\nlogarithmic relative entropy is singled out as the unique tool for updating\nthat (a) is of universal applicability; (b) that recognizes the value of prior\ninformation; and (c) that recognizes the privileged role played by the notion\nof independence in science. The resulting framework -- the ME method -- can\nhandle arbitrary priors and arbitrary constraints. It includes MaxEnt and\nBayes' rule as special cases and, therefore, it unifies entropic and Bayesian\nmethods into a single general inference scheme. The ME method goes beyond the\nmere selection of a single posterior, but also addresses the question of how\nmuch less probable other distributions might be, which provides a direct bridge\nto the theories of fluctuations and large deviations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 16:27:23 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Caticha", "Ariel", ""]]}, {"id": "2107.04542", "submitter": "Tim Weninger PhD", "authors": "Justus Hibshman and Tim Weninger", "title": "Higher Order Imprecise Probabilities and Statistical Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We generalize standard credal set models for imprecise probabilities to\ninclude higher order credal sets -- confidences about confidences. In doing so,\nwe specify how an agent's higher order confidences (credal sets) update upon\nobserving an event. Our model begins to address standard issues with imprecise\nprobability models, like Dilation and Belief Inertia. We conjecture that when\nhigher order credal sets contain all possible probability functions, then in\nthe limiting case the highest order confidences converge to form a uniform\ndistribution over the first order credal set, where we define uniformity in\nterms of the statistical distance metric (total variation distance). Finite\nsimulation supports the conjecture. We further suggest that this convergence\npresents the total-variation-uniform distribution as a natural, privileged\nprior for statistical hypothesis testing.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 16:58:43 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 15:50:14 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Hibshman", "Justus", ""], ["Weninger", "Tim", ""]]}, {"id": "2107.04552", "submitter": "Gabriel Ducrocq", "authors": "Nicolas Chopin and Gabriel Ducrocq", "title": "Fast compression of MCMC output", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose cube thinning, a novel method for compressing the output of a MCMC\n(Markov chain Monte Carlo) algorithm when control variates are available. It\namounts to resampling the initial MCMC sample (according to weights derived\nfrom control variates), while imposing equality constraints on averages of\nthese control variates, using the cube method of \\cite{Deville2004}. Its main\nadvantage is that its CPU cost is linear in $N$, the original sample size, and\nis constant in $M$, the required size for the compressed sample. This compares\nfavourably to Stein thinning \\citep{Riabiz2020}, which has complexity\n$\\mathcal{O}(NM^2)$, and which requires the availability of the gradient of the\ntarget log-density (which automatically implies the availability of control\nvariates). Our numerical experiments suggest that cube thinning is also\ncompetitive in terms of statistical error.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 17:11:26 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 21:26:39 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Chopin", "Nicolas", ""], ["Ducrocq", "Gabriel", ""]]}, {"id": "2107.04630", "submitter": "Florian Br\\\"uck", "authors": "Florian Br\\\"uck", "title": "Exact simulation of continuous max-id processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide two algorithms for the exact simulation of exchangeable\nmax-(min-)id stochastic processes and random vectors. Our algorithms only\nrequire the simulation of finite Poisson random measures and avoid the\nnecessity of computing conditional distributions of exponent measures.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 18:52:50 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Br\u00fcck", "Florian", ""]]}, {"id": "2107.04668", "submitter": "Ruda Zhang", "authors": "Ruda Zhang and Simon Mak and David Dunson", "title": "Gaussian Process Subspace Regression for Model Reduction", "comments": "20 pages, 4 figures; with supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace-valued functions arise in a wide range of problems, including\nparametric reduced order modeling (PROM). In PROM, each parameter point can be\nassociated with a subspace, which is used for Petrov-Galerkin projections of\nlarge system matrices. Previous efforts to approximate such functions use\ninterpolations on manifolds, which can be inaccurate and slow. To tackle this,\nwe propose a novel Bayesian nonparametric model for subspace prediction: the\nGaussian Process Subspace regression (GPS) model. This method is extrinsic and\nintrinsic at the same time: with multivariate Gaussian distributions on the\nEuclidean space, it induces a joint probability model on the Grassmann\nmanifold, the set of fixed-dimensional subspaces. The GPS adopts a simple yet\ngeneral correlation structure, and a principled approach for model selection.\nIts predictive distribution admits an analytical form, which allows for\nefficient subspace prediction over the parameter space. For PROM, the GPS\nprovides a probabilistic prediction at a new parameter point that retains the\naccuracy of local reduced models, at a computational complexity that does not\ndepend on system dimension, and thus is suitable for online computation. We\ngive four numerical examples to compare our method to subspace interpolation,\nas well as two methods that interpolate local reduced models. Overall, GPS is\nthe most data efficient, more computationally efficient than subspace\ninterpolation, and gives smooth predictions with uncertainty quantification.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 20:41:23 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Ruda", ""], ["Mak", "Simon", ""], ["Dunson", "David", ""]]}, {"id": "2107.04839", "submitter": "Xin Chen", "authors": "Xin Chen, Rui Song, Jiajia Zhang, Swann Arp Adams, Liuquan Sun, Wenbin\n  Lu", "title": "On Estimating Optimal Regime for Treatment Initiation Time Based on\n  Restricted Mean Residual Lifetime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When to initiate treatment on patients is an important problem in many\nmedical studies such as AIDS and cancer. In this article, we formulate the\ntreatment initiation time problem for time-to-event data and propose an optimal\nindividualized regime that determines the best treatment initiation time for\nindividual patients based on their characteristics. Different from existing\noptimal treatment regimes where treatments are undertaken at a pre-specified\ntime, here new challenges arise from the complicated missing mechanisms in\ntreatment initiation time data and the continuous treatment rule in terms of\ninitiation time. To tackle these challenges, we propose to use restricted mean\nresidual lifetime as a value function to evaluate the performance of different\ntreatment initiation regimes, and develop a nonparametric estimator for the\nvalue function, which is consistent even when treatment initiation times are\nnot completely observable and their distribution is unknown. We also establish\nthe asymptotic properties of the resulting estimator in the decision rule and\nits associated value function estimator. In particular, the asymptotic\ndistribution of the estimated value function is nonstandard, which follows a\nweighted chi-squared distribution. The finite-sample performance of the\nproposed method is evaluated by simulation studies and is further illustrated\nwith an application to a breast cancer data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 13:52:49 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 14:27:58 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Chen", "Xin", ""], ["Song", "Rui", ""], ["Zhang", "Jiajia", ""], ["Adams", "Swann Arp", ""], ["Sun", "Liuquan", ""], ["Lu", "Wenbin", ""]]}, {"id": "2107.04845", "submitter": "Marko Obradovi\\'c", "authors": "Wiktor Ejsmont, Bojana Milo\\v{s}evi\\'c, Marko Obradovi\\'c", "title": "A test for normality and independence based on characteristic function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we prove a generalization of the Ejsmont characterization of\nthe multivariate normal distribution. Based on it, we propose a new test for\nindependence and normality. The test uses an integral of the squared modulus of\nthe difference between the product of empirical characteristic functions and\nsome constant. Special attention is given to the case of testing univariate\nnormality in which we derive the test statistic explicitly in terms of Bessel\nfunction, and the case of testing bivariate normality and independence. The\ntests show quality performance in comparison to some popular powerful\ncompetitors.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 14:13:14 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ejsmont", "Wiktor", ""], ["Milo\u0161evi\u0107", "Bojana", ""], ["Obradovi\u0107", "Marko", ""]]}, {"id": "2107.04873", "submitter": "Salil Koner", "authors": "Salil Koner, Jonathan P Williams", "title": "The EAS approach to variable selection for multivariate response data in\n  high-dimensional settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend the epsilon admissible subsets (EAS) model selection\napproach, from its original construction in the high-dimensional linear\nregression setting, to an EAS framework for performing group variable selection\nin the high-dimensional multivariate regression setting. Assuming a\nmatrix-Normal linear model we show that the EAS strategy is asymptotically\nconsistent if there exists a sparse, true data generating set of predictors.\nNonetheless, our EAS strategy is designed to estimate a posterior-like,\ngeneralized fiducial distribution over a parsimonious class of models in the\nsetting of correlated predictors and/or in the absence of a sparsity\nassumption. The effectiveness of our approach, to this end, is demonstrated\nempirically in simulation studies, and is compared to other state-of-the-art\nmodel/variable selection procedures.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 16:47:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Koner", "Salil", ""], ["Williams", "Jonathan P", ""]]}, {"id": "2107.04923", "submitter": "Weixing Song", "authors": "Kanwal Ayub and Weixing Song", "title": "Extrapolation Estimation for Parametric Regression with Normal\n  Measurement Error", "comments": "30 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  For the general parametric regression models with covariates contaminated\nwith normal measurement errors, this paper proposes an accelerated version of\nthe classical simulation extrapolation algorithm to estimate the unknown\nparameters in the regression function. By applying the conditional expectation\ndirectly to the target function, the proposed algorithm successfully removes\nthe simulation step, by generating an estimation equation either for immediate\nuse or for extrapolating, thus significantly reducing the computational time.\nLarge sample properties of the resulting estimator, including the consistency\nand the asymptotic normality, are thoroughly discussed. Potential wide\napplications of the proposed estimation procedure are illustrated by examples,\nsimulation studies, as well as a real data analysis.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 22:41:11 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ayub", "Kanwal", ""], ["Song", "Weixing", ""]]}, {"id": "2107.04946", "submitter": "Javier Espinosa", "authors": "Javier Espinosa and Christian Hennig", "title": "Inference for the proportional odds cumulative logit model with\n  monotonicity constraints for ordinal predictors and ordinal response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proportional odds cumulative logit model (POCLM) is a standard regression\nmodel for an ordinal response. Ordinality of predictors can be incorporated by\nmonotonicity constraints for the corresponding parameters. It is shown that\nestimators defined by optimization, such as maximum likelihood estimators, for\nan unconstrained model and for parameters in the interior set of the parameter\nspace of a constrained model are asymptotically equivalent. This is used in\norder to derive asymptotic confidence regions and tests for the constrained\nmodel, involving simple modifications for finite samples. The finite sample\ncoverage probability of the confidence regions is investigated by simulation.\nTests concern the effect of individual variables, monotonicity, and a specified\nmonotonicity direction. The methodology is applied on real data related to the\nassessment of school performance.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 02:37:05 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Espinosa", "Javier", ""], ["Hennig", "Christian", ""]]}, {"id": "2107.04994", "submitter": "Junho Yang", "authors": "Suhasini Subba Rao, Junho Yang", "title": "A prediction perspective on the Wiener-Hopf equations for discrete time\n  series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wiener-Hopf equations are a Toeplitz system of linear equations that have\nseveral applications in time series. These include the update and prediction\nstep of the stationary Kalman filter equations and the prediction of bivariate\ntime series. The Wiener-Hopf technique is the classical tool for solving the\nequations, and is based on a comparison of coefficients in a Fourier series\nexpansion. The purpose of this note is to revisit the (discrete) Wiener-Hopf\nequations and obtain an alternative expression for the solution that is more in\nthe spirit of time series analysis. Specifically, we propose a solution to the\nWiener-Hopf equations that combines linear prediction with deconvolution.\n  The solution of the Wiener-Hopf equations requires one to obtain the spectral\nfactorization of the underlying spectral density function. For general spectral\ndensity functions this is infeasible. Therefore, it is usually assumed that the\nspectral density is rational, which allows one to obtain a computationally\ntractable solution. This leads to an approximation error when the underlying\nspectral density is not a rational function. We use the proposed solution\ntogether with Baxter's inequality to derive an error bound for the rational\nspectral density approximation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 08:49:37 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Rao", "Suhasini Subba", ""], ["Yang", "Junho", ""]]}, {"id": "2107.05072", "submitter": "Emilie Eliseussen {\\O}degaard", "authors": "Emilie Eliseussen, Thomas Fleischer and Valeria Vitelli", "title": "Rank-based Bayesian variable selection for genome-wide transcriptomic\n  analyses", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variable selection is crucial in high-dimensional omics-based analyses, since\nit is biologically reasonable to assume only a subset of non-noisy features\ncontributes to the data structures. However, the task is particularly hard in\nan unsupervised setting, and a priori ad hoc variable selection is still a very\nfrequent approach, despite the evident drawbacks and lack of reproducibility.\nWe propose a Bayesian variable selection approach for rank-based transcriptomic\nanalysis. Making use of data rankings instead of the actual continuous\nmeasurements increases the robustness of conclusions when compared to classical\nstatistical methods, and embedding variable selection into the inferential\ntasks allows complete reproducibility. Specifically, we develop a novel\nextension of the Bayesian Mallows model for variable selection that allows for\na full probabilistic analysis, leading to coherent quantification of\nuncertainties. We test our approach on simulated data using several data\ngenerating procedures, demonstrating the versatility and robustness of the\nmethod under different scenarios. We then use the novel approach to analyse\ngenome-wide RNAseq gene expression data from ovarian cancer samples: several\ngenes that affect cancer development are correctly detected in a completely\nunsupervised fashion, showing the method usefulness in the context of signature\ndiscovery for cancer genomics. Moreover, the possibility to also perform\nuncertainty quantification plays a key role in the subsequent biological\ninvestigation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 15:33:18 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Eliseussen", "Emilie", ""], ["Fleischer", "Thomas", ""], ["Vitelli", "Valeria", ""]]}, {"id": "2107.05414", "submitter": "Maria De Iorio", "authors": "Abhinav Natarajan, Maria De Iorio, Andreas Heinecke, Emanuel Mayer and\n  Simon Glenn", "title": "Cohesion and Repulsion in Bayesian Distance Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering in high-dimensions poses many statistical challenges. While\ntraditional distance-based clustering methods are computationally feasible,\nthey lack probabilistic interpretation and rely on heuristics for estimation of\nthe number of clusters. On the other hand, probabilistic model-based clustering\ntechniques often fail to scale and devising algorithms that are able to\neffectively explore the posterior space is an open problem. Based on recent\ndevelopments in Bayesian distance-based clustering, we propose a hybrid\nsolution that entails defining a likelihood on pairwise distances between\nobservations. The novelty of the approach consists in including both cohesion\nand repulsion terms in the likelihood, which allows for cluster\nidentifiability. This implies that clusters are composed of objects which have\nsmall \"dissimilarities\" among themselves (cohesion) and similar dissimilarities\nto observations in other clusters (repulsion). We show how this modelling\nstrategy has interesting connection with existing proposals in the literature\nas well as a decision-theoretic interpretation. The proposed method is\ncomputationally efficient and applicable to a wide variety of scenarios. We\ndemonstrate the approach in a simulation study and an application in digital\nnumismatics.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 13:34:28 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Natarajan", "Abhinav", ""], ["De Iorio", "Maria", ""], ["Heinecke", "Andreas", ""], ["Mayer", "Emanuel", ""], ["Glenn", "Simon", ""]]}, {"id": "2107.05427", "submitter": "Moritz Marbach", "authors": "Moritz Marbach", "title": "Choosing Imputation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imputing missing values is an important preprocessing step in data analysis,\nbut the literature offers little guidance on how to choose between different\nimputation models. This letter suggests adopting the imputation model that\ngenerates a density of imputed values most similar to those of the observed\nvalues for an incomplete variable after balancing all other covariates. We\nrecommend stable balancing weights as a practical approach to balance\ncovariates whose distribution is expected to differ if the values are not\nmissing completely at random. After balancing, discrepancy statistics can be\nused to compare the density of imputed and observed values. We illustrate the\napplication of the suggested approach using simulated and real-world survey\ndata from the American National Election Study, comparing popular imputation\napproaches including random forests, hot-deck, predictive mean matching, and\nmultivariate normal imputation. An R package implementing the suggested\napproach accompanies this letter.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 13:48:57 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Marbach", "Moritz", ""]]}, {"id": "2107.05559", "submitter": "Zhengfei Yu", "authors": "Jun Ma, Vadim Marmer and Zhengfei Yu", "title": "Inference on Individual Treatment Effects in Nonseparable Triangular\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nonseparable triangular models with a binary endogenous treatment and a\nbinary instrumental variable, Vuong and Xu (2017) show that the individual\ntreatment effects (ITEs) are identifiable. Feng, Vuong and Xu (2019) show that\na kernel density estimator that uses nonparametrically estimated ITEs as\nobservations is uniformly consistent for the density of the ITE. In this paper,\nwe establish the asymptotic normality of the density estimator of Feng, Vuong\nand Xu (2019) and show that despite their faster rate of convergence, ITEs'\nestimation errors have a non-negligible effect on the asymptotic distribution\nof the density estimator. We propose asymptotically valid standard errors for\nthe density of the ITE that account for estimated ITEs as well as bias\ncorrection. Furthermore, we develop uniform confidence bands for the density of\nthe ITE using nonparametric or jackknife multiplier bootstrap critical values.\nOur uniform confidence bands have correct coverage probabilities asymptotically\nwith polynomial error rates and can be used for inference on the shape of the\nITE's distribution.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 16:29:42 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 15:20:14 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ma", "Jun", ""], ["Marmer", "Vadim", ""], ["Yu", "Zhengfei", ""]]}, {"id": "2107.05719", "submitter": "Shengjia Zhao", "authors": "Shengjia Zhao, Michael P. Kim, Roshni Sahoo, Tengyu Ma, Stefano Ermon", "title": "Calibrating Predictions to Decisions: A Novel Approach to Multi-Class\n  Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When facing uncertainty, decision-makers want predictions they can trust. A\nmachine learning provider can convey confidence to decision-makers by\nguaranteeing their predictions are distribution calibrated -- amongst the\ninputs that receive a predicted class probabilities vector $q$, the actual\ndistribution over classes is $q$. For multi-class prediction problems, however,\nachieving distribution calibration tends to be infeasible, requiring sample\ncomplexity exponential in the number of classes $C$. In this work, we introduce\na new notion -- \\emph{decision calibration} -- that requires the predicted\ndistribution and true distribution to be ``indistinguishable'' to a set of\ndownstream decision-makers. When all possible decision makers are under\nconsideration, decision calibration is the same as distribution calibration.\nHowever, when we only consider decision makers choosing between a bounded\nnumber of actions (e.g. polynomial in $C$), our main result shows that\ndecisions calibration becomes feasible -- we design a recalibration algorithm\nthat requires sample complexity polynomial in the number of actions and the\nnumber of classes. We validate our recalibration algorithm empirically:\ncompared to existing methods, decision calibration improves decision-making on\nskin lesion and ImageNet classification with modern neural network predictors.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 20:17:28 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zhao", "Shengjia", ""], ["Kim", "Michael P.", ""], ["Sahoo", "Roshni", ""], ["Ma", "Tengyu", ""], ["Ermon", "Stefano", ""]]}, {"id": "2107.05730", "submitter": "Cody Carroll", "authors": "Cody Carroll and Hans-Georg M\\\"uller", "title": "Latent Transport Models for Multivariate Functional Data", "comments": "52 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multivariate functional data present theoretical and practical complications\nwhich are not found in univariate functional data. One of these is a situation\nwhere the component functions of multivariate functional data are positive and\nare subject to mutual time warping. That is, the component processes exhibit a\nsimilar shape but are subject to systematic phase variation across their time\ndomains. We introduce a novel model for multivariate functional data that\nincorporates such mutual time warping via nonlinear transport functions. This\nmodel allows for meaningful interpretation and is well suited to represent\nfunctional vector data. The proposed approach combines a random amplitude\nfactor for each component with population based registration across the\ncomponents of a multivariate functional data vector and also includes a latent\npopulation function, which corresponds to a common underlying trajectory as\nwell as subject-specific warping component. We also propose estimators for all\ncomponents of the model. The proposed approach not only leads to a novel\nrepresentation for multivariate functional data, but is also useful for\ndownstream analyses such as Fr\\'echet regression. Rates of convergence are\nestablished when curves are fully observed or observed with measurement error.\nThe usefulness of the model, interpretations and practical aspects are\nillustrated in simulations and with application to multivariate human growth\ncurves as well as multivariate environmental pollution data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 20:53:38 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Carroll", "Cody", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "2107.05734", "submitter": "Peter Gilbert", "authors": "Peter B. Gilbert, Youyi Fong, Marco Carone", "title": "Assessment of Immune Correlates of Protection via Controlled Vaccine\n  Efficacy and Controlled Risk", "comments": "25 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Immune correlates of protection (CoPs) are immunologic biomarkers accepted as\na surrogate for an infectious disease clinical endpoint and thus can be used\nfor traditional or provisional vaccine approval. To study CoPs in randomized,\nplacebo-controlled trials, correlates of risk (CoRs) are first assessed in\nvaccine recipients. This analysis does not assess causation, as a CoR may fail\nto be a CoP. We propose a causal CoP analysis that estimates the controlled\nvaccine efficacy curve across biomarker levels $s$, $CVE(s)$, equal to one\nminus the ratio of the controlled-risk curve $r_C(s)$ at $s$ and placebo risk,\nwhere $r_C(s)$ is causal risk if all participants are assigned vaccine and the\nbiomarker is set to $s$. The criterion for a useful CoP is wide variability of\n$CVE(s)$ in $s$. Moreover, estimation of $r_C(s)$ is of interest in itself,\nespecially in studies without a placebo arm. For estimation of $r_C(s)$,\nmeasured confounders can be adjusted for by any regression method that\naccommodates missing biomarkers, to which we add sensitivity analysis to\nquantify robustness of CoP evidence to unmeasured confounding. Application to\ntwo harmonized phase 3 trials supports that 50% neutralizing antibody titer has\nvalue as a controlled vaccine efficacy CoP for virologically confirmed dengue\n(VCD): in CYD14 the point estimate (95% confidence interval) for $CVE(s)$\naccounting for measured confounders and building in conservative margin for\nunmeasured confounding increases from 29.6% (95% CI 3.5 to 45.9) at titer 1:36\nto 78.5% (95% CI 67.9 to 86.8) at titer 1:1200; these estimates are 17.4% (95%\nCI -14.4 to 36.5) and 84.5% (95% CI 79.6 to 89.1) for CYD15.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 21:03:34 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Gilbert", "Peter B.", ""], ["Fong", "Youyi", ""], ["Carone", "Marco", ""]]}, {"id": "2107.05766", "submitter": "Xin Bing", "authors": "Xin Bing and Florentina Bunea and Seth Strimas-Mackey and Marten\n  Wegkamp", "title": "Likelihood estimation of sparse topic distributions in topic models and\n  its applications to Wasserstein document distance calculations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the estimation of high-dimensional, discrete, possibly\nsparse, mixture models in topic models. The data consists of observed\nmultinomial counts of $p$ words across $n$ independent documents. In topic\nmodels, the $p\\times n$ expected word frequency matrix is assumed to be\nfactorized as a $p\\times K$ word-topic matrix $A$ and a $K\\times n$\ntopic-document matrix $T$. Since columns of both matrices represent conditional\nprobabilities belonging to probability simplices, columns of $A$ are viewed as\n$p$-dimensional mixture components that are common to all documents while\ncolumns of $T$ are viewed as the $K$-dimensional mixture weights that are\ndocument specific and are allowed to be sparse. The main interest is to provide\nsharp, finite sample, $\\ell_1$-norm convergence rates for estimators of the\nmixture weights $T$ when $A$ is either known or unknown. For known $A$, we\nsuggest MLE estimation of $T$. Our non-standard analysis of the MLE not only\nestablishes its $\\ell_1$ convergence rate, but reveals a remarkable property:\nthe MLE, with no extra regularization, can be exactly sparse and contain the\ntrue zero pattern of $T$. We further show that the MLE is both minimax optimal\nand adaptive to the unknown sparsity in a large class of sparse topic\ndistributions. When $A$ is unknown, we estimate $T$ by optimizing the\nlikelihood function corresponding to a plug in, generic, estimator $\\hat{A}$ of\n$A$. For any estimator $\\hat{A}$ that satisfies carefully detailed conditions\nfor proximity to $A$, the resulting estimator of $T$ is shown to retain the\nproperties established for the MLE. The ambient dimensions $K$ and $p$ are\nallowed to grow with the sample sizes. Our application is to the estimation of\n1-Wasserstein distances between document generating distributions. We propose,\nestimate and analyze new 1-Wasserstein distances between two probabilistic\ndocument representations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 22:22:32 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bing", "Xin", ""], ["Bunea", "Florentina", ""], ["Strimas-Mackey", "Seth", ""], ["Wegkamp", "Marten", ""]]}, {"id": "2107.05805", "submitter": "Adam Peterson", "authors": "Adam Peterson and Emma Sanchez-Vaznaugh and Brisa Sanchez", "title": "Heterogeneous Effects in the Built Environment", "comments": "13 pages, 3 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach to estimate distance-dependent heterogeneous\nassociations between point-referenced exposures to built environment\ncharacteristics and health outcomes. By estimating associations that depend\nnon-linearly on distance between subjects and point-referenced exposures, this\nmethod addresses the modifiable area-unit problem that is pervasive in the\nbuilt environment literature. Additionally, by estimating heterogeneous\neffects, the method also addresses the uncertain geographic context problem.\nThe key innovation of our method is to combine ideas from the non-parametric\nfunction estimation literature and the Bayesian Dirichlet process literature.\nThe former is used to estimate nonlinear associations between subject's\noutcomes and proximate built environment features, and the latter identifies\nclusters within the population that have different effects. We study this\nmethod in simulations and apply our model to study heterogeneity in the\nassociation between fast food restaurant availability and weight status of\nchildren attending schools in Los Angeles, California.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 01:46:16 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Peterson", "Adam", ""], ["Sanchez-Vaznaugh", "Emma", ""], ["Sanchez", "Brisa", ""]]}, {"id": "2107.05915", "submitter": "Chaonan Jiang", "authors": "Chaonan Jiang, Davide La Vecchia and Riccardo Rastelli", "title": "Graphical Laplace-approximated maximum likelihood estimation:\n  approximated likelihood inference for network data analysis", "comments": "This paper still needs large modifications in the theory and\n  applications. I will submit it when it is ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive Laplace-approximated maximum likelihood estimators (GLAMLEs) of\nparameters in our Graph Generalized Linear Latent Variable Models. Then, we\nstudy the statistical properties of GLAMLEs when the number of nodes $n_V$ and\nthe observed times of a graph denoted by $K$ diverge to infinity. Finally, we\ndisplay the estimation results in a Monte Carlo simulation considering\ndifferent numbers of latent variables. Besides, we make a comparison between\nLaplace and variational approximations for inference of our model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 08:34:16 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 15:24:47 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Jiang", "Chaonan", ""], ["La Vecchia", "Davide", ""], ["Rastelli", "Riccardo", ""]]}, {"id": "2107.05933", "submitter": "Lingsong Meng", "authors": "Lingsong Meng and Zhiguang Huo", "title": "Outcome-guided Bayesian Clustering for Disease Subtype Discovery Using\n  High-dimensional Transcriptomic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of disease subtypes is an essential step for developing\nprecision medicine, and disease subtyping via omics data has become a popular\napproach. While promising, subtypes obtained from conventional approaches may\nnot be necessarily associated with clinical outcomes. The collection of rich\nclinical data along with omics data has provided an unprecedented opportunity\nto facilitate the disease subtyping process and to discovery clinically\nmeaningful disease subtypes. Thus, we developed an outcome-guided Bayesian\nclustering (GuidedBayesianClustering) method to fully integrate the clinical\ndata and the high-dimensional omics data. A Gaussian mixed model framework was\napplied to perform sample clustering; a spike-and-slab prior was utilized to\nperform gene selection; a mixture model prior was employed to incorporate the\nguidance from a clinical outcome variable; and a decision framework was adopted\nto infer the false discovery rate of the selected genes. We deployed conjugate\npriors to facilitate efficient Gibbs sampling. Our proposed full Bayesian\nmethod is capable of simultaneously (i) obtaining sample clustering (disease\nsubtype discovery); (ii) performing feature selection (select genes related to\nthe disease subtype); and (iii) utilizing clinical outcome variable to guide\nthe disease subtype discovery. The superior performance of the\nGuidedBayesianClustering was demonstrated through simulations and applications\nof breast cancer expression data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 09:10:15 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Meng", "Lingsong", ""], ["Huo", "Zhiguang", ""]]}, {"id": "2107.05956", "submitter": "Sourabh Bhattacharya", "authors": "Sourabh Bhattacharya", "title": "IID Sampling from Intractable Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel methodology for drawing iid realizations from any target\ndistribution on the Euclidean space with arbitrary dimension. No assumption of\ncompact support is necessary for the validity of our theory and method. Our\nidea is to construct an appropriate infinite sequence of concentric closed\nellipsoids, represent the target distribution as an infinite mixture on the\ncentral ellipsoid and the ellipsoidal annuli, and to construct efficient\nperfect samplers for the mixture components.\n  In contrast with most of the existing works on perfect sampling, ours is not\nonly a theoretically valid method, it is practically applicable to all target\ndistributions on any dimensional Euclidean space and very much amenable to\nparallel computation. We validate the practicality and usefulness of our\nmethodology by generating 10000 iid realizations from the standard\ndistributions such as normal, Student's t with 5 degrees of freedom and Cauchy,\nfor dimensions d = 1, 5, 10, 50, 100, as well as from a 50-dimensional mixture\nnormal distribution. The implementation time in all the cases are very\nreasonable, and often less than a minute in our parallel implementation. The\nresults turned out to be highly accurate.\n  We also apply our method to draw 10000 iid realizations from the posterior\ndistributions associated with the well-known Challenger data, a Salmonella data\nand the 160-dimensional challenging spatial example of the radionuclide count\ndata on Rongelap Island. Again, we are able to obtain quite encouraging results\nwith very reasonable computing time.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 09:58:02 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bhattacharya", "Sourabh", ""]]}, {"id": "2107.06006", "submitter": "Julien Bect", "authors": "S\\'ebastien Petit (L2S, GdR MASCOT-NUM), Julien Bect (L2S, GdR\n  MASCOT-NUM), Paul Feliot, Emmanuel Vazquez (L2S, GdR MASCOT-NUM)", "title": "Gaussian process interpolation: the choice of the family of models is\n  more important than that of the selection criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article revisits the fundamental problem of parameter selection for\nGaussian process interpolation. By choosing the mean and the covariance\nfunctions of a Gaussian process within parametric families, the user obtains a\nfamily of Bayesian procedures to perform predictions about the unknown\nfunction, and must choose a member of the family that will hopefully provide\ngood predictive performances. We base our study on the general concept of\nscoring rules, which provides an effective framework for building leave-one-out\nselection and validation criteria, and a notion of extended likelihood criteria\nbased on an idea proposed by Fasshauer and co-authors in 2009, which makes it\npossible to recover standard selection criteria such as, for instance, the\ngeneralized cross-validation criterion. Under this setting, we empirically show\non several test problems of the literature that the choice of an appropriate\nfamily of models is often more important than the choice of a particular\nselection criterion (e.g., the likelihood versus a leave-one-out selection\ncriterion). Moreover, our numerical results show that the regularity parameter\nof a Mat{\\'e}rn covariance can be selected effectively by most selection\ncriteria.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 11:57:56 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Petit", "S\u00e9bastien", "", "L2S, GdR MASCOT-NUM"], ["Bect", "Julien", "", "L2S, GdR\n  MASCOT-NUM"], ["Feliot", "Paul", "", "L2S, GdR MASCOT-NUM"], ["Vazquez", "Emmanuel", "", "L2S, GdR MASCOT-NUM"]]}, {"id": "2107.06040", "submitter": "Mingya Long", "authors": "Mingya Long, Zhengbang Li, Wei Zhang, and Qizhai Li*", "title": "Cauchy Combination Test for Sparse Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Aggregating multiple effects is often encountered in large-scale data\nanalysis where the fraction of significant effects is generally small. Many\nexisting methods cannot handle it effectively because of lack of computational\naccuracy for small p-values. The Cauchy combination test (abbreviated as CCT) (\nJ Am Statist Assoc, 2020, 115(529):393-402) is a powerful and computational\neffective test to aggregate individual $p$-values under arbitrary correlation\nstructures. This work revisits CCT and shows three key contributions including\nthat (i) the tail probability of CCT can be well approximated by a standard\nCauchy distribution under much more relaxed conditions placed on individual\np-values instead of the original test statistics; (ii) the relaxation\nconditions are shown to be satisfied for many popular copulas formulating\nbivariate distributions; (iii) the power of CCT is no less than that of the\nminimum-type test as the number of tests goes to infinity with some regular\nconditions. These results further broaden the theories and applications of CCT.\nThe simulation results verify the theoretic results and the performance of CCT\nis further evaluated with data from a prostate cancer study.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 12:50:09 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Long", "Mingya", ""], ["Li", "Zhengbang", ""], ["Zhang", "Wei", ""], ["Li*", "Qizhai", ""]]}, {"id": "2107.06089", "submitter": "Zeng-Hua Lu", "authors": "Giuseppe Cavaliere, Zeng-Hua Lu, Anders Rahbek, Yuhong Yang", "title": "MinP Score Tests with an Inequality Constrained Parameter Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Score tests have the advantage of requiring estimation alone of the model\nrestricted by the null hypothesis, which often is much simpler than models\ndefined under the alternative hypothesis. This is typically so when the\nalternative hypothesis involves inequality constraints. However, existing score\ntests address only jointly testing all parameters of interest; a leading\nexample is testing all ARCH parameters or variances of random coefficients\nbeing zero or not. In such testing problems rejection of the null hypothesis\ndoes not provide evidence on rejection of specific elements of parameter of\ninterest. This paper proposes a class of one-sided score tests for testing a\nmodel parameter that is subject to inequality constraints. Proposed tests are\nconstructed based on the minimum of a set of $p$-values. The minimand includes\nthe $p$-values for testing individual elements of parameter of interest using\nindividual scores. It may be extended to include a $p$-value of existing score\ntests. We show that our tests perform better than/or perform as good as\nexisting score tests in terms of joint testing, and has furthermore the added\nbenefit of allowing for simultaneously testing individual elements of parameter\nof interest. The added benefit is appealing in the sense that it can identify a\nmodel without estimating it. We illustrate our tests in linear regression\nmodels, ARCH and random coefficient models. A detailed simulation study is\nprovided to examine the finite sample performance of the proposed tests and we\nfind that our tests perform well as expected.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 13:42:47 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Cavaliere", "Giuseppe", ""], ["Lu", "Zeng-Hua", ""], ["Rahbek", "Anders", ""], ["Yang", "Yuhong", ""]]}, {"id": "2107.06093", "submitter": "Eric Yanchenko", "authors": "Eric Yanchenko and Srijan Sengupta", "title": "A model-agnostic hypothesis test for community structure and homophily\n  in networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks continue to be of great interest to statisticians, with an emphasis\non community detection. Less work, however, has addressed this question: given\nsome network, does it exhibit meaningful community structure? We propose to\nanswer this question in a principled manner by framing it as a statistical\nhypothesis in terms of a formal and model-agnostic homophily metric. Homophily\nis a well-studied network property where intra-community edges are more likely\nthan between-community edges. We use the homophily metric to identify and\ndistinguish between three concepts: nominal, collateral, and intrinsic\nhomophily. We propose a simple and interpretable test statistic leveraging this\nhomophily parameter and formulate both asymptotic and bootstrap-based rejection\nthresholds. We prove its asymptotic properties and demonstrate it outperforms\nbenchmark methods on both simulated and real world data. Furthermore, the\nproposed method yields rich, provocative insights on classic data sets; namely,\nthat meany well-studied networks do not actually have intrinsic homophily.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 21:17:06 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Yanchenko", "Eric", ""], ["Sengupta", "Srijan", ""]]}, {"id": "2107.06096", "submitter": "Thayer Alshaabi", "authors": "Yi Li, Asieh Ahani, Haimao Zhan, Kevin Foley, Thayer Alshaabi, Kelsey\n  Linnell, Peter Sheridan Dodds, Christopher M. Danforth, Adam Fox", "title": "Blending search queries with social media data to improve forecasts of\n  economic indicators", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forecasting of political, economic, and public health indicators using\ninternet activity has demonstrated mixed results. For example, while some\nmeasures of explicitly surveyed public opinion correlate well with social media\nproxies, the opportunity for profitable investment strategies to be driven\nsolely by sentiment extracted from social media appears to have expired.\nNevertheless, the internet's space of potentially predictive input signals is\ncombinatorially vast and will continue to invite careful exploration. Here, we\ncombine unemployment related search data from Google Trends with economic\nlanguage on Twitter to attempt to nowcast and forecast: 1. State and national\nunemployment claims for the US, and 2. Consumer confidence in G7 countries.\nBuilding off of a recently developed search-query-based model, we show that\nincorporating Twitter data improves forecasting of unemployment claims, while\nthe original method remains marginally better at nowcasting. Enriching the\ninput signal with temporal statistical features (e.g., moving average and rate\nof change) further reduces errors, and improves the predictive utility of the\nproposed method when applied to other economic indices, such as consumer\nconfidence.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 19:53:51 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Li", "Yi", ""], ["Ahani", "Asieh", ""], ["Zhan", "Haimao", ""], ["Foley", "Kevin", ""], ["Alshaabi", "Thayer", ""], ["Linnell", "Kelsey", ""], ["Dodds", "Peter Sheridan", ""], ["Danforth", "Christopher M.", ""], ["Fox", "Adam", ""]]}, {"id": "2107.06124", "submitter": "Oliver Dukes", "authors": "Oliver Dukes, Stijn Vansteelandt, David Whitney", "title": "On doubly robust inference for double machine learning", "comments": "38 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to concerns about parametric model misspecification, there is interest in\nusing machine learning to adjust for confounding when evaluating the causal\neffect of an exposure on an outcome. Unfortunately, exposure effect estimators\nthat rely on machine learning predictions are generally subject to so-called\nplug-in bias, which can render naive p-values and confidence intervals invalid.\nProgress has been made via proposals like targeted maximum likelihood\nestimation and more recently double machine learning, which rely on learning\nthe conditional mean of both the outcome and exposure. Valid inference can then\nbe obtained so long as both predictions converge (sufficiently fast) to the\ntruth. Focusing on partially linear regression models, we show that a specific\nimplementation of the machine learning techniques can yield exposure effect\nestimators that have small bias even when one of the first-stage predictions\ndoes not converge to the truth. The resulting tests and confidence intervals\nare doubly robust. We also show that the proposed estimators may fail to be\nregular when only one nuisance parameter is consistently estimated;\nnevertheless, we observe in simulation studies that our proposal leads to\nreduced bias and improved confidence interval coverage in moderate samples.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 14:24:33 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Dukes", "Oliver", ""], ["Vansteelandt", "Stijn", ""], ["Whitney", "David", ""]]}, {"id": "2107.06238", "submitter": "Ting Ye", "authors": "Ting Ye and Zhonghua Liu and Baoluo Sun and Eric Tchetgen Tchetgen", "title": "GENIUS-MAWII: For Robust Mendelian Randomization with Many Weak Invalid\n  Instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian randomization (MR) has become a popular approach to study causal\neffects by using genetic variants as instrumental variables. We propose a new\nMR method, GENIUS-MAWII, which simultaneously addresses the two salient\nphenomena that adversely affect MR analyses: many weak instruments and\nwidespread horizontal pleiotropy. Similar to MR GENIUS\n\\citep{Tchetgen2019_GENIUS}, we achieve identification of the treatment effect\nby leveraging heteroscedasticity of the exposure. We then derive the class of\ninfluence functions of the treatment effect, based on which, we construct a\ncontinuous updating estimator and establish its consistency and asymptotic\nnormality under a many weak invalid instruments asymptotic regime by developing\nnovel semiparametric theory. We also provide a measure of weak identification\nand graphical diagnostic tool. We demonstrate in simulations that GENIUS-MAWII\nhas clear advantages in the presence of directional or correlated horizontal\npleiotropy compared to other methods. We apply our method to study the effect\nof body mass index on systolic blood pressure using UK Biobank.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:50:32 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ye", "Ting", ""], ["Liu", "Zhonghua", ""], ["Sun", "Baoluo", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "2107.06398", "submitter": "Tim Morris", "authors": "Tim P. Morris, A. Sarah Walker, Elizabeth J. Williamson, Ian R. White", "title": "Planning a method for covariate adjustment in individually-randomised\n  trials: a practical guide", "comments": "1 figure, 5 main tables, 2 appendices, 2 appendix tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: It has long been advised to account for baseline covariates in\nthe analysis of confirmatory randomised trials, with the main statistical\njustifications being that this increases power and, when a randomisation scheme\nbalanced covariates, permits a valid estimate of experimental error. There are\nvarious methods available to account for covariates. Methods: We consider how,\nat the point of writing a statistical analysis plan, to choose between three\nbroad approaches: direct adjustment, standardisation and\ninverse-probability-of-treatment weighting (IPTW), which are in our view the\nmost promising methods. Using the GetTested trial, a randomised trial designed\nto assess the effectiveness of an electonic STI (sexually transmitted\ninfection) testing and results service, we illustrate how a method might be\nchosen in advance and show some of the anticipated issues in action. Results:\nThe choice of approach is not straightforward, particularly with models for\nbinary outcome measures, where we focus most of our attention. We compare the\nproperties of the three broad approaches in terms of the quantity they target\n(estimand), how a method performs under model misspecification, convergence\nissues, handling designed balance, precision of estimators, estimation of\nstandard errors, and finally clarify some issues around handling of missing\ndata. Conclusions: We conclude that no single approach is always best and\nexplain why the choice will depend on the trial context but encourage trialists\nto consider the three methods more routinely.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 21:14:31 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Morris", "Tim P.", ""], ["Walker", "A. Sarah", ""], ["Williamson", "Elizabeth J.", ""], ["White", "Ian R.", ""]]}, {"id": "2107.06428", "submitter": "Brian Trippe", "authors": "Brian L. Trippe, Hilary K. Finucane, Tamara Broderick", "title": "For high-dimensional hierarchical models, consider exchangeability of\n  effects across covariates instead of across datasets", "comments": "10 pages plus supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hierarchical Bayesian methods enable information sharing across multiple\nrelated regression problems. While standard practice is to model regression\nparameters (effects) as (1) exchangeable across datasets and (2) correlated to\ndiffering degrees across covariates, we show that this approach exhibits poor\nstatistical performance when the number of covariates exceeds the number of\ndatasets. For instance, in statistical genetics, we might regress dozens of\ntraits (defining datasets) for thousands of individuals (responses) on up to\nmillions of genetic variants (covariates). When an analyst has more covariates\nthan datasets, we argue that it is often more natural to instead model effects\nas (1) exchangeable across covariates and (2) correlated to differing degrees\nacross datasets. To this end, we propose a hierarchical model expressing our\nalternative perspective. We devise an empirical Bayes estimator for learning\nthe degree of correlation between datasets. We develop theory that demonstrates\nthat our method outperforms the classic approach when the number of covariates\ndominates the number of datasets, and corroborate this result empirically on\nseveral high-dimensional multiple regression and classification problems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 23:23:06 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Trippe", "Brian L.", ""], ["Finucane", "Hilary K.", ""], ["Broderick", "Tamara", ""]]}, {"id": "2107.06436", "submitter": "Arkaprava Roy", "authors": "Arkaprava Roy, Abhra Sarkar", "title": "Bayesian Semiparametric Multivariate Density Deconvolution via\n  Stochastic Rotation of Replicates", "comments": "arXiv admin note: text overlap with arXiv:1912.05084", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multivariate density deconvolution where the\ndistribution of a random vector needs to be estimated from replicates\ncontaminated with conditionally heteroscedastic measurement errors. We propose\na conceptually straightforward yet fundamentally novel and highly robust\napproach to multivariate density deconvolution by stochastically rotating the\nreplicates toward the corresponding true latent values. We also address the\nadditionally significantly challenging problem of accommodating conditionally\nheteroscedastic measurement errors in this newly introduced framework. We take\na Bayesian route to estimation and inference, implemented via an efficient\nMarkov chain Monte Carlo algorithm, appropriately accommodating uncertainty in\nall aspects of our analysis. Asymptotic convergence guarantees for the method\nare also established. We illustrate the method's empirical efficacy through\nsimulation experiments and its practical utility in estimating the long-term\njoint average intakes of different dietary components from their measurement\nerror contaminated 24-hour dietary recalls.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 00:39:43 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Roy", "Arkaprava", ""], ["Sarkar", "Abhra", ""]]}, {"id": "2107.06448", "submitter": "Jae-Kwang Kim", "authors": "Hang J. Kim and Zhonglei Wang and Jae Kwang Kim", "title": "Survey data integration for regression analysis using model calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider regression analysis in the context of data integration. To\ncombine partial information from external sources, we employ the idea of model\ncalibration which introduces a \"working\" reduced model based on the observed\ncovariates. The working reduced model is not necessarily correctly specified\nbut can be a useful device to incorporate the partial information from the\nexternal data. The actual implementation is based on a novel application of the\nempirical likelihood method. The proposed method is particularly attractive for\ncombining information from several sources with different missing patterns. The\nproposed method is applied to a real data example combining survey data from\nKorean National Health and Nutrition Examination Survey and big data from\nNational Health Insurance Sharing Service in Korea.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 01:46:39 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Kim", "Hang J.", ""], ["Wang", "Zhonglei", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "2107.06503", "submitter": "Weixin Yao", "authors": "Bradley Lubich, Daniel Jeske, Weixin Yao", "title": "Method of Moments Confidence Intervals for a Semi-Supervised\n  Two-Component Mixture Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of a distribution of responses from untreated patients and a shift\nof that distribution is a useful model for the responses from a group of\ntreated patients. The mixture model accounts for the fact that not all the\npatients in the treated group will respond to the treatment and consequently\ntheir responses follow the same distribution as the responses from untreated\npatients. The treatment effect in this context consists of both the fraction of\nthe treated patients that are responders and the magnitude of the shift in the\ndistribution for the responders. In this paper, we investigate properties of\nthe method of moment estimators for the treatment effect and demonstrate their\nusefulness for obtaining approximate confidence intervals without any\nparametric assumptions about the distribution of responses.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 06:17:54 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Lubich", "Bradley", ""], ["Jeske", "Daniel", ""], ["Yao", "Weixin", ""]]}, {"id": "2107.06539", "submitter": "Xuxue Sun", "authors": "Xuxue Sun, Mingyang Li", "title": "Bayesian Lifetime Regression with Multi-type Group-shared Latent\n  Heterogeneity", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Products manufactured from the same batch or utilized in the same region\noften exhibit correlated lifetime observations due to the latent heterogeneity\ncaused by the influence of shared but unobserved covariates. The unavailable\ngroup-shared covariates involve multiple different types (e.g., discrete,\ncontinuous, or mixed-type) and induce different structures of indispensable\ngroup-shared latent heterogeneity. Without carefully capturing such latent\nheterogeneity, the lifetime modeling accuracy will be significantly undermined.\nIn this work, we propose a generic Bayesian lifetime modeling approach by\ncomprehensively investigating the structures of group-shared latent\nheterogeneity caused by different types of group-shared unobserved covariates.\nThe proposed approach is flexible to characterize multi-type group-shared\nlatent heterogeneity in lifetime data. Besides, it can handle the case of lack\nof group membership information and address the issue of limited sample size.\nBayesian sampling algorithm with data augmentation technique is further\ndeveloped to jointly quantify the influence of observed covariates and\ngroup-shared latent heterogeneity. Further, we conduct comprehensive numerical\nstudy to demonstrate the improved performance of proposed modeling approach via\ncomparison with alternative models. We also present empirical study results to\ninvestigate the impacts of group number and sample size per group on estimating\nthe group-shared latent heterogeneity and to demonstrate model identifiability\nof proposed approach for different structures of unobserved group-shared\ncovariates. We also present a real case study to illustrate the effectiveness\nof proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 08:11:30 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Sun", "Xuxue", ""], ["Li", "Mingyang", ""]]}, {"id": "2107.06663", "submitter": "Serena Ng", "authors": "Richard Davis and Serena Ng", "title": "Time Series Estimation of the Dynamic Effects of Disaster-Type Shock", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper provides three results for SVARs under the assumption that the\nprimitive shocks are mutually independent. First, a framework is proposed to\nstudy the dynamic effects of disaster-type shocks with infinite variance. We\nshow that the least squares estimates of the VAR are consistent but have\nnon-standard properties. Second, it is shown that the restrictions imposed on a\nSVAR can be validated by testing independence of the identified shocks. The\ntest can be applied whether the data have fat or thin tails, and to over as\nwell as exactly identified models. Third, the disaster shock is identified as\nthe component with the largest kurtosis, where the mutually independent\ncomponents are estimated using an estimator that is valid even in the presence\nof an infinite variance shock. Two applications are considered. In the first,\nthe independence test is used to shed light on the conflicting evidence\nregarding the role of uncertainty in economic fluctuations. In the second,\ndisaster shocks are shown to have short term economic impact arising mostly\nfrom feedback dynamics.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 12:56:46 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Davis", "Richard", ""], ["Ng", "Serena", ""]]}, {"id": "2107.06675", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "M5 Competition Uncertainty: Overdispersion, distributional forecasting,\n  GAMLSS and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The M5 competition uncertainty track aims for probabilistic forecasting of\nsales of thousands of Walmart retail goods. We show that the M5 competition\ndata faces strong overdispersion and sporadic demand, especially zero demand.\nWe discuss resulting modeling issues concerning adequate probabilistic\nforecasting of such count data processes. Unfortunately, the majority of\npopular prediction methods used in the M5 competition (e.g. lightgbm and\nxgboost GBMs) fails to address the data characteristics due to the considered\nobjective functions. The distributional forecasting provides a suitable\nmodeling approach for to the overcome those problems. The GAMLSS framework\nallows flexible probabilistic forecasting using low dimensional distributions.\nWe illustrate, how the GAMLSS approach can be applied for the M5 competition\ndata by modeling the location and scale parameter of various distributions,\ne.g. the negative binomial distribution. Finally, we discuss software packages\nfor distributional modeling and their drawback, like the R package gamlss with\nits package extensions, and (deep) distributional forecasting libraries such as\nTensorFlow Probability.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 13:05:55 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Ziel", "Florian", ""]]}, {"id": "2107.06815", "submitter": "Thien Le", "authors": "Thien-Minh Le and Ping-Shou Zhong", "title": "High-dimensional Precision Matrix Estimation with a Known Graphical\n  Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A precision matrix is the inverse of a covariance matrix. In this paper, we\nstudy the problem of estimating the precision matrix with a known graphical\nstructure under high-dimensional settings. We propose a simple estimator of the\nprecision matrix based on the connection between the known graphical structure\nand the precision matrix. We obtain the rates of convergence of the proposed\nestimators and derive the asymptotic normality of the proposed estimator in the\nhigh-dimensional setting when the data dimension grows with the sample size.\nNumerical simulations are conducted to demonstrate the performance of the\nproposed method. We also show that the proposed method outperforms some\nexisting methods that do not utilize the graphical structure information.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 18:44:33 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Le", "Thien-Minh", ""], ["Zhong", "Ping-Shou", ""]]}, {"id": "2107.06867", "submitter": "Anthony McIntosh PhD", "authors": "Anthony R McIntosh", "title": "Comparison of Canonical Correlation and Partial Least Squares analyses\n  of simulated and empirical data", "comments": "40 pages, 12 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we compared the general forms of CCA and PLS on three\nsimulated and two empirical datasets, all having large sample sizes. We took\nsuccessively smaller subsamples of these data to evaluate sensitivity,\nreliability, and reproducibility. In null data having no correlation within or\nbetween blocks, both methods showed equivalent false positive rates across\nsample sizes. Both methods also showed equivalent detection in data with weak\nbut reliable effects until sample sizes drop below n=50. In the case of strong\neffects, both methods showed similar performance unless the correlations of\nitems within one data block were high. For PLS, the results were reproducible\nacross sample sizes for strong effects, except at the smallest sample sizes. On\nthe contrary, the reproducibility for CCA declined when the within-block\ncorrelations were high. This was ameliorated if a principal components analysis\n(PCA) was performed and the component scores used to calculate the cross-block\nmatrix. The outcome of our examination gives three messages. First, for data\nwith reasonable within and between block structure, CCA and PLS give comparable\nresults. Second, if there are high correlations within either block, this can\ncompromise the reliability of CCA results. This known issue of CCA can be\nremedied with PCA before cross-block calculation. This, however, assumes that\nthe PCA structure is stable for a given sample. Third, null hypothesis testing\ndoes not guarantee that the results are reproducible, even with large sample\nsizes. This final outcome suggests that both statistical significance and\nreproducibility be assessed for any data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 17:36:32 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["McIntosh", "Anthony R", ""]]}, {"id": "2107.06939", "submitter": "Shihao Wu", "authors": "Ziwei Zhu, Shihao Wu", "title": "On the early solution path of best subset selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early solution path, which tracks the first few variables that enter the\nmodel of a selection procedure, is of profound importance to scientific\ndiscovery. In practice, it is often statistically intangible to identify all\nthe important features with no false discovery, let alone the intimidating\nexpense of experiments to test their significance. Such realistic limitation\ncalls for statistical guarantee for the early discovery of a model selector to\nnavigate scientific adventure on the sea of big data. In this paper, we focus\non the early solution path of best subset selection (BSS), where the sparsity\nconstraint is set to be lower than the true sparsity. Under a sparse\nhigh-dimensional linear model, we establish the sufficient and (near) necessary\ncondition for BSS to achieve sure early selection, or equivalently, zero false\ndiscovery throughout its entire early path. Essentially, this condition boils\ndown to a lower bound of the minimum projected signal margin that characterizes\nthe fundamental gap in signal capturing between sure selection models and those\nwith spurious discovery. Defined through projection operators, this margin is\nindependent of the restricted eigenvalues of the design, suggesting the\nrobustness of BSS against collinearity. On the numerical aspect, we choose\nCoSaMP (Compressive Sampling Matching Pursuit) to approximate the BSS\nsolutions, and we show that the resulting early path exhibits much lower false\ndiscovery rate (FDR) than LASSO, MCP and SCAD, especially in presence of highly\ncorrelated design. Finally, we apply CoSaMP to perform preliminary feature\nscreening for the knockoff filter to enhance its power.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 19:06:16 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Zhu", "Ziwei", ""], ["Wu", "Shihao", ""]]}, {"id": "2107.06971", "submitter": "Joann Jasiak", "authors": "Christian Gourieroux, Joann Jasiak", "title": "Temporally Local Maximum Likelihood with Application to SIS Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parametric estimators applied by rolling are commonly used in the\nanalysis of time series with nonlinear features, such as structural change due\nto time varying parameters and local trends. This paper examines the properties\nof rolling estimators in the class of Temporally Local Maximum Likelihood\n(TLML) estimators. We study the TLML estimators of constant parameters,\nstochastic and stationary parameters and parameters with the Ultra Long Run\n(ULR) dynamics bridging the gap between the constant and stochastic parameters.\nMoreover, we explore the properties of TLML estimators in an application to the\nSusceptible-Infected-Susceptible (SIS) epidemiological model and illustrate\ntheir finite sample performance in a simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 20:15:05 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Gourieroux", "Christian", ""], ["Jasiak", "Joann", ""]]}, {"id": "2107.06979", "submitter": "Joann Jasiak", "authors": "Christian Gourieroux, Joann Jasiak", "title": "Generalized Covariance Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of semi-parametric dynamic models with strong white noise\nerrors. This class of processes includes the standard Vector Autoregressive\n(VAR) model, the nonfundamental structural VAR, the mixed causal-noncausal\nmodels, as well as nonlinear dynamic models such as the (multivariate) ARCH-M\nmodel. For estimation of processes in this class, we propose the Generalized\nCovariance (GCov) estimator, which is obtained by minimizing a residual-based\nmultivariate portmanteau statistic as an alternative to the Generalized Method\nof Moments. We derive the asymptotic properties of the GCov estimator and of\nthe associated residual-based portmanteau statistic. Moreover, we show that the\nGCov estimators are semi-parametrically efficient and the residual-based\nportmanteau statistics are asymptotically chi-square distributed. The finite\nsample performance of the GCov estimator is illustrated in a simulation study.\nThe estimator is also applied to a dynamic model of cryptocurrency prices.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 20:26:57 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Gourieroux", "Christian", ""], ["Jasiak", "Joann", ""]]}, {"id": "2107.07026", "submitter": "Budhi Arta Surya", "authors": "Budhi Surya", "title": "A new class of conditional Markov jump processes with regime switching\n  and path dependence: properties and maximum likelihood estimation", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.SY eess.SY math.IT q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a new class of conditional Markov jump processes with\nregime switching and paths dependence. The key novel feature of the developed\nprocess lies on its ability to switch the transition rate as it moves from one\nstate to another with switching probability depending on the current state and\ntime of the process as well as its past trajectories. As such, the transition\nfrom current state to another depends on the holding time of the process in the\nstate. Distributional properties of the process are given explicitly in terms\nof the speed regimes represented by a finite number of different transition\nmatrices, the probabilities of selecting regime membership within each state,\nand past realization of the process. In particular, it has distributional\nequivalent stochastic representation with a general mixture of Markov jump\nprocesses introduced in Frydman and Surya (2020). Maximum likelihood estimates\n(MLE) of the distribution parameters of the process are derived in closed form.\nThe estimation is done iteratively using the EM algorithm. Akaike information\ncriterion is used to assess the goodness-of-fit of the selected model. An\nexplicit observed Fisher information matrix of the MLE is derived for the\ncalculation of standard errors of the MLE. The information matrix takes on a\nsimplified form of the general matrix formula of Louis (1982). Large sample\nproperties of the MLE are presented. In particular, the covariance matrix for\nthe MLE of transition rates is equal to the Cram\\'er-Rao lower bound, and is\nless for the MLE of regime membership. The simulation study confirms these\nfindings and shows that the parameter estimates are accurate, consistent, and\nhave asymptotic normality as the sample size increases.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 22:30:53 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Surya", "Budhi", ""]]}, {"id": "2107.07086", "submitter": "Jared Huling", "authors": "Jared D. Huling, Noah Greifer, Guanhua Chen", "title": "Independence weights for causal inference with continuous exposures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Studying causal effects of continuous exposures is important for gaining a\ndeeper understanding of many interventions, policies, or medications, yet\nresearchers are often left with observational studies for doing so. In the\nobservational setting, confounding is a barrier to estimation of causal\neffects. Weighting approaches seek to control for confounding by reweighting\nsamples so that confounders are comparable across different values of the\nexposure, yet for continuous exposures, weighting methods are highly sensitive\nto model misspecification. In this paper we elucidate the key property that\nmakes weights effective in estimating causal quantities involving continuous\nexposures. We show that to eliminate confounding, weights should make exposure\nand confounders independent on the weighted scale. We develop a measure that\ncharacterizes the degree to which a set of weights induces such independence.\nFurther, we propose a new model-free method for weight estimation by optimizing\nour measure. We study the theoretical properties of our measure and our\nweights, and prove that our weights can explicitly mitigate exposure-confounder\ndependence. The empirical effectiveness of our approach is demonstrated in a\nsuite of challenging numerical experiments, where we find that our weights are\nquite robust and work well under a broad range of settings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 02:38:26 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Huling", "Jared D.", ""], ["Greifer", "Noah", ""], ["Chen", "Guanhua", ""]]}, {"id": "2107.07246", "submitter": "David Angwenyi Dr.", "authors": "David Angwenyi", "title": "Estimation of spatially varying parameters with application to\n  hyperbolic SPDEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  More often than not, we encounter problems with varying parameters as opposed\nto those that are static. In this paper, we treat the estimation of parameters\nwhich vary with space. We use Metropolis-Hastings algorithm as a selection\ncriteria for the maximum filter likelihood. Comparisons are made with the use\nof joint estimation of both the spatially varying parameters and the state. We\nillustrate the procedures employed in this paper by means of two hyperbolic\nSPDEs: the advection and the wave equation. The Metropolis-Hastings procedure\nregisters better estimates.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 11:09:22 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Angwenyi", "David", ""]]}, {"id": "2107.07257", "submitter": "Oliver Feng", "authors": "Oliver Y. Feng, Yining Chen, Qiyang Han, Raymond J. Carroll and\n  Richard J. Samworth", "title": "Nonparametric, tuning-free estimation of S-shaped functions", "comments": "79 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the nonparametric estimation of an S-shaped regression function.\nThe least squares estimator provides a very natural, tuning-free approach, but\nresults in a non-convex optimisation problem, since the inflection point is\nunknown. We show that the estimator may nevertheless be regarded as a\nprojection onto a finite union of convex cones, which allows us to propose a\nmixed primal-dual bases algorithm for its efficient, sequential computation.\nAfter developing a projection framework that demonstrates the consistency and\nrobustness to misspecification of the estimator, our main theoretical results\nprovide sharp oracle inequalities that yield worst-case and adaptive risk\nbounds for the estimation of the regression function, as well as a rate of\nconvergence for the estimation of the inflection point. These results reveal\nnot only that the estimator achieves the minimax optimal rate of convergence\nfor both the estimation of the regression function and its inflection point (up\nto a logarithmic factor in the latter case), but also that it is able to\nachieve an almost-parametric rate when the true regression function is\npiecewise affine with not too many affine pieces. Simulations and a real data\napplication to air pollution modelling also confirm the desirable finite-sample\nproperties of the estimator, and our algorithm is implemented in the R package\nSshaped.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 11:28:01 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Feng", "Oliver Y.", ""], ["Chen", "Yining", ""], ["Han", "Qiyang", ""], ["Carroll", "Raymond J.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2107.07278", "submitter": "Ian White", "authors": "Ian R. White, Tim P Morris, Elizabeth Williamson", "title": "Covariate adjustment in randomised trials: canonical link functions\n  protect against model mis-specification", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Covariate adjustment has the potential to increase power in the analysis of\nrandomised trials, but mis-specification of the adjustment model could cause\nerror. We explore what error is possible when the adjustment model omits a\ncovariate by randomised treatment interaction, in a setting where the covariate\nis perfectly balanced between randomised treatments. We use mathematical\narguments and analyses of single hypothetical data sets.\n  We show that analysis by a generalised linear model with the canonical link\nfunction leads to no error under the null -- that is, if treatment effect is\ntruly zero under the adjusted model then it is also zero under the unadjusted\nmodel. However, using non-canonical link functions does not give this property\nand leads to potentially important error under the null. The error is present\neven in large samples and hence constitutes bias.\n  We conclude that covariate adjustment analyses of randomised trials should\navoid non-canonical links. If a marginal risk difference is the target of\nestimation then this should not be estimated using an identity link;\nalternative preferable methods include standardisation and inverse probability\nof treatment weighting.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 12:10:44 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["White", "Ian R.", ""], ["Morris", "Tim P", ""], ["Williamson", "Elizabeth", ""]]}, {"id": "2107.07313", "submitter": "Vincent Geels", "authors": "Vincent Geels, Matthew Pratola, Radu Herbei", "title": "The Taxicab Sampler: MCMC for Discrete Spaces with Application to Tree\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of exploring discrete but very complex state spaces\nin Bayesian models, we propose a novel Markov Chain Monte Carlo search\nalgorithm: the taxicab sampler. We describe the construction of this sampler\nand discuss how its interpretation and usage differs from that of standard\nMetropolis-Hastings as well as the closely-related Hamming ball sampler. The\nproposed taxicab sampling algorithm is then shown to demonstrate substantial\nimprovement in computation time relative to a na\\\"ive Metropolis-Hastings\nsearch in a motivating Bayesian regression tree count model, in which we\nleverage the discrete state space assumption to construct a novel likelihood\nfunction that allows for flexibly describing different mean-variance\nrelationships while preserving parameter interpretability compared to existing\nlikelihood functions for count data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 13:34:20 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Geels", "Vincent", ""], ["Pratola", "Matthew", ""], ["Herbei", "Radu", ""]]}, {"id": "2107.07317", "submitter": "Jin Zhu", "authors": "Xueqin Wang, Jin Zhu, Wenliang Pan, Junhao Zhu, Heping Zhang", "title": "Nonparametric Statistical Inference via Metric Distribution Function in\n  Metric Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distribution function is essential in statistical inference, and connected\nwith samples to form a directed closed loop by the correspondence theorem in\nmeasure theory and the Glivenko-Cantelli and Donsker properties. This\nconnection creates a paradigm for statistical inference. However, existing\ndistribution functions are defined in Euclidean spaces and no longer convenient\nto use in rapidly evolving data objects of complex nature. It is imperative to\ndevelop the concept of distribution function in a more general space to meet\nemerging needs. Note that the linearity allows us to use hypercubes to define\nthe distribution function in a Euclidean space, but without the linearity in a\nmetric space, we must work with the metric to investigate the probability\nmeasure. We introduce a class of metric distribution functions through the\nmetric between random objects and a fixed location in metric spaces. We\novercome this challenging step by proving the correspondence theorem and the\nGlivenko-Cantelli theorem for metric distribution functions in metric spaces\nthat lie the foundation for conducting rational statistical inference for\nmetric space-valued data. Then, we develop homogeneity test and mutual\nindependence test for non-Euclidean random objects, and present comprehensive\nempirical evidence to support the performance of our proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 13:37:09 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wang", "Xueqin", ""], ["Zhu", "Jin", ""], ["Pan", "Wenliang", ""], ["Zhu", "Junhao", ""], ["Zhang", "Heping", ""]]}, {"id": "2107.07322", "submitter": "Ziyu Xu", "authors": "Ziyu Xu, Ruodu Wang, Aaditya Ramdas", "title": "A unified framework for bandit multiple testing", "comments": "37 pages. 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In bandit multiple hypothesis testing, each arm corresponds to a different\nnull hypothesis that we wish to test, and the goal is to design adaptive\nalgorithms that correctly identify large set of interesting arms (true\ndiscoveries), while only mistakenly identifying a few uninteresting ones (false\ndiscoveries). One common metric in non-bandit multiple testing is the false\ndiscovery rate (FDR). We propose a unified, modular framework for bandit FDR\ncontrol that emphasizes the decoupling of exploration and summarization of\nevidence. We utilize the powerful martingale-based concept of ``e-processes''\nto ensure FDR control for arbitrary composite nulls, exploration rules and\nstopping times in generic problem settings. In particular, valid FDR control\nholds even if the reward distributions of the arms could be dependent, multiple\narms may be queried simultaneously, and multiple (cooperating or competing)\nagents may be querying arms, covering combinatorial semi-bandit type settings\nas well. Prior work has considered in great detail the setting where each arm's\nreward distribution is independent and sub-Gaussian, and a single arm is\nqueried at each step. Our framework recovers matching sample complexity\nguarantees in this special case, and performs comparably or better in practice.\nFor other settings, sample complexities will depend on the finer details of the\nproblem (composite nulls being tested, exploration algorithm, data dependence\nstructure, stopping rule) and we do not explore these; our contribution is to\nshow that the FDR guarantee is clean and entirely agnostic to these details.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 13:47:28 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Xu", "Ziyu", ""], ["Wang", "Ruodu", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2107.07350", "submitter": "Kartik Waghmare", "authors": "Kartik G. Waghmare and Victor M. Panaretos", "title": "The Completion of Covariance Kernels", "comments": "Typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of positive-semidefinite continuation: extending a\npartially specified covariance kernel from a subdomain $\\Omega$ of a domain\n$I\\times I$ to a covariance kernel on the entire domain $I\\times I$. For a\nbroad class of domains $\\Omega$ called serrated domains, we are able to present\na complete theory. Namely, we demonstrate that a canonical completion always\nexists and can be explicitly constructed. We characterise all possible\ncompletions as suitable perturbations of the canonical completion, and\ndetermine necessary and sufficient conditions for a unique completion to exist.\nWe interpret the canonical completion via the graphical model structure it\ninduces on the associated Gaussian process. Furthermore, we show how the\nestimation of the canonical completion reduces to the solution of a system of\nlinear statistical inverse problems in the space of Hilbert-Schmidt operators,\nand derive rates of convergence under standard source conditions. We conclude\nby providing extensions of our theory to more general forms of domains.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 14:06:40 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 13:15:52 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Waghmare", "Kartik G.", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "2107.07371", "submitter": "Jae-Kwang Kim", "authors": "Hengfang Wang and Jae Kwang Kim", "title": "Statistical inference using Regularized M-estimation in the reproducing\n  kernel Hilbert space for handling missing data", "comments": "arXiv admin note: text overlap with arXiv:2102.00058", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Imputation and propensity score weighting are two popular techniques for\nhandling missing data. We address these problems using the regularized\nM-estimation techniques in the reproducing kernel Hilbert space. Specifically,\nwe first use the kernel ridge regression to develop imputation for handling\nitem nonresponse. While this nonparametric approach is potentially promising\nfor imputation, its statistical properties are not investigated in the\nliterature. Under some conditions on the order of the tuning parameter, we\nfirst establish the root-$n$ consistency of the kernel ridge regression\nimputation estimator and show that it achieves the lower bound of the\nsemiparametric asymptotic variance. A nonparametric propensity score estimator\nusing the reproducing kernel Hilbert space is also developed by a novel\napplication of the maximum entropy method for the density ratio function\nestimation. We show that the resulting propensity score estimator is\nasymptotically equivalent to the kernel ridge regression imputation estimator.\nResults from a limited simulation study are also presented to confirm our\ntheory. The proposed method is applied to analyze the air pollution data\nmeasured in Beijing, China.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 14:51:39 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wang", "Hengfang", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "2107.07475", "submitter": "Ian Grooms", "authors": "Ian Grooms", "title": "A comparison of nonlinear extensions to the ensemble Kalman filter:\n  Gaussian Anamorphosis and Two-Step Ensemble Filters", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews two nonlinear, non-Gaussian extensions of the Ensemble\nKalman Filter: Gaussian anamorphosis (GA) methods and two-step updates, of\nwhich the rank histogram filter (RHF) is a prototypical example. GA-EnKF\nmethods apply univariate transforms to the state and observation variables to\nmake their distribution more Gaussian before applying an EnKF. The two-step\nmethods use a scalar Bayesian update for the first step, followed by linear\nregression for the second step. The connection of the two-step framework to the\nfull Bayesian problem is made, which opens the door to more advanced two-step\nmethods in the full Bayesian setting. A new method for the first part of the\ntwo-step framework is proposed, with a similar form to the RHF but a different\nmotivation, called the `improved RHF' (iRHF). A suite of experiments with the\nLorenz-`96 model demonstrate situations where the GA-EnKF methods are similar\nto EnKF, and where they outperform EnKF. The experiments also strongly support\nthe accuracy of the RHF and iRHF filters for nonlinear and non-Gaussian\nobservations; these methods uniformly beat the EnKF and GA-EnKF methods in the\nexperiments reported here. The new iRHF method is only more accurate than RHF\nat small ensemble sizes in the experiments reported here.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:23:13 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Grooms", "Ian", ""]]}, {"id": "2107.07483", "submitter": "Francisco Valente", "authors": "Francisco Valente, Sim\\~ao Paredes, Jorge Henriques", "title": "Personalized and Reliable Decision Sets: Enhancing Interpretability in\n  Clinical Decision Support Systems", "comments": "Accepted to the ICML 2021 Workshop on Interpretable Machine Learning\n  in Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a novel clinical decision support system and\ndiscuss its interpretability-related properties. It combines a decision set of\nrules with a machine learning scheme to offer global and local\ninterpretability. More specifically, machine learning is used to predict the\nlikelihood of each of those rules to be correct for a particular patient, which\nmay also contribute to better predictive performances. Moreover, the\nreliability analysis of individual predictions is also addressed, contributing\nto further personalized interpretability. The combination of these several\nelements may be crucial to obtain the clinical stakeholders' trust, leading to\na better assessment of patients' conditions and improvement of the physicians'\ndecision-making.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:36:24 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Valente", "Francisco", ""], ["Paredes", "Sim\u00e3o", ""], ["Henriques", "Jorge", ""]]}, {"id": "2107.07511", "submitter": "Anastasios Angelopoulos", "authors": "Anastasios N. Angelopoulos, Stephen Bates", "title": "A Gentle Introduction to Conformal Prediction and Distribution-Free\n  Uncertainty Quantification", "comments": "Blog and tutorial video\n  http://angelopoulos.ai/blog/posts/gentle-intro/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box machine learning learning methods are now routinely used in\nhigh-risk settings, like medical diagnostics, which demand uncertainty\nquantification to avoid consequential model failures. Distribution-free\nuncertainty quantification (distribution-free UQ) is a user-friendly paradigm\nfor creating statistically rigorous confidence intervals/sets for such\npredictions. Critically, the intervals/sets are valid without distributional\nassumptions or model assumptions, with explicit guarantees with finitely many\ndatapoints. Moreover, they adapt to the difficulty of the input; when the input\nexample is difficult, the uncertainty intervals/sets are large, signaling that\nthe model might be wrong. Without much work, one can use distribution-free\nmethods on any underlying algorithm, such as a neural network, to produce\nconfidence sets guaranteed to contain the ground truth with a user-specified\nprobability, such as 90%. Indeed, the methods are easy-to-understand and\ngeneral, applying to many modern prediction problems arising in the fields of\ncomputer vision, natural language processing, deep reinforcement learning, and\nso on. This hands-on introduction is aimed at a reader interested in the\npractical implementation of distribution-free UQ, including conformal\nprediction and related methods, who is not necessarily a statistician. We will\ninclude many explanatory illustrations, examples, and code samples in Python,\nwith PyTorch syntax. The goal is to provide the reader a working understanding\nof distribution-free UQ, allowing them to put confidence intervals on their\nalgorithms, with one self-contained document.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:59:50 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Angelopoulos", "Anastasios N.", ""], ["Bates", "Stephen", ""]]}, {"id": "2107.07513", "submitter": "George Moustakides", "authors": "George V. Moustakides, Xujun Liu and Olgica Milenkovic", "title": "Optimal Stopping Methodology for the Secretary Problem with Random\n  Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Candidates arrive sequentially for an interview process which results in them\nbeing ranked relative to their predecessors. Based on the ranks available at\neach time, one must develop a decision mechanism that selects or dismisses the\ncurrent candidate in an effort to maximize the chance to select the best. This\nclassical version of the \"Secretary problem\" has been studied in depth using\nmostly combinatorial approaches, along with numerous other variants. In this\nwork we consider a particular new version where during reviewing one is allowed\nto query an external expert to improve the probability of making the correct\ndecision. Unlike existing formulations, we consider experts that are not\nnecessarily infallible and may provide suggestions that can be faulty. For the\nsolution of our problem we adopt a probabilistic methodology and view the\nquerying times as consecutive stopping times which we optimize with the help of\noptimal stopping theory. For each querying time we must also design a mechanism\nto decide whether we should terminate the search at the querying time or not.\nThis decision is straightforward under the usual assumption of infallible\nexperts but, when experts are faulty, it has a far more intricate structure.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 10:50:08 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Moustakides", "George V.", ""], ["Liu", "Xujun", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "2107.07561", "submitter": "Luiza Piancastelli", "authors": "Luiza S.C. Piancastelli, Nial Friel, Wagner Barreto-Souza and Hernando\n  Ombao", "title": "Multivariate Conway-Maxwell-Poisson Distribution: Sarmanov Method and\n  Doubly-Intractable Bayesian Inference", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a multivariate count distribution with Conway-Maxwell\n(COM)-Poisson marginals is proposed. To do this, we develop a modification of\nthe Sarmanov method for constructing multivariate distributions. Our\nmultivariate COM-Poisson (MultCOMP) model has desirable features such as (i) it\nadmits a flexible covariance matrix allowing for both negative and positive\nnon-diagonal entries; (ii) it overcomes the limitation of the existing\nbivariate COM-Poisson distributions in the literature that do not have\nCOM-Poisson marginals; (iii) it allows for the analysis of multivariate counts\nand is not just limited to bivariate counts. Inferential challenges are\npresented by the likelihood specification as it depends on a number of\nintractable normalizing constants involving the model parameters. These\nobstacles motivate us to propose a Bayesian inferential approach where the\nresulting doubly-intractable posterior is dealt with via the exchange algorithm\nand the Grouped Independence Metropolis-Hastings algorithm. Numerical\nexperiments based on simulations are presented to illustrate the proposed\nBayesian approach. We analyze the potential of the MultCOMP model through a\nreal data application on the numbers of goals scored by the home and away teams\nin the Premier League from 2018 to 2021. Here, our interest is to assess the\neffect of a lack of crowds during the COVID-19 pandemic on the well-known home\nteam advantage. A MultCOMP model fit shows that there is evidence of a\ndecreased number of goals scored by the home team, not accompanied by a reduced\nscore from the opponent. Hence, our analysis suggests a smaller home team\nadvantage in the absence of crowds, which agrees with the opinion of several\nfootball experts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 18:48:42 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Piancastelli", "Luiza S. C.", ""], ["Friel", "Nial", ""], ["Barreto-Souza", "Wagner", ""], ["Ombao", "Hernando", ""]]}, {"id": "2107.07575", "submitter": "Caleb Miles", "authors": "Caleb H. Miles and Antoine Chambaz", "title": "Optimal tests of the composite null hypothesis arising in mediation\n  analysis", "comments": "40 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The indirect effect of an exposure on an outcome through an intermediate\nvariable can be identified by a product of regression coefficients under\ncertain causal and regression modeling assumptions. Thus, the null hypothesis\nof no indirect effect is a composite null hypothesis, as the null holds if\neither regression coefficient is zero. A consequence is that existing\nhypothesis tests are either severely underpowered near the origin (i.e., when\nboth coefficients are small with respect to standard errors) or do not preserve\ntype 1 error uniformly over the null hypothesis space. We propose hypothesis\ntests that (i) preserve level alpha type 1 error, (ii) meaningfully improve\npower when both true underlying effects are small relative to sample size, and\n(iii) preserve power when at least one is not. One approach gives a closed-form\ntest that is minimax optimal with respect to local power over the alternative\nparameter space. Another uses sparse linear programming to produce an\napproximately optimal test for a Bayes risk criterion. We provide an R package\nthat implements the minimax optimal test.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 19:20:24 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Miles", "Caleb H.", ""], ["Chambaz", "Antoine", ""]]}, {"id": "2107.07594", "submitter": "Ryan Peterson", "authors": "Ryan A. Peterson and Joseph E. Cavanaugh", "title": "Ranked Sparsity: A Cogent Regularization Framework for Selecting and\n  Estimating Feature Interactions and Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore and illustrate the concept of ranked sparsity, a phenomenon that\noften occurs naturally in modeling applications when an expected disparity\nexists in the quality of information between different feature sets. Its\npresence can cause traditional and modern model selection methods to fail\nbecause such procedures commonly presume that each potential parameter is\nequally worthy of entering into the final model - we call this presumption\n\"covariate equipoise\". However, this presumption does not always hold,\nespecially in the presence of derived variables. For instance, when all\npossible interactions are considered as candidate predictors, the premise of\ncovariate equipoise will often produce over-specified and opaque models. The\nsheer number of additional candidate variables grossly inflates the number of\nfalse discoveries in the interactions, resulting in unnecessarily complex and\ndifficult-to-interpret models with many (truly spurious) interactions. We\nsuggest a modeling strategy that requires a stronger level of evidence in order\nto allow certain variables (e.g. interactions) to be selected in the final\nmodel. This ranked sparsity paradigm can be implemented with the\nsparsity-ranked lasso (SRL). We compare the performance of SRL relative to\ncompeting methods in a series of simulation studies, showing that the SRL is a\nvery attractive method because it is fast, accurate, and produces more\ntransparent models (with fewer false interactions). We illustrate its utility\nin an application to predict the survival of lung cancer patients using a set\nof gene expression measurements and clinical covariates, searching in\nparticular for gene-environment interactions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 20:15:22 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Peterson", "Ryan A.", ""], ["Cavanaugh", "Joseph E.", ""]]}, {"id": "2107.07605", "submitter": "Guy Nason Prof.", "authors": "Guy P Nason and James L Wei", "title": "Quantifying the economic response to COVID-19 mitigations and death\n  rates via forecasting Purchasing Managers' Indices using Generalised Network\n  Autoregressive models with exogenous variables", "comments": "To be read before the Royal Statistical Society at the Society's 2021\n  annual conference held in Manchester on Wednesday, September 8th 2021, the\n  President, Professor Sylvia Richardson, in the Chair. Accepted by the Journal\n  of the Royal Statistical Society, Series A", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge of the current state of economies, how they respond to COVID-19\nmitigations and indicators, and what the future might hold for them is\nimportant. We use recently-developed generalised network autoregressive (GNAR)\nmodels, using trade-determined networks, to model and forecast the Purchasing\nManagers' Indices for a number of countries. We use networks that link\ncountries where the links themselves, or their weights, are determined by the\ndegree of export trade between the countries. We extend these models to include\nnode-specific time series exogenous variables (GNARX models), using this to\nincorporate COVID-19 mitigation stringency indices and COVID-19 death rates\ninto our analysis. The highly parsimonious GNAR models considerably outperform\nvector autoregressive models in terms of mean-squared forecasting error and our\nGNARX models themselves outperform GNAR ones. Further mixed frequency modelling\npredicts the extent to which that the UK economy will be affected by harsher,\nweaker or no interventions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 14:45:30 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Nason", "Guy P", ""], ["Wei", "James L", ""]]}, {"id": "2107.07640", "submitter": "Sergio Hernan Garrido Mejia", "authors": "Sergio Hernan Garrido Mejia, Elke Kirschbaum, Dominik Janzing", "title": "Obtaining Causal Information by Merging Datasets with MAXENT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The investigation of the question \"which treatment has a causal effect on a\ntarget variable?\" is of particular relevance in a large number of scientific\ndisciplines. This challenging task becomes even more difficult if not all\ntreatment variables were or even cannot be observed jointly with the target\nvariable. Another similarly important and challenging task is to quantify the\ncausal influence of a treatment on a target in the presence of confounders. In\nthis paper, we discuss how causal knowledge can be obtained without having\nobserved all variables jointly, but by merging the statistical information from\ndifferent datasets. We first show how the maximum entropy principle can be used\nto identify edges among random variables when assuming causal sufficiency and\nan extended version of faithfulness. Additionally, we derive bounds on the\ninterventional distribution and the average causal effect of a treatment on a\ntarget variable in the presence of confounders. In both cases we assume that\nonly subsets of the variables have been observed jointly.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 23:16:36 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Mejia", "Sergio Hernan Garrido", ""], ["Kirschbaum", "Elke", ""], ["Janzing", "Dominik", ""]]}, {"id": "2107.07648", "submitter": "Yutong Wu", "authors": "Yutong Wu, Erich D. Jarvis and Abhra Sarkar", "title": "Bayesian Markov Renewal Mixed Models for Vocalization Syntax", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Studying the neurological, genetic and evolutionary basis of human vocal\ncommunication mechanisms is an important field of neuroscience. In the absence\nof high quality data on humans, mouse vocalization experiments in laboratory\nsettings have been proven to be useful in providing valuable insights into\nmammalian vocal development and evolution, including especially the impact of\ncertain genetic mutations. Data sets from mouse vocalization experiments\nusually consist of categorical syllable sequences along with continuous\ninter-syllable interval times for mice of different genotypes vocalizing under\nvarious contexts. Few statistical models have considered the inference for both\ntransition probabilities and inter-state intervals. The latter is of particular\nimportance as increased inter-state intervals can be an indication of possible\nvocal impairment. In this paper, we propose a class of novel Markov renewal\nmixed models that capture the stochastic dynamics of both state transitions and\ninter-state interval times. Specifically, we model the transition dynamics and\nthe inter-state intervals using Dirichlet and gamma mixtures, respectively,\nallowing the mixture probabilities in both cases to vary flexibly with fixed\ncovariate effects as well as random individual-specific effects. We apply our\nmodel to analyze the impact of a mutation in the Foxp2 gene on mouse vocal\nbehavior. We find that genotypes and social contexts significantly affect the\ninter-state interval times but, compared to previous analyses, the influences\nof genotype and social context on the syllable transition dynamics are weaker.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 00:06:22 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 22:09:21 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Wu", "Yutong", ""], ["Jarvis", "Erich D.", ""], ["Sarkar", "Abhra", ""]]}, {"id": "2107.07736", "submitter": "Xiaotian Zheng", "authors": "Xiaotian Zheng, Athanasios Kottas and Bruno Sans\\'o", "title": "Nearest-Neighbor Geostatistical Models for Non-Gaussian Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a class of nearest neighbor mixture transition distribution\nprocess (NNMP) models that provides flexibility and scalability for\nnon-Gaussian geostatistical data. We use a directed acyclic graph to define a\nproper spatial process with finite-dimensional distributions given by finite\nmixtures. We develop conditions to construct general NNMP models with\npre-specified stationary marginal distributions. We also establish lower bounds\nfor the strength of the tail dependence implied by NNMP models, demonstrating\nthe flexibility of the proposed methodology for modeling multivariate\ndependence through bivariate distribution specification. To implement inference\nand prediction, we formulate a Bayesian hierarchical model for the data, using\nthe NNMP prior model for the spatial random effects process. From an\ninferential point of view, the NNMP model lays out a new computational approach\nto handling large spatial data sets, leveraging the mixture model structure to\navoid computational issues that arise from large matrix operations. We\nillustrate the benefits of the NNMP modeling framework using synthetic data\nexamples and through analysis of sea surface temperature data from the\nMediterranean sea.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 07:13:26 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Zheng", "Xiaotian", ""], ["Kottas", "Athanasios", ""], ["Sans\u00f3", "Bruno", ""]]}, {"id": "2107.07963", "submitter": "Wagner Barreto-Souza", "authors": "Wagner Barreto-Souza and Ngai Hang Chan", "title": "Nearly Unstable Integer-Valued ARCH Process and Unit Root Testing", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a Nearly Unstable INteger-valued AutoRegressive\nConditional Heteroskedasticity (NU-INARCH) process for dealing with count time\nseries data. It is proved that a proper normalization of the NU-INARCH process\nendowed with a Skorohod topology weakly converges to a Cox-Ingersoll-Ross\ndiffusion. The asymptotic distribution of the conditional least squares\nestimator of the correlation parameter is established as a functional of\ncertain stochastic integrals. Numerical experiments based on Monte Carlo\nsimulations are provided to verify the behavior of the asymptotic distribution\nunder finite samples. These simulations reveal that the nearly unstable\napproach provides satisfactory and better results than those based on the\nstationarity assumption even when the true process is not that close to\nnon-stationarity. A unit root test is proposed and its Type-I error and power\nare examined via Monte Carlo simulations. As an illustration, the proposed\nmethodology is applied to the daily number of deaths due to COVID-19 in the\nUnited Kingdom.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:24:21 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Barreto-Souza", "Wagner", ""], ["Chan", "Ngai Hang", ""]]}, {"id": "2107.07967", "submitter": "Georgia Papadogeorgou", "authors": "Fan Li (Duke University), Zizhong Tian, Jennifer Bobb, Georgia\n  Papadogeorgou, Fan Li (Yale University School of Public Health)", "title": "Clarifying Selection Bias in Cluster Randomized Trials: Estimands and\n  Estimation", "comments": "Keywords: causal inference, cluster randomized trial,\n  intention-to-treat, heterogeneous treatment effect, post-randomization,\n  principal stratification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In cluster randomized trials (CRTs), patients are typically recruited after\nclusters are randomized, and the recruiters and patients are not blinded to the\nassignment. This leads to differential recruitment process and systematic\ndifferences in baseline characteristics of the recruited patients between\nintervention and control arms, inducing post-randomization selection bias. We\nrigorously define the causal estimands in the presence of post-randomization\nconfounding. We elucidate the conditions under which standard covariate\nadjustment methods can validly estimate these estimands. We discuss the\nadditional data and assumptions necessary for estimating the causal effects\nwhen such conditions are not met. Adopting the principal stratification\nframework in causal inference, we clarify there are two intention-to-treat\n(ITT) causal estimands in CRTs: one for the overall population and one for the\nrecruited population. We derive the analytical formula of the two estimands in\nterms of principal-stratum-specific causal effects. We assess the empirical\nperformance of two common covariate adjustment methods, multivariate regression\nand propensity score weighting, under different data generating processes. When\ntreatment effects are heterogeneous across principal strata, the ITT effect on\nthe overall population differs from the ITT effect on the recruited population.\nA naive ITT analysis of the recruited sample leads to biased estimate of both\nITT effects. In the presence of post-randomization selection and without\nadditional data on the non-recruited subjects, the ITT effect on the recruited\npopulation is estimable only when the treatment effects are homogenous between\nprincipal strata, and the ITT effect on the overall population is generally not\nestimable. The extent to which covariate adjustment can remove selection bias\ndepends on the degree of effect heterogeneity across principal strata.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:33:11 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Li", "Fan", "", "Duke University"], ["Tian", "Zizhong", "", "Duke University"], ["Bobb", "Jennifer", "", "Duke University"], ["Papadogeorgou", "Georgia", "", "Duke University"], ["Li", "Fan", "", "Duke University"]]}, {"id": "2107.08019", "submitter": "Richard Warr", "authors": "Jared M. Clark and Richard L. Warr", "title": "Bootstrapping Through Discrete Convolutional Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Bootstrapping was designed to randomly resample data from a fixed sample\nusing Monte Carlo techniques. However, the original sample itself defines a\ndiscrete distribution. Convolutional methods are well suited for discrete\ndistributions, and we show the advantages of utilizing these techniques for\nbootstrapping. The discrete convolutional approach can provide exact numerical\nsolutions for bootstrap quantities, or at least mathematical error bounds. In\ncontrast, Monte Carlo bootstrap methods can only provide confidence intervals\nwhich converge slowly. Additionally, for some problems the computation time of\nthe convolutional approach can be dramatically less than that of Monte Carlo\nresampling. This article provides several examples of bootstrapping using the\nproposed convolutional technique and compares the results to those of the Monte\nCarlo bootstrap, and to those of the competing saddlepoint method.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 17:19:55 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Clark", "Jared M.", ""], ["Warr", "Richard L.", ""]]}, {"id": "2107.08030", "submitter": "Adrien Brilhault", "authors": "Adrien Brilhault, Sergio Neuenschwander, Ricardo Araujo Rios", "title": "A New Robust Multivariate Mode Estimator for Eye-tracking Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose in this work a new method for estimating the main mode of\nmultivariate distributions, with application to eye-tracking calibrations. When\nperforming eye-tracking experiments with poorly cooperative subjects, such as\ninfants or monkeys, the calibration data generally suffer from high\ncontamination. Outliers are typically organized in clusters, corresponding to\nthe time intervals when subjects were not looking at the calibration points. In\nthis type of multimodal distributions, most central tendency measures fail at\nestimating the principal fixation coordinates (the first mode), resulting in\nerrors and inaccuracies when mapping the gaze to the screen coordinates. Here,\nwe developed a new algorithm to identify the first mode of multivariate\ndistributions, named BRIL, which rely on recursive depth-based filtering. This\nnovel approach was tested on artificial mixtures of Gaussian and Uniform\ndistributions, and compared to existing methods (conventional depth medians,\nrobust estimators of location and scatter, and clustering-based approaches). We\nobtained outstanding performances, even for distributions containing very high\nproportions of outliers, both grouped in clusters and randomly distributed.\nFinally, we demonstrate the strength of our method in a real-world scenario\nusing experimental data from eye-tracking calibrations with Capuchin monkeys,\nespecially for distributions where other algorithms typically lack accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 17:45:19 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Brilhault", "Adrien", ""], ["Neuenschwander", "Sergio", ""], ["Rios", "Ricardo Araujo", ""]]}, {"id": "2107.08062", "submitter": "James Jackson", "authors": "James Jackson, Robin Mitra, Brian Francis and Iain Dove", "title": "Moving towards practical user-friendly synthesis: Scalable synthetic\n  data methods for large confidential administrative databases using saturated\n  count models", "comments": "37 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past three decades, synthetic data methods for statistical\ndisclosure control have continually developed; methods have adapted to account\nfor different data types, but mainly within the domain of survey data sets.\nCertain characteristics of administrative databases - sometimes just the sheer\nvolume of records of which they are comprised - present challenges from a\nsynthesis perspective and thus require special attention. This paper, through\nthe fitting of saturated models, presents a way in which administrative\ndatabases can not only be synthesized quickly, but also allows risk and utility\nto be formalised in a manner inherently unfeasible in other techniques. The\npaper explores how the flexibility afforded by two-parameter count models (the\nnegative binomial and Poisson-inverse Gaussian) can be utilised to protect\nrespondents' - especially uniques' - privacy in synthetic data. Finally an\nempirical example is carried out through the synthesis of a database which can\nbe viewed as a good representative to the English School Census.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 18:08:26 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Jackson", "James", ""], ["Mitra", "Robin", ""], ["Francis", "Brian", ""], ["Dove", "Iain", ""]]}, {"id": "2107.08072", "submitter": "Jennifer Bobb", "authors": "Jennifer F. Bobb, Maricela F. Cruz, Stephen J. Mooney, Adam\n  Drewnowski, David Arterburn, Andrea J. Cook", "title": "Accounting for spatial confounding in epidemiological studies with\n  individual-level exposures: An exposure-penalized spline approach", "comments": "30 pages, 5 figures, supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of unmeasured spatial confounding, spatial models may\nactually increase (rather than decrease) bias, leading to uncertainty as to how\nthey should be applied in practice. We evaluated spatial modeling approaches\nthrough simulation and application to a big data electronic health record\nstudy. Whereas the risk of bias was high for purely spatial exposures (e.g.,\nbuilt environment), we found very limited potential for increased bias for\nindividual-level exposures that cluster spatially (e.g., smoking status). We\nalso proposed a novel exposure-penalized spline approach that selects the\ndegree of spatial smoothing to explain spatial variability in the exposure.\nThis approach appeared promising for efficiently reducing spatial confounding\nbias.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 18:31:11 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bobb", "Jennifer F.", ""], ["Cruz", "Maricela F.", ""], ["Mooney", "Stephen J.", ""], ["Drewnowski", "Adam", ""], ["Arterburn", "David", ""], ["Cook", "Andrea J.", ""]]}, {"id": "2107.08075", "submitter": "Erin Hartman", "authors": "Erin Hartman, Chad Hazlett and Ciara Sterbenz", "title": "Kpop: A kernel balancing approach for reducing specification assumptions\n  in survey weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the precipitous decline in response rates, researchers and pollsters\nhave been left with highly non-representative samples, relying on constructed\nweights to make these samples representative of the desired target population.\nThough practitioners employ valuable expert knowledge to choose what variables,\n$X$ must be adjusted for, they rarely defend particular functional forms\nrelating these variables to the response process or the outcome. Unfortunately,\ncommonly-used calibration weights -- which make the weighted mean $X$ in the\nsample equal that of the population -- only ensure correct adjustment when the\nportion of the outcome and the response process left unexplained by linear\nfunctions of $X$ are independent. To alleviate this functional form dependency,\nwe describe kernel balancing for population weighting (kpop). This approach\nreplaces the design matrix $\\mathbf{X}$ with a kernel matrix, $\\mathbf{K}$\nencoding high-order information about $\\mathbf{X}$. Weights are then found to\nmake the weighted average row of $\\mathbf{K}$ among sampled units approximately\nequal that of the target population. This produces good calibration on a wide\nrange of smooth functions of $X$, without relying on the user to explicitly\nspecify those functions. We describe the method and illustrate it by\napplication to polling data from the 2016 U.S. presidential election.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 18:38:24 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Hartman", "Erin", ""], ["Hazlett", "Chad", ""], ["Sterbenz", "Ciara", ""]]}, {"id": "2107.08112", "submitter": "Szymon Sacher", "authors": "Szymon Sacher, Laura Battaglia, Stephen Hansen", "title": "Hamiltonian Monte Carlo for Regression with High-Dimensional Categorical\n  Data", "comments": "30 pages 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models are becoming increasingly popular in economics for\nhigh-dimensional categorical data such as text and surveys. Often the resulting\nlow-dimensional representations are plugged into downstream econometric models\nthat ignore the statistical structure of the upstream model, which presents\nserious challenges for valid inference. We show how Hamiltonian Monte Carlo\n(HMC) implemented with parallelized automatic differentiation provides a\ncomputationally efficient, easy-to-code, and statistically robust solution for\nthis problem. Via a series of applications, we show that modeling integrated\nstructure can non-trivially affect inference and that HMC appears to markedly\noutperform current approaches to inference in integrated models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 20:40:54 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Sacher", "Szymon", ""], ["Battaglia", "Laura", ""], ["Hansen", "Stephen", ""]]}, {"id": "2107.08288", "submitter": "Jianhua Huang", "authors": "Rui Tuo, Shiyuan He, Arash Pourhabib, Yu Ding and Jianhua Z. Huang", "title": "A Reproducing Kernel Hilbert Space Approach to Functional Calibration of\n  Computer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a frequentist solution to the functional calibration\nproblem, where the value of a calibration parameter in a computer model is\nallowed to vary with the value of control variables in the physical system. The\nneed of functional calibration is motivated by engineering applications where\nusing a constant calibration parameter results in a significant mismatch\nbetween outputs from the computer model and the physical experiment.\nReproducing kernel Hilbert spaces (RKHS) are used to model the optimal\ncalibration function, defined as the functional relationship between the\ncalibration parameter and control variables that gives the best prediction.\nThis optimal calibration function is estimated through penalized least squares\nwith an RKHS-norm penalty and using physical data. An uncertainty\nquantification procedure is also developed for such estimates. Theoretical\nguarantees of the proposed method are provided in terms of prediction\nconsistency and consistency of estimating the optimal calibration function. The\nproposed method is tested using both real and synthetic data and exhibits more\nrobust performance in prediction and uncertainty quantification than the\nexisting parametric functional calibration method and a state-of-art Bayesian\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 17:12:48 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Tuo", "Rui", ""], ["He", "Shiyuan", ""], ["Pourhabib", "Arash", ""], ["Ding", "Yu", ""], ["Huang", "Jianhua Z.", ""]]}, {"id": "2107.08296", "submitter": "Guenther Walther", "authors": "Guenther Walther", "title": "Calibrating the scan statistic with size-dependent critical values:\n  heuristics, methodology and computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the scan statistic with variable window size favors the\ndetection of signals with small spatial extent and there is a corresponding\nloss of power for signals with large spatial extent. Recent results have shown\nthat this loss is not inevitable: Using critical values that depend on the size\nof the window allows optimal detection for all signal sizes simultaneously, so\nthere is no substantial price to pay for not knowing the correct window size\nand for scanning with a variable window size. This paper gives a review of the\nheuristics and methodology for such size-dependent critical values, their\napplications to various settings including the multivariate case, and recent\nresults about fast algorithms for computing scan statistics.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 18:00:13 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Walther", "Guenther", ""]]}, {"id": "2107.08338", "submitter": "Jin Liu", "authors": "Jin Liu, Robert A. Perera", "title": "Assessing Mediational Processes in Parallel Bilinear Spline Growth Curve\n  Models in the Framework of Individual Measurement Occasions", "comments": "Draft version 1.1, 07/17/2021. This paper has not been peer reviewed.\n  Please do not copy or cite without author's permission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple existing studies have developed multivariate growth models with\nnonlinear functional forms to explore joint development where two longitudinal\nrecords are associated over time. However, multiple repeated outcomes are not\nnecessarily synchronous. Accordingly, it is of interest to investigate an\nassociation between two repeated variables on different occasions, for example,\nhow a short-term change of one variable affects a long-term change of the\nother(s). One statistical tool for such analyses is longitudinal mediation\nmodels. In this study, we extend latent growth mediation models with linear\ntrajectories (Cheong et al., 2003) and develop two models to evaluate\nmediational processes where the bilinear spline (i.e., the linear-linear\npiecewise) growth model is utilized to capture the change patterns. We define\nthe mediational process as either the baseline covariate or the change of\ncovariate influencing the change of the mediator, which, in turn, affects the\nchange of the outcome. We present the proposed models by simulation studies.\nOur simulation studies demonstrate that the proposed mediational models can\nprovide unbiased and accurate point estimates with target coverage\nprobabilities with a 95% confidence interval. To illustrate modeling\nprocedures, we analyze empirical longitudinal records of multiple disciplinary\nsubjects, including reading, mathematics, and science test scores, from Grade K\nto Grade 5. The empirical analyses demonstrate that the proposed model can\nestimate covariates' direct and indirect effects on the change of the outcome.\nThrough the real-world data analyses, we also provide a set of feasible\nrecommendations for empirical researchers. We also provide the corresponding\ncode for the proposed models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 01:19:17 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Liu", "Jin", ""], ["Perera", "Robert A.", ""]]}, {"id": "2107.08353", "submitter": "Chirag Gupta", "authors": "Chirag Gupta and Aaditya K. Ramdas", "title": "Top-label calibration", "comments": "33 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of post-hoc calibration for multiclass classification,\nwith an emphasis on histogram binning. Multiple works have focused on\ncalibration with respect to the confidence of just the predicted class (or\n'top-label'). We find that the popular notion of confidence calibration [Guo et\nal., 2017] is not sufficiently strong -- there exist predictors that are not\ncalibrated in any meaningful way but are perfectly confidence calibrated. We\npropose a closely related (but subtly different) notion, top-label calibration,\nthat accurately captures the intuition and simplicity of confidence\ncalibration, but addresses its drawbacks. We formalize a histogram binning (HB)\nalgorithm that reduces top-label multiclass calibration to the binary case,\nprove that it has clean theoretical guarantees without distributional\nassumptions, and perform a methodical study of its practical performance. Some\nprediction tasks require stricter notions of multiclass calibration such as\nclass-wise or canonical calibration. We formalize appropriate HB algorithms\ncorresponding to each of these goals. In experiments with deep neural nets, we\nfind that our principled versions of HB are often better than temperature\nscaling, for both top-label and class-wise calibration. Code for this work will\nbe made publicly available at https://github.com/aigen/df-posthoc-calibration.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 03:27:50 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gupta", "Chirag", ""], ["Ramdas", "Aaditya K.", ""]]}, {"id": "2107.08359", "submitter": "Yuan Ke", "authors": "Wenxuan Zhong, Yuan Ke, Ye Wang, Yongkai Chen, Jinyang Chen, Ping Ma", "title": "Best Subset Selection: Statistical Computing Meets Quantum Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of quantum computers, quantum algorithms have been\nstudied extensively. However, quantum algorithms tackling statistical problems\nare still lacking. In this paper, we propose a novel non-oracular quantum\nadaptive search (QAS) method for the best subset selection problems. QAS\nperforms almost identically to the naive best subset selection method but\nreduces its computational complexity from $O(D)$ to $O(\\sqrt{D}\\log_2D)$, where\n$D=2^p$ is the total number of subsets over $p$ covariates. Unlike existing\nquantum search algorithms, QAS does not require the oracle information of the\ntrue solution state and hence is applicable to various statistical learning\nproblems with random observations. Theoretically, we prove QAS attains any\narbitrary success probability $q \\in (0.5, 1)$ within $O(\\log_2D)$ iterations.\nWhen the underlying regression model is linear, we propose a quantum linear\nprediction method that is faster than its classical counterpart. We further\nintroduce a hybrid quantum-classical strategy to avoid the capacity bottleneck\nof existing quantum computing systems and boost the success probability of QAS\nby majority voting. The effectiveness of this strategy is justified by both\ntheoretical analysis and extensive empirical experiments on quantum and\nclassical computers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 04:19:11 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhong", "Wenxuan", ""], ["Ke", "Yuan", ""], ["Wang", "Ye", ""], ["Chen", "Yongkai", ""], ["Chen", "Jinyang", ""], ["Ma", "Ping", ""]]}, {"id": "2107.08380", "submitter": "Mar\\'ia Fernanda Gil-Leyva Villa", "authors": "Pierpaolo De Blasi and Mar\\'ia F. Gil-Leyva", "title": "Gibbs sampling for mixtures in order of appearance: the ordered\n  allocation sampler", "comments": "36 pages, 14 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs sampling methods for mixture models are based on data augmentation\nschemes that account for the unobserved partition in the data. Conditional\nsamplers rely on allocation variables that identify each observation with a\nmixture component. They are known to suffer from slow mixing in infinite\nmixtures, where some form of truncation, either deterministic or random, is\nrequired. In mixtures with random number of components, the exploration of\nparameter spaces of different dimensions can also be challenging. We tackle\nthese issues by expressing the mixture components in the random order of\nappearance in an exchangeable sequence directed by the mixing distribution. We\nderive a sampler that is straightforward to implement for mixing distributions\nwith tractable size-biased ordered weights. In infinite mixtures, no form of\ntruncation is necessary. As for finite mixtures with random dimension, a simple\nupdating of the number of components is obtained by a blocking argument, thus,\neasing challenges found in trans-dimensional moves via Metropolis-Hasting\nsteps. Additionally, the latent clustering structure of the model is encrypted\nby means of an ordered partition with blocks labelled in the least element\norder, which mitigates the label-switching problem. We illustrate through a\nsimulation study the good mixing performance of the new sampler.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 07:16:43 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["De Blasi", "Pierpaolo", ""], ["Gil-Leyva", "Mar\u00eda F.", ""]]}, {"id": "2107.08529", "submitter": "Min Tsao Dr.", "authors": "Min Tsao", "title": "Regression model selection via log-likelihood ratio and constrained\n  minimum criterion", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although the log-likelihood is widely used in model selection, the\nlog-likelihood ratio has had few applications in this area. We develop a\nlog-likelihood ratio based method for selecting regression models by focusing\non the set of models deemed plausible by the likelihood ratio test. We show\nthat when the sample size is large and the significance level of the test is\nsmall, there is a high probability that the smallest model in the set is the\ntrue model; thus, we select this smallest model. The significance level serves\nas a parameter for this method. We consider three levels of this parameter in a\nsimulation study and compare this method with the Akaike Information Criterion\nand Bayesian Information Criterion to demonstrate its excellent accuracy and\nadaptability to different sample sizes. We also apply this method to select a\nlogistic regression model for a South African heart disease dataset.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 19:59:24 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Tsao", "Min", ""]]}, {"id": "2107.08533", "submitter": "Fei Zhou", "authors": "Fei Zhou, Xi Lu, Jie Ren, Kun Fan, Shuangge Ma, Cen Wu", "title": "Sparse group variable selection for gene-environment interactions in the\n  longitudinal study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized variable selection for high dimensional longitudinal data has\nreceived much attention as accounting for the correlation among repeated\nmeasurements and providing additional and essential information for improved\nidentification and prediction performance. Despite the success, in longitudinal\nstudies the potential of penalization methods is far from fully understood for\naccommodating structured sparsity. In this article, we develop a sparse group\npenalization method to conduct the bi-level gene-environment (G$\\times$E)\ninteraction study under the repeatedly measured phenotype. Within the quadratic\ninference function (QIF) framework, the proposed method can achieve\nsimultaneous identification of main and interaction effects on both the group\nand individual level. Simulation studies have shown that the proposed method\noutperforms major competitors. In the case study of asthma data from the\nChildhood Asthma Management Program (CAMP), we conduct G$\\times$E study by\nusing high dimensional SNP data as the Genetic factor and the longitudinal\ntrait, forced expiratory volume in one second (FEV1), as phenotype. Our method\nleads to improved prediction and identification of main and interaction effects\nwith important implications.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 20:23:02 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhou", "Fei", ""], ["Lu", "Xi", ""], ["Ren", "Jie", ""], ["Fan", "Kun", ""], ["Ma", "Shuangge", ""], ["Wu", "Cen", ""]]}, {"id": "2107.08558", "submitter": "Duligur Ibeling", "authors": "Duligur Ibeling, Thomas Icard", "title": "A Topological Perspective on Causal Inference", "comments": "ICML 2021 NACI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a topological learning-theoretic perspective on causal\ninference by introducing a series of topologies defined on general spaces of\nstructural causal models (SCMs). As an illustration of the framework we prove a\ntopological causal hierarchy theorem, showing that substantive assumption-free\ncausal inference is possible only in a meager set of SCMs. Thanks to a known\ncorrespondence between open sets in the weak topology and statistically\nverifiable hypotheses, our results show that inductive assumptions sufficient\nto license valid causal inferences are statistically unverifiable in principle.\nSimilar to no-free-lunch theorems for statistical inference, the present\nresults clarify the inevitability of substantial assumptions for causal\ninference. An additional benefit of our topological approach is that it easily\naccommodates SCMs with infinitely many variables. We finally suggest that the\nframework may be helpful for the positive project of exploring and assessing\nalternative causal-inductive assumptions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 23:09:03 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ibeling", "Duligur", ""], ["Icard", "Thomas", ""]]}, {"id": "2107.08595", "submitter": "Xiaowei Zhang", "authors": "Liang Ding, Rui Tuo, Xiaowei Zhang", "title": "High-Dimensional Simulation Optimization via Brownian Fields and Sparse\n  Grids", "comments": "Main body: 36 pages, 7 figures, 2 tables. Supplemental material: 32\n  pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  High-dimensional simulation optimization is notoriously challenging. We\npropose a new sampling algorithm that converges to a global optimal solution\nand suffers minimally from the curse of dimensionality. The algorithm consists\nof two stages. First, we take samples following a sparse grid experimental\ndesign and approximate the response surface via kernel ridge regression with a\nBrownian field kernel. Second, we follow the expected improvement strategy --\nwith critical modifications that boost the algorithm's sample efficiency -- to\niteratively sample from the next level of the sparse grid. Under mild\nconditions on the smoothness of the response surface and the simulation noise,\nwe establish upper bounds on the convergence rate for both noise-free and noisy\nsimulation samples. These upper bounds deteriorate only slightly in the\ndimension of the feasible set, and they can be improved if the objective\nfunction is known to be of a higher-order smoothness. Extensive numerical\nexperiments demonstrate that the proposed algorithm dramatically outperforms\ntypical alternatives in practice.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 03:03:27 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 01:57:21 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Ding", "Liang", ""], ["Tuo", "Rui", ""], ["Zhang", "Xiaowei", ""]]}, {"id": "2107.08724", "submitter": "Tengyao Wang", "authors": "Hanqing Cai and Tengyao Wang", "title": "Estimation of high-dimensional change-points under a group sparsity\n  structure", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Change-points are a routine feature of 'big data' observed in the form of\nhigh-dimensional data streams. In many such data streams, the component series\npossess group structures and it is natural to assume that changes only occur in\na small number of all groups. We propose a new change point procedure, called\n'groupInspect', that exploits the group sparsity structure to estimate a\nprojection direction so as to aggregate information across the component series\nto successfully estimate the change-point in the mean structure of the series.\nWe prove that the estimated projection direction is minimax optimal, up to\nlogarithmic factors, when all group sizes are of comparable order. Moreover,\nour theory provide strong guarantees on the rate of convergence of the\nchange-point location estimator. Numerical studies demonstrates the competitive\nperformance of groupInspect in a wide range of settings and a real data example\nconfirms the practical usefulness of our procedure.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:51:57 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Cai", "Hanqing", ""], ["Wang", "Tengyao", ""]]}, {"id": "2107.08860", "submitter": "Arne Kramer-Sunderbrink", "authors": "Robin Tim Dreher, Leona Hoffmann, Arne Kramer-Sunderbrink, Peter\n  P\\\"utz, Robin Werner", "title": "A Proposed Hybrid Effect Size Plus $p$-Value Criterion: A Replication of\n  Goodman et al. (2019)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent simulation study, Goodman et al. (2019) compare several methods\nwith regard to their performance of type I and type II error rates in case of a\nthick null hypothesis that includes all values that are practically equivalent\nto the point null hypothesis. They propose a hybrid decision criterion only\ndeclaring a result \"significant\" if both a small $p$-value and a sufficiently\nlarge effect size are obtained. We successfully replicate the results using our\nown software code in R and discuss an additional decision method that maintains\na pre-defined false positive rate. We confirm that the hybrid decision\ncriterion has comparably low error rates in settings one can check for but\npoint out that the false discovery rate cannot be easily controlled by the\nresearcher. Our analyses are readily accessible and customizable on\nhttps://github.com/drehero/goodman-replication.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 13:22:38 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Dreher", "Robin Tim", ""], ["Hoffmann", "Leona", ""], ["Kramer-Sunderbrink", "Arne", ""], ["P\u00fctz", "Peter", ""], ["Werner", "Robin", ""]]}, {"id": "2107.08950", "submitter": "Silvia De Nicol\\`o", "authors": "Silvia De Nicol\\`o and Maria Rosaria Ferrante and Silvia Pacei", "title": "Mind the Income Gap: Behavior of Inequality Estimators from Complex\n  Survey Small Samples", "comments": "29 pages, 5 figures. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Income inequality measures are biased in small samples leading generally to\nan underestimation. After investigating the nature of the bias, we propose a\nbias-correction framework for a large class of inequality measures comprising\nGini Index, Generalized Entropy and Atkinson families by accounting for complex\nsurvey designs. The proposed methodology is based on Taylor's expansions and\nGeneralized Linearization Method, and does not require any parametric\nassumption on income distribution, being very flexible. Design-based\nperformance evaluation of the suggested correction has been carried out using\ndata taken from EU-SILC survey. Results show a noticeable bias reduction for\nall measures. A bootstrap variance estimation proposal and a distributional\nanalysis follow in order to provide a comprehensive overview of the behavior of\ninequality estimators in small samples. Results about estimators distributions\nshow increasing positive skewness and leptokurtosis at decreasing sample sizes,\nconfirming the non-applicability of classical asymptotic results in small\nsamples and suggesting the development of alternative methods of inference.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 15:06:26 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 08:33:41 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["De Nicol\u00f2", "Silvia", ""], ["Ferrante", "Maria Rosaria", ""], ["Pacei", "Silvia", ""]]}, {"id": "2107.09009", "submitter": "Andreas Markoulidakis Mr", "authors": "Andreas Markoulidakis, Peter Holmans, Philip Pallmann, Monica Busse,\n  Beth Ann Griffin", "title": "How balance and sample size impact bias in the estimation of causal\n  treatment effects: A simulation study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Observational studies are often used to understand relationships between\nexposures and outcomes. They do not, however, allow conclusions about causal\nrelationships to be drawn unless statistical techniques are used to account for\nthe imbalance of confounders across exposure groups. Propensity score and\nbalance weighting (PSBW) are useful techniques that aim to reduce the\nimbalances between exposure groups by weighting the groups to look alike on the\nobserved confounders. Despite the plethora of available methods to estimate\nPSBW, there is little guidance on what one defines as adequate balance, and\nunbiased and robust estimation of the causal treatment effect is not guaranteed\nunless several conditions hold. Accurate inference requires that 1. the\ntreatment allocation mechanism is known, 2. the relationship between the\nbaseline covariates and the outcome is known, 3. adequate balance of baseline\ncovariates is achieved post-weighting, 4. a proper set of covariates to control\nfor confounding bias is known, and 5. a large enough sample size is available.\nIn this article, we use simulated data of various sizes to investigate the\ninfluence of these five factors on statistical inference. Our findings provide\nevidence that the maximum Kolmogorov- Smirnov statistic is the proper\nstatistical measure to assess balance on the baseline covariates, in contrast\nto the mean standardised mean difference used in many applications, and 0.1 is\na suitable threshold to consider as acceptable balance. Finally, we recommend\nthat 60-80 observations, per confounder per treatment group, are required to\nobtain a reliable and unbiased estimation of the causal treatment effect.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 16:38:38 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Markoulidakis", "Andreas", ""], ["Holmans", "Peter", ""], ["Pallmann", "Philip", ""], ["Busse", "Monica", ""], ["Griffin", "Beth Ann", ""]]}, {"id": "2107.09150", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul and George Michailidis", "title": "Inference for Change Points in High Dimensional Mean Shift Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider the problem of constructing confidence intervals for the\nlocations of change points in a high-dimensional mean shift model. To that end,\nwe develop a locally refitted least squares estimator and obtain component-wise\nand simultaneous rates of estimation of the underlying change points. The\nsimultaneous rate is the sharpest available in the literature by at least a\nfactor of $\\log p,$ while the component-wise one is optimal. These results\nenable existence of limiting distributions. Component-wise distributions are\ncharacterized under both vanishing and non-vanishing jump size regimes, while\njoint distributions for any finite subset of change point estimates are\ncharacterized under the latter regime, which also yields asymptotic\nindependence of these estimates. The combined results are used to construct\nasymptotically valid component-wise and simultaneous confidence intervals for\nthe change point parameters. The results are established under a high\ndimensional scaling, allowing for diminishing jump sizes, in the presence of\ndiverging number of change points and under subexponential errors. They are\nillustrated on synthetic data and on sensor measurements from smartphones for\nactivity recognition.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 20:56:15 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Kaul", "Abhishek", ""], ["Michailidis", "George", ""]]}, {"id": "2107.09194", "submitter": "William Stephenson", "authors": "William T. Stephenson and Zachary Frangella and Madeleine Udell and\n  Tamara Broderick", "title": "Can we globally optimize cross-validation loss? Quasiconvexity in ridge\n  regression", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models like LASSO and ridge regression are extensively used in practice due\nto their interpretability, ease of use, and strong theoretical guarantees.\nCross-validation (CV) is widely used for hyperparameter tuning in these models,\nbut do practical optimization methods minimize the true out-of-sample loss? A\nrecent line of research promises to show that the optimum of the CV loss\nmatches the optimum of the out-of-sample loss (possibly after simple\ncorrections). It remains to show how tractable it is to minimize the CV loss.\nIn the present paper, we show that, in the case of ridge regression, the CV\nloss may fail to be quasiconvex and thus may have multiple local optima. We can\nguarantee that the CV loss is quasiconvex in at least one case: when the\nspectrum of the covariate matrix is nearly flat and the noise in the observed\nresponses is not too high. More generally, we show that quasiconvexity status\nis independent of many properties of the observed data (response norm,\ncovariate-matrix right singular vectors and singular-value scaling) and has a\ncomplex dependence on the few that remain. We empirically confirm our theory\nusing simulated experiments.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 23:22:24 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Stephenson", "William T.", ""], ["Frangella", "Zachary", ""], ["Udell", "Madeleine", ""], ["Broderick", "Tamara", ""]]}, {"id": "2107.09218", "submitter": "Jianing Fan", "authors": "Jianing Fan and Hans-Georg M\\\"uller", "title": "Conditional Wasserstein Barycenters and Interpolation/Extrapolation of\n  Distributions", "comments": "42 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly complex data analysis tasks motivate the study of the dependency\nof distributions of multivariate continuous random variables on scalar or\nvector predictors. Statistical regression models for distributional responses\nso far have primarily been investigated for the case of one-dimensional\nresponse distributions. We investigate here the case of multivariate response\ndistributions while adopting the 2-Wasserstein metric in the distribution\nspace. The challenge is that unlike the situation in the univariate case, the\noptimal transports that correspond to geodesics in the space of distributions\nwith the 2-Wasserstein metric do not have an explicit representation for\nmultivariate distributions. We show that under some regularity assumptions the\nconditional Wasserstein barycenters constructed for a geodesic in the Euclidean\npredictor space form a corresponding geodesic in the Wasserstein distribution\nspace and demonstrate how the notion of conditional barycenters can be\nharnessed to interpolate as well as extrapolate multivariate distributions. The\nutility of distributional inter- and extrapolation is explored in simulations\nand examples. We study both global parametric-like and local smoothing-like\nmodels to implement conditional Wasserstein barycenters and establish\nasymptotic convergence properties for the corresponding estimates. For\nalgorithmic implementation we make use of a Sinkhorn entropy-penalized\nalgorithm. Conditional Wasserstein barycenters and distribution extrapolation\nare illustrated with applications in climate science and studies of aging.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 01:25:05 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Fan", "Jianing", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "2107.09399", "submitter": "Hanfang Hsueh", "authors": "Han-Fang Hsueh, Anneli Guthke, Thomas W\\\"ohling, Wolfgang Nowak", "title": "Diagnosis of model-structural errors with a sliding time-window Bayesian\n  analysis", "comments": "58 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deterministic hydrological models with uncertain, but\ninferred-to-be-time-invariant parameters typically show time-dependent model\nstructural errors. Such errors can occur if a hydrological process is active in\ncertain time periods in nature, but is not resolved by the model. Such missing\nprocesses could become visible during calibration as time-dependent best-fit\nvalues of model parameters. We propose a formal time-windowed Bayesian analysis\nto diagnose this type of model error, formalizing the question \\In which period\nof the calibration time-series does the model statistically disqualify itself\nas quasi-true?\" Using Bayesian model evidence (BME) as model performance\nmetric, we determine how much the data in time windows of the calibration\ntime-series support or refute the model. Then, we track BME over sliding time\nwindows to obtain a dynamic, time-windowed BME (tBME) and search for sudden\ndecreases that indicate an onset of model error. tBME also allows us to perform\na formal, sliding likelihood-ratio test of the model against the data. Our\nproposed approach is designed to detect error occurrence on various temporal\nscales, which is especially useful in hydrological modelling. We illustrate\nthis by applying our proposed method to soil moisture modeling. We test tBME as\nmodel error indicator on several synthetic and real-world test cases that we\ndesigned to vary in error sources and error time scales. Results prove the\nusefulness of the framework for detecting structural errors in dynamic models.\nMoreover, the time sequence of posterior parameter distributions helps to\ninvestigate the reasons for model error and provide guidance for model\nimprovement.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 10:38:39 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Hsueh", "Han-Fang", ""], ["Guthke", "Anneli", ""], ["W\u00f6hling", "Thomas", ""], ["Nowak", "Wolfgang", ""]]}, {"id": "2107.09538", "submitter": "Brian Bush", "authors": "Brian W. Bush, Joanne Wendelberger, Rebecca Hanes", "title": "Adaptively Sampling via Regional Variance-Based Sensitivities", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": "NREL/JA-6A20-78775", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the well-established variance-based methods for global\nsensitivity analysis, we develop a local total sensitivity index that\ndecomposes the global total sensitivity conditions by independent variables'\nvalues. We employ this local sensitivity index in a new method of experimental\ndesign that sequentially and adaptively samples the domain of a multivariate\nfunction according to local contributions to the global variance. The method is\ndemonstrated on a nonlinear illustrative example that has a three-dimensional\ndomain and a three-dimensional codomain, but also on a complex,\nhigh-dimensional simulation for assessing the industrial viability of the\nproduction of bioproducts from biomass.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 14:51:36 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Bush", "Brian W.", ""], ["Wendelberger", "Joanne", ""], ["Hanes", "Rebecca", ""]]}, {"id": "2107.09586", "submitter": "Claudia Di Caterina", "authors": "Claudia Di Caterina, Davide Ferrari", "title": "Sparse composite likelihood selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Composite likelihood has shown promise in settings where the number of\nparameters $p$ is large due to its ability to break down complex models into\nsimpler components, thus enabling inference even when the full likelihood is\nnot tractable. Although there are a number of ways to formulate a valid\ncomposite likelihood in the finite-$p$ setting, there does not seem to exist\nagreement on how to construct composite likelihoods that are comp utationally\nefficient and statistically sound when $p$ is allowed to diverge. This article\nintroduces a method to select sparse composite likelihoods by minimizing a\ncriterion representing the statistical efficiency of the implied estimator plus\nan $L_1$-penalty discouraging the inclusion of too many sub-likelihood terms.\nConditions under which consistent model selection occurs are studied. Examples\nillustrating the procedure are analysed in detail and applied to real data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 16:01:46 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Di Caterina", "Claudia", ""], ["Ferrari", "Davide", ""]]}, {"id": "2107.09629", "submitter": "Qianfan Wu", "authors": "Philip Protter, Qianfan Wu, Shihao Yang", "title": "Order Book Queue Hawkes-Markovian Modeling", "comments": "71 pages, 80 figures, submitted to Journal of American Statistical\n  Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article presents a Hawkes process model with Markovian baseline\nintensities for high-frequency order book data modeling. We classify intraday\norder book trading events into a range of categories based on their order types\nand the price changes after their arrivals. To capture the stimulating effects\nbetween multiple types of order book events, we use the multivariate Hawkes\nprocess to model the self- and mutually-exciting event arrivals. We also\nintegrate a Markovian baseline intensity into the event arrival dynamic, by\nincluding the impacts of order book liquidity state and time factor to the\nbaseline intensity. A regression-based non-parametric estimation procedure is\nadopted to estimate the model parameters in our Hawkes+Markovian model. To\neliminate redundant model parameters, LASSO regularization is incorporated in\nthe estimation procedure. Besides, model selection method based on Akaike\nInformation Criteria is applied to evaluate the effect of each part of the\nproposed model. An implementation example based on real LOB data is provided.\nThrough the example, we study the empirical shapes of Hawkes excitement\nfunctions, the effects of liquidity state as well as time factors, the LASSO\nvariable selection, and the explanatory power of Hawkes and Markovian elements\nto the dynamics of the order book.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 17:20:22 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 13:34:46 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Protter", "Philip", ""], ["Wu", "Qianfan", ""], ["Yang", "Shihao", ""]]}, {"id": "2107.09730", "submitter": "Liangyuan Hu", "authors": "Jung-Yi Joyce Lin, Liangyuan Hu, Chuyue Huang, Steven Lawrence, Usha\n  Govindarajulu", "title": "Strategies for variable selection in large-scale healthcare database\n  studies with missing covariate and outcome data", "comments": "18 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior work has shown that combining bootstrap imputation with tree-based\nmachine learning variable selection methods can recover the good performance\nachievable on fully observed data when covariate and outcome data are missing\nat random (MAR). This approach however is computationally expensive, especially\non large-scale datasets. We propose an inference-based method RR-BART, that\nleverages the likelihood-based Bayesian machine learning technique, Bayesian\nAdditive Regression Trees, and uses Rubin's rule to combine the estimates and\nvariances of the variable importance measures on multiply imputed datasets for\nvariable selection in the presence of missing data. A representative simulation\nstudy suggests that RR-BART performs at least as well as combining bootstrap\nwith BART, BI-BART, but offers substantial computational savings, even in\ncomplex conditions of nonlinearity and nonadditivity with a large percentage of\noverall missingness under the MAR mechanism. RR-BART is also less sensitive to\nthe end note prior via the hyperparameter $k$ than BI-BART, and does not depend\non the selection threshold value $\\pi$ as required by BI-BART. Our simulation\nstudies also suggest that encoding the missing values of a binary predictor as\na separate category significantly improves the power of selecting the binary\npredictor for BI-BART. We further demonstrate the methods via a case study of\nrisk factors for 3-year incidence of metabolic syndrome with data from the\nStudy of Women's Health Across the Nation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 19:01:24 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Lin", "Jung-Yi Joyce", ""], ["Hu", "Liangyuan", ""], ["Huang", "Chuyue", ""], ["Lawrence", "Steven", ""], ["Govindarajulu", "Usha", ""]]}, {"id": "2107.09749", "submitter": "Shanghong Xie", "authors": "Shanghong Xie, Wenbo Wang, Qinxia Wang, Yuanjia Wang, Donglin Zeng", "title": "Evaluating Effectiveness of Public Health Intervention Strategies for\n  Mitigating COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) pandemic is an unprecedented global\npublic health challenge. In the United States (US), state governments have\nimplemented various non-pharmaceutical interventions (NPIs), such as physical\ndistance closure (lockdown), stay-at-home order, mandatory facial mask in\npublic in response to the rapid spread of COVID-19. To evaluate the\neffectiveness of these NPIs, we propose a nested case-control design with\npropensity score weighting under the quasi-experiment framework to estimate the\naverage intervention effect on disease transmission across states. We further\ndevelop a method to test for factors that moderate intervention effect to\nassist precision public health intervention. Our method takes account of the\nunderlying dynamics of disease transmission and balance state-level\npre-intervention characteristics. We prove that our estimator provides causal\nintervention effect under assumptions. We apply this method to analyze US\nCOVID-19 incidence cases to estimate the effects of six interventions. We show\nthat lockdown has the largest effect on reducing transmission and reopening\nbars significantly increase transmission. States with a higher percentage of\nnon-white population are at greater risk of increased $R_t$ associated with\nreopening bars.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 20:07:50 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Xie", "Shanghong", ""], ["Wang", "Wenbo", ""], ["Wang", "Qinxia", ""], ["Wang", "Yuanjia", ""], ["Zeng", "Donglin", ""]]}, {"id": "2107.09757", "submitter": "Francisco Mois\\'es C\\^andido de Medeiros", "authors": "Joyce B. Rocha, Francisco M.C. Medeiros and Dione M. Valen\\c{c}a", "title": "Log-symmetric models with cure fraction with application to leprosy\n  reactions data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a log-symmetric survival model with cure fraction,\nconsidering that the distributions of lifetimes for susceptible individuals\nbelong to the log-symmetric class of distributions. This class has continuous,\nstrictly positive, and asymmetric distributions, including the log-normal,\nlog-$t$-Student, Birnbaum-Saunders, log-logistic I, log-logistic II,\nlog-normal-contaminated, log-exponential-power, and log-slash distributions.\nThe log-symmetric class is quite flexible and allows for including bimodal\ndistributions and outliers. This includes explanatory variables through the\nparameter associated with the cure fraction. We evaluate the performance of the\nproposed model through extensive simulation studies and consider a real data\napplication to evaluate the effect of factors on the immunity to leprosy\nreactions in patients with Hansen's disease.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 20:25:42 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Rocha", "Joyce B.", ""], ["Medeiros", "Francisco M. C.", ""], ["Valen\u00e7a", "Dione M.", ""]]}, {"id": "2107.09765", "submitter": "Jaime Sevilla", "authors": "Jaime Sevilla and Alexandra Mayn", "title": "A conditional independence test for causality in econometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Y-test is a useful tool for detecting missing confounders in the context\nof a multivariate regression.However, it is rarely used in practice since it\nrequires identifying multiple conditionally independent instruments, which is\noften impossible. We propose a heuristic test which relaxes the independence\nrequirement. We then show how to apply this heuristic test on a price-demand\nand a firm loan-productivity problem. We conclude that the test is informative\nwhen the variables are linearly related with Gaussian additive noise, but it\ncan be misleading in other contexts. Still, we believe that the test can be a\nuseful concept for falsifying a proposed control set.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 07:35:13 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Sevilla", "Jaime", ""], ["Mayn", "Alexandra", ""]]}, {"id": "2107.09810", "submitter": "Suvra Pal", "authors": "Sandip Barui and Suvra Pal and Nutan Mishra and Katherine Davies", "title": "A Stochastic Version of the EM Algorithm for Mixture Cure Rate Model\n  with Exponentiated Weibull Family of Lifetimes", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handling missing values plays an important role in the analysis of survival\ndata, especially, the ones marked by cure fraction. In this paper, we discuss\nthe properties and implementation of stochastic approximations to the\nexpectation-maximization (EM) algorithm to obtain maximum likelihood (ML) type\nestimates in situations where missing data arise naturally due to right\ncensoring and a proportion of individuals are immune to the event of interest.\nA flexible family of three parameter exponentiated-Weibull (EW) distributions\nis assumed to characterize lifetimes of the non-immune individuals as it\naccommodates both monotone (increasing and decreasing) and non-monotone\n(unimodal and bathtub) hazard functions. To evaluate the performance of the SEM\nalgorithm, an extensive simulation study is carried out under various parameter\nsettings. Using likelihood ratio test we also carry out model discrimination\nwithin the EW family of distributions. Furthermore, we study the robustness of\nthe SEM algorithm with respect to outliers and algorithm starting values. Few\nscenarios where stochastic EM (SEM) algorithm outperforms the well-studied EM\nalgorithm are also examined in the given context. For further demonstration, a\nreal survival data on cutaneous melanoma is analyzed using the proposed cure\nrate model with EW lifetime distribution and the proposed estimation technique.\nThrough this data, we illustrate the applicability of the likelihood ratio test\ntowards rejecting several well-known lifetime distributions that are nested\nwithin the wider class of EW distributions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 23:55:14 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Barui", "Sandip", ""], ["Pal", "Suvra", ""], ["Mishra", "Nutan", ""], ["Davies", "Katherine", ""]]}, {"id": "2107.09812", "submitter": "John Kidd", "authors": "John Kidd and Dan-Yu Lin", "title": "Improving the Power to Detect Indirect Effects in Mediation Analysis", "comments": "15 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis seeks to determine whether an independent variable\naffects a response variable directly or whether it does so indirectly, by way\nof a mediator. The existing statistical tests to determine the existence of an\nindirect effect are overly conservative or have inflated type I error. In this\narticle, we propose two methods based on the principle of intersection-union\ntests that offer improvements in power while controlling the type I error. We\ndemonstrate the advantages of the proposed methods through extensive\nsimulation. Finally, we provide an application to a large proteomic study.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 00:02:41 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Kidd", "John", ""], ["Lin", "Dan-Yu", ""]]}, {"id": "2107.10017", "submitter": "Samuel Watson", "authors": "Samuel I Watson, Joshua Akinyemi, Karla Hemming", "title": "Frequentist inference for cluster randomised trials with multiple\n  primary outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of a single primary outcome is generally either recommended or\nrequired by many influential randomised trial guidelines to avoid the problem\nof \"multiple testing\". Without correction, the probability of rejecting at\nleast one of a set of null hypotheses (the family-wise error rate) is often\nmuch greater than the nominal rate of any single test so that statistics like\np-values and confidence intervals have no reliable interpretation. Cluster\nrandomised trials though may require multiple outcomes to adequately describe\nthe effects of often complex and multi-faceted interventions. We propose a\nmethod for inference for cluster randomised trials with multiple outcomes that\nensures a nominal family-wise error rate and produces simultaneous confidence\nintervals with nominal \"family-wise\" coverage. We adapt the resampling-based\nstepdown procedure of Romano and Wolf (2005) using a randomisation-test\napproach within a generalised linear model framework. We then adapt the\nRobbins-Monro search procedure for confidence interval limits proposed by\nGarthwaite and Buckland (1996) to this stepdown process to produce a set of\nconfidence intervals. We show that this procedure has nominal error rates and\ncoverage in a simulation-based study of parallel and stepped-wedge cluster\nrandomised studies and compare results from the analysis of a real-world\nstepped-wedge trial under both the proposed and more standard analyses.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 11:27:15 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Watson", "Samuel I", ""], ["Akinyemi", "Joshua", ""], ["Hemming", "Karla", ""]]}, {"id": "2107.10148", "submitter": "Jingyu Ji", "authors": "Jingyu Ji, Deyuan Li and Zhengjun Zhang", "title": "Decoupling Systemic Risk into Endopathic and Exopathic Competing Risks\n  Through Autoregressive Conditional Accelerated Fr\\'echet Model", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying systemic risk patterns in geopolitical, economic, financial,\nenvironmental, transportation, epidemiological systems, and their impacts is\nthe key to risk management. This paper introduces two new endopathic and\nexopathic competing risks. The paper integrates the new extreme value theory\nfor maxima of maxima and the autoregressive conditional Fr\\'echet model for\nsystemic risk into a new autoregressive conditional accelerated Fr\\'echet\n(AcAF) model, which enables decoupling systemic risk into endopathic and\nexopathic competing risks. The paper establishes the probabilistic properties\nof stationarity and ergodicity of the AcAF model. Statistical inference is\ndeveloped through conditional maximum likelihood estimation. The consistency\nand asymptotic normality of the estimators are derived. Simulation demonstrates\nthe efficiency of the proposed estimators and the AcAF model's flexibility in\nmodeling heterogeneous data. Empirical studies on the stock returns in S\\&P 500\nand the cryptocurrency trading show the superior performance of the proposed\nmodel in terms of the identified risk patterns, endopathic and exopathic\ncompeting risks, being informative with greater interpretability, enhancing the\nunderstanding of the systemic risks of a market and their causes, and making\nbetter risk management possible.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 15:25:23 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Ji", "Jingyu", ""], ["Li", "Deyuan", ""], ["Zhang", "Zhengjun", ""]]}, {"id": "2107.10175", "submitter": "Vivekananda Roy", "authors": "Run Wang, Somak Dutta, and Vivekananda Roy", "title": "Bayesian iterative screening in ultra-high dimensional settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection in ultra-high dimensional linear regression is often\npreceded by a screening step to significantly reduce the dimension. Here a\nBayesian variable screening method (BITS) is developed. BITS can successfully\nintegrate prior knowledge, if any, on effect sizes, and the number of true\nvariables. BITS iteratively includes potential variables with the highest\nposterior probability accounting for the already selected variables. It is\nimplemented by a fast Cholesky update algorithm and is shown to have the\nscreening consistency property. BITS is built based on a model with Gaussian\nerrors, yet, the screening consistency is proved to hold under more general\ntail conditions. The notion of posterior screening consistency allows the\nresulting model to provide a good starting point for further Bayesian variable\nselection methods. A new screening consistent stopping rule based on posterior\nprobability is developed. Simulation studies and real data examples are used to\ndemonstrate scalability and fine screening performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 16:01:13 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Wang", "Run", ""], ["Dutta", "Somak", ""], ["Roy", "Vivekananda", ""]]}, {"id": "2107.10365", "submitter": "Helton Saulo", "authors": "Alan Dasilva and Helton Saulo", "title": "Scale-mixture Birnbaum-Saunders quantile regression models applied to\n  personal accident insurance data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modeling of personal accident insurance data has been a topic of extreme\nrelevance in the insurance literature. In general, the data often exhibit\npositive asymmetry and heavy tails and non-quantile Birnbaum-Saunders\nregression models have been used in the modeling strategy. In this work, we\npropose a new quantile regression model based on the scale-mixture\nBirnbaum-Saunders distribution, which is reparametrized by inserting a quantile\nparameter. The maximum likelihood estimates of the model parameters are\nobtained via the EM algorithm. Two Monte Carlo simulation studies were\nperformed using the \\texttt{R} software. The first study aims to analyze the\nperformance of the maximum likelihood estimates, the information criteria AIC,\nAICc, BIC, HIC, the root of the mean square error, and the randomized quantile\nand generalized Cox-Snell residuals. In the second simulation study, the size\nand power of the the Wald, likelihood ratio, score and gradient tests are\nevaluated. The two simulation studies were conducted considering different\nquantiles of interest and sample sizes. Finally, a real insurance data set is\nanalyzed to illustrate the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 21:50:41 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Dasilva", "Alan", ""], ["Saulo", "Helton", ""]]}, {"id": "2107.10397", "submitter": "Mohammadhossein Toutiaee", "authors": "Mohammadhossein Toutiaee, Xiaochuan Li, Yogesh Chaudhari, Shophine\n  Sivaraja, Aishwarya Venkataraj, Indrajeet Javeri, Yuan Ke, Ismailcem Arpinar,\n  Nicole Lazar, John Miller", "title": "Improving COVID-19 Forecasting using eXogenous Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the pandemic course in the United States by\nconsidering national and state levels data. We propose and compare multiple\ntime-series prediction techniques which incorporate auxiliary variables. One\ntype of approach is based on spatio-temporal graph neural networks which\nforecast the pandemic course by utilizing a hybrid deep learning architecture\nand human mobility data. Nodes in this graph represent the state-level deaths\ndue to COVID-19, edges represent the human mobility trend and temporal edges\ncorrespond to node attributes across time. The second approach is based on a\nstatistical technique for COVID-19 mortality prediction in the United States\nthat uses the SARIMA model and eXogenous variables. We evaluate these\ntechniques on both state and national levels COVID-19 data in the United States\nand claim that the SARIMA and MCP models generated forecast values by the\neXogenous variables can enrich the underlying model to capture complexity in\nrespectively national and state levels data. We demonstrate significant\nenhancement in the forecasting accuracy for a COVID-19 dataset, with a maximum\nimprovement in forecasting accuracy by 64.58% and 59.18% (on average) over the\nGCN-LSTM model in the national level data, and 58.79% and 52.40% (on average)\nover the GCN-LSTM model in the state level data. Additionally, our proposed\nmodel outperforms a parallel study (AUG-NN) by 27.35% improvement of accuracy\non average.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 03:26:18 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Toutiaee", "Mohammadhossein", ""], ["Li", "Xiaochuan", ""], ["Chaudhari", "Yogesh", ""], ["Sivaraja", "Shophine", ""], ["Venkataraj", "Aishwarya", ""], ["Javeri", "Indrajeet", ""], ["Ke", "Yuan", ""], ["Arpinar", "Ismailcem", ""], ["Lazar", "Nicole", ""], ["Miller", "John", ""]]}, {"id": "2107.10505", "submitter": "Alexandre Hippert-Ferrer", "authors": "Alexandre Hippert-Ferrer (1), Mohammed Nabil El Korso (2), Arnaud\n  Breloy (2) and Guillaume Ginolhac (3) ((1) L2S, Paris-Saclay University,\n  Paris, France, (2) LEME, Paris Nanterre University, Paris, France, (3)\n  LISTIC, Savoie Mont Blanc University, Annecy, France)", "title": "Robust low-rank covariance matrix estimation with a general pattern of\n  missing values", "comments": "32 pages, 10 figures, submitted to Elsevier Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper tackles the problem of robust covariance matrix estimation when\nthe data is incomplete. Classical statistical estimation methodologies are\nusually built upon the Gaussian assumption, whereas existing robust estimation\nones assume unstructured signal models. The former can be inaccurate in\nreal-world data sets in which heterogeneity causes heavy-tail distributions,\nwhile the latter does not profit from the usual low-rank structure of the\nsignal. Taking advantage of both worlds, a covariance matrix estimation\nprocedure is designed on a robust (compound Gaussian) low-rank model by\nleveraging the observed-data likelihood function within an\nexpectation-maximization algorithm. It is also designed to handle general\npattern of missing values. The proposed procedure is first validated on\nsimulated data sets. Then, its interest for classification and clustering\napplications is assessed on two real data sets with missing values, which\ninclude multispectral and hyperspectral time series.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 07:55:25 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Hippert-Ferrer", "Alexandre", ""], ["Korso", "Mohammed Nabil El", ""], ["Breloy", "Arnaud", ""], ["Ginolhac", "Guillaume", ""]]}, {"id": "2107.10572", "submitter": "Ines Wilms", "authors": "Ines Wilms, Rebecca Killick and David S. Matteson", "title": "Graphical Influence Diagnostics for Changepoint Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Changepoint models enjoy a wide appeal in a variety of disciplines to model\nthe heterogeneity of ordered data. Graphical influence diagnostics to\ncharacterize the influence of single observations on changepoint models are,\nhowever, lacking. We address this gap by developing a framework for\ninvestigating instabilities in changepoint segmentations and assessing the\ninfluence of single observations on various outputs of a changepoint analysis.\nWe construct graphical diagnostic plots that allow practitioners to assess\nwhether instabilities occur; how and where they occur; and to detect\ninfluential individual observations triggering instability. We analyze well-log\ndata to illustrate how such influence diagnostic plots can be used in practice\nto reveal features of the data that may otherwise remain hidden.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 10:54:49 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Wilms", "Ines", ""], ["Killick", "Rebecca", ""], ["Matteson", "David S.", ""]]}, {"id": "2107.10885", "submitter": "Yanbo Tang", "authors": "Yanbo Tang and Nancy Reid", "title": "Laplace and Saddlepoint Approximations in High Dimensions", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We examine the behaviour of the Laplace and saddlepoint approximations in the\nhigh-dimensional setting, where the dimension of the model is allowed to\nincrease with the number of observations. Approximations to the joint density,\nthe marginal posterior density and the conditional density are considered. Our\nresults show that under the mildest assumptions on the model, the error of the\njoint density approximation is $O(p^4/n)$ if $p = o(n^{1/4})$ for the Laplace\napproximation and saddlepoint approximation, with improvements being possible\nunder additional assumptions. Stronger results are obtained for the\napproximation to the marginal posterior density.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 19:04:02 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Tang", "Yanbo", ""], ["Reid", "Nancy", ""]]}, {"id": "2107.10911", "submitter": "Arjun Sondhi", "authors": "Arjun Sondhi", "title": "Estimating survival parameters under conditionally independent left\n  truncation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  EHR-derived databases are commonly subject to left truncation, a type of\nselection bias induced due to patients needing to survive long enough to\nsatisfy certain entry criteria. Standard methods to adjust for left truncation\nbias rely on an assumption of marginal independence between entry and survival\ntimes, which may not always be satisfied in practice. In this work, we examine\nhow a weaker assumption of conditional independence can result in unbiased\nestimation of common statistical parameters. In particular, we show the\nestimability of conditional parameters in a truncated dataset, and of marginal\nparameters that leverage reference data containing non-truncated data on\nconfounders. The latter is complementary to observational causal inference\nmethodology applied to real world external comparators, which is a common use\ncase for real world databases. We implement our proposed methods in simulation\nstudies, demonstrating unbiased estimation and valid statistical inference. We\nalso illustrate estimation of a survival distribution under conditionally\nindependent left truncation in a real world clinico-genomic database.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 20:14:57 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Sondhi", "Arjun", ""]]}, {"id": "2107.10947", "submitter": "Nhat Ho", "authors": "Nhat Ho and Stephen G. Walker", "title": "On Integral Theorems: Monte Carlo Estimators and Optimal Functions", "comments": "18 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2106.06608", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.CA stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of integral theorems based on cyclic functions and\nRiemann sums approximating integrals theorem. The Fourier integral theorem,\nderived as a combination of a transform and inverse transform, arises as a\nspecial case. The integral theorems provide natural estimators of density\nfunctions via Monte Carlo integration. Assessments of the quality of the\ndensity estimators can be used to obtain optimal cyclic functions which\nminimize square integrals. Our proof techniques rely on a variational approach\nin ordinary differential equations and the Cauchy residue theorem in complex\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 22:25:21 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ho", "Nhat", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2107.10955", "submitter": "Xiaodong Li", "authors": "Xingmei Lou, Yu Hu, Xiaodong Li", "title": "Linear Polytree Structural Equation Models: Structural Learning and\n  Inverse Correlation Estimation", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in the problem of learning the directed acyclic graph (DAG)\nwhen data are generated from a linear structural equation model (SEM) and the\ncausal structure can be characterized by a polytree. Specially, under both\nGaussian and sub-Gaussian models, we study the sample size conditions for the\nwell-known Chow-Liu algorithm to exactly recover the equivalence class of the\npolytree, which is uniquely represented by a CPDAG. We also study the error\nrate for the estimation of the inverse correlation matrix under such models.\nOur theoretical findings are illustrated by comprehensive numerical\nsimulations, and experiments on benchmark data also demonstrate the robustness\nof the method when the ground truth graphical structure can only be\napproximated by a polytree.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 23:22:20 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Lou", "Xingmei", ""], ["Hu", "Yu", ""], ["Li", "Xiaodong", ""]]}, {"id": "2107.10959", "submitter": "Zhe Fei", "authors": "Zhe Fei, Qi Zheng, Hyokyoung G. Hong, Yi Li", "title": "Inference for High Dimensional Censored Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the availability of high dimensional genetic biomarkers, it is of\ninterest to identify heterogeneous effects of these predictors on patients'\nsurvival, along with proper statistical inference. Censored quantile regression\nhas emerged as a powerful tool for detecting heterogeneous effects of\ncovariates on survival outcomes. To our knowledge, there is little work\navailable to draw inference on the effects of high dimensional predictors for\ncensored quantile regression. This paper proposes a novel procedure to draw\ninference on all predictors within the framework of global censored quantile\nregression, which investigates covariate-response associations over an interval\nof quantile levels, instead of a few discrete values. The proposed estimator\ncombines a sequence of low dimensional model estimates that are based on\nmulti-sample splittings and variable selection. We show that, under some\nregularity conditions, the estimator is consistent and asymptotically follows a\nGaussian process indexed by the quantile level. Simulation studies indicate\nthat our procedure can properly quantify the uncertainty of the estimates in\nhigh dimensional settings. We apply our method to analyze the heterogeneous\neffects of SNPs residing in lung cancer pathways on patients' survival, using\nthe Boston Lung Cancer Survival Cohort, a cancer epidemiology study on the\nmolecular mechanism of lung cancer.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 23:57:06 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Fei", "Zhe", ""], ["Zheng", "Qi", ""], ["Hong", "Hyokyoung G.", ""], ["Li", "Yi", ""]]}, {"id": "2107.11002", "submitter": "Di Wang", "authors": "Di Wang, Ruey S. Tsay", "title": "Robust Estimation of High-Dimensional Vector Autoregressive Models", "comments": "37 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-dimensional time series data appear in many scientific areas in the\ncurrent data-rich environment. Analysis of such data poses new challenges to\ndata analysts because of not only the complicated dynamic dependence between\nthe series, but also the existence of aberrant observations, such as missing\nvalues, contaminated observations, and heavy-tailed distributions. For\nhigh-dimensional vector autoregressive (VAR) models, we introduce a unified\nestimation procedure that is robust to model misspecification, heavy-tailed\nnoise contamination, and conditional heteroscedasticity. The proposed\nmethodology enjoys both statistical optimality and computational efficiency,\nand can handle many popular high-dimensional models, such as sparse,\nreduced-rank, banded, and network-structured VAR models. With proper\nregularization and data truncation, the estimation convergence rates are shown\nto be nearly optimal under a bounded fourth moment condition. Consistency of\nthe proposed estimators is also established under a relaxed bounded\n$(2+2\\epsilon)$-th moment condition, for some $\\epsilon\\in(0,1)$, with slower\nconvergence rates associated with $\\epsilon$. The efficacy of the proposed\nestimation methods is demonstrated by simulation and a real example.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 02:39:12 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Wang", "Di", ""], ["Tsay", "Ruey S.", ""]]}, {"id": "2107.11014", "submitter": "Fan Yang", "authors": "Guanglei Hong, Fan Yang, and Xu Qin", "title": "Post-Treatment Confounding in Causal Mediation Studies: A Cutting-Edge\n  Problem and A Novel Solution via Sensitivity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In causal mediation studies that decompose an average treatment effect into a\nnatural indirect effect (NIE) and a natural direct effect (NDE), examples of\npost-treatment confounding are abundant. Past research has generally considered\nit infeasible to adjust for a post-treatment confounder of the mediator-outcome\nrelationship due to incomplete information: it is observed under the actual\ntreatment condition while missing under the counterfactual treatment condition.\nThis study proposes a new sensitivity analysis strategy for handling\npost-treatment confounding and incorporates it into weighting-based causal\nmediation analysis without making extra identification assumptions. Under the\nsequential ignorability of the treatment assignment and of the mediator, we\nobtain the conditional distribution of the post-treatment confounder under the\ncounterfactual treatment as a function of not just pretreatment covariates but\nalso its counterpart under the actual treatment. The sensitivity analysis then\ngenerates a bound for the NIE and that for the NDE over a plausible range of\nthe conditional correlation between the post-treatment confounder under the\nactual and that under the counterfactual conditions. Implemented through either\nimputation or integration, the strategy is suitable for binary as well as\ncontinuous measures of post-treatment confounders. Simulation results\ndemonstrate major strengths and potential limitations of this new solution. A\nre-analysis of the National Evaluation of Welfare-to-Work Strategies (NEWWS)\nRiverside data reveals that the initial analytic results are sensitive to\nomitted post-treatment confounding.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 03:27:42 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Hong", "Guanglei", ""], ["Yang", "Fan", ""], ["Qin", "Xu", ""]]}, {"id": "2107.11025", "submitter": "Xiaomeng Qi", "authors": "Xiaomeng Qi and Zhangsheng Yu", "title": "Kernel regression for cause-specific hazard models with time-dependent\n  coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competing risk data appear widely in modern biomedical research.\nCause-specific hazard models are often used to deal with competing risk data in\nthe past two decades. There is no current study on the kernel likelihood method\nfor the cause-specific hazard model with time-varying coefficients. We propose\nto use the local partial log-likelihood approach for nonparametric time-varying\ncoefficient estimation. Simulation studies demonstrate that our proposed\nnonparametric kernel estimator has a good performance under assumed finite\nsample settings. Finally, we apply the proposed method to analyze a diabetes\ndialysis study with competing death causes.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 04:34:35 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Qi", "Xiaomeng", ""], ["Yu", "Zhangsheng", ""]]}, {"id": "2107.11155", "submitter": "Gianluigi Pillonetto Dr.", "authors": "Gianluigi Pillonetto", "title": "Estimation of sparse linear dynamic networks using the stable spline\n  horseshoe prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of the so-called dynamic networks is one of the most\nchallenging problems appeared recently in control literature. Such systems\nconsist of large-scale interconnected systems, also called modules. To recover\nfull networks dynamics the two crucial steps are topology detection, where one\nhas to infer from data which connections are active, and modules estimation.\nSince a small percentage of connections are effective in many real systems, the\nproblem finds also fundamental connections with group-sparse estimation. In\nparticular, in the linear setting modules correspond to unknown impulse\nresponses expected to have null norm but in a small fraction of samples. This\npaper introduces a new Bayesian approach for linear dynamic networks\nidentification where impulse responses are described through the combination of\ntwo particular prior distributions. The first one is a block version of the\nhorseshoe prior, a model possessing important global-local shrinkage features.\nThe second one is the stable spline prior, that encodes information on\nsmooth-exponential decay of the modules. The resulting model is called stable\nspline horseshoe (SSH) prior. It implements aggressive shrinkage of small\nimpulse responses while larger impulse responses are conveniently subject to\nstable spline regularization. Inference is performed by a Markov Chain Monte\nCarlo scheme, tailored to the dynamic context and able to efficiently return\nthe posterior of the modules in sampled form. We include numerical studies that\nshow how the new approach can accurately reconstruct sparse networks dynamics\nalso when thousands of unknown impulse response coefficients must be inferred\nfrom data sets of relatively small size.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 12:00:14 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Pillonetto", "Gianluigi", ""]]}, {"id": "2107.11195", "submitter": "Ethan Alt", "authors": "Ethan M. Alt, Matthew A. Psioda, Joseph G. Ibrahim", "title": "A hierarchical prior for generalized linear models based on predictions\n  for the mean response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been increased interest in using prior information in statistical\nanalyses. For example, in rare diseases, it can be difficult to establish\ntreatment efficacy based solely on data from a prospective study due to low\nsample sizes. To overcome this issue, an informative prior for the treatment\neffect may be elicited. We develop a novel extension of the conjugate prior of\nChen and Ibrahim (2003) that enables practitioners to elicit a prior prediction\nfor the mean response for generalized linear models, treating the prediction as\nrandom. We refer to the hierarchical prior as the hierarchical prediction\nprior. For i.i.d. settings and the normal linear model, we derive cases for\nwhich the hyperprior is a conjugate prior. We also develop an extension of the\nHPP in situations where summary statistics from a previous study are available,\ndrawing comparisons with the power prior. The HPP allows for discounting based\non the quality of individual level predictions, having the potential to provide\nefficiency gains (e.g., lower MSE) where predictions are incompatible with the\ndata. An efficient Markov chain Monte Carlo algorithm is developed.\nApplications illustrate that inferences under the HPP are more robust to\nprior-data conflict compared to selected non-hierarchical priors.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 12:56:15 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Alt", "Ethan M.", ""], ["Psioda", "Matthew A.", ""], ["Ibrahim", "Joseph G.", ""]]}, {"id": "2107.11270", "submitter": "Efstathios Paparoditis", "authors": "Jens-Peter Kreiss, Efstathios Paparoditis", "title": "Bootstrapping Whittle Estimators", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fitting parametric models by optimizing frequency domain objective functions\nis an attractive approach of parameter estimation in time series analysis.\nWhittle estimators are a prominent example in this context. Under weak\nconditions and the (realistic) assumption that the true spectral density of the\nunderlying process does not necessarily belong to the parametric class of\nspectral densities fitted, the distribution of Whittle estimators typically\ndepends on difficult to estimate characteristics of the underlying process.\nThis makes the implementation of asymptotic results for the construction of\nconfidence intervals or for assessing the variability of estimators, difficult\nin practice. This paper proposes a frequency domain bootstrap method to\nestimate the distribution of Whittle estimators which is asymptotically valid\nunder assumptions that not only allow for (possible) model misspecification but\nalso for weak dependence conditions which are satisfied by a wide range of\nstationary stochastic processes. Adaptions of the bootstrap procedure developed\nto incorporate different modifications of Whittle estimators proposed in the\nliterature, like for instance, tapered, de-biased or boundary extended Whittle\nestimators, are also considered. Simulations demonstrate the capabilities of\nthe bootstrap method proposed and its good finite sample performance. A\nreal-life data analysis also is presented.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 14:39:47 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Kreiss", "Jens-Peter", ""], ["Paparoditis", "Efstathios", ""]]}, {"id": "2107.11316", "submitter": "Noirrit Kiran Chandra", "authors": "Noirrit Kiran Chandra, Peter Mueller and Abhra Sarkar", "title": "Bayesian Precision Factor Analysis for High-dimensional Sparse Gaussian\n  Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian graphical models are popular tools for studying the dependence\nrelationships between different random variables. We propose a novel approach\nto Gaussian graphical models that relies on decomposing the precision matrix\nencoding the conditional independence relationships into a low rank and a\ndiagonal component. Such decompositions are already popular for modeling large\ncovariance matrices as they admit a latent factor based representation that\nallows easy inference but are yet to garner widespread use in precision matrix\nmodels due to their computational intractability. We show that a simple latent\nvariable representation for such decomposition in fact exists for precision\nmatrices as well. The latent variable construction provides fundamentally novel\ninsights into Gaussian graphical models. It is also immediately useful in\nBayesian settings in achieving efficient posterior inference via a\nstraightforward Gibbs sampler that scales very well to high-dimensional\nproblems far beyond the limits of the current state-of-the-art. The ability to\nefficiently explore the full posterior space allows the model uncertainty to be\neasily assessed and the underlying graph to be determined via a novel posterior\nfalse discovery rate control procedure. The decomposition also crucially allows\nus to adapt sparsity inducing priors to shrink insignificant off-diagonal\nentries toward zero, making the approach adaptable to high-dimensional\nsmall-sample-size sparse settings. We evaluate the method's empirical\nperformance through synthetic experiments and illustrate its practical utility\nin data sets from two different application domains.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 15:51:54 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Chandra", "Noirrit Kiran", ""], ["Mueller", "Peter", ""], ["Sarkar", "Abhra", ""]]}, {"id": "2107.11323", "submitter": "Ian Waudby-Smith", "authors": "Ian Waudby-Smith and Philip B. Stark and Aaditya Ramdas", "title": "RiLACS: Risk-Limiting Audits via Confidence Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately determining the outcome of an election is a complex task with many\npotential sources of error, ranging from software glitches in voting machines\nto procedural lapses to outright fraud. Risk-limiting audits (RLA) are\nstatistically principled \"incremental\" hand counts that provide statistical\nassurance that reported outcomes accurately reflect the validly cast votes. We\npresent a suite of tools for conducting RLAs using confidence sequences --\nsequences of confidence sets which uniformly capture an electoral parameter of\ninterest from the start of an audit to the point of an exhaustive recount with\nhigh probability. Adopting the SHANGRLA framework, we design nonnegative\nmartingales which yield computationally and statistically efficient confidence\nsequences and RLAs for a wide variety of election types.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 16:03:40 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Waudby-Smith", "Ian", ""], ["Stark", "Philip B.", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2107.11403", "submitter": "Gecia Bravo-Hermsdorff", "authors": "Gecia Bravo-Hermsdorff, Lee M. Gunderson, Pierre-Andr\\'e Maugis, Carey\n  E. Priebe", "title": "A principled (and practical) test for network comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How might one test the hypothesis that graphs were sampled from the same\ndistribution? Here, we compare two statistical tests that address this\nquestion. The first uses the observed subgraph densities themselves as\nestimates of those of the underlying distribution. The second test uses a new\napproach that converts these subgraph densities into estimates of the graph\ncumulants of the distribution. We demonstrate -- via theory, simulation, and\napplication to real data -- the superior statistical power of using graph\ncumulants.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 18:10:59 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 03:02:59 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Bravo-Hermsdorff", "Gecia", ""], ["Gunderson", "Lee M.", ""], ["Maugis", "Pierre-Andr\u00e9", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2107.11449", "submitter": "\\'Angel Gonz\\'alez-Prieto Dr.", "authors": "Jessica D\\'iaz, Jorge P\\'erez, Carolina Gallardo, \\'Angel\n  Gonz\\'alez-Prieto", "title": "Applying Inter-rater Reliability and Agreement in Grounded Theory\n  Studies in Software Engineering", "comments": "20 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the qualitative research on empirical software engineering\nthat applies Grounded Theory is increasing. Grounded Theory (GT) is a technique\nfor developing theory inductively e iteratively from qualitative data based on\ntheoretical sampling, coding, constant comparison, memoing, and saturation, as\nmain characteristics. Large or controversial GT studies may involve multiple\nresearchers in collaborative coding, which requires a kind of rigor and\nconsensus that an individual coder does not. Although many qualitative\nresearchers reject quantitative measures in favor of other qualitative\ncriteria, many others are committed to measuring consensus through Inter-Rater\nReliability (IRR) and/or Inter-Rater Agreement (IRA) techniques to develop a\nshared understanding of the phenomenon being studied. However, there are no\nspecific guidelines about how and when to apply IRR/IRA during the iterative\nprocess of GT, so researchers have been using ad hoc methods for years. This\npaper presents a process for systematically applying IRR/IRA in GT studies that\nmeets the iterative nature of this qualitative research method, which is\nsupported by a previous systematic literature review on applying IRR/RA in GT\nstudies in software engineering. This process allows researchers to\nincrementally generate a theory while ensuring consensus on the constructs that\nsupport it and, thus, improving the rigor of qualitative research. This\nformalization helps researchers to apply IRR/IRA to GT studies when various\nraters are involved in coding. Measuring consensus among raters promotes\ncommunicability, transparency, reflexivity, replicability, and trustworthiness\nof the research.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 20:26:14 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["D\u00edaz", "Jessica", ""], ["P\u00e9rez", "Jorge", ""], ["Gallardo", "Carolina", ""], ["Gonz\u00e1lez-Prieto", "\u00c1ngel", ""]]}, {"id": "2107.11456", "submitter": "Ricardo Pedroso", "authors": "Ricardo C. Pedroso, Rosangela H. Loschi and Fernando Andr\\'es Quintana", "title": "Multipartition model for multiple change point identification", "comments": "60 pages, 33 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the main goals in multiple change point problems are the estimation of\nthe number and positions of the change points, as well as the regime structure\nin the clusters induced by those changes. The product partition model (PPM) is\na widely used approach for the detection of multiple change points. The\ntraditional PPM assumes that change points split the set of time points in\nrandom clusters that define a partition of the time axis. It is then typically\nassumed that sampling model parameter values within each of these blocks are\nidentical. Because changes in different parameters of the observational model\nmay occur at different times, the PPM thus fails to identify the parameters\nthat experienced those changes. A similar problem may occur when detecting\nchanges in multivariate time series. To solve this important limitation, we\nintroduce a multipartition model to detect multiple change points occurring in\nseveral parameters at possibly different times. The proposed model assumes that\nthe changes experienced by each parameter generate a different random partition\nof the time axis, which facilitates identifying which parameters have changed\nand when they do so. We discuss a partially collapsed Gibbs sampler scheme to\nimplement posterior simulation under the proposed model. We apply the proposed\nmodel to identify multiple change points in Normal means and variances and\nevaluate the performance of the proposed model through Monte Carlo simulations\nand data illustrations. Its performance is compared with some previously\nproposed approaches for change point problems. These studies show that the\nproposed model is competitive and enriches the analysis of change point\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 20:48:03 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Pedroso", "Ricardo C.", ""], ["Loschi", "Rosangela H.", ""], ["Quintana", "Fernando Andr\u00e9s", ""]]}, {"id": "2107.11459", "submitter": "Peter Gilbert", "authors": "Lars van der Laan, Wenbo Zhang, Peter B. Gilbert", "title": "Efficient nonparametric estimation of the covariate-adjusted\n  threshold-response function, a support-restricted stochastic intervention", "comments": "51 pages including supplement, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying a biomarker or treatment-dose threshold that marks a specified\nlevel of risk is an important problem, especially in clinical trials. This\nrisk, viewed as a function of thresholds and possibly adjusted for covariates,\nwe call the threshold-response function. Extending the work of Donovan, Hudgens\nand Gilbert (2019), we propose a nonparametric efficient estimator for the\ncovariate-adjusted threshold-response function, which utilizes machine learning\nand Targeted Minimum-Loss Estimation (TMLE). We additionally propose a more\ngeneral estimator, based on sequential regression, that also applies when there\nis outcome missingness. We show that the threshold-response for a given\nthreshold may be viewed as the expected outcome under a stochastic intervention\nwhere all participants are given a treatment dose above the threshold. We prove\nthe estimator is efficient and characterize its asymptotic distribution. A\nmethod to construct simultaneous 95% confidence bands for the\nthreshold-response function and its inverse is given. Furthermore, we discuss\nhow to adjust our estimator when the treatment or biomarker is\nmissing-at-random, as is the case in clinical trials with biased sampling\ndesigns, using inverse-probability-weighting. The methods are assessed in a\ndiverse set of simulation settings with rare outcomes and cumulative\ncase-control sampling. The methods are employed to estimate neutralizing\nantibody thresholds for virologically confirmed dengue risk in the CYD14 and\nCYD15 dengue vaccine trials.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 20:57:54 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["van der Laan", "Lars", ""], ["Zhang", "Wenbo", ""], ["Gilbert", "Peter B.", ""]]}, {"id": "2107.11538", "submitter": "Xiaochao Xia", "authors": "Xiaochao Xia", "title": "A Robust Partial Correlation-based Screening Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a computationally fast and working efficient tool, sure independence\nscreening has received much attention in solving ultrahigh dimensional\nproblems. This paper contributes two robust sure screening approaches that\nsimultaneously take into account heteroscedasticity, outliers, heavy-tailed\ndistribution, continuous or discrete response, and confounding effect, from the\nperspective of model-free. First, we define a robust correlation measure only\nusing two random indicators, and introduce a screener using that correlation.\nSecond, we propose a robust partial correlation-based screening approach when\nan exposure variable is available. To remove the confounding effect of the\nexposure on both response and each covariate, we use a nonparametric regression\nwith some specified loss function. More specifically, a robust\ncorrelation-based screening method (RC-SIS) and a robust partial\ncorrelation-based screening framework (RPC-SIS) including two concrete\nscreeners: RPC-SIS(L2) and RPC-SIS(L1), are formed. Third, we establish sure\nscreening properties of RC-SIS for which the response variable can be either\ncontinuous or discrete, as well as those of RPC-SIS(L2) and RPC-SIS(L1) under\nsome regularity conditions. Our approaches are essentially nonparametric, and\nperform robustly for both the response and the covariates. Finally, extensive\nsimulation studies and two applications are carried out to demonstrate the\nsuperiority of our proposed approaches.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 05:40:00 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xia", "Xiaochao", ""]]}, {"id": "2107.11546", "submitter": "Nilanjana Laha", "authors": "Nilanjana Laha, Zoe Moodie, Ying Huang, Alex Luedtke", "title": "Improved inference for vaccine-induced immune responses via\n  shape-constrained methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the performance of shape-constrained methods for evaluating immune\nresponse profiles from early-phase vaccine trials. The motivating problem for\nthis work involves quantifying and comparing the IgG binding immune responses\nto the first and second variable loops (V1V2 region) arising in HVTN 097 and\nHVTN 100 HIV vaccine trials. We consider unimodal and log-concave\nshape-constrained methods to compare the immune profiles of the two vaccines,\nwhich is reasonable because the data support that the underlying densities of\nthe immune responses could have these shapes. To this end, we develop novel\nshape-constrained tests of stochastic dominance and shape-constrained plug-in\nestimators of the Hellinger distance between two densities. Our techniques are\neither tuning parameter free, or rely on only one tuning parameter, but their\nperformance is either better (the tests of stochastic dominance) or comparable\nwith the nonparametric methods (the estimators of Hellinger distance). The\nminimal dependence on tuning parameters is especially desirable in clinical\ncontexts where analyses must be prespecified and reproducible. Our methods are\nsupported by theoretical results and simulation studies.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 06:47:27 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Laha", "Nilanjana", ""], ["Moodie", "Zoe", ""], ["Huang", "Ying", ""], ["Luedtke", "Alex", ""]]}, {"id": "2107.11687", "submitter": "Jixian Wang", "authors": "Jixian Wang", "title": "On matching-adjusted indirect comparison and calibration estimation", "comments": "26 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect comparisons have been increasingly used to compare data from\ndifferent sources such as clinical trials and observational data in, e.g., a\ndisease registry. To adjust for population differences between data sources,\nmatching-adjusted indirect comparison (MAIC) has been used in several\napplications including health technology assessment and drug regulatory\nsubmissions. In fact, MAIC can be considered as a special case of a range of\nmethods known as calibration estimation in survey sampling. However, to our\nbest knowledge, this connection has not been examined in detail. This paper\nmakes three contributions: 1. We examined this connection by comparing MAIC and\na few commonly used calibration estimation methods, including the entropy\nbalancing approach, which is equivalent to MAIC. 2. We considered the standard\nerror (SE) estimation of the MAIC estimators and propose a model-independent SE\nestimator and examine its performance by simulation. 3. We conducted a\nsimulation to compare these commonly used approaches to evaluate their\nperformance in indirect comparison scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 20:43:16 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wang", "Jixian", ""]]}, {"id": "2107.11732", "submitter": "Ruoxuan Xiong", "authors": "Ruoxuan Xiong, Allison Koenecke, Michael Powell, Zhu Shen, Joshua T.\n  Vogelstein, Susan Athey", "title": "Federated Causal Inference in Heterogeneous Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing observational data from multiple sources can be useful for\nincreasing statistical power to detect a treatment effect; however, practical\nconstraints such as privacy considerations may restrict individual-level\ninformation sharing across data sets. This paper develops federated methods\nthat only utilize summary-level information from heterogeneous data sets. Our\nfederated methods provide doubly-robust point estimates of treatment effects as\nwell as variance estimates. We derive the asymptotic distributions of our\nfederated estimators, which are shown to be asymptotically equivalent to the\ncorresponding estimators from the combined, individual-level data. We show that\nto achieve these properties, federated methods should be adjusted based on\nconditions such as whether models are correctly specified and stable across\nheterogeneous data sets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 05:55:00 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xiong", "Ruoxuan", ""], ["Koenecke", "Allison", ""], ["Powell", "Michael", ""], ["Shen", "Zhu", ""], ["Vogelstein", "Joshua T.", ""], ["Athey", "Susan", ""]]}, {"id": "2107.11765", "submitter": "Rodrigo Labouriau", "authors": "Jeanett S. Pelck and Rodrigo Labouriau", "title": "Conditional Inference for Multivariate Generalised Linear Mixed Models", "comments": "35 pages, 2 figures and 5 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method for inference in generalised linear mixed models (GLMMs)\nand several extensions of these models. First, we extend the GLMM by allowing\nthe distribution of the random components to be non-Gaussian, that is, assuming\nan absolutely continuous distribution with respect to the Lebesgue measure that\nis symmetric around zero, unimodal and with finite moments up to fourth-order.\nSecond, we allow the conditional distribution to follow a dispersion model\ninstead of exponential dispersion models. Finally, we extend these models to a\nmultivariate framework where multiple responses are combined by imposing a\nmultivariate absolute continuous distribution on the random components\nrepresenting common clusters of observations in all the marginal models.\n  Maximum likelihood inference in these models involves evaluating an integral\nthat often cannot be computed in closed form. We suggest an inference method\nthat predicts values of random components and does not involve the integration\nof conditional likelihood quantities.\n  The multivariate GLMMs that we studied can be constructed with marginal GLMMs\nof different statistical nature, and at the same time, represent complex\ndependence structure providing a rather flexible tool for applications.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 09:26:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Pelck", "Jeanett S.", ""], ["Labouriau", "Rodrigo", ""]]}, {"id": "2107.11785", "submitter": "Manuele Leonelli", "authors": "Manuele Leonelli, Ramsiya Ramanathan, Rachel L. Wilkerson", "title": "Sensitivity and robustness analysis in Bayesian networks with the\n  bnmonitor R package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian networks are a class of models that are widely used for risk\nassessment of complex operational systems. There are now multiple approaches,\nas well as implemented software, that guide their construction via data\nlearning or expert elicitation. However, a constructed Bayesian network needs\nto be validated before it can be used for practical risk assessment. Here, we\nillustrate the usage of the bnmonitor R package: the first comprehensive\nsoftware for the validation of a Bayesian network. An applied data analysis\nusing bnmonitor is carried out over a medical dataset to illustrate the use of\nits wide array of functions.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 11:23:43 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Leonelli", "Manuele", ""], ["Ramanathan", "Ramsiya", ""], ["Wilkerson", "Rachel L.", ""]]}, {"id": "2107.11869", "submitter": "Timothy Christensen", "authors": "Xiaohong Chen, Timothy Christensen, Sid Kankanala", "title": "Adaptive Estimation and Uniform Confidence Bands for Nonparametric IV", "comments": "The data-driven choice of sieve dimension in this paper is based on\n  and supersedes Section 3 of the preprint arXiv:1508.03365v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce computationally simple, data-driven procedures for estimation\nand inference on a structural function $h_0$ and its derivatives in\nnonparametric models using instrumental variables. Our first procedure is a\nbootstrap-based, data-driven choice of sieve dimension for sieve nonparametric\ninstrumental variables (NPIV) estimators. When implemented with this\ndata-driven choice, sieve NPIV estimators of $h_0$ and its derivatives are\nadaptive: they converge at the best possible (i.e., minimax) sup-norm rate,\nwithout having to know the smoothness of $h_0$, degree of endogeneity of the\nregressors, or instrument strength. Our second procedure is a data-driven\napproach for constructing honest and adaptive uniform confidence bands (UCBs)\nfor $h_0$ and its derivatives. Our data-driven UCBs guarantee coverage for\n$h_0$ and its derivatives uniformly over a generic class of data-generating\nprocesses (honesty) and contract at, or within a logarithmic factor of, the\nminimax sup-norm rate (adaptivity). As such, our data-driven UCBs deliver\nasymptotic efficiency gains relative to UCBs constructed via the usual approach\nof undersmoothing. In addition, both our procedures apply to nonparametric\nregression as a special case. We use our procedures to estimate and perform\ninference on a nonparametric gravity equation for the intensive margin of firm\nexports and find evidence against common parameterizations of the distribution\nof unobserved firm productivity.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 18:46:33 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chen", "Xiaohong", ""], ["Christensen", "Timothy", ""], ["Kankanala", "Sid", ""]]}, {"id": "2107.11931", "submitter": "Jun Li", "authors": "Jun Li", "title": "Max-Type and Sum-Type Procedures for Online Change-Point Detection in\n  the Mean of High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two procedures to detect a change in the mean of high-dimensional\nonline data. One is based on a max-type U-statistic and another is based on a\nsum-type U-statistic. Theoretical properties of the two procedures are explored\nin the high dimensional setting. More precisely, we derive their average run\nlengths (ARLs) when there is no change point, and expected detection delays\n(EDDs) when there is a change point. Accuracy of the theoretical results is\nconfirmed by simulation studies. The practical use of the proposed procedures\nis demonstrated by detecting an abrupt change in PM2.5 concentrations. The\ncurrent study attempts to extend the results of the CUSUM and Shiryayev-Roberts\nprocedures previously established in the univariate setting.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 02:49:27 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Li", "Jun", ""]]}, {"id": "2107.11971", "submitter": "Tahir Mahmood Dr", "authors": "Inez Maria Zwetsloot, Tahir Mahmood, Funmilola Mary Taiwo and Zezhong\n  Wang", "title": "A Real Time Monitoring Approach for Bivariate Event Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of changes in the frequency of events is an important task,\nin, for example, disease surveillance, monitoring of high-quality processes,\nreliability monitoring and public health. In this article, we focus on\ndetecting changes in multivariate event data, by monitoring the\ntime-between-events (TBE). Existing multivariate TBE charts are limited in the\nsense that, they only signal after an event occurred for each of the individual\nprocesses. This results in delays (i.e., long time to signal), especially if it\nis of interest to detect a change in one or a few of the processes. We propose\na bivariate TBE (BTBE) chart which is able to signal in real time. We derive\nanalytical expressions for the control limits and average time-to-signal\nperformance, conduct a performance evaluation and compare our chart to an\nexisting method. The findings showed that our method is a realistic approach to\nmonitor bivariate time-between-event data, and has better detection ability\nthan existing methods. A large benefit of our method is that it signals in\nreal-time and that due to the analytical expressions no simulation is needed.\nThe proposed method is implemented on a real-life dataset related to AIDS.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 05:51:51 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zwetsloot", "Inez Maria", ""], ["Mahmood", "Tahir", ""], ["Taiwo", "Funmilola Mary", ""], ["Wang", "Zezhong", ""]]}, {"id": "2107.11998", "submitter": "Mohd Arshad", "authors": "Ashok Kumar Pathak, Mohd. Arshad, Qazi J. Azhad, Mukti Khetan and\n  Arvind Pandey", "title": "A Novel Bivariate Generalized Weibull Distribution with Properties and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Univariate Weibull distribution is a well-known lifetime distribution and has\nbeen widely used in reliability and survival analysis. In this paper, we\nintroduce a new family of bivariate generalized Weibull (BGW) distributions,\nwhose univariate marginals are exponentiated Weibull distribution. Different\nstatistical quantiles like marginals, conditional distribution, conditional\nexpectation, product moments, correlation and a measure component reliability\nare derived. Various measures of dependence and statistical properties along\nwith ageing properties are examined. Further, the copula associated with BGW\ndistribution and its various important properties are also considered. The\nmethods of maximum likelihood and Bayesian estimation are employed to estimate\nunknown parameters of the model. A Monte Carlo simulation and real data study\nare carried out to demonstrate the performance of the estimators and results\nhave proven the effectiveness of the distribution in real-life situations\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 07:17:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Pathak", "Ashok Kumar", ""], ["Arshad", "Mohd.", ""], ["Azhad", "Qazi J.", ""], ["Khetan", "Mukti", ""], ["Pandey", "Arvind", ""]]}, {"id": "2107.12162", "submitter": "Omid Shojaee", "authors": "Hassan Piriaeia and Omid Shojaee", "title": "E-Bayesian Estimation For Some Characteristics Of Weibull Generalized\n  Exponential Progressive Type-II Censored Samples", "comments": "18 page", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Estimation of reliability and hazard rate is one of the most important\nproblems raised in many applications especially in engineering studies as well\nas human lifetime. In this regard, different methods of estimation have been\nused. Each method exploits various tools and suffers from problems such as\ncomplexity of computations, low precision, and so forth. This study is employed\nthe E-Bayesian method, for estimating the parameter and survival functions of\nthe Weibull Generalized Exponential distribution. The estimators are obtained\nunder squared error and LINEX loss functions under progressive type-II censored\nsamples. E-Bayesian estimations are derived based on three priors of\nhyperparameters to investigate the influence of different priors on\nestimations. The asymptotic behaviours of E-Bayesian estimations have been\ninvestigated as well as relationships among them. Finally, a comparison among\nthe maximum likelihood, Bayes, and E-Bayesian estimations are made, using real\ndata and Monte Carlo simulation. Results show that the new method is more\nefficient than previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:29:31 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Piriaeia", "Hassan", ""], ["Shojaee", "Omid", ""]]}, {"id": "2107.12298", "submitter": "Gaelle Saint-Hilary", "authors": "Tom Menzies (1,2), Gaelle Saint-Hilary (3,4) and Pavel Mozgunov (5)\n  ((1) Clinical Trials Research Unit, Leeds Institute of Clinical Trials\n  Research, University of Leeds, Leeds, UK, (2) Department of Mathematics and\n  Statistics, Lancaster University, Lancaster, UK, (3) Department of\n  Biostatistics, Institut de Recherches Internationales Servier (IRIS),\n  Suresnes, France, (4) Dipartimento di Scienze Matematiche (DISMA) Giuseppe\n  Luigi Lagrange, Politecnico di Torino, Torino, Italy, (5) Medical and\n  Pharmaceutical Statistics Research Unit, Department of Mathematics and\n  Statistics, Lancaster University, Lancaster, UK)", "title": "A Comparison of Various Aggregation Functions in Multi-Criteria Decision\n  Analysis for Drug Benefit-Risk Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-criteria decision analysis (MCDA) is a quantitative approach to the\ndrug benefit-risk assessment (BRA) which allows for consistent comparisons by\nsummarising all benefits and risks in a single score. The MCDA consists of\nseveral components, one of which is the utility (or loss) score function that\ndefines how benefits and risks are aggregated into a single quantity. While a\nlinear utility score is one of the most widely used approach in BRA, it is\nrecognised that it can result in counter-intuitive decisions, for example,\nrecommending a treatment with extremely low benefits or high risks. To overcome\nthis problem, alternative approaches to the scores construction, namely,\nproduct, multi-linear and Scale Loss Score models, were suggested. However, to\ndate, the majority of arguments concerning the differences implied by these\nmodels are heuristic. In this work, we consider four models to calculate the\naggregated utility/loss scores and compared their performance in an extensive\nsimulation study over many different scenarios, and in a case study. It is\nfound that the product and Scale Loss Score models provide more intuitive\ntreatment recommendation decisions in the majority of scenarios compared to the\nlinear and multi-linear models, and are more robust to the correlation in the\ncriteria.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 16:14:23 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Menzies", "Tom", ""], ["Saint-Hilary", "Gaelle", ""], ["Mozgunov", "Pavel", ""]]}, {"id": "2107.12365", "submitter": "Yuling Yan", "authors": "Yuling Yan, Yuxin Chen, Jianqing Fan", "title": "Inference for Heteroskedastic PCA with Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies how to construct confidence regions for principal\ncomponent analysis (PCA) in high dimension, a problem that has been vastly\nunder-explored. While computing measures of uncertainty for nonlinear/nonconvex\nestimators is in general difficult in high dimension, the challenge is further\ncompounded by the prevalent presence of missing data and heteroskedastic noise.\nWe propose a suite of solutions to perform valid inference on the principal\nsubspace based on two estimators: a vanilla SVD-based approach, and a more\nrefined iterative scheme called $\\textsf{HeteroPCA}$ (Zhang et al., 2018). We\ndevelop non-asymptotic distributional guarantees for both estimators, and\ndemonstrate how these can be invoked to compute both confidence regions for the\nprincipal subspace and entrywise confidence intervals for the spiked covariance\nmatrix. Particularly worth highlighting is the inference procedure built on top\nof $\\textsf{HeteroPCA}$, which is not only valid but also statistically\nefficient for broader scenarios (e.g., it covers a wider range of missing rates\nand signal-to-noise ratios). Our solutions are fully data-driven and adaptive\nto heteroskedastic random noise, without requiring prior knowledge about the\nnoise levels and noise distributions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 17:59:01 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Yan", "Yuling", ""], ["Chen", "Yuxin", ""], ["Fan", "Jianqing", ""]]}, {"id": "2107.12420", "submitter": "Zhaonan Qu", "authors": "Zhaonan Qu, Ruoxuan Xiong, Jizhou Liu, Guido Imbens", "title": "Efficient Treatment Effect Estimation in Observational Studies under\n  Heterogeneous Partial Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many observational studies in social science and medical applications,\nsubjects or individuals are connected, and one unit's treatment and attributes\nmay affect another unit's treatment and outcome, violating the stable unit\ntreatment value assumption (SUTVA) and resulting in interference. To enable\nfeasible inference, many previous works assume the ``exchangeability'' of\ninterfering units, under which the effect of interference is captured by the\nnumber or ratio of treated neighbors. However, in many applications with\ndistinctive units, interference is heterogeneous. In this paper, we focus on\nthe partial interference setting, and restrict units to be exchangeable\nconditional on observable characteristics. Under this framework, we propose\ngeneralized augmented inverse propensity weighted (AIPW) estimators for general\ncausal estimands that include direct treatment effects and spillover effects.\nWe show that they are consistent, asymptotically normal, semiparametric\nefficient, and robust to heterogeneous interference as well as model\nmisspecifications. We also apply our method to the Add Health dataset and find\nthat smoking behavior exhibits interference on academic outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 18:27:51 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Qu", "Zhaonan", ""], ["Xiong", "Ruoxuan", ""], ["Liu", "Jizhou", ""], ["Imbens", "Guido", ""]]}, {"id": "2107.12462", "submitter": "Jan Posp\\'i\\v{s}il", "authors": "Jan Matas and Jan Posp\\'i\\v{s}il", "title": "Robustness and sensitivity analyses for rough Volterra stochastic\n  volatility models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we perform robustness and sensitivity analysis of several\ncontinuous-time rough Volterra stochastic volatility models with respect to the\nprocess of market calibration. Robustness is understood in the sense of\nsensitivity to changes in the option data structure. The latter analyses then\nshould validate the hypothesis on importance of the roughness in the volatility\nprocess dynamics. Empirical study is performed on a data set of Apple Inc.\nequity options traded in four different days in April and May 2015. In\nparticular, the results for RFSV, rBergomi and aRFSV models are provided.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 20:29:10 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Matas", "Jan", ""], ["Posp\u00ed\u0161il", "Jan", ""]]}, {"id": "2107.12487", "submitter": "Honghe Zhao", "authors": "Honghe Zhao, Shu Yang", "title": "Outcome-Adjusted Balance Measure for Generalized Propensity Score Model\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose the outcome-adjusted balance measure to perform\nmodel selection for the generalized propensity score (GPS), which serves as an\nessential component in estimation of the pairwise average treatment effects\n(ATEs) in observational studies with more than two treatment levels. The\nprimary goal of the balance measure is to identify the GPS model specification\nsuch that the resulting ATE estimator is consistent and efficient. Following\nrecent empirical and theoretical evidence, we establish that the optimal GPS\nmodel should only include covariates related to the outcomes. Given a\ncollection of candidate GPS models, the outcome-adjusted balance measure\nimputes all baseline covariates by matching on each candidate model, and\nselects the model that minimizes a weighted sum of absolute mean differences\nbetween the imputed and original values of the covariates. The weights are\ndefined to leverage the covariate-outcome relationship, so that GPS models\nwithout optimal variable selection are penalized. Under appropriate\nassumptions, we show that the outcome-adjusted balance measure consistently\nselects the optimal GPS model, so that the resulting GPS matching estimator is\nasymptotically normal and efficient. We compare its finite sample performance\nwith existing measures in a simulation study. We illustrate an application of\nthe proposed methodology in the analysis of the Tutoring data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 21:25:26 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zhao", "Honghe", ""], ["Yang", "Shu", ""]]}, {"id": "2107.12494", "submitter": "Zheng Fang", "authors": "Zheng Fang", "title": "A Unifying Framework for Testing Shape Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper makes the following econometric contributions. First, we develop a\nunifying framework for testing shape restrictions based on the Wald principle.\nSecond, we examine the applicability and usefulness of some prominent shape\nenforcing operators in implementing our test, including rearrangement and the\ngreatest convex minorization (or the least concave majorization). In\nparticular, the influential rearrangement operator is inapplicable due to a\nlack of convexity, while the greatest convex minorization is shown to enjoy the\nanalytic properties required to employ our framework. The importance of\nconvexity in establishing size control has been noted elsewhere in the\nliterature. Third, we show that, despite that the projection operator may not\nbe well-defined/behaved in general non-Hilbert parameter spaces (e.g., ones\ndefined by uniform norms), one may nonetheless devise a powerful distance-based\ntest by applying our framework. The finite sample performance of our test is\nevaluated through Monte Carlo simulations, and its empirical relevance is\nshowcased by investigating the relationship between weekly working hours and\nthe annual wage growth in the high-end labor market.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 21:57:42 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Fang", "Zheng", ""]]}, {"id": "2107.12586", "submitter": "Weixing Song", "authors": "Weixing Song, Kanwal Ayub, Jianhong Shi", "title": "Extrapolation Estimation for Nonparametric Regression with Measurement\n  Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the nonparametric regression models with covariates contaminated with\nnormal measurement errors, this paper proposes an extrapolation algorithm to\nestimate the nonparametric regression functions. By applying the conditional\nexpectation directly to the kernel-weighted least squares of the deviations\nbetween the local linear approximation and the observed responses, the proposed\nalgorithm successfully bypasses the simulation step needed in the classical\nsimulation extrapolation method, thus significantly reducing the computational\ntime. It is noted that the proposed method also provides an exact form of the\nextrapolation function, but the extrapolation estimate generally cannot be\nobtained by simply setting the extrapolation variable to negative one in the\nfitted extrapolation function if the bandwidth is less than the standard\ndeviation of the measurement error. Large sample properties of the proposed\nestimation procedure are discussed, as well as simulation studies and a real\ndata example being conducted to illustrate its applications.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 04:16:54 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Song", "Weixing", ""], ["Ayub", "Kanwal", ""], ["Shi", "Jianhong", ""]]}, {"id": "2107.12592", "submitter": "Insha Ullah Dr", "authors": "Insha Ullah, Kerrie Mengersen, Rob J Hyndman and James McGree", "title": "Detection of cybersecurity attacks through analysis of web browsing\n  activities using principal component analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Organizations such as government departments and financial institutions\nprovide online service facilities accessible via an increasing number of\ninternet connected devices which make their operational environment vulnerable\nto cyber attacks. Consequently, there is a need to have mechanisms in place to\ndetect cyber security attacks in a timely manner. A variety of Network\nIntrusion Detection Systems (NIDS) have been proposed and can be categorized\ninto signature-based NIDS and anomaly-based NIDS. The signature-based NIDS,\nwhich identify the misuse through scanning the activity signature against the\nlist of known attack activities, are criticized for their inability to identify\nnew attacks (never-before-seen attacks). Among anomaly-based NIDS, which\ndeclare a connection anomalous if it expresses deviation from a trained model,\nthe unsupervised learning algorithms circumvent this issue since they have the\nability to identify new attacks. In this study, we use an unsupervised learning\nalgorithm based on principal component analysis to detect cyber attacks. In the\ntraining phase, our approach has the advantage of also identifying outliers in\nthe training dataset. In the monitoring phase, our approach first identifies\nthe affected dimensions and then calculates an anomaly score by aggregating\nacross only those components that are affected by the anomalies. We explore the\nperformance of the algorithm via simulations and through two applications,\nnamely to the UNSW-NB15 dataset recently released by the Australian Centre for\nCyber Security and to the well-known KDD'99 dataset. The algorithm is scalable\nto large datasets in both training and monitoring phases, and the results from\nboth the simulated and real datasets show that the method has promise in\ndetecting suspicious network activities.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 04:38:46 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Ullah", "Insha", ""], ["Mengersen", "Kerrie", ""], ["Hyndman", "Rob J", ""], ["McGree", "James", ""]]}, {"id": "2107.12649", "submitter": "Martin Kroll", "authors": "L\\'aszl\\'o Gy\\\"orfi and Martin Kroll", "title": "$L_1$ density estimation from privatised data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classical problem of nonparametric density estimation, but\nimpose local differential privacy constraints. Under such constraints, the\noriginal data $X_1,\\ldots,X_n$, taking values in $\\mathbb{R}^d $, cannot be\ndirectly observed, and all estimators are functions of the randomised output of\na suitable privacy mechanism. The statistician is free to choose the form of\nthe privacy mechanism, and in this work we propose to add Laplace distributed\nnoise to a discretisation of the location of a vector $X_i$. Based on these\nrandomised data, we design a novel estimator of the density function, which can\nbe viewed as a privatised version of the well-studied histogram density\nestimator. Our theoretical results include universal pointwise consistency and\nstrong universal $L_1$-consistency. In addition, a convergence rate over\nclasses of Lipschitz functions is derived, which is complemented by a matching\nminimax lower bound.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 08:00:56 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Gy\u00f6rfi", "L\u00e1szl\u00f3", ""], ["Kroll", "Martin", ""]]}, {"id": "2107.12713", "submitter": "Zijun Gao", "authors": "Zijun Gao and Trevor Hastie", "title": "LinCDE: Conditional Density Estimation via Lindsey's Method", "comments": "50 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional density estimation is a fundamental problem in statistics, with\nscientific and practical applications in biology, economics, finance and\nenvironmental studies, to name a few. In this paper, we propose a conditional\ndensity estimator based on gradient boosting and Lindsey's method (LinCDE).\nLinCDE admits flexible modeling of the density family and can capture\ndistributional characteristics like modality and shape. In particular, when\nsuitably parametrized, LinCDE will produce smooth and non-negative density\nestimates. Furthermore, like boosted regression trees, LinCDE does automatic\nfeature selection. We demonstrate LinCDE's efficacy through extensive\nsimulations and several real data examples.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 10:16:56 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Gao", "Zijun", ""], ["Hastie", "Trevor", ""]]}, {"id": "2107.12747", "submitter": "Pekka Laitila", "authors": "Pekka Laitila and Kai Virtanen", "title": "Technical properties of Ranked Nodes Method", "comments": "39 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents analytical and experimental results on the ranked nodes\nmethod (RNM) that is used to construct conditional probability tables for\nBayesian networks by expert elicitation. The majority of the results are\nfocused on a setting in which RNM is applied to a child node and parent nodes\nthat all have the same amount discrete ordinal states. The results indicate on\nRNM properties that can be used to support its future elaboration and\ndevelopment.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 11:42:27 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Laitila", "Pekka", ""], ["Virtanen", "Kai", ""]]}, {"id": "2107.12757", "submitter": "Ernest Pokropek", "authors": "Artur Pokropek and Ernest Pokropek", "title": "Deep Neural Networks for Detecting Statistical Model Misspecifications.\n  The Case of Measurement Invariance", "comments": "30 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While in recent years a number of new statistical approaches have been\nproposed to model group differences with a different assumption on the nature\nof the measurement invariance of the instruments, the tools for detecting local\nspecifications of these models have not been fully developed yet. The main type\nof local misspecification concerning comparability is the non-invariance of\nindicators (called also differential item functioning; DIF). Such\nnon-invariance could arise from poor translations or significant cultural\ndifferences. In this study, we present a novel approach to detect such\nmisspecifications using a Deep Neural Network (DNN). We compared the proposed\nmodel with the most popular traditional methods: modification indices (MI) and\nexpected parameters change (EPC) indicators from the confirmatory factor\nanalysis (CFA) modelling, logistic DIF detection, and sequential procedure\nintroduced with the CFA alignment approach. Simulation studies show that\nproposed method outperformed traditional methods in almost all scenarios, or it\nwas at least as accurate as the best one. We also provide an empirical example\nutilizing European Social Survey (ESS) data including items known to be\nmiss-translated, which are correctly identified by our approach and DIF\ndetection based on logistic regression. This study provides a strong foundation\nfor the future development of machine learning algorithms for detection of\nstatistical model misspecifications.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 11:58:07 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Pokropek", "Artur", ""], ["Pokropek", "Ernest", ""]]}, {"id": "2107.12890", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal", "title": "Subset selection for linear mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linear mixed models (LMMs) are instrumental for regression analysis with\nstructured dependence, such as grouped, clustered, or multilevel data. However,\nselection among the covariates--while accounting for this structured\ndependence--remains a challenge. We introduce a Bayesian decision analysis for\nsubset selection with LMMs. Using a Mahalanobis loss function that incorporates\nthe structured dependence, we derive optimal linear actions for any subset of\ncovariates and under any Bayesian LMM. Crucially, these actions inherit\nshrinkage or regularization and uncertainty quantification from the underlying\nBayesian LMM. Rather than selecting a single \"best\" subset, which is often\nunstable and limited in its information content, we collect the acceptable\nfamily of subsets that nearly match the predictive ability of the \"best\"\nsubset. The acceptable family is summarized by its smallest member and key\nvariable importance metrics. Customized subset search and out-of-sample\napproximation algorithms are provided for more scalable computing. These tools\nare applied to simulated data and a longitudinal physical activity dataset, and\nin both cases demonstrate excellent prediction, estimation, and selection\nability.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 15:47:44 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Kowal", "Daniel R.", ""]]}, {"id": "2107.12987", "submitter": "Graciela Boente Prof.", "authors": "Graciela Boente and Alejandra Mercedes Martinez", "title": "A robust spline approach in partially linear additive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially linear additive models generalize the linear models since they\nmodel the relation between a response variable and covariates by assuming that\nsome covariates are supposed to have a linear relation with the response but\neach of the others enter with unknown univariate smooth functions. The harmful\neffect of outliers either in the residuals or in the covariates involved in the\nlinear component has been described in the situation of partially linear\nmodels, that is, when only one nonparametric component is involved in the\nmodel. When dealing with additive components, the problem of providing reliable\nestimators when atypical data arise, is of practical importance motivating the\nneed of robust procedures. Hence, we propose a family of robust estimators for\npartially linear additive models by combining $B-$splines with robust linear\nregression estimators. We obtain consistency results, rates of convergence and\nasymptotic normality for the linear components, under mild assumptions. A Monte\nCarlo study is carried out to compare the performance of the robust proposal\nwith its classical counterpart under different models and contamination\nschemes. The numerical experiments show the advantage of the proposed\nmethodology for finite samples. We also illustrate the usefulness of the\nproposed approach on a real data set.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 17:56:17 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Boente", "Graciela", ""], ["Martinez", "Alejandra Mercedes", ""]]}, {"id": "2107.13068", "submitter": "Mohammad Taha Bahadori", "authors": "Mohammad Taha Bahadori and Eric Tchetgen Tchetgen and David E.\n  Heckerman", "title": "End-to-End Balancing for Causal Continuous Treatment-Effect Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of observational causal inference with continuous\ntreatment. We focus on the challenge of estimating the causal response curve\nfor infrequently-observed treatment values. We design a new algorithm based on\nthe framework of entropy balancing which learns weights that directly maximize\ncausal inference accuracy using end-to-end optimization. Our weights can be\ncustomized for different datasets and causal inference algorithms. We propose a\nnew theory for consistency of entropy balancing for continuous treatments.\nUsing synthetic and real-world data, we show that our proposed algorithm\noutperforms the entropy balancing in terms of causal inference accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 20:04:59 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Bahadori", "Mohammad Taha", ""], ["Tchetgen", "Eric Tchetgen", ""], ["Heckerman", "David E.", ""]]}, {"id": "2107.13070", "submitter": "Timothy Lycurgus", "authors": "Timothy Lycurgus and Ben B. Hansen", "title": "An Aggregation Scheme for Increased Power in Primary Outcome Analysis", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel aggregation scheme increases power in randomized controlled trials\nand quasi-experiments when the intervention possesses a robust and\nwell-articulated theory of change. Longitudinal data analyzing interventions\noften include multiple observations on individuals, some of which may be more\nlikely to manifest a treatment effect than others. An intervention's theory of\nchange provides guidance as to which of those observations are best situated to\nexhibit that treatment effect. Our power-maximizing weighting for\nrepeated-measurements with delayed-effects scheme, PWRD aggregation, converts\nthe theory of change into a test statistic with improved Pitman efficiency,\ndelivering tests with greater statistical power. We illustrate this method on\nan IES-funded cluster randomized trial testing the efficacy of a reading\nintervention designed to assist early elementary students at risk of falling\nbehind their peers. The salient theory of change holds program benefits to be\ndelayed and non-uniform, experienced after a student's performance stalls. This\nintervention is not found to have an effect, but the PWRD technique's effect on\npower is found to be comparable to that of a doubling of (cluster-level) sample\nsize.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 20:09:40 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lycurgus", "Timothy", ""], ["Hansen", "Ben B.", ""]]}, {"id": "2107.13075", "submitter": "Danilo Quagliotti", "authors": "Danilo Quagliotti", "title": "Modeling the systematic behavior at the micro and nano length scale", "comments": "41 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brisk progression of the industrial digital innovation, leading to high\ndegree of automation and big data transfer in manufacturing technologies,\ndemands continuous development of appropriate off-line metrology methods to\nsupport processes' quality with a tolerable assessment of the measurement\nuncertainty. On the one hand specific-area references propose methods that are\nnot yet well optimized to the changed background, and on the other,\ninternational general recommendations guide to effective uncertainty\nevaluation, but suggesting procedures that are not necessarily proven efficient\nat the micro- and nano-dimensional scale. The well-known GUM approach (i.e.\nfrequentist statistics) was analyzed with the aim to test consistently its\napplicability to micro/nano dimensional and surface topography measurements.\nThe investigation assessed three different clarifying situations, giving rise\nto consistent model equations, and to the achievement of the traceability. The\nchoice of the cases provided a number of influence factors, which are typical\nliabilities at the micro and nano-length scale, and that have been related to\nthe correction of the systematic behavior, viz. the amount of repeated\nmeasurements, the time sequence of the acquired micrographs and the instruments\nused. Such approach allowed the successful implementation of the GUM approach\nto micro/nano dimensional and topographic measurements, and also the appraisal\nof the level of efficacy of the method, its application limits and hints on\npossible future developments.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 20:25:48 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Quagliotti", "Danilo", ""]]}, {"id": "2107.13346", "submitter": "Alicia Curth", "authors": "Alicia Curth and Mihaela van der Schaar", "title": "Doing Great at Estimating CATE? On the Neglected Assumptions in\n  Benchmark Comparisons of Treatment Effect Estimators", "comments": "Workshop on the Neglected Assumptions in Causal Inference at the\n  International Conference on Machine Learning (ICML), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The machine learning toolbox for estimation of heterogeneous treatment\neffects from observational data is expanding rapidly, yet many of its\nalgorithms have been evaluated only on a very limited set of semi-synthetic\nbenchmark datasets. In this paper, we show that even in arguably the simplest\nsetting -- estimation under ignorability assumptions -- the results of such\nempirical evaluations can be misleading if (i) the assumptions underlying the\ndata-generating mechanisms in benchmark datasets and (ii) their interplay with\nbaseline algorithms are inadequately discussed. We consider two popular machine\nlearning benchmark datasets for evaluation of heterogeneous treatment effect\nestimators -- the IHDP and ACIC2016 datasets -- in detail. We identify problems\nwith their current use and highlight that the inherent characteristics of the\nbenchmark datasets favor some algorithms over others -- a fact that is rarely\nacknowledged but of immense relevance for interpretation of empirical results.\nWe close by discussing implications and possible next steps.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 13:21:27 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Curth", "Alicia", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2107.13394", "submitter": "Syed Hasib Akhter Faruqui", "authors": "Syed Hasib Akhter Faruqui, Adel Alaeddini, Jing Wang, Susan P\n  Fisher-Hoch, and Joseph B Mccormic", "title": "Nonlinear State Space Modeling and Control of the Impact of Patients'\n  Modifiable Lifestyle Behaviors on the Emergence of Multiple Chronic\n  Conditions", "comments": "Submitted to IEEE Access for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence and progression of multiple chronic conditions (MCC) over time\noften form a dynamic network that depends on patient's modifiable risk factors\nand their interaction with non-modifiable risk factors and existing conditions.\nContinuous time Bayesian networks (CTBNs) are effective methods for modeling\nthe complex network of MCC relationships over time. However, CTBNs are not able\nto effectively formulate the dynamic impact of patient's modifiable risk\nfactors on the emergence and progression of MCC. Considering a functional CTBN\n(FCTBN) to represent the underlying structure of the MCC relationships with\nrespect to individuals' risk factors and existing conditions, we propose a\nnonlinear state-space model based on Extended Kalman filter (EKF) to capture\nthe dynamics of the patients' modifiable risk factors and existing conditions\non the MCC evolution over time. We also develop a tensor control chart to\ndynamically monitor the effect of changes in the modifiable risk factors of\nindividual patients on the risk of new chronic conditions emergence. We\nvalidate the proposed approach based on a combination of simulation and real\ndata from a dataset of 385 patients from Cameron County Hispanic Cohort (CCHC)\nover multiple years. The dataset examines the emergence of 5 chronic conditions\n(Diabetes, Obesity, Cognitive Impairment, Hyperlipidemia, and Hypertension)\nbased on 4 modifiable risk factors representing lifestyle behaviors (Diet,\nExercise, Smoking Habit, and Drinking Habit) and 3 non-modifiable risk factors,\nincluding demographic information (Age, Gender, Education). The results\ndemonstrate the effectiveness of the proposed methodology for dynamic\nprediction and monitoring of the risk of MCC emergence in individual patients.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 18:01:46 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Faruqui", "Syed Hasib Akhter", ""], ["Alaeddini", "Adel", ""], ["Wang", "Jing", ""], ["Fisher-Hoch", "Susan P", ""], ["Mccormic", "Joseph B", ""]]}, {"id": "2107.13430", "submitter": "Kiheiji Nishida", "authors": "Kiheiji Nishida and Kanta Naito", "title": "Kernel Density Estimation by Stagewise Algorithm with a Simple\n  Dictionary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies kernel density estimation by stagewise minimization\nalgorithm with a simple dictionary on U-divergence. We randomly split an i.i.d.\nsample into the two disjoint sets, one to be used for constructing the kernels\nin the dictionary and the other for evaluating the estimator, and implement the\nalgorithm. The resulting estimator brings us data-adaptive weighting parameters\nand bandwidth matrices, and realizes a sparse representation of kernel density\nestimation. We present the non-asymptotic error bounds of our estimator and\nconfirm its performance by simulations compared with the direct plug-in\nbandwidth matrices and the reduced set density estimator.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 17:05:06 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Nishida", "Kiheiji", ""], ["Naito", "Kanta", ""]]}, {"id": "2107.13480", "submitter": "Erin Craig", "authors": "Erin Craig, Chenyang Zhong and Robert Tibshirani", "title": "Survival stacking: casting survival analysis as a classification problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there are many well-developed data science methods for classification\nand regression, there are relatively few methods for working with\nright-censored data. Here, we present \"survival stacking\": a method for casting\nsurvival analysis problems as classification problems, thereby allowing the use\nof general classification methods and software in a survival setting. Inspired\nby the Cox partial likelihood, survival stacking collects features and outcomes\nof survival data in a large data frame with a binary outcome. We show that\nsurvival stacking with logistic regression is approximately equivalent to the\nCox proportional hazards model. We further recommend methods for evaluating\nmodel performance in the survival stacked setting, and we illustrate survival\nstacking on real and simulated data. By reframing survival problems as\nclassification problems, we make it possible for data scientists to use\nwell-known learning algorithms (including random forests, gradient boosting\nmachines and neural networks) in a survival setting, and lower the barrier for\nflexible survival modeling.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:42:09 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Craig", "Erin", ""], ["Zhong", "Chenyang", ""], ["Tibshirani", "Robert", ""]]}, {"id": "2107.13535", "submitter": "Americo Cunha Jr", "authors": "Mario Germ\\'an Sandoval, Americo Cunha Jr, Rubens Sampaio", "title": "Identification of parameters in the torsional dynamics of a drilling\n  process through Bayesian statistics", "comments": null, "journal-ref": "Mec\\'anica Computacional, vol. 32, pp. 763-773, 2013", "doi": null, "report-no": null, "categories": "stat.ME cs.CE physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents the estimation of the parameters of an experimental setup,\nwhich is modeled as a system with three degrees of freedom, composed by a\nshaft, two rotors, and a DC motor, that emulates a drilling process. A Bayesian\ntechnique is used in the estimation process, to take into account the\nuncertainties and variabilities intrinsic to the measurement taken, which are\nmodeled as a noise of Gaussian nature. With this procedure it is expected to\ncheck the reliability of the nominal values of the physical parameters of the\ntest rig. An estimation process assuming that nine parameters of the\nexperimental apparatus are unknown is conducted, and the results show that for\nsome quantities the relative deviation with respect to the nominal values is\nvery high. This deviation evidentiates a strong deficiency in the mathematical\nmodel used to describe the dynamic behavior of the experimental apparatus.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 03:14:09 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Sandoval", "Mario Germ\u00e1n", ""], ["Cunha", "Americo", "Jr"], ["Sampaio", "Rubens", ""]]}, {"id": "2107.13737", "submitter": "Lihua Lei", "authors": "Dmitry Arkhangelsky, Guido W. Imbens, Lihua Lei, Xiaoman Luo", "title": "Double-Robust Two-Way-Fixed-Effects Regression For Panel Data", "comments": "Main text (39 pages); appendix (61 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN q-fin.EC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new estimator for the average causal effects of a binary\ntreatment with panel data in settings with general treatment patterns. Our\napproach augments the two-way-fixed-effects specification with the\nunit-specific weights that arise from a model for the assignment mechanism. We\nshow how to construct these weights in various settings, including situations\nwhere units opt into the treatment sequentially. The resulting estimator\nconverges to an average (over units and time) treatment effect under the\ncorrect specification of the assignment model. We show that our estimator is\nmore robust than the conventional two-way estimator: it remains consistent if\neither the assignment mechanism or the two-way regression model is correctly\nspecified and performs better than the two-way-fixed-effect estimator if both\nare locally misspecified. This strong double robustness property quantifies the\nbenefits from modeling the assignment process and motivates using our estimator\nin practice.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 04:09:57 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Arkhangelsky", "Dmitry", ""], ["Imbens", "Guido W.", ""], ["Lei", "Lihua", ""], ["Luo", "Xiaoman", ""]]}, {"id": "2107.13756", "submitter": "Yuting Ye", "authors": "Yuting Ye, Peter J. Bickel", "title": "Binomial Mixture Model With U-shape Constraint", "comments": "45 pages, 26 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study the binomial mixture model under the regime that\nthe binomial size $m$ can be relatively large compared to the sample size $n$.\nThis project is motivated by the GeneFishing method (Liu et al., 2019), whose\noutput is a combination of the parameter of interest and the subsampling noise.\nTo tackle the noise in the output, we utilize the observation that the density\nof the output has a U shape and model the output with the binomial mixture\nmodel under a U shape constraint. We first analyze the estimation of the\nunderlying distribution F in the binomial mixture model under various\nconditions for F. Equipped with these theoretical understandings, we propose a\nsimple method Ucut to identify the cutoffs of the U shape and recover the\nunderlying distribution based on the Grenander estimator (Grenander, 1956). It\nhas been shown that when $m = {\\Omega}(n^{\\frac{2}{3}})$, he identified cutoffs\nconverge at the rate $O(n^{-\\frac{1}{3}})$. The $L_1$ distance between the\nrecovered distribution and the true one decreases at the same rate. To\ndemonstrate the performance, we apply our method to varieties of simulation\nstudies, a GTEX dataset used in (Liu et al., 2019) and a single cell dataset\nfrom Tabula Muris.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 05:53:25 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Ye", "Yuting", ""], ["Bickel", "Peter J.", ""]]}, {"id": "2107.13779", "submitter": "Gery Geenens", "authors": "Gery Geenens, Alicia Nieto-Reyes and Giacomo Francisci", "title": "Statistical depth in abstract metric spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of depth has proved very important for multivariate and\nfunctional data analysis, as it essentially acts as a surrogate for the notion\na ranking of observations which is absent in more than one dimension. Motivated\nby the rapid development of technology, in particular the advent of `Big Data',\nwe extend here that concept to general metric spaces, propose a natural depth\nmeasure and explore its properties as a statistical depth function. Working in\na general metric space allows the depth to be tailored to the data at hand and\nto the ultimate goal of the analysis, a very desirable property given the\npolymorphic nature of modern data sets. This flexibility is thoroughly\nillustrated by several real data analyses.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 07:22:45 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Geenens", "Gery", ""], ["Nieto-Reyes", "Alicia", ""], ["Francisci", "Giacomo", ""]]}, {"id": "2107.13920", "submitter": "Antonio D'Ambrosio Dr.", "authors": "Alessio Baldassarre, Elise Dusseldorp, Antonio D'Ambrosio, Mark de\n  Rooij, Claudio Conversano", "title": "The Bradly-Terry Regression Trunk approach for modelling preference data\n  with small trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces the Bradley-Terry Regression Trunk model, a novel\nprobabilistic approach for the analysis of preference data expressed through\npaired comparison rankings. In some cases, it may be reasonable to assume that\nthe preferences expressed by individuals depend on their characteristics.\nWithin the framework of tree-based partitioning, we specify a tree-based model\nestimating the joint effects of subject-specific covariates over and above\ntheir main effects. We combine a tree-based model and the log-linear\nBradley-Terry model using the outcome of the comparisons as response variable.\nThe proposed model provides a solution to discover interaction effects when no\na-priori hypotheses are available. It produces a small tree, called trunk, that\nrepresents a fair compromise between a simple interpretation of the interaction\neffects and an easy to read partition of judges based on their characteristics\nand the preferences they have expressed. We present an application on a real\ndata set following two different approaches, and a simulation study to test the\nmodel's performance. Simulations showed that the quality of the model\nperformance increases when the number of rankings and objects increases. In\naddition, the performance is considerably amplified when the judges'\ncharacteristics have a high impact on their choices.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 11:57:30 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Baldassarre", "Alessio", ""], ["Dusseldorp", "Elise", ""], ["D'Ambrosio", "Antonio", ""], ["de Rooij", "Mark", ""], ["Conversano", "Claudio", ""]]}, {"id": "2107.13956", "submitter": "Li Su", "authors": "Li Su, Yafeng Cheng, Dora I.A. Pereira, Jonathan J. Powell", "title": "Modelling disease progression with multi-level electronic health records\n  data and informative observation times: an application to treating iron\n  deficiency anaemia in primary care of the UK", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modelling disease progression of iron deficiency anaemia (IDA) following oral\niron supplement prescriptions is a prerequisite for evaluating the\ncost-effectiveness of oral iron supplements. Electronic health records (EHRs)\nfrom the Clinical Practice Research Datalink (CPRD) provide rich longitudinal\ndata on IDA disease progression in patients registered with 663 General\nPractitioner (GP) practices in the UK, but they also create challenges in\nstatistical analyses. First, the CPRD data are clustered at multi-levels (i.e.,\nGP practices and patients), but their large volume makes it computationally\ndifficult to implement estimation of standard random effects models for\nmulti-level data. Second, observation times in the CPRD data are irregular and\ncould be informative about the disease progression. For example, shorter/longer\ngap times between GP visits could be associated with deteriorating/improving\nIDA. Existing methods to address informative observation times are mostly based\non complex joint models, which adds more computational burden. To tackle these\nchallenges, we develop a computationally efficient approach to modelling\ndisease progression with EHRs data while accounting for variability at\nmulti-level clusters and informative observation times. We apply the proposed\nmethod to the CPRD data to investigate IDA improvement and treatment\nintolerance following oral iron prescriptions in primary care of the UK.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 13:34:55 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Su", "Li", ""], ["Cheng", "Yafeng", ""], ["Pereira", "Dora I. A.", ""], ["Powell", "Jonathan J.", ""]]}, {"id": "2107.14054", "submitter": "Noa Kallioinen", "authors": "Noa Kallioinen, Topi Paananen, Paul-Christian B\\\"urkner, Aki Vehtari", "title": "Detecting and diagnosing prior and likelihood sensitivity with\n  power-scaling", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the sensitivity of the posterior to perturbations of the prior\nand likelihood is an important part of the Bayesian workflow. We introduce a\npractical and computationally efficient sensitivity analysis approach that is\napplicable to a wide range of models, based on power-scaling perturbations. We\nsuggest a diagnostic based on this that can indicate the presence of prior-data\nconflict or likelihood noninformativity. The approach can be easily included in\nBayesian workflows with minimal work by the model builder. We present the\nimplementation of the approach in our new R package priorsense and demonstrate\nthe workflow on case studies of real data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 14:43:49 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Kallioinen", "Noa", ""], ["Paananen", "Topi", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Vehtari", "Aki", ""]]}, {"id": "2107.14151", "submitter": "Aniruddha Rajendra Rao", "authors": "Aniruddha Rajendra Rao, Matthew Reimherr", "title": "Modern Non-Linear Function-on-Function Regression", "comments": "6 figures, 5 tables (including supplementary material), 16 pages\n  (including supplementary material). arXiv admin note: text overlap with\n  arXiv:2104.09371", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new class of non-linear function-on-function regression models\nfor functional data using neural networks. We propose a framework using a\nhidden layer consisting of continuous neurons, called a continuous hidden\nlayer, for functional response modeling and give two model fitting strategies,\nFunctional Direct Neural Network (FDNN) and Functional Basis Neural Network\n(FBNN). Both are designed explicitly to exploit the structure inherent in\nfunctional data and capture the complex relations existing between the\nfunctional predictors and the functional response. We fit these models by\nderiving functional gradients and implement regularization techniques for more\nparsimonious results. We demonstrate the power and flexibility of our proposed\nmethod in handling complex functional models through extensive simulation\nstudies as well as real data examples.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 16:19:59 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Rao", "Aniruddha Rajendra", ""], ["Reimherr", "Matthew", ""]]}, {"id": "2107.14172", "submitter": "Michael Celentano", "authors": "Michael Celentano, Andrea Montanari", "title": "CAD: Debiasing the Lasso with inaccurate covariate model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of estimating a low-dimensional parameter in\nhigh-dimensional linear regression. Constructing an approximately unbiased\nestimate of the parameter of interest is a crucial step towards performing\nstatistical inference. Several authors suggest to orthogonalize both the\nvariable of interest and the outcome with respect to the nuisance variables,\nand then regress the residual outcome with respect to the residual variable.\nThis is possible if the covariance structure of the regressors is perfectly\nknown, or is sufficiently structured that it can be estimated accurately from\ndata (e.g., the precision matrix is sufficiently sparse).\n  Here we consider a regime in which the covariate model can only be estimated\ninaccurately, and hence existing debiasing approaches are not guaranteed to\nwork. When errors in estimating the covariate model are correlated with errors\nin estimating the linear model parameter, an incomplete elimination of the bias\noccurs. We propose the Correlation Adjusted Debiased Lasso (CAD), which nearly\neliminates this bias in some cases, including cases in which the estimation\nerrors are neither negligible nor orthogonal.\n  We consider a setting in which some unlabeled samples might be available to\nthe statistician alongside labeled ones (semi-supervised learning), and our\nguarantees hold under the assumption of jointly Gaussian covariates. The new\ndebiased estimator is guaranteed to cancel the bias in two cases: (1) when the\ntotal number of samples (labeled and unlabeled) is larger than the number of\nparameters, or (2) when the covariance of the nuisance (but not the effect of\nthe nuisance on the variable of interest) is known. Neither of these cases is\ntreated by state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 16:49:26 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Celentano", "Michael", ""], ["Montanari", "Andrea", ""]]}, {"id": "2107.14197", "submitter": "Fredrik S\\\"avje", "authors": "Fredrik S\\\"avje", "title": "Randomization does not imply unconfoundedness", "comments": "Presented at The Neglected Assumptions in Causal Inference Workshop @\n  ICML 2021 ( https://sites.google.com/view/naci2021/ )", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common assumption in causal inference is that random treatment assignment\nensures that potential outcomes are independent of treatment, or in one word,\nunconfoundedness. This paper highlights that randomization and unconfoundedness\nare separate properties, and neither implies the other. A study with random\ntreatment assignment does not have to be unconfounded, and a study with\ndeterministic assignment can still be unconfounded. A corollary is that a\npropensity score is not the same thing as a treatment assignment probability.\nThese facts should not be taken as arguments against randomization. The moral\nof this paper is that randomization is useful only when investigators know or\ncan reconstruct the assignment process.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:37:34 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["S\u00e4vje", "Fredrik", ""]]}]