[{"id": "2007.00096", "submitter": "Suzie Brown", "authors": "Suzie Brown, Paul A. Jenkins, Adam M. Johansen, Jere Koskela", "title": "Simple conditions for convergence of sequential Monte Carlo genealogies\n  with applications", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present simple conditions under which the limiting genealogical process\nassociated with a class of interacting particle systems with non-neutral\nselection mechanisms, as the number of particles grows, is a time-rescaled\nKingman coalescent. Sequential Monte Carlo algorithms are popular methods for\napproximating integrals in problems such as non-linear filtering and smoothing\nwhich employ this type of particle system. Their performance depends strongly\non the properties of the induced genealogical process. We verify the conditions\nof our main result for standard sequential Monte Carlo algorithms with a broad\nclass of low-variance resampling schemes, as well as for conditional sequential\nMonte Carlo with multinomial resampling.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 20:28:02 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 13:08:04 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Brown", "Suzie", ""], ["Jenkins", "Paul A.", ""], ["Johansen", "Adam M.", ""], ["Koskela", "Jere", ""]]}, {"id": "2007.00106", "submitter": "Andrew Giffin", "authors": "Andrew Giffin, Brian Reich, Shu Yang, Ana Rappold", "title": "Generalized propensity score approach to causal inference with spatial\n  interference", "comments": "Submitted. 15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many spatial phenomena exhibit treatment interference where treatments at one\nlocation may affect the response at other locations. Because interference\nviolates the stable unit treatment value assumption, standard methods for\ncausal inference do not apply. We propose a new causal framework to recover\ndirect and spill-over effects in the presence of spatial interference, taking\ninto account that treatments at nearby locations are more influential than\ntreatments at locations further apart. Under the no unmeasured confounding\nassumption, we show that a generalized propensity score is sufficient to remove\nall measured confounding. To reduce dimensionality issues, we propose a\nBayesian spline-based regression model accounting for a sufficient set of\nvariables for the generalized propensity score. A simulation study demonstrates\nthe accuracy and coverage properties. We apply the method to estimate the\ncausal effect of wildland fires on air pollution in the Western United States\nover 2005--2018.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 21:07:06 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Giffin", "Andrew", ""], ["Reich", "Brian", ""], ["Yang", "Shu", ""], ["Rappold", "Ana", ""]]}, {"id": "2007.00150", "submitter": "Graciela Boente Prof.", "authors": "Ana M. Bianco, Graciela Boente and Wenceslao Gonzalez-Manteiga", "title": "A robust approach for ROC curves with covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Receiver Operating Characteristic (ROC) curve is a useful tool that\nmeasures the discriminating power of a continuous variable or the accuracy of a\npharmaceutical or medical test to distinguish between two conditions or\nclasses. In certain situations, the practitioner may be able to measure some\ncovariates related to the diagnostic variable which can increase the\ndiscriminating power of the ROC curve. To protect against the existence of\natypical data among the observations, a procedure to obtain robust estimators\nfor the ROC curve in presence of covariates is introduced. The considered\nproposal focusses on a semiparametric approach which fits a location-scale\nregression model to the diagnostic variable and considers empirical estimators\nof the regression residuals distributions. Robust parametric estimators are\ncombined with adaptive weighted empirical distribution estimators to\ndown-weight the influence of outliers. The uniform consistency of the proposal\nis derived under mild assumptions. A Monte Carlo study is carried out to\ncompare the performance of the robust proposed estimators with the classical\nones both, in clean and contaminated samples. A real data set is also analysed.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 23:43:58 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Bianco", "Ana M.", ""], ["Boente", "Graciela", ""], ["Gonzalez-Manteiga", "Wenceslao", ""]]}, {"id": "2007.00180", "submitter": "Hamed Nikbakht", "authors": "Konstantinos G. Papakonstantinou and Hamed Nikbakht", "title": "Hamiltonian MCMC methods for estimating rare events probabilities in\n  high-dimensional problems", "comments": "arXiv admin note: text overlap with arXiv:1909.03575", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and efficient estimation of rare events probabilities is of\nsignificant importance, since often the occurrences of such events have\nwidespread impacts. The focus in this work is on precisely quantifying these\nprobabilities, often encountered in reliability analysis of complex engineering\nsystems, based on an introduced framework termed Approximate Sampling Target\nwith Post-processing Adjustment (ASTPA), which herein is integrated with and\nsupported by gradient-based Hamiltonian Markov Chain Monte Carlo (HMCMC)\nmethods. The basic idea is to construct a relevant target distribution by\nweighting the high-dimensional random variable space through a one-dimensional\noutput likelihood model, using the limit-state function. To sample from this\ntarget distribution, we exploit HMCMC algorithms, a family of MCMC methods that\nadopts physical system dynamics, rather than solely using a proposal\nprobability distribution, to generate distant sequential samples, and we\ndevelop a new Quasi-Newton mass preconditioned HMCMC scheme (QNp-HMCMC), which\nis particularly efficient and suitable for high-dimensional spaces. To\neventually compute the rare event probability, an original post-sampling step\nis devised using an inverse importance sampling procedure based on the already\nobtained samples. The statistical properties of the estimator are analyzed as\nwell, and the performance of the proposed methodology is examined in detail and\ncompared against Subset Simulation in a series of challenging low- and\nhigh-dimensional problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 01:56:51 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Papakonstantinou", "Konstantinos G.", ""], ["Nikbakht", "Hamed", ""]]}, {"id": "2007.00185", "submitter": "Gregorio Caetano", "authors": "Carolina Caetano and Gregorio Caetano and Juan Carlos Escanciano", "title": "Regression Discontinuity Design with Multivalued Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study identification and estimation in the Regression Discontinuity Design\n(RDD) with a multivalued treatment variable. We also allow for the inclusion of\ncovariates. We show that without additional information, treatment effects are\nnot identified. We give necessary and sufficient conditions that lead to\nidentification of LATEs as well as of weighted averages of the conditional\nLATEs. We show that if the first stage discontinuities of the multiple\ntreatments conditional on covariates are linearly independent, then it is\npossible to identify multivariate weighted averages of the treatment effects\nwith convenient identifiable weights. If, moreover, treatment effects do not\nvary with some covariates or a flexible parametric structure can be assumed, it\nis possible to identify (in fact, over-identify) all the treatment effects. The\nover-identification can be used to test these assumptions. We propose a simple\nestimator, which can be programmed in packaged software as a Two-Stage Least\nSquares regression, and packaged standard errors and tests can also be used.\nFinally, we implement our approach to identify the effects of different types\nof insurance coverage on health care utilization, as in Card, Dobkin and\nMaestas (2008).\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 02:22:26 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Caetano", "Carolina", ""], ["Caetano", "Gregorio", ""], ["Escanciano", "Juan Carlos", ""]]}, {"id": "2007.00187", "submitter": "Yi Liu", "authors": "Yi Liu and Veronika Rockova", "title": "Variable Selection via Thompson Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling is a heuristic algorithm for the multi-armed bandit problem\nwhich has a long tradition in machine learning. The algorithm has a Bayesian\nspirit in the sense that it selects arms based on posterior samples of reward\nprobabilities of each arm. By forging a connection between combinatorial binary\nbandits and spike-and-slab variable selection, we propose a stochastic\noptimization approach to subset selection called Thompson Variable Selection\n(TVS). TVS is a framework for interpretable machine learning which does not\nrely on the underlying model to be linear. TVS brings together Bayesian\nreinforcement and machine learning in order to extend the reach of Bayesian\nsubset selection to non-parametric models and large datasets with very many\npredictors and/or very many observations. Depending on the choice of a reward,\nTVS can be deployed in offline as well as online setups with streaming data\nbatches. Tailoring multiplay bandits to variable selection, we provide regret\nbounds without necessarily assuming that the arm mean rewards be unrelated. We\nshow a very strong empirical performance on both simulated and real data.\nUnlike deterministic optimization methods for spike-and-slab variable\nselection, the stochastic nature makes TVS less prone to local convergence and\nthereby more robust.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 02:22:53 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 19:31:04 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Liu", "Yi", ""], ["Rockova", "Veronika", ""]]}, {"id": "2007.00248", "submitter": "Tin Lok James Ng", "authors": "Tin Lok James Ng, Andrew Zammit-Mangion", "title": "Non-Homogeneous Poisson Process Intensity Modeling and Estimation using\n  Measure Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-homogeneous Poisson processes are used in a wide range of scientific\ndisciplines, ranging from the environmental sciences to the health sciences.\nOften, the central object of interest in a point process is the underlying\nintensity function. Here, we present a general model for the intensity function\nof a non-homogeneous Poisson process using measure transport. The model is\nbuilt from a flexible bijective mapping that maps from the underlying intensity\nfunction of interest to a simpler reference intensity function. We enforce\nbijectivity by modeling the map as a composition of multiple simple bijective\nmaps, and show that the model exhibits an important approximation property.\nEstimation of the flexible mapping is accomplished within an optimization\nframework, wherein computations are efficiently done using recent technological\nadvances in deep learning and a graphics processing unit. Although we find that\nintensity function estimates obtained with our method are not necessarily\nsuperior to those obtained using conventional methods, the modeling\nrepresentation brings with it other advantages such as facilitated point\nprocess simulation and uncertainty quantification. Modeling point processes in\nhigher dimensions is also facilitated using our approach. We illustrate the use\nof our model on both simulated data, and a real data set containing the\nlocations of seismic events near Fiji since 1964.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 05:16:57 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Ng", "Tin Lok James", ""], ["Zammit-Mangion", "Andrew", ""]]}, {"id": "2007.00267", "submitter": "Jana de Wiljes", "authors": "Elena Saggioro, Jana de Wiljes, Marlene Kretschmer and Jakob Runge", "title": "Reconstructing regime-dependent causal relationships from observational\n  time series", "comments": null, "journal-ref": null, "doi": "10.1063/5.0020538", "report-no": null, "categories": "stat.ME math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring causal relations from observational time series data is a key\nproblem across science and engineering whenever experimental interventions are\ninfeasible or unethical. Increasing data availability over the past decades has\nspurred the development of a plethora of causal discovery methods, each\naddressing particular challenges of this difficult task. In this paper we focus\non an important challenge that is at the core of time series causal discovery:\nregime-dependent causal relations. Often dynamical systems feature transitions\ndepending on some, often persistent, unobserved background regime, and\ndifferent regimes may exhibit different causal relations. Here, we assume a\npersistent and discrete regime variable leading to a finite number of regimes\nwithin which we may assume stationary causal relations. To detect\nregime-dependent causal relations, we combine the conditional\nindependence-based PCMCI method with a regime learning optimisation approach.\nPCMCI allows for linear and nonlinear, high-dimensional time series causal\ndiscovery. Our method, Regime-PCMCI, is evaluated on a number of numerical\nexperiments demonstrating that it can distinguish regimes with different causal\ndirections, time lags, effects and sign of causal links, as well as changes in\nthe variables' autocorrelation. Further, Regime-PCMCI is employed to\nobservations of El Ni\\~no Southern Oscillation and Indian rainfall,\ndemonstrating skill also in real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 06:30:45 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Saggioro", "Elena", ""], ["de Wiljes", "Jana", ""], ["Kretschmer", "Marlene", ""], ["Runge", "Jakob", ""]]}, {"id": "2007.00296", "submitter": "Sothea Has", "authors": "Sothea Has (LPSM)", "title": "A Kernel-based Consensual Aggregation for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce a kernel-based consensual aggregation method\nfor regression problems. We aim to flexibly combine individual regression\nestimators $r_1, r_2, \\ldots, r_M$ using a weighted average where the weights\nare defined based on some kernel function to build a target prediction. This\nwork extends the context of Biau et al. (2016) to a more general kernel-based\nframework. We show that this more general configuration also inherits the\nconsistency of the basic consistent estimators. Moreover, an optimization\nmethod based on gradient descent algorithm is proposed to efficiently and\nrapidly estimate the key parameter of the strategy. The numerical experiments\ncarried out on several simulated and real datasets are also provided to\nillustrate the speed-up of gradient descent algorithm in estimating the key\nparameter and the improvement of overall performance of the method with the\nintroduction of smoother kernel functions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 07:43:30 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 07:59:44 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 08:54:23 GMT"}, {"version": "v4", "created": "Wed, 28 Apr 2021 09:02:45 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Has", "Sothea", "", "LPSM"]]}, {"id": "2007.00340", "submitter": "Evangelia Kalligiannaki", "authors": "Tangxin Jin, Anthony Chazirakis, Evangelia Kalligiannaki, Vagelis\n  Harmandaris, Markos A. Katsoulakis", "title": "Data-driven Uncertainty Quantification for Systematic Coarse-grained\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present methodologies for the quantification of confidence\nin bottom-up coarse-grained models for molecular and macromolecular systems.\nCoarse-graining methods have been extensively used in the past decades in order\nto extend the length and time scales accessible by simulation methodologies.\nThe quantification, though, of induced errors due to the limited availability\nof fine-grained data is not yet established. Here, we employ rigorous\nstatistical methods to deduce guarantees for the optimal coarse models obtained\nvia approximations of the multi-body potential of mean force, with the relative\nentropy, the relative entropy rate minimization, and the force matching\nmethods. Specifically, we present and apply statistical approaches, such as\nbootstrap and jackknife, to infer confidence sets for a limited number of\nsamples, i.e., molecular configurations. Moreover, we estimate asymptotic\nconfidence intervals assuming adequate sampling of the phase space. We\ndemonstrate the need for non-asymptotic methods and quantify confidence sets\nthrough two applications. The first is a two-scale fast/slow diffusion process\nprojected on the slow process. With this benchmark example, we establish the\nmethodology for both independent and time-series data. Second, we apply these\nuncertainty quantification approaches on a polymeric bulk system. We consider\nan atomistic polyethylene melt as the prototype system for developing\ncoarse-graining tools for macromolecular systems. For this system, we estimate\nthe coarse-grained force field and present confidence levels with respect to\nthe number of available microscopic data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 09:10:38 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Jin", "Tangxin", ""], ["Chazirakis", "Anthony", ""], ["Kalligiannaki", "Evangelia", ""], ["Harmandaris", "Vagelis", ""], ["Katsoulakis", "Markos A.", ""]]}, {"id": "2007.00402", "submitter": "Christian Agrell", "authors": "Christian Agrell and Kristina Rognlien Dahl", "title": "Sequential Bayesian optimal experimental design for structural\n  reliability analysis", "comments": "27 pages, 13 figures", "journal-ref": "Statistics and Computing, vol. 31, no. 27 (2021)", "doi": "10.1007/s11222-021-10000-2", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural reliability analysis is concerned with estimation of the\nprobability of a critical event taking place, described by $P(g(\\textbf{X})\n\\leq 0)$ for some $n$-dimensional random variable $\\textbf{X}$ and some\nreal-valued function $g$. In many applications the function $g$ is practically\nunknown, as function evaluation involves time consuming numerical simulation or\nsome other form of experiment that is expensive to perform. The problem we\naddress in this paper is how to optimally design experiments, in a Bayesian\ndecision theoretic fashion, when the goal is to estimate the probability\n$P(g(\\textbf{X}) \\leq 0)$ using a minimal amount of resources. As opposed to\nexisting methods that have been proposed for this purpose, we consider a\ngeneral structural reliability model given in hierarchical form. We therefore\nintroduce a general formulation of the experimental design problem, where we\ndistinguish between the uncertainty related to the random variable $\\textbf{X}$\nand any additional epistemic uncertainty that we want to reduce through\nexperimentation. The effectiveness of a design strategy is evaluated through a\nmeasure of residual uncertainty, and efficient approximation of this quantity\nis crucial if we want to apply algorithms that search for an optimal strategy.\nThe method we propose is based on importance sampling combined with the\nunscented transform for epistemic uncertainty propagation. We implement this\nfor the myopic (one-step look ahead) alternative, and demonstrate the\neffectiveness through a series of numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 11:47:15 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Agrell", "Christian", ""], ["Dahl", "Kristina Rognlien", ""]]}, {"id": "2007.00454", "submitter": "Maochao Xu", "authors": "Lei Hua, Maochao Xu", "title": "Pricing cyber insurance for a large-scale network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facing the lack of cyber insurance loss data, we propose an innovative\napproach for pricing cyber insurance for a large-scale network based on\nsynthetic data. The synthetic data is generated by the proposed risk spreading\nand recovering algorithm that allows infection and recovery events to occur\nsequentially, and allows dependence of random waiting time to infection for\ndifferent nodes. The scale-free network framework is adopted to account for the\ntopology uncertainty of the random large-scale network. Extensive simulation\nstudies are conducted to understand the risk spreading and recovering\nmechanism, and to uncover the most important underwriting risk factors. A case\nstudy is also presented to demonstrate that the proposed approach and algorithm\ncan be adapted accordingly to provide reference for cyber insurance pricing.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 13:54:14 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Hua", "Lei", ""], ["Xu", "Maochao", ""]]}, {"id": "2007.00456", "submitter": "Christoph Muehlmann", "authors": "Christoph Muehlmann, Kamila Fa\\v{c}evicov\\'a, Al\\v{z}b\\v{e}ta Gardlo,\n  Hana Jane\\v{c}kov\\'a, Klaus Nordhausen", "title": "Independent Component Analysis for Compositional Data", "comments": null, "journal-ref": "Advances in Contemporary Statistics and Econometrics. Springer,\n  Cham. 2021", "doi": "10.1007/978-3-030-73249-3_27", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional data represent a specific family of multivariate data, where\nthe information of interest is contained in the ratios between parts rather\nthan in absolute values of single parts. The analysis of such specific data is\nchallenging as the application of standard multivariate analysis tools on the\nraw observations can lead to spurious results. Hence, it is appropriate to\napply certain transformations prior further analysis. One popular multivariate\ndata analysis tool is independent component analysis. Independent component\nanalysis aims to find statistically independent components in the data and as\nsuch might be seen as an extension to principal component analysis. In this\npaper we examine an approach of how to apply independent component analysis on\ncompositional data by respecting the nature of the former and demonstrate the\nusefulness of this procedure on a metabolomic data set.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 12:57:44 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Muehlmann", "Christoph", ""], ["Fa\u010devicov\u00e1", "Kamila", ""], ["Gardlo", "Al\u017eb\u011bta", ""], ["Jane\u010dkov\u00e1", "Hana", ""], ["Nordhausen", "Klaus", ""]]}, {"id": "2007.00520", "submitter": "Tyler VanderWeele", "authors": "Tyler J. VanderWeele", "title": "Constructed measures and causal inference: towards a new model of\n  measurement for psychosocial constructs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Psychosocial constructs can only be assessed indirectly, and measures are\ntypically formed by a combination of indicators that are thought to relate to\nthe construct. Reflective and formative measurement models offer different\nconceptualizations of the relation between the indicators and what is sometimes\nconceived of as a univariate latent variable supposed to correspond in some way\nto the construct. It is argued that the empirical implications of reflective\nand formative models will often be violated by data since the causally relevant\nconstituents will generally be multivariate, not univariate. These empirical\nimplications can be formally tested but factor analysis is not adequate to do\nso. It is argued that formative models misconstrue the relationship between the\nconstructed measures and the underlying reality by which causal processes\noperate, but that reflective models misconstrue the nature of the underlying\nreality itself by typically presuming that the constituents of it that are\ncausally efficacious are unidimensional. The ensuing problems arising from\nthese misconstruals are discussed. A causal interpretation is proposed of\nassociations between constructed measures and various outcomes that is\napplicable to both reflective and formative models and is applicable even if\nthe usual assumptions of these models are violated. An outline for a new model\nof the process of measure construction is put forward. Discussion is given to\nthe practical implications of these observations and proposals for the\nprovision of definitions, the selection of items, item-by-item analyses, the\nconstruction of measures, and the interpretation of the associations of these\nmeasures with subsequent outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 14:34:50 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 01:31:20 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["VanderWeele", "Tyler J.", ""]]}, {"id": "2007.00529", "submitter": "Matteo Fontana", "authors": "Fabio Centofanti, Matteo Fontana, Antonio Lepore and Simone Vantini", "title": "Smooth Lasso Estimator for the Function-on-Function Linear Regression\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": "MOX-Report:33/2020", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new estimator, named as S-LASSO, is proposed for the coefficient function\nof a functional linear regression model where values of the response function,\nat a given domain point, depends on the full trajectory of the covariate\nfunction. The S-LASSO estimator is shown to be able to increase the\ninterpretability of the model, by better locating regions where the coefficient\nfunction is zero, and to smoothly estimate non-zero values of the coefficient\nfunction. The sparsity of the estimator is ensured by a functional LASSO\npenalty whereas the smoothness is provided by two roughness penalties. The\nresulting estimator is proved to be estimation and pointwise sign consistent.\nVia an extensive Monte Carlo simulation study, the estimation and predictive\nperformance of the S-LASSO estimator are shown to be better than (or at worst\ncomparable with) competing estimators already presented in the literature\nbefore. Practical advantages of the S-LASSO estimator are illustrated through\nthe analysis of the well known \\textit{Canadian weather} and \\textit{Swedish\nmortality data\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 14:48:44 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Centofanti", "Fabio", ""], ["Fontana", "Matteo", ""], ["Lepore", "Antonio", ""], ["Vantini", "Simone", ""]]}, {"id": "2007.00574", "submitter": "Imma Valentina Curato Dr", "authors": "Dirk-Philip Brandes, Imma Valentina Curato and Robert Stelzer", "title": "Inheritance of strong mixing and weak dependence under renewal sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X$ be a continuous-time strongly mixing or weakly dependent process and\n$T$ a renewal process independent of $X$ with inter-arrival times $\\{\\tau_i\\}$.\nWe show general conditions under which the sampled process\n$(X_{T_i},\\tau_i)^{\\top}$ is strongly mixing or weakly dependent. Moreover, we\nexplicitly compute the strong mixing or weak dependence coefficients of the\nrenewal sampled process and show that exponential or power decay of the\ncoefficients of $X$ is preserved (at least asymptotically). Our results imply\nthat essentially all central limit theorems available in the literature for\nstrongly mixing or weakly dependent processes can be applied when renewal\nsampled observations of the process $X$ are at disposal.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:00:28 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Brandes", "Dirk-Philip", ""], ["Curato", "Imma Valentina", ""], ["Stelzer", "Robert", ""]]}, {"id": "2007.00596", "submitter": "Fan Chen", "authors": "Fan Chen and Karl Rohe", "title": "A New Basis for Sparse PCA", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The statistical and computational performance of sparse principal component\nanalysis (PCA) can be dramatically improved when the principal components are\nallowed to be sparse in a rotated eigenbasis. For this, we propose a new method\nfor sparse PCA. In the simplest version of the algorithm, the component scores\nand loadings are initialized with a low-rank singular value decomposition.\nThen, the singular vectors are rotated with orthogonal rotations to make them\napproximately sparse. Finally, soft-thresholding is applied to the rotated\nsingular vectors. This approach differs from prior approaches because it uses\nan orthogonal rotation to approximate a sparse basis. Our sparse PCA framework\nis versatile; for example, it extends naturally to the two-way analysis of a\ndata matrix for simultaneous dimensionality reduction of rows and columns. We\nidentify the close relationship between sparse PCA and independent component\nanalysis for separating sparse signals. We provide empirical evidence showing\nthat for the same level of sparsity, the proposed sparse PCA method is more\nstable and can explain more variance compared to alternative methods. Through\nthree applications---sparse coding of images, analysis of transcriptome\nsequencing data, and large-scale clustering of Twitter accounts, we demonstrate\nthe usefulness of sparse PCA in exploring modern multivariate data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:32:22 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Chen", "Fan", ""], ["Rohe", "Karl", ""]]}, {"id": "2007.00628", "submitter": "Noam Finkelstein", "authors": "Noam Finkelstein, Ilya Shpitser", "title": "Deriving Bounds and Inequality Constraints Using LogicalRelations Among\n  Counterfactuals", "comments": null, "journal-ref": "Proceedings of the Thirty Sixth Conference on Uncertainty in\n  Artificial Intelligence (UAI-36th), 2020", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal parameters may not be point identified in the presence of unobserved\nconfounding. However, information about non-identified parameters, in the form\nof bounds, may still be recovered from the observed data in some cases. We\ndevelop a new general method for obtaining bounds on causal parameters using\nrules of probability and restrictions on counterfactuals implied by causal\ngraphical models. We additionally provide inequality constraints on functionals\nof the observed data law implied by such causal models. Our approach is\nmotivated by the observation that logical relations between identified and\nnon-identified counterfactual events often yield information about\nnon-identified events. We show that this approach is powerful enough to recover\nknown sharp bounds and tight inequality constraints, and to derive novel bounds\nand constraints.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:25:44 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Finkelstein", "Noam", ""], ["Shpitser", "Ilya", ""]]}, {"id": "2007.00723", "submitter": "Ning Ning", "authors": "Ning Ning, Edward Ionides, Ya'acov Ritov", "title": "Scalable Monte Carlo Inference and Rescaled Local Asymptotic Normality", "comments": "41 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we generalize the property of local asymptotic normality (LAN)\nto an enlarged neighborhood, under the name of rescaled local asymptotic\nnormality (RLAN). We obtain sufficient conditions for a regular parametric\nmodel to satisfy RLAN. We show that RLAN supports the construction of a\nstatistically efficient estimator which maximizes a cubic approximation to the\nlog-likelihood on this enlarged neighborhood. In the context of Monte Carlo\ninference, we find that this maximum cubic likelihood estimator can maintain\nits statistical efficiency in the presence of asymptotically increasing Monte\nCarlo error in likelihood evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:54:51 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 20:56:20 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ning", "Ning", ""], ["Ionides", "Edward", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "2007.00725", "submitter": "Oliver Hines", "authors": "Oliver Hines, Stijn Vansteelandt, Karla Diaz-Ordaz", "title": "Robust Inference for Mediated Effects in Partially Linear Models", "comments": null, "journal-ref": null, "doi": "10.1007/s11336-021-09768-z", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider mediated effects of an exposure, X on an outcome, Y, via a\nmediator, M, under no unmeasured confounding assumptions in the setting where\nmodels for the conditional expectation of the mediator and outcome are\npartially linear. We propose G-estimators for the direct and indirect effect\nand demonstrate consistent asymptotic normality for indirect effects when\nmodels for the conditional means of M, or X and Y are correctly specified, and\nfor direct effects, when models for the conditional means of Y, or X and M are\ncorrect. This marks an improvement, in this particular setting, over previous\n`triple' robust methods, which do not assume partially linear mean models.\nTesting of the no-mediation hypothesis is inherently problematic due to the\ncomposite nature of the test (either X has no effect on M or M no effect on Y),\nleading to low power when both effect sizes are small. We use Generalized\nMethods of Moments (GMM) results to construct a new score testing framework,\nwhich includes as special cases the no-mediation and the no-direct-effect\nhypotheses. The proposed tests rely on an orthogonal estimation strategy for\nestimating nuisance parameters. Simulations show that the GMM based tests\nperform better in terms of power and small sample performance compared with\ntraditional tests in the partially linear setting, with drastic improvement\nunder model misspecification. New methods are illustrated in a mediation\nanalysis of data from the COPERS trial, a randomized trial investigating the\neffect of a non-pharmacological intervention of patients suffering from chronic\npain. An accompanying R package implementing these methods can be found at\ngithub.com/ohines/plmed.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:58:07 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 22:29:22 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 16:02:06 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Hines", "Oliver", ""], ["Vansteelandt", "Stijn", ""], ["Diaz-Ordaz", "Karla", ""]]}, {"id": "2007.00742", "submitter": "Ronaldo Dias", "authors": "Ronaldo Dias and Guilherme Ludwig and Paul Sampson", "title": "Scalable modeling of nonstationary covariance functions with non-folding\n  B-spline deformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for nonstationary covariance function modeling, based on\nthe spatial deformation method of Sampson and Guttorp [1992], but using a\nlow-rank, scalable deformation function written as a linear combination of the\ntensor product of B-spline basis. This approach addresses two important\nweaknesses in current computational aspects. First, it allows one to constrain\nestimated 2D deformations to be non-folding (bijective) in 2D. This requirement\nof the model has, up to now,been addressed only by arbitrary levels of spatial\nsmoothing. Second, basis functions with compact support enable the application\nto large datasets of spatial monitoring sites of environmental data. An\napplication to rainfall data in southeastern Brazil illustrates the method\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 20:42:33 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Dias", "Ronaldo", ""], ["Ludwig", "Guilherme", ""], ["Sampson", "Paul", ""]]}, {"id": "2007.00774", "submitter": "Raphael Huser", "authors": "Rapha\\\"el Huser and Jennifer L. Wadsworth", "title": "Advances in Statistical Modeling of Spatial Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical modeling of spatial extremes relies on asymptotic models (i.e.,\nmax-stable processes or $r$-Pareto processes) for block maxima or peaks over\nhigh thresholds, respectively. However, at finite levels, empirical evidence\noften suggests that such asymptotic models are too rigidly constrained, and\nthat they do not adequately capture the frequent situation where more severe\nevents tend to be spatially more localized. In other words, these asymptotic\nmodels have a strong tail dependence that persists at increasingly high levels,\nwhile data usually suggest that it should weaken instead. Another well-known\nlimitation of classical spatial extremes models is that they are either\ncomputationally prohibitive to fit in high dimensions, or they need to be\nfitted using less efficient techniques. In this review paper, we describe\nrecent progress in the modeling and inference for spatial extremes, focusing on\nnew models that have more flexible tail structures that can bridge asymptotic\ndependence classes, and that are more easily amenable to likelihood-based\ninference for large datasets. In particular, we discuss various types of random\nscale constructions, as well as the conditional spatial extremes model, which\nhave recently been getting increasing attention within the statistics of\nextremes community. We illustrate some of these new spatial models on two\ndifferent environmental applications.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 21:32:52 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 19:43:56 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 19:50:45 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Huser", "Rapha\u00ebl", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "2007.00797", "submitter": "Indrabati Bhattacharya", "authors": "Indrabati Bhattacharya and Subhashis Ghosal", "title": "Bayesian Multivariate Quantile Regression Using Dependent Dirichlet\n  Process Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider a non-parametric Bayesian approach to\nmultivariate quantile regression. The collection of related conditional\ndistributions of a response vector Y given a univariate covariate X is modeled\nusing a Dependent Dirichlet Process (DDP) prior. The DDP is used to introduce\ndependence across x. As the realizations from a Dirichlet process prior are\nalmost surely discrete, we need to convolve it with a kernel. To model the\nerror distribution as flexibly as possible, we use a countable mixture of\nmultidimensional normal distributions as our kernel. For posterior\ncomputations, we use a truncated stick-breaking representation of the DDP. This\napproximation enables us to deal with only a finitely number of parameters. We\nuse a Block Gibbs sampler for estimating the model parameters. We illustrate\nour method with simulation studies and real data applications. Finally, we\nprovide a theoretical justification for the proposed method through posterior\nconsistency. Our proposed procedure is new even when the response is\nunivariate.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 22:40:40 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Bhattacharya", "Indrabati", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "2007.00803", "submitter": "Tianxi Li", "authors": "Can M. Le and Tianxi Li", "title": "Linear regression and its inference on noisy network-linked data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression on a set of observations linked by a network has been an\nessential tool in modeling the relationship between response and covariates\nwith additional network data. Despite its wide range of applications in many\nareas, such as in social sciences and health-related research, the problem has\nnot been well-studied in statistics so far. Previous methods either lack\ninference tools or rely on restrictive assumptions on social effects and\nusually assume that networks are observed without errors, which is unrealistic\nin many problems. This paper proposes a linear regression model with\nnonparametric network effects. The model does not assume that the relational\ndata or network structure is exactly observed; thus, the method can be provably\nrobust to a certain network perturbation level. A set of asymptotic inference\nresults is established under a general requirement of the network observational\nerrors, and the robustness of this method is studied in the specific setting\nwhen the errors come from random network models. We discover a phase-transition\nphenomenon of the inference validity concerning the network density when no\nprior knowledge of the network model is available while also showing a\nsignificant improvement achieved by knowing the network model. As a by-product\nof this analysis, we derive a rate-optimal concentration bound for random\nsubspace projection that may be of independent interest. Extensive simulation\nstudies are conducted to verify these theoretical results and demonstrate the\nadvantage of the proposed method over existing work in terms of accuracy and\ncomputational efficiency under different data-generating models. The method is\nthen applied to adolescent network data to study the gender and racial\ndifference in social activities.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 23:01:22 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 15:54:32 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Le", "Can M.", ""], ["Li", "Tianxi", ""]]}, {"id": "2007.00830", "submitter": "Charles R Doss", "authors": "Fadoua Balabdaoui, Charles R. Doss, and C\\'ecile Durot", "title": "Unlinked monotone regression", "comments": "60 pages; 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider so-called univariate unlinked (sometimes ``decoupled,'' or\n``shuffled'') regression when the unknown regression curve is monotone. In\nstandard monotone regression, one observes a pair $(X,Y)$ where a response $Y$\nis linked to a covariate $X$ through the model $Y= m_0(X) + \\epsilon$, with\n$m_0$ the (unknown) monotone regression function and $\\epsilon$ the unobserved\nerror (assumed to be independent of $X$). In the unlinked regression setting\none gets only to observe a vector of realizations from both the response $Y$\nand from the covariate $X$ where now $Y \\stackrel{d}{=} m_0(X) + \\epsilon$.\nThere is no (observed) pairing of $X$ and $Y$. Despite this, it is actually\nstill possible to derive a consistent non-parametric estimator of $m_0$ under\nthe assumption of monotonicity of $m_0$ and knowledge of the distribution of\nthe noise $\\epsilon$. In this paper, we establish an upper bound on the rate of\nconvergence of such an estimator under minimal assumption on the distribution\nof the covariate $X$. We discuss extensions to the case in which the\ndistribution of the noise is unknown. We develop a second order algorithm for\nits computation, and we demonstrate its use on synthetic data. Finally, we\napply our method (in a fully data driven way, without knowledge of the error\ndistribution) on longitudinal data from the US Consumer Expenditure Survey.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 01:42:50 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 22:47:21 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Balabdaoui", "Fadoua", ""], ["Doss", "Charles R.", ""], ["Durot", "C\u00e9cile", ""]]}, {"id": "2007.00836", "submitter": "Rui Duan", "authors": "Rui Duan, Jin Piao, Arielle Marks-Anglin, Jiayi Tong, Lifeng Lin,\n  Haitao Chu, Jing Ning and Yong Chen", "title": "Testing for publication bias in meta-analysis under Copas selection\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In meta-analyses, publication bias is a well-known, important and challenging\nissue because the validity of the results from a meta-analysis is threatened if\nthe sample of studies retrieved for review is biased. One popular method to\ndeal with publication bias is the Copas selection model, which provides a\nflexible sensitivity analysis for correcting the estimates with considerable\ninsight into the data suppression mechanism. However, rigorous testing\nprocedures under the Copas selection model to detect bias are lacking. To fill\nthis gap, we develop a score-based test for detecting publication bias under\nthe Copas selection model. We reveal that the behavior of the standard score\ntest statistic is irregular because the parameters of the Copas selection model\ndisappear under the null hypothesis, leading to an identifiability problem. We\npropose a novel test statistic and derive its limiting distribution. A\nbootstrap procedure is provided to obtain the p-value of the test for practical\napplications. We conduct extensive Monte Carlo simulations to evaluate the\nperformance of the proposed test and apply the method to several existing\nmeta-analyses.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 02:03:22 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Duan", "Rui", ""], ["Piao", "Jin", ""], ["Marks-Anglin", "Arielle", ""], ["Tong", "Jiayi", ""], ["Lin", "Lifeng", ""], ["Chu", "Haitao", ""], ["Ning", "Jing", ""], ["Chen", "Yong", ""]]}, {"id": "2007.00974", "submitter": "Niklas Maltzahn", "authors": "N. Maltzahn, R. Hoff, O. O. Aalen, I. S. Mehlum, H. Putter and J. M.\n  Gran", "title": "A hybrid landmark Aalen-Johansen estimator for transition probabilities\n  in partially non-Markov multi-state models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-state models are increasingly being used to model complex\nepidemiological and clinical outcomes over time. It is common to assume that\nthe models are Markov, but the assumption can often be unrealistic. The Markov\nassumption is seldomly checked and violations can lead to biased estimation for\nmany parameters of interest. As argued by Datta and Satten (2001), the\nAalen-Johansen estimator of occupation probabilities is consistent also in the\nnon-Markov case. Putter and Spitoni (2018) exploit this fact to construct a\nconsistent estimator of state transition probabilities, the landmark\nAalen-Johansen estimator, which does not rely on the Markov assumption. A\ndisadvantage of landmarking is data reduction, leading to a loss of power. This\nis problematic for less traveled transitions, and undesirable when such\ntransitions indeed exhibit Markov behaviour. Using a framework of partially\nnon-Markov multi-state models we suggest a hybrid landmark Aalen-Johansen\nestimator for transition probabilities. The proposed estimator is a compromise\nbetween regular Aalen-Johansen and landmark estimation, using transition\nspecific landmarking, and can drastically improve statistical power. The\nmethods are compared in a simulation study and in a real data application\nmodelling individual transitions between states of sick leave, disability,\neducation, work and unemployment. In the application, a birth cohort of 184951\nNorwegian men are followed for 14 years from the year they turn 21, using data\nfrom national registries.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 09:18:19 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 08:58:13 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Maltzahn", "N.", ""], ["Hoff", "R.", ""], ["Aalen", "O. O.", ""], ["Mehlum", "I. S.", ""], ["Putter", "H.", ""], ["Gran", "J. M.", ""]]}, {"id": "2007.00996", "submitter": "Bruno Sudret", "authors": "X. Zhu and B. Sudret", "title": "Emulation of stochastic simulators using generalized lambda models", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2020-006B", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic simulators are ubiquitous in many fields of applied sciences and\nengineering. In the context of uncertainty quantification and optimization, a\nlarge number of simulations are usually necessary, which become intractable for\nhigh-fidelity models. Thus surrogate models of stochastic simulators have been\nintensively investigated in the last decade. In this paper, we present a novel\napproach to surrogate the response distribution of a stochastic simulator which\nuses generalized lambda distributions, whose parameters are represented by\npolynomial chaos expansions of the model inputs. As opposed to most existing\napproaches, this new method does not require replicated runs of the simulator\nat each point of the experimental design. We propose a new fitting procedure\nwhich combines maximum conditional likelihood estimation with (modified)\nfeasible generalized least-squares. We compare our method with state-of-the-art\nnonparametric kernel estimation on four different applications stemming from\nmathematical finance and epidemiology. Its performance is illustrated in terms\nof the accuracy of both the mean/variance of the stochastic simulator and the\nresponse distribution. As the proposed approach can also be used with\nexperimental designs containing replications, we carry out a comparison on two\nof the examples, that show that replications do not help to get a better\noverall accuracy, and may even worsen the results (at fixed total number of\nruns of the simulator).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 10:08:34 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 16:27:32 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Zhu", "X.", ""], ["Sudret", "B.", ""]]}, {"id": "2007.01010", "submitter": "Christoph Muehlmann", "authors": "Christoph Muehlmann, Hannu Oja, Klaus Nordhausen", "title": "Sliced Inverse Regression for Spatial Data", "comments": null, "journal-ref": "Festschrift in Honor of R. Dennis Cook. Springer, Cham. 2021", "doi": "10.1007/978-3-030-69009-0_5", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliced inverse regression is one of the most popular sufficient dimension\nreduction methods. Originally, it was designed for independent and identically\ndistributed data and recently extend to the case of serially and spatially\ndependent data. In this work we extend it to the case of spatially dependent\ndata where the response might depend also on neighbouring covariates when the\nobservations are taken on a grid-like structure as it is often the case in\neconometric spatial regression applications. We suggest guidelines on how to\ndecide upon the dimension of the subspace of interest and also which spatial\nlag might be of interest when modeling the response. These guidelines are\nsupported by a conducted simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 10:45:59 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Muehlmann", "Christoph", ""], ["Oja", "Hannu", ""], ["Nordhausen", "Klaus", ""]]}, {"id": "2007.01058", "submitter": "Zhenhua Lin", "authors": "Zhenhua Lin, Miles E. Lopes and Hans-Georg M\\\"uller", "title": "High-dimensional MANOVA via Bootstrapping and its Application to\n  Functional and Sparse Count Data", "comments": "80 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to the problem of high-dimensional multivariate\nANOVA via bootstrapping max statistics that involve the differences of sample\nmean vectors. The proposed method proceeds via the construction of simultaneous\nconfidence regions for the differences of population mean vectors. It is suited\nto simultaneously test the equality of several pairs of mean vectors of\npotentially more than two populations. By exploiting the variance decay\nproperty that is a natural feature in relevant applications, we are able to\nprovide dimension-free and nearly-parametric convergence rates for Gaussian\napproximation, bootstrap approximation, and the size of the test. We\ndemonstrate the proposed approach with ANOVA problems for functional data and\nsparse count data. The proposed methodology is shown to work well in\nsimulations and several real data applications.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 12:31:21 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 14:02:30 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2021 06:46:17 GMT"}, {"version": "v4", "created": "Mon, 19 Apr 2021 02:23:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lin", "Zhenhua", ""], ["Lopes", "Miles E.", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "2007.01157", "submitter": "Olga Vsevolozhskaya", "authors": "Olga A. Vsevolozhskaya, Karl C. Alcover, James C. Anthony, Dmitri V.\n  Zaykin", "title": "A new measure for the analysis of epidemiological associations: Cannabis\n  use disorder examples", "comments": "arXiv admin note: substantial text overlap with arXiv:1806.04251", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyses of population-based surveys are instrumental to research on\nprevention and treatment of mental and substance use disorders.\nPopulation-based data provides descriptive characteristics of multiple\ndeterminants of public health and are typically available to researchers as an\nannual data release. To provide trends in national estimates or to update the\nexisting ones, a meta-analytical approach to year-by-year data is typically\nemployed with ORs as effect sizes. However, if the estimated ORs exhibit\ndifferent patterns over time, some normalization of ORs may be warranted. We\npropose a new normalized measure of effect size and derive an asymptotic\ndistribution for the respective test statistic. The normalization constant is\nbased on the maximum range of the standardized log(OR), for which we establish\na connection to the Laplace Limit Constant. Furthermore, we propose to employ\nstandardized log(OR) in a novel way to obtain accurate posterior inference.\nThrough simulation studies, we show that our new statistic is more powerful\nthan the traditional one for testing the hypothesis OR=1. We then applied it to\nthe United States population estimates of co-occurrence of side effect\nproblem-experiences (SEPE) among newly incident cannabis users, based on the\nthe National Survey on Drug Use and Health (NSDUH), 2004-2014.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 15:38:29 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Vsevolozhskaya", "Olga A.", ""], ["Alcover", "Karl C.", ""], ["Anthony", "James C.", ""], ["Zaykin", "Dmitri V.", ""]]}, {"id": "2007.01237", "submitter": "Chenguang Dai", "authors": "Chenguang Dai, Buyu Lin, Xin Xing, Jun S. Liu", "title": "A Scale-free Approach for False Discovery Rate Control in Generalized\n  Linear Models", "comments": "60 pages, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized linear models (GLM) have been widely used in practice to\nmodel non-Gaussian response variables. When the number of explanatory features\nis relatively large, scientific researchers are of interest to perform\ncontrolled feature selection in order to simplify the downstream analysis. This\npaper introduces a new framework for feature selection in GLMs that can achieve\nfalse discovery rate (FDR) control in two asymptotic regimes. The key step is\nto construct a mirror statistic to measure the importance of each feature,\nwhich is based upon two (asymptotically) independent estimates of the\ncorresponding true coefficient obtained via either the data-splitting method or\nthe Gaussian mirror method. The FDR control is achieved by taking advantage of\nthe mirror statistic's property that, for any null feature, its sampling\ndistribution is (asymptotically) symmetric about 0. In the moderate-dimensional\nsetting in which the ratio between the dimension (number of features) p and the\nsample size n converges to a fixed value, we construct the mirror statistic\nbased on the maximum likelihood estimation. In the high-dimensional setting\nwhere p is much larger than n, we use the debiased Lasso to build the mirror\nstatistic. Compared to the Benjamini-Hochberg procedure, which crucially relies\non the asymptotic normality of the Z statistic, the proposed methodology is\nscale free as it only hinges on the symmetric property, thus is expected to be\nmore robust in finite-sample cases. Both simulation results and a real data\napplication show that the proposed methods are capable of controlling the FDR,\nand are often more powerful than existing methods including the\nBenjamini-Hochberg procedure and the knockoff filter.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 16:37:13 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Dai", "Chenguang", ""], ["Lin", "Buyu", ""], ["Xing", "Xin", ""], ["Liu", "Jun S.", ""]]}, {"id": "2007.01253", "submitter": "Shasha Han", "authors": "Shasha Han and Donald Rubin", "title": "Contrast Specific Propensity Scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basic propensity score methodology is designed to balance multivariate\npre-treatment covariates when comparing one active treatment with one control\ntreatment. Practical settings often involve comparing more than two treatments,\nwhere more complicated contrasts than the basic treatment-control one,(1,-1),\nare relevant. Here, we propose the use of contrast-specific propensity scores\n(CSPS). CSPS allow the creation of treatment groups of units that are balanced\nwith respect to bifurcations of the specified contrasts and the multivariate\nspace spanned by them.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:02:52 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Han", "Shasha", ""], ["Rubin", "Donald", ""]]}, {"id": "2007.01258", "submitter": "Tam\\'as Rudas", "authors": "Ori Davidov, Tamas Rudas", "title": "On the use of historical estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of historical estimates in current studies is common in a wide\nvariety of application areas. Nevertheless, despite their routine use the\nuncertainty associated with historical estimates is rarely properly accounted\nfor in the analysis. In this communication we review common practices and then\nprovide a mathematical formulation and a principled methodology for addressing\nthe problem of drawing inferences in the presence of historical data. Three\ndistinct variants are investigated in detail; the corresponding limiting\ndistributions are found and compared. The design of future studies, given\nhistorical data, is also explored and relations with a variety of other\nwell--studied statistical problems discussed.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:10:30 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Davidov", "Ori", ""], ["Rudas", "Tamas", ""]]}, {"id": "2007.01281", "submitter": "Art Owen", "authors": "Christopher Hoyt and Art B. Owen", "title": "Efficient estimation of the ANOVA mean dimension, with an application to\n  neural net classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean dimension of a black box function of $d$ variables is a convenient\nway to summarize the extent to which it is dominated by high or low order\ninteractions. It is expressed in terms of $2^d-1$ variance components but it\ncan be written as the sum of $d$ Sobol' indices that can be estimated by leave\none out methods. We compare the variance of these leave one out methods: a\nGibbs sampler called winding stairs, a radial sampler that changes each\nvariable one at a time from a baseline, and a naive sampler that never reuses\nfunction evaluations and so costs about double the other methods. For an\nadditive function the radial and winding stairs are most efficient. For a\nmultiplicative function the naive method can easily be most efficient if the\nfactors have high kurtosis. As an illustration we consider the mean dimension\nof a neural network classifier of digits from the MNIST data set. The\nclassifier is a function of $784$ pixels. For that problem, winding stairs is\nthe best algorithm. We find that inputs to the final softmax layer have mean\ndimensions ranging from $1.35$ to $2.0$.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:44:10 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 05:43:20 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 17:43:56 GMT"}, {"version": "v4", "created": "Wed, 30 Dec 2020 20:54:40 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Hoyt", "Christopher", ""], ["Owen", "Art B.", ""]]}, {"id": "2007.01283", "submitter": "Lu Zhang", "authors": "Lu Zhang and Lucas Janson", "title": "Floodgate: inference for model-free variable importance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern applications seek to understand the relationship between an\noutcome variable $Y$ and a covariate $X$ in the presence of a (possibly\nhigh-dimensional) confounding variable $Z$. Although much attention has been\npaid to testing whether $Y$ depends on $X$ given $Z$, in this paper we seek to\ngo beyond testing by inferring the strength of that dependence. We first define\nour estimand, the minimum mean squared error (mMSE) gap, which quantifies the\nconditional relationship between $Y$ and $X$ in a way that is deterministic,\nmodel-free, interpretable, and sensitive to nonlinearities and interactions. We\nthen propose a new inferential approach called floodgate that can leverage any\nworking regression function chosen by the user (allowing, e.g., it to be fitted\nby a state-of-the-art machine learning algorithm or be derived from qualitative\ndomain knowledge) to construct asymptotic confidence bounds, and we apply it to\nthe mMSE gap. In addition to proving floodgate's asymptotic validity, we\nrigorously quantify its accuracy (distance from confidence bound to estimand)\nand robustness. We demonstrate floodgate's performance in a series of\nsimulations and apply it to data from the UK Biobank to infer the strengths of\ndependence of platelet count on various groups of genetic mutations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:47:58 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 20:19:41 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 03:00:55 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Zhang", "Lu", ""], ["Janson", "Lucas", ""]]}, {"id": "2007.01360", "submitter": "Connor Dowd", "authors": "Connor Dowd", "title": "A New ECDF Two-Sample Test Statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical cumulative distribution functions (ECDFs) have been used to test\nthe hypothesis that two samples come from the same distribution since the\nseminal contribution by Kolmogorov and Smirnov. This paper describes a\nstatistic which is usable under the same conditions as Kolmogorov-Smirnov, but\nprovides more power than other extant tests in that vein. I demonstrate a valid\n(conservative) procedure for producing finite-sample p-values. I outline the\nclose relationship between this statistic and its two main predecessors. I also\nprovide a public R package (CRAN: twosamples [2018]) implementing the testing\nprocedure in $O(N\\log(N))$ time with $O(N)$ memory. Using the package's\nfunctions, I perform several simulation studies showing the power improvements.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 19:55:37 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Dowd", "Connor", ""]]}, {"id": "2007.01370", "submitter": "Thomas Webster", "authors": "Thomas F. Webster and Marc G. Weisskopf", "title": "Epidemiology of exposure to mixtures: we cant be casual about causality\n  when using or testing methods", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: There is increasing interest in approaches for analyzing the\neffect of exposure mixtures on health. A key issue is how to simultaneously\nanalyze often highly collinear components of the mixture, which can create\nproblems such as confounding by co-exposure and co-exposure amplification bias\n(CAB). Evaluation of novel mixtures methods, typically using synthetic data, is\ncritical to their ultimate utility. Objectives: This paper aims to answer two\nquestions. How do causal models inform the interpretation of statistical models\nand the creation of synthetic data used to test them? Are novel mixtures\nmethods susceptible to CAB? Methods: We use directed acyclic graphs (DAGs) and\nlinear models to derive closed form solutions for model parameters to examine\nhow underlying causal assumptions affect the interpretation of model results.\nResults: The same beta coefficients estimated by a statistical model can have\ndifferent interpretations depending on the assumed causal structure. Similarly,\nthe method used to simulate data can have implications for the underlying DAG\n(and vice versa), and therefore the identification of the parameter being\nestimated with an analytic approach. We demonstrate that methods that can\nreproduce results of linear regression, such as Bayesian kernel machine\nregression and the new quantile g-computation approach, will be subject to CAB.\nHowever, under some conditions, estimates of an overall effect of the mixture\nis not subject to CAB and even has reduced uncontrolled bias. Discussion: Just\nas DAGs encode a priori subject matter knowledge allowing identification of\nvariable control needed to block analytic bias, we recommend explicitly\nidentifying DAGs underlying synthetic data created to test statistical mixtures\napproaches. Estimates of the total effect of a mixture is an important but\nrelatively underexplored topic that warrants further investigation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 20:14:34 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Webster", "Thomas F.", ""], ["Weisskopf", "Marc G.", ""]]}, {"id": "2007.01390", "submitter": "Olli Saarela", "authors": "Olli Saarela, Christian Rohrbeck, Elja Arjas", "title": "Bayesian non-parametric ordinal regression under a monotonicity\n  constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to the nominal scale, ordinal scale for a categorical outcome\nvariable has the property of making a monotonicity assumption for the covariate\neffects meaningful. This assumption is encoded in the commonly used\nproportional odds model, but there it is combined with other parametric\nassumptions such as linearity and additivity. Herein, we consider models where\nmonotonicity is used as the only modeling assumption when modeling the effects\nof covariates on the cumulative probabilities of ordered outcome categories. We\nare not aware of other non-parametric multivariable monotonic ordinal models\nfor inference purposes. We generalize our previously proposed Bayesian\nmonotonic multivariable regression model to ordinal outcomes, and propose an\nestimation procedure based on reversible jump Markov chain Monte Carlo. The\nmodel is based on a marked point process construction, which allows it to\napproximate arbitrary monotonic regression function shapes, and has a built-in\ncovariate selection property. We study the performance of the proposed approach\nthrough extensive simulation studies, and demonstrate its practical application\nin two real data examples.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 21:22:06 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 15:53:10 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Saarela", "Olli", ""], ["Rohrbeck", "Christian", ""], ["Arjas", "Elja", ""]]}, {"id": "2007.01478", "submitter": "Yongyi Guo", "authors": "Jianqing Fan, Yongyi Guo, Ziwei Zhu", "title": "When is best subset selection the \"best\"?", "comments": "47 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Best subset selection (BSS) is fundamental in statistics and machine\nlearning. Despite the intensive studies of it, the fundamental question of when\nBSS is truly the \"best\", namely yielding the oracle estimator, remains\npartially answered. In this paper, we address this important issue by giving a\nweak sufficient condition and a strong necessary condition for BSS to exactly\nrecover the true model. We also give a weak sufficient condition for BSS to\nachieve the sure screening property. On the optimization aspect, we find that\nthe exact combinatorial minimizer for BSS is unnecessary: all the established\nstatistical properties for the best subset carry over to any sparse model whose\nresidual sum of squares is close enough to that of the best subset. In\nparticular, we show that an iterative hard thresholding (IHT) algorithm can\nfind a sparse subset with the sure screening property within logarithmic steps;\nanother round of BSS within this set can recover the true model. The simulation\nstudies and real data examples show that IHT yields lower false discovery rates\nand higher true positive rates than the competing approaches including LASSO,\nSCAD and SIS.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 03:32:02 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Fan", "Jianqing", ""], ["Guo", "Yongyi", ""], ["Zhu", "Ziwei", ""]]}, {"id": "2007.01484", "submitter": "Yanxun Xu", "authors": "Yuliang Li, Yang Ni, Leah H. Rubin, Amanda B. Spence, Yanxun Xu", "title": "BAGEL: A Bayesian Graphical Model for Inferring Drug Effect\n  Longitudinally on Depression in People with HIV", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access and adherence to antiretroviral therapy (ART) has transformed the face\nof HIV infection from a fatal to a chronic disease. However, ART is also known\nfor its side effects. Studies have reported that ART is associated with\ndepressive symptomatology. Large-scale HIV clinical databases with individuals'\nlongitudinal depression records, ART medications, and clinical characteristics\noffer researchers unprecedented opportunities to study the effects of ART drugs\non depression over time. We develop BAGEL, a Bayesian graphical model to\ninvestigate longitudinal effects of ART drugs on a range of depressive symptoms\nwhile adjusting for participants' demographic, behavior, and clinical\ncharacteristics, and taking into account the heterogeneous population through a\nBayesian nonparametric prior. We evaluate BAGEL through simulation studies.\nApplication to a dataset from the Women's Interagency HIV Study yields\ninterpretable and clinically useful results. BAGEL not only can improve our\nunderstanding of ART drugs effects on disparate depression symptoms, but also\nhas clinical utility in guiding informed and effective treatment selection to\nfacilitate precision medicine in HIV.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 03:44:58 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Li", "Yuliang", ""], ["Ni", "Yang", ""], ["Rubin", "Leah H.", ""], ["Spence", "Amanda B.", ""], ["Xu", "Yanxun", ""]]}, {"id": "2007.01485", "submitter": "Jacob Priddle", "authors": "Jacob W. Priddle, and Christopher Drovandi", "title": "Transformations in Semi-Parametric Bayesian Synthetic Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian synthetic likelihood (BSL) is a popular method for performing\napproximate Bayesian inference when the likelihood function is intractable. In\nsynthetic likelihood methods, the likelihood function is approximated\nparametrically via model simulations, and then standard likelihood-based\ntechniques are used to perform inference. The Gaussian synthetic likelihood\nestimator has become ubiquitous in BSL literature, primarily for its simplicity\nand ease of implementation. However, it is often too restrictive and may lead\nto poor posterior approximations. Recently, a more flexible semi-parametric\nBayesian synthetic likelihood (semiBSL) estimator has been introduced, which is\nsignificantly more robust to irregularly distributed summary statistics. In\nthis work, we propose a number of extensions to semiBSL. First, we consider\neven more flexible estimators of the marginal distributions using\ntransformation kernel density estimation. Second, we propose whitening semiBSL\n(wsemiBSL) -- a method to significantly improve the computational efficiency of\nsemiBSL. wsemiBSL uses an approximate whitening transformation to decorrelate\nsummary statistics at each algorithm iteration. The methods developed herein\nsignificantly improve the versatility and efficiency of BSL algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 03:58:53 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Priddle", "Jacob W.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "2007.01497", "submitter": "Hao Chen", "authors": "Jingru Zhang, Hao Chen, Xiao-Hua Zhou", "title": "New Non-parametric Tests for Multivariate Paired Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paired data are common in many fields, such as medical diagnosis and\nlongitudinal data analysis, where measurements are taken for the same set of\nobjects under different conditions. In many of these studies, the number of\nmeasurements could be large. Existing methods either impose strong assumptions\nor have power decrease fast as the number of measurements increases. In this\nwork, we propose new non-parametric tests for paired data. These tests exhibit\nsubstantial power improvements over existing methods under moderate- to high-\ndimensional data. We also derived asymptotic distributions of the new tests and\nthe approximate $p$-values based on them are reasonably accurate under finite\nsamples through simulation studies, making the new tests easy-off-the-shelf\ntools for real applications. The proposed tests are illustrated through the\nanalysis of a real data set on the Alzheimer's disease research.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 04:59:26 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Zhang", "Jingru", ""], ["Chen", "Hao", ""], ["Zhou", "Xiao-Hua", ""]]}, {"id": "2007.01539", "submitter": "Edgar Bueno", "authors": "Edgar Bueno and Dan Hedlin", "title": "A method to find an efficient and robust sampling strategy under model\n  uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of deciding on sampling strategy, in particular\nsampling design. We propose a risk measure, whose minimizing value guides the\nchoice. The method makes use of a superpopulation model and takes into account\nuncertainty about its parameters. The method is illustrated with a real\ndataset, yielding satisfactory results. As a baseline, we use the strategy that\ncouples probability proportional-to-size sampling with the difference\nestimator, as it is known to be optimal when the superpopulation model is fully\nknown. We show that, even under moderate misspecifications of the model, this\nstrategy is not robust and can be outperformed by some alternatives\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 08:01:23 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Bueno", "Edgar", ""], ["Hedlin", "Dan", ""]]}, {"id": "2007.01615", "submitter": "Debraj Das", "authors": "Debraj Das and Priyam Das", "title": "On Second order correctness of Bootstrap in Logistic Regression", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the fields of clinical trials, biomedical surveys, marketing, banking,\nwith dichotomous response variable, the logistic regression is considered as an\nalternative convenient approach to linear regression. In this paper, we develop\na novel bootstrap technique based on perturbation resampling method for\napproximating the distribution of the maximum likelihood estimator (MLE) of the\nregression parameter vector. We establish second order correctness of the\nproposed bootstrap method after proper studentization and smoothing. It is\nshown that inferences drawn based on the proposed bootstrap method are more\naccurate compared to that based on asymptotic normality. The main challenge in\nestablishing second order correctness remains in the fact that the response\nvariable being binary, the resulting MLE has a lattice structure. We show the\ndirect bootstrapping approach fails even after studentization. We adopt\nsmoothing technique developed in Lahiri (1993) to ensure that the smoothed\nstudentized version of the MLE has a density. Similar smoothing strategy is\nemployed to the bootstrap version also to achieve second order correct\napproximation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 11:18:20 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 14:58:05 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Das", "Debraj", ""], ["Das", "Priyam", ""]]}, {"id": "2007.01669", "submitter": "Yuya Yoshikawa", "authors": "Yuya Yoshikawa, Tomoharu Iwata", "title": "Gaussian Process Regression with Local Explanation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process regression (GPR) is a fundamental model used in machine\nlearning. Owing to its accurate prediction with uncertainty and versatility in\nhandling various data structures via kernels, GPR has been successfully used in\nvarious applications. However, in GPR, how the features of an input contribute\nto its prediction cannot be interpreted. Herein, we propose GPR with local\nexplanation, which reveals the feature contributions to the prediction of each\nsample, while maintaining the predictive performance of GPR. In the proposed\nmodel, both the prediction and explanation for each sample are performed using\nan easy-to-interpret locally linear model. The weight vector of the locally\nlinear model is assumed to be generated from multivariate Gaussian process\npriors. The hyperparameters of the proposed models are estimated by maximizing\nthe marginal likelihood. For a new test sample, the proposed model can predict\nthe values of its target variable and weight vector, as well as their\nuncertainties, in a closed form. Experimental results on various benchmark\ndatasets verify that the proposed model can achieve predictive performance\ncomparable to those of GPR and superior to that of existing interpretable\nmodels, and can achieve higher interpretability than them, both quantitatively\nand qualitatively.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 13:22:24 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 13:23:07 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 10:13:54 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Yoshikawa", "Yuya", ""], ["Iwata", "Tomoharu", ""]]}, {"id": "2007.01680", "submitter": "Svetlana Cherlin Cherlin", "authors": "Svetlana Cherlin and James M. S. Wason", "title": "Developing a predictive signature for two trial endpoints using the\n  cross-validated risk scores method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing cross-validated risk scores (CVRS) design has been proposed for\ndeveloping and testing the efficacy of a treatment in a high-efficacy patient\ngroup (the sensitive group) using high-dimensional data (such as genetic data).\nThe design is based on computing a risk score for each patient and dividing\nthem into clusters using a non-parametric clustering procedure. In some\nsettings it is desirable to consider the trade-off between two outcomes, such\nas efficacy and toxicity, or cost and effectiveness. With this motivation, we\nextend the CVRS design (CVRS2) to consider two outcomes. The design employs\nbivariate risk scores that are divided into clusters. We assess the properties\nof the CVRS2 using simulated data and illustrate its application on a\nrandomised psychiatry trial. We show that CVRS2 is able to reliably identify\nthe sensitive group (the group for which the new treatment provides benefit on\nboth outcomes) in the simulated data. We apply the CVRS2 design to a psychology\nclinical trial that had offender status and substance use status as two\noutcomes and collected a large number of baseline covariates. The CVRS2 design\nyields a significant treatment effect for both outcomes, while the CVRS\napproach identified a significant effect for the offender status only after\npre-filtering the covariates.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 13:39:17 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Cherlin", "Svetlana", ""], ["Wason", "James M. S.", ""]]}, {"id": "2007.01784", "submitter": "Lixia Hu", "authors": "Lixia Hu, Tao Huang and Jinhong You", "title": "Unified statistical inference for a novel nonlinear dynamic\n  functional/longitudinal data model", "comments": "29 pages; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In light of recent work studying massive functional/longitudinal data, such\nas the resulting data from the COVID-19 pandemic, we propose a novel\nfunctional/longitudinal data model which is a combination of the popular\nvarying coefficient (VC) model and additive model. We call it Semi-VCAM in\nwhich the response could be a functional/longitudinal variable, and the\nexplanatory variables could be a mixture of functional/longitudinal and scalar\nvariables. Notably some of the scalar variables could be categorical variables\nas well. The Semi-VCAM simultaneously allows for both substantial flexibility\nand the maintaining of one-dimensional rates of convergence. A local linear\nsmoothing with the aid of an initial B spline series approximation is developed\nto estimate the unknown functional effects in the model. To avoid the\nsubjective choice between the sparse and dense cases of the data, we establish\nthe asymptotic theories of the resultant Pilot Estimation Based Local Linear\nEstimators (PEBLLE) on a unified framework of sparse, dense and ultra-dense\ncases of the data. Moreover, we construct unified consistent tests to justify\nwhether a parsimony submodel is sufficient or not. These test methods also\navoid the subjective choice between the sparse, dense and ultra dense cases of\nthe data. Extensive Monte Carlo simulation studies investigating the finite\nsample performance of the proposed methodologies confirm our asymptotic\nresults. We further illustrate our methodologies via analyzing the COVID-19\ndata from China and the CD4 data.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:18:22 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Hu", "Lixia", ""], ["Huang", "Tao", ""], ["You", "Jinhong", ""]]}, {"id": "2007.01884", "submitter": "Andreas Gerhardus", "authors": "Andreas Gerhardus and Jakob Runge", "title": "High-recall causal discovery for autocorrelated time series with latent\n  confounders", "comments": "55 pages, 26 figures; added reference to related work plus\n  accompanying dicussion in section 3.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for linear and nonlinear, lagged and contemporaneous\nconstraint-based causal discovery from observational time series in the\npresence of latent confounders. We show that existing causal discovery methods\nsuch as FCI and variants suffer from low recall in the autocorrelated time\nseries case and identify low effect size of conditional independence tests as\nthe main reason. Information-theoretical arguments show that effect size can\noften be increased if causal parents are included in the conditioning sets. To\nidentify parents early on, we suggest an iterative procedure that utilizes\nnovel orientation rules to determine ancestral relationships already during the\nedge removal phase. We prove that the method is order-independent, and sound\nand complete in the oracle case. Extensive simulation studies for different\nnumbers of variables, time lags, sample sizes, and further cases demonstrate\nthat our method indeed achieves much higher recall than existing methods for\nthe case of autocorrelated continuous variables while keeping false positives\nat the desired level. This performance gain grows with stronger\nautocorrelation. At https://github.com/jakobrunge/tigramite we provide Python\ncode for all methods involved in the simulation studies.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 18:01:04 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 19:00:08 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 19:00:08 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Gerhardus", "Andreas", ""], ["Runge", "Jakob", ""]]}, {"id": "2007.01888", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul, Stergios B. Fotopoulos, Venkata K. Jandhyala, Abolfazl\n  Safikhani", "title": "Inference on the change point in high dimensional time series models via\n  plug in least squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a plug in least squares estimator for the change point parameter\nwhere change is in the mean of a high dimensional random vector under\nsubgaussian or subexponential distributions. We obtain sufficient conditions\nunder which this estimator possesses sufficient adaptivity against plug in\nestimates of mean parameters in order to yield an optimal rate of convergence\n$O_p(\\xi^{-2})$ in the integer scale. This rate is preserved while allowing\nhigh dimensionality as well as a potentially diminishing jump size $\\xi,$\nprovided $s\\log (p\\vee T)=o(\\surd(Tl_T))$ or $s\\log^{3/2}(p\\vee\nT)=o(\\surd(Tl_T))$ in the subgaussian and subexponential cases, respectively.\nHere $s,p,T$ and $l_T$ represent a sparsity parameter, model dimension,\nsampling period and the separation of the change point from its parametric\nboundary. Moreover, since the rate of convergence is free of $s,p$ and\nlogarithmic terms of $T,$ it allows the existence of limiting distributions.\nThese distributions are then derived as the {\\it argmax} of a two sided\nnegative drift Brownian motion or a two sided negative drift random walk under\nvanishing and non-vanishing jump size regimes, respectively. Thereby allowing\ninference of the change point parameter in the high dimensional setting.\nFeasible algorithms for implementation of the proposed methodology are\nprovided. Theoretical results are supported with monte-carlo simulations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 18:08:12 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 06:15:20 GMT"}, {"version": "v3", "created": "Sat, 11 Jul 2020 05:49:14 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Kaul", "Abhishek", ""], ["Fotopoulos", "Stergios B.", ""], ["Jandhyala", "Venkata K.", ""], ["Safikhani", "Abolfazl", ""]]}, {"id": "2007.02037", "submitter": "Yongxin Li", "authors": "Yongxin Li, Liujun Chen, Deyuan Li, Hansheng Wang", "title": "Estimating Extreme Value Index by Subsampling for Massive Datasets with\n  Heavy-Tailed Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern statistical analyses often encounter datasets with massive sizes and\nheavy-tailed distributions. For datasets with massive sizes, traditional\nestimation methods can hardly be used to estimate the extreme value index\ndirectly. To address the issue, we propose here a subsampling-based method.\nSpecifically, multiple subsamples are drawn from the whole dataset by using the\ntechnique of simple random subsampling with replacement. Based on each\nsubsample, an approximate maximum likelihood estimator can be computed. The\nresulting estimators are then averaged to form a more accurate one. Under\nappropriate regularity conditions, we show theoretically that the proposed\nestimator is consistent and asymptotically normal. With the help of the\nestimated extreme value index, a normal range can be established for a\nheavy-tailed random variable. Observations that fall outside the range should\nbe treated as suspected records and can be practically regarded as outliers.\nExtensive simulation experiments are provided to demonstrate the promising\nperformance of our method. A real data analysis is also presented for\nillustration purpose.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 08:05:16 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 11:52:08 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Li", "Yongxin", ""], ["Chen", "Liujun", ""], ["Li", "Deyuan", ""], ["Wang", "Hansheng", ""]]}, {"id": "2007.02105", "submitter": "Edsel Pena", "authors": "T. KIm and B. Lieberman and G. Luta and E. Pena", "title": "Prediction Regions for Poisson and Over-Dispersed Poisson Regression\n  Models with Applications to Forecasting Number of Deaths during the COVID-19\n  Pandemic", "comments": "There are 16 Figures with some containing one to four plot panels.\n  The appendix section are supplementary materials. Without these supplementary\n  materials, there are 35 pages in this manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the current Coronavirus Disease (COVID-19) pandemic, which is\ndue to the SARS-CoV-2 virus, and the important problem of forecasting daily\ndeaths and cumulative deaths, this paper examines the construction of\nprediction regions or intervals under the Poisson regression model and for an\nover-dispersed Poisson regression model. For the Poisson regression model,\nseveral prediction regions are developed and their performance are compared\nthrough simulation studies. The methods are applied to the problem of\nforecasting daily and cumulative deaths in the United States (US) due to\nCOVID-19. To examine their performance relative to what actually happened,\ndaily deaths data until May 15th were used to forecast cumulative deaths by\nJune 1st. It was observed that there is over-dispersion in the observed data\nrelative to the Poisson regression model. An over-dispersed Poisson regression\nmodel is therefore proposed. This new model builds on frailty ideas in Survival\nAnalysis and over-dispersion is quantified through an additional parameter. The\nPoisson regression model is a hidden model in this over-dispersed Poisson\nregression model and obtains as a limiting case when the over-dispersion\nparameter increases to infinity. A prediction region for the cumulative number\nof US deaths due to COVID-19 by July 16th, given the data until July 2nd, is\npresented. Finally, the paper discusses limitations of proposed procedures and\nmentions open research problems, as well as the dangers and pitfalls when\nforecasting on a long horizon, with focus on this pandemic where events, both\nforeseen and unforeseen, could have huge impacts on point predictions and\nprediction regions.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 14:20:59 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 01:45:23 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["KIm", "T.", ""], ["Lieberman", "B.", ""], ["Luta", "G.", ""], ["Pena", "E.", ""]]}, {"id": "2007.02117", "submitter": "Wessel Van Wieringen", "authors": "Wessel N. van Wieringen, Harald Binder", "title": "Transfer learning of regression models from a sequence of datasets by\n  penalized estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning refers to the promising idea of initializing model fits\nbased on pre-training on other data. We particularly consider regression\nmodeling settings where parameter estimates from previous data can be used as\nanchoring points, yet may not be available for all parameters, thus covariance\ninformation cannot be reused. A procedure that updates through targeted\npenalized estimation, which shrinks the estimator towards a nonzero value, is\npresented. The parameter estimate from the previous data serves as this nonzero\nvalue when an update is sought from novel data. This naturally extends to a\nsequence of data sets with the same response, but potentially only partial\noverlap in covariates. The iteratively updated regression parameter estimator\nis shown to be asymptotically unbiased and consistent. The penalty parameter is\nchosen through constrained cross-validated loglikelihood optimization. The\nconstraint bounds the amount of shrinkage of the updated estimator toward the\ncurrent one from below. The bound aims to preserve the (updated) estimator's\ngoodness-of-fit on all-but-the-novel data. The proposed approach is compared to\nother regression modeling procedures. Finally, it is illustrated on an\nepidemiological study where the data arrive in batches with different\ncovariate-availability and the model is re-fitted with the availability of a\nnovel batch.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 15:02:25 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["van Wieringen", "Wessel N.", ""], ["Binder", "Harald", ""]]}, {"id": "2007.02189", "submitter": "Tahani Coolen-Maturi Dr", "authors": "Tahani Coolen-Maturi, Frank P.A. Coolen, Narayanaswamy Balakrishnan", "title": "The joint survival signature of coherent systems with shared components", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of joint bivariate signature, introduced by Navarro et al.\n(2013), is a useful tool for quantifying the reliability of two systems with\nshared components. As with the univariate system signature, introduced by\nSamaniego (2007), its applications are limited to systems with only one type of\ncomponents, which restricts its practical use. Coolen and Coolen-Maturi (2012)\nintroduced the survival signature, which generalizes Samaniego's signature and\ncan be used for systems with multiple types of components. This paper\nintroduces a joint survival signature for multiple systems with multiple types\nof components and with some components shared between systems. A particularly\nimportant feature is that the functioning of these systems can be considered at\ndifferent times, enabling computation of relevant conditional probabilities\nwith regard to a system's functioning conditional on the status of another\nsystem with which it shares components. Several opportunities for practical\napplication and related challenges for further development of the presented\nconcept are briefly discussed, setting out an important direction for future\nresearch.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 21:23:18 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 18:57:21 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Coolen-Maturi", "Tahani", ""], ["Coolen", "Frank P. A.", ""], ["Balakrishnan", "Narayanaswamy", ""]]}, {"id": "2007.02192", "submitter": "Se Yoon Lee", "authors": "Se Yoon Lee, Debdeep Pati, Bani K. Mallick", "title": "Tail-adaptive Bayesian shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern genomic studies are increasingly focused on discovering more and more\ninteresting genes associated with a health response. Traditional shrinkage\npriors are primarily designed to detect a handful of signals from tens and\nthousands of predictors. Under diverse sparsity regimes, the nature of signal\ndetection is associated with a tail behaviour of a prior. A desirable tail\nbehaviour is called tail-adaptive shrinkage property where tail-heaviness of a\nprior gets adaptively larger (or smaller) as a sparsity level increases (or\ndecreases) to accommodate more (or less) signals. We propose a\nglobal-local-tail (GLT) Gaussian mixture distribution to ensure this property\nand provide accurate inference under diverse sparsity regimes. Incorporating a\npeaks-over-threshold method in extreme value theory, we develop an automated\ntail learning algorithm for the GLT prior. We compare the performance of the\nGLT prior to the Horseshoe in two gene expression datasets and numerical\nexamples. Results suggest that varying tail rule is advantageous over fixed\ntail rule under diverse sparsity domains.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 21:40:12 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 03:47:49 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Lee", "Se Yoon", ""], ["Pati", "Debdeep", ""], ["Mallick", "Bani K.", ""]]}, {"id": "2007.02228", "submitter": "Zhihua Ma", "authors": "Zhihua Ma, Guanyu Hu, Ming-Hui Chen", "title": "Bayesian Hierarchical Spatial Regression Models for Spatial Data in the\n  Presence of Missing Covariates with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, survey data are collected from different survey centers\nin different regions. It happens that in some circumstances, response variables\nare completely observed while the covariates have missing values. In this\npaper, we propose a joint spatial regression model for the response variable\nand missing covariates via a sequence of one-dimensional conditional spatial\nregression models. We further construct a joint spatial model for missing\ncovariate data mechanisms. The properties of the proposed models are examined\nand a Markov chain Monte Carlo sampling algorithm is used to sample from the\nposterior distribution. In addition, the Bayesian model comparison criteria,\nthe modified Deviance Information Criterion (mDIC) and the modified Logarithm\nof the Pseudo-Marginal Likelihood (mLPML), are developed to assess the fit of\nspatial regression models for spatial data. Extensive simulation studies are\ncarried out to examine empirical performance of the proposed methods. We\nfurther apply the proposed methodology to analyze a real data set from a\nChinese Health and Nutrition Survey (CHNS) conducted in 2011.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 02:50:52 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Ma", "Zhihua", ""], ["Hu", "Guanyu", ""], ["Chen", "Ming-Hui", ""]]}, {"id": "2007.02266", "submitter": "Shuoran Li", "authors": "Shuoran Li, Lili Zhao", "title": "Adverse event enrichment tests using VAERS", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vaccination safety is critical for individual and public health. Many\nexisting methods have been used to conduct safety studies with the VAERS\n(Vaccine Adverse Event Reporting System) database. However, these methods\nfrequently identify many adverse event (AE) signals and they are often hard to\ninterpret in a biological context. The AE ontology introduces biologically\nmeaningful structures to the VAERS database by connecting similar AEs, which\nprovides meaningful interpretation for the underlying safety issues. In this\npaper, we develop rigorous statistical methods to identify \"interesting\" AE\ngroups by performing AE enrichment analysis. We extend existing gene enrichment\ntests to perform AE enrichment analysis. Unlike the continuous gene expression\ndata, AE data are counts. Therefore, AE data has many zeros and ties. We\npropose two enrichment tests, AEFisher and AEKS. AEFisher is a modified\nFisher's exact test based on pre-selected significant AEs, while AEKS is based\non a modified Kolmogorov-Smirnov statistic. Both tests incorporate the special\nfeatures of the AE data. The proposed methods were evaluated using simulation\nstudies and were further illustrated on two studies using VAERS data. By\nappropriately addressing the issues of ties and excessive zeros in AE count\ndata, our enrichment tests performed well as demonstrated by simulation studies\nand analyses of VAERS data. The proposed methods were implemented in R package\nAEenrich and can be installed from the Comprehensive R Archive Network, CRAN.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 08:33:06 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Li", "Shuoran", ""], ["Zhao", "Lili", ""]]}, {"id": "2007.02305", "submitter": "Kattumannil Sudheesh Dr", "authors": "Sudheesh K Kattumannil, Sreedevi E P, Sankaran P G", "title": "Semiparametric transformation model for competing risks data with cure\n  fraction", "comments": "We propose a new methodology in mixture cure rate model", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling and analysis of competing risks data with long-term survivors is an\nimportant area of research in recent years. For example, in the study of cancer\npatients treated for soft tissue sarcoma, patient may die due to different\ncauses. Considerable portion of the patients may remain cancer free after the\ntreatment. Accordingly, it is important to incorporate long-term survivors in\nthe analysis of competing risks data. Motivated by this, we propose a new\nmethod for the analysis of competing risks data with long term survivors. The\nnew method enables us to estimate the overall survival probability without\nestimating the cure fraction. We formulate the effects of covariates on\nsub-distribution (cumulative incidence) functions using linear transformation\nmodel. Estimating equations based on counting process are developed to find the\nestimators of regression coefficients. The asymptotic properties of the\nestimators are studied using martingale theory. An extensive Monte Carlo\nsimulation study is carried out to assess the finite sample performance of the\nproposed estimators. Finally, we illustrate our method using a real data set.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 12:08:23 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Kattumannil", "Sudheesh K", ""], ["P", "Sreedevi E", ""], ["G", "Sankaran P", ""]]}, {"id": "2007.02339", "submitter": "Shu Yang", "authors": "Shu Yang, Yilong Zhang, Guanghan Frank Liu, and Qian Guan", "title": "SMIM: a unified framework of Survival sensitivity analysis using\n  Multiple Imputation and Martingale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Censored survival data are common in clinical trial studies. We propose a\nunified framework for sensitivity analysis to censoring at random in survival\ndata using multiple imputation and martingale, called SMIM. The proposed\nframework adopts the \\delta-adjusted and control-based models, indexed by the\nsensitivity parameter, entailing censoring at random and a wide collection of\ncensoring not at random assumptions. Also, it targets for a broad class of\ntreatment effect estimands defined as functionals of treatment-specific\nsurvival functions, taking into account of missing data due to censoring.\nMultiple imputation facilitates the use of simple full-sample estimation;\nhowever, the standard Rubin's combining rule may overestimate the variance for\ninference in the sensitivity analysis framework. We decompose the multiple\nimputation estimator into a martingale series based on the sequential\nconstruction of the estimator and propose the wild bootstrap inference by\nresampling the martingale series. The new bootstrap inference has a theoretical\nguarantee for consistency and is computationally efficient compared to the\nnon-parametric bootstrap counterpart. We evaluate the finite-sample performance\nof the proposed SMIM through simulation and an application on a HIV clinical\ntrial.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 13:47:06 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 09:54:17 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Yang", "Shu", ""], ["Zhang", "Yilong", ""], ["Liu", "Guanghan Frank", ""], ["Guan", "Qian", ""]]}, {"id": "2007.02404", "submitter": "Elynn Chen", "authors": "Elynn Y. Chen, Dong Xia, Chencheng Cai, Jianqing Fan", "title": "Semiparametric Tensor Factor Analysis by Iteratively Projected SVD", "comments": "23 pages, 6 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a general framework of Semiparametric TEnsor FActor\nanalysis (STEFA) that focuses on the methodology and theory of low-rank tensor\ndecomposition with auxiliary covariates. STEFA models extend tensor factor\nmodels by incorporating instrumental covariates in the loading matrices. We\npropose an algorithm of Iteratively Projected SVD (IP-SVD) for the\nsemiparametric estimations. It iteratively projects tensor data onto the linear\nspace spanned by covariates and applies SVD on matricized tensors over each\nmode. We establish the convergence rates of the loading matrices and the core\ntensor factor. Compared with the Tucker decomposition, IP-SVD yields more\naccurate estimates with a faster convergence rate. Besides estimation, we show\nseveral prediction methods with newly observed covariates based on the STEFA\nmodel. On both real and synthetic tensor data, we demonstrate the efficacy of\nthe STEFA model and the IP-SVD algorithm on both the estimation and prediction\ntasks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 18:20:10 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Chen", "Elynn Y.", ""], ["Xia", "Dong", ""], ["Cai", "Chencheng", ""], ["Fan", "Jianqing", ""]]}, {"id": "2007.02432", "submitter": "Jin Liu", "authors": "Jin Liu, Robert A. Perera", "title": "Extending Mixture of Experts Model to Investigate Heterogeneity of\n  Trajectories: When, Where and How to Add Which Covariates", "comments": "Draft version 1.7, 06/01/2021. This paper has not been peer reviewed.\n  Please do not copy or cite without author's permission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers are usually interested in examining the impact of covariates when\nseparating heterogeneous samples into latent classes that are more homogeneous.\nThe majority of theoretical and empirical studies with such aims have focused\non identifying covariates as predictors of class membership in the structural\nequation modeling framework. In other words, the covariates only indirectly\naffect the sample heterogeneity. However, the covariates' influence on\nbetween-individual differences can also be direct. This article presents a\nmixture model that investigates covariates to explain within-cluster and\nbetween-cluster heterogeneity simultaneously, known as a mixture-of-experts\n(MoE) model. This study aims to extend the MoE framework to investigate\nheterogeneity in nonlinear trajectories: to identify latent classes, covariates\nas predictors to clusters, and covariates that explain within-cluster\ndifferences in change patterns over time. Our simulation studies demonstrate\nthat the proposed model generally estimates the parameters unbiasedly,\nprecisely and exhibits appropriate empirical coverage for a nominal 95%\nconfidence interval. This study also proposes implementing structural equation\nmodel forests to shrink the covariate space of the proposed mixture model. We\nillustrate how to select covariates and construct the proposed model with\nlongitudinal mathematics achievement data. Additionally, we demonstrate that\nthe proposed mixture model can be further extended in the structural equation\nmodeling framework by allowing the covariates that have direct effects to be\ntime-varying.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 19:43:34 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 00:21:57 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 01:40:04 GMT"}, {"version": "v4", "created": "Sat, 19 Dec 2020 21:54:44 GMT"}, {"version": "v5", "created": "Mon, 28 Dec 2020 04:00:52 GMT"}, {"version": "v6", "created": "Mon, 26 Apr 2021 22:55:06 GMT"}, {"version": "v7", "created": "Wed, 2 Jun 2021 00:56:58 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liu", "Jin", ""], ["Perera", "Robert A.", ""]]}, {"id": "2007.02476", "submitter": "Richard Valliant", "authors": "Lingxiao Wang, Richard Valliant, and Yan Li", "title": "Adjusted Logistic Propensity Weighting Methods for Population Inference\n  using Nonprobability Volunteer-Based Epidemiologic Cohorts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many epidemiologic studies forgo probability sampling and turn to\nnonprobability volunteer-based samples because of cost, response burden, and\ninvasiveness of biological samples. However, finite population inference is\ndifficult to make from the nonprobability samples due to the lack of population\nrepresentativeness. Aiming for making inferences at the population level using\nnonprobability samples, various inverse propensity score weighting (IPSW)\nmethods have been studied with the propensity defined by the participation rate\nof population units in the nonprobability sample. In this paper, we propose an\nadjusted logistic propensity weighting (ALP) method to estimate the\nparticipation rates for nonprobability sample units. Compared to existing IPSW\nmethods, the proposed ALP method is easy to implement by ready-to-use software\nwhile producing approximately unbiased estimators for population quantities\nregardless of the nonprobability sample rate. The efficiency of the ALP\nestimator can be further improved by scaling the survey sample weights in\npropensity estimation. Taylor linearization variance estimators are proposed\nfor ALP estimators of finite population means that account for all sources of\nvariability. The proposed ALP methods are evaluated numerically via simulation\nstudies and empirically using the na\\\"ive unweighted National Health and\nNutrition Examination Survey III sample, while taking the 1997 National Health\nInterview Survey as the reference, to estimate the 15-year mortality rates.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 00:13:58 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 23:15:27 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 21:50:38 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Wang", "Lingxiao", ""], ["Valliant", "Richard", ""], ["Li", "Yan", ""]]}, {"id": "2007.02514", "submitter": "Aaron Fisher", "authors": "Aaron Fisher", "title": "Treatment Effect Bias from Sample Snooping: Blinding Outcomes is Neither\n  Necessary nor Sufficient", "comments": "version notes: dramatic rewrite with new theoretical results and\n  simulations. Most of the technical results from first version are now\n  contained in the supplement, with new results taking their place in the main\n  text. The supplement is available as an ancillary file (see link on\n  right-hand side of this page)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular guidance on observational data analysis states that outcomes should\nbe blinded when determining matching criteria or propensity scores. Such a\nblinding is informally said to maintain the \"objectivity\" of the analysis, and\nto prevent analysts from artificially amplifying the treatment effect by\nexploiting chance imbalances. Contrary to this notion, we show that outcome\nblinding is not a sufficient safeguard against fishing. Blinded and unblinded\nanalysts can produce bias of the same order of magnitude in cases where the\noutcomes can be approximately predicted from baseline covariates. We illustrate\nthis vulnerability with a combination of analytical results and simulations.\nFinally, to show that outcome blinding is not necessary to prevent bias, we\noutline an alternative sample partitioning procedure for estimating the average\ntreatment effect on the controls, or the average treatment effect on the\ntreated. This procedure uses all of the the outcome data from all partitions in\nthe final analysis step, but does not require the analysis to not be fully\nprespecified.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 03:47:49 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 20:57:07 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 23:05:57 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Fisher", "Aaron", ""]]}, {"id": "2007.02596", "submitter": "Bruno Ebner", "authors": "Bruno Ebner and Norbert Henze and David Strieder", "title": "Testing normality in any dimension by Fourier methods in a multivariate\n  Stein equation", "comments": "37 pages, 1 figure, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel class of affine invariant and consistent tests for\nmultivariate normality. The tests are based on a characterization of the\nstandard $d$-variate normal distribution by means of the unique solution of an\ninitial value problem connected to a partial differential equation, which is\nmotivated by a multivariate Stein equation. The test criterion is a suitably\nweighted $L^2$-statistic. We derive the limit distribution of the test\nstatistic under the null hypothesis as well as under contiguous and fixed\nalternatives to normality. A consistent estimator of the limiting variance\nunder fixed alternatives as well as an asymptotic confidence interval of the\ndistance of an underlying alternative with respect to the multivariate normal\nlaw is derived. In simulation studies, we show that the tests are strong in\ncomparison with prominent competitors, and that the empirical coverage rate of\nthe asymptotic confidence interval converges to the nominal level. We present a\nreal data example, and we outline topics for further research.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 09:15:59 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Ebner", "Bruno", ""], ["Henze", "Norbert", ""], ["Strieder", "David", ""]]}, {"id": "2007.02633", "submitter": "Xinwei Shen", "authors": "Xinwei Shen, Kani Chen, Wen Yu", "title": "Surprise sampling: improving and extending the local case-control\n  sampling", "comments": null, "journal-ref": "Electron. J. Statist. 15(1): 2454-2482, 2021", "doi": "10.1214/21-EJS1844", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fithian and Hastie (2014) proposed a new sampling scheme called local\ncase-control (LCC) sampling that achieves stability and efficiency by utilizing\na clever adjustment pertained to the logistic model. It is particularly useful\nfor classification with large and imbalanced data. This paper proposes a more\ngeneral sampling scheme based on a working principle that data points deserve\nhigher sampling probability if they contain more information or appear\n\"surprising\" in the sense of, for example, a large error of pilot prediction or\na large absolute score. Compared with the relevant existing sampling schemes,\nas reported in Fithian and Hastie (2014) and Ai, et al. (2018), the proposed\none has several advantages. It adaptively gives out the optimal forms to a\nvariety of objectives, including the LCC and Ai et al. (2018)'s sampling as\nspecial cases. Under same model specifications, the proposed estimator also\nperforms no worse than those in the literature. The estimation procedure is\nvalid even if the model is misspecified and/or the pilot estimator is\ninconsistent or dependent on full data. We present theoretical justifications\nof the claimed advantages and optimality of the estimation and the sampling\ndesign. Different from Ai, et al. (2018), our large sample theory are\npopulation-wise rather than data-wise. Moreover, the proposed approach can be\napplied to unsupervised learning studies, since it essentially only requires a\nspecific loss function and no response-covariate structure of data is needed.\nNumerical studies are carried out and the evidence in support of the theory is\nshown.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 10:46:10 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Shen", "Xinwei", ""], ["Chen", "Kani", ""], ["Yu", "Wen", ""]]}, {"id": "2007.02714", "submitter": "Brian Reich", "authors": "Brian J Reich, Shu Yang, Yawen Guan, Andrew B Giffin, Matthew J Miller\n  and Ana G Rappold", "title": "A review of spatial causal inference methods for environmental and\n  epidemiological applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific rigor and computational methods of causal inference have had\ngreat impacts on many disciplines, but have only recently begun to take hold in\nspatial applications. Spatial casual inference poses analytic challenges due to\ncomplex correlation structures and interference between the treatment at one\nlocation and the outcomes at others. In this paper, we review the current\nliterature on spatial causal inference and identify areas of future work. We\nfirst discuss methods that exploit spatial structure to account for unmeasured\nconfounding variables. We then discuss causal analysis in the presence of\nspatial interference including several common assumptions used to reduce the\ncomplexity of the interference patterns under consideration. These methods are\nextended to the spatiotemporal case where we compare and contrast the potential\noutcomes framework with Granger causality, and to geostatistical analyses\ninvolving spatial random fields of treatments and responses. The methods are\nintroduced in the context of observational environmental and epidemiological\nstudies, and are compared using both a simulation study and analysis of the\neffect of ambient air pollution on COVID-19 mortality rate. Code to implement\nmany of the methods using the popular Bayesian software OpenBUGS is provided.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:03:31 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Reich", "Brian J", ""], ["Yang", "Shu", ""], ["Guan", "Yawen", ""], ["Giffin", "Andrew B", ""], ["Miller", "Matthew J", ""], ["Rappold", "Ana G", ""]]}, {"id": "2007.02725", "submitter": "Michael Chappell", "authors": "Michael A. Chappell and Mark W. Woolrich", "title": "The FMRIB Variational Bayesian Inference Tutorial II: Stochastic\n  Variational Bayes", "comments": "Example code and exercises associated with this tutorial can be found\n  here: https://vb-tutorial.readthedocs.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian methods have proved powerful in many applications for the inference\nof model parameters from data. These methods are based on Bayes' theorem, which\nitself is deceptively simple. However, in practice the computations required\nare intractable even for simple cases. Hence methods for Bayesian inference\nhave historically either been significantly approximate, e.g., the Laplace\napproximation, or achieve samples from the exact solution at significant\ncomputational expense, e.g., Markov Chain Monte Carlo methods. Since around the\nyear 2000 so-called Variational approaches to Bayesian inference have been\nincreasingly deployed. In its most general form Variational Bayes (VB) involves\napproximating the true posterior probability distribution via another more\n'manageable' distribution, the aim being to achieve as good an approximation as\npossible. In the original FMRIB Variational Bayes tutorial we documented an\napproach to VB based that took a 'mean field' approach to forming the\napproximate posterior, required the conjugacy of prior and likelihood, and\nexploited the Calculus of Variations, to derive an iterative series of update\nequations, akin to Expectation Maximisation. In this tutorial we revisit VB,\nbut now take a stochastic approach to the problem that potentially circumvents\nsome of the limitations imposed by the earlier methodology. This new approach\nbears a lot of similarity to, and has benefited from, computational methods\napplied to machine learning algorithms. Although, what we document here is\nstill recognisably Bayesian inference in the classic sense, and not an attempt\nto use machine learning as a black-box to solve the inference problem.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 11:31:52 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 10:30:42 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Chappell", "Michael A.", ""], ["Woolrich", "Mark W.", ""]]}, {"id": "2007.02739", "submitter": "Georges Sfeir", "authors": "Georges Sfeir, Maya Abou-Zeid, Filipe Rodrigues, Francisco Camara\n  Pereira, Isam Kaysi", "title": "Semi-nonparametric Latent Class Choice Model with a Flexible Class\n  Membership Component: A Mixture Model Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a semi-nonparametric Latent Class Choice Model (LCCM)\nwith a flexible class membership component. The proposed model formulates the\nlatent classes using mixture models as an alternative approach to the\ntraditional random utility specification with the aim of comparing the two\napproaches on various measures including prediction accuracy and representation\nof heterogeneity in the choice process. Mixture models are parametric\nmodel-based clustering techniques that have been widely used in areas such as\nmachine learning, data mining and patter recognition for clustering and\nclassification problems. An Expectation-Maximization (EM) algorithm is derived\nfor the estimation of the proposed model. Using two different case studies on\ntravel mode choice behavior, the proposed model is compared to traditional\ndiscrete choice models on the basis of parameter estimates' signs, value of\ntime, statistical goodness-of-fit measures, and cross-validation tests. Results\nshow that mixture models improve the overall performance of latent class choice\nmodels by providing better out-of-sample prediction accuracy in addition to\nbetter representations of heterogeneity without weakening the behavioral and\neconomic interpretability of the choice models.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:19:26 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Sfeir", "Georges", ""], ["Abou-Zeid", "Maya", ""], ["Rodrigues", "Filipe", ""], ["Pereira", "Francisco Camara", ""], ["Kaysi", "Isam", ""]]}, {"id": "2007.02751", "submitter": "Klaus Nordhausen", "authors": "Una Radojicic and Klaus Nordhausen", "title": "Non-Gaussian component analysis: testing the dimension of the signal\n  subspace", "comments": null, "journal-ref": "In Maciak, M., Pestas, M. and Schindler, M. (editors) \"Analytical\n  Methods in Statistics. AMISTAT 2019\", 101-123, Springer, Cham", "doi": "10.1007/978-3-030-48814-7_6", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction is a common strategy in multivariate data analysis which\nseeks a subspace which contains all interesting features needed for the\nsubsequent analysis. Non-Gaussian component analysis attempts for this purpose\nto divide the data into a non-Gaussian part, the signal, and a Gaussian part,\nthe noise. We will show that the simultaneous use of two scatter functionals\ncan be used for this purpose and suggest a bootstrap test to test the dimension\nof the non-Gaussian subspace. Sequential application of the test can then for\nexample be used to estimate the signal dimension.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:44:17 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Radojicic", "Una", ""], ["Nordhausen", "Klaus", ""]]}, {"id": "2007.02763", "submitter": "Tom\\'a\\v{s} Rub\\'in", "authors": "Tom\\'a\\v{s} Rub\\'in", "title": "Yield curve and macroeconomy interaction: evidence from the\n  non-parametric functional lagged regression approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viewing a yield curve as a sparse collection of measurements on a latent\ncontinuous random function allows us to model it statistically as a sparsely\nobserved functional time series. Doing so, we use the state-of-the-art methods\nin non-parametric statistical inference for sparsely observed functional time\nseries to analyse the lagged regression dependence of the US Treasury yield\ncurve on US macroeconomic variables. Our non-parametric analysis confirms\nprevious findings established under parametric assumptions, namely a strong\nimpact of the federal funds rate on the short end of the yield curve and a\nmoderate effect of the annual inflation on the longer end of the yield curve.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 14:07:38 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Rub\u00edn", "Tom\u00e1\u0161", ""]]}, {"id": "2007.02844", "submitter": "Vera Djordjilovi\\'c", "authors": "Vera Djordjilovi\\'c and Jesse Hemerik and Magne Thoresen", "title": "On optimal two-stage testing of multiple mediators", "comments": "20 pages, 5 gifures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis in high-dimensional settings often involves identifying\npotential mediators among a large number of measured variables. For this\npurpose, a two-step familywise error rate procedure called ScreenMin has been\nrecently proposed (Djordjilovi\\'c et al. 2019). In ScreenMin, variables are\nfirst screened and only those that pass the screening are tested. The proposed\nthreshold for selection has been shown to guarantee asymptotic familywise error\nrate. In this work, we investigate the impact of the selection threshold on the\nfinite sample familywise error rate. We derive a power maximizing selection\nthreshold and show that it is well approximated by an adaptive threshold of\nWang et al. (2016). We illustrate the investigated procedures on a case-control\nstudy examining the effect of fish intake on the risk of colorectal adenoma.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:53:31 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Djordjilovi\u0107", "Vera", ""], ["Hemerik", "Jesse", ""], ["Thoresen", "Magne", ""]]}, {"id": "2007.02852", "submitter": "Daniel Jacob", "authors": "Daniel Jacob", "title": "Cross-Fitting and Averaging for Machine Learning Estimation of\n  Heterogeneous Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the finite sample performance of sample splitting,\ncross-fitting and averaging for the estimation of the conditional average\ntreatment effect. Recently proposed methods, so-called meta-learners, make use\nof machine learning to estimate different nuisance functions and hence allow\nfor fewer restrictions on the underlying structure of the data. To limit a\npotential overfitting bias that may result when using machine learning methods,\ncross-fitting estimators have been proposed. This includes the splitting of the\ndata in different folds to reduce bias and averaging over folds to restore\nefficiency. To the best of our knowledge, it is not yet clear how exactly the\ndata should be split and averaged. We employ a Monte Carlo study with different\ndata generation processes and consider twelve different estimators that vary in\nsample-splitting, cross-fitting and averaging procedures. We investigate the\nperformance of each estimator independently on four different meta-learners:\nthe doubly-robust-learner, R-learner, T-learner and X-learner. We find that the\nperformance of all meta-learners heavily depends on the procedure of splitting\nand averaging. The best performance in terms of mean squared error (MSE) among\nthe sample split estimators can be achieved when applying cross-fitting plus\ntaking the median over multiple different sample-splitting iterations. Some\nmeta-learners exhibit a high variance when the lasso is included in the ML\nmethods. Excluding the lasso decreases the variance and leads to robust and at\nleast competitive results.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 16:09:00 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 17:34:29 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Jacob", "Daniel", ""]]}, {"id": "2007.02857", "submitter": "Jack Gorham", "authors": "Jackson Gorham, Anant Raj, Lester Mackey", "title": "Stochastic Stein Discrepancies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein discrepancies (SDs) monitor convergence and non-convergence in\napproximate inference when exact integration and sampling are intractable.\nHowever, the computation of a Stein discrepancy can be prohibitive if the Stein\noperator - often a sum over likelihood terms or potentials - is expensive to\nevaluate. To address this deficiency, we show that stochastic Stein\ndiscrepancies (SSDs) based on subsampled approximations of the Stein operator\ninherit the convergence control properties of standard SDs with probability 1.\nAlong the way, we establish the convergence of Stein variational gradient\ndescent (SVGD) on unbounded domains, resolving an open question of Liu (2017).\nIn our experiments with biased Markov chain Monte Carlo (MCMC) hyperparameter\ntuning, approximate MCMC sampler selection, and stochastic SVGD, SSDs deliver\ncomparable inferences to standard SDs with orders of magnitude fewer likelihood\nevaluations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 16:15:33 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 18:48:24 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 01:49:55 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2020 18:56:24 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Gorham", "Jackson", ""], ["Raj", "Anant", ""], ["Mackey", "Lester", ""]]}, {"id": "2007.02861", "submitter": "Luka Petrovi\\'c", "authors": "Luka V. Petrovi\\'c and Ingo Scholtes", "title": "Learning the Markov order of paths in a network", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning the Markov order in categorical sequences\nthat represent paths in a network, i.e. sequences of variable lengths where\ntransitions between states are constrained to a known graph. Such data pose\nchallenges for standard Markov order detection methods and demand modelling\ntechniques that explicitly account for the graph constraint. Adopting a\nmulti-order modelling framework for paths, we develop a Bayesian learning\ntechnique that (i) more reliably detects the correct Markov order compared to a\ncompeting method based on the likelihood ratio test, (ii) requires considerably\nless data compared to methods using AIC or BIC, and (iii) is robust against\npartial knowledge of the underlying constraints. We further show that a\nrecently published method that uses a likelihood ratio test has a tendency to\noverfit the true Markov order of paths, which is not the case for our Bayesian\ntechnique. Our method is important for data scientists analyzing patterns in\ncategorical sequence data that are subject to (partially) known constraints,\ne.g. sequences with forbidden words, mobility trajectories and click stream\ndata, or sequence data in bioinformatics. Addressing the key challenge of model\nselection, our work is further relevant for the growing body of research that\nemphasizes the need for higher-order models in network analysis.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 16:27:02 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Petrovi\u0107", "Luka V.", ""], ["Scholtes", "Ingo", ""]]}, {"id": "2007.03016", "submitter": "Yajuan Si", "authors": "Yajuan Si, Steve Heeringa, David Johnson, Roderick Little, Wenshuo\n  Liu, Fabian Pfeffer and Trivellore Raghunathan", "title": "Multiple Imputation with Massive Data: An Application to the Panel Study\n  of Income Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation (MI) is a popular and well-established method for\nhandling missing data in multivariate data sets, but its practicality for use\nin massive and complex data sets has been questioned. One such data set is the\nPanel Study of Income Dynamics (PSID), a longstanding and extensive survey of\nhousehold income and wealth in the United States. Missing data for this survey\nare currently handled using traditional hot deck methods. We use a sequential\nregression/ chained-equation approach, using the software IVEware, to multiply\nimpute cross-sectional wealth data in the 2013 PSID, and compare analyses of\nthe resulting imputed data with results from the current hot deck approach.\nPractical difficulties, such as non-normally distributed variables, skip\npatterns, categorical variables with many levels, and multicollinearity, are\ndescribed together with our approaches to overcoming them. We evaluate the\nimputation quality and validity with internal diagnostics and external\nbenchmarking data. MI produces improvements over the existing hot deck approach\nby helping preserve correlation structures with efficiency gains. We recommend\nthe practical implementation of MI and expect greater gains when the fraction\nof missing information is large.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:05:17 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 23:14:43 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 02:35:36 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Si", "Yajuan", ""], ["Heeringa", "Steve", ""], ["Johnson", "David", ""], ["Little", "Roderick", ""], ["Liu", "Wenshuo", ""], ["Pfeffer", "Fabian", ""], ["Raghunathan", "Trivellore", ""]]}, {"id": "2007.03069", "submitter": "Kirk Bansak", "authors": "Kirk Bansak", "title": "A Minimum-Risk Dynamic Assignment Mechanism Along with Approximations,\n  Extensions, and Application to Refugee Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classic linear assignment problem, items must be assigned to agents in\na manner that minimizes the sum of the costs for each item-agent assignment,\nwhere the costs of all possible item-agent pairings are observed in advance.\nThis is a well-known and well-characterized problem, and algorithms exist to\nattain the solution. In contrast, less attention has been given to the dynamic\nversion of this problem where each item must be assigned to an agent\nsequentially upon arrival without knowledge of the future items to arrive.\nMotivated by an application in the globally pressing domain of international\nasylum and refugee resettlement, specifically that of matching refugees and\nasylum seekers to geographic localities within a host country, this study\nproposes an assignment mechanism that combines linear assignment programming\nsolutions with stochastic programming methods to minimize the expected loss\nwhen assignments must be made in this dynamic sequential fashion, and offers an\nalgorithm for implementing the mechanism. The study also presents an\napproximate version of the mechanism and accompanying algorithm that is more\ncomputationally efficient, a prediction-based alternative approximation that\ncombines stochastic programming with machine learning to achieve even greater\nefficiency, as well as heuristic mechanisms. In addition, the study provides an\nextension to dynamic batch assignment, where items arrive and must be assigned\nsequentially in groups. Real-world refugee resettlement data in the United\nStates are used to illustrate the methods and show how they could be\npractically implemented in order to optimize refugees' and asylum seekers'\nemployment prospects (or other integration outcomes) in their host countries.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 21:28:15 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 00:01:14 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Bansak", "Kirk", ""]]}, {"id": "2007.03238", "submitter": "Alexander Fisch", "authors": "Alexander T. M. Fisch, Idris A. Eckley, P. Fearnhead", "title": "Innovative And Additive Outlier Robust Kalman Filtering With A Robust\n  Particle Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose CE-BASS, a particle mixture Kalman filter which is\nrobust to both innovative and additive outliers, and able to fully capture\nmulti-modality in the distribution of the hidden state. Furthermore, the\nparticle sampling approach re-samples past states, which enables CE-BASS to\nhandle innovative outliers which are not immediately visible in the\nobservations, such as trend changes. The filter is computationally efficient as\nwe derive new, accurate approximations to the optimal proposal distributions\nfor the particles. The proposed algorithm is shown to compare well with\nexisting approaches and is applied to both machine temperature and server data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:11:09 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Fisch", "Alexander T. M.", ""], ["Eckley", "Idris A.", ""], ["Fearnhead", "P.", ""]]}, {"id": "2007.03288", "submitter": "Tat-Thang Vo", "authors": "Tat-Thang Vo, Hilary Davies-Kershaw, Ruth Hackett, Stijn Vansteelandt", "title": "Longitudinal mediation analysis of time-to-event endpoints in the\n  presence of competing risks", "comments": "Currently under peer-review at Statistics in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This proposal is motivated by an analysis of the English Longitudinal Study\nof Ageing (ELSA), which aims to investigate the role of loneliness in\nexplaining the negative impact of hearing loss on dementia. The methodological\nchallenges that complicate this mediation analysis include the use of a\ntime-to-event endpoint subject to competing risks, as well as the presence of\nfeedback relationships between the mediator and confounders that are both\nrepeatedly measured over time. To account for these challenges, we introduce\nnatural effect proportional (cause-specific) hazard models. These extend\nmarginal structural proportional (cause-specific) hazard models to enable\neffect decomposition. We show that under certain causal assumptions, the\npath-specific direct and indirect effects indexing this model are identifiable\nfrom the observed data. We next propose an inverse probability weighting\napproach to estimate these effects. On the ELSA data, this approach reveals\nlittle evidence that the total efect of hearing loss on dementia is mediated\nthrough the feeling of loneliness, with a non-statistically significant\nindirect effect equal to 1.012 (hazard ratio (HR) scale; 95% confidence\ninterval (CI) 0.986 to 1.053).\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:03:10 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Vo", "Tat-Thang", ""], ["Davies-Kershaw", "Hilary", ""], ["Hackett", "Ruth", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2007.03297", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Yang Yang", "title": "Forecasting Australian subnational age-specific mortality rates", "comments": "27 pages, 9 figures", "journal-ref": "Journal of Population Research (2020)", "doi": "10.1007/s12546-020-09250-0", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling sub-national mortality rates, it is important to incorporate\nany possible correlation among sub-populations to improve forecast accuracy.\nMoreover, forecasts at the sub-national level should aggregate consistently\nacross the forecasts at the national level. In this study, we apply a grouped\nmultivariate functional time series to forecast Australian regional and remote\nage-specific mortality rates and reconcile forecasts in a group structure using\nvarious methods. Our proposed method compares favorably to a grouped univariate\nfunctional time series forecasting method by comparing one-step-ahead to\nfive-step-ahead point forecast accuracy. Thus, we demonstrate that joint\nmodeling of sub-populations with similar mortality patterns can improve point\nforecast accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:28:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Shang", "Han Lin", ""], ["Yang", "Yang", ""]]}, {"id": "2007.03303", "submitter": "Matteo Fasiolo", "authors": "Matteo Fasiolo, Simon N. Wood, Margaux Zaffran, Rapha\\\"el Nedellec,\n  Yannig Goude", "title": "qgam: Bayesian non-parametric quantile regression modelling in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized additive models (GAMs) are flexible non-linear regression models,\nwhich can be fitted efficiently using the approximate Bayesian methods provided\nby the mgcv R package. While the GAM methods provided by mgcv are based on the\nassumption that the response distribution is modelled parametrically, here we\ndiscuss more flexible methods that do not entail any parametric assumption. In\nparticular, this article introduces the qgam package, which is an extension of\nmgcv providing fast calibrated Bayesian methods for fitting quantile GAMs\n(QGAMs) in R. QGAMs are based on a smooth version of the pinball loss of\nKoenker (2005), rather than on a likelihood function, hence jointly achieving\nsatisfactory accuracy of the quantile point estimates and coverage of the\ncorresponding credible intervals requires adopting the specialized Bayesian\nfitting framework of Fasiolo, Wood, Zaffran, Nedellec, and Goude (2020b). Here\nwe detail how this framework is implemented in qgam and we provide examples\nillustrating how the package should be used in practice.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:32:11 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Fasiolo", "Matteo", ""], ["Wood", "Simon N.", ""], ["Zaffran", "Margaux", ""], ["Nedellec", "Rapha\u00ebl", ""], ["Goude", "Yannig", ""]]}, {"id": "2007.03308", "submitter": "Tahani Coolen-Maturi Dr", "authors": "Tahani Coolen-Maturi", "title": "The ordering of future observations from multiple groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many situations where comparison of different groups is of great\ninterest. Considering the ordering of the efficiency of some treatments is an\nexample. We present nonparametric predictive inference (NPI) for the ordering\nof real-valued future observations from multiple independent groups. The\nuncertainty is quantified using NPI lower and upper probabilities for the event\nthat the next future observations from these groups are ordered in a specific\nway. Several applications of these NPI lower and upper probabilities are\nexplored, including multiple groups inference, diagnostic accuracy and ranked\nset sampling.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:41:53 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 13:28:54 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Coolen-Maturi", "Tahani", ""]]}, {"id": "2007.03571", "submitter": "Sudhansu Sekhar Maiti", "authors": "Sudhansu S. Maiti, Molay Kumar Ruidas, Sumanta Adhya", "title": "A Natural Discrete One Parameter Polynomial Exponential Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new natural discrete version of the one parameter polynomial\nexponential family of distributions have been proposed and studied. The\ndistribution is named as Natural Discrete One Parameter Polynomial Exponential\n(NDOPPE) distribution. Structural and reliability properties have been studied.\nEstimation procedure of the parameter of the distribution have been mentioned.\nCompound NDOPPE distribution in the context of collective risk model have been\nobtained in closed form. The new compound distribution has been compared with\nthe classical compound Poisson, compound Negative binomial, compound discrete\nLindley, compound xgamma-I and compound xgamma-II distributions regarding\nsuitability of modelling extreme data with the help of some automobile claim.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:47:19 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Maiti", "Sudhansu S.", ""], ["Ruidas", "Molay Kumar", ""], ["Adhya", "Sumanta", ""]]}, {"id": "2007.03681", "submitter": "Prateek Bansal", "authors": "Prateek Bansal, Rico Krueger, Daniel J. Graham", "title": "Fast Bayesian Estimation of Spatial Count Data Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial count data models are used to explain and predict the frequency of\nphenomena such as traffic accidents in geographically distinct entities such as\ncensus tracts or road segments. These models are typically estimated using\nBayesian Markov chain Monte Carlo (MCMC) simulation methods, which, however,\nare computationally expensive and do not scale well to large datasets.\nVariational Bayes (VB), a method from machine learning, addresses the\nshortcomings of MCMC by casting Bayesian estimation as an optimisation problem\ninstead of a simulation problem. Considering all these advantages of VB, a VB\nmethod is derived for posterior inference in negative binomial models with\nunobserved parameter heterogeneity and spatial dependence. P\\'olya-Gamma\naugmentation is used to deal with the non-conjugacy of the negative binomial\nlikelihood and an integrated non-factorised specification of the variational\ndistribution is adopted to capture posterior dependencies. The benefits of the\nproposed approach are demonstrated in a Monte Carlo study and an empirical\napplication on estimating youth pedestrian injury counts in census tracts of\nNew York City. The VB approach is around 45 to 50 times faster than MCMC on a\nregular eight-core processor in a simulation and an empirical study, while\noffering similar estimation and predictive accuracy. Conditional on the\navailability of computational resources, the embarrassingly parallel\narchitecture of the proposed VB method can be exploited to further accelerate\nits estimation by up to 20 times.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 10:24:45 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 13:19:03 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Bansal", "Prateek", ""], ["Krueger", "Rico", ""], ["Graham", "Daniel J.", ""]]}, {"id": "2007.03699", "submitter": "Michael Muthukrishna", "authors": "Carl Falk and Michael Muthukrishna", "title": "Parsimony in Model Selection: Tools for Assessing Fit Propensity", "comments": "36 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theories can be represented as statistical models for empirical testing.\nThere is a vast literature on model selection and multimodel inference that\nfocuses on how to assess which statistical model, and therefore which theory,\nbest fits the available data. For example, given some data, one can compare\nmodels on various information criterion or other fit statistics. However, what\nthese indices fail to capture is the full range of counterfactuals. That is,\nsome models may fit the given data better not because they represent a more\ncorrect theory, but simply because these models have more fit propensity - a\ntendency to fit a wider range of data, even nonsensical data, better. Current\napproaches fall short in considering the principle of parsimony (Occam's\nRazor), often equating it with the number of model parameters. Here we offer a\ntoolkit for researchers to better study and understand parsimony through the\nfit propensity of Structural Equation Models. We provide an R package\n(ockhamSEM) built on the popular lavaan package. To illustrate the importance\nof evaluating fit propensity, we use ockhamSEM to investigate the factor\nstructure of the Rosenberg Self-Esteem Scale.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:00:03 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Falk", "Carl", ""], ["Muthukrishna", "Michael", ""]]}, {"id": "2007.03732", "submitter": "Tracy Qi Dong", "authors": "Tracy Qi Dong and Jon Wakefield", "title": "Space-time smoothing models for sub-national measles routine\n  immunization coverage estimation with complex survey data", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite substantial advances in global measles vaccination, measles disease\nburden remains high in many low- and middle-income countries. A key public\nhealth strategy for controling measles in such high-burden settings is to\nconduct supplementary immunization activities (SIAs) in the form of mass\nvaccination campaigns, in addition to delivering scheduled vaccination through\nroutine immunization (RI) programs. To achieve balanced implementations of RI\nand SIAs, robust measurement of sub-national RI-specific coverage is crucial.\nIn this paper, we develop a space-time smoothing model for estimating\nRI-specific coverage of the first dose of measles-containing-vaccines (MCV1) at\nsub-national level using complex survey data. The application that motivated\nthis work is estimation of the RI-specific MCV1 coverage in Nigeria's 36 states\nand the Federal Capital Territory. Data come from four Demographic and Health\nSurveys, three Multiple Indicator Cluster Surveys, and two National Nutrition\nand Health Surveys conducted in Nigeria between 2003 and 2018. Our method\nincorporates information from the SIA calendar published by the World Health\nOrganization and accounts for the impact of SIAs on the overall MCV1 coverage,\nas measured by cross-sectional surveys. The model can be used to analyze data\nfrom multiple surveys with different data collection schemes and construct\ncoverage estimates with uncertainty that reflects the various sampling designs.\nImplementation of our method can be done efficiently using integrated nested\nLaplace approximation (INLA).\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:44:00 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Dong", "Tracy Qi", ""], ["Wakefield", "Jon", ""]]}, {"id": "2007.03792", "submitter": "Alexander Kaizer", "authors": "Alexander M. Kaizer, Joseph S. Koopmeiners, Nan Chen, Brian P. Hobbs", "title": "Statistical design considerations for trials that study multiple\n  indications", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breakthroughs in cancer biology have defined new research programs\nemphasizing the development of therapies that target specific pathways in tumor\ncells. Innovations in clinical trial design have followed with master protocols\ndefined by inclusive eligibility criteria and evaluations of multiple therapies\nand/or histologies. Consequently, characterization of subpopulation\nheterogeneity has become central to the formulation and selection of a study\ndesign. However, this transition to master protocols has led to challenges in\nidentifying the optimal trial design and proper calibration of hyperparameters.\nWe often evaluate a range of null and alternative scenarios, however there has\nbeen little guidance on how to synthesize the potentially disparate\nrecommendations for what may be optimal. This may lead to the selection of\nsuboptimal designs and statistical methods that do not fully accommodate the\nsubpopulation heterogeneity. This article proposes novel optimization criteria\nfor calibrating and evaluating candidate statistical designs of master\nprotocols in the presence of the potential for treatment effect heterogeneity\namong enrolled patient subpopulations. The framework is applied to demonstrate\nthe statistical properties of conventional study designs when treatments offer\nheterogeneous benefit as well as identify optimal designs devised to monitor\nthe potential for heterogeneity among patients with differing clinical\nindications using Bayesian modeling.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 21:20:00 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Kaizer", "Alexander M.", ""], ["Koopmeiners", "Joseph S.", ""], ["Chen", "Nan", ""], ["Hobbs", "Brian P.", ""]]}, {"id": "2007.03942", "submitter": "Bruno Sudret", "authors": "H. M. Kroetz, M. Moustapha, A.T. Beck and B. Sudret", "title": "A two-level Kriging-based approach with active learning for solving\n  time-variant risk optimization problems", "comments": null, "journal-ref": "Reliability Engineering & System Safety, 203, November 2020,\n  107033", "doi": "10.1016/j.ress.2020.107033", "report-no": "RSUQ-2020-007", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have been proposed in the literature to solve\nreliability-based optimization problems, where failure probabilities are design\nconstraints. However, few methods address the problem of life-cycle cost or\nrisk optimization, where failure probabilities are part of the objective\nfunction. Moreover, few papers in the literature address time-variant\nreliability problems in life-cycle cost or risk optimization formulations; in\nparticular, because most often computationally expensive Monte Carlo simulation\nis required. This paper proposes a numerical framework for solving general risk\noptimization problems involving time-variant reliability analysis. To alleviate\nthe computational burden of Monte Carlo simulation, two adaptive coupled\nsurrogate models are used: the first one to approximate the objective function,\nand the second one to approximate the quasi-static limit state function. An\niterative procedure is implemented for choosing additional support points to\nincrease the accuracy of the surrogate models. Three application problems are\nused to illustrate the proposed approach. Two examples involve random load and\nrandom resistance degradation processes. The third problem is related to\nload-path dependent failures. This subject had not yet been addressed in the\ncontext of risk-based optimization. It is shown herein that accurate solutions\nare obtained, with extremely limited numbers of objective function and limit\nstate functions calls.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 08:00:02 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Kroetz", "H. M.", ""], ["Moustapha", "M.", ""], ["Beck", "A. T.", ""], ["Sudret", "B.", ""]]}, {"id": "2007.03973", "submitter": "Satoshi Usami", "authors": "Satoshi Usami", "title": "Within-Person Variability Score-Based Causal Inference: A Two-Step\n  Estimation for Joint Effects of Time-Varying Treatments", "comments": "Supplemental materials are available upon request. Or, access\n  http://usami-lab.com/Arxiv_20210715_ALL.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavioral science researchers have recently shown strong interest in\ndisaggregating within-person relations from between-person differences (stable\ntraits) using longitudinal data. In this paper, we propose a method of\nwithin-person variability score-based causal inference for estimating joint\neffects of time-varying continuous treatments by effectively controlling for\nstable traits. After explaining the assumed data-generating process and\nproviding formal definitions of stable trait factors and within-person\nvariability scores, we introduce the proposed method, which consists of a\ntwo-step analysis. Within-person variability scores for each person, which are\ndisaggregated from stable traits of that person, are first calculated using\nweights based on a best linear correlation preserving predictor through\nstructural equation modeling. Causal parameters are then estimated via a\npotential outcome approach, either marginal structural models (MSMs) or\nstructural nested mean models (SNMMs), using calculated within-person\nvariability scores. The method does not assume linearity for observed\ntime-varying confounders at the within-person level. We emphasize the use of\nSNMMs with G-estimation because of its property of being doubly robust to model\nmisspecifications in how observed time-varying confounders are associated with\ntreatments/predictors and outcomes at the within-person level. Through\nsimulation and empirical application to data regarding sleep habits and mental\nhealth status from the Tokyo Teen Cohort study, we show that the proposed\nmethod can recover causal parameters well and that causal estimates might be\nseverely biased if one does not properly account for stable traits.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 09:08:38 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 21:17:51 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Usami", "Satoshi", ""]]}, {"id": "2007.04009", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn and Frank Schaarschmidt", "title": "A Tukey type trend test for repeated carcinogenicity bioassays,\n  motivated by multiple glyphosate studies", "comments": "1 fig", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the last two decades, significant methodological progress to the\nsimultaneous inference of simple and complex randomized designs, particularly\nproportions as endpoints, occurred. This includes: i) the new Tukey trend test\napproach, ii) multiple contrast tests for binomial proportions, iii) multiple\ncontrast tests for poly-k estimates, and Add-1 approximation for one-sided\ninference. This report focus on a new Tukey type trend test to evaluate\nrepeated long-term carcinogenicity bioassays which was motivated by multiple\nglyphosate studies. Notice, it is not the aim here to contribute to the\nevaluation of Glyphosate and its controversies. By means of the CRAN-packages\ntukeytrend, MCPAN, multcomp the real data analysis is straightforward possible.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 10:26:47 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Hothorn", "Ludwig A.", ""], ["Schaarschmidt", "Frank", ""]]}, {"id": "2007.04037", "submitter": "Daniel Nevo", "authors": "Daniel Nevo, Deborah Blacker, Eric B. Larson, Sebastien Haneuse", "title": "Modeling semi-competing risks data as a longitudinal bivariate process", "comments": "36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Adult Changes in Thought (ACT) study is a long-running prospective study\nof incident all-cause dementia and Alzheimer's disease (AD). As the cohort\nages, death (a terminal event) is a prominent competing risk for AD (a\nnon-terminal event), although the reverse is not the case. As such, analyses of\ndata from ACT can be placed within the semi-competing risks framework. Central\nto semi-competing risks, and in contrast to standard competing risks, is that\none can learn about the dependence structure between the two events. To-date,\nhowever, most methods for semi-competing risks treat dependence as a nuisance\nand not a potential source of new clinical knowledge. We propose a novel\nregression-based framework that views the two time-to-event outcomes through\nthe lens of a longitudinal bivariate process on a partition of the time scale.\nA key innovation of the framework is that dependence is represented in two\ndistinct forms, $\\textit{local}$ and $\\textit{global}$ dependence, both of\nwhich have intuitive clinical interpretations. Estimation and inference are\nperformed via penalized maximum likelihood, and can accommodate right\ncensoring, left truncation and time-varying covariates. The framework is used\nto investigate the role of gender and having $\\ge$1 APOE-$\\epsilon4$ allele on\nthe joint risk of AD and death.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 11:28:00 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Nevo", "Daniel", ""], ["Blacker", "Deborah", ""], ["Larson", "Eric B.", ""], ["Haneuse", "Sebastien", ""]]}, {"id": "2007.04042", "submitter": "Mayu Hiraishi", "authors": "Mayu Hiraishi, Kensuke Tanioka and Toshio Shimokawa", "title": "Concordance Rate of a Four-Quadrant Plot for Repeated Measurements", "comments": "The purpose of the paper has been changed. In the first version, we\n  focused on robustness in the assessment of \"agreement\", however, robustness\n  has been found to be inappropriate in actual clinical practice", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Before new clinical measurement methods are implemented in clinical practice,\nit must be confirmed whether their results are equivalent to those of existing\nmethods. The agreement of the trend between these methods is evaluated using\nthe four-quadrant plot, which describes the trend of change in each difference\nof the two measurement methods' values in sequential time points, and the\nplot's concordance rate, which is calculated using the sum of data points in\nthe four-quadrant plot that agree with this trend divided by the number of all\naccepted data points. However, the conventional concordance rate does not\nconsider the covariance between the data on individual subjects, which may\naffect its proper evaluation. Therefore, we proposed a new concordance rate\ncalculated by each individual according to the number of agreement. Moreover,\nthis proposed method can set a parameter that the minimum concordant number\nbetween two measurement techniques. The parameter can provide a more detailed\ninterpretation of the degree of agreement. A numerical simulation conducted\nwith several factors indicated that the proposed method resulted in a more\naccurate evaluation. We also showed a real data and compared the proposed\nmethod with the conventional approach. Then, we concluded the discussion with\nthe implementation in clinical studies.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 11:33:50 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 14:07:05 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 12:07:26 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Hiraishi", "Mayu", ""], ["Tanioka", "Kensuke", ""], ["Shimokawa", "Toshio", ""]]}, {"id": "2007.04155", "submitter": "Yanxun Xu", "authors": "William Hua, Hongyuan Mei, Sarah Zohar, Magali Giral and Yanxun Xu", "title": "Personalized Dynamic Treatment Regimes in Continuous Time: A Bayesian\n  Approach for Optimizing Clinical Decisions with Timing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate models of clinical actions and their impacts on disease progression\nare critical for estimating personalized optimal dynamic treatment regimes\n(DTRs) in medical/health research, especially in managing chronic conditions.\nTraditional statistical methods for DTRs usually focus on estimating the\noptimal treatment or dosage at each given medical intervention, but overlook\nthe important question of \"when this intervention should happen.\" We fill this\ngap by developing a two-step Bayesian approach to optimize clinical decisions\nwith timing. In the first step, we build a generative model for a sequence of\nmedical interventions-which are discrete events in continuous time-with a\nmarked temporal point process (MTPP) where the mark is the assigned treatment\nor dosage. Then this clinical action model is embedded into a Bayesian joint\nframework where the other components model clinical observations including\nlongitudinal medical measurements and time-to-event data conditional on\ntreatment histories. In the second step, we propose a policy gradient method to\nlearn the personalized optimal clinical decision that maximizes the patient\nsurvival by interacting the MTPP with the model on clinical observations while\naccounting for uncertainties in clinical observations learned from the\nposterior inference of the Bayesian joint model in the first step. A signature\napplication of the proposed approach is to schedule follow-up visitations and\nassign a dosage at each visitation for patients after kidney transplantation.\nWe evaluate our approach with comparison to alternative methods on both\nsimulated and real-world datasets. In our experiments, the personalized\ndecisions made by the proposed method are clinically useful: they are\ninterpretable and successfully help improve patient survival.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 14:37:22 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 16:29:22 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 15:29:05 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Hua", "William", ""], ["Mei", "Hongyuan", ""], ["Zohar", "Sarah", ""], ["Giral", "Magali", ""], ["Xu", "Yanxun", ""]]}, {"id": "2007.04177", "submitter": "Rafael De Andrade Moral", "authors": "John Haslett, Andrew C. Parnell, John Hinde, Rafael A. Moral", "title": "Modelling excess zeros in count data: A new perspective on modelling\n  approaches", "comments": "41 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the analysis of count data in which the observed frequency of\nzero counts is unusually large, typically with respect to the Poisson\ndistribution. We focus on two alternative modelling approaches: Over-Dispersion\n(OD) models, and Zero-Inflation (ZI) models, both of which can be seen as\ngeneralisations of the Poisson distribution; we refer to these as Implicit and\nExplicit ZI models, respectively. Although sometimes seen as competing\napproaches, they can be complementary; OD is a consequence of ZI modelling, and\nZI is a by-product of OD modelling. The central objective in such analyses is\noften concerned with inference on the effect of covariates on the mean, in\nlight of the apparent excess of zeros in the counts. Typically the modelling of\nthe excess zeros per se is a secondary objective and there are choices to be\nmade between, and within, the OD and ZI approaches. The contribution of this\npaper is primarily conceptual. We contrast, descriptively, the impact on zeros\nof the two approaches. We further offer a novel descriptive characterisation of\nalternative ZI models, including the classic hurdle and mixture models, by\nproviding a unifying theoretical framework for their comparison. This in turn\nleads to a novel and technically simpler ZI model. We develop the underlying\ntheory for univariate counts and touch on its implication for multivariate\ncount data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:07:40 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 16:05:22 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Haslett", "John", ""], ["Parnell", "Andrew C.", ""], ["Hinde", "John", ""], ["Moral", "Rafael A.", ""]]}, {"id": "2007.04229", "submitter": "Dootika Vats", "authors": "Kushagra Gupta and Dootika Vats", "title": "Estimating Monte Carlo variance from multiple Markov chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-increasing power of the personal computer has led to easy parallel\nimplementations of Markov chain Monte Carlo (MCMC). However, almost all work in\nestimating the variance of Monte Carlo averages, including the efficient batch\nmeans (BM) estimator, focuses on a single-chain MCMC run. We demonstrate that\nsimply averaging covariance matrix estimators from multiple chains (average BM)\ncan yield critical underestimates in small sample sizes, especially for slow\nmixing Markov chains. We propose a multivariate replicated batch means (RBM)\nestimator that utilizes information from parallel chains, thereby correcting\nfor the underestimation. Under weak conditions on the mixing rate of the\nprocess, the RBM and ABM estimator are both strongly consistent and exhibit\nsimilar large-sample bias and variance. However, in small runs the RBM\nestimator can be dramatically superior. This is demonstrated through a variety\nof examples, including a two-variable Gibbs sampler for a bivariate Gaussian\ntarget distribution. Here, we obtain a closed-form expression for the\nasymptotic covariance matrix of the Monte Carlo estimator, a useful result for\nbenchmarking in the future.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 16:14:08 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 08:49:48 GMT"}, {"version": "v3", "created": "Sun, 26 Jul 2020 16:57:15 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Gupta", "Kushagra", ""], ["Vats", "Dootika", ""]]}, {"id": "2007.04285", "submitter": "Gang Li", "authors": "Gang Li, Jan Hannig", "title": "Deep Fiducial Inference", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the mid-2000s, there has been a resurrection of interest in modern\nmodifications of fiducial inference. To date, the main computational tool to\nextract a generalized fiducial distribution is Markov chain Monte Carlo (MCMC).\nWe propose an alternative way of computing a generalized fiducial distribution\nthat could be used in complex situations. In particular, to overcome the\ndifficulty when the unnormalized fiducial density (needed for MCMC), we design\na fiducial autoencoder (FAE). The fitted autoencoder is used to generate\ngeneralized fiducial samples of the unknown parameters. To increase accuracy,\nwe then apply an approximate fiducial computation (AFC) algorithm, by rejecting\nsamples that when plugged into a decoder do not replicate the observed data\nwell enough. Our numerical experiments show the effectiveness of our FAE-based\ninverse solution and the excellent coverage performance of the AFC corrected\nFAE solution.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 17:33:14 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Li", "Gang", ""], ["Hannig", "Jan", ""]]}, {"id": "2007.04286", "submitter": "John Harlim", "authors": "Faheem Gilani, Dimitrios Giannakis, and John Harlim", "title": "Kernel-based Prediction of Non-Markovian Time Series", "comments": "7 figures", "journal-ref": null, "doi": "10.1016/j.physd.2020.132829", "report-no": null, "categories": "stat.ME math.DS physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric method to predict non-Markovian time series of partially\nobserved dynamics is developed. The prediction problem we consider is a\nsupervised learning task of finding a regression function that takes a delay\nembedded observable to the observable at a future time. When delay embedding\ntheory is applicable, the proposed regression function is a consistent\nestimator of the flow map induced by the delay embedding. Furthermore, the\ncorresponding Mori-Zwanzig equation governing the evolution of the observable\nsimplifies to only a Markovian term, represented by the regression function. We\nrealize this supervised learning task with a class of kernel-based linear\nestimators, the kernel analog forecast (KAF), which are consistent in the limit\nof large data. In a scenario with a high-dimensional covariate space, we employ\na Markovian kernel smoothing method which is computationally cheaper than the\nNystr\\\"om projection method for realizing KAF. In addition to the guaranteed\ntheoretical convergence, we numerically demonstrate the effectiveness of this\napproach on higher-dimensional problems where the relevant kernel features are\ndifficult to capture with the Nystr\\\"om method. Given noisy training data, we\npropose a nonparametric smoother as a de-noising method. Numerically, we show\nthat the proposed smoother is more accurate than EnKF and 4Dvar in de-noising\nsignals corrupted by independent (but not necessarily identically distributed)\nnoise, even if the smoother is constructed using a data set corrupted by white\nnoise. We show skillful prediction using the KAF constructed from the denoised\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 17:33:24 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 17:43:29 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 15:28:47 GMT"}, {"version": "v4", "created": "Tue, 12 Jan 2021 20:19:47 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Gilani", "Faheem", ""], ["Giannakis", "Dimitrios", ""], ["Harlim", "John", ""]]}, {"id": "2007.04358", "submitter": "Owen Thomas", "authors": "Owen Thomas and Henri Pesonen and Jukka Corander", "title": "Generalised Bayes Updates with $f$-divergences through Probabilistic\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stream of algorithmic advances has steadily increased the popularity of the\nBayesian approach as an inference paradigm, both from the theoretical and\napplied perspective. Even with apparent successes in numerous application\nfields, a rising concern is the robustness of Bayesian inference in the\npresence of model misspecification, which may lead to undesirable extreme\nbehavior of the posterior distributions for large sample sizes. Generalized\nbelief updating with a loss function represents a central principle to making\nBayesian inference more robust and less vulnerable to deviations from the\nassumed model. Here we consider such updates with $f$-divergences to quantify a\ndiscrepancy between the assumed statistical model and the probability\ndistribution which generated the observed data. Since the latter is generally\nunknown, estimation of the divergence may be viewed as an intractable problem.\nWe show that the divergence becomes accessible through the use of probabilistic\nclassifiers that can leverage an estimate of the ratio of two probability\ndistributions even when one or both of them is unknown. We demonstrate the\nbehavior of generalized belief updates for various specific choices under the\n$f$-divergence family. We show that for specific divergence functions such an\napproach can even improve on methods evaluating the correct model likelihood\nfunction analytically.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 18:34:12 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Thomas", "Owen", ""], ["Pesonen", "Henri", ""], ["Corander", "Jukka", ""]]}, {"id": "2007.04386", "submitter": "Hannah Director", "authors": "Hannah M. Director, Adrian E. Raftery", "title": "Contour Models for Boundaries Enclosing Star-Shaped and Approximately\n  Star-Shaped Polygons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boundaries on spatial fields divide regions with particular features from\nsurrounding background areas. These boundaries are often described with contour\nlines. To measure and record these boundaries, contours are often represented\nas ordered sequences of spatial points that connect to form a line. Methods to\nidentify boundary lines from interpolated spatial fields are well-established.\nLess attention has been paid to how to model sequences of connected spatial\npoints. For data of the latter form, we introduce the Gaussian Star-shaped\nContour Model (GSCM). GSMCs generate sequences of spatial points via generating\nsets of distances in various directions from a fixed starting point. The GSCM\nis designed for modeling contours that enclose regions that are star-shaped\npolygons or approximately star-shaped polygons. Metrics are introduced to\nassess the extent to which a polygon deviates from star-shaped. Simulation\nstudies illustrate the performance of the GSCM in various scenarios and an\nanalysis of Arctic sea ice edge contour data highlights how GSCMs can be\napplied to observational data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 19:24:47 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Director", "Hannah M.", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "2007.04387", "submitter": "Huiming Lin", "authors": "Huiming Lin and Meng Li", "title": "Double spike Dirichlet priors for structured weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assigning weights to a large pool of objects is a fundamental task in a wide\nvariety of applications. In this article, we introduce a concept of structured\nhigh-dimensional probability simplexes, whose most components are zero or near\nzero and the remaining ones are close to each other. Such structure is well\nmotivated by 1) high-dimensional weights that are common in modern\napplications, and 2) ubiquitous examples in which equal weights -- despite\ntheir simplicity -- often achieve favorable or even state-of-the-art predictive\nperformances. This particular structure, however, presents unique challenges\nboth computationally and statistically. To address these challenges, we propose\na new class of double spike Dirichlet priors to shrink a probability simplex to\none with the desired structure. When applied to ensemble learning, such priors\nlead to a Bayesian method for structured high-dimensional ensembles that is\nuseful for forecast combination and improving random forests, while enabling\nuncertainty quantification. We design efficient Markov chain Monte Carlo\nalgorithms for easy implementation. Posterior contraction rates are established\nto provide theoretical support. We demonstrate the wide applicability and\ncompetitive performance of the proposed methods through simulations and two\nreal data applications using the European Central Bank Survey of Professional\nForecasters dataset and a UCI dataset.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 19:32:27 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 00:52:23 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Lin", "Huiming", ""], ["Li", "Meng", ""]]}, {"id": "2007.04441", "submitter": "Andersen Chang", "authors": "Andersen Chang and Minjie Wang and Genevera Allen", "title": "Sparse Regression for Extreme Values", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of selecting features associated with extreme values in\nhigh dimensional linear regression. Normally, in linear modeling problems, the\npresence of abnormal extreme values or outliers is considered an anomaly which\nshould either be removed from the data or remedied using robust regression\nmethods. In many situations, however, the extreme values in regression modeling\nare not outliers but rather the signals of interest; consider traces from\nspiking neurons, volatility in finance, or extreme events in climate science,\nfor example. In this paper, we propose a new method for sparse high-dimensional\nlinear regression for extreme values which is motivated by the Subbotin, or\ngeneralized normal distribution, which we call the extreme value linear\nregression model. For our method, we utilize an $\\ell_p$ norm loss where $p$ is\nan even integer greater than two; we demonstrate that this loss increases the\nweight on extreme values. We prove consistency and variable selection\nconsistency for the extreme value linear regression with a Lasso penalty, which\nwe term the Extreme Lasso, and we also analyze the theoretical impact of\nextreme value observations on the model parameter estimates using the concept\nof influence functions. Through simulation studies and a real-world data\nexample, we show that the Extreme Lasso outperforms other methods currently\nused in the literature for selecting features of interest associated with\nextreme values in high-dimensional regression.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:24:49 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 22:57:26 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Chang", "Andersen", ""], ["Wang", "Minjie", ""], ["Allen", "Genevera", ""]]}, {"id": "2007.04443", "submitter": "Henry Lam", "authors": "Henry Lam, Haidong Li, Xuhui Zhang", "title": "Minimax Efficient Finite-Difference Stochastic Gradient Estimators Using\n  Black-Box Function Evaluations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard approaches to stochastic gradient estimation, with only noisy\nblack-box function evaluations, use the finite-difference method or its\nvariants. While natural, it is open to our knowledge whether their statistical\naccuracy is the best possible. This paper argues so by showing that central\nfinite-difference is a nearly minimax optimal zeroth-order gradient estimator\nfor a suitable class of objective functions and mean squared risk, among both\nthe class of linear estimators and the much larger class of all (nonlinear)\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:29:45 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 16:05:05 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Lam", "Henry", ""], ["Li", "Haidong", ""], ["Zhang", "Xuhui", ""]]}, {"id": "2007.04445", "submitter": "Muxuan Liang", "authors": "Muxuan Liang, Young-Geun Choi, Yang Ning, Maureen A Smith, Ying-Qi\n  Zhao", "title": "Estimation and inference on high-dimensional individualized treatment\n  rule in observational data using split-and-pooled de-correlated score", "comments": "15 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing adoption of electronic health records, there is an\nincreasing interest in developing individualized treatment rules, which\nrecommend treatments according to patients' characteristics, from large\nobservational data. However, there is a lack of valid inference procedures for\nsuch rules developed from this type of data in the presence of high-dimensional\ncovariates. In this work, we develop a penalized doubly robust method to\nestimate the optimal individualized treatment rule from high-dimensional data.\nWe propose a split-and-pooled de-correlated score to construct hypothesis tests\nand confidence intervals. Our proposal utilizes the data splitting to conquer\nthe slow convergence rate of nuisance parameter estimations, such as\nnon-parametric methods for outcome regression or propensity models. We\nestablish the limiting distributions of the split-and-pooled de-correlated\nscore test and the corresponding one-step estimator in high-dimensional\nsetting. Simulation and real data analysis are conducted to demonstrate the\nsuperiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:36:06 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 08:48:18 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 22:17:33 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Liang", "Muxuan", ""], ["Choi", "Young-Geun", ""], ["Ning", "Yang", ""], ["Smith", "Maureen A", ""], ["Zhao", "Ying-Qi", ""]]}, {"id": "2007.04470", "submitter": "Diana Cai", "authors": "Diana Cai, Trevor Campbell, Tamara Broderick", "title": "Finite mixture models do not reliably learn the number of components", "comments": "Proceedings of the 38th International Conference on Machine Learning\n  (ICML), to appear. 25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists and engineers are often interested in learning the number of\nsubpopulations (or components) present in a data set. A common suggestion is to\nuse a finite mixture model (FMM) with a prior on the number of components. Past\nwork has shown the resulting FMM component-count posterior is consistent; that\nis, the posterior concentrates on the true, generating number of components.\nBut consistency requires the assumption that the component likelihoods are\nperfectly specified, which is unrealistic in practice. In this paper, we add\nrigor to data-analysis folk wisdom by proving that under even the slightest\nmodel misspecification, the FMM component-count posterior diverges: the\nposterior probability of any particular finite number of components converges\nto 0 in the limit of infinite data. Contrary to intuition, posterior-density\nconsistency is not sufficient to establish this result. We develop novel\nsufficient conditions that are more realistic and easily checkable than those\ncommon in the asymptotics literature. We illustrate practical consequences of\nour theory on simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 23:05:18 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 16:12:03 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 15:38:13 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Cai", "Diana", ""], ["Campbell", "Trevor", ""], ["Broderick", "Tamara", ""]]}, {"id": "2007.04511", "submitter": "Elizabeth Ogburn", "authors": "Bonnie Smith, Elizabeth L. Ogburn, Matt McGue, Saonli Basu, Daniel O.\n  Scharfstein", "title": "Causal Effects in Twin Studies: the Role of Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of twins designs to address causal questions is becoming increasingly\npopular. A standard assumption is that there is no interference between\ntwins---that is, no twin's exposure has a causal impact on their co-twin's\noutcome. However, there may be settings in which this assumption would not\nhold, and this would (1) impact the causal interpretation of parameters\nobtained by commonly used existing methods; (2) change which effects are of\ngreatest interest; and (3) impact the conditions under which we may estimate\nthese effects. We explore these issues, and we derive semi-parametric efficient\nestimators for causal effects in the presence of interference between twins.\nUsing data from the Minnesota Twin Family Study, we apply our estimators to\nassess whether twins' consumption of alcohol in early adolescence may have a\ncausal impact on their co-twins' substance use later in life.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 02:21:56 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Smith", "Bonnie", ""], ["Ogburn", "Elizabeth L.", ""], ["McGue", "Matt", ""], ["Basu", "Saonli", ""], ["Scharfstein", "Daniel O.", ""]]}, {"id": "2007.04518", "submitter": "Hee-Seok Oh", "authors": "Ha-Young Shin and Hee-Seok Oh", "title": "Robust Geodesic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies robust regression for data on Riemannian manifolds.\nGeodesic regression is the generalization of linear regression to a setting\nwith a manifold-valued dependent variable and one or more real-valued\nindependent variables. The existing work on geodesic regression uses the\nsum-of-squared errors to find the solution, but as in the classical Euclidean\ncase, the least-squares method is highly sensitive to outliers. In this paper,\nwe use M-type estimators, including the $L_1$, Huber and Tukey biweight\nestimators, to perform robust geodesic regression, and describe how to\ncalculate the tuning parameters for the latter two. We also show that, on\ncompact symmetric spaces, all M-type estimators are maximum likelihood\nestimators, and argue for the overall superiority of the $L_1$ estimator over\nthe $L_2$ and Huber estimators on high-dimensional manifolds and over the Tukey\nbiweight estimator on compact high-dimensional manifolds. Results from\nnumerical examples, including analysis of real neuroimaging data, demonstrate\nthe promising empirical properties of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 02:41:32 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 08:16:43 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Shin", "Ha-Young", ""], ["Oh", "Hee-Seok", ""]]}, {"id": "2007.04558", "submitter": "Dehan Kong", "authors": "Dengdeng Yu, Linbo Wang, Dehan Kong, Hongtu Zhu", "title": "Beyond Scalar Treatment: A Causal Analysis of Hippocampal Atrophy on\n  Behavioral Deficits in Alzheimer's Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease is a progressive form of dementia that results in\nproblems with memory, thinking and behavior. It often starts with abnormal\naggregation and deposition of beta-amyloid and tau, followed by neuronal damage\nsuch as atrophy of the hippocampi, and finally leads to behavioral deficits.\nDespite significant progress in finding biomarkers associated with behavioral\ndeficits, the underlying causal mechanism remains largely unknown. Here we\ninvestigate whether and how hippocampal atrophy contributes to behavioral\ndeficits based on a large-scale observational study conducted by the\nAlzheimer's Disease Neuroimaging Initiative (ADNI). As a key novelty, we use 2D\nrepresentations of the hippocampi, which allows us to better understand atrophy\nassociated with different subregions. It, however, introduces methodological\nchallenges as existing causal inference methods are not well suited for\nexploiting structural information embedded in the 2D exposures. Moreover, our\ndata contain more than 6 million clinical and genetic covariates, necessitating\nappropriate confounder selection methods. We hence develop a novel two-step\ncausal inference approach tailored for our ADNI data application. Analysis\nresults suggest that atrophy of CA1 and subiculum subregions may cause more\nsevere behavioral deficits compared to CA2 and CA3 subregions. We further\nevaluate our method using simulations and provide theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 05:08:38 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Yu", "Dengdeng", ""], ["Wang", "Linbo", ""], ["Kong", "Dehan", ""], ["Zhu", "Hongtu", ""]]}, {"id": "2007.04713", "submitter": "Savi Virolainen", "authors": "Savi Virolainen", "title": "Structural Gaussian mixture vector autoregressive model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A structural version of the Gaussian mixture vector autoregressive model is\nintroduced. The shocks are identified by combining simultaneous diagonalization\nof the error term covariance matrices with zero and sign constraints. It turns\nout that this often leads to less restrictive identification conditions than in\nconventional SVAR models, while some of the constraints are also testable. The\naccompanying R-package gmvarkit provides easy-to-use tools for estimating the\nmodels and applying the introduced methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 11:19:16 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 15:28:43 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 13:59:33 GMT"}, {"version": "v4", "created": "Fri, 18 Dec 2020 14:32:41 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Virolainen", "Savi", ""]]}, {"id": "2007.04791", "submitter": "Charlotte Baey", "authors": "Charlotte Baey and Estelle Kuhn", "title": "varTestnlme: an R package for Variance Components Testing in Linear and\n  Nonlinear Mixed-effects Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of variance components testing arises naturally when building\nmixed-effects models, to decide which effects should be modeled as fixed or\nrandom. While tests for fixed effects are available in R for models fitted with\nlme4, tools are missing when it comes to random effects. The varTestnlme\npackage for R aims at filling this gap. It allows to test whether any subset of\nthe variances and covariances corresponding to any subset of the random\neffects, are equal to zero using asymptotic property of the likelihood ratio\ntest statistic. It also offers the possibility to test simultaneously for fixed\neffects and variance components. It can be used for linear, generalized linear\nor nonlinear mixed-effects models fitted via lme4, nlme or saemix. Theoretical\nproperties of the used likelihood ratio test are recalled, numerical methods\nused to implement the test procedure are detailed and examples based on\ndifferent real datasets using different mixed models are provided.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:32:12 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 16:21:54 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 14:25:06 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Baey", "Charlotte", ""], ["Kuhn", "Estelle", ""]]}, {"id": "2007.04799", "submitter": "Sebastian Fuchs", "authors": "Sebastian Fuchs and F. Marta L. Di Lascio and Fabrizio Durante", "title": "Dissimilarity functions for rank-invariant hierarchical clustering of\n  continuous variables", "comments": "38 pages, 10 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theoretical framework is presented for a (copula-based) notion of\ndissimilarity between continuous random vectors and its main properties are\nstudied. The proposed dissimilarity assigns the smallest value to a pair of\nrandom vectors that are comonotonic. Various properties of this dissimilarity\nare studied, with special attention to those that are prone to the hierarchical\nagglomerative methods, such as reducibility. Some insights are provided for the\nuse of such a measure in clustering algorithms and a simulation study is\npresented. Real case studies illustrate the main features of the whole\nmethodology.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:42:30 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 11:32:57 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 10:40:49 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Fuchs", "Sebastian", ""], ["Di Lascio", "F. Marta L.", ""], ["Durante", "Fabrizio", ""]]}, {"id": "2007.04956", "submitter": "Isaac Lavine", "authors": "Isaac Lavine, Andrew Cron, and Mike West", "title": "Bayesian Computation in Dynamic Latent Factor Models", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian computation for filtering and forecasting analysis is developed for\na broad class of dynamic models. The ability to scale-up such analyses in\nnon-Gaussian, nonlinear multivariate time series models is advanced through the\nintroduction of a novel copula construction in sequential filtering of coupled\nsets of dynamic generalized linear models. The new copula approach is\nintegrated into recently introduced multiscale models in which univariate time\nseries are coupled via nonlinear forms involving dynamic latent factors\nrepresenting cross-series relationships. The resulting methodology offers\ndramatic speed-up in online Bayesian computations for sequential filtering and\nforecasting in this broad, flexible class of multivariate models. Two examples\nin nonlinear models for very heterogeneous time series of non-negative counts\ndemonstrate massive computational efficiencies relative to existing\nsimulation-based methods, while defining similar filtering and forecasting\noutcomes.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:37:35 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Lavine", "Isaac", ""], ["Cron", "Andrew", ""], ["West", "Mike", ""]]}, {"id": "2007.05052", "submitter": "Zachary Fisher", "authors": "Zachary F. Fisher and Younghoon Kim and Barbara Fredrickson and Vladas\n  Pipiras", "title": "Penalized Estimation and Forecasting of Multiple Subject Intensive\n  Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intensive Longitudinal Data (ILD) is an increasingly common data type in the\nsocial and behavioral sciences. Despite the many benefits these data provide,\nlittle work has been dedicated to realizing the potential such data hold for\nforecasting dynamic processes at the individual level. To address this gap in\nthe literature we present the multi-VAR framework, a novel methodological\napproach for penalized estimation and forecasting of ILD collected from\nmultiple individuals. Importantly, our approach estimates models for all\nindividuals simultaneously and is capable of adaptively adjusting to the amount\nof heterogeneity exhibited across individual dynamic processes. To accomplish\nthis we propose proximal gradient descent algorithm for solving the multi-VAR\nproblem and prove the consistency of the recovered transition matrices. We\nevaluate the forecasting performance of our method in comparison with a number\nof benchmark forecasting methods and provide an illustrative example involving\nthe day-to-day emotional experiences of 16 individuals over an 11-week period.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 20:34:23 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Fisher", "Zachary F.", ""], ["Kim", "Younghoon", ""], ["Fredrickson", "Barbara", ""], ["Pipiras", "Vladas", ""]]}, {"id": "2007.05114", "submitter": "Andrea Arnold", "authors": "Leah Mitchell, Andrea Arnold", "title": "Analyzing the Effects of Observation Function Selection in Ensemble\n  Kalman Filtering for Epidemic Models", "comments": "29 pages, 13 figures. This is the accepted manuscript of an article\n  published in Mathematical Biosciences. The published journal article is\n  available online at https://doi.org/10.1016/j.mbs.2021.108655", "journal-ref": "Mathematical Biosciences 339 (2021) 108655", "doi": "10.1016/j.mbs.2021.108655", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Ensemble Kalman Filter (EnKF) is a popular sequential data assimilation\nmethod that has been increasingly used for parameter estimation and forecast\nprediction in epidemiological studies. The observation function plays a\ncritical role in the EnKF framework, connecting the unknown system variables\nwith the observed data. Key differences in observed data and modeling\nassumptions have led to the use of different observation functions in the\nepidemic modeling literature. In this work, we present a novel computational\nanalysis demonstrating the effects of observation function selection when using\nthe EnKF for state and parameter estimation in this setting. In examining the\nuse of four epidemiologically-inspired observation functions of different forms\nin connection with the classic Susceptible-Infectious-Recovered (SIR) model, we\nshow how incorrect observation modeling assumptions (i.e., fitting incidence\ndata with a prevalence model, or neglecting under-reporting) can lead to\ninaccurate filtering estimates and forecast predictions. Results demonstrate\nthe importance of choosing an observation function that well interprets the\navailable data on the corresponding EnKF estimates in several filtering\nscenarios, including state estimation with known parameters, and combined state\nand parameter estimation with both constant and time-varying parameters.\nNumerical experiments further illustrate how modifying the observation noise\ncovariance matrix in the filter can help to account for uncertainty in the\nobservation function in certain cases.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 00:11:48 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 17:57:27 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mitchell", "Leah", ""], ["Arnold", "Andrea", ""]]}, {"id": "2007.05117", "submitter": "Zehang Li", "authors": "Zehang Richard Li, Bryan D Martin, Tracy Qi Dong, Geir-Arne Fuglstad,\n  John Paige, Andrea Riebler, Samuel Clark, Jon Wakefield", "title": "Space-Time Smoothing of Demographic and Health Indicators using the R\n  Package SUMMER", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of complex survey data, and the continued need\nfor estimates of demographic and health indicators at a fine spatial and\ntemporal scale, which leads to issues of data sparsity, has led to the need for\nspatio-temporal smoothing methods that acknowledge the manner in which the data\nwere collected. The open source R package SUMMER implements a variety of\nmethods for spatial or spatio-temporal smoothing of survey data. The emphasis\nis on small-area estimation. We focus primarily on indicators in a low and\nmiddle-income countries context. Our methods are particularly useful for data\nfrom Demographic Health Surveys and Multiple Indicator Cluster Surveys. We\nbuild upon functions within the survey package, and use INLA for fast Bayesian\ncomputation. This paper includes a brief overview of these methods and\nillustrates the workflow of accessing and processing surveys, estimating\nsubnational child mortality rates, and visualizing results with both simulated\ndata and DHS surveys.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 00:17:39 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Li", "Zehang Richard", ""], ["Martin", "Bryan D", ""], ["Dong", "Tracy Qi", ""], ["Fuglstad", "Geir-Arne", ""], ["Paige", "John", ""], ["Riebler", "Andrea", ""], ["Clark", "Samuel", ""], ["Wakefield", "Jon", ""]]}, {"id": "2007.05157", "submitter": "Audra McMillan", "authors": "Daniel Alabi, Audra McMillan, Jayshree Sarathy, Adam Smith and Salil\n  Vadhan", "title": "Differentially Private Simple Linear Regression", "comments": "20 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economics and social science research often require analyzing datasets of\nsensitive personal information at fine granularity, with models fit to small\nsubsets of the data. Unfortunately, such fine-grained analysis can easily\nreveal sensitive individual information. We study algorithms for simple linear\nregression that satisfy differential privacy, a constraint which guarantees\nthat an algorithm's output reveals little about any individual input data\nrecord, even to an attacker with arbitrary side information about the dataset.\nWe consider the design of differentially private algorithms for simple linear\nregression for small datasets, with tens to hundreds of datapoints, which is a\nparticularly challenging regime for differential privacy. Focusing on a\nparticular application to small-area analysis in economics research, we study\nthe performance of a spectrum of algorithms we adapt to the setting. We\nidentify key factors that affect their performance, showing through a range of\nexperiments that algorithms based on robust estimators (in particular, the\nTheil-Sen estimator) perform well on the smallest datasets, but that other more\nstandard algorithms do better as the dataset size increases.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 04:28:43 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Alabi", "Daniel", ""], ["McMillan", "Audra", ""], ["Sarathy", "Jayshree", ""], ["Smith", "Adam", ""], ["Vadhan", "Salil", ""]]}, {"id": "2007.05354", "submitter": "Ilyas Bakbergenuly", "authors": "Elena Kulinskaya, David C. Hoaglin, and Ilyas Bakbergenuly", "title": "Exploring Consequences of Simulation Design for Apparent Performance of\n  Statistical Methods. 2: Results from simulations with normally and uniformly\n  distributed sample sizes", "comments": "8 pages and full simulation results as ancillary files (Appendix A\n  and Appendix B), comprising 400 figures in each of the files, each figure\n  presenting 12 combinations of sample sizes and numbers of studies. arXiv\n  admin note: text overlap with arXiv:2006.16638", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report continues our investigation of effects a simulation design may\nhave on the conclusions on performance of statistical methods. In the context\nof meta-analysis of log-odds-ratios, we consider five generation mechanisms for\ncontrol probabilities and log-odds-ratios. Our first report (Kulinskaya et al.\n2020) considered constant sample sizes. Here we report on the results for\nnormally and uniformly distributed sample sizes.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:25:42 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Kulinskaya", "Elena", ""], ["Hoaglin", "David C.", ""], ["Bakbergenuly", "Ilyas", ""]]}, {"id": "2007.05424", "submitter": "Arthur Frouin", "authors": "Arthur Frouin (1), Claire Dandine-Roulland (1), Morgane Pierre-Jean\n  (1), Jean-Fran\\c{c}ois Deleuze (1), Christophe Ambroise (2), Edith Le Floch\n  (1) ((1) CNRGH, Institut Jacob, CEA - Universit\\'e Paris-Saclay, (2) LaMME,\n  Universit\\'e Paris-Saclay, CNRS, Universit\\'e d'\\'Evry val d'Essonne)", "title": "High heritability does not imply accurate prediction under the small\n  additive effects hypothesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-Wide Association Studies (GWAS) explain only a small fraction of\nheritability for most complex human phenotypes. Genomic heritability estimates\nthe variance explained by the SNPs on the whole genome using mixed models and\naccounts for the many small contributions of SNPs in the explanation of a\nphenotype.\n  This paper approaches heritability from a machine learning perspective, and\nexamines the close link between mixed models and ridge regression. Our\ncontribution is twofold. First, we propose estimating genomic heritability\nusing a predictive approach via ridge regression and Generalized Cross\nValidation (GCV). We show that this is consistent with classical mixed model\nbased estimation. Second, we derive simple formulae that express prediction\naccuracy as a function of the ratio n/p, where n is the population size and p\nthe total number of SNPs. These formulae clearly show that a high heritability\ndoes not imply an accurate prediction when p>n.\n  Both the estimation of heritability via GCV and the prediction accuracy\nformulae are validated using simulated data and real data from UK Biobank.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 14:57:24 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 07:11:50 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Frouin", "Arthur", ""], ["Dandine-Roulland", "Claire", ""], ["Pierre-Jean", "Morgane", ""], ["Deleuze", "Jean-Fran\u00e7ois", ""], ["Ambroise", "Christophe", ""], ["Floch", "Edith Le", ""]]}, {"id": "2007.05521", "submitter": "Elynn Chen", "authors": "Elynn Y. Chen, Jianqing Fan, Xuening Zhu", "title": "Community Network Auto-Regression for High-Dimensional Time Series", "comments": "59 pages; 10 figures; submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling responses on the nodes of a large-scale network is an important task\nthat arises commonly in practice. This paper proposes a community network\nvector autoregressive (CNAR) model, which utilizes the network structure to\ncharacterize the dependence and intra-community homogeneity of the high\ndimensional time series. The CNAR model greatly increases the flexibility and\ngenerality of the network vector autoregressive (Zhu et al, 2017, NAR) model by\nallowing heterogeneous network effects across different network communities. In\naddition, the non-community-related latent factors are included to account for\nunknown cross-sectional dependence. The number of network communities can\ndiverge as the network expands, which leads to estimating a diverging number of\nmodel parameters. We obtain a set of stationary conditions and develop an\nefficient two-step weighted least-squares estimator. The consistency and\nasymptotic normality properties of the estimators are established. The\ntheoretical results show that the two-step estimator improves the one-step\nestimator by an order of magnitude when the error admits a factor structure.\nThe advantages of the CNAR model are further illustrated on a variety of\nsynthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 17:56:11 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Chen", "Elynn Y.", ""], ["Fan", "Jianqing", ""], ["Zhu", "Xuening", ""]]}, {"id": "2007.05748", "submitter": "Christian Hennig", "authors": "Christian Hennig", "title": "Frequentism-as-model", "comments": "34 pages no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most statisticians are aware that probability models interpreted in a\nfrequentist manner are not really true in objective reality, but only\nidealisations. I argue that this is often ignored when actually applying\nfrequentist methods and interpreting the results, and that keeping up the\nawareness for the essential difference between reality and models can lead to a\nmore appropriate use and interpretation of frequentist models and methods,\ncalled frequentism-as-model. This is elaborated showing connections to existing\nwork, appreciating the special role of i.i.d. models and subject matter\nknowledge, giving an account of how and under what conditions models that are\nnot true can be useful, giving detailed interpretations of tests and confidence\nintervals, confronting their implicit compatibility logic with the inverse\nprobability logic of Bayesian inference, re-interpreting the role of model\nassumptions, appreciating robustness, and the role of ``interpretative\nequivalence'' of models. Epistemic (often referred to as Bayesian) probability\nshares the issue that its models are only idealisations and not really true for\nmodelling reasoning about uncertainty, meaning that it does not have an\nessential advantage over frequentism, as is often claimed. Bayesian statistics\ncan be combined with frequentism-as-model, leading to what Gelman and Hennig\n(2017) call ``falsificationist Bayes''.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 11:22:16 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Hennig", "Christian", ""]]}, {"id": "2007.05812", "submitter": "Fl\\'avio Gon\\c{c}alves", "authors": "Flavio B. Gon\\c{c}alves, Krzysztof G. {\\L}atuszy\\'nski, Gareth O.\n  Roberts", "title": "Exact Bayesian inference for diffusion driven Cox processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel methodology to perform Bayesian inference\nfor Cox processes in which the intensity function is driven by a diffusion\nprocess. The novelty lies on the fact that no discretisation error is involved,\ndespite the non-tractability of both the likelihood function and the transition\ndensity of the diffusion. The methodology is based on an MCMC algorithm and its\nexactness is built on retrospective sampling techniques. The efficiency of the\nmethodology is investigated in some simulated examples and its applicability is\nillustrated in some real data analyses.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 16:41:28 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Gon\u00e7alves", "Flavio B.", ""], ["\u0141atuszy\u0144ski", "Krzysztof G.", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "2007.05857", "submitter": "Lourens  Waldorp", "authors": "Lourens Waldorp and Maarten Marsman and Denny Borsboom", "title": "Reliability of decisions based on tests: Fourier analysis of Boolean\n  decision functions", "comments": "41 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Items in a test are often used as a basis for making decisions and such tests\nare therefore required to have good psychometric properties, like\nunidimensionality. In many cases the sum score is used in combination with a\nthreshold to decide between pass or fail, for instance. Here we consider\nwhether such a decision function is appropriate, without a latent variable\nmodel, and which properties of a decision function are desirable. We consider\nreliability (stability) of the decision function, i.e., does the decision\nchange upon perturbations, or changes in a fraction of the outcomes of the\nitems (measurement error). We are concerned with questions of whether the sum\nscore is the best way to aggregate the items, and if so why. We use ideas from\ntest theory, social choice theory, graphical models, computer science and\nprobability theory to answer these questions. We conclude that a weighted sum\nscore has desirable properties that (i) fit with test theory and is observable\n(similar to a condition like conditional association), (ii) has the property\nthat a decision is stable (reliable), and (iii) satisfies Rousseau's criterion\nthat the input should match the decision. We use Fourier analysis of Boolean\nfunctions to investigate whether a decision function is stable and to figure\nout which (set of) items has proportionally too large an influence on the\ndecision. To apply these techniques we invoke ideas from graphical models and\nuse a pseudo-likelihood factorisation of the probability distribution.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 21:24:48 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Waldorp", "Lourens", ""], ["Marsman", "Maarten", ""], ["Borsboom", "Denny", ""]]}, {"id": "2007.05974", "submitter": "Saswati Saha", "authors": "Saswati Saha and Werner Brannath", "title": "Point and interval estimation of the target dose using weighted\n  regression modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a Phase II dose-finding study with a placebo control, a new drug with\nseveral dose levels is compared with a placebo to test for the effectiveness of\nthe new drug. The main focus of such studies often lies in the characterization\nof the dose-response relationship followed by the estimation of a target dose\nthat leads to a clinically relevant effect over the placebo. This target dose\nis known as the minimum effective dose (MED) in a drug development study.\nSeveral approaches exist that combine multiple comparison procedures with\nmodeling techniques to efficiently estimate the dose-response model and\nthereafter select the target dose. Despite the flexibility of the existing\napproaches, they cannot completely address the model uncertainty in the\nmodel-selection step and may lead to target dose estimates that are biased. In\nthis article, we propose two new MED estimation approaches based on weighted\nregression modeling that are robust against deviations from the dose-response\nmodel assumptions. These approaches are compared with existing approaches with\nregard to their accuracy in point and interval estimation of the MED. We\nillustrate by a simulation study that by integrating one of the new dose\nestimation approaches with the existing dose-response profile estimation\napproaches one can take into account the uncertainty of the model selection\nstep.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 12:31:07 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Saha", "Saswati", ""], ["Brannath", "Werner", ""]]}, {"id": "2007.06038", "submitter": "Yannis Yatracos", "authors": "Yannis G. Yatracos", "title": "Fiducial Matching for the Approximate Posterior: F-ABC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  F-ABC is introduced, using universal sufficient statistics, unlike previous\nABC papers, e.g. Bernton et al. (2019), and avoiding in the approximate\nposterior artifacts due to a Kernel. The nature of matching tolerance is\nexamined and indications for determining its values are presented. F-ABC does\nnot face concerns associated with ABC. Asymptotics and simulation results are\nalso presented.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 17:01:08 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Yatracos", "Yannis G.", ""]]}, {"id": "2007.06054", "submitter": "Vanda In\\'acio", "authors": "Vanda Inacio, Vanda M. Lourenco, Miguel de Carvalho, Richard A.\n  Parker, Vincent Gnanapragasam", "title": "Robust and flexible inference for the covariate-specific ROC curve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic tests are of critical importance in health care and medical\nresearch. Motivated by the impact that atypical and outlying test outcomes\nmight have on the assessment of the discriminatory ability of a diagnostic\ntest, we develop a flexible and robust model for conducting inference about the\ncovariate-specific receiver operating characteristic (ROC) curve that\nsafeguards against outlying test results while also accommodating for possible\nnonlinear effects of the covariates. Specifically, we postulate a\nlocation-scale additive regression model for the test outcomes in both the\ndiseased and nondiseased populations, combining additive cubic B-splines and\nM-estimation for the regression function, while the residuals are estimated via\na weighted empirical distribution function. The results of the simulation study\nshow that our approach successfully recovers the true covariate-specific ROC\ncurve and corresponding area under the curve on a variety of conceivable test\noutcomes contamination scenarios. Our method is applied to a dataset derived\nfrom a prostate cancer study where we seek to assess the ability of the\nProstate Health Index to discriminate between men with and without Gleason 7 or\nabove prostate cancer, and if and how such discriminatory capacity changes with\nage.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 18:14:53 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 17:23:59 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 15:36:09 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Inacio", "Vanda", ""], ["Lourenco", "Vanda M.", ""], ["de Carvalho", "Miguel", ""], ["Parker", "Richard A.", ""], ["Gnanapragasam", "Vincent", ""]]}, {"id": "2007.06076", "submitter": "Rakheon Kim", "authors": "Rakheon Kim, Samuel Mueller and Tanya P. Garcia", "title": "svReg: Structural Varying-coefficient regression to differentiate how\n  regional brain atrophy affects motor impairment for Huntington disease\n  severity groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Huntington disease, identification of brain regions related to motor\nimpairment can be useful for developing interventions to alleviate the motor\nsymptom, the major symptom of the disease. However, the effects from the brain\nregions to motor impairment may vary for different groups of patients. Hence,\nour interest is not only to identify the brain regions but also to understand\nhow their effects on motor impairment differ by patient groups. This can be\ncast as a model selection problem for a varying-coefficient regression.\nHowever, this is challenging when there is a pre-specified group structure\namong variables. We propose a novel variable selection method for a\nvarying-coefficient regression with such structured variables. Our method is\nempirically shown to select relevant variables consistently. Also, our method\nscreens irrelevant variables better than existing methods. Hence, our method\nleads to a model with higher sensitivity, lower false discovery rate and higher\nprediction accuracy than the existing methods. Finally, we found that the\neffects from the brain regions to motor impairment differ by disease severity\nof the patients. To the best of our knowledge, our study is the first to\nidentify such interaction effects between the disease severity and brain\nregions, which indicates the need for customized intervention by disease\nseverity.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 19:51:10 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Kim", "Rakheon", ""], ["Mueller", "Samuel", ""], ["Garcia", "Tanya P.", ""]]}, {"id": "2007.06114", "submitter": "Luca Insolia", "authors": "Luca Insolia, Ana Kenney, Francesca Chiaromonte, and Giovanni Felici", "title": "Simultaneous Feature Selection and Outlier Detection with Optimality\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse estimation methods capable of tolerating outliers have been broadly\ninvestigated in the last decade. We contribute to this research considering\nhigh-dimensional regression problems contaminated by multiple mean-shift\noutliers which affect both the response and the design matrix. We develop a\ngeneral framework for this class of problems and propose the use of\nmixed-integer programming to simultaneously perform feature selection and\noutlier detection with provably optimal guarantees. We characterize the\ntheoretical properties of our approach, i.e. a necessary and sufficient\ncondition for the robustly strong oracle property, which allows the number of\nfeatures to exponentially increase with the sample size; the optimal estimation\nof the parameters; and the breakdown point of the resulting estimates.\nMoreover, we provide computationally efficient procedures to tune integer\nconstraints and to warm-start the algorithm. We show the superior performance\nof our proposal compared to existing heuristic methods through numerical\nsimulations and an application investigating the relationships between the\nhuman microbiome and childhood obesity.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 22:26:01 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Insolia", "Luca", ""], ["Kenney", "Ana", ""], ["Chiaromonte", "Francesca", ""], ["Felici", "Giovanni", ""]]}, {"id": "2007.06129", "submitter": "Peter Mueller", "authors": "Fernand A. Quintana and Peter Mueller and Alejandro Jara and Steven N.\n  MacEachern", "title": "The Dependent Dirichlet Process and Related Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard regression approaches assume that some finite number of the response\ndistribution characteristics, such as location and scale, change as a\n(parametric or nonparametric) function of predictors. However, it is not always\nappropriate to assume a location/scale representation, where the error\ndistribution has unchanging shape over the predictor space. In fact, it often\nhappens in applied research that the distribution of responses under study\nchanges with predictors in ways that cannot be reasonably represented by a\nfinite dimensional functional form. This can seriously affect the answers to\nthe scientific questions of interest, and therefore more general approaches are\nindeed needed. This gives rise to the study of fully nonparametric regression\nmodels. We review some of the main Bayesian approaches that have been employed\nto define probability models where the complete response distribution may vary\nflexibly with predictors. We focus on developments based on modifications of\nthe Dirichlet process, historically termed dependent Dirichlet processes, and\nsome of the extensions that have been proposed to tackle this general problem\nusing nonparametric approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 23:22:26 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Quintana", "Fernand A.", ""], ["Mueller", "Peter", ""], ["Jara", "Alejandro", ""], ["MacEachern", "Steven N.", ""]]}, {"id": "2007.06154", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Alain Desgagn\\'e and Pierre Lafaye de Micheaux and Fr\\'ed\\'eric Ouimet", "title": "A comprehensive empirical power comparison of univariate goodness-of-fit\n  tests for the Laplace distribution", "comments": "32 pages, 2 figures, 22 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we survey the 40 univariate Laplace goodness-of-fit tests\ncurrently available in the literature. We provide a brief description of each\ntest, and we include an expression of their test statistic. An empirical power\ncomparison of the 40 tests is carried out using Monte Carlo simulations, with\nthe sample sizes $n = 20, 50, 100, 200$, the significance levels $\\alpha =\n0.01, 0.05, 0.10$, and 400 alternatives comprising a variety of asymmetric and\nsymmetric light/heavy-tailed distributions. Aside from the unmatched size of\nour study, the main contribution is the proposal of an innovative design for\nthe selection of alternatives. The 400 alternatives are evenly divided into 20\nclasses, each of them corresponding to 20 alternatives of a specific family of\ndistributions with the appropriate parameter chosen to cover the whole range of\npower curves. An analysis of the results leads to a recommendation of the best\ntests for different types of alternatives. A real-data example is also\npresented, where the Laplace tests are applied to the weekly log returns of the\nAMZN stock over a recent five-year period. Complete tables for the empirical\npower results are provided in the appendices.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 02:06:50 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 22:31:30 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Desgagn\u00e9", "Alain", ""], ["de Micheaux", "Pierre Lafaye", ""], ["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2007.06160", "submitter": "Shuaimin Kang", "authors": "Shuaimin Kang, Krista Gile and Megan Price", "title": "Nested Dirichlet Process For Population Size Estimation From Multi-list\n  Recapture Data", "comments": "24 pages, 9 figures, submitted to Biometrics for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Heterogeneity of response patterns is important in estimating the size of a\nclosed population from multiple recapture data when capture patterns are\ndifferent over time and location. In this paper, we extend the non-parametric\none layer latent class model for multiple recapture data proposed by\nManrique-Vallier (2016) to a nested latent class model with the first layer\nmodeling individual heterogeneity and the second layer modeling location-time\ndifferences. Location-time groups with similar recording patterns are in the\nsame top layer latent class and individuals within each top layer class are\ndependent. The nested latent class model incorporates hierarchical\nheterogeneity into the modeling to estimate population size from multi-list\nrecapture data. This approach leads to more accurate population size estimation\nand reduced uncertainty. We apply the method to estimating casualties from the\nSyrian conflict.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 02:52:19 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Kang", "Shuaimin", ""], ["Gile", "Krista", ""], ["Price", "Megan", ""]]}, {"id": "2007.06169", "submitter": "Tetsuya Kaji", "authors": "Tetsuya Kaji, Elena Manresa, Guillaume Pouliot", "title": "An Adversarial Approach to Structural Estimation", "comments": "58 pages, 3 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new simulation-based estimation method, adversarial estimation,\nfor structural models. The estimator is formulated as the solution to a minimax\nproblem between a generator (which generates synthetic observations using the\nstructural model) and a discriminator (which classifies if an observation is\nsynthetic). The discriminator maximizes the accuracy of its classification\nwhile the generator minimizes it. We show that, with a sufficiently rich\ndiscriminator, the adversarial estimator attains parametric efficiency under\ncorrect specification and the parametric rate under misspecification. We\nadvocate the use of a neural network as a discriminator that can exploit\nadaptivity properties and attain fast rates of convergence. We apply our method\nto the elderly's saving decision model and show that including gender and\nhealth profiles in the discriminator uncovers the bequest motive as an\nimportant source of saving across the wealth distribution, not only for the\nrich.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 03:31:02 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Kaji", "Tetsuya", ""], ["Manresa", "Elena", ""], ["Pouliot", "Guillaume", ""]]}, {"id": "2007.06298", "submitter": "Mehdi Dagdoug", "authors": "Mehdi Dagdoug, Camelia Goga and David Haziza", "title": "Imputation procedures in surveys using nonparametric and machine\n  learning methods: an empirical comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric and machine learning methods are flexible methods for obtaining\naccurate predictions. Nowadays, data sets with a large number of predictors and\ncomplex structures are fairly common. In the presence of item nonresponse,\nnonparametric and machine learning procedures may thus provide a useful\nalternative to traditional imputation procedures for deriving a set of imputed\nvalues. In this paper, we conduct an extensive empirical investigation that\ncompares a number of imputation procedures in terms of bias and efficiency in a\nwide variety of settings, including high-dimensional data sets. The results\nsuggest that a number of machine learning procedures perform very well in terms\nof bias and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:31:24 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 09:29:52 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Dagdoug", "Mehdi", ""], ["Goga", "Camelia", ""], ["Haziza", "David", ""]]}, {"id": "2007.06357", "submitter": "Phillip Murray", "authors": "Phillip Murray, Riccardo Passeggeri, Almut E.D. Veraart and Mikko S.\n  Pakkanen", "title": "Feasible Inference for Stochastic Volatility in Brownian Semistationary\n  Processes", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the finite sample behaviour of a number of estimators\nfor the integrated power volatility process of a Brownian semistationary\nprocess in the non semi-martingale setting. We establish three consistent\nfeasible estimators for the integrated volatility, two derived from parametric\nmethods and one non-parametrically. We then use a simulation study to compare\nthe convergence properties of the estimators to one another, and to a benchmark\nof an infeasible estimator. We further establish bounds for the asymptotic\nvariance of the infeasible estimator and assess whether a central limit theorem\nwhich holds for the infeasible estimator can be translated into a feasible\nlimit theorem for the non-parametric estimator.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 13:06:42 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 14:00:58 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Murray", "Phillip", ""], ["Passeggeri", "Riccardo", ""], ["Veraart", "Almut E. D.", ""], ["Pakkanen", "Mikko S.", ""]]}, {"id": "2007.06476", "submitter": "Daniel Iong", "authors": "Daniel Iong, Qingyuan Zhao, Yang Chen", "title": "A Latent Mixture Model for Heterogeneous Causal Mechanisms in Mendelian\n  Randomization", "comments": "38 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian Randomization (MR) is a popular method in epidemiology and genetics\nthat uses genetic variation as instrumental variables for causal inference.\nExisting MR methods usually assume most genetic variants are valid instrumental\nvariables that identify a common causal effect. There is a general lack of\nawareness that this effect homogeneity assumption can be violated when there\nare multiple causal pathways involved, even if all the instrumental variables\nare valid. In this article, we introduce a latent mixture model MR-PATH that\ngroups instruments that yield similar causal effect estimates together. We\ndevelop a Monte-Carlo EM algorithm to fit this mixture model, derive\napproximate confidence intervals for uncertainty quantification, and adopt a\nmodified Bayesian Information Criterion (BIC) for model selection. We verify\nthe efficacy of the Monte-Carlo EM algorithm, confidence intervals, and model\nselection criterion using numerical simulations. We identify potential\nmechanistic heterogeneity when applying our method to estimate the effect of\nhigh-density lipoprotein cholesterol on coronary heart disease and the effect\nof adiposity on type II diabetes.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 16:14:54 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 14:07:55 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Iong", "Daniel", ""], ["Zhao", "Qingyuan", ""], ["Chen", "Yang", ""]]}, {"id": "2007.06541", "submitter": "Ben Boukai", "authors": "Ben Boukai and Jiayue Wang", "title": "Bayesian Modeling of COVID-19 Positivity Rate -- the Indiana experience", "comments": "13 pages, 7 figures and 2 tables. The numerical results provided were\n  obtained via an updatable R Markdown document", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short technical report we model, within the Bayesian framework, the\nrate of positive tests reported by the the State of Indiana, accounting also\nfor the substantial variability (and overdispeartion) in the daily count of the\ntests performed. The approach we take, results with a simple procedure for\nprediction, a posteriori, of this rate of 'positivity' and allows for an easy\nand a straightforward adaptation by any agency tracking daily results of\nCOVID-19 tests. The numerical results provided herein were obtained via an\nupdatable R Markdown document.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 11:18:01 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Boukai", "Ben", ""], ["Wang", "Jiayue", ""]]}, {"id": "2007.06602", "submitter": "Peter Ashcroft", "authors": "Peter Ashcroft, Jana S. Huisman, Sonja Lehtinen, Judith A. Bouman,\n  Christian L. Althaus, Roland R. Regoes, Sebastian Bonhoeffer", "title": "COVID-19 infectivity profile correction", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The infectivity profile of an individual with COVID-19 is attributed to the\npaper Temporal dynamics in viral shedding and transmissibility of COVID-19 by\nHe et al., published in Nature Medicine in April 2020. However, the analysis\nwithin this paper contains a mistake such that the published infectivity\nprofile is incorrect and the conclusion that infectiousness begins 2.3 days\nbefore symptom onset is no longer supported. In this document we discuss the\nerror and compute the correct infectivity profile. We also establish confidence\nintervals on this profile, quantify the difference between the published and\nthe corrected profiles, and discuss an issue of normalisation when fitting\nserial interval data. This infectivity profile plays a central role in policy\nand decision making, thus it is crucial that this issue is corrected with the\nutmost urgency to prevent the propagation of this error into further studies\nand policies. We hope that this preprint will reach all researchers and policy\nmakers who are using the incorrect infectivity profile to inform their work.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 18:08:03 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ashcroft", "Peter", ""], ["Huisman", "Jana S.", ""], ["Lehtinen", "Sonja", ""], ["Bouman", "Judith A.", ""], ["Althaus", "Christian L.", ""], ["Regoes", "Roland R.", ""], ["Bonhoeffer", "Sebastian", ""]]}, {"id": "2007.06635", "submitter": "Mehrdad Naderi Dr", "authors": "Elham Mirfarah and Mehrdad Naderi and Ding-Geng Chen", "title": "Mixture of linear experts model for censored data: A novel approach with\n  scale-mixture of normal distributions", "comments": "21 pages,", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical mixture of linear experts (MoE) model is one of the widespread\nstatistical frameworks for modeling, classification, and clustering of data.\nBuilt on the normality assumption of the error terms for mathematical and\ncomputational convenience, the classical MoE model has two challenges: 1) it is\nsensitive to atypical observations and outliers, and 2) it might produce\nmisleading inferential results for censored data. The paper is then aimed to\nresolve these two challenges, simultaneously, by proposing a novel robust MoE\nmodel for model-based clustering and discriminant censored data with the\nscale-mixture of normal class of distributions for the unobserved error terms.\nBased on this novel model, we develop an analytical expectation-maximization\n(EM) type algorithm to obtain the maximum likelihood parameter estimates.\nSimulation studies are carried out to examine the performance, effectiveness,\nand robustness of the proposed methodology. Finally, real data is used to\nillustrate the superiority of the new model.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 19:15:39 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Mirfarah", "Elham", ""], ["Naderi", "Mehrdad", ""], ["Chen", "Ding-Geng", ""]]}, {"id": "2007.06772", "submitter": "Bo Zhang", "authors": "Bo Zhang, Siyu Heng, Emily J. MacKay, Ting Ye", "title": "Bridging preference-based instrumental variable studies and\n  cluster-randomized encouragement experiments: study design, noncompliance,\n  and average cluster effect ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable methods are widely used in medical and social science\nresearch to draw causal conclusions when the treatment and outcome are\nconfounded by unmeasured confounding variables. One important feature of such\nstudies is that the instrumental variable is often applied at the cluster\nlevel, e.g., hospitals' or physicians' preference for a certain treatment where\neach hospital or physician naturally defines a cluster. This paper proposes to\nembed such observational instrumental variable data into a cluster-randomized\nencouragement experiment using statistical matching. Potential outcomes and\ncausal assumptions underpinning the design are formalized and examined. Testing\nprocedures for two commonly-used estimands, Fisher's sharp null hypothesis and\nthe pooled effect ratio, are extended to the current setting. We then introduce\na novel cluster-heterogeneous proportional treatment effect model and the\nrelevant estimand: the average cluster effect ratio. This new estimand is\nadvantageous over the structural parameter in a constant proportional treatment\neffect model in that it allows treatment heterogeneity, and is advantageous\nover the pooled effect ratio estimand in that it is immune to Simpson's\nparadox. We develop an asymptotically valid randomization-based testing\nprocedure for this new estimand based on solving a mixed integer\nquadratically-constrained optimization problem. The proposed design and\ninferential methods are applied to a study of the effect of using\ntransesophageal echocardiography during CABG surgery on patients' 30-day\nmortality rate.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 02:13:12 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 21:59:42 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhang", "Bo", ""], ["Heng", "Siyu", ""], ["MacKay", "Emily J.", ""], ["Ye", "Ting", ""]]}, {"id": "2007.06856", "submitter": "Alessandra Menafoglio", "authors": "Federico Gatti, Alessandra Menafoglio, Niccol\\`o Togni, Luca\n  Bonaventura, Davide Brambilla, Monica Papini, Laura Longoni", "title": "A novel dowscaling procedure for compositional data in the Aitchison\n  geometry with application to soil texture data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel downscaling procedure for compositional\nquantities based on the Aitchison geometry. The method is able to naturally\nconsider compositional constraints, i.e. unit-sum and positivity. We show that\nthe method can be used in a block sequential Gaussian simulation framework in\norder to assess the variability of downscaled quantities. Finally, to validate\nthe method, we test it first in an idealized scenario and then apply it for the\ndownscaling of digital soil maps on a more realistic case study. The digital\nsoil maps for the realistic case study are obtained from SoilGrids, a system\nfor automated soil mapping based on state-of-the-art spatial predictions\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 07:05:24 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Gatti", "Federico", ""], ["Menafoglio", "Alessandra", ""], ["Togni", "Niccol\u00f2", ""], ["Bonaventura", "Luca", ""], ["Brambilla", "Davide", ""], ["Papini", "Monica", ""], ["Longoni", "Laura", ""]]}, {"id": "2007.06944", "submitter": "Augusto Fasano", "authors": "Augusto Fasano and Daniele Durante", "title": "A Class of Conjugate Priors for Multinomial Probit Models which Includes\n  the Multivariate Normal One", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multinomial probit models are widely-implemented representations which allow\nboth classification and inference by learning changes in vectors of class\nprobabilities with a set of p observed predictors. Although various frequentist\nmethods have been developed for estimation, inference and classification within\nsuch a class of models, Bayesian inference is still lagging behind. This is due\nto the apparent absence of a tractable class of conjugate priors, that may\nfacilitate posterior inference on the multinomial probit coefficients. Such an\nissue has motivated increasing efforts toward the development of effective\nMarkov chain Monte Carlo methods, but state-of-the-art solutions still face\nsevere computational bottlenecks, especially in large p settings. In this\narticle, we prove that the entire class of unified skew-normal (SUN)\ndistributions is conjugate to a wide variety of multinomial probit models, and\nwe exploit the SUN properties to improve upon state-of-art-solutions for\nposterior inference and classification both in terms of closed-form results for\nkey functionals of interest, and also by developing novel computational methods\nrelying either on independent and identically distributed samples from the\nexact posterior or on scalable and accurate variational approximations based on\nblocked partially-factorized representations. As illustrated in a\ngastrointestinal lesions application, the magnitude of the improvements\nrelative to current methods is particularly evident, in practice, when the\nfocus is on large p applications.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 10:08:23 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Fasano", "Augusto", ""], ["Durante", "Daniele", ""]]}, {"id": "2007.07021", "submitter": "Ray Bai", "authors": "Ray Bai", "title": "Spike-and-Slab Group Lasso for Consistent Estimation and Variable\n  Selection in Non-Gaussian Generalized Additive Models", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study estimation and variable selection in non-Gaussian Bayesian\ngeneralized additive models (GAMs) under a spike-and-slab prior for grouped\nvariables. Our framework subsumes GAMs for logistic regression, Poisson\nregression, negative binomial regression, and gamma regression, and encompasses\nboth canonical and non-canonical link functions. Under mild conditions, we\nestablish posterior contraction rates and model selection consistency when $p\n\\gg n$. For computation, we propose an EM algorithm for obtaining MAP estimates\nin our model, which is available in the R package sparseGAM. We illustrate our\nmethod on both synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 13:29:13 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 19:57:31 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 06:13:38 GMT"}, {"version": "v4", "created": "Mon, 21 Sep 2020 16:57:47 GMT"}, {"version": "v5", "created": "Sat, 5 Jun 2021 15:22:40 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bai", "Ray", ""]]}, {"id": "2007.07052", "submitter": "KongFatt Wong-Lin", "authors": "Niamh McCombe, Xuemei Ding, Girijesh Prasad, David P. Finn, Stephen\n  Todd, Paula L. McClean, KongFatt Wong-Lin", "title": "Predicting feature imputability in the absence of ground truth", "comments": "5 pages, 3 figures, 1 table. In: Proceedings of the 37th\n  International Conference on Machine Learning (ICML), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data imputation is the most popular method of dealing with missing values,\nbut in most real life applications, large missing data can occur and it is\ndifficult or impossible to evaluate whether data has been imputed accurately\n(lack of ground truth). This paper addresses these issues by proposing an\neffective and simple principal component based method for determining whether\nindividual data features can be accurately imputed - feature imputability. In\nparticular, we establish a strong linear relationship between principal\ncomponent loadings and feature imputability, even in the presence of extreme\nmissingness and lack of ground truth. This work will have important\nimplications in practical data imputation strategies.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:24:07 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["McCombe", "Niamh", ""], ["Ding", "Xuemei", ""], ["Prasad", "Girijesh", ""], ["Finn", "David P.", ""], ["Todd", "Stephen", ""], ["McClean", "Paula L.", ""], ["Wong-Lin", "KongFatt", ""]]}, {"id": "2007.07127", "submitter": "Sam Witty", "authors": "Sam Witty, Kenta Takatsu, David Jensen, Vikash Mansinghka", "title": "Causal Inference using Gaussian Processes with Structured Latent\n  Confounders", "comments": "to be published at ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Latent confounders---unobserved variables that influence both treatment and\noutcome---can bias estimates of causal effects. In some cases, these\nconfounders are shared across observations, e.g. all students taking a course\nare influenced by the course's difficulty in addition to any educational\ninterventions they receive individually. This paper shows how to\nsemiparametrically model latent confounders that have this structure and\nthereby improve estimates of causal effects. The key innovations are a\nhierarchical Bayesian model, Gaussian processes with structured latent\nconfounders (GP-SLC), and a Monte Carlo inference algorithm for this model\nbased on elliptical slice sampling. GP-SLC provides principled Bayesian\nuncertainty estimates of individual treatment effect with minimal assumptions\nabout the functional forms relating confounders, covariates, treatment, and\noutcome. Finally, this paper shows GP-SLC is competitive with or more accurate\nthan widely used causal inference techniques on three benchmark datasets,\nincluding the Infant Health and Development Program and a dataset showing the\neffect of changing temperatures on state-wide energy consumption across New\nEngland.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 15:45:28 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Witty", "Sam", ""], ["Takatsu", "Kenta", ""], ["Jensen", "David", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "2007.07253", "submitter": "Elena Sellentin", "authors": "Amy J. Louca, Elena Sellentin", "title": "The impact of signal-to-noise, redshift, and angular range on the bias\n  of weak lensing 2-point functions", "comments": "Accepted by the Open Journal of Astrophysics", "journal-ref": null, "doi": "10.21105/astro.2007.07253", "report-no": null, "categories": "astro-ph.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weak lensing data follow a naturally skewed distribution, implying the data\nvector most likely yielded from a survey will systematically fall below its\nmean. Although this effect is qualitatively known from CMB-analyses, correctly\naccounting for it in weak lensing is challenging, as a direct transfer of the\nCMB results is quantitatively incorrect. While a previous study (Sellentin et\nal. 2018) focused on the magnitude of this bias, we here focus on the frequency\nof this bias, its scaling with redshift, and its impact on the signal-to-noise\nof a survey. Filtering weak lensing data with COSEBIs, we show that weak\nlensing likelihoods are skewed up until $\\ell \\approx 100$, whereas\nCMB-likelihoods Gaussianize already at $\\ell \\approx 20$. While\nCOSEBI-compressed data on KiDS- and DES-like redshift- and angular ranges\nfollow Gaussian distributions, we detect skewness at 6$\\sigma$ significance for\nhalf of a Euclid- or LSST-like data set, caused by the wider coverage and\ndeeper reach of these surveys. Computing the signal-to-noise ratio per data\npoint, we show that precisely the data points of highest signal-to-noise are\nthe most biased. Over all redshifts, this bias affects at least 10% of a\nsurvey's total signal-to-noise, at high redshifts up to 25%. The bias is\naccordingly expected to impact parameter inference. The bias can be handled by\ndeveloping non-Gaussian likelihoods. Otherwise, it could be reduced by removing\nthe data points of highest signal-to-noise.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 18:00:00 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 12:09:18 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Louca", "Amy J.", ""], ["Sellentin", "Elena", ""]]}, {"id": "2007.07383", "submitter": "Georg Gottwald A.", "authors": "Georg A. Gottwald and Sebastian Reich", "title": "Supervised learning from noisy observations: Combining machine-learning\n  techniques with data assimilation", "comments": null, "journal-ref": null, "doi": "10.1016/j.physd.2021.132911", "report-no": null, "categories": "physics.data-an cs.LG physics.comp-ph stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven prediction and physics-agnostic machine-learning methods have\nattracted increased interest in recent years achieving forecast horizons going\nwell beyond those to be expected for chaotic dynamical systems. In a separate\nstrand of research data-assimilation has been successfully used to optimally\ncombine forecast models and their inherent uncertainty with incoming noisy\nobservations. The key idea in our work here is to achieve increased forecast\ncapabilities by judiciously combining machine-learning algorithms and data\nassimilation. We combine the physics-agnostic data-driven approach of random\nfeature maps as a forecast model within an ensemble Kalman filter data\nassimilation procedure. The machine-learning model is learned sequentially by\nincorporating incoming noisy observations. We show that the obtained forecast\nmodel has remarkably good forecast skill while being computationally cheap once\ntrained. Going beyond the task of forecasting, we show that our method can be\nused to generate reliable ensembles for probabilistic forecasting as well as to\nlearn effective model closure in multi-scale systems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 22:29:37 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 07:44:28 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 11:45:38 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Gottwald", "Georg A.", ""], ["Reich", "Sebastian", ""]]}, {"id": "2007.07426", "submitter": "Daniel Andr\\'es D\\'iaz-Pach\\'on", "authors": "Daniel Andr\\'es D\\'iaz-Pach\\'on and J Sunil Rao", "title": "A simple correction for COVID-19 sampling bias", "comments": "14 pages. Title changed. The whole Section 7 with information from\n  Lombardy, Italy, was added (another real dataset). Some typos were corrected.\n  In spite of several lengthy additions, no substantial changes were done to\n  the paper. The goal of the additions was more to clarify than to correct", "journal-ref": "Journal of Theoretical Biology Journal of Theoretical Biology,\n  Volume 512, 7 March 2021, 110556", "doi": "10.1016/j.jtbi.2020.110556", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  COVID-19 testing has become a standard approach for estimating prevalence\nwhich then assist in public health decision making to contain and mitigate the\nspread of the disease. The sampling designs used are often biased in that they\ndo not reflect the true underlying populations. For instance, individuals with\nstrong symptoms are more likely to be tested than those with no symptoms. This\nresults in biased estimates of prevalence (too high). Typical post-sampling\ncorrections are not always possible. Here we present a simple bias correction\nmethodology derived and adapted from a correction for publication bias in meta\nanalysis studies. The methodology is general enough to allow a wide variety of\ncustomization making it more useful in practice. Implementation is easily done\nusing already collected information. Via a simulation and two real datasets, we\nshow that the bias corrections can provide dramatic reductions in estimation\nerror.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 01:32:39 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 01:29:08 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 04:58:26 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["D\u00edaz-Pach\u00f3n", "Daniel Andr\u00e9s", ""], ["Rao", "J Sunil", ""]]}, {"id": "2007.07447", "submitter": "Mason A. Porter", "authors": "Alice C. Schwarze and Mason A. Porter", "title": "Motifs for processes on networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI math.DS nlin.AO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of motifs in networks can help researchers uncover links between\nthe structure and function of networks in biology, sociology, economics, and\nmany other areas. Empirical studies of networks have identified feedback loops,\nfeedforward loops, and several other small structures as \"motifs\" that occur\nfrequently in real-world networks and may contribute by various mechanisms to\nimportant functions in these systems. However, these mechanisms are unknown for\nmany of these motifs. We propose to distinguish between \"structure motifs\"\n(i.e., graphlets) in networks and \"process motifs\" (which we define as\nstructured sets of walks) on networks and consider process motifs as building\nblocks of processes on networks. Using the steady-state covariances and\nsteady-state correlations in a multivariate Ornstein--Uhlenbeck process on a\nnetwork as examples, we demonstrate that the distinction between structure\nmotifs and process motifs makes it possible to gain quantitative insights into\nmechanisms that contribute to important functions of dynamical systems on\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 02:44:48 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 23:10:13 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 04:23:20 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Schwarze", "Alice C.", ""], ["Porter", "Mason A.", ""]]}, {"id": "2007.07498", "submitter": "Zheng Tracy Ke", "authors": "Zhirui Hu, Zheng Tracy Ke, Jun S Liu", "title": "Measurement error models: from nonparametric methods to deep neural\n  networks", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning has inspired recent interests in applying neural\nnetworks in statistical inference. In this paper, we investigate the use of\ndeep neural networks for nonparametric regression with measurement errors. We\npropose an efficient neural network design for estimating measurement error\nmodels, in which we use a fully connected feed-forward neural network (FNN) to\napproximate the regression function $f(x)$, a normalizing flow to approximate\nthe prior distribution of $X$, and an inference network to approximate the\nposterior distribution of $X$. Our method utilizes recent advances in\nvariational inference for deep neural networks, such as the importance weight\nautoencoder, doubly reparametrized gradient estimator, and non-linear\nindependent components estimation. We conduct an extensive numerical study to\ncompare the neural network approach with classical nonparametric methods and\nobserve that the neural network approach is more flexible in accommodating\ndifferent classes of regression functions and performs superior or comparable\nto the best available method in nearly all settings.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 06:05:37 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Hu", "Zhirui", ""], ["Ke", "Zheng Tracy", ""], ["Liu", "Jun S", ""]]}, {"id": "2007.07635", "submitter": "Marie-Colette van Lieshout", "authors": "M.C. de Jongh, M.N.M. van Lieshout", "title": "Testing biodiversity using inhomogeneous summary statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  McGill's theory of biodiversity is based upon three axioms: individuals of\nthe same species cluster together, many rare species co-exist with a few common\nones and individuals of different species grow independently of each other.\nOver the past decade, classical point pattern analyses have been employed to\nverify these axioms based on the false assumption of stationarity. In this\npaper, we use inhomogeneous versions of the classical summary statistics for\nspatial point patterns to assess the validity of McGill's first and third\naxioms for data obtained from a 50 hectare plot on Barro Colorado Island.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:47:58 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["de Jongh", "M. C.", ""], ["van Lieshout", "M. N. M.", ""]]}, {"id": "2007.07687", "submitter": "Vanda In\\'acio", "authors": "Vanda Inacio, Maria Xose Rodriguez-Alvarez, Pilar Gayoso-Diz", "title": "Statistical Evaluation of Medical Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this review, we present an overview of the main aspects related to the\nstatistical evaluation of medical tests for diagnosis and prognosis. Measures\nof diagnostic performance for binary tests, such as sensitivity, specificity,\nand predictive values, are introduced, and extensions to the case of\ncontinuous-outcome tests are detailed. Special focus is placed on the receiver\noperating characteristic (ROC) curve and its estimation, with the topic of\ncovariate adjustment receiving a great deal of attention. The extension to the\ncase of time-dependent ROC curves for evaluating prognostic accuracy is also\ntouched upon. We apply several of the approaches described to a dataset derived\nfrom a study aimed to evaluate the ability of HOMA-IR (homeostasis model\nassessment of insulin resistance) levels to identify individuals at high\ncardio-metabolic risk and how such discriminatory ability might be influenced\nby age and gender. We also outline software available for the implementation of\nthe methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 13:56:49 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Inacio", "Vanda", ""], ["Rodriguez-Alvarez", "Maria Xose", ""], ["Gayoso-Diz", "Pilar", ""]]}, {"id": "2007.07724", "submitter": "Aritz Adin", "authors": "E. Orozco-Acosta, A. Adin, M. D. Ugarte", "title": "Scalable Bayesian modeling for smoothing disease risks in large spatial\n  data sets", "comments": null, "journal-ref": "Spatial Statistics (2021), 41, 100496", "doi": "10.1016/j.spasta.2021.100496", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have been proposed in the spatial statistics literature for\nthe analysis of big data sets in continuous domains. However, new methods for\nanalyzing high-dimensional areal data are still scarce. Here, we propose a\nscalable Bayesian modeling approach for smoothing mortality (or incidence)\nrisks in high-dimensional data, that is, when the number of small areas is very\nlarge. The method is implemented in the R add-on package bigDM. Model fitting\nand inference is based on the idea of \"divide and conquer\" and use integrated\nnested Laplace approximations and numerical integration. We analyze the\nproposal's empirical performance in a comprehensive simulation study that\nconsider two model-free settings. Finally, the methodology is applied to\nanalyze male colorectal cancer mortality in Spanish municipalities showing its\nbenefits with regard to the standard approach in terms of goodness of fit and\ncomputational time.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:50:20 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Orozco-Acosta", "E.", ""], ["Adin", "A.", ""], ["Ugarte", "M. D.", ""]]}, {"id": "2007.07799", "submitter": "Charles Truong", "authors": "Flavien Quijoux, Charles Truong, Ali\\'enor Vienne-Jumeau, Laurent\n  Oudre, Fran\\c{c}ois BERTIN-HUGAULT, Philippe ZAWIEJA, Marie LEFEVRE,\n  Pierre-Paul VIDAL, Damien RICARD", "title": "Meta-analysis parameters computation: a Python approach to facilitate\n  the crossing of experimental conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.MS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis is a data aggregation method that establishes an overall and\nobjective level of evidence based on the results of several studies. It is\nnecessary to maintain a high level of homogeneity in the aggregation of data\ncollected from a systematic literature review. However, the current tools do\nnot allow a cross-referencing of the experimental conditions that could explain\nthe heterogeneity observed between studies. This article aims at proposing a\nPython programming code containing several functions allowing the analysis and\nrapid visualization of data from many studies, while allowing the possibility\nof cross-checking the results by experimental condition.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 09:42:28 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Quijoux", "Flavien", ""], ["Truong", "Charles", ""], ["Vienne-Jumeau", "Ali\u00e9nor", ""], ["Oudre", "Laurent", ""], ["BERTIN-HUGAULT", "Fran\u00e7ois", ""], ["ZAWIEJA", "Philippe", ""], ["LEFEVRE", "Marie", ""], ["VIDAL", "Pierre-Paul", ""], ["RICARD", "Damien", ""]]}, {"id": "2007.07839", "submitter": "Ugur Korkut Pata", "authors": "Ugur Korkut Pata", "title": "COVID-19 Induced Economic Uncertainty: A Comparison between the United\n  Kingdom and the United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN physics.soc-ph q-fin.EC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The purpose of this study is to investigate the effects of the COVID-19\npandemic on economic policy uncertainty in the US and the UK. The impact of the\nincrease in COVID-19 cases and deaths in the country, and the increase in the\nnumber of cases and deaths outside the country may vary. To examine this, the\nstudy employs bootstrap ARDL cointegration approach from March 8, 2020 to May\n24, 2020. According to the bootstrap ARDL results, a long-run equilibrium\nrelationship is confirmed for five out of the 10 models. The long-term\ncoefficients obtained from the ARDL models suggest that an increase in COVID-19\ncases and deaths outside of the UK and the US has a significant effect on\neconomic policy uncertainty. The US is more affected by the increase in the\nnumber of COVID-19 cases. The UK, on the other hand, is more negatively\naffected by the increase in the number of COVID-19 deaths outside the country\nthan the increase in the number of cases. Moreover, another important finding\nfrom the study demonstrates that COVID-19 is a factor of great uncertainty for\nboth countries in the short-term.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 00:22:07 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Pata", "Ugur Korkut", ""]]}, {"id": "2007.07847", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee and Sourabh Bhattacharya", "title": "A Bayesian Multiple Testing Paradigm for Model Selection in Inverse\n  Regression Problems", "comments": "Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a novel Bayesian multiple testing formulation for\nmodel and variable selection in inverse setups, judiciously embedding the idea\nof inverse reference distributions proposed by Bhattacharya (2013) in a mixture\nframework consisting of the competing models. We develop the theory and methods\nin the general context encompassing parametric and nonparametric competing\nmodels, dependent data, as well as misspecifications. Our investigation shows\nthat asymptotically the multiple testing procedure almost surely selects the\nbest possible inverse model that minimizes the minimum Kullback-Leibler\ndivergence from the true model. We also show that the error rates, namely,\nversions of the false discovery rate and the false non-discovery rate converge\nto zero almost surely as the sample size goes to infinity. Asymptotic\n{\\alpha}-control of versions of the false discovery rate and its impact on the\nconvergence of false non-discovery rate versions, are also investigated.\n  Our simulation experiments involve small sample based selection among inverse\nPoisson log regression and inverse geometric logit and probit regression, where\nthe regressions are either linear or based on Gaussian processes. Additionally,\nvariable selection is also considered. Our multiple testing results turn out to\nbe very encouraging in the sense of selecting the best models in all the\nnon-misspecified and misspecified cases.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 17:12:52 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "2007.07930", "submitter": "David R\\\"ugamer", "authors": "David R\\\"ugamer, Philipp F.M. Baumann, Sonja Greven", "title": "Selective Inference for Additive and Linear Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of conducting valid inference for additive\nand linear mixed models after model selection. One possible solution to\novercome overconfident inference results after model selection is selective\ninference, which constitutes a post-selection inference framework, yielding\nvalid inference statements by conditioning on the selection event. We extend\nrecent work on selective inference to the class of additive and linear mixed\nmodels for any type of model selection mechanism that can be expressed as a\nfunction of the outcome variable (and potentially on covariates on which it\nconditions). We investigate the properties of our proposal in simulation\nstudies and apply the framework to a data set in monetary economics. Due to the\ngenerality of our proposed approach, the presented approach also works for\nnon-standard selection procedures, which we demonstrate in our application.\nHere, the final additive mixed model is selected using a hierarchical selection\nprocedure, which is based on the conditional Akaike information criterion and\ninvolves varying data set sizes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 18:18:12 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 14:08:01 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["R\u00fcgamer", "David", ""], ["Baumann", "Philipp F. M.", ""], ["Greven", "Sonja", ""]]}, {"id": "2007.07953", "submitter": "Aaron Molstad", "authors": "Aaron J. Molstad and Adam J. Rothman", "title": "A likelihood-based approach for multivariate categorical response\n  regression in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized likelihood method to fit the bivariate categorical\nresponse regression model. Our method allows practitioners to estimate which\npredictors are irrelevant, which predictors only affect the marginal\ndistributions of the bivariate response, and which predictors affect both the\nmarginal distributions and log odds ratios. To compute our estimator, we\npropose an efficient first order algorithm which we extend to settings where\nsome subjects have only one response variable measured, i.e., the\nsemi-supervised setting. We derive an asymptotic error bound which illustrates\nthe performance of our estimator in high-dimensional settings. Generalizations\nto the multivariate categorical response regression model are proposed.\nFinally, simulation studies and an application in pan-cancer risk prediction\ndemonstrate the usefulness of our method in terms of interpretability and\nprediction accuracy. An R package implementing the proposed method is available\nfor download at github.com/ajmolstad/BvCategorical.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 19:10:34 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 04:15:09 GMT"}, {"version": "v3", "created": "Sat, 19 Sep 2020 19:32:25 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Molstad", "Aaron J.", ""], ["Rothman", "Adam J.", ""]]}, {"id": "2007.08111", "submitter": "Tao Guo", "authors": "Pavlos Nikolopoulos, Tao Guo, Sundara Rajan Srinivasavaradhan,\n  Christina Fragouli, Suhas Diggavi", "title": "Community aware group testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose algorithms that leverage a known community\nstructure to make group testing more efficient. We consider a population\norganized in disjoint communities: each individual participates in a community,\nand its infection probability depends on the community (s)he participates in.\nUse cases include families, students who participate in several classes, and\nworkers who share common spaces. Group testing reduces the number of tests\nneeded to identify the infected individuals by pooling diagnostic samples and\ntesting them together. We show that if we design the testing strategy taking\ninto account the community structure, we can significantly reduce the number of\ntests needed for adaptive and non-adaptive group testing, and can improve the\nreliability in cases where tests are noisy.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 04:53:42 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 18:47:56 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 23:55:53 GMT"}, {"version": "v4", "created": "Sun, 20 Dec 2020 17:57:14 GMT"}, {"version": "v5", "created": "Wed, 17 Mar 2021 00:16:51 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Nikolopoulos", "Pavlos", ""], ["Guo", "Tao", ""], ["Srinivasavaradhan", "Sundara Rajan", ""], ["Fragouli", "Christina", ""], ["Diggavi", "Suhas", ""]]}, {"id": "2007.08189", "submitter": "Juha Karvanen", "authors": "Juha Karvanen, Santtu Tikka, Antti Hyttinen", "title": "Do-search -- a tool for causal inference and study design with multiple\n  data sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemiological evidence is based on multiple data sources including clinical\ntrials, cohort studies, surveys, registries and expert opinions. Merging\ninformation from different sources opens up new possibilities for the\nestimation of causal effects. We show how causal effects can be identified and\nestimated by combining experiments and observations in real and realistic\nscenarios. As a new tool, we present do-search, a recently developed\nalgorithmic approach that can determine the identifiability of a causal effect.\nThe approach is based on do-calculus, and it can utilize data with non-trivial\nmissing data and selection bias mechanisms. When the effect is identifiable,\ndo-search outputs an identifying formula on which numerical estimation can be\nbased. When the effect is not identifiable, we can use do-search to recognize\nadditional data sources and assumptions that would make the effect\nidentifiable. Throughout the paper, we consider the effect of salt-adding\nbehavior on blood pressure mediated by the salt intake as an example. The\nidentifiability of this effect is resolved in various scenarios with different\nassumptions on confounding. There are scenarios where the causal effect is\nidentifiable from a chain of experiments but not from survey data, as well as\nscenarios where the opposite is true. As an illustration, we use survey data\nfrom NHANES 2013--2016 and the results from a meta-analysis of randomized\ncontrolled trials and estimate the reduction in average systolic blood pressure\nunder an intervention where the use of table salt is discontinued.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 08:56:46 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Karvanen", "Juha", ""], ["Tikka", "Santtu", ""], ["Hyttinen", "Antti", ""]]}, {"id": "2007.08190", "submitter": "Kelly Van Lancker", "authors": "Kelly Van Lancker, Oliver Dukes and Stijn Vansteelandt", "title": "Principled Selection of Baseline Covariates to Account for Censoring in\n  Randomized Trials with a Survival Endpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of randomized trials with time-to-event endpoints is nearly\nalways plagued by the problem of censoring. As the censoring mechanism is\nusually unknown, analyses typically employ the assumption of non-informative\ncensoring. While this assumption usually becomes more plausible as more\nbaseline covariates are being adjusted for, such adjustment also raises\nconcerns. Pre-specification of which covariates will be adjusted for (and how)\nis difficult, thus prompting the use of data-driven variable selection\nprocedures, which may impede valid inferences to be drawn. The adjustment for\ncovariates moreover adds concerns about model misspecification, and the fact\nthat each change in adjustment set, also changes the censoring assumption and\nthe treatment effect estimand. In this paper, we discuss these concerns and\npropose a simple variable selection strategy that aims to produce a valid test\nof the null in large samples. The proposal can be implemented using\noff-the-shelf software for (penalized) Cox regression, and is empirically found\nto work well in simulation studies and real data analyses.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 08:57:44 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Van Lancker", "Kelly", ""], ["Dukes", "Oliver", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2007.08352", "submitter": "Christian R\\\"over", "authors": "Christian R\\\"over, Ralf Bender, Sofia Dias, Christopher H. Schmid,\n  Heinz Schmidli, Sibylle Sturtz, Sebastian Weber, Tim Friede", "title": "On weakly informative prior distributions for the heterogeneity\n  parameter in Bayesian random-effects meta-analysis", "comments": "42 pages, 10 figures, 20 tables", "journal-ref": "Research Synthesis Methods, 12(4):448-474, 2021", "doi": "10.1002/jrsm.1475", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normal-normal hierarchical model (NNHM) constitutes a simple and widely\nused framework for meta-analysis. In the common case of only few studies\ncontributing to the meta-analysis, standard approaches to inference tend to\nperform poorly, and Bayesian meta-analysis has been suggested as a potential\nsolution. The Bayesian approach, however, requires the sensible specification\nof prior distributions. While non-informative priors are commonly used for the\noverall mean effect, the use of weakly informative priors has been suggested\nfor the heterogeneity parameter, in particular in the setting of (very) few\nstudies. To date, however, a consensus on how to generally specify a weakly\ninformative heterogeneity prior is lacking. Here we investigate the problem\nmore closely and provide some guidance on prior specification.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 14:26:26 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 15:57:42 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 09:07:23 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["R\u00f6ver", "Christian", ""], ["Bender", "Ralf", ""], ["Dias", "Sofia", ""], ["Schmid", "Christopher H.", ""], ["Schmidli", "Heinz", ""], ["Sturtz", "Sibylle", ""], ["Weber", "Sebastian", ""], ["Friede", "Tim", ""]]}, {"id": "2007.08369", "submitter": "Ivan Kojadinovic", "authors": "Mark Holmes and Ivan Kojadinovic", "title": "Open-end nonparametric sequential change-point detection based on the\n  retrospective CUSUM statistic", "comments": "41 pages, 7 figures, 3 tables, some typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of online monitoring is to issue an alarm as soon as there is\nsignificant evidence in the collected observations to suggest that the\nunderlying data generating mechanism has changed. This work is concerned with\nopen-end, nonparametric procedures that can be interpreted as statistical\ntests. The proposed monitoring schemes consist of computing the so-called\nretrospective CUSUM statistic (or minor variations thereof) after the arrival\nof each new observation. After proposing suitable threshold functions for the\nchosen detectors, the asymptotic validity of the procedures is investigated in\nthe special case of monitoring for changes in the mean, both under the null\nhypothesis of stationarity and relevant alternatives. To carry out the\nsequential tests in practice, an approach based on an asymptotic regression\nmodel is used to estimate high quantiles of relevant limiting distributions.\nMonte Carlo experiments demonstrate the good finite-sample behavior of the\nproposed monitoring schemes and suggest that they are superior to existing\ncompetitors as long as changes do not occur at the very beginning of the\nmonitoring. Extensions to statistics exhibiting an asymptotic mean-like\nbehavior are briefly discussed. Finally, the application of the derived\nsequential change-point detection tests is succinctly illustrated on\ntemperature anomaly data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 14:44:43 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 11:43:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Holmes", "Mark", ""], ["Kojadinovic", "Ivan", ""]]}, {"id": "2007.08458", "submitter": "Tom\\'a\\v{s} Rub\\'in", "authors": "Tom\\'a\\v{s} Rub\\'in, Victor M. Panaretos", "title": "Spectral Simulation of Functional Time Series", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methodology allowing to simulate a stationary functional time\nseries defined by means of its spectral density operators. Our framework is\ngeneral, in that it encompasses any such stationary functional time series,\nwhether linear or not. The methodology manifests particularly significant\ncomputational gains if the spectral density operators are specified by means of\ntheir eigendecomposition or as a filtering of white noise. In the special case\nof linear processes, we determine the analytical expressions for the spectral\ndensity operators of functional autoregressive (fractionally integrated) moving\naverage processes, and leverage these as part of our spectral approach, leading\nto substantial improvements over time-domain simulation methods in some cases.\nThe methods are implemented as an R package (specsimfts) accompanied by several\ndemo files that are easy to modify and can be easily used by researchers aiming\nto probe the finite-sample performance of their functional time series\nmethodology by means of simulation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:51:43 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Rub\u00edn", "Tom\u00e1\u0161", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "2007.08569", "submitter": "Daniele Durante", "authors": "Sirio Legramanti, Tommaso Rigon, Daniele Durante and David B. Dunson", "title": "Extended Stochastic Block Models with Application to Criminal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliably learning group structure among nodes in network data is challenging\nin modern applications. We are motivated by covert networks encoding\nrelationships among criminals. These data are subject to measurement errors and\nexhibit a complex combination of an unknown number of core-periphery,\nassortative and disassortative structures that may unveil the internal\narchitecture of the criminal organization. The coexistence of such noisy block\nstructures limits the reliability of community detection algorithms routinely\napplied to criminal networks, and requires extensions of model-based solutions\nto realistically characterize the node partition process, incorporate\ninformation from node attributes, and provide improved strategies for\nestimation, uncertainty quantification, model selection and prediction. To\naddress these goals, we develop a novel class of extended stochastic block\nmodels (ESBM) that infer groups of nodes having common connectivity patterns\nvia Gibbs-type priors on the partition process. This choice encompasses several\nrealistic priors for criminal networks, covering solutions with fixed, random\nand infinite number of possible groups, and facilitates inclusion of node\nattributes in a principled manner. Among the new alternatives in our class, we\nfocus on the Gnedin process as a realistic prior that allows the number of\ngroups to be finite, random and subject to a reinforcement process coherent\nwith the modular structures in organized crime. A collapsed Gibbs sampler is\nproposed for the whole ESBM class, and refined strategies for estimation,\nprediction, uncertainty quantification and model selection are outlined. ESBM\nperformance is illustrated in realistic simulations and in an application to an\nItalian Mafia network, where we learn key block patterns revealing a complex\nhierarchical structure of the organization, mostly hidden from state-of-the-art\nalternative solutions.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:06:16 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 11:51:40 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Legramanti", "Sirio", ""], ["Rigon", "Tommaso", ""], ["Durante", "Daniele", ""], ["Dunson", "David B.", ""]]}, {"id": "2007.08594", "submitter": "Steffen Ventz", "authors": "Steffen Ventz, Rahul Mazumder, Lorenzo Trippa", "title": "Integration of Survival Data from Multiple Studies", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a statistical procedure that integrates survival data from\nmultiple biomedical studies, to improve the accuracy of predictions of survival\nor other events, based on individual clinical and genomic profiles, compared to\nmodels developed leveraging only a single study or meta-analytic methods. The\nmethod accounts for potential differences in the relation between predictors\nand outcomes across studies, due to distinct patient populations, treatments\nand technologies to measure outcomes and biomarkers. These differences are\nmodeled explicitly with study-specific parameters. We use hierarchical\nregularization to shrink the study-specific parameters towards each other and\nto borrow information across studies. Shrinkage of the study-specific\nparameters is controlled by a similarity matrix, which summarizes differences\nand similarities of the relations between covariates and outcomes across\nstudies. We illustrate the method in a simulation study and using a collection\nof gene-expression datasets in ovarian cancer. We show that the proposed model\nincreases the accuracy of survival prediction compared to alternative\nmeta-analytic methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:52:37 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Ventz", "Steffen", ""], ["Mazumder", "Rahul", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "2007.08623", "submitter": "C\\'eline L\\'evy-Leduc", "authors": "M. Gomtsyan, C. L\\'evy-Leduc, S. Ouadah and L. Sansonnet", "title": "Variable selection in sparse GLARMA models", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.07085", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel and efficient two-stage variable selection\napproach for sparse GLARMA models, which are pervasive for modeling\ndiscrete-valued time series. Our approach consists in iteratively combining the\nestimation of the autoregressive moving average (ARMA) coefficients of GLARMA\nmodels with regularized methods designed for performing variable selection in\nregression coefficients of Generalized Linear Models (GLM). We first establish\nthe consistency of the ARMA part coefficient estimators in a specific case.\nThen, we explain how to efficiently implement our approach. Finally, we assess\nthe performance of our methodology using synthetic data and compare it with\nalternative methods. Our approach is very attractive since it benefits from a\nlow computational load and is able to outperform the other methods in terms of\ncoefficient estimation, particularly in recovering the non null regression\ncoefficients.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 15:11:10 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Gomtsyan", "M.", ""], ["L\u00e9vy-Leduc", "C.", ""], ["Ouadah", "S.", ""], ["Sansonnet", "L.", ""]]}, {"id": "2007.08648", "submitter": "Qinglong Tian", "authors": "Qinglong Tian, Fanqi Meng, Daniel J. Nordman, William Q. Meeker", "title": "Predicting the Number of Future Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes prediction methods for the number of future events from\na population of units associated with an on-going time-to-event process.\nExamples include the prediction of warranty returns and the prediction of the\nnumber of future product failures that could cause serious threats to property\nor life. Important decisions such as whether a product recall should be\nmandated are often based on such predictions. Data, generally right-censored\n(and sometimes left truncated and right-censored), are used to estimate the\nparameters of a time-to-event distribution. This distribution can then be used\nto predict the number of events over future periods of time. Such predictions\nare sometimes called within-sample predictions and differ from other prediction\nproblems considered in most of the prediction literature. This paper shows that\nthe plug-in (also known as estimative or naive) prediction method is not\nasymptotically correct (i.e., for large amounts of data, the coverage\nprobability always fails to converge to the nominal confidence level). However,\na commonly used prediction calibration method is shown to be asymptotically\ncorrect for within-sample predictions, and two alternative\npredictive-distributionbased methods that perform better than the calibration\nmethod are presented and justified.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 21:20:06 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 20:32:49 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 06:00:29 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Tian", "Qinglong", ""], ["Meng", "Fanqi", ""], ["Nordman", "Daniel J.", ""], ["Meeker", "William Q.", ""]]}, {"id": "2007.08675", "submitter": "Dabao Zhang", "authors": "Dabao Zhang", "title": "Coefficients of Determination for Mixed-Effects Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coefficient of determination is well defined for linear models and its\nextension is long wanted for mixed-effects models. We revisit its extension to\ndefine measures for proportions of variation explained by the whole model,\nfixed effects only, and random effects only. We propose to calculate\nunexplained variations conditional on individual random and/or fixed effects so\nas to keep individual heterogeneity brought by available predictors. While\nnaturally defined for linear mixed models, these measures can be defined for a\ngeneralized linear mixed model using a distance measured along its variance\nfunction, accounting for its heteroscedasticity.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 22:28:54 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 12:35:40 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Zhang", "Dabao", ""]]}, {"id": "2007.08719", "submitter": "Michael Schweinberger", "authors": "Minjeong Jeon and Ick Hoon Jin and Michael Schweinberger and Samuel\n  Baugh", "title": "Mapping unobserved item-respondent interactions: A latent space item\n  response model with interaction map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic item response models assume that all items with the same difficulty\nhave the same response probability among all respondents with the same ability.\nThese assumptions, however, may very well be violated in practice, and it is\nnot straightforward to assess whether these assumptions are violated, because\nneither the abilities of respondents nor the difficulties of items are\nobserved. An example is an educational assessment where unobserved\nheterogeneity is present, arising from unobserved variables such as cultural\nbackground and upbringing of students, the quality of mentorship and other\nforms of emotional and professional support received by students, and other\nunobserved variables that may affect response probabilities. To address such\nviolations of assumptions, we introduce a novel latent space model which\nassumes that both items and respondents are embedded in an unobserved metric\nspace, with the probability of a correct response decreasing as a function of\nthe distance between the respondent's and the item's position in the latent\nspace. The resulting latent space approach provides an interaction map that\nrepresents interactions of respondents and items, and helps derive insightful\ndiagnostic information on items as well as respondents. In practice, such\ninteraction maps enable teachers to detect students from underrepresented\ngroups who need more support than other students. We provide empirical evidence\nto demonstrate the usefulness of the proposed latent space approach, along with\nsimulation results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 02:00:54 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 01:08:53 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 01:15:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Jeon", "Minjeong", ""], ["Jin", "Ick Hoon", ""], ["Schweinberger", "Michael", ""], ["Baugh", "Samuel", ""]]}, {"id": "2007.08812", "submitter": "Nataliya Sokolovska", "authors": "Nataliya Sokolovska (SU), Pierre-Henri Wuillemin", "title": "Latent Instrumental Variables as Priors in Causal Inference based on\n  Independence of Cause and Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference methods based on conditional independence construct Markov\nequivalent graphs, and cannot be applied to bivariate cases. The approaches\nbased on independence of cause and mechanism state, on the contrary, that\ncausal discovery can be inferred for two observations. In our contribution, we\nchallenge to reconcile these two research directions. We study the role of\nlatent variables such as latent instrumental variables and hidden common causes\nin the causal graphical structures. We show that the methods based on the\nindependence of cause and mechanism, indirectly contain traces of the existence\nof the hidden instrumental variables. We derive a novel algorithm to infer\ncausal relationships between two variables, and we validate the proposed method\non simulated data and on a benchmark of cause-effect pairs. We illustrate by\nour experiments that the proposed approach is simple and extremely competitive\nin terms of empirical accuracy compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 08:18:19 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Sokolovska", "Nataliya", "", "SU"], ["Wuillemin", "Pierre-Henri", ""]]}, {"id": "2007.08944", "submitter": "Simone Padoan PhD", "authors": "Simone A. Padoan and Gilles Stupfler", "title": "Joint inference on extreme expectiles for multivariate heavy-tailed\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of expectiles, originally introduced in the context of testing for\nhomoscedasticity and conditional symmetry of the error distribution in linear\nregression, induces a law-invariant, coherent and elicitable risk measure that\nhas received a significant amount of attention in actuarial and financial risk\nmanagement contexts. A number of recent papers have focused on the behaviour\nand estimation of extreme expectile-based risk measures and their potential for\nrisk management. Joint inference of several extreme expectiles has however been\nleft untouched; in fact, even the inference of a marginal extreme expectile\nturns out to be a difficult problem in finite samples. We investigate the\nsimultaneous estimation of several extreme marginal expectiles of a random\nvector with heavy-tailed marginal distributions. This is done in a general\nextremal dependence model where the emphasis is on pairwise dependence between\nthe margins. We use our results to derive accurate confidence regions for\nextreme expectiles, as well as a test for the equality of several extreme\nexpectiles. Our methods are showcased in a finite-sample simulation study and\non real financial data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 12:45:01 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Padoan", "Simone A.", ""], ["Stupfler", "Gilles", ""]]}, {"id": "2007.09043", "submitter": "Matthieu Garcin", "authors": "Matthieu Garcin, Jules Klein, Sana Laaribi", "title": "Estimation of time-varying kernel densities and chronology of the impact\n  of COVID-19 on financial markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time-varying kernel density estimation relies on two free parameters: the\nbandwidth and the discount factor. We propose to select these parameters so as\nto minimize a criterion consistent with the traditional requirements of the\nvalidation of a probability density forecast. These requirements are both the\nuniformity and the independence of the so-called probability integral\ntransforms, which are the forecast time-varying cumulated distributions applied\nto the observations. We thus build a new numerical criterion incorporating both\nthe uniformity and independence properties by the mean of an adapted\nKolmogorov-Smirnov statistic. We apply this method to financial markets during\nthe COVID-19 crisis. We determine the time-varying density of daily price\nreturns of several stock indices and, using various divergence statistics, we\nare able to describe the chronology of the crisis as well as regional\ndisparities. For instance, we observe a more limited impact of COVID-19 on\nfinancial markets in China, a strong impact in the US, and a slow recovery in\nEurope.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 15:13:52 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Garcin", "Matthieu", ""], ["Klein", "Jules", ""], ["Laaribi", "Sana", ""]]}, {"id": "2007.09118", "submitter": "Hans Kersting", "authors": "Hans Kersting, Maren Mahsereci", "title": "A Fourier State Space Model for Bayesian ODE Filters", "comments": "5 pages, 2 figures, ICML Workshop on Invertible Neural Networks,\n  Normalizing Flows, and Explicit Likelihood Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian ODE filtering is a probabilistic numerical method to solve ordinary\ndifferential equations (ODEs). It computes a Bayesian posterior over the\nsolution from evaluations of the vector field defining the ODE. Its most\npopular version, which employs an integrated Brownian motion prior, uses Taylor\nexpansions of the mean to extrapolate forward and has the same convergence\nrates as classical numerical methods. As the solution of many important ODEs\nare periodic functions (oscillators), we raise the question whether Fourier\nexpansions can also be brought to bear within the framework of Gaussian ODE\nfiltering. To this end, we construct a Fourier state space model for ODEs and a\n`hybrid' model that combines a Taylor (Brownian motion) and Fourier state space\nmodel. We show by experiments how the hybrid model might become useful in\ncheaply predicting until the end of the time domain.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 17:14:45 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 12:04:02 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Kersting", "Hans", ""], ["Mahsereci", "Maren", ""]]}, {"id": "2007.09225", "submitter": "Kedong Chen", "authors": "Kedong Chen, William Li, Sijian Wang", "title": "An Easy-to-Implement Hierarchical Standardization for Variable Selection\n  Under Strong Heredity Constraint", "comments": "32 pages", "journal-ref": "Journal of Statistical Theory and Practice, 14 (3), 1-32 (2020)", "doi": "10.1007/s42519-020-00102-x", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many practical problems, the regression models follow the strong heredity\nproperty (also known as the marginality), which means they include parent main\neffects when a second-order effect is present. Existing methods rely mostly on\nspecial penalty functions or algorithms to enforce the strong heredity in\nvariable selection. We propose a novel hierarchical standardization procedure\nto maintain strong heredity in variable selection. Our method is effortless to\nimplement and is applicable to any variable selection method for any type of\nregression. The performance of the hierarchical standardization is comparable\nto that of the regular standardization. We also provide robustness checks and\nreal data analysis to illustrate the merits of our method.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 20:57:47 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chen", "Kedong", ""], ["Li", "William", ""], ["Wang", "Sijian", ""]]}, {"id": "2007.09279", "submitter": "Matthew Heiner", "authors": "Matthew Heiner and Athanasios Kottas", "title": "Autoregressive Density Modeling with the Gaussian Process Mixture\n  Transition Distribution", "comments": null, "journal-ref": "Journal of Time Series Analysis (2021)", "doi": "10.1111/jtsa.12603", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a mixture model for transition density approximation, together\nwith soft model selection, in the presence of noisy and heterogeneous nonlinear\ndynamics. Our model builds on the Gaussian mixture transition distribution\n(MTD) model for continuous state spaces, extending component means with\nnonlinear functions that are modeled using Gaussian process (GP) priors. The\nresulting model flexibly captures nonlinear and heterogeneous lag dependence\nwhen several mixture components are active, identifies low-order nonlinear\ndependence while inferring relevant lags when few components are active, and\naverages over multiple and competing single-lag models to quantify/propagate\nuncertainty. Sparsity-inducing priors on the mixture weights aid in selecting a\nsubset of active lags. The hierarchical model specification follows conventions\nfor both GP regression and MTD models, admitting a convenient Gibbs sampling\nscheme for posterior inference. We demonstrate properties of the proposed model\nwith two simulated and two real time series, emphasizing approximation of\nlag-dependent transition densities and model selection. In most cases, the\nmodel decisively recovers important features. The proposed model provides a\nsimple, yet flexible framework that preserves useful and distinguishing\ncharacteristics of the MTD model class.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 23:41:05 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Heiner", "Matthew", ""], ["Kottas", "Athanasios", ""]]}, {"id": "2007.09317", "submitter": "Rui Hu", "authors": "Rui Hu, Ion Bica and Zhichun Zhai", "title": "Robust Optimal Design when Missing Data Happen at Random", "comments": "22 pages. Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate the robust optimal design problem for the\nprediction of response when the fitted regression models are only approximately\nspecified, and observations might be missing completely at random. The\nintuitive idea is as follows: We assume that data are missing at random, and\nthe complete case analysis is applied. To account for the occurrence of missing\ndata, the design criterion we choose is the mean, for the missing indicator, of\nthe averaged (over the design space) mean squared errors of the predictions. To\ndescribe the uncertainty in the specification of the real underlying model, we\nimpose a neighborhood structure on the regression response and maximize,\nanalytically, the \\textbf{M}ean of the averaged \\textbf{M}ean squared\n\\textbf{P}rediction \\textbf{E}rrors (MMPE), over the entire neighborhood. The\nmaximized MMPE is the ``worst'' loss in the neighborhood of the fitted\nregression model. Minimizing the maximum MMPE over the class of designs, we\nobtain robust ``minimax'' designs. The robust designs constructed afford\nprotection from increases in prediction errors resulting from model\nmisspecifications.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 03:23:29 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Hu", "Rui", ""], ["Bica", "Ion", ""], ["Zhai", "Zhichun", ""]]}, {"id": "2007.09394", "submitter": "Maximilian Scholz", "authors": "Maximilian Scholz, Richard Torkar", "title": "An empirical study of Linespots: A novel past-fault algorithm", "comments": "20 pages, 13 figures, submitted to STVR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the novel past-faults fault prediction algorithm\nLinespots, based on the Bugspots algorithm. We analyze the predictive\nperformance and runtime of Linespots compared to Bugspots with an empirical\nstudy using the most significant self-built dataset as of now, including\nhigh-quality samples for validation. As a novelty in fault prediction, we use\nBayesian data analysis and Directed Acyclic Graphs to model the effects. We\nfound consistent improvements in the predictive performance of Linespots over\nBugspots for all seven evaluation metrics. We conclude that Linespots should be\nused over Bugspots in all cases where no real-time performance is necessary.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 10:23:13 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 15:03:12 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 11:39:53 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Scholz", "Maximilian", ""], ["Torkar", "Richard", ""]]}, {"id": "2007.09539", "submitter": "Moo K. Chung", "authors": "Moo K. Chung", "title": "Gaussian kernel smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image acquisition and segmentation are likely to introduce noise. Further\nimage processing such as image registration and parameterization can introduce\nadditional noise. It is thus imperative to reduce noise measurements and boost\nsignal. In order to increase the signal-to-noise ratio (SNR) and smoothness of\ndata required for the subsequent random field theory based statistical\ninference, some type of smoothing is necessary. Among many image smoothing\nmethods, Gaussian kernel smoothing has emerged as a de facto smoothing\ntechnique among brain imaging researchers due to its simplicity in numerical\nimplementation. Gaussian kernel smoothing also increases statistical\nsensitivity and statistical power as well as Gausianness. Gaussian kernel\nsmoothing can be viewed as weighted averaging of voxel values. Then from the\ncentral limit theorem, the weighted average should be more Gaussian.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 00:19:07 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 00:37:44 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Chung", "Moo K.", ""]]}, {"id": "2007.09576", "submitter": "Ting Ye", "authors": "Ting Ye, Yanyao Yi, Jun Shao", "title": "Inference on Average Treatment Effect under Minimization and Other\n  Covariate-Adaptive Randomization Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariate-adaptive randomization schemes such as the minimization and\nstratified permuted blocks are often applied in clinical trials to balance\ntreatment assignments across prognostic factors. The existing theoretical\ndevelopments on inference after covariate-adaptive randomization are mostly\nlimited to situations where a correct model between the response and covariates\ncan be specified or the randomization method has well-understood properties.\nBased on stratification with covariate levels utilized in randomization and a\nfurther adjusting for covariates not used in randomization, in this article we\npropose several estimators for model free inference on average treatment effect\ndefined as the difference between response means under two treatments. We\nestablish asymptotic normality of the proposed estimators under all popular\ncovariate-adaptive randomization schemes including the minimization whose\ntheoretical property is unclear, and we show that the asymptotic distributions\nare invariant with respect to covariate-adaptive randomization methods.\nConsistent variance estimators are constructed for asymptotic inference.\nAsymptotic relative efficiencies and finite sample properties of estimators are\nalso studied. We recommend using one of our proposed estimators for valid and\nmodel free inference after covariate-adaptive randomization.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 03:19:19 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ye", "Ting", ""], ["Yi", "Yanyao", ""], ["Shao", "Jun", ""]]}, {"id": "2007.09649", "submitter": "Qianqian Zhu", "authors": "Songhua Tan and Qianqian Zhu", "title": "Asymmetric linear double autoregression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the asymmetric linear double autoregression, which\njointly models the conditional mean and conditional heteroscedasticity\ncharacterized by asymmetric effects. A sufficient condition is established for\nthe existence of a strictly stationary solution. With a quasi-maximum\nlikelihood estimation (QMLE) procedure introduced, a Bayesian information\ncriterion (BIC) and its modified version are proposed for model selection. To\ndetect asymmetric effects in the volatility, the Wald, Lagrange multiplier and\nquasi-likelihood ratio test statistics are put forward, and their limiting\ndistributions are established under both null and local alternative hypotheses.\nMoreover, a mixed portmanteau test is constructed to check the adequacy of the\nfitted model. All asymptotic properties of inference tools including QMLE,\nBICs, asymmetric tests and the mixed portmanteau test, are established without\nany moment condition on the data process, which makes the new model and its\ninference tools applicable for heavy-tailed data. Simulation studies indicate\nthat the proposed methods perform well in finite samples, and an empirical\napplication to S\\&P500 Index illustrates the usefulness of the new model.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 11:10:24 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 12:42:54 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 12:23:41 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Tan", "Songhua", ""], ["Zhu", "Qianqian", ""]]}, {"id": "2007.09672", "submitter": "Zachary Fisher", "authors": "Zachary F. Fisher, Sy-Miin Chow, Peter C. M. Molenaar, Barbara L.\n  Fredrickson, Vladas Pipiras and Kathleen M. Gates", "title": "A Square-Root Second-Order Extended Kalman Filtering Approach for\n  Estimating Smoothly Time-Varying Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers collecting intensive longitudinal data (ILD) are increasingly\nlooking to model psychological processes, such as emotional dynamics, that\norganize and adapt across time in complex and meaningful ways. This is also the\ncase for researchers looking to characterize the impact of an intervention on\nindividual behavior. To be useful, statistical models must be capable of\ncharacterizing these processes as complex, time-dependent phenomenon, otherwise\nonly a fraction of the system dynamics will be recovered. In this paper we\nintroduce a Square-Root Second-Order Extended Kalman Filtering approach for\nestimating smoothly time-varying parameters. This approach is capable of\nhandling dynamic factor models where the relations between variables underlying\nthe processes of interest change in a manner that may be difficult to specify\nin advance. We examine the performance of our approach in a Monte Carlo\nsimulation and show the proposed algorithm accurately recovers the unobserved\nstates in the case of a bivariate dynamic factor model with time-varying\ndynamics and treatment effects. Furthermore, we illustrate the utility of our\napproach in characterizing the time-varying effect of a meditation intervention\non day-to-day emotional experiences.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 13:47:57 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Fisher", "Zachary F.", ""], ["Chow", "Sy-Miin", ""], ["Molenaar", "Peter C. M.", ""], ["Fredrickson", "Barbara L.", ""], ["Pipiras", "Vladas", ""], ["Gates", "Kathleen M.", ""]]}, {"id": "2007.09720", "submitter": "Wennan Chang", "authors": "Wennan Chang, Changlin Wan, Yong Zang, Chi Zhang, Sha Cao", "title": "Supervised clustering of high dimensional data using regularized mixture\n  modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying relationships between molecular variations and their clinical\npresentations has been challenged by the heterogeneous causes of a disease. It\nis imperative to unveil the relationship between the high dimensional molecular\nmanifestations and the clinical presentations, while taking into account the\npossible heterogeneity of the study subjects. We proposed a novel supervised\nclustering algorithm using penalized mixture regression model, called CSMR, to\ndeal with the challenges in studying the heterogeneous relationships between\nhigh dimensional molecular features to a phenotype. The algorithm was adapted\nfrom the classification expectation maximization algorithm, which offers a\nnovel supervised solution to the clustering problem, with substantial\nimprovement on both the computational efficiency and biological\ninterpretability. Experimental evaluation on simulated benchmark datasets\ndemonstrated that the CSMR can accurately identify the subspaces on which\nsubset of features are explanatory to the response variables, and it\noutperformed the baseline methods. Application of CSMR on a drug sensitivity\ndataset again demonstrated the superior performance of CSMR over the others,\nwhere CSMR is powerful in recapitulating the distinct subgroups hidden in the\npool of cell lines with regards to their coping mechanisms to different drugs.\nCSMR represents a big data analysis tool with the potential to resolve the\ncomplexity of translating the clinical manifestations of the disease to the\nreal causes underpinning it. We believe that it will bring new understanding to\nthe molecular basis of a disease, and could be of special relevance in the\ngrowing field of personalized medicine.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 17:04:07 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chang", "Wennan", ""], ["Wan", "Changlin", ""], ["Zang", "Yong", ""], ["Zhang", "Chi", ""], ["Cao", "Sha", ""]]}, {"id": "2007.09738", "submitter": "Samuel Perreault", "authors": "Samuel Perreault and Johanna Neslehova and Thierry Duchesne", "title": "Hypothesis tests for structured rank correlation matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint modeling of a large number of variables often requires dimension\nreduction strategies that lead to structural assumptions of the underlying\ncorrelation matrix, such as equal pair-wise correlations within subsets of\nvariables. The underlying correlation matrix is thus of interest for both model\nspecification and model validation. In this paper, we develop tests of the\nhypothesis that the entries of the Kendall rank correlation matrix are linear\ncombinations of a smaller number of parameters. The asymptotic behaviour of the\nproposed test statistics is investigated both when the dimension is fixed and\nwhen it grows with the sample size. We pay special attention to the restricted\nhypothesis of partial exchangeability, which contains full exchangeability as a\nspecial case. We show that under partial exchangeability, the test statistics\nand their large-sample distributions simplify, which leads to computational\nadvantages and better performance of the tests. We propose various scalable\nnumerical strategies for implementation of the proposed procedures, investigate\ntheir finite sample behaviour through simulations, and demonstrate their use on\na real dataset of mean sea levels at various geographical locations.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 18:27:16 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Perreault", "Samuel", ""], ["Neslehova", "Johanna", ""], ["Duchesne", "Thierry", ""]]}, {"id": "2007.09751", "submitter": "Arun Kuchibhotla", "authors": "Arun Kumar Kuchibhotla and Alessandro Rinaldo and Larry Wasserman", "title": "Berry-Esseen Bounds for Projection Parameters and Partial Correlations\n  with Increasing Dimension", "comments": "54 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear regression model can be used even when the true regression\nfunction is not linear. The resulting estimated linear function is the best\nlinear approximation to the regression function and the vector $\\beta$ of the\ncoefficients of this linear approximation are the projection parameter. We\nprovide finite sample bounds on the Normal approximation to the law of the\nleast squares estimator of the projection parameters normalized by the\nsandwich-based standard error. Our results hold in the increasing dimension\nsetting and under minimal assumptions on the distribution of the response\nvariable. Furthermore, we construct confidence sets for $\\beta$ in the form of\nhyper-rectangles and establish rates on their coverage accuracy. We provide\nanalogous results for partial correlations among the entries of sub-Gaussian\nvectors.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 18:53:20 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "2007.09811", "submitter": "Liangyu Zhu", "authors": "Liangyu Zhu, Wenbin Lu, Michael R. Kosorok, Rui Song", "title": "Kernel Assisted Learning for Personalized Dose Finding", "comments": "Accepted for KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An individualized dose rule recommends a dose level within a continuous safe\ndose range based on patient level information such as physical conditions,\ngenetic factors and medication histories. Traditionally, personalized dose\nfinding process requires repeating clinical visits of the patient and frequent\nadjustments of the dosage. Thus the patient is constantly exposed to the risk\nof underdosing and overdosing during the process. Statistical methods for\nfinding an optimal individualized dose rule can lower the costs and risks for\npatients. In this article, we propose a kernel assisted learning method for\nestimating the optimal individualized dose rule. The proposed methodology can\nalso be applied to all other continuous decision-making problems. Advantages of\nthe proposed method include robustness to model misspecification and capability\nof providing statistical inference for the estimated parameters. In the\nsimulation studies, we show that this method is capable of identifying the\noptimal individualized dose rule and produces favorable expected outcomes in\nthe population. Finally, we illustrate our approach using data from a warfarin\ndosing study for thrombosis patients.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 23:03:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhu", "Liangyu", ""], ["Lu", "Wenbin", ""], ["Kosorok", "Michael R.", ""], ["Song", "Rui", ""]]}, {"id": "2007.09812", "submitter": "Liangyu Zhu", "authors": "Liangyu Zhu, Wenbin Lu, Rui Song", "title": "Causal Effect Estimation and Optimal Dose Suggestions in Mobile Health", "comments": "Accepted for ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose novel structural nested models to estimate causal\neffects of continuous treatments based on mobile health data. To find the\ntreatment regime that optimizes the expected short-term outcomes for patients,\nwe define a weighted lag-K advantage as the value function. The optimal\ntreatment regime is then defined to be the one that maximizes the value\nfunction. Our method imposes minimal assumptions on the data generating\nprocess. Statistical inference is provided for the estimated parameters.\nSimulation studies and an application to the Ohio type 1 diabetes dataset show\nthat our method could provide meaningful insights for dose suggestions with\nmobile health data.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 23:07:23 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 06:18:21 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Zhu", "Liangyu", ""], ["Lu", "Wenbin", ""], ["Song", "Rui", ""]]}, {"id": "2007.09844", "submitter": "Chelsea Mitchell", "authors": "Chelsea Mitchell, Abdel-Salam Abdel-Salam, D'Arcy Mays", "title": "Bayesian EWMA and CUSUM Control Charts Under Different Loss Functions", "comments": "19 pages, 5 tables, 35 equations, presented at Joint Statistical\n  Meeting (08/2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Exponentially Weighted Moving Average (EWMA) and Cumulative Sum (CUSUM)\ncontrol charts have been used in profile monitoring to track drift shifts that\noccur in a monitored process. We construct Bayesian EWMA and Bayesian CUSUM\ncharts informed by posterior and posterior predictive distributions using\ndifferent loss functions, prior distributions, and likelihood distributions. A\nsimulation study is performed, and the performance of the charts are evaluated\nvia average run length (ARL), standard deviation of the run length (SDRL),\naverage time to signal (ATS), and standard deviation of time to signal (SDTS).\nA sensitivity analysis is conducted using choices for the smoothing parameter,\nout-of-control shift size, and hyper-parameters of the distribution. Based on\nobtained results, we provide recommendations for use of the Bayesian EWMA and\nBayesian CUSUM control charts.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 02:29:08 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Mitchell", "Chelsea", ""], ["Abdel-Salam", "Abdel-Salam", ""], ["Mays", "D'Arcy", ""]]}, {"id": "2007.09851", "submitter": "Rina Barber", "authors": "Rina Foygel Barber and Lucas Janson", "title": "Testing goodness-of-fit and conditional independence with approximate\n  co-sufficient sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goodness-of-fit (GoF) testing is ubiquitous in statistics, with direct ties\nto model selection, confidence interval construction, conditional independence\ntesting, and multiple testing, just to name a few applications. While testing\nthe GoF of a simple (point) null hypothesis provides an analyst great\nflexibility in the choice of test statistic while still ensuring validity, most\nGoF tests for composite null hypotheses are far more constrained, as the test\nstatistic must have a tractable distribution over the entire null model space.\nA notable exception is co-sufficient sampling (CSS): resampling the data\nconditional on a sufficient statistic for the null model guarantees valid GoF\ntesting using any test statistic the analyst chooses. But CSS testing requires\nthe null model to have a compact (in an information-theoretic sense) sufficient\nstatistic, which only holds for a very limited class of models; even for a null\nmodel as simple as logistic regression, CSS testing is powerless. In this\npaper, we leverage the concept of approximate sufficiency to generalize CSS\ntesting to essentially any parametric model with an asymptotically-efficient\nestimator; we call our extension \"approximate CSS\" (aCSS) testing. We quantify\nthe finite-sample Type I error inflation of aCSS testing and show that it is\nvanishing under standard maximum likelihood asymptotics, for any choice of test\nstatistic. We apply our proposed procedure both theoretically and in simulation\nto a number of models of interest to demonstrate its finite-sample Type I error\nand power.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 02:44:39 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 02:01:03 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Janson", "Lucas", ""]]}, {"id": "2007.09910", "submitter": "Yi Yu", "authors": "Yi Yu and Sabyasachi Chatterjee", "title": "Localising change points in piecewise polynomials of general degrees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are concerned with a sequence of univariate random variables\nwith piecewise polynomial means and independent sub-Gaussian noise. The\nunderlying polynomials are allowed to be of arbitrary but fixed degrees. All\nthe other model parameters are allowed to vary depending on the sample size.\n  We propose a two-step estimation procedure based on the $\\ell_0$-penalisation\nand provide upper bounds on the localisation error. We complement these results\nby deriving a global information-theoretic lower bounds, which show that our\ntwo-step estimators are nearly minimax rate-optimal. We also show that our\nestimator enjoys near optimally adaptive performance by attaining individual\nlocalisation errors depending on the level of smoothness at individual change\npoints of the underlying signal. In addition, under a special smoothness\nconstraint, we provide a minimax lower bound on the localisation errors. This\nlower bound is independent of the polynomial orders and is sharper than the\nglobal minimax lower bound.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 07:28:11 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 12:33:20 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Yu", "Yi", ""], ["Chatterjee", "Sabyasachi", ""]]}, {"id": "2007.09927", "submitter": "Fangzheng Lin", "authors": "Fangzheng Lin, Yanlin Tang, Huichen Zhu, Zhongyi Zhu", "title": "Spatially Clustered Varying Coefficient Model", "comments": "59 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various applications with large spatial regions, the relationship between\nthe response variable and the covariates is expected to exhibit complex spatial\npatterns. We propose a spatially clustered varying coefficient model, where the\nregression coefficients are allowed to vary smoothly within each cluster but\nchange abruptly across the boundaries of adjacent clusters, and we develop a\nunified approach for simultaneous coefficient estimation and cluster\nidentification. The varying coefficients are approximated by penalized splines,\nand the clusters are identified through a fused concave penalty on differences\nin neighboring locations, where the spatial neighbors are specified by the\nminimum spanning tree (MST). The optimization is solved efficiently based on\nthe alternating direction method of multipliers, utilizing the sparsity\nstructure from MST. Furthermore, we establish the oracle property of the\nproposed method considering the structure of MST. Numerical studies show that\nthe proposed method can efficiently incorporate spatial neighborhood\ninformation and automatically detect possible spatially clustered patterns in\nthe regression coefficients. An empirical study in oceanography illustrates\nthat the proposed method is promising to provide informative results.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 08:17:55 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Lin", "Fangzheng", ""], ["Tang", "Yanlin", ""], ["Zhu", "Huichen", ""], ["Zhu", "Zhongyi", ""]]}, {"id": "2007.10014", "submitter": "Debo Cheng", "authors": "Debo Cheng and Jiuyong Li and Lin Liu and Jixue Liu and Thuc Duy Le\n  (University of South Australia)", "title": "Local search for efficient causal effect estimation", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal effect estimation from observational data is an important but\nchallenging problem. Causal effect estimation with unobserved variables in data\nis even more difficult. The challenges lie in (1) whether the causal effect can\nbe estimated from observational data (identifiability); (2) accuracy of\nestimation (unbiasedness), and (3) fast data-driven algorithm for the\nestimation (efficiency). Each of the above problems by its own, is challenging.\nThere does not exist many data-driven methods for causal effect estimation so\nfar, and they solve one or two of the above problems, but not all. In this\npaper, we present an algorithm that is fast, unbiased and is able to confirm if\na causal effect is identifiable or not under a very practical and commonly seen\nproblem setting. To achieve high efficiency, we approach the causal effect\nestimation problem as a local search for the minimal adjustment variable sets\nin data. We have shown that identifiability and unbiased estimation can be both\nresolved using data in our problem setting, and we have developed theorems to\nsupport the local search for searching for adjustment variable sets to achieve\nunbiased causal effect estimation. We make use of frequent pattern mining\nstrategy to further speed up the search process. Experiments performed on an\nextensive collection of synthetic and real-world datasets demonstrate that the\nproposed algorithm outperforms the state-of-the-art causal effect estimation\nmethods in both accuracy and time-efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 11:43:36 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Cheng", "Debo", "", "University of South Australia"], ["Li", "Jiuyong", "", "University of South Australia"], ["Liu", "Lin", "", "University of South Australia"], ["Liu", "Jixue", "", "University of South Australia"], ["Le", "Thuc Duy", "", "University of South Australia"]]}, {"id": "2007.10183", "submitter": "Linyi Zou", "authors": "Linyi Zou, Hui Guo and Carlo Berzuini", "title": "Overlapping-sample Mendelian randomisation with multiple exposures: A\n  Bayesian approach", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Mendelian randomization (MR) has been widely applied to causal\ninference in medical research. It uses genetic variants as instrumental\nvariables (IVs) to investigate putative causal relationship between an exposure\nand an outcome. Traditional MR methods have dominantly focussed on a two-sample\nsetting in which IV-exposure association study and IV-outcome association study\nare independent. However, it is not uncommon that participants from the two\nstudies fully overlap (one-sample) or partly overlap (overlapping-sample).\nMethods: We proposed a method that is applicable to all the three sample\nsettings. In essence, we converted a two- or overlapping- sample problem to a\none-sample problem where data of some or all of the individuals were\nincomplete. Assume that all individuals were drawn from the same population and\nunmeasured data were missing at random. Then the unobserved data were treated\nau pair with the model parameters as unknown quantities, and thus, could be\nimputed iteratively conditioning on the observed data and estimated parameters\nusing Markov chain Monte Carlo. We generalised our model to allow for\npleiotropy and multiple exposures and assessed its performance by a number of\nsimulations using four metrics: mean, standard deviation, coverage and power.\nResults: Higher sample overlapping rate and stronger instruments led to\nestimates with higher precision and power. Pleiotropy had a notably negative\nimpact on the estimates. Nevertheless, overall the coverages were high and our\nmodel performed well in all the sample settings. Conclusions: Our model offers\nthe flexibility of being applicable to any of the sample settings, which is an\nimportant addition to the MR literature which has restricted to one- or two-\nsample scenarios. Given the nature of Bayesian inference, it can be easily\nextended to more complex MR analysis in medical research.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 15:18:33 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 14:48:54 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zou", "Linyi", ""], ["Guo", "Hui", ""], ["Berzuini", "Carlo", ""]]}, {"id": "2007.10393", "submitter": "Isabel Fulcher", "authors": "Katherine Evans, Isabel Fulcher, and Eric J. Tchetgen Tchetgen", "title": "A coherent likelihood parametrization for doubly robust estimation of a\n  causal effect with missing confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data and confounding are two problems researchers face in\nobservational studies for comparative effectiveness. Williamson et al. (2012)\nrecently proposed a unified approach to handle both issues concurrently using a\nmultiply-robust (MR) methodology under the assumption that confounders are\nmissing at random. Their approach considers a union of models in which any\nsubmodel has a parametric component while the remaining models are\nunrestricted. We show that while their estimating function is MR in theory, the\npossibility for multiply robust inference is complicated by the fact that\nparametric models for different components of the union model are not variation\nindependent and therefore the MR property is unlikely to hold in practice. To\naddress this, we propose an alternative transparent parametrization of the\nlikelihood function, which makes explicit the model dependencies between\nvarious nuisance functions needed to evaluate the MR efficient score. The\nproposed method is genuinely doubly-robust (DR) in that it is consistent and\nasymptotic normal if one of two sets of modeling assumptions holds. We evaluate\nthe performance and doubly robust property of the DR method via a simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 18:28:42 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Evans", "Katherine", ""], ["Fulcher", "Isabel", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "2007.10432", "submitter": "Sokbae Lee", "authors": "Sokbae Lee, Bernard Salani\\'e", "title": "Filtered and Unfiltered Treatment Effects with Targeting Instruments", "comments": "60 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivalued treatments are commonplace in applications. We explore the use of\ndiscrete-valued instruments to control for selection bias in this setting. We\nestablish conditions under which counterfactual averages and treatment effects\nare identified for heterogeneous complier groups. These conditions restrict (i)\nthe unobserved heterogeneity in treatment assignment, (ii) how the instruments\ntarget the treatments, and optionally (iii) the extent to which counterfactual\naverages are heterogeneous. We allow for limitations in the analyst's\ninformation via the concept of a filtered treatment. Finally, we illustrate the\nusefulness of our framework by applying it to data from the Student Achievement\nand Retention Project and the Head Start Impact Study.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 19:43:31 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 21:57:41 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Lee", "Sokbae", ""], ["Salani\u00e9", "Bernard", ""]]}, {"id": "2007.10438", "submitter": "William Fithian", "authors": "William Fithian and Lihua Lei", "title": "Conditional calibration for false discovery rate control under\n  dependence", "comments": "26 pages main text, 17 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of methods for finite-sample false discovery rate\n(FDR) control in multiple testing problems with dependent test statistics where\nthe dependence is fully or partially known. Our approach separately calibrates\na data-dependent p-value rejection threshold for each hypothesis, relaxing or\ntightening the threshold as appropriate to target exact FDR control. In\naddition to our general framework we propose a concrete algorithm, the\ndependence-adjusted Benjamini-Hochberg (dBH) procedure, which adaptively\nthresholds the q-value for each hypothesis. Under positive regression\ndependence the dBH procedure uniformly dominates the standard BH procedure, and\nin general it uniformly dominates the Benjamini-Yekutieli (BY) procedure (also\nknown as BH with log correction). Simulations and real data examples illustrate\npower gains over competing approaches to FDR control under dependence.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:04:59 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Fithian", "William", ""], ["Lei", "Lihua", ""]]}, {"id": "2007.10586", "submitter": "Archer Zhang", "authors": "Archer Gong Zhang, Guangyu Zhu and Jiahua Chen", "title": "Empirical Likelihood Ratio Test on quantiles under a Density Ratio Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population quantiles are important parameters in many applications.\nEnthusiasm for the development of effective statistical inference procedures\nfor quantiles and their functions has been high for the past decade. In this\narticle, we study inference methods for quantiles when multiple samples from\nlinked populations are available. The research problems we consider have a wide\nrange of applications. For example, to study the evolution of the economic\nstatus of a country, economists monitor changes in the quantiles of annual\nhousehold incomes, based on multiple survey datasets collected annually. Even\nwith multiple samples, a routine approach would estimate the quantiles of\ndifferent populations separately. Such approaches ignore the fact that these\npopulations are linked and share some intrinsic latent structure. Recently,\nmany researchers have advocated the use of the density ratio model (DRM) to\naccount for this latent structure and have developed more efficient procedures\nbased on pooled data. The nonparametric empirical likelihood (EL) is\nsubsequently employed. Interestingly, there has been no discussion in this\ncontext of the EL-based likelihood ratio test (ELRT) for population quantiles.\nWe explore the use of the ELRT for hypotheses concerning quantiles and\nconfidence regions under the DRM. We show that the ELRT statistic has a\nchi-square limiting distribution under the null hypothesis. Simulation\nexperiments show that the chi-square distributions approximate the\nfinite-sample distributions well and lead to accurate tests and confidence\nregions. The DRM helps to improve statistical efficiency. We also give a\nreal-data example to illustrate the efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 03:59:54 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 06:36:33 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Zhang", "Archer Gong", ""], ["Zhu", "Guangyu", ""], ["Chen", "Jiahua", ""]]}, {"id": "2007.10612", "submitter": "Art Owen", "authors": "Swarnadip Ghosh and Trevor Hastie and Art B. Owen", "title": "Backfitting for large scale crossed random effects regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models with crossed random effect errors can be very expensive to\ncompute. The cost of both generalized least squares and Gibbs sampling can\neasily grow as $N^{3/2}$ (or worse) for $N$ observations. Papaspiliopoulos et\nal. (2020) present a collapsed Gibbs sampler that costs $O(N)$, but under an\nextremely stringent sampling model. We propose a backfitting algorithm to\ncompute a generalized least squares estimate and prove that it costs $O(N)$. A\ncritical part of the proof is in ensuring that the number of iterations\nrequired is $O(1)$ which follows from keeping a certain matrix norm below\n$1-\\delta$ for some $\\delta>0$. Our conditions are greatly relaxed compared to\nthose for the collapsed Gibbs sampler, though still strict. Empirically, the\nbackfitting algorithm has a norm below $1-\\delta$ under conditions that are\nless strict than those in our assumptions. We illustrate the new algorithm on a\nratings data set from Stitch Fix.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 06:00:59 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 21:06:38 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 22:20:09 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Ghosh", "Swarnadip", ""], ["Hastie", "Trevor", ""], ["Owen", "Art B.", ""]]}, {"id": "2007.10656", "submitter": "Lourens  Waldorp", "authors": "Lourens Waldorp and Maarten Marsman", "title": "Relations between networks, regression, partial correlation, and latent\n  variable model", "comments": "21 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Gaussian graphical model (GGM) has become a popular tool for analyzing\nnetworks of psychological variables. In a recent paper in this journal, Forbes,\nWright, Markon, and Krueger (FWMK) voiced the concern that GGMs that are\nestimated from partial correlations wrongfully remove the variance that is\nshared by its constituents. If true, this concern has grave consequences for\nthe application of GGMs. Indeed, if partial correlations only capture the\nunique covariances, then the data that come from a unidimensional latent\nvariable model ULVM should be associated with an empty network (no edges), as\nthere are no unique covariances in a ULVM. We know that this cannot be true,\nwhich suggests that FWMK are missing something with their claim. We introduce a\nconnection between the ULVM and the GGM and use that connection to prove that\nwe find a fully-connected and not an empty network associated with a ULVM. We\nthen use the relation between GGMs and linear regression to show that the\npartial correlation indeed does not remove the common variance.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 08:29:05 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Waldorp", "Lourens", ""], ["Marsman", "Maarten", ""]]}, {"id": "2007.10727", "submitter": "Matthieu Garcin", "authors": "Ayoub Ammy-Driss and Matthieu Garcin", "title": "Efficiency of the financial markets during the COVID-19 crisis:\n  time-varying parameters of fractional stable dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.GN stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the impact of COVID-19 on financial markets. It\nfocuses on the evolution of the market efficiency, using two efficiency\nindicators: the Hurst exponent and the memory parameter of a fractional\nL\\'evy-stable motion. The second approach combines, in the same model of\ndynamic, an alpha-stable distribution and a dependence structure between price\nreturns. We provide a dynamic estimation method for the two efficiency\nindicators. This method introduces a free parameter, the discount factor, which\nwe select so as to get the best alpha-stable density forecasts for observed\nprice returns. The application to stock indices during the COVID-19 crisis\nshows a strong loss of efficiency for US indices. On the opposite, Asian and\nAustralian indices seem less affected and the inefficiency of these markets\nduring the COVID-19 crisis is even questionable.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:39:41 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 17:16:15 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ammy-Driss", "Ayoub", ""], ["Garcin", "Matthieu", ""]]}, {"id": "2007.10731", "submitter": "Benjamin Guedj", "authors": "Arthur Leroy and Pierre Latouche and Benjamin Guedj and Servane Gey", "title": "MAGMA: Inference and Prediction with Multi-Task Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate the problem of multiple time series forecasting, with the\nobjective to improve multiple-step-ahead predictions. We propose a multi-task\nGaussian process framework to simultaneously model batches of individuals with\na common mean function and a specific covariance structure. This common mean is\ndefined as a Gaussian process for which the hyper-posterior distribution is\ntractable. Therefore an EM algorithm can be derived for simultaneous\nhyper-parameters optimisation and hyper-posterior computation. Unlike previous\napproaches in the literature, we account for uncertainty and handle uncommon\ngrids of observations while maintaining explicit formulations, by modelling the\nmean process in a non-parametric probabilistic framework. We also provide\npredictive formulas integrating this common mean process. This approach greatly\nimproves the predictive performance far from observations, where information\nshared across individuals provides a relevant prior mean. Our overall algorithm\nis called \\textsc{Magma} (standing for Multi tAsk Gaussian processes with\ncommon MeAn), and publicly available as a R package. The quality of the mean\nprocess estimation, predictive performances, and comparisons to alternatives\nare assessed in various simulated scenarios and on real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:43:54 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Leroy", "Arthur", ""], ["Latouche", "Pierre", ""], ["Guedj", "Benjamin", ""], ["Gey", "Servane", ""]]}, {"id": "2007.10768", "submitter": "Wencan Zhu", "authors": "Wencan Zhu, C\\'eline L\\'evy-Leduc, Nils Tern\\`es", "title": "A variable selection approach for highly correlated predictors in\n  high-dimensional genomic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genomic studies, identifying biomarkers associated with a variable of\ninterest is a major concern in biomedical research. Regularized approaches are\nclassically used to perform variable selection in high-dimensional linear\nmodels. However, these methods can fail in highly correlated settings. We\npropose a novel variable selection approach called WLasso, taking these\ncorrelations into account. It consists in rewriting the initial\nhigh-dimensional linear model to remove the correlation between the biomarkers\n(predictors) and in applying the generalized Lasso criterion. The performance\nof WLasso is assessed using synthetic data in several scenarios and compared\nwith recent alternative approaches. The results show that when the biomarkers\nare highly correlated, WLasso outperforms the other approaches in sparse\nhigh-dimensional frameworks. The method is also successfully illustrated on\npublicly available gene expression data in breast cancer. Our method is\nimplemented in the WLasso R package which is available from the Comprehensive R\nArchive Network.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 13:05:21 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Zhu", "Wencan", ""], ["L\u00e9vy-Leduc", "C\u00e9line", ""], ["Tern\u00e8s", "Nils", ""]]}, {"id": "2007.10776", "submitter": "Yu Gui", "authors": "Yu Gui", "title": "ADAGES: adaptive aggregation with stability for distributed feature\n  selection", "comments": "24 pages, 9 figures; ACM-IMS Foundations of Data Science Conference", "journal-ref": "Proceedings of the 2020 ACM-IMS Foundations of Data Science\n  Conference", "doi": "10.1145/3412815.3416881", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of \"big\" data, not only the large amount of data keeps motivating\ndistributed computing, but concerns on data privacy also put forward the\nemphasis on distributed learning. To conduct feature selection and to control\nthe false discovery rate in a distributed pattern with multi-machines or\nmulti-institutions, an efficient aggregation method is necessary. In this\npaper, we propose an adaptive aggregation method called ADAGES which can be\nflexibly applied to any machine-wise feature selection method. We will show\nthat our method is capable of controlling the overall FDR with a theoretical\nfoundation while maintaining power as good as the Union aggregation rule in\npractice.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 13:19:31 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 10:00:55 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 06:14:40 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Gui", "Yu", ""]]}, {"id": "2007.10780", "submitter": "L\\'eo Belzile", "authors": "L\\'eo R. Belzile and Anthony C. Davison", "title": "Improved inference on risk measures for univariate extremes", "comments": "30 pages and 10 figures, plus Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the use of likelihood asymptotics for inference on risk measures\nin univariate extreme value problems, focusing on estimation of high quantiles\nand similar summaries of risk for uncertainty quantification. We study whether\nhigher-order approximation based on the tangent exponential model can provide\nimproved inferences, and conclude that inference based on maxima is generally\nrobust to mild model misspecification and that profile likelihood-based\nconfidence intervals will often be adequate, whereas inferences based on\nthreshold exceedances can be badly biased but may be improved by higher-order\nmethods, at least for moderate sample sizes. We use the methods to shed light\non catastrophic rainfall in Venezuela, flooding in Venice, and the lifetimes of\nItalian semi-supercentenarians.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 13:26:20 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 17:17:28 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Belzile", "L\u00e9o R.", ""], ["Davison", "Anthony C.", ""]]}, {"id": "2007.10899", "submitter": "Richard Jones", "authors": "Tomas Kalibera and Richard Jones", "title": "Quantifying Performance Changes with Effect Size Confidence Intervals", "comments": "A preliminary version of a portion of this work was presented at the\n  Third European Performance Engineering Workshop", "journal-ref": null, "doi": null, "report-no": "University of Kent TR 4-12", "categories": "stat.ME cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measuring performance & quantifying a performance change are core evaluation\ntechniques in programming language and systems research. Of 122 recent\nscientific papers, as many as 65 included experimental evaluation that\nquantified a performance change using a ratio of execution times. Few of these\npapers evaluated their results with the level of rigour that has come to be\nexpected in other experimental sciences. The uncertainty of measured results\nwas largely ignored. Scarcely any of the papers mentioned uncertainty in the\nratio of the mean execution times, and most did not even mention uncertainty in\nthe two means themselves. Most of the papers failed to address the\nnon-deterministic execution of computer programs (caused by factors such as\nmemory placement, for example), and none addressed non-deterministic\ncompilation. It turns out that the statistical methods presented in the\ncomputer systems performance evaluation literature for the design and summary\nof experiments do not readily allow this either. This poses a hazard to the\nrepeatability, reproducibility and even validity of quantitative results.\n  Inspired by statistical methods used in other fields of science, and building\non results in statistics that did not make it to introductory textbooks, we\npresent a statistical model that allows us both to quantify uncertainty in the\nratio of (execution time) means and to design experiments with a rigorous\ntreatment of those multiple sources of non-determinism that might impact\nmeasured performance. Better still, under our framework summaries can be as\nsimple as \"system A is faster than system B by 5.5% $\\pm$ 2.5%, with 95%\nconfidence\", a more natural statement than those derived from typical current\npractice, which are often misinterpreted.\n  November 2013\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 15:44:34 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Kalibera", "Tomas", ""], ["Jones", "Richard", ""]]}, {"id": "2007.11037", "submitter": "Mike West", "authors": "Mike West", "title": "Bayesian Decision Analysis and Constrained Forecasting", "comments": "17 pages (including title page and Supplementary Material). 4 figures\n  in main text, 3 in supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian decision analysis perspective on problems of constrained\nforecasting is presented and developed, motivated by increasing interest in\nproblems of aggregate and hierarchical forecasting coupled with short-comings\nof traditional, purely inferential approaches. Foundational and pedagogic\ndevelopments underlie new methodological approaches to such problems, explored\nand exemplified in contexts of total-constrained forecasting linked to\nmotivating applications in commercial forecasting. The new perspective is\ncomplementary and integrated with traditional Bayesian inference approaches,\nwhile offering new practical methodology when the traditional view is\nchallenged. Examples explore ranges of practically relevant loss functions in\nsimple, illustrative contexts that highlight the opportunities for methodology\nas well as practically important questions of how constrained forecasting is\nimpacted by dependencies among outcomes being predicted. The paper couples this\ncore development with arguments in support of a broader view of Bayesian\ndecision analysis than is typically adopted, involving studies of predictive\ndistributions of loss function values under putative optimal decisions.\nAdditional examples highlight the practical importance of this broader view in\nthe constrained forecasting context. Extensions to more general constrained\nforecasting problems, and connections with broader interests in forecast\nreconciliation and aggregation are noted along with other broader\nconsiderations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 18:38:16 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["West", "Mike", ""]]}, {"id": "2007.11048", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen", "title": "Maximum likelihood estimation of potential energy in interacting\n  particle systems from single-trajectory data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the parameter estimation problem for the quadratic\npotential energy in interacting particle systems from continuous-time and\nsingle-trajectory data. Even though such dynamical systems are\nhigh-dimensional, we show that the vanilla maximum likelihood estimator\n(without regularization) is able to estimate the interaction potential\nparameter with optimal rate of convergence simultaneously in mean-field limit\nand in long-time dynamics. This to some extend avoids the\ncurse-of-dimensionality for estimating large dynamical systems under symmetry\nof the particle interaction.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 19:15:37 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 01:24:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chen", "Xiaohui", ""]]}, {"id": "2007.11049", "submitter": "Nikola Surjanovic", "authors": "Nikola Surjanovic, Richard Lockhart, and Thomas M. Loughin", "title": "A Generalized Hosmer-Lemeshow Goodness-of-Fit Test for a Family of\n  Generalized Linear Models", "comments": "37 pages; modified/updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models (GLMs) are used within a vast number of application\ndomains. However, formal goodness of fit (GOF) tests for the overall fit of the\nmodel$-$so-called \"global\" tests$-$seem to be in wide use only for certain\nclasses of GLMs. In this paper we develop and apply a new global\ngoodness-of-fit test, similar to the well-known and commonly used\nHosmer-Lemeshow (HL) test, that can be used with a wide variety of GLMs. The\ntest statistic is a variant of the HL test statistic, but we rigorously derive\nan asymptotically correct sampling distribution of the test statistic using\nmethods of Stute and Zhu (2002). Our new test is relatively straightforward to\nimplement and interpret. We demonstrate the test on a real data set, and\ncompare the performance of our new test with other global GOF tests for GLMs,\nfinding that our test provides competitive or comparable power in various\nsimulation settings. Our test also avoids the use of kernel-based estimators,\nused in various GOF tests for regression, thereby avoiding the issues of\nbandwidth selection and the curse of dimensionality. Since the asymptotic\nsampling distribution is known, a bootstrap procedure for the calculation of a\np-value is also not necessary, and we therefore find that performing our test\nis computationally efficient.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 19:17:59 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 03:33:41 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Surjanovic", "Nikola", ""], ["Lockhart", "Richard", ""], ["Loughin", "Thomas M.", ""]]}, {"id": "2007.11123", "submitter": "Peng Liu", "authors": "Peng Liu, Yusi Fang, Zhao Ren, Lu Tang and George C. Tseng", "title": "Outcome-Guided Disease Subtyping for High-Dimensional Omics Data", "comments": "29 pages in total, 4 figures, 2 tables and 1 supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput microarray and sequencing technology have been used to\nidentify disease subtypes that could not be observed otherwise by using\nclinical variables alone. The classical unsupervised clustering strategy\nconcerns primarily the identification of subpopulations that have similar\npatterns in gene features. However, as the features corresponding to irrelevant\nconfounders (e.g. gender or age) may dominate the clustering process, the\nresulting clusters may or may not capture clinically meaningful disease\nsubtypes. This gives rise to a fundamental problem: can we find a subtyping\nprocedure guided by a pre-specified disease outcome? Existing methods, such as\nsupervised clustering, apply a two-stage approach and depend on an arbitrary\nnumber of selected features associated with outcome. In this paper, we propose\na unified latent generative model to perform outcome-guided disease subtyping\nconstructed from omics data, which improves the resulting subtypes concerning\nthe disease of interest. Feature selection is embedded in a regularization\nregression. A modified EM algorithm is applied for numerical computation and\nparameter estimation. The proposed method performs feature selection, latent\nsubtype characterization and outcome prediction simultaneously. To account for\npossible outliers or violation of mixture Gaussian assumption, we incorporate\nrobust estimation using adaptive Huber or median-truncated loss function.\nExtensive simulations and an application to complex lung diseases with\ntranscriptomic and clinical data demonstrate the ability of the proposed method\nto identify clinically relevant disease subtypes and signature genes suitable\nto explore toward precision medicine.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:56:18 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Liu", "Peng", ""], ["Fang", "Yusi", ""], ["Ren", "Zhao", ""], ["Tang", "Lu", ""], ["Tseng", "George C.", ""]]}, {"id": "2007.11131", "submitter": "Yu-hsuan Wang", "authors": "Y. Samuel Wang and Mathias Drton", "title": "Causal Discovery with Unobserved Confounding and non-Gaussian Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering causal structure from multivariate\nobservational data. We assume that the data arise from a linear structural\nequation model (SEM) in which the idiosyncratic errors are allowed to be\ndependent in order to capture possible latent confounding. Each SEM can be\nrepresented by a graph where vertices represent observed variables, directed\nedges represent direct causal effects, and bidirected edges represent\ndependence among error terms. Specifically, we assume that the true model\ncorresponds to a bow-free acyclic path diagram, i.e., a graph that has at most\none edge between any pair of nodes and is acyclic in the directed part. We show\nthat when the errors are non-Gaussian, the exact causal structure encoded by\nsuch a graph, and not merely an equivalence class, can be consistently\nrecovered from observational data. The Bow-free Acylic Non-Gaussian (BANG)\nmethod we propose for this purpose uses estimates of suitable moments, but, in\ncontrast to previous results, does not require specifying the number of latent\nvariables a priori. We illustrate the effectiveness of BANG in simulations and\nan application to an ecology data set.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 23:29:53 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Wang", "Y. Samuel", ""], ["Drton", "Mathias", ""]]}, {"id": "2007.11152", "submitter": "Junlong Zhao", "authors": "Yiwei Fan, Xiaoling Lu, Yufeng Liu, and Junlong Zhao", "title": "Angle-based hierarchical classification using exact label embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical classification problems are commonly seen in practice. However,\nmost existing methods do not fully utilize the hierarchical information among\nclass labels. In this paper, a novel label embedding approach is proposed,\nwhich keeps the hierarchy of labels exactly, and reduces the complexity of the\nhypothesis space significantly. Based on the newly proposed label embedding\napproach, a new angle-based classifier is developed for hierarchical\nclassification. Moreover, to handle massive data, a new (weighted) linear loss\nis designed, which has a closed form solution and is computationally efficient.\nTheoretical properties of the new method are established and intensive\nnumerical comparisons with other methods are conducted. Both simulations and\napplications in document categorization demonstrate the advantages of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 01:21:48 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Fan", "Yiwei", ""], ["Lu", "Xiaoling", ""], ["Liu", "Yufeng", ""], ["Zhao", "Junlong", ""]]}, {"id": "2007.11306", "submitter": "Erez Buchweitz", "authors": "Erez Buchweitz, Shlomo Ahal, Oded Papish, Guy Adini", "title": "Two-Stage Regularization of Pseudo-Likelihood Estimators with\n  Application to Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimators derived from score functions that are not the likelihood are in\nwide use in practical and modern applications. Their regularization is often\ncarried by pseudo-posterior estimation, equivalently by adding penalty to the\nscore function. We argue that this approach is suboptimal, and propose a\ntwo-staged alternative involving estimation of a new score function which\nbetter approximates the true likelihood for the purpose of regularization. Our\napproach typically identifies with maximum a-posteriori estimation if the\noriginal score function is in fact the likelihood. We apply our theory to\nfitting ordinary least squares (OLS) under contemporaneous exogeneity, a\nsetting appearing often in time series and in which OLS is the estimator of\nchoice by practitioners.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 09:51:22 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 07:28:51 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Buchweitz", "Erez", ""], ["Ahal", "Shlomo", ""], ["Papish", "Oded", ""], ["Adini", "Guy", ""]]}, {"id": "2007.11418", "submitter": "Christopher Geoga", "authors": "Christopher J. Geoga and Mihai Anitescu and Michael L. Stein", "title": "Flexible nonstationary spatio-temporal modeling of high-frequency\n  monitoring data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many physical datasets are generated by collections of instruments that make\nmeasurements at regular time intervals. For such regular monitoring data, we\nextend the framework of half-spectral covariance functions to the case of\nnonstationarity in space and time and demonstrate that this method provides a\nnatural and tractable way to incorporate complex behaviors into a covariance\nmodel. Further, we use this method with fully time-domain computations to\nobtain bona fide maximum likelihood estimators---as opposed to using\nWhittle-type likelihood approximations, for example---that can still be\ncomputed efficiently. We apply this method to very high-frequency Doppler LIDAR\nvertical wind velocity measurements, demonstrating that the model can\nexpressively capture the extreme nonstationarity of dynamics above and below\nthe atmospheric boundary layer and, more importantly, the interaction of the\nprocess dynamics across it.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 13:28:14 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Geoga", "Christopher J.", ""], ["Anitescu", "Mihai", ""], ["Stein", "Michael L.", ""]]}, {"id": "2007.11521", "submitter": "Alexandre M\\\"osching", "authors": "Alexandre M\\\"osching and Lutz D\\\"umbgen", "title": "Estimation of a Likelihood Ratio Ordered Family of Distributions -- with\n  a Connection to Total Positivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider bivariate observations $(X_1,Y_1), \\ldots, (X_n,Y_n) \\in\n\\mathbb{R}\\times \\mathbb{R}$ with unknown conditional distributions $Q_x$ of\n$Y$, given that $X = x$. The goal is to estimate these distributions under the\nsole assumption that $Q_x$ is isotonic in $x$ with respect to likelihood ratio\norder. If the observations are identically distributed, a related goal is to\nestimate the joint distribution $\\mathcal{L}(X,Y)$ under the sole assumption\nthat it is totally positive of order two in a certain sense. After reviewing\nand generalizing the concepts of likelihood ratio order and total positivity of\norder two, an algorithm is developed which estimates the unknown family of\ndistributions $(Q_x)_x$ via empirical likelihood. The benefit of the stronger\nregularization imposed by likelihood ratio order over the usual stochastic\norder is evaluated in terms of estimation and predictive performances on\nsimulated as well as real data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 16:28:32 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 16:18:46 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["M\u00f6sching", "Alexandre", ""], ["D\u00fcmbgen", "Lutz", ""]]}, {"id": "2007.11573", "submitter": "Rui Gao", "authors": "Rui Gao and Simo S\\\"arkk\\\"a and Rub\\'en Claveria-Vega and Simon\n  Godsill", "title": "Autonomous Tracking and State Estimation with Generalised Group Lasso", "comments": "14pags, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of autonomous tracking and state estimation for marine\nvessels, autonomous vehicles, and other dynamic signals under a (structured)\nsparsity assumption. The aim is to improve the tracking and estimation accuracy\nwith respect to classical Bayesian filters and smoothers. We formulate the\nestimation problem as a dynamic generalised group Lasso problem and develop a\nclass of smoothing-and-splitting methods to solve it. The Levenberg--Marquardt\niterated extended Kalman smoother-based multi-block alternating direction\nmethod of multipliers (LM-IEKS-mADMM) algorithms are based on the alternating\ndirection method of multipliers (ADMM) framework. This leads to minimisation\nsubproblems with an inherent structure to which three new augmented recursive\nsmoothers are applied. Our methods can deal with large-scale problems without\npre-processing for dimensionality reduction. Moreover, the methods allow one to\nsolve nonsmooth nonconvex optimisation problems. We then prove that under mild\nconditions, the proposed methods converge to a stationary point of the\noptimisation problem. By simulated and real-data experiments including\nmulti-sensor range measurement problems, marine vessel tracking, autonomous\nvehicle tracking, and audio signal restoration, we show the practical\neffectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 17:54:41 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 20:30:26 GMT"}, {"version": "v3", "created": "Sun, 30 May 2021 09:06:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gao", "Rui", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Claveria-Vega", "Rub\u00e9n", ""], ["Godsill", "Simon", ""]]}, {"id": "2007.11638", "submitter": "C. H. Bryan Liu", "authors": "C. H. Bryan Liu (1 and 2), Emma J. McCoy (1) ((1) Imperial College\n  London, (2) ASOS.com)", "title": "An Evaluation Framework for Personalization Strategy Experiment Designs", "comments": "Presented in the AdKDD 2020 workshop, in conjunction with The 26th\n  ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2020. Main\n  paper: 7 pages, 2 figures, 2 tables, Supplementary document: 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Controlled Experiments (OCEs) are the gold standard in evaluating the\neffectiveness of changes to websites. An important type of OCE evaluates\ndifferent personalization strategies, which present challenges in low test\npower and lack of full control in group assignment. We argue that getting the\nright experiment setup -- the allocation of users to treatment/analysis groups\n-- should take precedence of post-hoc variance reduction techniques in order to\nenable the scaling of the number of experiments. We present an evaluation\nframework that, along with a few simple rule of thumbs, allow experimenters to\nquickly compare which experiment setup will lead to the highest probability of\ndetecting a treatment effect under their particular circumstance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 19:21:39 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 18:58:45 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Liu", "C. H. Bryan", "", "1 and 2"], ["McCoy", "Emma J.", ""]]}, {"id": "2007.11657", "submitter": "Ryan Elmore", "authors": "Ryan Elmore", "title": "Modeling Sums of Exchangeable Binary Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new model for sums of exchangeable binary random variables.\nThe proposed distribution is an approximation to the exact distributional form,\nand relies on the theory of completely monotone functions and the Laplace\ntransform of a gamma distribution function. Using Monte Carlo methods, we show\nthat this new model compares favorably to the beta binomial model with respect\nto estimating the success probability of the Bernoulli trials and the\ncorrelation between any two variables in the exchangeable set. We apply the new\nmethodology to two classic data sets and the results are summarized.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 20:15:58 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 20:15:22 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Elmore", "Ryan", ""]]}, {"id": "2007.11700", "submitter": "Claudia Wehrhahn", "authors": "Claudia Wehrhahn, Ruth Fuentes-Garc\\'ia, Rams\\'es H. Mena, Fabrizio\n  Leisen, Maria Elena Gonz\\'alez-Villalpando, and Clicerio\n  Gonz\\'alez-Villalpando", "title": "A Copula-based Fully Bayesian Nonparametric Evaluation of Cardiovascular\n  Risk Markers in the Mexico City Diabetes Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular disease lead the cause of death world wide and several studies\nhave been carried out to understand and explore cardiovascular risk markers in\nnormoglycemic and diabetic populations. In this work, we explore the\nassociation structure between hyperglycemic markers and cardiovascular risk\nmarkers controlled by triglycerides, body mass index, age and gender, for the\nnormoglycemic population in The Mexico City Diabetes Study. Understanding the\nassociation structure could contribute to the assessment of additional\ncardiovascular risk markers in this low income urban population with a high\nprevalence of classic cardiovascular risk biomarkers. The association structure\nis measured by conditional Kendall's tau, defined through conditional copula\nfunctions. The latter are in turn modeled under a fully Bayesian nonparametric\napproach, which allows the complete shape of the copula function to vary for\ndifferent values of the controlled covariates.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 22:13:19 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 21:33:16 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Wehrhahn", "Claudia", ""], ["Fuentes-Garc\u00eda", "Ruth", ""], ["Mena", "Rams\u00e9s H.", ""], ["Leisen", "Fabrizio", ""], ["Gonz\u00e1lez-Villalpando", "Maria Elena", ""], ["Gonz\u00e1lez-Villalpando", "Clicerio", ""]]}, {"id": "2007.11779", "submitter": "Boudewijn Roukema", "authors": "Boudewijn F. Roukema", "title": "Anti-clustering in the national SARS-CoV-2 daily infection counts", "comments": "28 pages, 17 figures, 12 tables; zenodo.4291207 at\n  https://zenodo.org/record/4291207, live git at\n  https://codeberg.org/boud/subpoisson, archived git at\n  https://archive.softwareheritage.org/swh:1:rev:72242ca8eade9659031ea00394a30e0cc5cc1c37;\n  v2: comparison with alternative methods and correlation with PFI, data\n  updated to May 2021", "journal-ref": null, "doi": "10.5281/zenodo.4765705", "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The noise in daily infection counts of an epidemic should be super-Poissonian\ndue to intrinsic epidemiological and administrative clustering. Here, we use\nthis clustering to classify the official national SARS-CoV-2 daily infection\ncounts and check for infection counts that are unusually anti-clustered. We\nadopt a one-parameter model of $\\phi'_i$ infections per cluster, dividing any\ndaily count $n_i$ into $n_i/\\phi'_i$ 'clusters', for 'country' $i$. We assume\nthat $n_i/\\phi'_i$ on a given day $j$ is drawn from a Poisson distribution\nwhose mean is robustly estimated from the four neighbouring days, and calculate\nthe inferred Poisson probability $P'_{ij}$ of the observation. The $P'_{ij}$\nvalues should be uniformly distributed. We find the value $\\phi_i$ that\nminimises the Kolmogorov-Smirnov distance from a uniform distribution. We\ninvestigate the $(\\phi_i, N_i)$ distribution, for total infection count $N_i$.\nWe find that most of the daily infection count sequences are inconsistent with\na Poissonian model. Most are found to be consistent with the $\\phi_i$ model,\nthe 28-, 14- and 7-day least noisy sequences for several countries are best\nmodelled as sub-Poissonian, suggesting a distinct epidemiological family. The\n28-day least noisy sequence of DZ (Algeria) has a preferred model that is\nstrongly sub-Poissonian, with $\\phi_i^{28} < 0.1$. TJ, TR, RU, BY, AL, AE, and\nNI have preferred models that are also sub-Poissonian, with $\\phi_i^{28} <\n0.5$. A statistically significant ($P^{\\tau} < 0.05$) correlation was found\nbetween the lack of media freedom in a country, as represented by a high\nReporters sans frontieres Press Freedom Index (PFI$^{2020}$), and the lack of\nstatistical noise in the country's daily counts. The $\\phi_i$ model appears to\nbe an effective detector of suspiciously low statistical noise in the national\nSARS-CoV-2 daily infection counts.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 04:12:03 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 00:16:22 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Roukema", "Boudewijn F.", ""]]}, {"id": "2007.11811", "submitter": "Xiaohan Yan", "authors": "Xiaohan Yan, Avleen S. Bijral", "title": "On a Bernoulli Autoregression Framework for Link Discovery and\n  Prediction", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dynamic prediction framework for binary sequences that is based\non a Bernoulli generalization of the auto-regressive process. Our approach\nlends itself easily to variants of the standard link prediction problem for a\nsequence of time dependent networks. Focusing on this dynamic network link\nprediction/recommendation task, we propose a novel problem that exploits\nadditional information via a much larger sequence of auxiliary networks and has\nimportant real-world relevance. To allow discovery of links that do not exist\nin the available data, our model estimation framework introduces a\nregularization term that presents a trade-off between the conventional link\nprediction and this discovery task. In contrast to existing work our stochastic\ngradient based estimation approach is highly efficient and can scale to\nnetworks with millions of nodes. We show extensive empirical results on both\nactual product-usage based time dependent networks and also present results on\na Reddit based data set of time dependent sentiment sequences.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 05:58:22 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Yan", "Xiaohan", ""], ["Bijral", "Avleen S.", ""]]}, {"id": "2007.11902", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen and Daniel V Fryer", "title": "A binary-response regression model based on support vector machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The soft-margin support vector machine (SVM) is a ubiquitous tool for\nprediction of binary-response data. However, the SVM is characterized entirely\nvia a numerical optimization problem, rather than a probability model, and thus\ndoes not directly generate probabilistic inferential statements as outputs. We\nconsider a probabilistic regression model for binary-response data that is\nbased on the optimization problem that characterizes the SVM. Under weak\nregularity assumptions, we prove that the maximum likelihood estimate (MLE) of\nour model exists, and that it is consistent and asymptotically normal. We\nfurther assess the performance of our model via simulation studies, and\ndemonstrate its use in real data applications regarding spam detection and well\nwater access.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 10:14:48 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Nguyen", "Hien D", ""], ["Fryer", "Daniel V", ""]]}, {"id": "2007.11936", "submitter": "Pierre E. Jacob", "authors": "Chenguang Dai, Jeremy Heng, Pierre E. Jacob, Nick Whiteley", "title": "An invitation to sequential Monte Carlo samplers", "comments": "review article, 34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo samplers provide consistent approximations of\nsequences of probability distributions and of their normalizing constants, via\nparticles obtained with a combination of importance weights and Markov\ntransitions. This article presents this class of methods and a number of recent\nadvances, with the goal of helping statisticians assess the applicability and\nusefulness of these methods for their purposes. Our presentation emphasizes the\nrole of bridging distributions for computational and statistical purposes.\nNumerical experiments are provided on simple settings such as multivariate\nNormals, logistic regression and a basic susceptible-infected-recovered model,\nillustrating the impact of the dimension, the ability to perform inference\nsequentially and the estimation of normalizing constants.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 11:26:01 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Dai", "Chenguang", ""], ["Heng", "Jeremy", ""], ["Jacob", "Pierre E.", ""], ["Whiteley", "Nick", ""]]}, {"id": "2007.11972", "submitter": "Yuxiao Li", "authors": "Yuxiao Li, Ying Sun, Brian J Reich", "title": "DeepKriging: Spatially Dependent Deep Neural Networks for Spatial\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In spatial statistics, a common objective is to predict the values of a\nspatial process at unobserved locations by exploiting spatial dependence. In\ngeostatistics, Kriging provides the best linear unbiased predictor using\ncovariance functions and is often associated with Gaussian processes. However,\nwhen considering non-linear prediction for non-Gaussian and categorical data,\nthe Kriging prediction is not necessarily optimal, and the associated variance\nis often overly optimistic. We propose to use deep neural networks (DNNs) for\nspatial prediction. Although DNNs are widely used for general classification\nand prediction, they have not been studied thoroughly for data with spatial\ndependence. In this work, we propose a novel neural network structure for\nspatial prediction by adding an embedding layer of spatial coordinates with\nbasis functions. We show in theory that the proposed DeepKriging method has\nmultiple advantages over Kriging and classical DNNs only with spatial\ncoordinates as features. We also provide density prediction for uncertainty\nquantification without any distributional assumption and apply the method to\nPM$_{2.5}$ concentrations across the continental United States.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 12:38:53 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 08:15:15 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Li", "Yuxiao", ""], ["Sun", "Ying", ""], ["Reich", "Brian J", ""]]}, {"id": "2007.12031", "submitter": "Jeong-Soo Park", "authors": "Yire Shin, Piyapatr Busababodhin, Jeong-Soo Park", "title": "The r-largest four parameter kappa distribution", "comments": "6 figures, 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized extreme value distribution (GEVD) has been widely used to\nmodel the extreme events in many areas. It is however limited to using only\nblock maxima, which motivated to model the GEVD dealing with $r$-largest order\nstatistics (rGEVD). The rGEVD which uses more than one extreme per block can\nsignificantly improves the performance of the GEVD. The four parameter kappa\ndistribution (K4D) is a generalization of some three-parameter distributions\nincluding the GEVD. It can be useful in fitting data when three parameters in\nthe GEVD are not sufficient to capture the variability of the extreme\nobservations. The K4D still uses only block maxima. In this study, we thus\nextend the K4D to deal with $r$-largest order statistics as analogy as the GEVD\nis extended to the rGEVD. The new distribution is called the $r$-largest four\nparameter kappa distribution (rK4D). We derive a joint probability density\nfunction (PDF) of the rK4D, and the marginal and conditional cumulative\ndistribution functions and PDFs. The maximum likelihood method is considered to\nestimate parameters. The usefulness and some practical concerns of the rK4D are\nillustrated by applying it to Venice sea-level data. This example study shows\nthat the rK4D gives better fit but larger variances of the parameter estimates\nthan the rGEVD. Some new $r$-largest distributions are derived as special cases\nof the rK4D, such as the $r$-largest logistic (rLD), generalized logistic\n(rGLD), and generalized Gumbel distributions (rGGD).\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 14:19:32 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Shin", "Yire", ""], ["Busababodhin", "Piyapatr", ""], ["Park", "Jeong-Soo", ""]]}, {"id": "2007.12175", "submitter": "Tomas Masak", "authors": "Tomas Masak, Soham Sarkar, Victor M. Panaretos", "title": "Principal Separable Component Analysis via the Partial Inner Product", "comments": "23 pages + appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-parametric estimation of covariance lies at the heart of functional\ndata analysis, whether for curve or surface-valued data. The case of a\ntwo-dimensional domain poses both statistical and computational challenges,\nwhich are typically alleviated by assuming separability. However, separability\nis often questionable, sometimes even demonstrably inadequate. We propose a\nframework for the analysis of covariance operators of random surfaces that\ngeneralises separability, while retaining its major advantages. Our approach is\nbased on the additive decomposition of the covariance into a series of\nseparable components. The decomposition is valid for any covariance over a\ntwo-dimensional domain. Leveraging the key notion of the partial inner product,\nwe generalise the power iteration method to general Hilbert spaces and show how\nthe aforementioned decomposition can be efficiently constructed in practice.\nTruncation of the decomposition and retention of the principal separable\ncomponents automatically induces a non-parametric estimator of the covariance,\nwhose parsimony is dictated by the truncation level. The resulting estimator\ncan be calculated, stored and manipulated with little computational overhead\nrelative to separability. The framework and estimation method are genuinely\nnon-parametric, since the considered decomposition holds for any covariance.\nConsistency and rates of convergence are derived under mild regularity\nassumptions, illustrating the trade-off between bias and variance regulated by\nthe truncation level. The merits and practical performance of the proposed\nmethodology are demonstrated in a comprehensive simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 12:21:21 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Masak", "Tomas", ""], ["Sarkar", "Soham", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "2007.12300", "submitter": "Max Goplerud", "authors": "Max Goplerud", "title": "Fast and Accurate Estimation of Non-Nested Binomial Hierarchical Models\n  Using Variational Inference", "comments": "Minor revisions to correct typos, add replication data, and rerun\n  models", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear hierarchical models are commonly used in many disciplines.\nHowever, inference in the presence of non-nested effects and on large datasets\nis challenging and computationally burdensome. This paper provides two\ncontributions to scalable and accurate inference. First, I derive a new\nmean-field variational algorithm for estimating binomial logistic hierarchical\nmodels with an arbitrary number of non-nested random effects. Second, I propose\n\"marginally augmented variational Bayes\" (MAVB) that further improves the\ninitial approximation through a step of Bayesian post-processing. I prove that\nMAVB provides a guaranteed improvement in the approximation quality at low\ncomputational cost and induces dependencies that were assumed away by the\ninitial factorization assumptions.\n  I apply these techniques to a study of voter behavior using a\nhigh-dimensional application of the popular approach of multilevel regression\nand post-stratification (MRP). Existing estimation took hours whereas the\nalgorithms proposed run in minutes. The posterior means are well-recovered even\nunder strong factorization assumptions. Applying MAVB further improves the\napproximation by partially correcting the under-estimated variance. The\nproposed methodology is implemented in an open source software package.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 00:05:50 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 20:57:55 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 00:16:53 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Goplerud", "Max", ""]]}, {"id": "2007.12313", "submitter": "Igor Silin", "authors": "Igor Silin, Jianqing Fan", "title": "Canonical thresholding for non-sparse high-dimensional linear regression", "comments": "40 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a high-dimensional linear regression problem. Unlike many papers\non the topic, we do not require sparsity of the regression coefficients;\ninstead, our main structural assumption is a decay of eigenvalues of the\ncovariance matrix of the data. We propose a new family of estimators, called\nthe canonical thresholding estimators, which pick largest regression\ncoefficients in the canonical form. The estimators admit an explicit form and\ncan be linked to LASSO and Principal Component Regression (PCR). A theoretical\nanalysis for both fixed design and random design settings is provided. Obtained\nbounds on the mean squared error and the prediction error of a specific\nestimator from the family allow to clearly state sufficient conditions on the\ndecay of eigenvalues to ensure convergence. In addition, we promote the use of\nthe relative errors, strongly linked with the out-of-sample $R^2$. The study of\nthese relative errors leads to a new concept of joint effective dimension,\nwhich incorporates the covariance of the data and the regression coefficients\nsimultaneously, and describes the complexity of a linear regression problem.\nNumerical simulations confirm good performance of the proposed estimators\ncompared to the previously developed methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 01:12:18 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Silin", "Igor", ""], ["Fan", "Jianqing", ""]]}, {"id": "2007.12325", "submitter": "Rami Mahdi", "authors": "Rami Mahdi", "title": "A Nonparametric Test of Dependence Based on Ensemble of Decision Trees", "comments": "35 pages, 6 figures, 1 table with 15 small figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a robust non-parametric measure of statistical dependence, or\ncorrelation, between two random variables is presented. The proposed\ncoefficient is a permutation-like statistic that quantifies how much the\nobserved sample S_n : {(X_i , Y_i), i = 1 . . . n} is discriminable from the\npermutated sample ^S_nn : {(X_i , Y_j), i, j = 1 . . . n}, where the two\nvariables are independent. The extent of discriminability is determined using\nthe predictions for the, interchangeable, leave-out sample from training an\naggregate of decision trees to discriminate between the two samples without\nmaterializing the permutated sample. The proposed coefficient is\ncomputationally efficient, interpretable, invariant to monotonic\ntransformations, and has a well-approximated distribution under independence.\nEmpirical results show the proposed method to have a high power for detecting\ncomplex relationships from noisy data.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 02:48:33 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Mahdi", "Rami", ""]]}, {"id": "2007.12333", "submitter": "Manoel Santos Neto", "authors": "Eliardo G. Costa and Manoel Santos-Neto", "title": "Optimal sample size for the Birnbaum-Saunders distribution under a\n  decision-theoretic approach", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Birnbaum-Saunders distribution has been widely applied in several areas\nof science and although several methodologies related to this distribution have\nbeen proposed, the problem of determining the optimal sample size for\nestimating its mean has not yet been studied. For this purpose, we propose a\nmethodology to determine the optimal sample size under a decision-theoretic\napproach. In this approach, we consider loss functions for point and interval\ninference. Finally, computational tools in the R language were developed to use\nin practice.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 03:42:18 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Costa", "Eliardo G.", ""], ["Santos-Neto", "Manoel", ""]]}, {"id": "2007.12448", "submitter": "Danijel Kivaranovic", "authors": "Danijel Kivaranovic, Hannes Leeb", "title": "A (tight) upper bound for the length of confidence intervals with\n  conditional coverage", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that two popular selective inference procedures, namely data carving\n(Fithian et al., 2017) and selection with a randomized response (Tian et al.,\n2018b), when combined with the polyhedral method (Lee et al., 2016), result in\nconfidence intervals whose length is bounded. This contrasts results for\nconfidence intervals based on the polyhedral method alone, whose expected\nlength is typically infinite (Kivaranovic and Leeb, 2020). Moreover, we show\nthat these two procedures always dominate corresponding sample-splitting\nmethods in terms of interval length.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 11:12:04 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 18:36:35 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kivaranovic", "Danijel", ""], ["Leeb", "Hannes", ""]]}, {"id": "2007.12616", "submitter": "Isabella Grabski", "authors": "Isabella N. Grabski, Roberta De Vito, Lorenzo Trippa, Giovanni\n  Parmigiani", "title": "Bayesian Combinatorial Multi-Study Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing multiple studies allows leveraging data from a range of sources and\npopulations, but until recently, there have been limited methodologies to\napproach the joint unsupervised analysis of multiple high-dimensional studies.\nA recent method, Bayesian Multi-Study Factor Analysis (BMSFA), identifies\nlatent factors common to all studies, as well as latent factors specific to\nindividual studies. However, BMSFA does not allow for partially shared factors,\ni.e. latent factors shared by more than one but less than all studies. We\nextend BMSFA by introducing a new method, Tetris, for Bayesian combinatorial\nmulti-study factor analysis, which identifies latent factors that can be shared\nby any combination of studies. We model the subsets of studies that share\nlatent factors with an Indian Buffet Process. We test our method with an\nextensive range of simulations, and showcase its utility not only in dimension\nreduction but also in covariance estimation. Finally, we apply Tetris to\nhigh-dimensional gene expression datasets to identify patterns in breast cancer\ngene expression, both within and across known classes defined by germline\nmutations.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 16:17:47 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Grabski", "Isabella N.", ""], ["De Vito", "Roberta", ""], ["Trippa", "Lorenzo", ""], ["Parmigiani", "Giovanni", ""]]}, {"id": "2007.12674", "submitter": "Audra McMillan", "authors": "Mark Bun and J\\\"org Drechsler and Marco Gaboardi and Audra McMillan", "title": "Controlling Privacy Loss in Survey Sampling (Working Paper)", "comments": "Working paper, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social science and economics research is often based on data collected in\nsurveys. Due to time and budgetary constraints, this data is often collected\nusing complex sampling schemes designed to increase accuracy while reducing the\ncosts of data collection. A commonly held belief is that the sampling process\naffords the data subjects some additional privacy. This intuition has been\nformalized in the differential privacy literature for simple random sampling: a\ndifferentially private mechanism run on a simple random subsample of a\npopulation provides higher privacy guarantees than when run on the entire\npopulation. In this work we initiate the study of the privacy implications of\nmore complicated sampling schemes including cluster sampling and stratified\nsampling. We find that not only do these schemes often not amplify privacy, but\nthat they can result in privacy degradation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:43:08 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Bun", "Mark", ""], ["Drechsler", "J\u00f6rg", ""], ["Gaboardi", "Marco", ""], ["McMillan", "Audra", ""]]}, {"id": "2007.12702", "submitter": "Brandon Stewart", "authors": "Justin Grimmer, Dean Knox and Brandon M. Stewart", "title": "Na\\\"ive regression requires weaker assumptions than factor models to\n  adjust for multiple cause confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The empirical practice of using factor models to adjust for shared,\nunobserved confounders, $\\mathbf{Z}$, in observational settings with multiple\ntreatments, $\\mathbf{A}$, is widespread in fields including genetics, networks,\nmedicine, and politics. Wang and Blei (2019, WB) formalizes these procedures\nand develops the \"deconfounder,\" a causal inference method using factor models\nof $\\mathbf{A}$ to estimate \"substitute confounders,\" $\\hat{\\mathbf{Z}}$, then\nestimating treatment effects by regressing the outcome, $\\mathbf{Y}$, on part\nof $\\mathbf{A}$ while adjusting for $\\hat{\\mathbf{Z}}$. WB claim the\ndeconfounder is unbiased when there are no single-cause confounders and\n$\\hat{\\mathbf{Z}}$ is \"pinpointed.\" We clarify pinpointing requires each\nconfounder to affect infinitely many treatments. We prove under these\nassumptions, a na\\\"ive semiparametric regression of $\\mathbf{Y}$ on\n$\\mathbf{A}$ is asymptotically unbiased. Deconfounder variants nesting this\nregression are therefore also asymptotically unbiased, but variants using\n$\\hat{\\mathbf{Z}}$ and subsets of causes require further untestable\nassumptions. We replicate every deconfounder analysis with available data and\nfind it fails to consistently outperform na\\\"ive regression. In practice, the\ndeconfounder produces implausible estimates in WB's case study to movie\nearnings: estimates suggest comic author Stan Lee's cameo appearances causally\ncontributed \\$15.5 billion, most of Marvel movie revenue. We conclude neither\napproach is a viable substitute for careful research design in real-world\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 18:00:02 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Grimmer", "Justin", ""], ["Knox", "Dean", ""], ["Stewart", "Brandon M.", ""]]}, {"id": "2007.12740", "submitter": "Yi Zhao", "authors": "Yi Zhao and Brian S. Caffo and Xi Luo", "title": "Principal Regression for High Dimensional Covariance Matrices", "comments": "21 pages of main text and references, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript presents an approach to perform generalized linear regression\nwith multiple high dimensional covariance matrices as the outcome. Model\nparameters are proposed to be estimated by maximizing a pseudo-likelihood. When\nthe data are high dimensional, the normal likelihood function is ill-posed as\nthe sample covariance matrix is rank-deficient. Thus, a well-conditioned linear\nshrinkage estimator of the covariance matrix is introduced. With multiple\ncovariance matrices, the shrinkage coefficients are proposed to be common\nacross matrices. Theoretical studies demonstrate that the proposed covariance\nmatrix estimator is optimal achieving the uniformly minimum quadratic loss\nasymptotically among all linear combinations of the identity matrix and the\nsample covariance matrix. Under regularity conditions, the proposed estimator\nof the model parameters is consistent. The superior performance of the proposed\napproach over existing methods is illustrated through simulation studies.\nImplemented to a resting-state functional magnetic resonance imaging study\nacquired from the Alzheimer's Disease Neuroimaging Initiative, the proposed\napproach identified a brain network within which functional connectivity is\nsignificantly associated with Apolipoprotein E $\\varepsilon$4, a strong genetic\nmarker for Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 19:09:54 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Zhao", "Yi", ""], ["Caffo", "Brian S.", ""], ["Luo", "Xi", ""]]}, {"id": "2007.12769", "submitter": "Weijia Zhang", "authors": "Weijia Zhang, Jiuyong Li, Lin Liu", "title": "A unified survey of treatment effect heterogeneity modeling and uplift\n  modeling", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central question in many fields of scientific research is to determine how\nan outcome would be affected by an action, or to measure the effect of an\naction (a.k.a treatment effect). In recent years, a need for estimating the\nheterogeneous treatment effects conditioning on the different characteristics\nof individuals has emerged from research fields such as personalized\nhealthcare, social science, and online marketing. To meet the need, researchers\nand practitioners from different communities have developed algorithms by\ntaking the treatment effect heterogeneity modeling approach and the uplift\nmodeling approach, respectively. In this paper, we provide a unified survey of\nthese two seemingly disconnected yet closely related approaches under the\npotential outcome framework. We then provide a structured survey of existing\nmethods by emphasizing on their inherent connections with a set of unified\nnotations to make comparisons of the different methods easy. We then review the\nmain applications of the surveyed methods in personalized marketing,\npersonalized medicine, and social studies. Finally, we summarize the existing\nsoftware packages and present discussions based on the use of methods on\nsynthetic, semi-synthetic and real world data sets and provide some general\nguidelines for choosing methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 02:16:02 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 13:26:40 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhang", "Weijia", ""], ["Li", "Jiuyong", ""], ["Liu", "Lin", ""]]}, {"id": "2007.12778", "submitter": "Rafael Stern", "authors": "Rafael Izbicki, Gilson Shimizu, Rafael B. Stern", "title": "CD-split and HPD-split: efficient conformal regions in high dimensions", "comments": "31 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal methods create prediction bands that control average coverage\nassuming solely i.i.d. data. Although the literature has mostly focused on\nprediction intervals, more general regions can often better represent\nuncertainty. For instance, a bimodal target is better represented by the union\nof two intervals. Such prediction regions are obtained by CD-split , which\ncombines the split method and a data-driven partition of the feature space\nwhich scales to high dimensions. CD-split however contains many tuning\nparameters, and their role is not clear. In this paper, we provide new insights\non CD-split by exploring its theoretical properties. In particular, we show\nthat CD-split converges asymptotically to the oracle highest predictive density\nset and satisfies local and asymptotic conditional validity. We also present\nsimulations which show how to tune CD-split. Finally, we introduce HPD-split, a\nvariation of CD-split that requires less tuning, and show that it shares the\nsame theoretical guarantees as CD-split. In a wide variety of our simulations,\nCD-split and HPD-split have a better conditional coverage and yield smaller\nprediction regions than other methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 21:42:34 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 18:46:30 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Izbicki", "Rafael", ""], ["Shimizu", "Gilson", ""], ["Stern", "Rafael B.", ""]]}, {"id": "2007.12805", "submitter": "Andrej Srakar", "authors": "Andrej Srakar (1 and 2) and Marilena Vecco (3) ((1) Institute for\n  Economic Research (IER), (2) School of Economics and Business, University of\n  Ljubljana, (3) Burgundy School of Business - Universit\\'e Bourgogne\n  Franche-Comt\\'e)", "title": "New clustering approach for symbolic polygonal data: application to the\n  clustering of entrepreneurial regimes", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entrepreneurial regimes are topic, receiving ever more research attention.\nExisting studies on entrepreneurial regimes mainly use common methods from\nmultivariate analysis and some type of institutional related analysis. In our\nanalysis, the entrepreneurial regimes is analyzed by applying a novel polygonal\nsymbolic data cluster analysis approach. Considering the diversity of data\nstructures in Symbolic Data Analysis (SDA), interval-valued data is the most\npopular. Yet, this approach requires assuming equidistribution hypothesis. We\nuse a novel polygonal cluster analysis approach to address this limitation with\nadditional advantages: to store more information, to significantly reduce large\ndata sets preserving the classical variability through polygon radius, and to\nopen new possibilities in symbolic data analysis. We construct a dynamic\ncluster analysis algorithm for this type of data with proving main theorems and\nlemmata to justify its usage. In the empirical part we use dataset of Global\nEntrepreneurship Monitor (GEM) for year 2015 to construct typologies of\ncountries based on responses to main entrepreneurial questions. The article\npresents a novel approach to clustering in statistical theory (with novel type\nof variables never accounted for) and application to a pressing issue in\nentrepreneurship with novel results.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 23:47:50 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Srakar", "Andrej", "", "1 and 2"], ["Vecco", "Marilena", ""]]}, {"id": "2007.12807", "submitter": "Boyu Ren", "authors": "Boyu Ren, Prasad Patil, Francesca Dominici, Giovanni Parmigiani,\n  Lorenzo Trippa", "title": "Cross-study learning for generalist and specialist predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration and use of data from multiple studies, for the development of\nprediction models is an important task in several scientific fields. We propose\na framework for generalist and specialist predictions that leverages multiple\ndatasets, with potential differences in the relationships between predictors\nand outcomes. Our framework uses stacking, and it includes three major\ncomponents: 1) an ensemble of prediction models trained on one or more\ndatasets, 2) task-specific utility functions and 3) a no-data-reuse technique\nfor estimating stacking weights. We illustrate that under mild regularity\nconditions the framework produces stacked PFs with oracle properties. In\nparticular we show that the the stacking weights are nearly optimal. We also\ncharacterize the scenario where the proposed no-data-reuse technique increases\nprediction accuracy compared to stacking with data reuse in a special case.We\nperform a simulation study to illustrate these results. We apply our framework\nto predict mortality using a collection of datasets on long-term exposure to\nair pollutants.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 23:51:18 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 04:43:47 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 14:45:16 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ren", "Boyu", ""], ["Patil", "Prasad", ""], ["Dominici", "Francesca", ""], ["Parmigiani", "Giovanni", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "2007.12852", "submitter": "Mingyuan Zhou", "authors": "Rahi Kalantari and Mingyuan Zhou", "title": "Graph Gamma Process Generalized Linear Dynamical Systems", "comments": "36 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce graph gamma process (GGP) linear dynamical systems to model\nreal-valued multivariate time series. For temporal pattern discovery, the\nlatent representation under the model is used to decompose the time series into\na parsimonious set of multivariate sub-sequences. In each sub-sequence,\ndifferent data dimensions often share similar temporal patterns but may exhibit\ndistinct magnitudes, and hence allowing the superposition of all sub-sequences\nto exhibit diverse behaviors at different data dimensions. We further\ngeneralize the proposed model by replacing the Gaussian observation layer with\nthe negative binomial distribution to model multivariate count time series.\nGenerated from the proposed GGP is an infinite dimensional directed sparse\nrandom graph, which is constructed by taking the logical OR operation of\ncountably infinite binary adjacency matrices that share the same set of\ncountably infinite nodes. Each of these adjacency matrices is associated with a\nweight to indicate its activation strength, and places a finite number of edges\nbetween a finite subset of nodes belonging to the same node community. We use\nthe generated random graph, whose number of nonzero-degree nodes is finite, to\ndefine both the sparsity pattern and dimension of the latent state transition\nmatrix of a (generalized) linear dynamical system. The activation strength of\neach node community relative to the overall activation strength is used to\nextract a multivariate sub-sequence, revealing the data pattern captured by the\ncorresponding community. On both synthetic and real-world time series, the\nproposed nonparametric Bayesian dynamic models, which are initialized at\nrandom, consistently exhibit good predictive performance in comparison to a\nvariety of baseline models, revealing interpretable latent state transition\npatterns and decomposing the time series into distinctly behaved sub-sequences.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 04:16:34 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kalantari", "Rahi", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2007.12922", "submitter": "Shu Yang", "authors": "Shu Yang, Donglin Zeng, and Xiaofei Wang", "title": "Improved Inference for Heterogeneous Treatment Effects Using Real-World\n  Data Subject to Hidden Confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The heterogeneity of treatment effect (HTE) lies at the heart of precision\nmedicine. Randomized clinical trials (RCTs) are gold-standard to estimate the\nHTE but are typically underpowered. While real-world data (RWD) have large\npredictive power but are often confounded due to lack of randomization of\ntreatment. In this article, we show that the RWD, even subject to hidden\nconfounding, may be used to empower RCTs in estimating the HTE using the\nconfounding function. The confounding function summarizes the impact of\nunmeasured confounders on the difference in the potential outcome between the\ntreated and untreated groups, accounting for the observed covariates, which is\nunidentifiable based only on the RWD. Coupling the RCT and RWD, we show that\nthe HTE and confounding function are identifiable. We then derive the\nsemiparametric efficient scores and integrative estimators of the HTE and\nconfounding function. We clarify the conditions under which the integrative\nestimator of the HTE is strictly more efficient than the RCT estimator. As a\nby-product, our framework can be used to generalize the average treatment\neffects from the RCT to a target population without requiring an overlap\ncovariate distribution assumption between the RCT and RWD. We illustrate the\nintegrative estimators with a simulation study and an application.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 12:57:30 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Yang", "Shu", ""], ["Zeng", "Donglin", ""], ["Wang", "Xiaofei", ""]]}, {"id": "2007.12973", "submitter": "Youjin Lee", "authors": "Youjin Lee, Edward H. Kennedy, and Nandita Mitra", "title": "Doubly Robust Nonparametric Instrumental Variable Estimators for\n  Survival Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable (IV) methods allow us the opportunity to address\nunmeasured confounding in causal inference. However, most IV methods are only\napplicable to discrete or continuous outcomes with very few IV methods for\ncensored survival outcomes. In this work we propose nonparametric estimators\nfor the local average treatment effect on survival probabilities under both\nnonignorable and ignorable censoring. We provide an efficient influence\nfunction-based estimator and a simple estimation procedure when the IV is\neither binary or continuous. The proposed estimators possess double-robustness\nproperties and can easily incorporate nonparametric estimation using machine\nlearning tools. In simulation studies, we demonstrate the flexibility and\nefficiency of our proposed estimators under various plausible scenarios. We\napply our method to the Prostate, Lung, Colorectal, and Ovarian Cancer\nScreening Trial for estimating the causal effect of screening on survival\nprobabilities and investigate the causal contrasts between the two\ninterventions under different censoring assumptions.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 16:50:59 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 18:14:05 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Lee", "Youjin", ""], ["Kennedy", "Edward H.", ""], ["Mitra", "Nandita", ""]]}, {"id": "2007.12974", "submitter": "Andrew Yiu", "authors": "Andrew Yiu (1), Robert J. B. Goudie (1), Stephen J. Sharp (2), Paul J.\n  Newcombe (1) and Brian D. M. Tom (1) ((1) MRC Biostatistics Unit, University\n  of Cambridge, UK, (2) MRC Epidemiology Unit, University of Cambridge, UK)", "title": "A Bayesian framework for case-cohort Cox regression: application to\n  dietary epidemiology", "comments": "34 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The case-cohort study design bypasses resource constraints by collecting\ncertain expensive covariates for only a small subset of the full cohort.\nWeighted Cox regression is the most widely used approach for analyzing\ncase-cohort data within the Cox model, but is inefficient. Alternative\napproaches based on multiple imputation and nonparametric maximum likelihood\nsuffer from incompatibility and computational issues respectively. We introduce\na novel Bayesian framework for case-cohort Cox regression that avoids the\naforementioned problems. Users can include auxiliary variables to help predict\nthe unmeasured expensive covariates with a prediction model of their choice,\nwhile the models for the nuisance parameters are nonparametrically specified\nand integrated out. Posterior sampling can be carried out using procedures\nbased on the pseudo-marginal MCMC algorithm. The method scales effectively to\nlarge, complex datasets, as demonstrated in our application: investigating the\nassociations between saturated fatty acids and type 2 diabetes using the\nEPIC-Norfolk study. As part of our analysis, we also develop a new approach for\nhandling compositional data in the Cox model, leading to more reliable and\ninterpretable results compared to previous studies. The performance of our\nmethod is illustrated with extensive simulations, including the use of\nsynthetic data generated by resampling from the application dataset. The code\nused to produce the results in this paper can be found at\nhttps://github.com/andrewyiu/bayes_cc .\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 16:52:43 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Yiu", "Andrew", ""], ["Goudie", "Robert J. B.", ""], ["Sharp", "Stephen J.", ""], ["Newcombe", "Paul J.", ""], ["Tom", "Brian D. M.", ""]]}, {"id": "2007.13037", "submitter": "Celso Cabral", "authors": "C. R. B. Cabral, N. L. de Souza, J. Le\\~ao", "title": "Bayesian Measurement Error Models Using Finite Mixtures of Scale\n  Mixtures of Skew-Normal Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a proposal to deal with the non-normality issue in the context of\nregression models with measurement errors when both the response and the\nexplanatory variable are observed with error. We extend the normal model by\njointly modeling the unobserved covariate and the random errors by a finite\nmixture of scale mixture of skew-normal distributions. This approach allows us\nto model data with great flexibility, accommodating skewness, heavy tails, and\nmulti-modality.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 00:29:50 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Cabral", "C. R. B.", ""], ["de Souza", "N. L.", ""], ["Le\u00e3o", "J.", ""]]}, {"id": "2007.13046", "submitter": "Jacques Balayla", "authors": "Jacques Balayla", "title": "Derivation of Generalized Equations for the Predictive Value of\n  Sequential Screening Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using Bayes' Theorem, we derive generalized equations to determine the\npositive and negative predictive value of screening tests undertaken\nsequentially. Where a is the sensitivity, b is the specificity, $\\phi$ is the\npre-test probability, the combined positive predictive value, $\\rho(\\phi)$, of\n$n$ serial positive tests, is described by:\n  $\\rho(\\phi) =\n\\frac{\\phi\\displaystyle\\prod_{i=1}^{n}a_n}{\\phi\\displaystyle\\prod_{i=1}^{n}a_n+(1-\\phi)\\displaystyle\\prod_{i=1}^{n}(1-b_n)}$\n  If the positive serial iteration is interrupted at term position $n_i-k$ by a\nconflicting negative result, then the resulting negative predictive value is\ngiven by:\n  $\\psi(\\phi) =\n\\frac{[(1-\\phi)b_{n-}]\\displaystyle\\prod_{i=b_{1+}}^{b_{(n-1)+}}(1-b_{n+})}{[\\phi(1-a_{n-})]\\displaystyle\\prod_{i=a_{1+}}^{a_{(n-1)+}}a_{n+}+[(1-\\phi)b_{n-}]\\displaystyle\\prod_{i=b_{1+}}^{b_{(n-1)+}}(1-b_{n+})}$\n  Finally, if the negative serial iteration is interrupted at term position\n$n_i-k$ by a conflicting positive result, then the resulting positive\npredictive value is given by: $\\lambda(\\phi)= \\frac{\\phi\na_{n+}\\displaystyle\\prod_{i=a_{1-}}^{a_{(n-1)-}}(1-a_{n-})}{\\phi\na_{n+}\\displaystyle\\prod_{i=a_{1-}}^{a_{(n-1)-}}(1-a_{n-})+[(1-\\phi)(1-b_{n+})]\\displaystyle\\prod_{i=b_{1-}}^{b_{(n-1)-}}b_{n-}}$\n  The aforementioned equations provide a measure of the predictive value in\ndifferent possible scenarios in which serial testing is undertaken. Their\nclinical utility is best observed in conditions with low pre-test probability\nwhere single tests are insufficient to achieve clinically significant\npredictive values and likewise, in clinical scenarios with a high pre-test\nprobability where confirmation of disease status is critical.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 02:58:08 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Balayla", "Jacques", ""]]}, {"id": "2007.13096", "submitter": "Hajime Kawahara", "authors": "Hajime Kawahara, Kento Masuda", "title": "Bayesian Dynamic Mapping of an Exo-Earth from Photometric Variability", "comments": "23 pages, 13 figures, accepted for publication in ApJ. The code is\n  available online at https://github.com/HajimeKawahara/sot . The retrieved\n  dynamic map of a real light curve of the Earth (movie) can be viewed at\n  https://www.youtube.com/watch?v=rGMWbAUAv4Y", "journal-ref": null, "doi": "10.3847/1538-4357/aba95e", "report-no": null, "categories": "astro-ph.EP astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photometric variability of a directly imaged exo-Earth conveys spatial\ninformation on its surface and can be used to retrieve a two-dimensional\ngeography and axial tilt of the planet (spin-orbit tomography). In this study,\nwe relax the assumption of the static geography and present a computationally\ntractable framework for dynamic spin-orbit tomography applicable to the\ntime-varying geography. First, a Bayesian framework of static spin-orbit\ntomography is revisited using analytic expressions of the Bayesian inverse\nproblem with a Gaussian prior. We then extend this analytic framework to a\ntime-varying one through a Gaussian process in time domain, and present\nanalytic expressions that enable efficient sampling from a full joint posterior\ndistribution of geography, axial tilt, spin rotation period, and\nhyperparameters in the Gaussian-process priors. Consequently, it only takes 0.3\ns for a laptop computer to sample one posterior dynamic map conditioned on the\nother parameters with 3,072 pixels and 1,024 time grids, for a total of $\\sim 3\n\\times 10^6$ parameters. We applied our dynamic mapping method on a toy model\nand found that the time-varying geography was accurately retrieved along with\nthe axial-tilt and spin rotation period. In addition, we demonstrated the use\nof dynamic spin-orbit tomography with a real multi-color light curve of the\nEarth as observed by the Deep Space Climate Observatory. We found that the\nresultant snapshots from the dominant component of a principle component\nanalysis roughly captured the large-scale, seasonal variations of the clear-sky\nand cloudy areas on the Earth.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 10:34:40 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Kawahara", "Hajime", ""], ["Masuda", "Kento", ""]]}, {"id": "2007.13275", "submitter": "Lars Vilhuber", "authors": "Kevin L. McKinney and Andrew S. Green and Lars Vilhuber and John M.\n  Abowd", "title": "Total Error and Variability Measures for the Quarterly Workforce\n  Indicators and LEHD Origin-Destination Employment Statistics in OnTheMap", "comments": null, "journal-ref": null, "doi": "10.1093/jssam/smaa029", "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We report results from the first comprehensive total quality evaluation of\nfive major indicators in the U.S. Census Bureau's Longitudinal\nEmployer-Household Dynamics (LEHD) Program Quarterly Workforce Indicators\n(QWI): total flow-employment, beginning-of-quarter employment, full-quarter\nemployment, average monthly earnings of full-quarter employees, and total\nquarterly payroll. Beginning-of-quarter employment is also the main tabulation\nvariable in the LEHD Origin-Destination Employment Statistics (LODES) workplace\nreports as displayed in OnTheMap (OTM), including OnTheMap for Emergency\nManagement. We account for errors due to coverage; record-level non-response;\nedit and imputation of item missing data; and statistical disclosure\nlimitation. The analysis reveals that the five publication variables under\nstudy are estimated very accurately for tabulations involving at least 10 jobs.\nTabulations involving three to nine jobs are a transition zone, where cells may\nbe fit for use with caution. Tabulations involving one or two jobs, which are\ngenerally suppressed on fitness-for-use criteria in the QWI and synthesized in\nLODES, have substantial total variability but can still be used to estimate\nstatistics for untabulated aggregates as long as the job count in the aggregate\nis more than 10.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 02:15:29 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["McKinney", "Kevin L.", ""], ["Green", "Andrew S.", ""], ["Vilhuber", "Lars", ""], ["Abowd", "John M.", ""]]}, {"id": "2007.13553", "submitter": "Julien Bect", "authors": "R\\'emi Stroh (LNE, L2S), Julien Bect (L2S, GdR MASCOT-NUM), S\\'everine\n  Demeyer (LNE), Nicolas Fischer (LNE), Damien Marquis (LNE), Emmanuel Vazquez\n  (L2S, GdR MASCOT-NUM)", "title": "Sequential design of multi-fidelity computer experiments: maximizing the\n  rate of stepwise uncertainty reduction", "comments": "Technometrics, Taylor & Francis", "journal-ref": null, "doi": "10.1080/00401706.2021.1935324", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with the sequential design of experiments for\n(deterministic or stochastic) multi-fidelity numerical simulators, that is,\nsimulators that offer control over the accuracy of simulation of the physical\nphenomenon or system under study. Very often, accurate simulations correspond\nto high computational efforts whereas coarse simulations can be obtained at a\nsmaller cost. In this setting, simulation results obtained at several levels of\nfidelity can be combined in order to estimate quantities of interest (the\noptimal value of the output, the probability that the output exceeds a given\nthreshold...) in an efficient manner. To do so, we propose a new Bayesian\nsequential strategy called Maximal Rate of Stepwise Uncertainty Reduction\n(MR-SUR), that selects additional simulations to be performed by maximizing the\nratio between the expected reduction of uncertainty and the cost of simulation.\nThis generic strategy unifies several existing methods, and provides a\nprincipled approach to develop new ones. We assess its performance on several\nexamples, including a computationally intensive problem of fire safety analysis\nwhere the quantity of interest is the probability of exceeding a tenability\nthreshold during a building fire.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 13:34:12 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 13:37:24 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Stroh", "R\u00e9mi", "", "LNE, L2S"], ["Bect", "Julien", "", "L2S, GdR MASCOT-NUM"], ["Demeyer", "S\u00e9verine", "", "LNE"], ["Fischer", "Nicolas", "", "LNE"], ["Marquis", "Damien", "", "LNE"], ["Vazquez", "Emmanuel", "", "L2S, GdR MASCOT-NUM"]]}, {"id": "2007.13716", "submitter": "Yuting Wei", "authors": "Michael Celentano, Andrea Montanari, Yuting Wei", "title": "The Lasso with general Gaussian designs with applications to hypothesis\n  testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lasso is a method for high-dimensional regression, which is now commonly\nused when the number of covariates $p$ is of the same order or larger than the\nnumber of observations $n$. Classical asymptotic normality theory is not\napplicable for this model due to two fundamental reasons: $(1)$ The regularized\nrisk is non-smooth; $(2)$ The distance between the estimator $\\bf\n\\widehat{\\theta}$ and the true parameters vector $\\bf \\theta^\\star$ cannot be\nneglected. As a consequence, standard perturbative arguments that are the\ntraditional basis for asymptotic normality fail.\n  On the other hand, the Lasso estimator can be precisely characterized in the\nregime in which both $n$ and $p$ are large, while $n/p$ is of order one. This\ncharacterization was first obtained in the case of standard Gaussian designs,\nand subsequently generalized to other high-dimensional estimation procedures.\nHere we extend the same characterization to Gaussian correlated designs with\nnon-singular covariance structure. This characterization is expressed in terms\nof a simpler ``fixed design'' model. We establish non-asymptotic bounds on the\ndistance between distributions of various quantities in the two models, which\nhold uniformly over signals $\\bf \\theta^\\star$ in a suitable sparsity class,\nand values of the regularization parameter.\n  As applications, we study the distribution of the debiased Lasso, and show\nthat a degrees-of-freedom correction is necessary for computing valid\nconfidence intervals.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 17:48:54 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Celentano", "Michael", ""], ["Montanari", "Andrea", ""], ["Wei", "Yuting", ""]]}, {"id": "2007.13741", "submitter": "Jing Xu", "authors": "Jing Xu, Xiaoxi Yan, Caroline Figueroa, Joseph Jay Williams, Bibhas\n  Chakraborty", "title": "Multi-Level Micro-Randomized Trial: Detecting the Proximal Effect of\n  Messages on Physical Activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological advancements in mobile devices have made it possible to deliver\nmobile health interventions to individuals. A novel intervention framework that\nemerges from such advancements is the just-in-time adaptive intervention\n(JITAI), where it aims to suggest the right support to the individual \"just in\ntime\", when their needs arise, thus having proximal, near future effects. The\nmicro-randomized trial (MRT) design was proposed recently to test the proximal\neffects of these JITAIs. In an MRT, participants are repeatedly randomized to\none of the intervention options of various in the intervention components, at a\nscale of hundreds or thousands of decision time points over the course of the\nstudy. However, the extant MRT framework only tests the proximal effect of\ntwo-level intervention components (e.g. control vs intervention). In this\npaper, we propose a novel version of MRT design with multiple levels per\nintervention component, which we call \"multi-level micro-randomized trial\"\n(MLMRT) design. The MLMRT extends the existing MRT design by allowing\nmulti-level intervention components, and the addition of more levels to the\ncomponents during the study period. We apply generalized estimating equation\ntype methodology on the longitudinal data arising from an MLMRT to develop the\nnovel test statistics for assessing the proximal effects and deriving the\nassociated sample size calculators. We conduct simulation studies to evaluate\nthe sample size calculators based on both power and precision. We have\ndeveloped an R shiny application of the sample size calculators. This proposed\ndesign is motivated by our involvement in the Diabetes and Mental Health\nAdaptive Notification Tracking and Evaluation (DIAMANTE) study. This study uses\na novel mobile application, also called \"DIAMANTE\", which delivers adaptive\ntext messages to encourage physical activity.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 14:53:01 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Xu", "Jing", ""], ["Yan", "Xiaoxi", ""], ["Figueroa", "Caroline", ""], ["Williams", "Joseph Jay", ""], ["Chakraborty", "Bibhas", ""]]}, {"id": "2007.14078", "submitter": "Tianbo Chen", "authors": "Tianbo Chen, Ying Sun, Carolina Euan and Hernando Ombao", "title": "Clustering Brain Signals: A Robust Approach Using Functional Data\n  Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze electroencephalograms (EEG) which are recordings of\nbrain electrical activity. We develop new clustering methods for identifying\nsynchronized brain regions, where the EEGs show similar oscillations or\nwaveforms according to their spectral densities. We treat the estimated\nspectral densities from many epochs or trials as functional data and develop\nclustering algorithms based on functional data ranking. The two proposed\nclustering algorithms use different dissimilarity measures: distance of the\nfunctional medians and the area of the central region. The performance of the\nproposed algorithms is examined by simulation studies. We show that, when\ncontaminations are present, the proposed methods for clustering spectral\ndensities are more robust than the mean-based methods. The developed methods\nare applied to two stages of resting state EEG data from a male college\nstudent, corresponding to early exploration of functional connectivity in the\nhuman brain.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 09:18:22 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Chen", "Tianbo", ""], ["Sun", "Ying", ""], ["Euan", "Carolina", ""], ["Ombao", "Hernando", ""]]}, {"id": "2007.14080", "submitter": "Shuang Song", "authors": "Wei Jiang, Shuang Song, Lin Hou, Hongyu Zhao", "title": "A set of efficient methods to generate high-dimensional binary data with\n  specified correlation structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional correlated binary data arise in many areas, such as observed\ngenetic variations in biomedical research. Data simulation can help researchers\nevaluate efficiency and explore properties of different computational and\nstatistical methods. Also, some statistical methods, such as Monte-Carlo\nmethods, rely on data simulation. Lunn and Davies (1998) proposed linear time\ncomplexity methods to generate correlated binary variables with three common\ncorrelation structures. However, it is infeasible to specify unequal\nprobabilities in their methods. In this manuscript, we introduce several\ncomputationally efficient algorithms that generate high-dimensional binary data\nwith specified correlation structures and unequal probabilities. Our algorithms\nhave linear time complexity with respect to the dimension for three commonly\nstudied correlation structures, namely exchangeable, decaying-product and\nK-dependent correlation structures. In addition, we extend our algorithms to\ngenerate binary data of general non-negative correlation matrices with\nquadratic time complexity. We provide an R package, CorBin, to implement our\nsimulation methods. Compared to the existing packages for binary data\ngeneration, the time cost to generate a 100-dimensional binary vector with the\ncommon correlation structures and general correlation matrices can be reduced\nup to $10^5$ folds and $10^3$ folds, respectively, and the efficiency can be\nfurther improved with the increase of dimensions. The R package CorBin is\navailable on CRAN at https://cran.r-project.org/.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 09:22:46 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Jiang", "Wei", ""], ["Song", "Shuang", ""], ["Hou", "Lin", ""], ["Zhao", "Hongyu", ""]]}, {"id": "2007.14085", "submitter": "Tianbo Chen", "authors": "Tianbo Chen, Ying Sun, Mehdi Maadooliat", "title": "Collective Spectral Density Estimation and Clustering for\n  Spatially-Correlated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a method for estimating and clustering\ntwo-dimensional spectral density functions (2D-SDFs) for spatial data from\nmultiple subregions. We use a common set of adaptive basis functions to explain\nthe similarities among the 2D-SDFs in a low-dimensional space and estimate the\nbasis coefficients by maximizing the Whittle likelihood with two penalties. We\napply these penalties to impose the smoothness of the estimated 2D-SDFs and the\nspatial dependence of the spatially-correlated subregions. The proposed\ntechnique provides a score matrix, that is comprised of the estimated\ncoefficients associated with the common set of basis functions representing the\n2D-SDFs. {Instead of clustering the estimated SDFs directly, we propose to\nemploy the score matrix for clustering purposes, taking advantage of its\nlow-dimensional property.} In a simulation study, we demonstrate that our\nproposed method outperforms other competing estimation procedures used for\nclustering. Finally, to validate the described clustering method, we apply the\nprocedure to soil moisture data from the Mississippi basin to produce\nhomogeneous spatial clusters. We produce animations to dynamically show the\nestimation procedure, including the estimated 2D-SDFs and the score matrix,\nwhich provide an intuitive illustration of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 09:43:46 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Chen", "Tianbo", ""], ["Sun", "Ying", ""], ["Maadooliat", "Mehdi", ""]]}, {"id": "2007.14109", "submitter": "Alessandro Gasparini", "authors": "Emma C. Martin and Alessandro Gasparini and Michael J. Crowther", "title": "merlin: An R package for Mixed Effects Regression for Linear, Nonlinear\n  and User-defined models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package merlin performs flexible joint modelling of hierarchical\nmulti-outcome data. Increasingly, multiple longitudinal biomarker measurements,\npossibly censored time-to-event outcomes and baseline characteristics are\navailable. However, there is limited software that allows all of this\ninformation to be incorporated into one model. In this paper, we present merlin\nwhich allows for the estimation of models with unlimited numbers of continuous,\nbinary, count and time-to-event outcomes, with unlimited levels of nested\nrandom effects. A wide variety of link functions, including the expected value,\nthe gradient and shared random effects, are available in order to link the\ndifferent outcomes in a biologically plausible way. The accompanying\npredict.merlin function allows for individual and population level predictions\nto be made from even the most complex models. There is the option to specify\nuser-defined families, making merlin ideal for methodological research. The\nflexibility of merlin is illustrated using an example in patients followed up\nafter heart valve replacement, beginning with a linear model, and finishing\nwith a joint multiple longitudinal and competing risks survival model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 10:26:00 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Martin", "Emma C.", ""], ["Gasparini", "Alessandro", ""], ["Crowther", "Michael J.", ""]]}, {"id": "2007.14190", "submitter": "Linbo Wang", "authors": "Dingke Tang, Dehan Kong, Wenliang Pan, Linbo Wang", "title": "Outcome model free causal inference with ultra-high dimensional\n  covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference has been increasingly reliant on observational studies with\nrich covariate information. To build tractable causal models, including the\npropensity score models, it is imperative to first extract important features\nfrom high dimensional data. Unlike the familiar task of variable selection for\nprediction modeling, our feature selection procedure aims to control for\nconfounding while maintaining efficiency in the resulting causal effect\nestimate. Previous empirical studies imply that one should aim to include all\npredictors of the outcome, rather than the treatment, in the propensity score\nmodel. In this paper, we formalize this intuition through rigorous proofs, and\npropose the causal ball screening for selecting these variables from modern\nultra-high dimensional data sets. A distinctive feature of our proposal is that\nwe do not require any modeling on the outcome regression, thus providing\nrobustness against misspecification of the functional form or violation of\nsmoothness conditions. Our theoretical analyses show that the proposed\nprocedure enjoys a number of oracle properties including model selection\nconsistency, normality and efficiency. Synthetic and real data analyses show\nthat our proposal performs favorably with existing methods in a range of\nrealistic settings.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:18:49 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Tang", "Dingke", ""], ["Kong", "Dehan", ""], ["Pan", "Wenliang", ""], ["Wang", "Linbo", ""]]}, {"id": "2007.14229", "submitter": "Diego Marcondes", "authors": "Diego Marcondes", "title": "Parameter estimation in dynamical systems via Statistical Learning: a\n  reinterpretation of Approximate Bayesian Computation applied to COVID-19\n  spread", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust parameter estimation method for dynamical systems based\non Statistical Learning techniques which aims to estimate a set of parameters\nthat well fit the dynamics in order to obtain robust evidences about the\nqualitative behaviour of its trajectory. The method is quite general and\nflexible, since it does not rely on any specific property of the dynamical\nsystem, and represents a reinterpretation of Approximate Bayesian Computation\nmethods through the lens of Statistical Learning. The method is specially\nuseful for estimating parameters in epidemiological compartmental models in\norder to obtain qualitative properties of a disease evolution. We apply it to\nsimulated and real data about COVID-19 spread in the US in order to evaluate\nqualitatively its evolution over time, showing how one may assess the\neffectiveness of measures implemented to slow the spread and some qualitative\nfeatures of the disease current and future evolution.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:52:20 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 18:38:05 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Marcondes", "Diego", ""]]}, {"id": "2007.14391", "submitter": "Jialei Chen", "authors": "Jialei Chen, Zhaonan Liu, Kan Wang, Chen Jiang, Chuck Zhang, Ben Wang", "title": "A calibration-free method for biosensing in cell manufacturing", "comments": null, "journal-ref": "IISE Transactions, 2020", "doi": "10.1080/24725854.2020.1856982", "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chimeric antigen receptor T cell therapy has demonstrated innovative\ntherapeutic effectiveness in fighting cancers; however, it is extremely\nexpensive due to the intrinsic patient-to-patient variability in cell\nmanufacturing. We propose in this work a novel calibration-free statistical\nframework to effectively recover critical quality attributes under the\npatient-to-patient variability. Specifically, we model this variability via a\npatient-specific calibration parameter, and use readings from multiple\nbiosensors to construct a patient-invariance statistic, thereby alleviating the\neffect of the calibration parameter. A carefully formulated optimization\nproblem and an algorithmic framework are presented to find the best\npatient-invariance statistic and the model parameters. Using the\npatient-invariance statistic, we can recover the critical quality attribute of\ninterest, free from the calibration parameter. We demonstrate improvements of\nthe proposed calibration-free method in different simulation experiments. In\nthe cell manufacturing case study, our method not only effectively recovers\nviable cell concentration for monitoring, but also reveals insights for the\ncell manufacturing process.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 22:37:56 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chen", "Jialei", ""], ["Liu", "Zhaonan", ""], ["Wang", "Kan", ""], ["Jiang", "Chen", ""], ["Zhang", "Chuck", ""], ["Wang", "Ben", ""]]}, {"id": "2007.14458", "submitter": "Linbo Wang", "authors": "Linbo Wang, Yuexia Zhang, Thomas S. Richardson, James M. Robins", "title": "Estimation of local treatment under the binary instrumental variable\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables are widely used to deal with unmeasured confounding in\nobservational studies and imperfect randomized controlled trials. In these\nstudies, researchers often target the so-called local average treatment effect\nas it is identifiable under mild conditions. In this paper, we consider\nestimation of the local average treatment effect under the binary instrumental\nvariable model. We discuss the challenges for causal estimation with a binary\noutcome, and show that surprisingly, it can be more difficult than the case\nwith a continuous outcome. We propose novel modeling and estimating procedures\nthat improve upon existing proposals in terms of model congeniality,\ninterpretability, robustness or efficiency. Our approach is illustrated via\nsimulation studies and a real data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 20:03:23 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wang", "Linbo", ""], ["Zhang", "Yuexia", ""], ["Richardson", "Thomas S.", ""], ["Robins", "James M.", ""]]}, {"id": "2007.14495", "submitter": "Peter Rousseeuw", "authors": "Jakob Raymaekers, Peter J. Rousseeuw, Mia Hubert", "title": "Class maps for visualizing classification results", "comments": "Appeared online, Technometrics", "journal-ref": null, "doi": "10.1080/00401706.2021.1927849", "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is a major tool of statistics and machine learning. A\nclassification method first processes a training set of objects with given\nclasses (labels), with the goal of afterward assigning new objects to one of\nthese classes. When running the resulting prediction method on the training\ndata or on test data, it can happen that an object is predicted to lie in a\nclass that differs from its given label. This is sometimes called label bias,\nand raises the question whether the object was mislabeled. The proposed class\nmap reflects the probability that an object belongs to an alternative class,\nhow far it is from the other objects in its given class, and whether some\nobjects lie far from all classes. The goal is to visualize aspects of the\nclassification results to obtain insight in the data. The display is\nconstructed for discriminant analysis, the k-nearest neighbor classifier,\nsupport vector machines, logistic regression, and coupling pairwise\nclassifications. It is illustrated on several benchmark datasets, including\nsome about images and texts.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 21:27:15 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 15:34:25 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 13:51:29 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Raymaekers", "Jakob", ""], ["Rousseeuw", "Peter J.", ""], ["Hubert", "Mia", ""]]}, {"id": "2007.14820", "submitter": "Srijan Sengupta", "authors": "Anirban Dasgupta and Srijan Sengupta", "title": "Scalable Estimation of Epidemic Thresholds via Node Sampling", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious or contagious diseases can be transmitted from one person to\nanother through social contact networks. In today's interconnected global\nsociety, such contagion processes can cause global public health hazards, as\nexemplified by the ongoing Covid-19 pandemic. It is therefore of great\npractical relevance to investigate the network trans-mission of contagious\ndiseases from the perspective of statistical inference. An important and widely\nstudied boundary condition for contagion processes over networks is the\nso-called epidemic threshold. The epidemic threshold plays a key role in\ndetermining whether a pathogen introduced into a social contact network will\ncause an epidemic or die out. In this paper, we investigate epidemic thresholds\nfrom the perspective of statistical network inference. We identify two major\nchallenges that are caused by high computational and sampling complexity of the\nepidemic threshold. We develop two statistically accurate and computationally\nefficient approximation techniques to address these issues under the Chung-Lu\nmodeling framework. The second approximation, which is based on random walk\nsampling, further enjoys the advantage of requiring data on a vanishingly small\nfraction of nodes. We establish theoretical guarantees for both methods and\ndemonstrate their empirical superiority.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 14:54:06 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Dasgupta", "Anirban", ""], ["Sengupta", "Srijan", ""]]}, {"id": "2007.14845", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Jeffrey W. Miller", "title": "Robust and Reproducible Model Selection Using Bagged Posteriors", "comments": "arXiv admin note: substantial text overlap with arXiv:1912.07104", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model selection is premised on the assumption that the data are\ngenerated from one of the postulated models. However, in many applications, all\nof these models are incorrect, which is known as model misspecification. When\nthe models are misspecified, two or more models can provide a nearly equally\ngood fit to the data, in which case Bayesian model selection can be highly\nunstable, potentially leading to self-contradictory findings. To remedy this\ninstability, we explore instead using bagging on the posterior distribution\n(\"BayesBag\") when performing model selection -- that is, averaging the\nposterior model probabilities over many bootstrapped datasets. We provide\ntheoretical results characterizing the asymptotic behavior of the standard\nposterior and the BayesBag posterior under misspecification, in the model\nselection setting. We empirically assess the BayesBag approach on synthetic and\nreal-world data in (i) feature selection for linear regression and (ii)\nphylogenetic tree reconstruction. Our theory and experiments show that, when\nall models are misspecified, BayesBag provides (a) greater reproducibility and\n(b) greater accuracy in selecting the correct model, compared to the standard\nBayesian posterior; on the other hand, under correct specification, BayesBag is\nslightly more conservative than the standard posterior, in the sense that\nBayesBag posterior probabilities tend to be slightly farther from the extremes\nof zero and one. Overall, our results demonstrate that BayesBag provides an\neasy-to-use and widely applicable approach that improves upon standard Bayesian\nmodel selection by making it more stable and reproducible.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 19:23:17 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 19:02:26 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Miller", "Jeffrey W.", ""]]}, {"id": "2007.14874", "submitter": "Lennart Oelschl\\\"ager", "authors": "Lennart Oelschl\\\"ager and Timo Adam", "title": "Detecting bearish and bullish markets in financial time series using\n  hierarchical hidden Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial markets exhibit alternating periods of rising and falling prices.\nStock traders seeking to make profitable investment decisions have to account\nfor those trends, where the goal is to accurately predict switches from bullish\ntowards bearish markets and vice versa. Popular tools for modeling financial\ntime series are hidden Markov models, where a latent state process is used to\nexplicitly model switches among different market regimes. In their basic form,\nhowever, hidden Markov models are not capable of capturing both short- and\nlong-term trends, which can lead to a misinterpretation of short-term price\nfluctuations as changes in the long-term trend. In this paper, we demonstrate\nhow hierarchical hidden Markov models can be used to draw a comprehensive\npicture of financial markets, which can contribute to the development of more\nsophisticated trading strategies. The feasibility of the suggested approach is\nillustrated in two real-data applications, where we model data from two major\nstock indices, the Deutscher Aktienindex and the Standard & Poor's 500.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 14:50:00 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Oelschl\u00e4ger", "Lennart", ""], ["Adam", "Timo", ""]]}, {"id": "2007.14900", "submitter": "Ioannis Kontoyiannis", "authors": "Ioannis Kontoyiannis, Lambros Mertzanis, Athina Panotopoulou, Ioannis\n  Papageorgiou, and Maria Skoularidou", "title": "Bayesian Context Trees: Modelling and exact inference for discrete time\n  series", "comments": "50 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new Bayesian modelling framework for the class of higher-order,\nvariable-memory Markov chains, and introduce an associated collection of\nmethodological tools for exact inference with discrete time series. We show\nthat a version of the context tree weighting algorithm can compute the prior\npredictive likelihood exactly (averaged over both models and parameters), and\ntwo related algorithms are introduced, which identify the a posteriori most\nlikely models and compute their exact posterior probabilities. All three\nalgorithms are deterministic and have linear-time complexity. A family of\nvariable-dimension Markov chain Monte Carlo samplers is also provided,\nfacilitating further exploration of the posterior. The performance of the\nproposed methods in model selection, Markov order estimation and prediction is\nillustrated through simulation experiments and real-world applications with\ndata from finance, genetics, neuroscience, and animal communication.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:16:49 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Kontoyiannis", "Ioannis", ""], ["Mertzanis", "Lambros", ""], ["Panotopoulou", "Athina", ""], ["Papageorgiou", "Ioannis", ""], ["Skoularidou", "Maria", ""]]}, {"id": "2007.14919", "submitter": "Jirong Yi", "authors": "Jirong Yi, Myung Cho, Xiaodong Wu, Weiyu Xu, and Raghu Mudumbai", "title": "Error Correction Codes for COVID-19 Virus and Antibody Testing: Using\n  Pooled Testing to Increase Test Reliability", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a novel method to increase the reliability of COVID-19 virus or\nantibody tests by using specially designed pooled testings. Instead of testing\nnasal swab or blood samples from individual persons, we propose to test\nmixtures of samples from many individuals. The pooled sample testing method\nproposed in this paper also serves a different purpose: for increasing test\nreliability and providing accurate diagnoses even if the tests themselves are\nnot very accurate. Our method uses ideas from compressed sensing and\nerror-correction coding to correct for a certain number of errors in the test\nresults. The intuition is that when each individual's sample is part of many\npooled sample mixtures, the test results from all of the sample mixtures\ncontain redundant information about each individual's diagnosis, which can be\nexploited to automatically correct for wrong test results in exactly the same\nway that error correction codes correct errors introduced in noisy\ncommunication channels. While such redundancy can also be achieved by simply\ntesting each individual's sample multiple times, we present simulations and\ntheoretical arguments that show that our method is significantly more efficient\nin increasing diagnostic accuracy. In contrast to group testing and compressed\nsensing which aim to reduce the number of required tests, this proposed error\ncorrection code idea purposefully uses pooled testing to increase test\naccuracy, and works not only in the \"undersampling\" regime, but also in the\n\"oversampling\" regime, where the number of tests is bigger than the number of\nsubjects. The results in this paper run against traditional beliefs that, \"even\nthough pooled testing increased test capacity, pooled testings were less\nreliable than testing individuals separately.\"\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:52:49 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Yi", "Jirong", ""], ["Cho", "Myung", ""], ["Wu", "Xiaodong", ""], ["Xu", "Weiyu", ""], ["Mudumbai", "Raghu", ""]]}, {"id": "2007.14961", "submitter": "Mario Beraha", "authors": "Mario Beraha, Matteo Pegoraro, Riccardo Peli and Alessandra Guglielmi", "title": "Spatially dependent mixture models via the Logistic Multivariate CAR\n  prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of spatially dependent areal data, where for each\narea independent observations are available, and propose to model the density\nof each area through a finite mixture of Gaussian distributions. The spatial\ndependence is introduced via a novel joint distribution for a collection of\nvectors in the simplex, that we term logisticMCAR. We show that salient\nfeatures of the logisticMCAR distribution can be described analytically, and\nthat a suitable augmentation scheme based on the P\\'olya-Gamma identity allows\nto derive an efficient Markov Chain Monte Carlo algorithm. When compared to\ncompetitors, our model has proved to better estimate densities in different\n(disconnected) areal locations when they have different characteristics. We\ndiscuss an application on a real dataset of Airbnb listings in the city of\nAmsterdam, also showing how to easily incorporate for additional covariate\ninformation in the model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 17:09:34 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 14:35:36 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Beraha", "Mario", ""], ["Pegoraro", "Matteo", ""], ["Peli", "Riccardo", ""], ["Guglielmi", "Alessandra", ""]]}, {"id": "2007.14971", "submitter": "Maryna Prus", "authors": "Maryna Prus", "title": "Equivalence theorems for compound design problems with application in\n  mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we consider design criteria which depend on several\ndesigns simultaneously. We formulate equivalence theorems based on moment\nmatrices (if criteria depend on designs via moment matrices) or with respect to\nthe designs themselves (for finite design regions). We apply the obtained\noptimality conditions to the multiple-group random coefficient regression\nmodels and illustrate the results by simple examples.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 17:31:41 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Prus", "Maryna", ""]]}, {"id": "2007.15039", "submitter": "Elizabeth Pei-Ting Chou", "authors": "Elizabeth Chou, Catie McVey, Yin-Chen Hsieh, Sabrina Enriquez, Fushing\n  Hsieh", "title": "Extreme-K categorical samples problem", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With histograms as its foundation, we develop Categorical Exploratory Data\nAnalysis (CEDA) under the extreme-$K$ sample problem, and illustrate its\nuniversal applicability through four 1D categorical datasets. Given a sizable\n$K$, CEDA's ultimate goal amounts to discover by data's information content via\ncarrying out two data-driven computational tasks: 1) establish a tree geometry\nupon $K$ populations as a platform for discovering a wide spectrum of patterns\namong populations; 2) evaluate each geometric pattern's reliability. In CEDA\ndevelopments, each population gives rise to a row vector of categories\nproportions. Upon the data matrix's row-axis, we discuss the pros and cons of\nEuclidean distance against its weighted version for building a binary\nclustering tree geometry. The criterion of choice rests on degrees of\nuniformness in column-blocks framed by this binary clustering tree. Each\ntree-leaf (population) is then encoded with a binary code sequence, so is\ntree-based pattern. For evaluating reliability, we adopt row-wise multinomial\nrandomness to generate an ensemble of matrix mimicries, so an ensemble of\nmimicked binary trees. Reliability of any observed pattern is its recurrence\nrate within the tree ensemble. A high reliability value means a deterministic\npattern. Our four applications of CEDA illuminate four significant aspects of\nextreme-$K$ sample problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 18:12:48 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Chou", "Elizabeth", ""], ["McVey", "Catie", ""], ["Hsieh", "Yin-Chen", ""], ["Enriquez", "Sabrina", ""], ["Hsieh", "Fushing", ""]]}, {"id": "2007.15043", "submitter": "Alex Fout", "authors": "Alex Fout, Bailey K. Fosdick, Matthew P. Hitt", "title": "Non-Uniform Sampling of Fixed Margin Binary Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Data sets in the form of binary matrices are ubiquitous across scientific\ndomains, and researchers are often interested in identifying and quantifying\nnoteworthy structure. One approach is to compare the observed data to that\nwhich might be obtained under a null model. Here we consider sampling from the\nspace of binary matrices which satisfy a set of marginal row and column sums.\nWhereas existing sampling methods have focused on uniform sampling from this\nspace, we introduce modified versions of two elementwise swapping algorithms\nwhich sample according to a non-uniform probability distribution defined by a\nweight matrix, which gives the relative probability of a one for each entry. We\ndemonstrate that values of zero in the weight matrix, i.e. structural zeros,\nare generally problematic for swapping algorithms, except when they have\nspecial monotonic structure. We explore the properties of our algorithms\nthrough simulation studies, and illustrate the potential impact of employing a\nnon-uniform null model using a classic bird habitation dataset.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 18:29:08 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 00:54:09 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2020 22:19:57 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2020 20:49:17 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Fout", "Alex", ""], ["Fosdick", "Bailey K.", ""], ["Hitt", "Matthew P.", ""]]}, {"id": "2007.15086", "submitter": "Fernanda Lang Schumacher", "authors": "Fernanda L. Schumacher, Dipak K. Dey, Victor H. Lachos", "title": "Approximate inferences for nonlinear mixed effects models with scale\n  mixtures of skew-normal distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear mixed effects models have received a great deal of attention in the\nstatistical literature in recent years because of their flexibility in handling\nlongitudinal studies, including human immunodeficiency virus viral dynamics,\npharmacokinetic analyses, and studies of growth and decay. A standard\nassumption in nonlinear mixed effects models for continuous responses is that\nthe random effects and the within-subject errors are normally distributed,\nmaking the model sensitive to outliers. We present a novel class of asymmetric\nnonlinear mixed effects models that provides efficient parameters estimation in\nthe analysis of longitudinal data. We assume that, marginally, the random\neffects follow a multivariate scale mixtures of skew--normal distribution and\nthat the random errors follow a symmetric scale mixtures of normal\ndistribution, providing an appealing robust alternative to the usual normal\ndistribution. We propose an approximate method for maximum likelihood\nestimation based on an EM-type algorithm that produces approximate maximum\nlikelihood estimates and significantly reduces the numerical difficulties\nassociated with the exact maximum likelihood estimation. Techniques for\nprediction of future responses under this class of distributions are also\nbriefly discussed. The methodology is illustrated through an application to\nTheophylline kinetics data and through some simulating studies.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 20:14:24 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Schumacher", "Fernanda L.", ""], ["Dey", "Dipak K.", ""], ["Lachos", "Victor H.", ""]]}, {"id": "2007.15137", "submitter": "Gabriela Ciuperca", "authors": "Gabriela Ciuperca", "title": "Real-time detection of a change-point in a linear expectile model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we address the real-time detection problem of a\nchange-point in the coefficients of a linear model with the possibility that\nthe model errors are asymmetrical and that the explanatory variables number is\nlarge. We build test statistics based on the cumulative sum (CUSUM) of the\nexpectile function derivatives calculated on the residuals obtained by the\nexpectile and adaptive LASSO expectile estimation methods. The asymptotic\ndistribution of these statistics are obtained under the hypothesis that the\nmodel does not change. Moreover, we prove that they diverge when the model\nchanges at an unknown observation. The asymptotic study of the test statistics\nunder these two hypotheses allows us to find the asymptotic critical region and\nthe stopping time, that is the observation where the model will change. The\nempirical performance is investigated by a comparative simulation study with\nother statistics of CUSUM type. Two examples on real data are also presented to\ndemonstrate its interest in practice.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 22:18:00 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Ciuperca", "Gabriela", ""]]}, {"id": "2007.15179", "submitter": "Linchuan Xu", "authors": "Kenji Yamanishi, Linchuan Xu, Ryo Yuki, Shintaro Fukushima, Chuan-hao\n  Lin", "title": "Detecting Change Signs with Differential MDL Change Statistics for\n  COVID-19 Pandemic Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with the issue of detecting changes and their signs from a\ndata stream. For example, when given time series of COVID-19 cases in a region,\nwe may raise early warning signals of outbreaks by detecting signs of changes\nin the cases. We propose a novel methodology to address this issue. The key\nidea is to employ a new information-theoretic notion, which we call the\ndifferential minimum description length change statistics (D-MDL), for\nmeasuring the scores of change sign. We first give a fundamental theory for\nD-MDL. We then demonstrate its effectiveness using synthetic datasets. We apply\nit to detecting early warning signals of the COVID-19 epidemic. We empirically\ndemonstrate that D-MDL is able to raise early warning signals of events such as\nsignificant increase/decrease of cases. Remarkably, for about $64\\%$ of the\nevents of significant increase of cases in 37 studied countries, our method can\ndetect warning signals as early as nearly six days on average before the\nevents, buying considerably long time for making responses. We further relate\nthe warning signals to the basic reproduction number $R0$ and the timing of\nsocial distancing. The results showed that our method can effectively monitor\nthe dynamics of $R0$, and confirmed the effectiveness of social distancing at\ncontaining the epidemic in a region. We conclude that our method is a promising\napproach to the pandemic analysis from a data science viewpoint. The software\nfor the experiments is available at\nhttps://github.com/IbarakikenYukishi/differential-mdl-change-statistics. An\nonline detection system is available at\nhttps://ibarakikenyukishi.github.io/d-mdl-html/index.html\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 01:52:30 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 06:29:00 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Yamanishi", "Kenji", ""], ["Xu", "Linchuan", ""], ["Yuki", "Ryo", ""], ["Fukushima", "Shintaro", ""], ["Lin", "Chuan-hao", ""]]}, {"id": "2007.15335", "submitter": "Kostas Loumponias", "authors": "Kostas Loumponias", "title": "Coloured Tobit Kalman Filter", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the Tobit Kalman filtering (TKF) process when the\none-dimensional measurements are censored and the noises of the state-space\nmodel are coloured. Two improvements of the standard TKF process are proposed.\nFirstly, the exact moments of the censored measurements are calculated via the\nmoment generating function of the censored measurements. Secondly, coloured\nnoises are considered in the proposed method in order to tackle real-life\nproblems, where the white noises are not common. The designed process is\nevaluated using two experiments-simulations. The results show that the proposed\nmethod outperforms other methods in minimizing the Root Mean Square Error\n(RMSE) in both experiments.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 09:31:45 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Loumponias", "Kostas", ""]]}, {"id": "2007.15413", "submitter": "Carlo Sguera", "authors": "Carlo Sguera and Sara L\\'opez-Pintado", "title": "A notion of depth for sparse functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data depth is a well-known and useful nonparametric tool for analyzing\nfunctional data. It provides a novel way of ranking a sample of curves from the\ncenter outwards and defining robust statistics, such as the median or trimmed\nmeans. It has also been used as a building block for functional outlier\ndetection methods and classification. Several notions of depth for functional\ndata were introduced in the literature in the last few decades. These\nfunctional depths can only be directly applied to samples of curves measured on\na fine and common grid. In practice, this is not always the case, and curves\nare often observed at sparse and subject dependent grids. In these scenarios\nthe usual approach consists in estimating the trajectories on a common dense\ngrid, and using the estimates in the depth analysis. This approach ignores the\nuncertainty associated with the curves estimation step. Our goal is to extend\nthe notion of depth so that it takes into account this uncertainty. Using both\nfunctional estimates and their associated confidence intervals, we propose a\nnew method that allows the curve estimation uncertainty to be incorporated into\nthe depth analysis. We describe the new approach using the modified band depth\nalthough any other functional depth could be used. The performance of the\nproposed methodology is illustrated using simulated curves in different\nsettings where we control the degree of sparsity. Also a real data set\nconsisting of female medflies egg-laying trajectories is considered. The\nresults show the benefits of using uncertainty when computing depth for sparse\nfunctional data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 12:18:27 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Sguera", "Carlo", ""], ["L\u00f3pez-Pintado", "Sara", ""]]}, {"id": "2007.15421", "submitter": "Abhirup Datta", "authors": "Arkajyoti Saha, Sumanta Basu, Abhirup Datta", "title": "Random Forests for dependent data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forest (RF) is one of the most popular methods for estimating\nregression functions. The local nature of the RF algorithm, based on intra-node\nmeans and variances, is ideal when errors are i.i.d. For dependent error\nprocesses like time series and spatial settings where data in all the nodes\nwill be correlated, operating locally ignores this dependence. Also, RF will\ninvolve resampling of correlated data, violating the principles of bootstrap.\nTheoretically, consistency of RF has been established for i.i.d. errors, but\nlittle is known about the case of dependent errors.\n  We propose RF-GLS, a novel extension of RF for dependent error processes in\nthe same way Generalized Least Squares (GLS) fundamentally extends Ordinary\nLeast Squares (OLS) for linear models under dependence. The key to this\nextension is the equivalent representation of the local decision-making in a\nregression tree as a global OLS optimization which is then replaced with a GLS\nloss to create a GLS-style regression tree. This also synergistically addresses\nthe resampling issue, as the use of GLS loss amounts to resampling uncorrelated\ncontrasts (pre-whitened data) instead of the correlated data. For spatial\nsettings, RF-GLS can be used in conjunction with Gaussian Process correlated\nerrors to generate kriging predictions at new locations. RF becomes a special\ncase of RF-GLS with an identity working covariance matrix.\n  We establish consistency of RF-GLS under beta- (absolutely regular) mixing\nerror processes and show that this general result subsumes important cases like\nautoregressive time series and spatial Matern Gaussian Processes. As a\nbyproduct, we also establish consistency of RF for beta-mixing processes, which\nto our knowledge, is the first such result for RF under dependence.\n  We empirically demonstrate the improvement achieved by RF-GLS over RF for\nboth estimation and prediction under dependence.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 12:36:09 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 15:10:51 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Saha", "Arkajyoti", ""], ["Basu", "Sumanta", ""], ["Datta", "Abhirup", ""]]}, {"id": "2007.15445", "submitter": "David Swanson", "authors": "David Swanson", "title": "Localizing differences in smooths with simultaneous confidence bounds on\n  the true discovery proportion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a method for localizing where two smooths differ using a true\ndiscovery proportion (TDP) based interpretation. The procedure yields a\nstatement on the proportion of some region where true differences exist between\ntwo smooths, which results from its use of hypothesis tests on groups of basis\ncoefficients underlying the smooths. The methodology avoids otherwise ad hoc\nmeans of doing so such as performing hypothesis tests on entire smooths of\nsubsetted data. TDP estimates are 1-$\\alpha$ confidence bounded simultaneously,\nassuring that the estimate for a region is a lower bound on the proportion of\nactual difference, or true discoveries, in that region with high confidence\nregardless of the number or location of regions estimated. Our procedure is\nbased on closed-testing [Hommel, 1986] and recent results of Goeman and Solari\n[2011] and Goeman et al [2019]. We develop expressions for the covariance of\nquadratic forms because of the multiple regression framework in which we use\nthe closed-testing procedure, which are shown to be non-negative in many\nsettings. Our method is well-powered because of a given result on the\noff-diagonal decay structure of the covariance matrix of penalized B-splines of\ndegree two or less. We demonstrate achievement of estimated TDP and nominal\ntype 1 error rates in simulation and analyze a data set of walking gait of\ncerebral palsy patients. Keywords: splines; smoothing; multiple testing;\nclosed-testing; simultaneous confidence\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 13:21:32 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 16:37:18 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 12:23:51 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Swanson", "David", ""]]}, {"id": "2007.15496", "submitter": "Daniel Hlubinka", "authors": "Marc Hallin, Daniel Hlubinka, and \\v{S}\\'arka Hudecov\\'a", "title": "Fully distribution-free center-outward rank tests for multiple-output\n  regression and MANOVA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending rank-based inference to a multivariate setting such as\nmultiple-output regression or MANOVA with unspecified d-dimensional error\ndensity has remained an open problem for more than half a century. None of the\nmany solutions proposed so far is enjoying the combination of\ndistribution-freeness and efficiency that makes rank-based inference a\nsuccessful tool in the univariate setting. A concept of center-outward\nmultivariate ranks and signs based on measure transportation ideas has been\nintroduced recently. Center-outward ranks and signs are not only\ndistribution-free but achieve in dimension d > 1 the (essential) maximal\nancillarity property of traditional univariate ranks, hence carry all the\n\"distribution-free information\" available in the sample. We derive here the\nH\\'ajek representation and asymptotic normality results required in the\nconstruction of center-outward rank tests for multiple-output regression and\nMANOVA. When based on appropriate spherical scores, these fully\ndistribution-free tests achieve parametric efficiency in the corresponding\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 14:41:51 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Hallin", "Marc", ""], ["Hlubinka", "Daniel", ""], ["Hudecov\u00e1", "\u0160\u00e1rka", ""]]}, {"id": "2007.15535", "submitter": "Jonas Krampe", "authors": "Jonas Krampe, Efstathios Paparoditis, Carsten Trenkler", "title": "Structural Inference in Sparse High-Dimensional Vector Autoregressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider statistical inference for impulse responses in sparse, structural\nhigh-dimensional vector autoregressive (SVAR) systems. We introduce consistent\nestimators of impulse responses in the high-dimensional setting and suggest\nvalid inference procedures for the same parameters. Statistical inference in\nour setting is much more involved since standard procedures, like the\ndelta-method, do not apply. By using local projection equations, we first\nconstruct a de-sparsified version of regularized estimators of the moving\naverage parameters associated with the VAR system. We then obtain estimators of\nthe structural impulse responses by combining the aforementioned de-sparsified\nestimators with a non-regularized estimator of the contemporaneous impact\nmatrix, also taking into account the high-dimensionality of the system. We show\nthat the distribution of the derived estimators of structural impulse responses\nhas a Gaussian limit. We also present a valid bootstrap procedure to estimate\nthis distribution. Applications of the inference procedure in the construction\nof confidence intervals for impulse responses as well as in tests for forecast\nerror variance decomposition are presented. Our procedure is illustrated by\nmeans of simulations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 15:43:15 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 15:00:17 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Krampe", "Jonas", ""], ["Paparoditis", "Efstathios", ""], ["Trenkler", "Carsten", ""]]}, {"id": "2007.15766", "submitter": "Wicher Bergsma", "authors": "Wicher Bergsma and Haziq Jamil", "title": "Regression modelling with I-priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the I-prior methodology as a unifying framework for estimating a\nvariety of regression models, including varying coefficient, multilevel,\nlongitudinal models, and models with functional covariates and responses. It\ncan also be used for multi-class classification, with low or high dimensional\ncovariates.\n  The I-prior is generally defined as a maximum entropy prior. For a regression\nfunction, the I-prior is Gaussian with covariance kernel proportional to the\nFisher information on the regression function, which is estimated by its\nposterior distribution under the I-prior. The I-prior has the intuitively\nappealing property that the more information is available on a linear\nfunctional of the regression function, the larger the prior variance, and the\nsmaller the influence of the prior mean on the posterior distribution.\n  Advantages compared to competing methods, such as Gaussian process regression\nor Tikhonov regularization, are ease of estimation and model comparison. In\nparticular, we develop an EM algorithm with a simple E and M step for\nestimating hyperparameters, facilitating estimation for complex models. We also\npropose a novel parsimonious model formulation, requiring a single scale\nparameter for each (possibly multidimensional) covariate and no further\nparameters for interaction effects. This simplifies estimation because fewer\nhyperparameters need to be estimated, and also simplifies model comparison of\nmodels with the same covariates but different interaction effects; in this\ncase, the model with the highest estimated likelihood can be selected.\n  Using a number of widely analyzed real data sets we show that predictive\nperformance of our methodology is competitive. An R-package implementing the\nmethodology is available (Jamil, 2019).\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 22:52:22 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 19:39:01 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 11:54:56 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bergsma", "Wicher", ""], ["Jamil", "Haziq", ""]]}, {"id": "2007.15814", "submitter": "Dandan Chen", "authors": "Dandan Chen (University of Illinois, Urbana-Champaign)", "title": "Performance of Multi-group DIF Methods in Assessing Cross-Country Score\n  Comparability of International Large-Scale Assessments", "comments": "29 pages, 6 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Standardized large-scale testing can be a debatable topic, in which test\nfairness sits at its very core. This study found that two out of five recent\nmulti-group DIF detection methods are capable of capturing both the uniform and\nnonuniform DIF that affects test fairness. Still, no prior research has\ndemonstrated the relative performance of these two methods when they are\ncompared with each other. These two methods are the improved Wald test and the\ngeneralized logistic regression procedure. This study assessed the\ncommonalities and differences between two sets of empirical results from these\ntwo methods with the latest TIMSS math score data. The primary conclusion was\nthat the improved Wald test is relatively more established than the generalized\nlogistic regression procedure for multi-group DIF analysis. Empirical results\nfrom this study may inform the selection of a multi-group DIF method in the\nILSA score analysis.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 02:42:19 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Chen", "Dandan", "", "University of Illinois, Urbana-Champaign"]]}, {"id": "2007.15835", "submitter": "Mukund Sudarshan", "authors": "Mukund Sudarshan, Wesley Tansey, Rajesh Ranganath", "title": "Deep Direct Likelihood Knockoffs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Predictive modeling often uses black box machine learning methods, such as\ndeep neural networks, to achieve state-of-the-art performance. In scientific\ndomains, the scientist often wishes to discover which features are actually\nimportant for making the predictions. These discoveries may lead to costly\nfollow-up experiments and as such it is important that the error rate on\ndiscoveries is not too high. Model-X knockoffs enable important features to be\ndiscovered with control of the FDR. However, knockoffs require rich generative\nmodels capable of accurately modeling the knockoff features while ensuring they\nobey the so-called \"swap\" property. We develop Deep Direct Likelihood Knockoffs\n(DDLK), which directly minimizes the KL divergence implied by the knockoff swap\nproperty. DDLK consists of two stages: it first maximizes the explicit\nlikelihood of the features, then minimizes the KL divergence between the joint\ndistribution of features and knockoffs and any swap between them. To ensure\nthat the generated knockoffs are valid under any possible swap, DDLK uses the\nGumbel-Softmax trick to optimize the knockoff generator under the worst-case\nswap. We find DDLK has higher power than baselines while controlling the false\ndiscovery rate on a variety of synthetic and real benchmarks including a task\ninvolving a large dataset from one of the epicenters of COVID-19.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 04:09:46 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Sudarshan", "Mukund", ""], ["Tansey", "Wesley", ""], ["Ranganath", "Rajesh", ""]]}, {"id": "2007.15877", "submitter": "Hang Deng", "authors": "Hang Deng", "title": "Slightly Conservative Bootstrap for Maxima of Sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the bootstrap for the maxima of the sums of independent random\nvariables, a problem of high relevance to many applications in modern\nstatistics. Since the consistency of bootstrap was justified by Gaussian\napproximation in Chernozhukov et al. (2013), quite a few attempts have been\nmade to sharpen the error bound for bootstrap and reduce the sample size\nrequirement for bootstrap consistency. In this paper, we show that the sample\nsize requirement can be dramatically improved when we make the inference\nslightly conservative, that is, to inflate the bootstrap quantile\n$t_{\\alpha}^*$ by a small fraction, e.g. by $1\\%$ to $1.01\\,t^*_\\alpha$. This\nsimple procedure yields error bounds for the coverage probability of\nconservative bootstrap at as fast a rate as $\\sqrt{(\\log p)/n}$ under suitable\nconditions, so that not only the sample size requirement can be reduced to\n$\\log p \\ll n$ but also the overall convergence rate is nearly parametric.\nFurthermore, we improve the error bound for the coverage probability of the\nstandard non-conservative bootstrap to $[(\\log (np))^3 (\\log p)^2/n]^{1/4}$\nunder general assumptions on data. These results are established for the\nempirical bootstrap and the multiplier bootstrap with third moment match. An\nimproved coherent Lindeberg interpolation method, originally proposed in Deng\nand Zhang (2017), is developed to derive sharper comparison bounds, especially\nfor the maxima.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 07:16:42 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Deng", "Hang", ""]]}, {"id": "2007.15896", "submitter": "Marco Stefanucci", "authors": "Marco Stefanucci, Stefano Mazzuco", "title": "Analyzing Cause-Specific Mortality Trends using Compositional Functional\n  Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the dynamics of cause--specific mortality rates among countries by\nconsidering them as compositions of functions. We develop a novel framework for\nsuch data structure, with particular attention to functional PCA. The\napplication of this method to a subset of the WHO mortality database reveals\nthe main modes of variation of cause--specific rates over years for men and\nwomen and enables us to perform clustering in the projected subspace. The\nresults give many insights of the ongoing trends, only partially explained by\npast literature, that the considered countries are undergoing. We are also able\nto show the different evolution of cause of death undergone by men and women:\nfor example, we can see that while lung cancer incidence is stabilizing for\nmen, it is still increasing for women.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 08:19:22 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Stefanucci", "Marco", ""], ["Mazzuco", "Stefano", ""]]}, {"id": "2007.15930", "submitter": "Ryan Martin", "authors": "Yue Yang and Ryan Martin", "title": "Variational approximations of empirical Bayes posteriors in\n  high-dimensional linear models", "comments": "30 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensions, the prior tails can have a significant effect on both\nposterior computation and asymptotic concentration rates. To achieve optimal\nrates while keeping the posterior computations relatively simple, an empirical\nBayes approach has recently been proposed, featuring thin-tailed conjugate\npriors with data-driven centers. While conjugate priors ease some of the\ncomputational burden, Markov chain Monte Carlo methods are still needed, which\ncan be expensive when dimension is high. In this paper, we develop a\nvariational approximation to the empirical Bayes posterior that is fast to\ncompute and retains the optimal concentration rate properties of the original.\nIn simulations, our method is shown to have superior performance compared to\nexisting variational approximations in the literature across a wide range of\nhigh-dimensional settings.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 09:50:01 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Yang", "Yue", ""], ["Martin", "Ryan", ""]]}, {"id": "2007.15931", "submitter": "Michael Vogt", "authors": "Marina Khismatullina and Michael Vogt", "title": "Nonparametric comparison of epidemic time trends: the case of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic is one of the most pressing issues at present. A\nquestion which is particularly important for governments and policy makers is\nthe following: Does the virus spread in the same way in different countries? Or\nare there significant differences in the development of the epidemic? In this\npaper, we devise new inference methods that allow to detect differences in the\ndevelopment of the COVID-19 epidemic across countries in a statistically\nrigorous way. In our empirical study, we use the methods to compare the\noutbreak patterns of the epidemic in a number of European countries.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 09:54:16 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 08:25:43 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Khismatullina", "Marina", ""], ["Vogt", "Michael", ""]]}, {"id": "2007.15935", "submitter": "Johannes Krisam", "authors": "Johannes Krisam, Dorothea Weber, Richard F. Schlenk, Meinhard Kieser", "title": "Enhancing single-arm phase II trials by inclusion of matched control\n  patients", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a novel treatment has successfully passed phase I, different options to\ndesign subsequent phase II trials are available. One approach is a single-arm\ntrial, comparing the response rate in the intervention group against a fixed\nproportion. Another alternative is to conduct a randomized phase II trial,\ncomparing the new treatment with placebo or the current standard. A significant\nproblem arises in both approaches when the investigated patient population is\nvery heterogeneous regarding prognostic factors. For the situation that a\nsubstantial dataset of historical controls exists, we propose an approach to\nenhance the classic single-arm trial design by including matched control\npatients. The outcome of the observed study population can be adjusted based on\nthe matched controls with a comparable distribution of known confounders. We\npropose an adaptive two-stage design with the options of early stopping for\nfutility and recalculation of the sample size taking the matching rate, number\nof matching partners, and observed treatment effect into account. The\nperformance of the proposed design in terms of type I error rate, power, and\nexpected sample size is investigated via simulation studies based on a\nhypothetical phase II trial investigating a novel therapy for patients with\nacute myeloid leukemia.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 10:03:58 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Krisam", "Johannes", ""], ["Weber", "Dorothea", ""], ["Schlenk", "Richard F.", ""], ["Kieser", "Meinhard", ""]]}, {"id": "2007.15955", "submitter": "Zhuozhao Zhan", "authors": "Osama Almalik, Zhuozhao Zhan, and Edwin R. van den Heuvel", "title": "Copas' method is sensitive to different mechanisms of publication bias", "comments": "1 Figure, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copas' method corrects a pooled estimate from an aggregated data\nmeta-analysis for publication bias. Its performance has been studied for one\nparticular mechanism of publication bias. We show through simulations that\nCopas' method is not robust against other realistic mechanisms. This questions\nthe usefulness of Copas' method, since publication bias mechanisms are\ntypically unknown in practice.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 10:57:08 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Almalik", "Osama", ""], ["Zhan", "Zhuozhao", ""], ["Heuvel", "Edwin R. van den", ""]]}, {"id": "2007.15991", "submitter": "Sarah Friedrich", "authors": "Sarah Friedrich and Tim Friede", "title": "Causal inference methods for small non-randomized studies: Methods and\n  an application in COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usual development cycles are too slow for the development of vaccines,\ndiagnostics and treatments in pandemics such as the ongoing SARS-CoV-2\npandemic. Given the pressure in such a situation, there is a risk that findings\nof early clinical trials are overinterpreted despite their limitations in terms\nof size and design. Motivated by a non-randomized open-label study\ninvestigating the efficacy of hydroxychloroquine in patients with COVID-19, we\ndescribe in a unified fashion various alternative approaches to the analysis of\nnon-randomized studies. A widely used tool to reduce the impact of\ntreatment-selection bias are so-called propensity score (PS) methods.\nConditioning on the propensity score allows one to replicate the design of a\nrandomized controlled trial, conditional on observed covariates. Extensions\ninclude the g-computation approach, which is less frequently applied, in\nparticular in clinical studies. Moreover, doubly robust estimators provide\nadditional advantages. Here, we investigate the properties of propensity score\nbased methods including three variations of doubly robust estimators in small\nsample settings, typical for early trials, in a simulation study. R code for\nthe simulations is provided.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 12:00:14 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 10:45:43 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Friedrich", "Sarah", ""], ["Friede", "Tim", ""]]}, {"id": "2007.16031", "submitter": "Xin Gao", "authors": "Xin Gao, Li Li, Li Luo", "title": "Decomposition of the Total Effect for Two Mediators: A Natural\n  Counterfactual Interaction Effect Framework", "comments": "112 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2004.06054", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis has been used in many disciplines to explain the mechanism\nor process that underlies an observed relationship between an exposure variable\nand an outcome variable via the inclusion of mediators. Decompositions of the\ntotal causal effect of an exposure variable into effects characterizing\nmediation pathways and interactions have gained an increasing amount of\ninterest in the last decade. In this work, we develop decompositions for\nscenarios where the two mediators are causally sequential or non-sequential.\nCurrent developments in this area have primarily focused on either\ndecompositions without interaction components or with interactions but assuming\nno causally sequential order between the mediators. We propose a new concept\ncalled natural counterfactual interaction effect that captures the two-way and\nthree-way interactions for both scenarios that extend the two-way mediated\ninteractions in literature. We develop a unified approach for decomposing the\ntotal effect into the effects that are due to mediation only, interaction only,\nboth mediation and interaction, neither mediation nor interaction within the\ncounterfactual framework. Finally, we illustrate the proposed decomposition\nmethod using a real data analysis where the two mediators are causally\nsequential.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 04:10:09 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Gao", "Xin", ""], ["Li", "Li", ""], ["Luo", "Li", ""]]}, {"id": "2007.16059", "submitter": "Antonio Elias", "authors": "Antonio El\\'ias (1 and 2), Ra\\'ul Jim\\'enez (1 and 2), Joe Yukich (2\n  and 3) ((1) Department of Statistics, Universidad Carlos III de Madrid, (2)\n  UC3M-Santander Big Data Institute, Universidad Carlos III de Madrid, (3)\n  Department of Mathematics, Lehigh University)", "title": "Localization processes for functional data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an alternative to $k$-nearest neighbors for functional data\nwhereby the approximating neighboring curves are piecewise functions built from\na functional sample. Using a locally defined distance function that satisfies\nstabilization criteria, we establish pointwise and global approximation results\nin function spaces when the number of data curves is large enough. We exploit\nthis feature to develop the asymptotic theory when a finite number of curves is\nobserved at time-points given by an i.i.d. sample whose cardinality increases\nup to infinity. We use these results to investigate the problem of estimating\nunobserved segments of a partially observed functional data sample as well as\nto study the problem of functional classification and outlier detection. For\nsuch problems, our methods are competitive with and sometimes superior to\nbenchmark predictions in the field.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 13:19:09 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 09:58:34 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["El\u00edas", "Antonio", "", "1 and 2"], ["Jim\u00e9nez", "Ra\u00fal", "", "1 and 2"], ["Yukich", "Joe", "", "2\n  and 3"]]}, {"id": "2007.16096", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb, Yaneer Bar-Yam, and Pasquale Cirillo", "title": "On Single Point Forecasts for Fat-Tailed Variables", "comments": "Accepted, International Journal of Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph econ.GN q-fin.EC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss common errors and fallacies when using naive \"evidence based\"\nempiricism and point forecasts for fat-tailed variables, as well as the\ninsufficiency of using naive first-order scientific methods for tail risk\nmanagement. We use the COVID-19 pandemic as the background for the discussion\nand as an example of a phenomenon characterized by a multiplicative nature, and\nwhat mitigating policies must result from the statistical properties and\nassociated risks. In doing so, we also respond to the points raised by\nIoannidis et al. (2020).\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 14:20:16 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Taleb", "Nassim Nicholas", ""], ["Bar-Yam", "Yaneer", ""], ["Cirillo", "Pasquale", ""]]}]