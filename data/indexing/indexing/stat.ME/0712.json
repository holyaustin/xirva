[{"id": "0712.0096", "submitter": "Florentin Smarandache", "authors": "Rajesh Singh, Pankaj Chauhan, Nirmala Sawan, Florentin Smarandache", "title": "Auxiliary Information and A Priori Values in Construction of Improved\n  Estimators", "comments": "74 pages", "journal-ref": "Third paper published in Bulletin of Statistics & Economics, Vol.\n  3, No. A09, pp. 13-18, Fall 2009.", "doi": null, "report-no": null, "categories": "stat.ME", "license": null, "abstract": "  This volume is a collection of six papers on the use of auxiliary information\nand 'a priori' values in construction of improved estimators. The work included\nhere will be of immense application for researchers and students who emply\nauxiliary information in any form.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2007 17:21:26 GMT"}], "update_date": "2010-02-19", "authors_parsed": [["Singh", "Rajesh", ""], ["Chauhan", "Pankaj", ""], ["Sawan", "Nirmala", ""], ["Smarandache", "Florentin", ""]]}, {"id": "0712.0189", "submitter": "Jeffrey Picka", "authors": "Jeffrey Picka and Mingxia Deng", "title": "Summarization and Classification of Non-Poisson Point Processes", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": null, "abstract": "  Fitting models for non-Poisson point processes is complicated by the lack of\ntractable models for much of the data. By using large samples of independent\nand identically distributed realizations and statistical learning, it is\npossible to identify absence of fit through finding a classification rule that\ncan efficiently identify single realizations of each type. The method requires\na much wider range of descriptive statistics than are currently in use, and a\nnew concept of model fitting which is derive from how physical laws are judged\nto fit data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2007 21:48:10 GMT"}], "update_date": "2007-12-04", "authors_parsed": [["Picka", "Jeffrey", ""], ["Deng", "Mingxia", ""]]}, {"id": "0712.0283", "submitter": "Anestis Antoniadis", "authors": "Anestis Antoniadis", "title": "Wavelet methods in statistics: Some recent developments and their\n  applications", "comments": "Published in at http://dx.doi.org/10.1214/07-SS014 the Statistics\n  Surveys (http://www.i-journals.org/ss/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistics Surveys 2007, Vol. 1, 16-55", "doi": "10.1214/07-SS014", "report-no": "IMS-SS-SS_2007_14", "categories": "stat.ME", "license": null, "abstract": "  The development of wavelet theory has in recent years spawned applications in\nsignal processing, in fast algorithms for integral transforms, and in image and\nfunction representation methods. This last application has stimulated interest\nin wavelet applications to statistics and to the analysis of experimental data,\nwith many successes in the efficient analysis, processing, and compression of\nnoisy signals and images. This is a selective review article that attempts to\nsynthesize some recent work on ``nonlinear'' wavelet methods in nonparametric\ncurve estimation and their role on a variety of applications. After a short\nintroduction to wavelet theory, we discuss in detail several wavelet shrinkage\nand wavelet thresholding estimators, scattered in the literature and developed,\nunder more or less standard settings, for density estimation from i.i.d.\nobservations or to denoise data modeled as observations of a signal with\nadditive noise. Most of these methods are fitted into the general concept of\nregularization with appropriately chosen penalty functions. A narrow range of\napplications in major areas of statistics is also discussed such as partial\nlinear regression models and functional index models. The usefulness of all\nthese methods are illustrated by means of simulations and practical examples.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2007 13:15:09 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Antoniadis", "Anestis", ""]]}, {"id": "0712.1027", "submitter": "Mu Zhu", "authors": "Mu Zhu", "title": "Kernels and Ensembles: Perspectives on Statistical Learning", "comments": "22 pages; 6 figures; sumitted to The American Statistician", "journal-ref": "The American Statistician, May 2008, Vol. 62, No. 2, Pages 97 -\n  109.", "doi": "10.1198/000313008X306367", "report-no": null, "categories": "stat.ME stat.ML", "license": null, "abstract": "  Since their emergence in the 1990's, the support vector machine and the\nAdaBoost algorithm have spawned a wave of research in statistical machine\nlearning. Much of this new research falls into one of two broad categories:\nkernel methods and ensemble methods. In this expository article, I discuss the\nmain ideas behind these two types of methods, namely how to transform linear\nalgorithms into nonlinear ones by using kernel functions, and how to make\npredictions with an ensemble or a collection of models rather than a single\nmodel. I also share my personal perspectives on how these ideas have influenced\nand shaped my own research. In particular, I present two recent algorithms that\nI have invented with my collaborators: LAGO, a fast kernel algorithm for\nunbalanced classification and rare target detection; and Darwinian evolution in\nparallel universes, an ensemble method for variable selection.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2007 20:31:29 GMT"}], "update_date": "2008-04-15", "authors_parsed": [["Zhu", "Mu", ""]]}, {"id": "0712.1342", "submitter": "Heng Lian", "authors": "Heng Lian", "title": "Stochastic adaptation of importance sampler", "comments": "11 pages, minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": null, "abstract": "  Improving efficiency of importance sampler is at the center of research in\nMonte Carlo methods. While adaptive approach is usually difficult within the\nMarkov Chain Monte Carlo framework, the counterpart in importance sampling can\nbe justified and validated easily. We propose an iterative adaptation method\nfor learning the proposal distribution of an importance sampler based on\nstochastic approximation. The stochastic approximation method can recruit\ngeneral iterative optimization techniques like the minorization-maximization\nalgorithm. The effectiveness of the approach in optimizing the Kullback\ndivergence between the proposal distribution and the target is demonstrated\nusing several simple examples.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2007 05:24:26 GMT"}], "update_date": "2007-12-11", "authors_parsed": [["Lian", "Heng", ""]]}, {"id": "0712.1663", "submitter": "Nicolai Meinshausen", "authors": "Nicolai Meinshausen, Peter Bickel, John Rice", "title": "Efficient blind search: Optimal power of detection under computational\n  cost constraints", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS180 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 38-60", "doi": "10.1214/08-AOAS180", "report-no": "IMS-AOAS-AOAS180", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some astronomy projects require a blind search through a vast number of\nhypotheses to detect objects of interest. The number of hypotheses to test can\nbe in the billions. A naive blind search over every single hypothesis would be\nfar too costly computationally. We propose a hierarchical scheme for blind\nsearch, using various \"resolution\" levels. At lower resolution levels,\n\"regions\" of interest in the search space are singled out with a low\ncomputational cost. These regions are refined at intermediate resolution levels\nand only the most promising candidates are finally tested at the original fine\nresolution. The optimal search strategy is found by dynamic programming. We\ndemonstrate the procedure for pulsar search from satellite gamma-ray\nobservations and show that the power of the naive blind search can almost be\nmatched with the hierarchical scheme while reducing the computational burden by\nmore than three orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2007 09:04:06 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2008 15:21:17 GMT"}, {"version": "v3", "created": "Sun, 1 Jun 2008 12:41:21 GMT"}, {"version": "v4", "created": "Fri, 15 May 2009 09:48:10 GMT"}], "update_date": "2009-05-15", "authors_parsed": [["Meinshausen", "Nicolai", ""], ["Bickel", "Peter", ""], ["Rice", "John", ""]]}, {"id": "0712.1692", "submitter": "Monika Meise", "authors": "P.L. Davies, M. Meise", "title": "Approximating Data with weighted smoothing Splines", "comments": null, "journal-ref": "Journal of Nonparametric Statistics, 20:3, (2008) 207-228", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a data set (t_i, y_i), i=1,..., n with the t_i in [0,1] non-parametric\nregression is concerned with the problem of specifying a suitable function\nf_n:[0,1] -> R such that the data can be reasonably approximated by the points\n(t_i, f_n(t_i)), i=1,..., n. If a data set exhibits large variations in local\nbehaviour, for example large peaks as in spectroscopy data, then the method\nmust be able to adapt to the local changes in smoothness. Whilst many methods\nare able to accomplish this they are less successful at adapting derivatives.\nIn this paper we show how the goal of local adaptivity of the function and its\nfirst and second derivatives can be attained in a simple manner using weighted\nsmoothing splines. A residual based concept of approximation is used which\nforces local adaptivity of the regression function together with a global\nregularization which makes the function as smooth as possible subject to the\napproximation constraints.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2007 12:14:42 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2009 15:48:24 GMT"}], "update_date": "2009-03-18", "authors_parsed": [["Davies", "P. L.", ""], ["Meise", "M.", ""]]}, {"id": "0712.2352", "submitter": "Guido Nolte", "authors": "Guido Nolte, Andreas Ziehe, Vadim V. Nikulin, Alois Schl\\\"ogl, Nicole\n  Kr\\\"amer, Tom Brismar, Klaus-Robert M\\\"uller", "title": "Robustly estimating the flow direction of information in complex\n  physical systems", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": "10.1103/PhysRevLett.100.234101", "report-no": null, "categories": "stat.ME stat.AP", "license": null, "abstract": "  We propose a new measure to estimate the direction of information flux in\nmultivariate time series from complex systems. This measure, based on the slope\nof the phase spectrum (Phase Slope Index) has invariance properties that are\nimportant for applications in real physical or biological systems: (a) it is\nstrictly insensitive to mixtures of arbitrary independent sources, (b) it gives\nmeaningful results even if the phase spectrum is not linear, and (c) it\nproperly weights contributions from different frequencies. Simulations of a\nclass of coupled multivariate random data show that for truly unidirectional\ninformation flow without additional noise contamination our measure detects the\ncorrect direction as good as the standard Granger causality. For random\nmixtures of independent sources Granger Causality erroneously yields highly\nsignificant results whereas our measure correctly becomes non-significant. An\napplication of our novel method to EEG data (88 subjects in eyes-closed\ncondition) reveals a strikingly clear front-to-back information flow in the\nvast majority of subjects and thus contributes to a better understanding of\ninformation processing in the brain.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2007 16:10:07 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Nolte", "Guido", ""], ["Ziehe", "Andreas", ""], ["Nikulin", "Vadim V.", ""], ["Schl\u00f6gl", "Alois", ""], ["Kr\u00e4mer", "Nicole", ""], ["Brismar", "Tom", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "0712.2526", "submitter": "Jon McAuliffe", "authors": "Michael Braun and Jon McAuliffe", "title": "Variational inference for large-scale models of discrete choice", "comments": "29 pages, 2 tables, 2 figures", "journal-ref": "Journal of the American Statistical Association (2010) 105(489):\n  324-334", "doi": "10.1198/jasa.2009.tm08030", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": null, "abstract": "  Discrete choice models are commonly used by applied statisticians in numerous\nfields, such as marketing, economics, finance, and operations research. When\nagents in discrete choice models are assumed to have differing preferences,\nexact inference is often intractable. Markov chain Monte Carlo techniques make\napproximate inference possible, but the computational cost is prohibitive on\nthe large data sets now becoming routinely available. Variational methods\nprovide a deterministic alternative for approximation of the posterior\ndistribution. We derive variational procedures for empirical Bayes and fully\nBayesian inference in the mixed multinomial logit model of discrete choice. The\nalgorithms require only that we solve a sequence of unconstrained optimization\nproblems, which are shown to be convex. Extensive simulations demonstrate that\nvariational methods achieve accuracy competitive with Markov chain Monte Carlo,\nat a small fraction of the computational cost. Thus, variational methods permit\ninferences on data sets that otherwise could not be analyzed without\nbias-inducing modifications to the underlying model.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2007 16:16:18 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2007 18:46:25 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2008 18:03:40 GMT"}], "update_date": "2010-06-04", "authors_parsed": [["Braun", "Michael", ""], ["McAuliffe", "Jon", ""]]}, {"id": "0712.2657", "submitter": "Rima Izem", "authors": "Rima Izem, J.S. Marron", "title": "Analysis of nonlinear modes of variation for functional data", "comments": "Published in at http://dx.doi.org/10.1214/07-EJS080 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Electronic Journal of Statistics 2007, Vol. 1, 641-676", "doi": "10.1214/07-EJS080", "report-no": "IMS-EJS-EJS_2007_80", "categories": "stat.ME", "license": null, "abstract": "  A set of curves or images of similar shape is an increasingly common\nfunctional data set collected in the sciences. Principal Component Analysis\n(PCA) is the most widely used technique to decompose variation in functional\ndata. However, the linear modes of variation found by PCA are not always\ninterpretable by the experimenters. In addition, the modes of variation of\ninterest to the experimenter are not always linear. We present in this paper a\nnew analysis of variance for Functional Data. Our method was motivated by\ndecomposing the variation in the data into predetermined and interpretable\ndirections (i.e. modes) of interest. Since some of these modes could be\nnonlinear, we develop a new defined ratio of sums of squares which takes into\naccount the curvature of the space of variation. We discuss, in the general\ncase, consistency of our estimates of variation, using mathematical tools from\ndifferential geometry and shape statistics. We successfully applied our method\nto a motivating example of biological data. This decomposition allows\nbiologists to compare the prevalence of different genetic tradeoffs in a\npopulation and to quantify the effect of selection on evolution.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2007 09:12:56 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Izem", "Rima", ""], ["Marron", "J. S.", ""]]}, {"id": "0712.2708", "submitter": "A. C. Davison", "authors": "A. C. Davison, N. Sartori", "title": "The Banff Challenge: Statistical Detection of a Noisy Signal", "comments": "Published in at http://dx.doi.org/10.1214/08-STS260 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2008, Vol. 23, No. 3, 354-364", "doi": "10.1214/08-STS260", "report-no": "IMS-STS-STS260", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle physics experiments such as those run in the Large Hadron Collider\nresult in huge quantities of data, which are boiled down to a few numbers from\nwhich it is hoped that a signal will be detected. We discuss a simple\nprobability model for this and derive frequentist and noninformative Bayesian\nprocedures for inference about the signal. Both are highly accurate in\nrealistic cases, with the frequentist procedure having the edge for interval\nestimation, and the Bayesian procedure yielding slightly better point\nestimates. We also argue that the significance, or $p$-value, function based on\nthe modified likelihood root provides a comprehensive presentation of the\ninformation in the data and should be used for inference.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2007 12:49:56 GMT"}, {"version": "v2", "created": "Thu, 17 Feb 2011 14:06:16 GMT"}], "update_date": "2011-02-18", "authors_parsed": [["Davison", "A. C.", ""], ["Sartori", "N.", ""]]}, {"id": "0712.3618", "submitter": "Aiyou Chen", "authors": "Aiyou Chen, Jin Cao, and Tian Bu", "title": "Network Tomography: Identifiability and Fourier Domain Estimation", "comments": "21 pages", "journal-ref": "IEEE INFOCOM 2007, p.1875-1883", "doi": "10.1109/INFCOM.2007.218", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": null, "abstract": "  The statistical problem for network tomography is to infer the distribution\nof $\\mathbf{X}$, with mutually independent components, from a measurement model\n$\\mathbf{Y}=A\\mathbf{X}$, where $A$ is a given binary matrix representing the\nrouting topology of a network under consideration. The challenge is that the\ndimension of $\\mathbf{X}$ is much larger than that of $\\mathbf{Y}$ and thus the\nproblem is often called ill-posed. This paper studies some statistical aspects\nof network tomography. We first address the identifiability issue and prove\nthat the $\\mathbf{X}$ distribution is identifiable up to a shift parameter\nunder mild conditions. We then use a mixture model of characteristic functions\nto derive a fast algorithm for estimating the distribution of $\\mathbf{X}$\nbased on the General method of Moments. Through extensive model simulation and\nreal Internet trace driven simulation, the proposed approach is shown to be\nfavorable comparing to previous methods using simple discretization for\ninferring link delays in a heterogeneous network.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2007 03:47:28 GMT"}], "update_date": "2007-12-24", "authors_parsed": [["Chen", "Aiyou", ""], ["Cao", "Jin", ""], ["Bu", "Tian", ""]]}, {"id": "0712.3735", "submitter": "Fabienne Comte", "authors": "Fabienne Comte (MAP5), Valentine Genon-Catalot (MAP5), Yves Rozenholc\n  (MAP5)", "title": "Nonparametric estimation for a stochastic volatility model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": null, "abstract": "  Consider discrete time observations (X_{\\ell\\delta})_{1\\leq \\ell \\leq n+1}$\nof the process $X$ satisfying $dX_t= \\sqrt{V_t} dB_t$, with $V_t$ a\none-dimensional positive diffusion process independent of the Brownian motion\n$B$. For both the drift and the diffusion coefficient of the unobserved\ndiffusion $V$, we propose nonparametric least square estimators, and provide\nbounds for theirrisk. Estimators are chosen among a collection of functions\nbelonging to a finite dimensional space whose dimension is selected by a data\ndriven procedure. Implementation on simulated data illustrates how the method\nworks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2007 16:17:25 GMT"}], "update_date": "2007-12-25", "authors_parsed": [["Comte", "Fabienne", "", "MAP5"], ["Genon-Catalot", "Valentine", "", "MAP5"], ["Rozenholc", "Yves", "", "MAP5"]]}, {"id": "0712.4166", "submitter": "Peter Hoff", "authors": "Peter Hoff", "title": "Simulation of the matrix Bingham-von Mises-Fisher distribution, with\n  applications to multivariate and relational data", "comments": "17 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": null, "abstract": "  Orthonormal matrices play an important role in reduced-rank matrix\napproximations and the analysis of matrix-valued data. A matrix Bingham-von\nMises-Fisher distribution is a probability distribution on the set of\northonormal matrices that includes linear and quadratic terms, and arises as a\nposterior distribution in latent factor models for multivariate and relational\ndata. This article describes rejection and Gibbs sampling algorithms for\nsampling from this family of distributions, and illustrates their use in the\nanalysis of a protein-protein interaction network.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2007 19:59:41 GMT"}], "update_date": "2007-12-28", "authors_parsed": [["Hoff", "Peter", ""]]}, {"id": "0712.4290", "submitter": "Adom Giffin", "authors": "Adom Giffin", "title": "Updating Probabilities: A Complex Agent Based Example", "comments": "Presented at the 7th International Conference on Complex Systems,\n  Boston, 2007. 9 pages, 1 figure", "journal-ref": "InterJournal of Complex Systems, 2273 (2008)", "doi": null, "report-no": null, "categories": "stat.ME cond-mat.stat-mech nlin.AO physics.bio-ph physics.data-an q-bio.MN stat.AP", "license": null, "abstract": "  It has been shown that one can accommodate data (Bayes) and constraints\n(MaxEnt) in one method, the method of Maximum (relative) Entropy (ME) (Giffin\n2007). In this paper we show a complex agent based example of inference with\ntwo different forms of information; moments and data. In this example, several\nagents each receive partial information about a system in the form of data. In\naddition, each agent agrees or is informed that there are certain global\nconstraints on the system that are always true. The agents are then asked to\nmake inferences about the entire system. The system becomes more complex as we\nadd agents and allow them to share information. This system can have a\ngeometrical form, such as a crystal structure. The shape may dictate how the\nagents are able to share information, such as sharing with nearest neighbors.\nThis method can be used to model many systems where the agents or cells have\nlocal or partial information but must adhere to some global rules. This could\nalso illustrate how the agents evolve and could illuminate emergent behavior of\nthe system.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2007 22:15:49 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Giffin", "Adom", ""]]}, {"id": "0712.4323", "submitter": "Bent J{\\o}rgensen", "authors": "Bent J{\\o}rgensen, Yuri Goegebeur and Jos\\'e Ra\\'ul Mart\\'inez", "title": "Dispersion Models for Extremes", "comments": "23 pages. Abstract submitted to the 56th Session of the ISI, Lisboa,\n  2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": null, "abstract": "  We propose extreme value analogues of natural exponential families and\nexponential dispersion models, and introduce the slope function as an analogue\nof the variance function. The set of quadratic and power slope functions\ncharacterize well-known families such as the Rayleigh, Gumbel, power, Pareto,\nlogistic, negative exponential, Weibull and Fr\\'echet. We show a convergence\ntheorem for slope functions, by which we may express the classical extreme\nvalue convergence results in terms of asymptotics for extreme dispersion\nmodels. The main idea is to explore the parallels between location families and\nnatural exponential families, and between the convolution and minimum\noperations.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2007 09:56:24 GMT"}], "update_date": "2007-12-31", "authors_parsed": [["J\u00f8rgensen", "Bent", ""], ["Goegebeur", "Yuri", ""], ["Mart\u00ednez", "Jos\u00e9 Ra\u00fal", ""]]}]