[{"id": "1009.0796", "submitter": "Roberto D. Pascual-Marqui", "authors": "Roberto D. Pascual-Marqui and Rolando J. Biscay-Lirio", "title": "Dynamic interactions in terms of senders, hubs, and receivers (SHR)\n  using the singular value decomposition of time series: Theory and brain\n  connectivity applications", "comments": "Technical report 2010-September-04, The KEY Institute for Brain-Mind\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.bio-ph q-bio.NC stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Understanding of normal and pathological brain function requires the\nidentification and localization of functional connections between specialized\nregions. The availability of high time resolution signals of electric neuronal\nactivity at several regions offers information for quantifying the connections\nin terms of information flow. When the signals cover the whole cortex, the\nnumber of connections is very large, making visualization and interpretation\nvery difficult. We introduce here the singular value decomposition of\ntime-lagged multiple signals, which localizes the senders, hubs, and receivers\n(SHR) of information transmission. Unlike methods that operate on large\nconnectivity matrices, such as correlation thresholding and graph-theoretic\nanalyses, this method operates on the multiple time series directly, providing\n3D brain images that assign a score to each location in terms of its sending,\nrelaying, and receiving capacity. The scope of the method is general and\nencompasses other applications outside the field of brain connectivity.\n", "versions": [{"version": "v1", "created": "Sat, 4 Sep 2010 01:26:31 GMT"}, {"version": "v2", "created": "Tue, 7 Sep 2010 00:36:11 GMT"}], "update_date": "2010-09-08", "authors_parsed": [["Pascual-Marqui", "Roberto D.", ""], ["Biscay-Lirio", "Rolando J.", ""]]}, {"id": "1009.0888", "submitter": "Artur Lemonte", "authors": "Artur J. Lemonte", "title": "A log-Birnbaum-Saunders Regression Model with Asymmetric Errors", "comments": "Submitted for publication", "journal-ref": null, "doi": "10.1080/00949655.2011.595715", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper by Leiva et al. (2010) introduced a skewed version of the\nsinh-normal distribution, discussed some of its properties and characterized an\nextension of the Birnbaum-Saunders distribution associated with this\ndistribution. In this paper, we introduce a skewed log-Birnbaum-Saunders\nregression model based on the skewed sinh-normal distribution. Some influence\nmethods, such as the local influence and generalized leverage are presented.\nAdditionally, we derived the normal curvatures of local influence under some\nperturbation schemes. An empirical application to a real data set is presented\nin order to illustrate the usefulness of the proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 5 Sep 2010 04:11:08 GMT"}], "update_date": "2011-11-22", "authors_parsed": [["Lemonte", "Artur J.", ""]]}, {"id": "1009.0891", "submitter": "Song Cai", "authors": "Song Cai, James V. Zidek, Nathaniel Newlands", "title": "Predicting Sequences of Progressive Events Times with Time-dependent\n  Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to modeling progressive event-history data\nwhen the overall objective is prediction based on time-dependent covariates.\nThis approach does not model the hazard function directly. Instead, it models\nthe process of the state indicators of the event history so that the\ntime-dependent covariates can be incorporated and predictors of the future\nevents easily formulated. Our model can be applied to a range of real-world\nproblems in medical and agricultural science.\n", "versions": [{"version": "v1", "created": "Sun, 5 Sep 2010 04:59:15 GMT"}], "update_date": "2010-09-07", "authors_parsed": [["Cai", "Song", ""], ["Zidek", "James V.", ""], ["Newlands", "Nathaniel", ""]]}, {"id": "1009.0942", "submitter": "Michael Krystek", "authors": "Michael Krystek", "title": "Bayesian theory of systematic measurement deviations", "comments": "This paper has been withdrawn by the author. The paper has been\n  rejected by the referees of MST. If someone is still interested in the\n  content, please contact me by e-mail (Michael.Krystek@ptb.de)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concerning systematic effects, the recommendation given in the GUM is to\ncorrect for them, but unfortunately no detailed information is available, how\nto do this. This publication will show, how systematic measurement deviations\ncan be handled correctly based on the Bayesian probability theory. After a\nshort overview about useful methods and tools, like the product rule of\nprobability theory, Bayes' theorem, the principle of maximum entropy, and the\nmarginalisation equation, an outline of a method to handle systematic\nmeasurement deviations is introduced. Finally some simple examples of practical\ninterest are given, in order to demonstrate the applicability of the suggested\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 5 Sep 2010 19:46:11 GMT"}, {"version": "v2", "created": "Sat, 23 Oct 2010 14:43:29 GMT"}, {"version": "v3", "created": "Tue, 15 Feb 2011 07:31:44 GMT"}], "update_date": "2011-02-16", "authors_parsed": [["Krystek", "Michael", ""]]}, {"id": "1009.1216", "submitter": "Nicolas Bousquet", "authors": "Alberto Pasanisi and Shuai Fu and Nicolas Bousquet", "title": "Estimating Discrete Markov Models From Various Incomplete Data Schemes", "comments": "26 pages - preprint accepted in 20th February 2012 for publication in\n  Computational Statistics and Data Analysis (please cite the journal's paper)", "journal-ref": "Computational Statistics and Data Analysis - Volume 56, Issue 9,\n  September 2012, Pages 2609-2625", "doi": "10.1016/j.csda.2012.02.027", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parameters of a discrete stationary Markov model are transition\nprobabilities between states. Traditionally, data consist in sequences of\nobserved states for a given number of individuals over the whole observation\nperiod. In such a case, the estimation of transition probabilities is\nstraightforwardly made by counting one-step moves from a given state to\nanother. In many real-life problems, however, the inference is much more\ndifficult as state sequences are not fully observed, namely the state of each\nindividual is known only for some given values of the time variable. A review\nof the problem is given, focusing on Monte Carlo Markov Chain (MCMC) algorithms\nto perform Bayesian inference and evaluate posterior distributions of the\ntransition probabilities in this missing-data framework. Leaning on the\ndependence between the rows of the transition matrix, an adaptive MCMC\nmechanism accelerating the classical Metropolis-Hastings algorithm is then\nproposed and empirically studied.\n", "versions": [{"version": "v1", "created": "Tue, 7 Sep 2010 07:49:21 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2012 14:04:08 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Pasanisi", "Alberto", ""], ["Fu", "Shuai", ""], ["Bousquet", "Nicolas", ""]]}, {"id": "1009.1436", "submitter": "Anton H. Westveld", "authors": "Anton H. Westveld, Peter D. Hoff", "title": "A mixed effects model for longitudinal relational and network data, with\n  applications to international trade and conflict", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS403 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2A, 843-872", "doi": "10.1214/10-AOAS403", "report-no": "IMS-AOAS-AOAS403", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is an approach to the modeling of longitudinal social\nnetwork or relational data. Such data arise from measurements on pairs of\nobjects or actors made at regular temporal intervals, resulting in a social\nnetwork for each point in time. In this article we represent the network and\ntemporal dependencies with a random effects model, resulting in a stochastic\nprocess defined by a set of stationary covariance matrices. Our approach builds\nupon the social relations models of Warner, Kenny and Stoto [Journal of\nPersonality and Social Psychology 37 (1979) 1742--1757] and Gill and Swartz\n[Canad. J. Statist. 29 (2001) 321--331] and allows for an intra- and\ninter-temporal representation of network structures. We apply the methodology\nto two longitudinal data sets: international trade (continuous response) and\nmilitarized interstate disputes (binary response).\n", "versions": [{"version": "v1", "created": "Wed, 8 Sep 2010 01:47:34 GMT"}, {"version": "v2", "created": "Sun, 26 Sep 2010 23:43:36 GMT"}, {"version": "v3", "created": "Wed, 17 Aug 2011 05:24:21 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Westveld", "Anton H.", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1009.1444", "submitter": "D\\'avid Papp", "authors": "D\\'avid Papp", "title": "Optimal designs for rational function regression", "comments": "25 pages. Previous version updated with more details in the theory\n  and additional examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider optimal non-sequential designs for a large class of (linear and\nnonlinear) regression models involving polynomials and rational functions with\nheteroscedastic noise also given by a polynomial or rational weight function.\nThe proposed method treats D-, E-, A-, and $\\Phi_p$-optimal designs in a\nunified manner, and generates a polynomial whose zeros are the support points\nof the optimal approximate design, generalizing a number of previously known\nresults of the same flavor. The method is based on a mathematical optimization\nmodel that can incorporate various criteria of optimality and can be solved\nefficiently by well established numerical optimization methods. In contrast to\nprevious optimization-based methods proposed for similar design problems, it\nalso has theoretical guarantee of its algorithmic efficiency; in fact, the\nrunning times of all numerical examples considered in the paper are negligible.\nThe stability of the method is demonstrated in an example involving high degree\npolynomials. After discussing linear models, applications for finding locally\noptimal designs for nonlinear regression models involving rational functions\nare presented, then extensions to robust regression designs, and trigonometric\nregression are shown. As a corollary, an upper bound on the size of the support\nset of the minimally-supported optimal designs is also found. The method is of\nconsiderable practical importance, with the potential for instance to impact\ndesign software development. Further study of the optimality conditions of the\nmain optimization model might also yield new theoretical insights.\n", "versions": [{"version": "v1", "created": "Wed, 8 Sep 2010 02:44:03 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2011 17:28:56 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Papp", "D\u00e1vid", ""]]}, {"id": "1009.1463", "submitter": "Jessica Kasza", "authors": "Jessica Kasza, Patty Solomon", "title": "A comparison of score-based methods for estimating Bayesian networks\n  using the Kullback-Leibler divergence", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare the performance of two methods for estimating\nBayesian networks from data containing exogenous variables and random effects.\nThe first method is fully Bayesian in which a prior distribution is placed on\nthe exogenous variables, whereas the second method, which we call the residual\napproach, accounts for the effects of exogenous variables by using the notion\nof restricted maximum likelihood. We review the two score-based metrics, then\nstudy their performance by measuring the Kullback Leibler divergence, or\ndistance, between the two resulting posterior density functions. The Kullback\nLeibler divergence provides a natural framework for comparing distributions.\nThe residual approach is considerably simpler to apply in practice and we\ndemonstrate its utility both theoretically and via simulations. In particular,\nin applications where the exogenous variables are not of primary interest, we\nshow that the potential loss of information about parameters and induced\ncomponents of correlation, is generally small.\n", "versions": [{"version": "v1", "created": "Wed, 8 Sep 2010 07:27:47 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2011 05:10:37 GMT"}], "update_date": "2011-12-02", "authors_parsed": [["Kasza", "Jessica", ""], ["Solomon", "Patty", ""]]}, {"id": "1009.1914", "submitter": "Anthony Lee", "authors": "Anthony Lee, Francois Caron, Arnaud Doucet, Chris Holmes", "title": "A Hierarchical Bayesian Framework for Constructing Sparsity-inducing\n  Priors", "comments": "Submitted for publication; corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection techniques have become increasingly popular amongst\nstatisticians due to an increased number of regression and classification\napplications involving high-dimensional data where we expect some predictors to\nbe unimportant. In this context, Bayesian variable selection techniques\ninvolving Markov chain Monte Carlo exploration of the posterior distribution\nover models can be prohibitively computationally expensive and so there has\nbeen attention paid to quasi-Bayesian approaches such as maximum a posteriori\n(MAP) estimation using priors that induce sparsity in such estimates. We focus\non this latter approach, expanding on the hierarchies proposed to date to\nprovide a Bayesian interpretation and generalization of state-of-the-art\npenalized optimization approaches and providing simultaneously a natural way to\ninclude prior information about parameters within this framework. We give\nexamples of how to use this hierarchy to compute MAP estimates for linear and\nlogistic regression as well as sparse precision-matrix estimates in Gaussian\ngraphical models. In addition, an adaptive group lasso method is derived using\nthe framework.\n", "versions": [{"version": "v1", "created": "Thu, 9 Sep 2010 23:28:44 GMT"}, {"version": "v2", "created": "Thu, 16 Sep 2010 20:20:37 GMT"}], "update_date": "2010-09-20", "authors_parsed": [["Lee", "Anthony", ""], ["Caron", "Francois", ""], ["Doucet", "Arnaud", ""], ["Holmes", "Chris", ""]]}, {"id": "1009.1926", "submitter": "Yuzo Maruyama", "authors": "Yuzo Maruyama and William E. Strawderman", "title": "Robust Bayesian variable selection with sub-harmonic priors", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies Bayesian variable selection in linear models with general\nspherically symmetric error distributions. We propose sub-harmonic priors which\narise as a class of mixtures of Zellner's g-priors for which the Bayes factors\nare independent of the underlying error distribution, as long as it is in the\nspherically symmetric class. Because of this invariance to spherically\nsymmetric error distribution, we refer to our method as a robust Bayesian\nvariable selection method. We demonstrate that our Bayes factors have model\nselection consistency and are coherent. We also develop Laplace approximations\nto Bayes factors for a number of recently studied mixtures of g-priors that\nhave recently appeared in the literature (including our own) for Gaussian\nerrors. These approximations, in each case, are given by the Gaussian Bayes\nfactor based on BIC times a simple rational function of the prior's\nhyper-parameters and the R^2's for the respective models. We also extend model\nselection consistency for several g-prior based Bayes factor methods for\nGaussian errors to the entire class of spherically symmetric error\ndistributions. Additionally we demonstrate that our class of sub-harmonic\npriors are the only ones within a large class of mixtures of g-priors studied\nin the literature which are robust in our sense. A simulation study and an\nanalysis of two real data sets indicates good performance of our robust Bayes\nfactors relative to BIC and to other mixture of g-prior based methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 03:59:24 GMT"}, {"version": "v2", "created": "Mon, 20 Sep 2010 02:04:50 GMT"}, {"version": "v3", "created": "Thu, 1 Dec 2011 02:01:40 GMT"}, {"version": "v4", "created": "Mon, 11 Mar 2013 02:28:43 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Maruyama", "Yuzo", ""], ["Strawderman", "William E.", ""]]}, {"id": "1009.2031", "submitter": "Artur Lemonte", "authors": "Artur J. Lemonte and Silvia L.P. Ferrari", "title": "Testing hypotheses in the Birnbaum-Saunders distribution under type-II\n  censored samples", "comments": "Submitted for publication", "journal-ref": null, "doi": "10.1016/j.csda.2011.02.005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-parameter Birnbaum-Saunders distribution has been used succesfully to\nmodel fatigue failure times. Although censoring is typical in reliability and\nsurvival studies, little work has been published on the analysis of censored\ndata for this distribution. In this paper, we address the issue of performing\ntesting inference on the two parameters of the Birnbaum-Saunders distribution\nunder type-II right censored samples. The likelihood ratio statistic and a\nrecently proposed statistic, the gradient statistic, provide a convenient\nframework for statistical inference in such a case, since they do not require\nto obtain, estimate or invert an information matrix, which is an advantage in\nproblems involving censored data. An extensive Monte Carlo simulation study is\ncarried out in order to investigate and compare the finite sample performance\nof the likelihood ratio and the gradient tests. Our numerical results show\nevidence that the gradient test should be preferred. Three empirical\napplications are presented.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 15:18:30 GMT"}], "update_date": "2011-11-22", "authors_parsed": [["Lemonte", "Artur J.", ""], ["Ferrari", "Silvia L. P.", ""]]}, {"id": "1009.2190", "submitter": "Saralees Nadarajah", "authors": "C. S. Withers, S. Nadarajah", "title": "Accurate inference for a one parameter distribution based on the mean of\n  a transformed sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great deal of inference in statistics is based on making the approximation\nthat a statistic is normally distributed. The error in doing so is generally\n$O(n^{-1/2})$ and can be very considerable when the distribution is heavily\nbiased or skew. This note shows how one may reduce this error to\n$O(n^{-(j+1)/2})$, where $j$ is a given integer. The case considered is when\nthe statistic is the mean of the sample values from a continuous one-parameter\ndistribution, after the sample has undergone an initial transformation.\n", "versions": [{"version": "v1", "created": "Sat, 11 Sep 2010 17:45:48 GMT"}], "update_date": "2010-09-14", "authors_parsed": [["Withers", "C. S.", ""], ["Nadarajah", "S.", ""]]}, {"id": "1009.2260", "submitter": "Mark Tygert", "authors": "William Perkins, Mark Tygert, and Rachel Ward", "title": "Computing the confidence levels for a root-mean-square test of\n  goodness-of-fit, II", "comments": "14 pages, 3 figures (each with two parts), 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends our earlier article, \"Computing the confidence levels for\na root-mean-square test of goodness-of-fit;\" unlike in the earlier article, the\nmodels in the present paper involve parameter estimation -- both the null and\nalternative hypotheses in the associated tests are composite. We provide\nefficient black-box algorithms for calculating the asymptotic confidence levels\nof a variant on the classic chi-squared test. In some circumstances, it is also\nfeasible to compute the exact confidence levels via Monte Carlo simulation.\n", "versions": [{"version": "v1", "created": "Sun, 12 Sep 2010 19:22:10 GMT"}, {"version": "v2", "created": "Wed, 12 Jan 2011 17:38:28 GMT"}, {"version": "v3", "created": "Thu, 22 Dec 2011 17:37:48 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["Perkins", "William", ""], ["Tygert", "Mark", ""], ["Ward", "Rachel", ""]]}, {"id": "1009.2300", "submitter": "Chenlei Leng", "authors": "Chenlei Leng, Minh Ngoc Tran and David Nott", "title": "Bayesian Adaptive Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Bayesian adaptive Lasso (BaLasso) for variable selection and\ncoefficient estimation in linear regression. The BaLasso is adaptive to the\nsignal level by adopting different shrinkage for different coefficients.\nFurthermore, we provide a model selection machinery for the BaLasso by\nassessing the posterior conditional mode estimates, motivated by the\nhierarchical Bayesian interpretation of the Lasso. Our formulation also permits\nprediction using a model averaging strategy. We discuss other variants of this\nnew approach and provide a unified framework for variable selection using\nflexible penalties. Empirical evidence of the attractiveness of the method is\ndemonstrated via extensive simulation studies and data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 13 Sep 2010 06:02:37 GMT"}], "update_date": "2010-09-14", "authors_parsed": [["Leng", "Chenlei", ""], ["Tran", "Minh Ngoc", ""], ["Nott", "David", ""]]}, {"id": "1009.2302", "submitter": "Chenlei Leng", "authors": "Minh-Ngoc Tran, David Nott and Chenlei Leng", "title": "The Predictive Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a shrinkage procedure for simultaneous variable selection and\nestimation in generalized linear models (GLMs) with an explicit predictive\nmotivation. The procedure estimates the coefficients by minimizing the\nKullback-Leibler divergence of a set of predictive distributions to the\ncorresponding predictive distributions for the full model, subject to an $l_1$\nconstraint on the coefficient vector. This results in selection of a\nparsimonious model with similar predictive performance to the full model.\nThanks to its similar form to the original lasso problem for GLMs, our\nprocedure can benefit from available $l_1$-regularization path algorithms.\nSimulation studies and real-data examples confirm the efficiency of our method\nin terms of predictive performance on future observations.\n", "versions": [{"version": "v1", "created": "Mon, 13 Sep 2010 06:08:16 GMT"}], "update_date": "2010-09-14", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Nott", "David", ""], ["Leng", "Chenlei", ""]]}, {"id": "1009.2522", "submitter": "Itai Dattner", "authors": "I. Dattner, B. Reiser", "title": "Estimation of distribution functions in measurement error models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical problems are related to the pointwise estimation of dis-\ntribution functions when data contains measurement errors. Motivation for these\nproblems comes from diverse fields such as astronomy, reliability, quality\ncontrol, public health and survey data. Recently, Dattner, Goldenshluger and\nJuditsky (2011) showed that an estimator based on a direct inversion formula\nfor distribution functions has nice properties when the tail of the\ncharacteristic function of the mea- surement error distribution decays\npolynomially. In this paper we derive theoretical properties for this estimator\nfor the case where the error distri- bution is smoother and study its finite\nsample behavior for different error distributions. Our method is data-driven in\nthe sense that we use only known information, namely, the error distribution\nand the data. Applica- tion of the estimator to estimating hypertension\nprevalence based on real data is also examined.\n", "versions": [{"version": "v1", "created": "Mon, 13 Sep 2010 20:59:10 GMT"}, {"version": "v2", "created": "Thu, 13 Jan 2011 06:07:04 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2012 09:42:41 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Dattner", "I.", ""], ["Reiser", "B.", ""]]}, {"id": "1009.3045", "submitter": "Ian Dryden", "authors": "Ian L. Dryden, Xavier Pennec and Jean-Marc Peyrat", "title": "Power Euclidean metrics for covariance matrices with application to\n  diffusion tensor imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various metrics for comparing diffusion tensors have been recently proposed\nin the literature. We consider a broad family of metrics which is indexed by a\nsingle power parameter. A likelihood-based procedure is developed for choosing\nthe most appropriate metric from the family for a given dataset at hand. The\napproach is analogous to using the Box-Cox transformation that is frequently\ninvestigated in regression analysis. The methodology is illustrated with a\nsimulation study and an application to a real dataset of diffusion tensor\nimages of canine hearts.\n", "versions": [{"version": "v1", "created": "Wed, 15 Sep 2010 21:32:36 GMT"}], "update_date": "2010-09-17", "authors_parsed": [["Dryden", "Ian L.", ""], ["Pennec", "Xavier", ""], ["Peyrat", "Jean-Marc", ""]]}, {"id": "1009.3072", "submitter": "Ian Dryden", "authors": "Kim Kenobi and Ian L. Dryden", "title": "Bayesian matching of unlabelled point sets using Procrustes and\n  configuration models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of matching unlabelled point sets using Bayesian inference is\nconsidered. Two recently proposed models for the likelihood are compared, based\non the Procrustes size-and-shape and the full configuration. Bayesian inference\nis carried out for matching point sets using Markov chain Monte Carlo\nsimulation. An improvement to the existing Procrustes algorithm is proposed\nwhich improves convergence rates, using occasional large jumps in the burn-in\nperiod. The Procrustes and configuration methods are compared in a simulation\nstudy and using real data, where it is of interest to estimate the strengths of\nmatches between protein binding sites. The performance of both methods is\ngenerally quite similar, and a connection between the two models is made using\na Laplace approximation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Sep 2010 01:44:17 GMT"}], "update_date": "2010-09-17", "authors_parsed": [["Kenobi", "Kim", ""], ["Dryden", "Ian L.", ""]]}, {"id": "1009.3203", "submitter": "Stephan Huckemann", "authors": "Stephan Huckemann", "title": "Intrinsic Inference on the Mean Geodesic of Planar Shapes and Tree\n  Discrimination by Leaf Growth", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For planar landmark based shapes, taking into account the non-Euclidean\ngeometry of the shape space, a statistical test for a common mean first\ngeodesic principal component (GPC) is devised. It rests on one of two\nasymptotic scenarios, both of which are identical in a Euclidean geometry. For\nboth scenarios, strong consistency and central limit theorems are established,\nalong with an algorithm for the computation of a Ziezold mean geodesic. In\napplication, this allows to verify the geodesic hypothesis for leaf growth of\nCanadian black poplars and to discriminate genetically different trees by\nobservations of leaf shape growth over brief time intervals. With a test based\non Procrustes tangent space coordinates, not involving the shape space's\ncurvature, neither can be achieved.\n", "versions": [{"version": "v1", "created": "Thu, 16 Sep 2010 15:42:39 GMT"}], "update_date": "2010-09-17", "authors_parsed": [["Huckemann", "Stephan", ""]]}, {"id": "1009.3560", "submitter": "Zhijian Wang", "authors": "Zhijian Wang", "title": "Social Spiral Pattern in Experimental 2x2 Games", "comments": "in Chinese, 4 Figures, Keyword: evolutionary game theory,\n  experimental economics, spiral pattern, mixed equilibrium, lattice vector\n  field, JEL: C91, C70", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO nlin.PS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With evolutionary game theory, mathematicians, physicists and theoretical\nbiologists usually show us beautiful figures of population dynamic patterns.\n2x2 game (matching pennies game) is one of the classical cases. In this letter,\nwe report our finding that, there exists a dynamical pattern, called as social\nspiral, in human subjects 2x2 experiment data. In a flow/velocity vector field\nmethod, we explore the data in the discrete lattices of the macro-level social\nstrategy space in the games, and then above spiral pattern emergent. This\nfinding hints that, there exists a macro-level order beyond the stochastic\nprocess in micro-level. We notice that, the vector pattern provides an\ninteresting way to conceal evolutionary game theory models and experimental\neconomics data. This lattice vector field method provides a novel way for\nmodels evaluating and experiment designing.\n", "versions": [{"version": "v1", "created": "Sat, 18 Sep 2010 15:15:45 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 06:29:50 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Wang", "Zhijian", ""]]}, {"id": "1009.3669", "submitter": "Michael Finegold", "authors": "Michael Finegold, Mathias Drton", "title": "Robust graphical modeling of gene networks using classical and\n  alternative T-distributions", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS410 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2A, 1057-1080", "doi": "10.1214/10-AOAS410", "report-no": "IMS-AOAS-AOAS410", "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical Gaussian models have proven to be useful tools for exploring\nnetwork structures based on multivariate data. Applications to studies of gene\nexpression have generated substantial interest in these models, and resulting\nrecent progress includes the development of fitting methodology involving\npenalization of the likelihood function. In this paper we advocate the use of\nmultivariate $t$-distributions for more robust inference of graphs. In\nparticular, we demonstrate that penalized likelihood inference combined with an\napplication of the EM algorithm provides a computationally efficient approach\nto model selection in the $t$-distribution case. We consider two versions of\nmultivariate $t$-distributions, one of which requires the use of approximation\ntechniques. For this distribution, we describe a Markov chain Monte Carlo EM\nalgorithm based on a Gibbs sampler as well as a simple variational\napproximation that makes the resulting method feasible in large problems.\n", "versions": [{"version": "v1", "created": "Sun, 19 Sep 2010 23:36:42 GMT"}, {"version": "v2", "created": "Tue, 21 Sep 2010 12:46:28 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2011 08:02:29 GMT"}], "update_date": "2011-08-10", "authors_parsed": [["Finegold", "Michael", ""], ["Drton", "Mathias", ""]]}, {"id": "1009.4241", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Heng Lian", "title": "Gaussian process single-index models as emulators for computer\n  experiments", "comments": "23 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A single-index model (SIM) provides for parsimonious multi-dimensional\nnonlinear regression by combining parametric (linear) projection with\nunivariate nonparametric (non-linear) regression models. We show that a\nparticular Gaussian process (GP) formulation is simple to work with and ideal\nas an emulator for some types of computer experiment as it can outperform the\ncanonical separable GP regression model commonly used in this setting. Our\ncontribution focuses on drastically simplifying, re-interpreting, and then\ngeneralizing a recently proposed fully Bayesian GP-SIM combination, and then\nillustrating its favorable performance on synthetic data and a real-data\ncomputer experiment. Two R packages, both released on CRAN, have been augmented\nto facilitate inference under our proposed model(s).\n", "versions": [{"version": "v1", "created": "Wed, 22 Sep 2010 01:09:03 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2011 02:34:39 GMT"}, {"version": "v3", "created": "Wed, 17 Aug 2011 16:39:18 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Lian", "Heng", ""]]}, {"id": "1009.4279", "submitter": "Elena Stanghellini", "authors": "Elena Stanghellini, Barbara Vantaggi", "title": "Identification of discrete concentration graph models with one hidden\n  binary variable", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJ435 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 5A, 1920-1937", "doi": "10.3150/12-BEJ435", "report-no": "IMS-BEJ-BEJ435", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditions are presented for different types of identifiability of discrete\nvariable models generated over an undirected graph in which one node represents\na binary hidden variable. These models can be seen as extensions of the latent\nclass model to allow for conditional associations between the observable random\nvariables. Since local identification corresponds to full rank of the\nparametrization map, we establish a necessary and sufficient condition for the\nrank to be full everywhere in the parameter space. The condition is based on\nthe topology of the undirected graph associated to the model. For non-full rank\nmodels, the obtained characterization allows us to find the subset of the\nparameter space where the identifiability breaks down.\n", "versions": [{"version": "v1", "created": "Wed, 22 Sep 2010 07:10:42 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2013 07:29:24 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Stanghellini", "Elena", ""], ["Vantaggi", "Barbara", ""]]}, {"id": "1009.5331", "submitter": "Yilun Chen", "authors": "Yilun Chen, Ami Wiesel, Alfred O. Hero III", "title": "Robust Shrinkage Estimation of High-dimensional Covariance Matrices", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2011.2138698", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address high dimensional covariance estimation for elliptical distributed\nsamples, which are also known as spherically invariant random vectors (SIRV) or\ncompound-Gaussian processes. Specifically we consider shrinkage methods that\nare suitable for high dimensional problems with a small number of samples\n(large $p$ small $n$). We start from a classical robust covariance estimator\n[Tyler(1987)], which is distribution-free within the family of elliptical\ndistribution but inapplicable when $n<p$. Using a shrinkage coefficient, we\nregularize Tyler's fixed point iterations. We prove that, for all $n$ and $p$,\nthe proposed fixed point iterations converge to a unique limit regardless of\nthe initial condition. Next, we propose a simple, closed-form and data\ndependent choice for the shrinkage coefficient, which is based on a minimum\nmean squared error framework. Simulations demonstrate that the proposed method\nachieves low estimation error and is robust to heavy-tailed samples. Finally,\nas a real world application we demonstrate the performance of the proposed\ntechnique in the context of activity/intrusion detection using a wireless\nsensor network.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 17:13:05 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Chen", "Yilun", ""], ["Wiesel", "Ami", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1009.5689", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni, Victor Chernozhukov, Lie Wang", "title": "Square-Root Lasso: Pivotal Recovery of Sparse Signals via Conic\n  Programming", "comments": null, "journal-ref": "Biometrika (2011) 98(4): 791-806", "doi": "10.1093/biomet/asr043", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a pivotal method for estimating high-dimensional sparse linear\nregression models, where the overall number of regressors $p$ is large,\npossibly much larger than $n$, but only $s$ regressors are significant. The\nmethod is a modification of the lasso, called the square-root lasso. The method\nis pivotal in that it neither relies on the knowledge of the standard deviation\n$\\sigma$ or nor does it need to pre-estimate $\\sigma$. Moreover, the method\ndoes not rely on normality or sub-Gaussianity of noise. It achieves near-oracle\nperformance, attaining the convergence rate $\\sigma \\{(s/n)\\log p\\}^{1/2}$ in\nthe prediction norm, and thus matching the performance of the lasso with known\n$\\sigma$. These performance results are valid for both Gaussian and\nnon-Gaussian errors, under some mild moment restrictions. We formulate the\nsquare-root lasso as a solution to a convex conic programming problem, which\nallows us to implement the estimator using efficient algorithmic methods, such\nas interior-point and first-order methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 20:39:14 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2011 14:59:58 GMT"}, {"version": "v3", "created": "Sun, 2 Oct 2011 02:34:49 GMT"}, {"version": "v4", "created": "Sun, 11 Dec 2011 17:19:18 GMT"}, {"version": "v5", "created": "Sun, 18 Dec 2011 22:23:18 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Wang", "Lie", ""]]}, {"id": "1009.5839", "submitter": "Nicole Kraemer", "authors": "Gilles Blanchard, Nicole Kraemer", "title": "Optimal learning rates for Kernel Conjugate Gradient regression", "comments": "to appear in Neural Information Processing Systems 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove rates of convergence in the statistical sense for kernel-based least\nsquares regression using a conjugate gradient algorithm, where regularization\nagainst overfitting is obtained by early stopping. This method is directly\nrelated to Kernel Partial Least Squares, a regression method that combines\nsupervised dimensionality reduction with least squares projection. The rates\ndepend on two key quantities: first, on the regularity of the target regression\nfunction and second, on the intrinsic dimensionality of the data mapped into\nthe kernel space. Lower bounds on attainable rates depending on these two\nquantities were established in earlier literature, and we obtain upper bounds\nfor the considered method that match these lower bounds (up to a log factor) if\nthe true regression function belongs to the reproducing kernel Hilbert space.\nIf this assumption is not fulfilled, we obtain similar convergence rates\nprovided additional unlabeled data are available. The order of the learning\nrates match state-of-the-art results that were recently obtained for least\nsquares support vector machines and for linear regularization operators.\n", "versions": [{"version": "v1", "created": "Wed, 29 Sep 2010 11:05:55 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Blanchard", "Gilles", ""], ["Kraemer", "Nicole", ""]]}, {"id": "1009.5981", "submitter": "David R. Bickel", "authors": "Marta Padilla and David R. Bickel", "title": "Empirical Bayes methods corrected for small numbers of tests", "comments": "This version adds new methods and a simulation study", "journal-ref": "Statistical Applications in Genetics and Molecular Biology 11 (5),\n  art. 4 (2012)", "doi": "10.1515/1544-6115.1807", "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histogram-based empirical Bayes methods developed for analyzing data for\nlarge numbers of genes, SNPs, or other biological features tend to have large\nbiases when applied to data with a smaller number of features such as genes\nwith expression measured conventionally, proteins, and metabolites. To analyze\nsuch small-scale and medium-scale data in an empirical Bayes framework, we\nintroduce corrections of maximum likelihood estimators (MLE) of the local false\ndiscovery rate (LFDR). In this context, the MLE estimates the LFDR, which is a\nposterior probability of null hypothesis truth, by estimating the prior\ndistribution. The corrections lie in excluding each feature when estimating one\nor more parameters on which the prior depends. An application of the new\nestimators and previous estimators to protein abundance data illustrates how\ndifferent estimators lead to very different conclusions about which proteins\nare affected by cancer.\n  The estimators are compared using simulated data of two different numbers of\nfeatures, two different detectability levels, and all possible numbers of\naffected features. The simulations show that some of the corrected MLEs\nsubstantially reduce a negative bias of the MLE. (The best-performing corrected\nMLE was derived from the minimum description length principle.) However, even\nthe corrected MLEs have strong negative biases when the proportion of features\nthat are unaffected is greater than 90%. Therefore, since the number of\naffected features is unknown in the case of real data, we recommend an\noptimally weighted combination of the best of the corrected MLEs with a\nconservative estimator that has weaker parametric assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Sep 2010 19:30:39 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2012 23:38:33 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Padilla", "Marta", ""], ["Bickel", "David R.", ""]]}]