[{"id": "1504.00071", "submitter": "Kwabena Doku-Amponsah", "authors": "Samuel Iddi and Kwabena Doku-Amponsah", "title": "Statistical model for overdispersed count outcome with many zeros: an\n  approach for direct marginal inference", "comments": "28 pages", "journal-ref": "South African Statistical Journal 50, pp 313 - 337 (2016)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginalized models are in great demand by most researchers in the life\nsciences particularly in clinical trials, epidemiology, health-economics,\nsurveys and many others since they allow generalization of inference to the\nentire population under study. For count data, standard procedures such as the\nPoisson regression and negative binomial model provide population average\ninference for model parameters. However, occurrence of excess zero counts and\nlack of independence in empirical data have necessitated their extension to\naccommodate these phenomena. These extensions, though useful, complicates\ninterpretations of effects. For example, the zero-inflated Poisson model\naccounts for the presence of excess zeros but the parameter estimates do not\nhave a direct marginal inferential ability as its base model, the Poisson\nmodel. Marginalizations due to the presence of excess zeros are underdeveloped\nthough demand for such is interestingly high. The aim of this paper is to\ndevelop a marginalized model for zero-inflated univariate count outcome in the\npresence of overdispersion. Emphasis is placed on methodological development,\nefficient estimation of model parameters, implementation and application to two\nempirical studies. A simulation study is performed to assess the performance of\nthe model. Results from the analysis of two case studies indicated that the\nrefined procedure performs significantly better than models which do not\nsimultaneously correct for overdispersion and presence of excess zero counts in\nterms of likelihood comparisons and AIC values. The simulation studies also\nsupported these findings. In addition, the proposed technique yielded small\nbiases and mean square errors for model parameters. To ensure that the proposed\nmethod enjoys widespread use, it is implemented using the SAS NLMIXED procedure\nwith minimal coding efforts.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 00:32:11 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Iddi", "Samuel", ""], ["Doku-Amponsah", "Kwabena", ""]]}, {"id": "1504.00160", "submitter": "Francesca Condino", "authors": "Francesca Condino and Filippo Domma", "title": "A new distribution function with bounded support: the reflected\n  Generalized Topp-Leone Power Series distribution", "comments": null, "journal-ref": null, "doi": "10.1007/s40300-016-0095-6", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new flexible class of distributions with bounded\nsupport, called reflected Generalized Topp-Leone Power Series (rGTL-PS),\nobtained by compounding the reflected Generalized Topp-Leone (van Drop and\nKotz, 2006) and the family of Power Series distributions. The proposed class\nincludes, as special cases, some new distributions with limited support such as\nthe rGTL-Logarithmic, the rGTL-Geometric, the rGTL-Poisson and rGTL-Binomial.\nThis work is an attempt to partially fill a gap regarding the presence, in the\nliterature, of continuous distributions with bounded support, which instead\nappear to be very useful in many real contexts, included the reliability. Some\nproperties of the class, including moments, hazard rate and quantile are\ninvestigated. Moreover, the maximum likelihood estimators of the parameters are\nexamined and the observed Fisher information matrix provided. Finally, in order\nto show the usefulness of the new class, some applications to real data are\nreported.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 09:32:11 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Condino", "Francesca", ""], ["Domma", "Filippo", ""]]}, {"id": "1504.00238", "submitter": "Peng Liao", "authors": "Peng Liao, Predrag Klasnja, Ambuj Tewari and Susan A. Murphy", "title": "Sample Size Calculations for Micro-randomized Trials in mHealth", "comments": "29 pages, 5 figures, 18 tables", "journal-ref": "Statistics in medicine 35, no. 12 (2016): 1944-1971", "doi": "10.1002/sim.6847", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use and development of mobile interventions are experiencing rapid\ngrowth. In \"just-in-time\" mobile interventions, treatments are provided via a\nmobile device and they are intended to help an individual make healthy\ndecisions \"in the moment,\" and thus have a proximal, near future impact.\nCurrently the development of mobile interventions is proceeding at a much\nfaster pace than that of associated data science methods. A first step toward\ndeveloping data-based methods is to provide an experimental design for testing\nthe proximal effects of these just-in-time treatments. In this paper, we\npropose a \"micro-randomized\" trial design for this purpose. In a\nmicro-randomized trial, treatments are sequentially randomized throughout the\nconduct of the study, with the result that each participant may be randomized\nat the 100s or 1000s of occasions at which a treatment might be provided.\nFurther, we develop a test statistic for assessing the proximal effect of a\ntreatment as well as an associated sample size calculator. We conduct\nsimulation evaluations of the sample size calculator in various settings. Rules\nof thumb that might be used in designing a micro-randomized trial are\ndiscussed. This work is motivated by our collaboration on the HeartSteps mobile\napplication designed to increase physical activity.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 14:15:22 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 17:20:46 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Liao", "Peng", ""], ["Klasnja", "Predrag", ""], ["Tewari", "Ambuj", ""], ["Murphy", "Susan A.", ""]]}, {"id": "1504.00298", "submitter": "Richard Everitt", "authors": "Richard G. Everitt, Adam M. Johansen, Ellen Rowing, Melina\n  Evdemon-Hogan", "title": "Bayesian model comparison with un-normalised likelihoods", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-016-9629-2", "report-no": null, "categories": "stat.CO physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for which the likelihood function can be evaluated only up to a\nparameter-dependent unknown normalising constant, such as Markov random field\nmodels, are used widely in computer science, statistical physics, spatial\nstatistics, and network analysis. However, Bayesian analysis of these models\nusing standard Monte Carlo methods is not possible due to the intractability of\ntheir likelihood functions. Several methods that permit exact, or close to\nexact, simulation from the posterior distribution have recently been developed.\nHowever, estimating the evidence and Bayes' factors (BFs) for these models\nremains challenging in general. This paper describes new random weight\nimportance sampling and sequential Monte Carlo methods for estimating BFs that\nuse simulation to circumvent the evaluation of the intractable likelihood, and\ncompares them to existing methods. In some cases we observe an advantage in the\nuse of biased weight estimates. An initial investigation into the theoretical\nand empirical properties of this class of methods is presented. Some support\nfor the use of biased estimates is presented, but we advocate caution in the\nuse of such estimates.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 17:10:25 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 15:42:08 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2016 23:33:41 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Everitt", "Richard G.", ""], ["Johansen", "Adam M.", ""], ["Rowing", "Ellen", ""], ["Evdemon-Hogan", "Melina", ""]]}, {"id": "1504.00313", "submitter": "Marco Iglesias", "authors": "Marco A. Iglesias, Yulong Lu and Andrew M. Stuart", "title": "A Bayesian Level Set Method for Geometric Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a level set based approach to Bayesian geometric inverse\nproblems. In these problems the interface between different domains is the key\nunknown, and is realized as the level set of a function. This function itself\nbecomes the object of the inference. Whilst the level set methodology has been\nwidely used for the solution of geometric inverse problems, the Bayesian\nformulation that we develop here contains two significant advances: firstly it\nleads to a well-posed inverse problem in which the posterior distribution is\nLipschitz with respect to the observed data; and secondly it leads to\ncomputationally expedient algorithms in which the level set itself is updated\nimplicitly via the MCMC methodology applied to the level set function- no\nexplicit velocity field is required for the level set interface. Applications\nare numerous and include medical imaging, modelling of subsurface formations\nand the inverse source problem; our theory is illustrated with computational\nresults involving the last two applications.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 17:36:49 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Iglesias", "Marco A.", ""], ["Lu", "Yulong", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "1504.00340", "submitter": "Ralph Kenna", "authors": "Robert S. MacKay, Ralph Kenna, Robert J. Low and Sarah Parker", "title": "Calibration with confidence: A principled method for panel assessment", "comments": "32 pages including supplementary information; 5 figures", "journal-ref": "R. Soc. open sci. 2017 4 160760", "doi": "10.1098/rsos.160760", "report-no": null, "categories": "stat.ME physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequently, a set of objects has to be evaluated by a panel of assessors, but\nnot every object is assessed by every assessor. A problem facing such panels is\nhow to take into account different standards amongst panel members and varying\nlevels of confidence in their scores. Here, a mathematically-based algorithm is\ndeveloped to calibrate the scores of such assessors, addressing both of these\nissues. The algorithm is based on the connectivity of the graph of assessors\nand objects evaluated, incorporating declared confidences as weights on its\nedges. If the graph is sufficiently well connected, relative standards can be\ninferred by comparing how assessors rate objects they assess in common,\nweighted by the levels of confidence of each assessment. By removing these\nbiases, \"true\" values are inferred for all the objects. Reliability estimates\nfor the resulting values are obtained. The algorithm is tested in two case\nstudies, one by computer simulation and another based on realistic evaluation\ndata. The process is compared to the simple averaging procedure in widespread\nuse, and to Fisher's additive incomplete block analysis. It is anticipated that\nthe algorithm will prove useful in a wide variety of situations such as\nevaluation of the quality of research submitted to national assessment\nexercises; appraisal of grant proposals submitted to funding panels; ranking of\njob applicants; and judgement of performances on degree courses wherein\ncandidates can choose from lists of options.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 11:27:57 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 21:31:25 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["MacKay", "Robert S.", ""], ["Kenna", "Ralph", ""], ["Low", "Robert J.", ""], ["Parker", "Sarah", ""]]}, {"id": "1504.00397", "submitter": "Nirian Mart\\'in", "authors": "Juana Mar\\'ia Alonso, Nirian Mart\\'in, Leandro Pardo", "title": "New improved estimators for overdispersion in models with clustered\n  multinomial data and unequal cluster sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is usual to rely on the quasi-likelihood methods for deriving statistical\nmethods applied to clustered multinomial data with no underlying distribution.\nEven though extensive literature can be encountered for these kind of data\nsets, there are few investigations to deal with unequal cluster sizes. This\npaper aims to contribute to fill this gap by proposing new estimators for the\nintracluster correlation coefficient.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 21:32:22 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 12:26:05 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2015 20:00:51 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Alonso", "Juana Mar\u00eda", ""], ["Mart\u00edn", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1504.00441", "submitter": "Simon Bonner", "authors": "Simon J Bonner, Matthew R Schofield, Patrik Noren, and Steven J Price", "title": "Extending the Latent Multinomial Model with Complex Error Processes and\n  Dynamic Markov Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latent multinomial model (LMM) model of Link et al. (2010) provided a\ngeneral framework for modelling mark-recapture data with potential errors in\nidentification. Key to this approach was a Markov chain Monte Carlo (MCMC)\nscheme for sampling possible configurations of the counts true capture\nhistories that could have generated the observed data. This MCMC algorithm used\nvectors from a basis for the kernel of the linear map between the true and\nobserved counts to move between the possible configurations of the true data.\nSchofield and Bonner (2015) showed that a strict basis was sufficient for some\nmodels of the errors, including the model presented by Link et al. (2010), but\na larger set called a Markov basis may be required for more complex models. We\naddress two further challenges with this approach: 1) that models with more\ncomplex error mechanisms do not fit easily within the LMM and 2) that the\nMarkov basis can be difficult or impossible to compute for even moderate sized\nstudies. We address these issues by extending the LMM to separately model the\ncapture/demographic process and the error process and by developing a new MCMC\nsampling scheme using dynamic Markov bases. Our work is motivated by a study of\nQueen snakes (Regina septemvittata) in Kentucky, USA, and we use simulation to\ncompare the use of PIT tags, with perfect identification, and brands, which are\nprone to error, when estimating survival rates.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 03:46:56 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Bonner", "Simon J", ""], ["Schofield", "Matthew R", ""], ["Noren", "Patrik", ""], ["Price", "Steven J", ""]]}, {"id": "1504.00476", "submitter": "Zacharie Naulet", "authors": "Zacharie Naulet and Eric Barat", "title": "Some aspects of symmetric Gamma process mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present some specific aspects of symmetric Gamma process\nmixtures for use in regression models. We propose a new Gibbs sampler for\nsimulating the posterior and we establish adaptive posterior rates of\nconvergence related to the Gaussian mean regression problem.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 08:48:47 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2015 12:25:58 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2015 20:12:16 GMT"}, {"version": "v4", "created": "Thu, 28 Jul 2016 08:25:00 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Naulet", "Zacharie", ""], ["Barat", "Eric", ""]]}, {"id": "1504.00494", "submitter": "Daniel Nevo", "authors": "Daniel Nevo, Ya'acov Ritov", "title": "Identifying a minimal class of models for high-dimensional data", "comments": "36 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection consistency in the high-dimensional regression setting can be\nachieved only if strong assumptions are fulfilled. We therefore suggest to\npursue a different goal, which we call a minimal class of models. The minimal\nclass of models includes models that are similar in their prediction accuracy\nbut not necessarily in their elements. We suggest a random search algorithm to\nreveal candidate models. The algorithm implements simulated annealing while\nusing a score for each predictor that we suggest to derive using a combination\nof the Lasso and the Elastic Net. The utility of using a minimal class of\nmodels is demonstrated in the analysis of two datasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 09:44:17 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2015 17:09:13 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2015 11:28:38 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Nevo", "Daniel", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1504.00534", "submitter": "Ruth Heller", "authors": "Marina Bogomolov and Ruth Heller", "title": "Assessing replicability of findings across two studies of multiple\n  features", "comments": null, "journal-ref": "Biometrika (2018)", "doi": "10.1093/biomet/asy029", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replicability analysis aims to identify the findings that replicated across\nindependent studies that examine the same features. We provide powerful novel\nreplicability analysis procedures for two studies for FWER and for FDR control\non the replicability claims. The suggested procedures first select the\npromising features from each study solely based on that study, and then test\nfor replicability only the features that were selected in both studies. We\nincorporate the plug-in estimates of the fraction of null hypotheses in one\nstudy among the selected hypotheses by the other study. Since the fraction of\nnulls in one study among the selected features from the other study is\ntypically small, the power gain can be remarkable. We provide theoretical\nguarantees for the control of the appropriate error rates, as well as\nsimulations that demonstrate the excellent power properties of the suggested\nprocedures. We demonstrate the usefulness of our procedures on real data\nexamples from two application fields: behavioural genetics and microarray\nstudies.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 13:04:36 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Bogomolov", "Marina", ""], ["Heller", "Ruth", ""]]}, {"id": "1504.00608", "submitter": "Deukwoo Kwon", "authors": "Deukwoo Kwon and Isildinha M. Reis", "title": "Simulation-based Estimation of Mean and Standard Deviation for\n  Meta-analysis via Approximate Bayesian Computation (ABC)", "comments": null, "journal-ref": "BMC Med Res Methodol. 2015. 12;15:61", "doi": "10.1186/s12874-015-0055-5", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: When conducting a meta-analysis of a continuous outcome,\nestimated means and standard deviations from the selected studies are required\nin order to obtain an overall estimate of the mean effect and its confidence\ninterval. If these quantities are not directly reported in the publications,\nthey need to must be estimated from other reported summary statistics, such as\nthe median, the minimum, the maximum, and quartiles. Methods: We propose a\nsimulation-based estimation approach using the Approximate Bayesian Computation\n(ABC) technique for estimating mean and standard deviation based on various\nsets of summary statistics found in published studies. We conduct a simulation\nstudy to compare the proposed ABC method with the existing methods of Hozo et\nal. (2005), Bland (2015), and Wan et al. (2014). Results: In the estimation of\nthe standard deviation, our ABC method performs best in skewed or heavy-tailed\ndistributions. The average relative error (ARE) approaches zero as sample size\nincreases. In the normal distribution, our ABC performs well. However, the Wan\net al. method is best since it is based on the normal distribution assumption.\nWhen the distribution is skewed or heavy-tailed, the ARE of Wan et al. moves\naway from zero even as sample size increases. In the estimation of the mean,\nour ABC method is best since the AREs converge to zero. Conclusion: ABC is a\nflexible method for estimating the study-specific mean and standard deviation\nfor meta-analysis, especially with underlying skewed or heavy-tailed\ndistributions. The ABC method can be applied using other reported summary\nstatistics such as the posterior mean and 95% credible interval when Bayesian\nanalysis has been employed.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 16:32:34 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kwon", "Deukwoo", ""], ["Reis", "Isildinha M.", ""]]}, {"id": "1504.00781", "submitter": "Bhaveshkumar Dharmani", "authors": "Dharmani Bhaveshkumar C", "title": "The Gram-Charlier A Series based Extended Rule-of-Thumb for Bandwidth\n  Selection in Univariate and Multivariate Kernel Density Estimations", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article derives a novel Gram-Charlier A (GCA) Series based Extended\nRule-of-Thumb (ExROT) for bandwidth selection in Kernel Density Estimation\n(KDE). There are existing various bandwidth selection rules achieving\nminimization of the Asymptotic Mean Integrated Square Error (AMISE) between the\nestimated probability density function (PDF) and the actual PDF. The rules\ndiffer in a way to estimate the integration of the squared second order\nderivative of an unknown PDF $(f(\\cdot))$, identified as the roughness\n$R(f''(\\cdot))$. The simplest Rule-of-Thumb (ROT) estimates $R(f''(\\cdot))$\nwith an assumption that the density being estimated is Gaussian. Intuitively,\nbetter estimation of $R(f''(\\cdot))$ and consequently better bandwidth\nselection rules can be derived, if the unknown PDF is approximated through an\ninfinite series expansion based on a more generalized density assumption. As a\ndemonstration and verification to this concept, the ExROT derived in the\narticle uses an extended assumption that the density being estimated is near\nGaussian. This helps use of the GCA expansion as an approximation to the\nunknown near Gaussian PDF. The ExROT for univariate KDE is extended to that for\nmultivariate KDE. The required multivariate AMISE criteria is re-derived using\nelementary calculus of several variables, instead of Tensor calculus. The\nderivation uses the Kronecker product and the vector differential operator to\nachieve the AMISE expression in vector notations. There is also derived ExROT\nfor kernel based density derivative estimator.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 08:42:44 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["C", "Dharmani Bhaveshkumar", ""]]}, {"id": "1504.00860", "submitter": "Petter Arnesen", "authors": "Petter Arnesen, Tracy Holsclaw and Padhraic Smyth", "title": "Bayesian Detection of Changepoints in Finite-State Markov Chains for\n  Multiple Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the analysis of sets of categorical sequences consisting of\npiecewise homogeneous Markov segments. The sequences are assumed to be governed\nby a common underlying process with segments occurring in the same order for\neach sequence. Segments are defined by a set of unobserved changepoints where\nthe positions and number of changepoints can vary from sequence to sequence. We\npropose a Bayesian framework for analyzing such data, placing priors on the\nlocations of the changepoints and on the transition matrices and using Markov\nchain Monte Carlo (MCMC) techniques to obtain posterior samples given the data.\nExperimental results using simulated data illustrates how the methodology can\nbe used for inference of posterior distributions for parameters and\nchangepoints, as well as the ability to handle considerable variability in the\nlocations of the changepoints across different sequences. We also investigate\nthe application of the approach to sequential data from two applications\ninvolving monsoonal rainfall patterns and branching patterns in trees.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 15:26:04 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2015 09:23:23 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Arnesen", "Petter", ""], ["Holsclaw", "Tracy", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1504.00865", "submitter": "Stephane Chretien", "authors": "Stephane Chretien and Franck Corset", "title": "A lower bound on the expected optimal value of certain random linear\n  programs and application to shortest paths and reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper studies the expectation of the inspection time in complex aging\nsystems. Under reasonable assumptions, this problem is reduced to studying the\nexpectation of the length of the shortest path in the directed degradation\ngraph of the systems where the parameters are given by a pool of experts. The\nexpectation itself being sometimes out of reach, in closed form or even through\nMonte Carlo simulations in the case of large systems, we propose an easily\ncomputable lower bound. The proposed bound applies to a rather general class of\nlinear programs with random nonnegative costs and is directly inspired from the\nupper bound of Dyer, Frieze and McDiarmid [Math.Programming {\\bf 35} (1986),\nno.1,3--16].\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 15:37:56 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 16:54:24 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Chretien", "Stephane", ""], ["Corset", "Franck", ""]]}, {"id": "1504.00917", "submitter": "Mar\\'ia Pilar Fr\\'ias Bustamante", "authors": "Mar\\'ia Pilar Fr\\'ias, Alexander V. Ivanov, Nikolai Leonenko,\n  Francisco Mart\\'inez, Mar\\'ia Dolores Ruiz-Medina", "title": "Detecting hidden periodicities for models with cyclical errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the estimation of parameters in the harmonic regression with\ncyclically dependent errors is addressed. Asymptotic properties of the\nleast-squares estimates are analyzed by simulation experiments. By numerical\nsimulation, we prove that consistency and asymptotic normality of the\nleast-squares parameter estimator studied holds under different scenarios,\nwhere theoretical results do not exist, and have yet to be proven. In\nparticular, these two asymptotic properties are shown by simulations for the\nleast-squares parameter estimator in the non-linear regression model analyzed,\nwhen its error term is defined as a non-linear transformation of a Gaussian\nrandom process displaying long-range dependence.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 19:40:00 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Fr\u00edas", "Mar\u00eda Pilar", ""], ["Ivanov", "Alexander V.", ""], ["Leonenko", "Nikolai", ""], ["Mart\u00ednez", "Francisco", ""], ["Ruiz-Medina", "Mar\u00eda Dolores", ""]]}, {"id": "1504.00945", "submitter": "Harrison B. Prosper", "authors": "Harrison B. Prosper", "title": "Practical Statistics for Particle Physicists", "comments": "presented at the 2012 European School of High-Energy Physics, La\n  Pommeraye, Anjou, France, 6-19 June 2012", "journal-ref": "CERN-2014-008, pp.195-216", "doi": "10.5170/CERN-2014-008", "report-no": null, "categories": "stat.ME hep-ex physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a few of the key ideas of statistical analysis using two\nreal-world examples to illustrate how these ideas are used in practice.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 21:45:55 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Prosper", "Harrison B.", ""]]}, {"id": "1504.00946", "submitter": "Laurel Stell", "authors": "Laurel Stell and Chiara Sabatti", "title": "Genetic variant selection: learning across traits and sites", "comments": "Published at http://www.genetics.org/content/202/2/439 in GENETICS\n  (http://www.genetics.org)", "journal-ref": "Genetics 2016, vol. 202, no. 2, 439-455", "doi": "10.1534/genetics.115.184572", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider resequencing studies of associated loci and the problem of\nprioritizing sequence variants for functional follow-up. Working within the\nmultivariate linear regression framework helps us to account for correlation\nacross variants, and adopting a Bayesian approach naturally leads to posterior\nprobabilities that incorporate all information about the variants' function. We\ndescribe two novel prior distributions that facilitate learning the role of\neach variant by borrowing evidence across phenotypes and across mutations in\nthe same gene. We illustrate their potential advantages with simulations and\nre-analyzing a dataset of sequencing variants.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 22:02:37 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 20:47:33 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2015 01:48:41 GMT"}, {"version": "v4", "created": "Mon, 4 Apr 2016 22:25:05 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Stell", "Laurel", ""], ["Sabatti", "Chiara", ""]]}, {"id": "1504.01061", "submitter": "Agust\\'in G. Nogales", "authors": "A.G. Nogales, P. P\\'erez and P. Monfort", "title": "Minimum Risk Equivariant Estimation of the Parameters of the General\n  Half-Normal Distribution by Means of a Monte Carlo Method to Approximate\n  Conditional Expectations", "comments": "14 pages Accepted for publication in Latin American Journal of\n  Probability and Mathematical. arXiv admin note: substantial text overlap with\n  arXiv:1306.1182", "journal-ref": "ALEA, Lat. Am. J. Probab. Math. Stat. (2015)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of estimating the parameters of the general\nhalf-normal distribution. Namely, the problem of determining the minimum risk\nequi\\-va\\-riant (MRE) estimators of the parameters is explored. Simulation\nstudies are realized to compare the behavior of these estimators with maximum\nlikelihood and unbiased estimators. A natural Monte Carlo method to compute\nconditional expectations is used to approximate the MRE estimation of the\nlocation parameter because its expression involves two conditional expectations\nnot easily computables. The used Monte Carlo method is justified by a theorem\nof Besicovitch on differentiation of measures, and has been slightly modified\nto solve a sort of \"curse of dimensionality\" problem appearing in the\nestimation of this parameter. This method has been implicitly used in the last\nyears in the context of ABC (approximate Bayesian computation) methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 22:01:27 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Nogales", "A. G.", ""], ["P\u00e9rez", "P.", ""], ["Monfort", "P.", ""]]}, {"id": "1504.01097", "submitter": "Deepesh Bhati Mr.", "authors": "Deepesh Bhati, Pooja Kumawat, E. G\\'omez D\\'eniz", "title": "A New Count Model Generated from Mixed Poisson Transmuted Exponential\n  Family with an application to Health Care Data", "comments": "this files contains 24 pages excluding references, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new mixed Poisson distribution is introduced. This new\ndistribution is obtained by utilizing mixing process, with Poisson distribution\nas mixed distribution and Transmuted Exponential distribution as mixing\ndistribution. Some distributional properties like unimodality, moments,\nover-dispersion, Taylor series expansion of proposed model are studied.\nEstimation of the parameters using method of moments, method of moments and\nproportion and maximum likelihood estimation along with data fitting experiment\nto show its advantage over some existing distribution. Further, an actuarial\napplications in context of aggregate claim distribution is discussed. Finally,\nwe discuss a count regression model based on proposed distribution and its\nusefulness over some well established model.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 08:13:16 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 16:51:05 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Bhati", "Deepesh", ""], ["Kumawat", "Pooja", ""], ["D\u00e9niz", "E. G\u00f3mez", ""]]}, {"id": "1504.01128", "submitter": "Peter Rousseeuw", "authors": "Mia Hubert, Peter J. Rousseeuw, Pieter Segaert", "title": "Multivariate and functional classification using depth and distance", "comments": null, "journal-ref": "Advances in Data Analysis and Classification, 2017, Vol. 11,\n  445-466", "doi": "10.1007/s11634-016-0269-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct classifiers for multivariate and functional data. Our approach\nis based on a kind of distance between data points and classes. The distance\nmeasure needs to be robust to outliers and invariant to linear transformations\nof the data. For this purpose we can use the bagdistance which is based on\nhalfspace depth. It satisfies most of the properties of a norm but is able to\nreflect asymmetry when the class is skewed. Alternatively we can compute a\nmeasure of outlyingness based on the skew-adjusted projection depth. In either\ncase we propose the DistSpace transform which maps each data point to the\nvector of its distances to all classes, followed by k-nearest neighbor (kNN)\nclassification of the transformed data points. This combines invariance and\nrobustness with the simplicity and wide applicability of kNN. The proposal is\ncompared with other methods in experiments with real and simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 15:32:37 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 15:57:07 GMT"}, {"version": "v3", "created": "Thu, 7 Jul 2016 16:35:26 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Hubert", "Mia", ""], ["Rousseeuw", "Peter J.", ""], ["Segaert", "Pieter", ""]]}, {"id": "1504.01147", "submitter": "Kiranmoy Chatterjee Mr.", "authors": "Kiranmoy Chatterjee (Bidhannagar College, Kolkata-700064, India) and\n  Diganta Mukherjee (Indian Statistical Institute, Kolkata-700108, India)", "title": "On the Population Size Estimation from Dual-record System:\n  Profile-Likelihood Approaches", "comments": "24 Pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by various applications, we consider the problem of homogeneous\nhuman population size (N) estimation from Dual-record system (DRS)\n(equivalently, two-sample capture-recapture experiment). The likelihood\nestimate from the independent capture-recapture model Mt is widely used in this\ncontext though appropriateness of the behavioral dependence model Mtb is\nunanimously acknowledged. Our primary aim is to investigate the use of several\nrelevant pseudo-likelihood methods profiling N, explicitly for model Mtb. An\nadjustment over profile likelihood is proposed. Simulation studies are carried\nout to evaluate the performance of the proposed method compared with Bayes\nestimate suggested for general capture-recapture experiment by Lee et al.\n(Statistica Sinica, 2003, vol. 13). We also analyse the effect of possible\nmodel mis-specification, due to the use of model Mt, in terms of efficiency and\nrobustness. Finally two real life examples with different characteristics are\npresented for illustration of the methodologies discussed.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 18:34:09 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Chatterjee", "Kiranmoy", "", "Bidhannagar College, Kolkata-700064, India"], ["Mukherjee", "Diganta", "", "Indian Statistical Institute, Kolkata-700108, India"]]}, {"id": "1504.01239", "submitter": "Thanakorn Nitithumbundit", "authors": "Thanakorn Nitithumbundit and Jennifer S.K. Chan", "title": "An ECM algorithm for Skewed Multivariate Variance Gamma Distribution in\n  Normal Mean-Variance Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normal mean-variance mixture distributions are widely applied to simplify a\nmodel's implementation and improve their computational efficiency under the\nMaximum Likelihood (ML) approach. Especially for distributions with normal\nmean-variance mixtures representation such as the multivariate skewed variance\ngamma (MSVG) distribution, it utilises the expectation-conditional-maximisation\n(ECM) algorithm to iteratively obtain the ML estimates. To facilitate\napplication to financial time series, the mean is further extended to include\nautoregressive terms. Techniques are proposed to deal with the unbounded\ndensity for small shape parameter and to speed up the convergence. Simulation\nstudies are conducted to demonstrate the applicability of this model and\nexamine estimation properties. Finally, the MSVG model is applied to analyse\nthe returns of five daily closing price market indices and standard errors for\nthe estimated parameters are computed using Louis's method.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 08:53:44 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 23:56:25 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Nitithumbundit", "Thanakorn", ""], ["Chan", "Jennifer S. K.", ""]]}, {"id": "1504.01551", "submitter": "Wagner Bonat", "authors": "Wagner Hugo Bonat and Bent J{\\o}rgensen", "title": "Multivariate Covariance Generalized Linear Models", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": "10.1111/rssc.12145", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for non-normal multivariate data analysis\ncalled multivariate covariance generalized linear models (McGLMs), designed to\nhandle multivariate response variables, along with a wide range of temporal and\nspatial correlation structures defined in terms of a covariance link function\ncombined with a matrix linear predictor involving known matrices. The method is\nmotivated by three data examples that are not easily handled by existing\nmethods. The first example concerns multivariate count data, the second\ninvolves response variables of mixed types, combined with repeated measures and\nlongitudinal structures, and the third involves a spatio-temporal analysis of\nrainfall data. The models take non-normality into account in the conventional\nway by means of a variance function, and the mean structure is modelled by\nmeans of a link function and a linear predictor. The models are fitted using an\nefficient Newton scoring algorithm based on quasi-likelihood and Pearson\nestimating functions, using only second-moment assumptions. This provides a\nunified approach to a wide variety of different types of response variables and\ncovariance structures, including multivariate extensions of repeated measures,\ntime series, longitudinal, spatial and spatio-temporal structures.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 11:18:53 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bonat", "Wagner Hugo", ""], ["J\u00f8rgensen", "Bent", ""]]}, {"id": "1504.01664", "submitter": "Javier  Gonz\\'alez", "authors": "Alberto Mu\\~noz, Gabriel Martos and Javier Gonz\\'alez", "title": "Level Sets Based Distances for Probability Measures and Ensembles with\n  Applications", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study Probability Measures (PM) from a functional point of\nview: we show that PMs can be considered as functionals (generalized functions)\nthat belong to some functional space endowed with an inner product. This\napproach allows us to introduce a new family of distances for PMs, based on the\naction of the PM functionals on `interesting' functions of the sample. We\npropose a specific (non parametric) metric for PMs belonging to this class,\nbased on the estimation of density level sets. Some real and simulated data\nsets are used to measure the performance of the proposed distance against a\nbattery of distances widely used in Statistics and related areas.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 16:54:45 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Mu\u00f1oz", "Alberto", ""], ["Martos", "Gabriel", ""], ["Gonz\u00e1lez", "Javier", ""]]}, {"id": "1504.01705", "submitter": "Sooraj K. Ambat", "authors": "Deepa K. G. and Sooraj K. Ambat and K.V.S. Hari", "title": "Fusion of Sparse Reconstruction Algorithms for Multiple Measurement\n  Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the recovery of sparse signals that share a common support from\nmultiple measurement vectors. The performance of several algorithms developed\nfor this task depends on parameters like dimension of the sparse signal,\ndimension of measurement vector, sparsity level, measurement noise. We propose\na fusion framework, where several multiple measurement vector reconstruction\nalgorithms participate and the final signal estimate is obtained by combining\nthe signal estimates of the participating algorithms. We present the conditions\nfor achieving a better reconstruction performance than the participating\nalgorithms. Numerical simulations demonstrate that the proposed fusion\nalgorithm often performs better than the participating algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 18:26:25 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["G.", "Deepa K.", ""], ["Ambat", "Sooraj K.", ""], ["Hari", "K. V. S.", ""]]}, {"id": "1504.01823", "submitter": "Anru Zhang", "authors": "Tianxi Cai, T. Tony Cai, Anru Zhang", "title": "Structured Matrix Completion with Applications to Genomic Data\n  Integration", "comments": "Accepted for publication in Journal of the American Statistical\n  Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Matrix completion has attracted significant recent attention in many fields\nincluding statistics, applied mathematics and electrical engineering. Current\nliterature on matrix completion focuses primarily on independent sampling\nmodels under which the individual observed entries are sampled independently.\nMotivated by applications in genomic data integration, we propose a new\nframework of structured matrix completion (SMC) to treat structured missingness\nby design. Specifically, our proposed method aims at efficient matrix recovery\nwhen a subset of the rows and columns of an approximately low-rank matrix are\nobserved. We provide theoretical justification for the proposed SMC method and\nderive lower bound for the estimation errors, which together establish the\noptimal rate of recovery over certain classes of approximately low-rank\nmatrices. Simulation studies show that the method performs well in finite\nsample under a variety of configurations. The method is applied to integrate\nseveral ovarian cancer genomic studies with different extent of genomic\nmeasurements, which enables us to construct more accurate prediction rules for\novarian cancer survival.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 04:14:07 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Cai", "Tianxi", ""], ["Cai", "T. Tony", ""], ["Zhang", "Anru", ""]]}, {"id": "1504.01865", "submitter": "Andrew Zammit-Mangion", "authors": "Noel Cressie and Andrew Zammit-Mangion", "title": "Multivariate Spatial Covariance Models: A Conditional Approach", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate geostatistics is based on modelling all covariances between all\npossible combinations of two or more variables at any sets of locations in a\ncontinuously indexed domain. Multivariate spatial covariance models need to be\nbuilt with care, since any covariance matrix that is derived from such a model\nmust be nonnegative-definite. In this article, we develop a conditional\napproach for spatial-model construction whose validity conditions are easy to\ncheck. We start with bivariate spatial covariance models and go on to\ndemonstrate the approach's connection to multivariate models defined by\nnetworks of spatial variables. In some circumstances, such as modelling\nrespiratory illness conditional on air pollution, the direction of conditional\ndependence is clear. When it is not, the two directional models can be\ncompared. More generally, the graph structure of the network reduces the number\nof possible models to compare. Model selection then amounts to finding possible\ncausative links in the network. We demonstrate our conditional approach on\nsurface temperature and pressure data, where the role of the two variables is\nseen to be asymmetric.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 08:32:53 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 09:00:04 GMT"}, {"version": "v3", "created": "Wed, 5 Oct 2016 23:21:50 GMT"}, {"version": "v4", "created": "Fri, 7 Oct 2016 19:34:01 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Cressie", "Noel", ""], ["Zammit-Mangion", "Andrew", ""]]}, {"id": "1504.01955", "submitter": "Paul S. Clarke", "authors": "Paul S. Clarke, Tom M. Palmer, Frank Windmeijer", "title": "Estimating Structural Mean Models with Multiple Instrumental Variables\n  Using the Generalised Method of Moments", "comments": "Published at http://dx.doi.org/10.1214/14-STS503 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 1, 96-117", "doi": "10.1214/14-STS503", "report-no": "IMS-STS-STS503", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables analysis using genetic markers as instruments is now a\nwidely used technique in epidemiology and biostatistics. As single markers tend\nto explain only a small proportion of phenotypic variation, there is increasing\ninterest in using multiple genetic markers to obtain more precise estimates of\ncausal parameters. Structural mean models (SMMs) are semiparametric models that\nuse instrumental variables to identify causal parameters. Recently, interest\nhas started to focus on using these models with multiple instruments,\nparticularly for multiplicative and logistic SMMs. In this paper we show how\nadditive, multiplicative and logistic SMMs with multiple orthogonal binary\ninstrumental variables can be estimated efficiently in models with no further\n(continuous) covariates, using the generalised method of moments (GMM)\nestimator. We discuss how the Hansen J-test can be used to test for model\nmisspecification, and how standard GMM software routines can be used to fit\nSMMs. We further show that multiplicative SMMs, like the additive SMM, identify\na weighted average of local causal effects if selection is monotonic. We use\nthese methods to reanalyse a study of the relationship between adiposity and\nhypertension using SMMs with two genetic markers as instruments for adiposity.\nWe find strong effects of adiposity on hypertension.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 13:22:52 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Clarke", "Paul S.", ""], ["Palmer", "Tom M.", ""], ["Windmeijer", "Frank", ""]]}, {"id": "1504.02382", "submitter": "Shahab Basiri", "authors": "Shahab Basiri, Esa Ollila and Visa Koivunen", "title": "Robust, scalable and fast bootstrap method for analyzing large scale\n  data", "comments": "This paper is submitted for publication in IEEE Transactions On\n  Signal Processing, 8 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TSP.2015.2498121", "report-no": null, "categories": "stat.ME cs.IR cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of performing statistical inference for\nlarge scale data sets i.e., Big Data. The volume and dimensionality of the data\nmay be so high that it cannot be processed or stored in a single computing\nnode. We propose a scalable, statistically robust and computationally efficient\nbootstrap method, compatible with distributed processing and storage systems.\nBootstrap resamples are constructed with smaller number of distinct data points\non multiple disjoint subsets of data, similarly to the bag of little bootstrap\nmethod (BLB) [1]. Then significant savings in computation is achieved by\navoiding the re-computation of the estimator for each bootstrap sample.\nInstead, a computationally efficient fixed-point estimation equation is\nanalytically solved via a smart approximation following the Fast and Robust\nBootstrap method (FRB) [2]. Our proposed bootstrap method facilitates the use\nof highly robust statistical methods in analyzing large scale data sets. The\nfavorable statistical properties of the method are established analytically.\nNumerical examples demonstrate scalability, low complexity and robust\nstatistical performance of the method in analyzing large data sets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 16:48:28 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2015 20:01:28 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Basiri", "Shahab", ""], ["Ollila", "Esa", ""], ["Koivunen", "Visa", ""]]}, {"id": "1504.02661", "submitter": "Sebastian  Dorn", "authors": "Sebastian Dorn and Torsten A. En{\\ss}lin", "title": "Stochastic determination of matrix determinants", "comments": "8 pages, 5 figures", "journal-ref": "Phys. Rev. E 92, 013302 (2015)", "doi": "10.1103/PhysRevE.92.013302", "report-no": null, "categories": "physics.data-an astro-ph.IM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix determinants play an important role in data analysis, in particular\nwhen Gaussian processes are involved. Due to currently exploding data volumes,\nlinear operations - matrices - acting on the data are often not accessible\ndirectly but are only represented indirectly in form of a computer routine.\nSuch a routine implements the transformation a data vector undergoes under\nmatrix multiplication. While efficient probing routines to estimate a matrix's\ndiagonal or trace, based solely on such computationally affordable\nmatrix-vector multiplications, are well known and frequently used in signal\ninference, there is no stochastic estimate for its determinant. We introduce a\nprobing method for the logarithm of a determinant of a linear operator. Our\nmethod rests upon a reformulation of the log-determinant by an integral\nrepresentation and the transformation of the involved terms into stochastic\nexpressions. This stochastic determinant determination enables large-size\napplications in Bayesian inference, in particular evidence calculations, model\ncomparison, and posterior determination.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 12:43:15 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 09:08:13 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Dorn", "Sebastian", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1504.02813", "submitter": "Camila Pedroso Estevam de Souza Dr.", "authors": "Camila P. E. de Souza, Nancy E. Heckman and Helena Xu", "title": "Switching nonparametric regression models for multi-curve data", "comments": "24 pages, 4 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and apply an approach for analyzing multi-curve data where each\ncurve is driven by a latent state process. The state at any particular point\ndetermines a smooth function, forcing the individual curve to switch from one\nfunction to another. Thus each curve follows what we call a switching\nnonparametric regression model. We develop an EM algorithm to estimate the\nmodel parameters. We also obtain standard errors for the parameter estimates of\nthe state process. We consider several types of state processes: independent\nand identically distributed, independent but depending on a covariate and\nMarkov. Simulation studies show the frequentist properties of our estimates. We\napply our methods to a data set of a building's power usage.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 23:04:27 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 21:36:30 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 16:42:09 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["de Souza", "Camila P. E.", ""], ["Heckman", "Nancy E.", ""], ["Xu", "Helena", ""]]}, {"id": "1504.02887", "submitter": "Matthias Killiches", "authors": "Matthias Killiches and Claudia Czado", "title": "Block-Maxima of Vines", "comments": "To appear in Extreme Value Modelling and Risk Analysis: Methods and\n  Applications. Eds. D. Dey and J. Yan. Chapman & Hall/CRC Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the dependence structure of finite block-maxima of multivariate\ndistributions. We provide a closed form expression for the copula density of\nthe vector of the block-maxima. Further, we show how partial derivatives of\nthree-dimensional vine copulas can be obtained by only one-dimensional\nintegration. Combining these results allows the numerical treatment of the\nblock-maxima of any three-dimensional vine copula for finite block-sizes. We\nlook at certain vine copula specifications and examine how the density of the\nblock-maxima behaves for different block-sizes. Additionally, a real data\nexample from hydrology is considered. In extreme-value theory for multivariate\nnormal distributions, a certain scaling of each variable and the correlation\nmatrix is necessary to obtain a non-trivial limiting distribution when the\nblock-size goes to infinity. This scaling is applied to different\nthree-dimensional vine copula specifications.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 15:15:43 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Killiches", "Matthias", ""], ["Czado", "Claudia", ""]]}, {"id": "1504.02913", "submitter": "Monia Ranalli", "authors": "Monia Ranalli and Roberto Rocci", "title": "A pairwise likelihood approach to simultaneous clustering and\n  dimensional reduction of ordinal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on clustering for continuous data is rich and wide;\ndifferently, that one developed for categorical data is still limited. In some\ncases, the problem is made more difficult by the presence of noise\nvariables/dimensions that do not contain information about the clustering\nstructure and could mask it. The aim of this paper is to propose a model for\nsimultaneous clustering and dimensionality reduction of ordered categorical\ndata able to detect the discriminative dimensions discarding the noise ones.\nFollowing the underlying response variable approach, the observed variables are\nconsidered as a discretization of underlying first-order latent continuous\nvariables distributed as a Gaussian mixture. To recognize discriminative and\nnoise dimensions, these variables are considered to be linear combinations of\ntwo independent sets of second-order latent variables where only one contains\nthe information about the cluster structure while the other contains noise\ndimensions. The model specification involves multidimensional integrals that\nmake the maximum likelihood estimation cumbersome and in some cases infeasible.\nTo overcome this issue the parameter estimation is carried out through an\nEM-like algorithm maximizing a pairwise log-likelihood. Examples of application\nof the model on real and simulated data are performed to show the effectiveness\nof the proposal.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 20:21:47 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Ranalli", "Monia", ""], ["Rocci", "Roberto", ""]]}, {"id": "1504.02935", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban, Kristen Fortney, Stuart K. Kim, Art B. Owen", "title": "Optimal Multiple Testing Under a Gaussian Prior on the Effect Sizes", "comments": null, "journal-ref": null, "doi": "10.1093/biomet/asv050", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new method for frequentist multiple testing with Bayesian prior\ninformation. Our procedure finds a new set of optimal p-value weights called\nthe Bayes weights. Prior information is relevant to many multiple testing\nproblems. Existing methods assume fixed, known effect sizes available from\nprevious studies. However, the case of uncertain information is usually the\nnorm. For a Gaussian prior on effect sizes, we show that finding the optimal\nweights is a non-convex problem. Despite the non-convexity, we give an\nefficient algorithm that solves this problem nearly exactly. We show that our\nmethod can discover new loci in genome-wide association studies. On several\ndata sets it compares favorably to other methods. Open source code is\navailable.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 05:49:29 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Dobriban", "Edgar", ""], ["Fortney", "Kristen", ""], ["Kim", "Stuart K.", ""], ["Owen", "Art B.", ""]]}, {"id": "1504.02995", "submitter": "Yuan Liao", "authors": "Jianqing Fan, Yuan Liao, Han Liu", "title": "An Overview on the Estimation of Large Covariance and Precision Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating large covariance and precision matrices are fundamental in modern\nmultivariate analysis. The problems arise from statistical analysis of large\npanel economics and finance data. The covariance matrix reveals marginal\ncorrelations between variables, while the precision matrix encodes conditional\ncorrelations between pairs of variables given the remaining variables. In this\npaper, we provide a selective review of several recent developments on\nestimating large covariance and precision matrices. We focus on two general\napproaches: rank based method and factor model based method. Theories and\napplications of both approaches are presented. These methods are expected to be\nwidely applicable to analysis of economic and financial data.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 18:08:17 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 21:37:00 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Fan", "Jianqing", ""], ["Liao", "Yuan", ""], ["Liu", "Han", ""]]}, {"id": "1504.03381", "submitter": "Cedric Ginestet", "authors": "Cedric E. Ginestet, Richard Emsley, Sabine Landau", "title": "Convex Combination of Ordinary Least Squares and Two-stage Least Squares\n  Estimators", "comments": "33 pages. 8 figures, 1 table. To be presented at UK-CIM (Causal\n  Inference Meeting) in Bristol, in April 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of confounders, the ordinary least squares (OLS) estimator is\nknown to be biased. This problem can be remedied by using the two-stage least\nsquares (TSLS) estimator, based on the availability of valid instrumental\nvariables (IVs). This reduction in bias, however, is offset by an increase in\nvariance. Under standard assumptions, the OLS has indeed a larger bias than the\nTSLS estimator; and moreover, one can prove that the sample variance of the OLS\nestimator is no greater than the one of the TSLS. Therefore, it is natural to\nask whether one could combine the desirable properties of the OLS and TSLS\nestimators. Such a trade-off can be achieved through a convex combination of\nthese two estimators, thereby producing our proposed convex least squares (CLS)\nestimator. The relative contribution of the OLS and TSLS estimators is here\nchosen to minimize a sample estimate of the mean squared error (MSE) of their\nconvex combination. This proportion parameter is proved to be unique, whenever\nthe OLS and TSLS differ in MSEs. Remarkably, we show that this proportion\nparameter can be estimated from the data, and that the resulting CLS estimator\nis consistent. We also show how the CLS framework can incorporate other\nasymptotically unbiased estimators, such as the jackknife IV estimator (JIVE).\nThe finite-sample properties of the CLS estimator are investigated using Monte\nCarlo simulations, in which we independently vary the amount of confounding and\nthe strength of the instrument. Overall, the CLS estimator is found to\noutperform the TSLS estimator in terms of MSE. The method is also applied to a\nclassic data set from econometrics, which models the financial return to\neducation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 22:20:22 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Ginestet", "Cedric E.", ""], ["Emsley", "Richard", ""], ["Landau", "Sabine", ""]]}, {"id": "1504.03384", "submitter": "Robert L Obenchain", "authors": "Robert L. Obenchain", "title": "Affine Reduction of Dimensionality: An Origin-Centric Perspective", "comments": "Risk Benefit Statistics LLC, wizbob@att.net", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider statistical methods for reduction of multivariate dimensionality\nthat have invariance and/or commutativity properties under the affine group of\ntransformations (origin translations plus linear combinations of coordinates\nalong initial axes). The methods discussed here differ from traditional\nprincipal component and coordinate approaches in that they are origin-centric.\nBecause all Cartesian coordinates of the origin are zero, it is the unique\nfixed point for subsequent linear transformations of point scatters. Whenever\nvisualizations allow shifting between and/or combining of Cartesian and polar\ncoordinate representations, as in Biplots, the location of this origin is\ncritical. Specifically, origin-centric visualizations enhance the psychology of\ngraphical perception by yielding scatters that can be interpreted as Dyson\nswarms. The key factor is typically the analyst's choice of origin via an\ninitial \"centering\" translation; this choice determines whether the recovered\nscatter will have either no points depicted as being near the origin or else\none (or more) points exactly coincident with this origin.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 22:38:47 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Obenchain", "Robert L.", ""]]}, {"id": "1504.03430", "submitter": "Hashem Salarzadeh Jenatabadi", "authors": "Hashem Salarzadeh Jenatabadi", "title": "A Tutorial for Analyzing Structural Equation Modelling", "comments": "7 pages and 1 chart", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a tutorial discussion on analyzing structural equation\nmodelling (SEM). SEM can be regarded as regression models with observed and\nunobserved indicators, have been extensively applied to practical and\nfundamental studies. We deliver an introduction to SEM method and a detailed\ndescription of how to deal with analyzing the data with this kind of modelling.\nThe intended audience is statisticians/mathematicians/methodologists who either\nknow about SEM or simple basic statistics especially in regression and\nlinear/nonlinear modeling, and Ph.D. students in statistics, mathematics,\nmanagement, psychology, and even computer science.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 06:35:54 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Jenatabadi", "Hashem Salarzadeh", ""]]}, {"id": "1504.03441", "submitter": "Hashem Salarzadeh Jenatabadi", "authors": "Hashem Salarzadeh Jenatabadi", "title": "An Overview of Path Analysis: Mediation Analysis Concept in Structural\n  Equation Modeling", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a tutorial discussion on path analysis structure with\nconcept of structural equation modelling (SEM). The paper delivers an\nintroduction to path analysis technique and explain to how to deal with\nanalyzing the data with this kind of statistical methodology especially with a\nmediator in the research model. The intended audience is statisticians,\nmathematicians, or methodologists who either know about SEM or simple basic\nstatistics especially in regression and linear/nonlinear modeling, and Ph.D.\nstudents in statistics, mathematics, management, psychology, and even computer\nscience.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 07:34:09 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Jenatabadi", "Hashem Salarzadeh", ""]]}, {"id": "1504.03454", "submitter": "Keren Shen", "authors": "Keren Shen, Jianfeng Yao and Wai Keung Li", "title": "Forecasting High-Dimensional Realized Volatility Matrices Using A Factor\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and forecasting covariance matrices of asset returns play a crucial\nrole in finance. The availability of high frequency intraday data enables the\nmodeling of the realized covariance matrix directly. However, most models in\nthe literature suffer from the curse of dimensionality. To solve the problem,\nwe propose a factor model with a diagonal CAW model for the factor realized\ncovariance matrices. Asymptotic theory is derived for the estimated parameters.\nIn an extensive empirical analysis, we find that the number of parameters can\nbe reduced significantly. Furthermore, the proposed model maintains a\ncomparable performance with a benchmark vector autoregressive model.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 08:43:32 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Shen", "Keren", ""], ["Yao", "Jianfeng", ""], ["Li", "Wai Keung", ""]]}, {"id": "1504.03574", "submitter": "Peter Aronow", "authors": "Peter M. Aronow, Forrest W. Crawford", "title": "Nonparametric Identification for Respondent-Driven Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling is a survey method for hidden or hard-to-reach\npopulations in which sampled individuals recruit others in the study population\nvia their social links. The most popular estimator for for the population mean\nassumes that individual sampling probabilities are proportional to each\nsubject's reported degree in a social network connecting members of the hidden\npopulation. However, it remains unclear under what circumstances these\nestimators are valid, and what assumptions are formally required to identify\npopulation quantities. In this short note we detail nonparametric\nidentification results for the population mean when the sampling probability is\nassumed to be a function of network degree known to scale. Importantly, we\nestablish general conditions for the consistency of the popular Volz-Heckathorn\n(VH) estimator. Our results imply that the conditions for consistency of the VH\nestimator are far less stringent than those suggested by recent work on\ndiagnostics for RDS. In particular, our results do not require random sampling\nor the existence of a network connecting the population.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 14:57:33 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Aronow", "Peter M.", ""], ["Crawford", "Forrest W.", ""]]}, {"id": "1504.03594", "submitter": "Wolfgang Garn", "authors": "Wolfgang Garn and James Aitken", "title": "Splitting hybrid Make-To-Order and Make-To-Stock demand profiles", "comments": "demand analysis; time series; outlier detection; production strategy;\n  Make-To-Order(MTO); Make-To-Stock(MTS); 15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a demand time series is analysed to support Make-To-Stock (MTS)\nand Make-To-Order (MTO) production decisions. Using a purely MTS production\nstrategy based on the given demand can lead to unnecessarily high inventory\nlevels thus it is necessary to identify likely MTO episodes.\n  This research proposes a novel outlier detection algorithm based on special\ndensity measures. We divide the time series' histogram into three clusters. One\nwith frequent-low volume covers MTS items whilst a second accounts for high\nvolumes which is dedicated to MTO items. The third cluster resides between the\nprevious two with its elements being assigned to either the MTO or MTS class.\nThe algorithm can be applied to a variety of time series such as stationary and\nnon-stationary ones.\n  We use empirical data from manufacturing to study the extent of inventory\nsavings. The percentage of MTO items is reflected in the inventory savings\nwhich were shown to be an average of 18.1%.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 15:54:29 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Garn", "Wolfgang", ""], ["Aitken", "James", ""]]}, {"id": "1504.03712", "submitter": "Kyungchul Song", "authors": "Kyungchul Song", "title": "Measuring the Graph Concordance of Locally Dependent Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a simple measure of a concordance pattern among\nobserved outcomes along a network, i.e., the pattern in which adjacent outcomes\ntend to be more strongly correlated than non-adjacent outcomes. The graph\nconcordance measure can be generally used to quantify the empirical relevance\nof a network in explaining cross-sectional dependence of the outcomes, and as\nshown in the paper, can also be used to quantify the extent of homophily under\ncertain conditions. When one observes a single large network, it is nontrivial\nto make inference about the concordance pattern. Assuming dependency graph,\nthis paper develops a permutation-based confidence interval for the graph\nconcordance measure. The confidence interval is valid in finite samples when\nthe outcomes are exchangeable, and under the dependency graph assumption\ntogether with other regularity conditions, is shown to exhibit asymptotic\nvalidity. Monte Carlo simulation results show that the validity of the\npermutation method is more robust to various graph configurations than the\nasymptotic method.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 20:35:59 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2015 18:14:19 GMT"}, {"version": "v3", "created": "Mon, 12 Sep 2016 21:36:58 GMT"}, {"version": "v4", "created": "Thu, 6 Apr 2017 23:22:12 GMT"}, {"version": "v5", "created": "Thu, 6 Jul 2017 01:00:25 GMT"}, {"version": "v6", "created": "Thu, 31 Aug 2017 21:59:31 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Song", "Kyungchul", ""]]}, {"id": "1504.03718", "submitter": "Hyunseung Kang", "authors": "Hyunseung Kang, T. Tony Cai, Dylan S. Small", "title": "A simple and robust confidence interval for causal effects with possibly\n  invalid instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables have been widely used to estimate the causal effect of\na treatment on an outcome. Existing confidence intervals for causal effects\nbased on instrumental variables assume that all of the putative instrumental\nvariables are valid; a valid instrumental variable is a variable that affects\nthe outcome only by affecting the treatment and is not related to unmeasured\nconfounders. However, in practice, some of the putative instrumental variables\nare likely to be invalid. This paper presents a simple and general approach to\nconstruct a confidence interval that is robust to possibly invalid instruments.\nThe robust confidence interval has theoretical guarantees on having the correct\ncoverage and can also be used to assess the sensitivity of inference when\ninstrumental variables assumptions are violated. The paper also shows that the\nrobust confidence interval outperforms traditional confidence intervals popular\nin instrumental variables literature when invalid instruments are present. The\nnew approach is applied to a developmental economics study of the causal effect\nof income on food expenditures.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 20:49:06 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2015 03:28:35 GMT"}, {"version": "v3", "created": "Tue, 12 Jul 2016 21:57:46 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Kang", "Hyunseung", ""], ["Cai", "T. Tony", ""], ["Small", "Dylan S.", ""]]}, {"id": "1504.03733", "submitter": "Leopoldo Catania", "authors": "Mauro Bernardi and Leopoldo Catania", "title": "Switching-GAS Copula Models With Application to Systemic Risk", "comments": "59 pages, 60 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent financial disasters have emphasised the need to accurately predict\nextreme financial losses and their consequences for the institutions belonging\nto a given financial market. The ability of econometric models to predict\nextreme events strongly relies on their flexibility to account for the highly\nnonlinear and asymmetric dependence observed in financial returns. We develop a\nnew class of flexible Copula models where the evolution of the dependence\nparameters follow a Markov-Switching Generalised Autoregressive Score (SGASC)\ndynamics. Maximum Likelihood estimation is consistently performed using the\nInference Functions for Margins (IFM) approach and a version of the\nExpectation-Maximisation (EM) algorithm specifically tailored to this class of\nmodels. The SGASC models are then used to estimate the Conditional\nValue-at-Risk (CoVaR), which is defined as the VaR of a given asset conditional\non another asset (or portfolio) being in financial distress, and the\nConditional Expected Shortfall (CoES). Our empirical investigation shows that\nthe proposed SGASC models are able to explain and predict the systemic risk\ncontribution of several European countries. Moreover, we also find that the\nSGASC models outperform competitors using several CoVaR backtesting procedures.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 21:43:23 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 17:50:06 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2016 16:34:01 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Bernardi", "Mauro", ""], ["Catania", "Leopoldo", ""]]}, {"id": "1504.03804", "submitter": "Subhajit Dutta Dr.", "authors": "Subhajit Dutta and Anil K. Ghosh", "title": "Multi-scale Classification using Localized Spatial Depth", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we develop and investigate a new classifier based on\nfeatures extracted using spatial depth. Our construction is based on fitting a\ngeneralized additive model to the posterior probabilities of the different\ncompeting classes. To cope with possible multi-modal as well as non-elliptic\npopulation distributions, we develop a localized version of spatial depth and\nuse that with varying degrees of localization to build the classifier. Final\nclassification is done by aggregating several posterior probability estimates\neach of which is obtained using localized spatial depth with a fixed scale of\nlocalization. The proposed classifier can be conveniently used even when the\ndimension is larger than the sample size, and its good discriminatory power for\nsuch data has been established using theoretical as well as numerical results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 07:27:41 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Dutta", "Subhajit", ""], ["Ghosh", "Anil K.", ""]]}, {"id": "1504.04100", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh and Ayanendranath Basu", "title": "Testing Composite Null Hypothesis Based on $S$-Divergences", "comments": "13 pages", "journal-ref": "Statistics and Probability Letters, July 2016, 114, 38--47", "doi": "10.1016/j.spl.2016.02.007", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust test for composite null hypothesis based on the general\n$S$-divergence family. This requires a non-trivial extension of the results of\nGhosh et al.~(2015). We derive the asymptotic and theoretical robustness\nproperties of the resulting test along with the properties of the minimum\n$S$-divergence estimators under parameter restrictions imposed by the null\nhypothesis. An illustration in the context of the normal model is also\npresented.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 05:38:25 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1504.04259", "submitter": "Nicolas Marie Nicolas Marie", "authors": "Francis Lavergne and Nicolas Marie", "title": "A Skew Normal Model of the Dose-Effect Relation in Pharmacology", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a skew-normal model of the relation between a dose $d >\n0$ and a quantitative measure $E(d)$ of an effect of the administered drug.\nPrecisely, $E(d)$ is a measure of the therapeutic response or a measure of a\nside-effect. Some existing and additional properties of the logistic functions\nare proved, and a skew-normal model of the escape time of rats under an\nexperimental antidepressant medication is provided.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 14:56:31 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Lavergne", "Francis", ""], ["Marie", "Nicolas", ""]]}, {"id": "1504.04489", "submitter": "Garritt Page", "authors": "Garritt L. Page and Fernando A. Quintana", "title": "Spatial Product Partition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling geostatistical or areal data, spatial structure is commonly\naccommodated via a covariance function for the former and a neighborhood\nstructure for the latter. In both cases the resulting spatial structure is a\nconsequence of implicit spatial grouping in that observations near in space are\nassumed to behave similarly. It would be desirable to develop spatial methods\nthat explicitly model the partitioning of spatial locations providing more\ncontrol over resulting spatial structures and being able to better balance\nglobal vs local spatial dependence. To this end, we extend product partition\nmodels to a spatial setting so that the partitioning of locations into\nspatially dependent clusters is explicitly modeled. We explore the spatial\nstructures that result from employing a spatial product partition model and\ndemonstrate its flexibility in accommodating many types of spatial\ndependencies. We illustrate the method's utility through simulation studies and\nan education application.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 11:58:29 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Page", "Garritt L.", ""], ["Quintana", "Fernando A.", ""]]}, {"id": "1504.04595", "submitter": "Richard Samworth", "authors": "Timothy I. Cannings and Richard J. Samworth", "title": "Random-projection ensemble classification", "comments": "49 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a very general method for high-dimensional classification, based\non careful combination of the results of applying an arbitrary base classifier\nto random projections of the feature vectors into a lower-dimensional space. In\none special case that we study in detail, the random projections are divided\ninto disjoint groups, and within each group we select the projection yielding\nthe smallest estimate of the test error. Our random projection ensemble\nclassifier then aggregates the results of applying the base classifier on the\nselected projections, with a data-driven voting threshold to determine the\nfinal assignment. Our theoretical results elucidate the effect on performance\nof increasing the number of projections. Moreover, under a boundary condition\nimplied by the sufficient dimension reduction assumption, we show that the test\nexcess risk of the random projection ensemble classifier can be controlled by\nterms that do not depend on the original data dimension and a term that becomes\nnegligible as the number of projections increases. The classifier is also\ncompared empirically with several other popular high-dimensional classifiers\nvia an extensive simulation study, which reveals its excellent finite-sample\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 18:13:50 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 13:49:11 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Cannings", "Timothy I.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1504.04696", "submitter": "Arnak Dalalyan S.", "authors": "Samuel Balmand, Arnak S. Dalalyan", "title": "On estimation of the diagonal elements of a sparse precision matrix", "comments": "Companion R package at\n  http://cran.r-project.org/web/packages/DESP/index.html", "journal-ref": "Electron. J. Statist. Volume 10, Number 1, 1551-1579 (2016)", "doi": "10.1214/16-EJS1148", "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present several estimators of the diagonal elements of the\ninverse of the covariance matrix, called precision matrix, of a sample of iid\nrandom vectors. The focus is on high dimensional vectors having a sparse\nprecision matrix. It is now well understood that when the underlying\ndistribution is Gaussian, the columns of the precision matrix can be estimated\nindependently form one another by solving linear regression problems under\nsparsity constraints. This approach leads to a computationally efficient\nstrategy for estimating the precision matrix that starts by estimating the\nregression vectors, then estimates the diagonal entries of the precision matrix\nand, in a final step, combines these estimators for getting estimators of the\noff-diagonal entries. While the step of estimating the regression vector has\nbeen intensively studied over the past decade, the problem of deriving\nstatistically accurate estimators of the diagonal entries has received much\nless attention. The goal of the present paper is to fill this gap by presenting\nfour estimators---that seem the most natural ones---of the diagonal entries of\nthe precision matrix and then performing a comprehensive empirical evaluation\nof these estimators. The estimators under consideration are the residual\nvariance, the relaxed maximum likelihood, the symmetry-enforced maximum\nlikelihood and the penalized maximum likelihood. We show, both theoretically\nand empirically, that when the aforementioned regression vectors are estimated\nwithout error, the symmetry-enforced maximum likelihood estimator has the\nsmallest estimation error. However, in a more realistic setting when the\nregression vector is estimated by a sparsity-favoring computationally efficient\nmethod, the qualities of the estimators become relatively comparable with a\nslight advantage for the residual variance estimator.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 08:56:38 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2015 10:04:17 GMT"}, {"version": "v3", "created": "Fri, 29 May 2015 12:17:08 GMT"}, {"version": "v4", "created": "Wed, 25 May 2016 14:16:17 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Balmand", "Samuel", ""], ["Dalalyan", "Arnak S.", ""]]}, {"id": "1504.04700", "submitter": "Gerhard Tutz", "authors": "Gerhard Tutz and Moritz Berger", "title": "Tree-Structured Modelling of Categorical Predictors in Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear and additive models are very efficient regression tools\nbut the selection of relevant terms becomes difficult if higher order\ninteractions are needed. In contrast, tree-based methods also known as\nrecursive partitioning are explicitly designed to model a specific form of\ninteraction but with their focus on interaction tend to neglect the main\neffects. The method proposed here focusses on the main effects of categorical\npredictors by using tree type methods to obtain clusters. In particular when\nthe predictor has many categories one wants to know which of the categories\nhave to be distinguished with respect to their effect on the response. The\ntree-structured approach allows to detect clusters of categories that share the\nsame effect while letting other variables, in particular metric variables, have\na linear or additive effect on the response. An algorithm for the fitting is\nproposed and various stopping criteria are evaluated. The preferred stopping\ncriterion is based on $p$-values representing a conditional inference\nprocedure. In addition, stability of clusters are investigated and the\nrelevance of variables is investigated by bootstrap methods. Several\napplications show the usefulness of tree-structured clustering and a small\nsimulation study demonstrates that the fitting procedure works well.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 09:26:55 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Tutz", "Gerhard", ""], ["Berger", "Moritz", ""]]}, {"id": "1504.04935", "submitter": "Jianfeng Yao", "authors": "Weiming Li, Jiaqi Chen and Jianfeng Yao", "title": "Testing the independence of two random vectors where only one dimension\n  is large", "comments": "16 pages and 2 figures", "journal-ref": "Statistics: A Journal of Theoretical and Applied Statistics\n  51(1):141-153, January 2017", "doi": "10.1080/02331888.2016.1266988", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For testing the independence of two vectors with respective dimensions $p_1$\nand $p_2$, the existing literature in high-dimensional statistics all assume\nthat both dimensions $p_1$ and $p_2$ grow to infinity with the sample size.\nHowever, as evidenced in the RNA-sequencing data analysis discussed in the\npaper, it happens frequently that one of the dimension is quite small and the\nother quite large compared to the sample size. In this paper, we address this\nnew asymptotic framework for the independence test. A new test procedure is\nintroduced and its asymptotic normality is established when the vectors are\nnormal distributed. A Mote-Carlo study demonstrates the consistency of the\nprocedure and exhibits its superiority over some existing high-dimensional\nprocedures. Applied to the RNA-sequencing data mentioned above, we obtain very\nconvincing results on pairwise independence/dependence of gene isoform\nexpressions as attested by prior knowledge established in that field. Lastly,\nMonte-Carlo experiments show that the procedure is robust against the normality\nassumption on the population vectors.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 04:26:38 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Li", "Weiming", ""], ["Chen", "Jiaqi", ""], ["Yao", "Jianfeng", ""]]}, {"id": "1504.04941", "submitter": "Patrick Perry", "authors": "Patrick O. Perry", "title": "Fast Moment-Based Estimation for Hierarchical Models", "comments": "36 pages, 7 figures; includes supplementary material; accepted for\n  publication at JRSS-B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical models allow for heterogeneous behaviours in a population while\nsimultaneously borrowing estimation strength across all subpopulations.\nUnfortunately, existing likelihood-based methods for fitting hierarchical\nmodels have high computational demands, and these demands have limited their\nadoption in large-scale prediction and inference problems. This paper proposes\na moment-based procedure for estimating the parameters of a hierarchical model\nwhich has its roots in a method originally introduced by Cochran in 1937. The\nmethod trades statistical efficiency for computational efficiency. It gives\nconsistent parameter estimates, competitive prediction error performance, and\nsubstantial computational improvements. When applied to a large-scale\nrecommender system application and compared to a standard maximum likelihood\nprocedure, the method delivers competitive prediction performance while\nreducing the sequential computation time from hours to minutes.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 05:46:13 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 02:18:28 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 18:40:11 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Perry", "Patrick O.", ""]]}, {"id": "1504.05415", "submitter": "Harold Bae", "authors": "Harold Bae, Thomas Perls, Martin Steinberg, Paola Sebastiani", "title": "Bayesian Polynomial Regression Models to Fit Multiple Genetic Models for\n  Quantitative Traits", "comments": "Published at http://dx.doi.org/10.1214/14-BA880 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 1, 53-74", "doi": "10.1214/14-BA880", "report-no": "VTeX-BA-BA880", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a coherent Bayesian framework for selection of the most likely\nmodel from the five genetic models (genotypic, additive, dominant, co-dominant,\nand recessive) commonly used in genetic association studies. The approach uses\na polynomial parameterization of genetic data to simultaneously fit the five\nmodels and save computations. We provide a closed-form expression of the\nmarginal likelihood for normally distributed data, and evaluate the performance\nof the proposed method and existing method through simulated and real\ngenome-wide data sets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 13:09:25 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Bae", "Harold", ""], ["Perls", "Thomas", ""], ["Steinberg", "Martin", ""], ["Sebastiani", "Paola", ""]]}, {"id": "1504.05438", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Christopher R. Genovese, Larry Wasserman", "title": "Density Level Sets: Asymptotics, Inference, and Visualization", "comments": "Accepted to JASA-T&M. 40 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive asymptotic theory for the plug-in estimate for density level sets\nunder Hausdoff loss. Based on the asymptotic theory, we propose two bootstrap\nconfidence regions for level sets. The confidence regions can be used to\nperform tests for anomaly detection and clustering. We also introduce a\ntechnique to visualize high dimensional density level sets by combining mode\nclustering and multidimensional scaling.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 14:04:07 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 15:35:53 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Genovese", "Christopher R.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1504.05651", "submitter": "Kun Zhang", "authors": "Kun Zhang, Jiji Zhang, Bernhard Sch\\\"olkopf", "title": "Distinguishing Cause from Effect Based on Exogeneity", "comments": "11 pages, 4 figures, published in Proceedings of the 15th conference\n  on Theoretical Aspects of Rationality and Knowledge (TARK'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in structural equation modeling have produced several\nmethods that can usually distinguish cause from effect in the two-variable\ncase. For that purpose, however, one has to impose substantial structural\nconstraints or smoothness assumptions on the functional causal models. In this\npaper, we consider the problem of determining the causal direction from a\nrelated but different point of view, and propose a new framework for causal\ndirection determination. We show that it is possible to perform causal\ninference based on the condition that the cause is \"exogenous\" for the\nparameters involved in the generating process from the cause to the effect. In\nthis way, we avoid the structural constraints required by the SEM-based\napproaches. In particular, we exploit nonparametric methods to estimate\nmarginal and conditional distributions, and propose a bootstrap-based approach\nto test for the exogeneity condition; the testing results indicate the causal\ndirection between two variables. The proposed method is validated on both\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 04:35:33 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Zhang", "Kun", ""], ["Zhang", "Jiji", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1504.05659", "submitter": "ShengLi Tzeng", "authors": "ShengLi Tzeng, Hsin-Cheng Huang", "title": "Multi-Resolution Spatial Random-Effects Models for Irregularly Spaced\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatial random-effects model is flexible in modeling spatial covariance\nfunctions, and is computationally efficient for spatial prediction via fixed\nrank kriging. However, the success of this model depends on an appropriate set\nof basis functions. In this research, we propose a class of basis functions\nextracted from thin-plate splines. These functions are ordered in terms of\ntheir degrees of smoothness with a higher-order function corresponding to\nlarger-scale features and a lower-order one corresponding to smaller-scale\ndetails, leading to a parsimonious representation for a nonstationary spatial\ncovariance function. Consequently, only a small to moderate number of functions\nare needed in a spatial random-effects model. The proposed class of basis\nfunctions has several advantages over commonly used ones. First, we do not need\nto concern about the allocation of the basis functions, but simply select the\ntotal number of functions corresponding to a resolution. Second, only a small\nnumber of basis functions is usually required, which facilitates computation.\nThird, estimation variability of model parameters can be considerably reduced,\nand hence more precise covariance function estimates can be obtained. Fourth,\nthe proposed basis functions depend only on the data locations but not the\nmeasurements taken at those locations, and are applicable regardless of whether\nthe data locations are sparse or irregularly spaced. In addition, we derive a\nsimple close-form expression for the maximum likelihood estimates of model\nparameters in the spatial random-effects model. Some numerical examples are\nprovided to demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 05:44:44 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Tzeng", "ShengLi", ""], ["Huang", "Hsin-Cheng", ""]]}, {"id": "1504.05690", "submitter": "Gabriela Ciuperca", "authors": "Gabriela Ciuperca and Zahraa Salloum", "title": "Empirical likelihood test for high-dimensional two-sample model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non parametric method based on the empirical likelihood is proposed for\ndetecting the change in the coefficients of high-dimensional linear model where\nthe number of model variables may increase as the sample size increases. This\namounts to testing the null hypothesis of no change against the alternative of\none change in the regression coefficients. Based on the theoretical asymptotic\nbehaviour of the empirical likelihood ratio statistic, we propose, for a fixed\ndesign, a simpler test statistic, easier to use in practice. The asymptotic\nnormality of the proposed test statistic under the null hypothesis is proved, a\nresult which is different from the $\\chi^2$ law for a model with a fixed\nvariable number. Under alternative hypothesis, the test statistic diverges. We\ncan then find the asymptotic confidence region for the difference of parameters\nof the two phases. Some Monte-Carlo simulations study the behaviour of the\nproposed test statistic.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 08:27:43 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 17:07:49 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Ciuperca", "Gabriela", ""], ["Salloum", "Zahraa", ""]]}, {"id": "1504.05738", "submitter": "Eleni Ioanna Delatola", "authors": "Eleni Ioanna Delatola, Emilie Lebarbier, Tristan Mary-Huard,\n  Fran\\c{c}ois Radvanyi, St\\'ephane Robin, Jennifer Wong", "title": "SegCorr: a statistical procedure for the detection of genomic regions of\n  correlated expression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Motivation: Detecting local correlations in expression between neighbor genes\nalong the genome has proved to be an effective strategy to identify possible\ncauses of transcriptional deregulation in cancer. It has been successfully used\nto illustrate the role of mechanisms such as copy number variation (CNV) or\nepigenetic alterations as factors that may significantly alter expression in\nlarge chromosomic regions (gene silencing or gene activation).\n  Results: The identification of correlated regions requires segmenting the\ngene expression correlation matrix into regions of homogeneously correlated\ngenes and assessing whether the observed local correlation is significantly\nhigher than the background chromosomal correlation. A unified statistical\nframework is proposed to achieve these two tasks, where optimal segmentation is\nefficiently performed using dynamic programming algorithm, and detection of\nhighly correlated regions is then achieved using an exact test procedure. We\nalso propose a simple and efficient procedure to correct the expression signal\nfor mechanisms already known to impact expression correlation. The performance\nand robustness of the proposed procedure, called SegCorr, are evaluated on\nsimulated data. The procedure is illustrated on cancer data, where the signal\nis corrected for correlations possibly caused by copy number variation. The\ncorrection permitted the detection of regions with high correlations linked to\nDNA methylation.\n  Availability and implementation: R package SegCorr is available on the CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 11:27:05 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Delatola", "Eleni Ioanna", ""], ["Lebarbier", "Emilie", ""], ["Mary-Huard", "Tristan", ""], ["Radvanyi", "Fran\u00e7ois", ""], ["Robin", "St\u00e9phane", ""], ["Wong", "Jennifer", ""]]}, {"id": "1504.05781", "submitter": "Edward Cohen", "authors": "E.A.K. Cohen, D. Kim and R.J. Ober", "title": "Cramer-Rao Lower Bound for Point Based Image Registration with\n  Heteroscedastic Error Model for Application in Single Molecule Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cramer-Rao lower bound for the estimation of the affine transformation\nparameters in a multivariate heteroscedastic errors-in-variables model is\nderived. The model is suitable for feature-based image registration in which\nboth sets of control points are localized with errors whose covariance matrices\nvary from point to point. With focus given to the registration of fluorescence\nmicroscopy images, the Cramer-Rao lower bound for the estimation of a feature's\nposition (e.g. of a single molecule) in a registered image is also derived. In\nthe particular case where all covariance matrices for the localization errors\nare scalar multiples of a common positive definite matrix (e.g. the identity\nmatrix), as can be assumed in fluorescence microscopy, then simplified\nexpressions for the Cramer-Rao lower bound are given. Under certain simplifying\nassumptions these expressions are shown to match asymptotic distributions for a\npreviously presented set of estimators. Theoretical results are verified with\nsimulations and experimental data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 13:07:56 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2015 15:44:48 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Cohen", "E. A. K.", ""], ["Kim", "D.", ""], ["Ober", "R. J.", ""]]}, {"id": "1504.05845", "submitter": "Qing Mai", "authors": "Qing Mai, Yi Yang and Hui Zou", "title": "Multiclass Sparse Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years many sparse linear discriminant analysis methods have been\nproposed for high-dimensional classification and variable selection. However,\nmost of these proposals focus on binary classification and they are not\ndirectly applicable to multiclass classification problems. There are two sparse\ndiscriminant analysis methods that can handle multiclass classification\nproblems, but their theoretical justifications remain unknown. In this paper,\nwe propose a new multiclass sparse discriminant analysis method that estimates\nall discriminant directions simultaneously. We show that when applied to the\nbinary case our proposal yields a classification direction that is equivalent\nto those by two successful binary sparse LDA methods in the literature. An\nefficient algorithm is developed for computing our method with high-dimensional\ndata. Variable selection consistency and rates of convergence are established\nunder the ultrahigh dimensionality setting. We further demonstrate the superior\nperformance of our proposal over the existing methods on simulated and real\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 15:13:54 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Mai", "Qing", ""], ["Yang", "Yi", ""], ["Zou", "Hui", ""]]}, {"id": "1504.05994", "submitter": "Simo S\\\"arkk\\\"a", "authors": "Simo S\\\"arkk\\\"a, Jouni Hartikainen, Lennart Svensson and Fredrik\n  Sandblom", "title": "On the relation between Gaussian process quadratures and sigma-point\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is concerned with Gaussian process quadratures, which are\nnumerical integration methods based on Gaussian process regression methods, and\nsigma-point methods, which are used in advanced non-linear Kalman filtering and\nsmoothing algorithms. We show that many sigma-point methods can be interpreted\nas Gaussian quadrature based methods with suitably selected covariance\nfunctions. We show that this interpretation also extends to more general\nmultivariate Gauss--Hermite integration methods and related spherical cubature\nrules. Additionally, we discuss different criteria for selecting the\nsigma-point locations: exactness for multivariate polynomials up to a given\norder, minimum average error, and quasi-random point sets. The performance of\nthe different methods is tested in numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 21:49:42 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["S\u00e4rkk\u00e4", "Simo", ""], ["Hartikainen", "Jouni", ""], ["Svensson", "Lennart", ""], ["Sandblom", "Fredrik", ""]]}, {"id": "1504.06074", "submitter": "Ran Shi", "authors": "Ran Shi and Jian Kang", "title": "Thresholded Multiscale Gaussian Processes with Application to Bayesian\n  Feature Selection for Massive Neuroimaging Data", "comments": "37 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the needs of selecting important features for massive\nneuroimaging data, we propose a spatially varying coefficient model (SVCMs)\nwith sparsity and piecewise smoothness imposed on the coefficient functions. A\nnew class of nonparametric priors is developed based on thresholded\nmultiresolution Gaussian processes (TMGP). We show that the TMGP has a large\nsupport on a space of sparse and piecewise smooth functions, leading to\nposterior consistency in coefficient function estimation and feature selection.\nAlso, we develop a method for prior specifications of thresholding parameters\nin TMGPs and discuss their theoretical properties. Efficient posterior\ncomputation algorithms are developed by adopting a kernel convolution approach,\nwhere a modified square exponential kernel is chosen taking the advantage that\nthe analytical form of the eigen decomposition is available. Based on\nsimulation studies, we demonstrate that our methods can achieve better\nperformance in estimating the spatially varying coefficient. Also, the proposed\nmodel has been applied to an analysis of resting state functional magnetic\nresonance imaging (Rs-fMRI) data from the Autism Brain Imaging Data Exchange\n(ABIDE) study, it provides biologically meaningful results.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 08:12:22 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 23:36:41 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Shi", "Ran", ""], ["Kang", "Jian", ""]]}, {"id": "1504.06173", "submitter": "Juho Kokkala", "authors": "Juho Kokkala, Arno Solin and Simo S\\\"arkk\\\"a", "title": "Sigma-Point Filtering and Smoothing Based Parameter Estimation in\n  Nonlinear Dynamic Systems", "comments": "Revised version. 14 pages, 11 figures. Submitted to Journal of\n  Advances in Information Fusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.DS math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider approximate maximum likelihood parameter estimation in nonlinear\nstate-space models. We discuss both direct optimization of the likelihood and\nexpectation--maximization (EM). For EM, we also give closed-form expressions\nfor the maximization step in a class of models that are linear in parameters\nand have additive noise. To obtain approximations to the filtering and\nsmoothing distributions needed in the likelihood-maximization methods, we focus\non using Gaussian filtering and smoothing algorithms that employ sigma-points\nto approximate the required integrals. We discuss different sigma-point schemes\nbased on the third, fifth, seventh, and ninth order unscented transforms and\nthe Gauss--Hermite quadrature rule. We compare the performance of the methods\nin two simulated experiments: a univariate nonlinear growth model as well as\ntracking of a maneuvering target. In the experiments, we also compare against\napproximate likelihood estimates obtained by particle filtering and extended\nKalman filtering based methods. The experiments suggest that the higher-order\nunscented transforms may in some cases provide more accurate estimates\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 13:26:39 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 18:28:08 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Kokkala", "Juho", ""], ["Solin", "Arno", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1504.06305", "submitter": "Ping Li", "authors": "Martin Slawski, Ping Li, Matthias Hein", "title": "Regularization-free estimation in trace regression with symmetric\n  positive semidefinite matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, trace regression models have received considerable\nattention in the context of matrix completion, quantum state tomography, and\ncompressed sensing. Estimation of the underlying matrix from\nregularization-based approaches promoting low-rankedness, notably nuclear norm\nregularization, have enjoyed great popularity. In the present paper, we argue\nthat such regularization may no longer be necessary if the underlying matrix is\nsymmetric positive semidefinite (\\textsf{spd}) and the design satisfies certain\nconditions. In this situation, simple least squares estimation subject to an\n\\textsf{spd} constraint may perform as well as regularization-based approaches\nwith a proper choice of the regularization parameter, which entails knowledge\nof the noise level and/or tuning. By contrast, constrained least squares\nestimation comes without any tuning parameter and may hence be preferred due to\nits simplicity.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 19:30:38 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Slawski", "Martin", ""], ["Li", "Ping", ""], ["Hein", "Matthias", ""]]}, {"id": "1504.06342", "submitter": "Santosh Nannuru", "authors": "Santosh Nannuru, Stephane Blouin, Mark Coates and Michael Rabbat", "title": "Multisensor CPHD filter", "comments": null, "journal-ref": "IEEE Transactions on Aerospace and Electronic Systems, Volume: 52,\n  Issue: 4, August 2016", "doi": "10.1109/TAES.2016.150265", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The single sensor probability hypothesis density (PHD) and cardinalized\nprobability hypothesis density (CPHD) filters have been developed in the\nliterature using the random finite set framework. The existing multisensor\nextensions of these filters have limitations such as sensor order dependence,\nnumerical instability or high computational requirements. In this paper we\nderive update equations for the multisensor CPHD filter. The multisensor PHD\nfilter is derived as a special case. Exact implementation of the multisensor\nCPHD involves sums over all partitions of the measurements from different\nsensors and is thus intractable. We propose a computationally tractable\napproximation which combines a greedy measurement partitioning algorithm with\nthe Gaussian mixture representation of the PHD. Our greedy approximation method\nallows the user to control the tradeoff between computational overhead and\napproximation accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 20:57:19 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 02:08:19 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Nannuru", "Santosh", ""], ["Blouin", "Stephane", ""], ["Coates", "Mark", ""], ["Rabbat", "Michael", ""]]}, {"id": "1504.06506", "submitter": "Susanne Strohmaier", "authors": "Susanne Strohmaier, Kjetil R{\\o}ysland, Rune Hoff, {\\O}rnulf Borgan,\n  Terje Pedersen, Odd O. Aalen", "title": "Dynamic path analysis - A useful tool to investigate mediation processes\n  in clinical survival trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When it comes to clinical survival trials, regulatory restrictions usually\nrequire the application of methods that solely utilize baseline covariates and\nthe intention-to-treat principle. Thereby a lot of potentially useful\ninformation is lost, as collection of time-to-event data often goes hand in\nhand with collection of information on biomarkers and other internal\ntime-dependent covariates. However, there are tools to incorporate information\nfrom repeated measurements in a useful manner that can help to shed more light\non the underlying treatment mechanisms. We consider dynamic path analysis, a\nmodel for mediation analysis in the presence of a time-to-event outcome and\ntime-dependent covariates to investigate direct and indirect effects in a study\nof different lipid lowering treatments in patients with previous myocardial\ninfarctions. Further, we address the question whether survival in itself may\nproduce associations between the treatment and the mediator in dynamic path\nanalysis and give an argument that, due to linearity of the assumed additive\nhazard model, this is not the case. We further elaborate on our view that, when\nstudying mediation, we are actually dealing with underlying processes rather\nthan single variables measured only once during the study period. This becomes\napparent in results from various models applied to the study of lipid lowering\ntreatments as well as our additionally conducted simulation study, where we\nclearly observe, that discarding information on repeated measurements can lead\nto potentially erroneous conclusions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 13:45:16 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Strohmaier", "Susanne", ""], ["R\u00f8ysland", "Kjetil", ""], ["Hoff", "Rune", ""], ["Borgan", "\u00d8rnulf", ""], ["Pedersen", "Terje", ""], ["Aalen", "Odd O.", ""]]}, {"id": "1504.06637", "submitter": "Yue Hu", "authors": "Yue Hu, Eric Chi, Genevera I. Allen", "title": "ADMM Algorithmic Regularization Paths for Sparse Statistical Machine\n  Learning", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization approaches based on operator splitting are becoming popular for\nsolving sparsity regularized statistical machine learning models. While many\nhave proposed fast algorithms to solve these problems for a single\nregularization parameter, conspicuously less attention has been given to\ncomputing regularization paths, or solving the optimization problems over the\nfull range of regularization parameters to obtain a sequence of sparse models.\nIn this chapter, we aim to quickly approximate the sequence of sparse models\nassociated with regularization paths for the purposes of statistical model\nselection by using the building blocks from a classical operator splitting\nmethod, the Alternating Direction Method of Multipliers (ADMM). We begin by\nproposing an ADMM algorithm that uses warm-starts to quickly compute the\nregularization path. Then, by employing approximations along this warm-starting\nADMM algorithm, we propose a novel concept that we term the ADMM Algorithmic\nRegularization Path. Our method can quickly outline the sequence of sparse\nmodels associated with the regularization path in computational time that is\noften less than that of using the ADMM algorithm to solve the problem at a\nsingle regularization parameter. We demonstrate the applicability and\nsubstantial computational savings of our approach through three popular\nexamples, sparse linear regression, reduced-rank multi-task learning, and\nconvex clustering.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 20:48:51 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Hu", "Yue", ""], ["Chi", "Eric", ""], ["Allen", "Genevera I.", ""]]}, {"id": "1504.06638", "submitter": "Fl\\'avio Gon\\c{c}alves", "authors": "Fl\\'avio B. Gon\\c{c}alves and Dani Gamerman", "title": "Exact Bayesian inference in spatio-temporal Cox processes driven by\n  multivariate Gaussian processes", "comments": null, "journal-ref": "Journal of the Royal Statistical Society Series B, 80, 157-175,\n  2018", "doi": "10.1111/rssb.12237", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel inference methodology to perform Bayesian\ninference for spatiotemporal Cox processes where the intensity function depends\non a multivariate Gaussian process. Dynamic Gaussian processes are introduced\nto allow for evolution of the intensity function over discrete time. The\nnovelty of the method lies on the fact that no discretisation error is involved\ndespite the non-tractability of the likelihood function and infinite\ndimensionality of the problem. The method is based on a Markov chain Monte\nCarlo algorithm that samples from the joint posterior distribution of the\nparameters and latent variables of the model. The models are defined in a\ngeneral and flexible way but they are amenable to direct sampling from the\nrelevant distributions, due to careful characterisation of its components. The\nmodels also allow for the inclusion of regression covariates and/or temporal\ncomponents to explain the variability of the intensity function. These\ncomponents may be subject to relevant interaction with space and/or time. Real\nand simulated examples illustrate the methodology, followed by concluding\nremarks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 20:48:54 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 12:29:50 GMT"}, {"version": "v3", "created": "Sun, 10 Mar 2019 23:13:42 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Gon\u00e7alves", "Fl\u00e1vio B.", ""], ["Gamerman", "Dani", ""]]}, {"id": "1504.06653", "submitter": "Paul Northrop", "authors": "Paul Northrop (1), Nicolas Attalides (1), Philip Jonathan (2) ((1)\n  University College London, UK (2) Shell Projects and Technology, Manchester,\n  UK)", "title": "Cross-validatory extreme value threshold selection and uncertainty with\n  application to ocean storm severity", "comments": "24 pages, 15 figures. Confidence intervals in Figure 2 corrected. The\n  final publication is available at Wiley via\n  http://dx.doi.org/10.1111/rssc.12159", "journal-ref": null, "doi": "10.1111/rssc.12159", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designs conditions for marine structures are typically informed by\nthreshold-based extreme value analyses of oceanographic variables, in which\nexcesses of a high threshold are modelled by a generalized Pareto (GP)\ndistribution. Too low a threshold leads to bias from model mis-specification;\nraising the threshold increases the variance of estimators: a bias-variance\ntrade-off. Many existing threshold selection methods do not address this\ntrade-off directly, but rather aim to select the lowest threshold above which\nthe GP model is judged to hold approximately. In this paper Bayesian\ncross-validation is used to address the trade-off by comparing thresholds based\non predictive ability at extreme levels. Extremal inferences can be sensitive\nto the choice of a single threshold. We use Bayesian model-averaging to combine\ninferences from many thresholds, thereby reducing sensitivity to the choice of\na single threshold. The methodology is applied to significant wave height\ndatasets from the northern North Sea and the Gulf of Mexico.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 22:10:06 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 20:36:31 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 09:39:56 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Northrop", "Paul", ""], ["Attalides", "Nicolas", ""], ["Jonathan", "Philip", ""]]}, {"id": "1504.06880", "submitter": "Minh Au", "authors": "Minh Au, Basile L. Agba and Fran\\c{c}ois Gagnon", "title": "An analysis of transient impulsive noise in a Poisson field of\n  interferers for wireless channel in substation environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In substations, the presence of random transient impulsive interference\nsources makes noise highly non-Gaussian. In this paper, the primary interest is\nto provide a general model for wireless channel in presence of these transient\nimpulsive noise for space-time signal processing problems. We assume a\nsuperposition of independent interference sources randomly distributed in\nspace-time in a Poisson field of interferers. By using stochastic geometry\napproach, first order and second order statistics can be derived from basic\nwaveforms of impulsive interferers. We use discrete-time series model to\nsimulate the random transient impulsive waveforms. It is demonstrated that the\namplitude distribution and density of the proposed model converges to\n$\\alpha$-stable distributions and their power spectral densities are $\\sim\n1/(f-f_{0})^{k}$ where $f_{0}\\geq 0$ is a resonant frequency and $k > 0$.\nMeasurements and computer simulations are provided where impulsive noise are to\ndemonstrate the efficiency of the analysis.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 20:54:36 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Au", "Minh", ""], ["Agba", "Basile L.", ""], ["Gagnon", "Fran\u00e7ois", ""]]}, {"id": "1504.06964", "submitter": "Fulton Wang", "authors": "Fulton Wang and Tyler H. McCormick and Cynthia Rudin and John Gore", "title": "Modeling Recovery Curves With Application to Prostatectomy", "comments": "Accepted to Biostatistics, 2018. Includes supplementary material and\n  high resolution images of predictions for patients", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian model that predicts recovery curves based on\ninformation available before the disruptive event. A recovery curve of interest\nis the quantified sexual function of prostate cancer patients after\nprostatectomy surgery. We illustrate the utility of our model as a\npre-treatment medical decision aid, producing personalized predictions that are\nboth interpretable and accurate. We uncover covariate relationships that agree\nwith and supplement that in existing medical literature.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 08:14:33 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 16:52:41 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2017 04:45:09 GMT"}, {"version": "v4", "created": "Thu, 1 Jun 2017 15:54:40 GMT"}, {"version": "v5", "created": "Tue, 27 Feb 2018 14:18:27 GMT"}, {"version": "v6", "created": "Mon, 5 Mar 2018 03:42:28 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Wang", "Fulton", ""], ["McCormick", "Tyler H.", ""], ["Rudin", "Cynthia", ""], ["Gore", "John", ""]]}, {"id": "1504.07005", "submitter": "Arthur Tenenhaus", "authors": "Michel Tenenhaus and Arthur Tenenhaus and Patrick J. F. Groenen", "title": "Regularized Consensus PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new framework for many multiblock component methods (including consensus\nand hierarchical PCA) is proposed. It is based on the consensus PCA model: a\nscheme connecting each block of variables to a superblock obtained by\nconcatenation of all blocks. Regularized consensus PCA is obtained by applying\nregularized generalized canonical correlation analysis to this scheme for the\nfunction $g(x) = x^m$ where $m \\ge 1$. A gradient algorithm is proposed. At\nconvergence, a solution of the stationary equation related to the optimization\nproblem is obtained. For m = 1, 2 or 4 and shrinkage constants equal to 0 or 1,\nmany multiblock component methods are recovered.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 09:55:05 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Tenenhaus", "Michel", ""], ["Tenenhaus", "Arthur", ""], ["Groenen", "Patrick J. F.", ""]]}, {"id": "1504.07336", "submitter": "Mohammad Jafari Jozani", "authors": "Armin Hatefi and Mohammad Jafari Jozani", "title": "Information content of partially rank-ordered set samples", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially rank-ordered set (PROS) sampling is a generalization of ranked set\nsampling in which rankers are not required to fully rank the sampling units in\neach set, hence having more flexibility to perform the necessary judgemental\nranking process. The PROS sampling has a wide range of applications in\ndifferent fields ranging from environmental and ecological studies to medical\nresearch and it has been shown to be superior over ranked set sampling and\nsimple random sampling for estimating the population mean. In this paper, we\nstudy the Fisher information content and uncertainty structure of the PROS\nsamples and compare them with those of simple random sample (SRS) and ranked\nset sample (RSS) counterparts of the same size from the underlying population.\nWe study the uncertainty structure in terms of the Shannon entropy, Renyi\nentropy and Kullback-Leibler (KL) discrimination measures. Several examples\nincluding the FI of PROS samples from the location-scale family of\ndistributions as well as a regression model are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 03:09:52 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Hatefi", "Armin", ""], ["Jozani", "Mohammad Jafari", ""]]}, {"id": "1504.07536", "submitter": "Sergei Rodionov", "authors": "Sergei Rodionov", "title": "A sequential method of detecting abrupt changes in the correlation\n  coefficient and its application to Bering Sea climate", "comments": "18 pages, 11 figures", "journal-ref": "Climate 2015, 3(3), 474-491", "doi": "10.3390/cli3030474", "report-no": null, "categories": "stat.ME physics.ao-ph physics.ins-det stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new method of regime shift detection in the correlation coefficient is\nproposed. The method is designed to find multiple change-points with unknown\nlocations in time series. It signals a possible regime shift in real time and\nallows for its monitoring. The method is tested on randomly generated time\nseries with predefined change-points. It is applied to examine structural\nchanges in the Bering Sea climate. A major shift is found in 1967, which\ncoincides with a transition from a zonal type of atmospheric circulation to a\nmeridional one. The roles of the Siberian and Alaskan centers of action on\nwinter temperatures in the eastern Bering Sea have been investigated.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 19:13:51 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 16:32:07 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Rodionov", "Sergei", ""]]}, {"id": "1504.07676", "submitter": "Matt Olson", "authors": "Abraham J. Wyner, Matthew Olson, Justin Bleich, David Mease", "title": "Explaining the Success of AdaBoost and Random Forests as Interpolating\n  Classifiers", "comments": "40 pages, 11 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large literature explaining why AdaBoost is a successful\nclassifier. The literature on AdaBoost focuses on classifier margins and\nboosting's interpretation as the optimization of an exponential likelihood\nfunction. These existing explanations, however, have been pointed out to be\nincomplete. A random forest is another popular ensemble method for which there\nis substantially less explanation in the literature. We introduce a novel\nperspective on AdaBoost and random forests that proposes that the two\nalgorithms work for similar reasons. While both classifiers achieve similar\npredictive accuracy, random forests cannot be conceived as a direct\noptimization procedure. Rather, random forests is a self-averaging,\ninterpolating algorithm which creates what we denote as a \"spikey-smooth\"\nclassifier, and we view AdaBoost in the same light. We conjecture that both\nAdaBoost and random forests succeed because of this mechanism. We provide a\nnumber of examples and some theoretical justification to support this\nexplanation. In the process, we question the conventional wisdom that suggests\nthat boosting algorithms for classification require regularization or early\nstopping and should be limited to low complexity classes of learners, such as\ndecision stumps. We conclude that boosting should be used like random forests:\nwith large decision trees and without direct regularization or early stopping.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 22:34:25 GMT"}, {"version": "v2", "created": "Sat, 29 Apr 2017 23:25:20 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Wyner", "Abraham J.", ""], ["Olson", "Matthew", ""], ["Bleich", "Justin", ""], ["Mease", "David", ""]]}, {"id": "1504.07715", "submitter": "Yichi Zhang", "authors": "Yichi Zhang, Eric B. Laber, Anastasios Tsiatis, Marie Davidian", "title": "Using Decision Lists to Construct Interpretable and Parsimonious\n  Treatment Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A treatment regime formalizes personalized medicine as a function from\nindividual patient characteristics to a recommended treatment. A high-quality\ntreatment regime can improve patient outcomes while reducing cost, resource\nconsumption, and treatment burden. Thus, there is tremendous interest in\nestimating treatment regimes from observational and randomized studies.\nHowever, the development of treatment regimes for application in clinical\npractice requires the long-term, joint effort of statisticians and clinical\nscientists. In this collaborative process, the statistician must integrate\nclinical science into the statistical models underlying a treatment regime and\nthe clinician must scrutinize the estimated treatment regime for scientific\nvalidity. To facilitate meaningful information exchange, it is important that\nestimated treatment regimes be interpretable in a subject-matter context. We\npropose a simple, yet flexible class of treatment regimes whose members are\nrepresentable as a short list of if-then statements. Regimes in this class are\nimmediately interpretable and are therefore an appealing choice for broad\napplication in practice. We derive a robust estimator of the optimal regime\nwithin this class and demonstrate its finite sample performance using\nsimulation experiments. The proposed method is illustrated with data from two\nclinical trials.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 03:54:30 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Zhang", "Yichi", ""], ["Laber", "Eric B.", ""], ["Tsiatis", "Anastasios", ""], ["Davidian", "Marie", ""]]}, {"id": "1504.07882", "submitter": "Simon E. F. Spencer", "authors": "Simon E. F. Spencer, Steven M. Hill, Sach Mukherjee", "title": "Inferring network structure from interventional time-course experiments", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS806 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 507-524", "doi": "10.1214/15-AOAS806", "report-no": "IMS-AOAS-AOAS806", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are widely used to study biological networks. Interventions\non network nodes are an important feature of many experimental designs for the\nstudy of biological networks. In this paper we put forward a causal variant of\ndynamic Bayesian networks (DBNs) for the purpose of modeling time-course data\nwith interventions. The models inherit the simplicity and computational\nefficiency of DBNs but allow interventional data to be integrated into network\ninference. We show empirical results, on both simulated and experimental data,\nthat demonstrate the need to appropriately handle interventions when\ninterventions form part of the design.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 15:07:35 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 05:10:05 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Spencer", "Simon E. F.", ""], ["Hill", "Steven M.", ""], ["Mukherjee", "Sach", ""]]}, {"id": "1504.08031", "submitter": "Xiaoying Tian Harris", "authors": "Xiaoying Tian, and Joshua R. Loftus, and Jonathan E. Taylor", "title": "Selective inference with unknown variance via the square-root LASSO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been much recent work on inference after model selection when the\nnoise level is known, however, $\\sigma$ is rarely known in practice and its\nestimation is difficult in high-dimensional settings. In this work we propose\nusing the square-root LASSO (also known as the scaled LASSO) to perform\nselective inference for the coefficients and the noise level simultaneously.\nThe square-root LASSO has the property that choosing a reasonable tuning\nparameter is scale-free, namely it does not depend on the noise level in the\ndata. We provide valid p-values and confidence intervals for the coefficients\nafter selection, and estimates for model specific variance. Our estimates\nperform better than other estimates of $\\sigma^2$ in simulation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 21:38:34 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 22:41:48 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Tian", "Xiaoying", ""], ["Loftus", "Joshua R.", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "1504.08133", "submitter": "Christopher Yau", "authors": "Michalis K. Titsias and Christopher Yau", "title": "The Hamming Ball Sampler", "comments": "16 pages, 4 figures. No supplementary information included. Corrected\n  Figure 4 and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Hamming Ball Sampler, a novel Markov Chain Monte Carlo\nalgorithm, for efficient inference in statistical models involving\nhigh-dimensional discrete state spaces. The sampling scheme uses an auxiliary\nvariable construction that adaptively truncates the model space allowing\niterative exploration of the full model space in polynomial time. The approach\ngeneralizes conventional Gibbs sampling schemes for discrete spaces and can be\nconsidered as a Big Data-enabled MCMC algorithm that provides an intuitive\nmeans for user-controlled balance between statistical efficiency and\ncomputational tractability. We illustrate the generic utility of our sampling\nalgorithm through application to a range of statistical models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 09:21:36 GMT"}, {"version": "v2", "created": "Sun, 3 May 2015 17:37:50 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Titsias", "Michalis K.", ""], ["Yau", "Christopher", ""]]}, {"id": "1504.08164", "submitter": "Jian Shi", "authors": "Peirong Xu, Youngjo Lee, Jian Qing Shi", "title": "Automatic Detection of Significant Areas for Functional Data with\n  Directional Error Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To detect differences between the mean curves of two samples in longitudinal\nstudy or functional data analysis, we usually need to partition the temporal or\nspatial domain into several pre-determined sub-areas. In this paper we apply\nthe idea of large-scale multiple testing to find the significant sub-areas\nautomatically in a general functional data analysis framework. A nonparametric\nGaussian process regression model is introduced for two-sided multiple tests.\nWe derive an optimal test which controls directional false discovery rates and\npropose a procedure by approximating it on a continuum. The proposed procedure\ncontrols directional false discovery rates at any specified level\nasymptotically. In addition, it is computationally inexpensive and able to\naccommodate different time points for observations across the samples.\nSimulation studies are presented to demonstrate its finite sample performance.\nWe also apply it to an executive function research in children with Hemiplegic\nCerebral Palsy and extend it to the equivalence tests.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 10:45:17 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Xu", "Peirong", ""], ["Lee", "Youngjo", ""], ["Shi", "Jian Qing", ""]]}, {"id": "1504.08319", "submitter": "Changshuai Wei", "authors": "Changshuai Wei, Robert C. Elston and Qing Lu", "title": "A weighted U statistic for association analysis considering genetic\n  heterogeneity", "comments": null, "journal-ref": null, "doi": "10.1002/sim.6877", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Converging evidence suggests that common complex diseases with the same or\nsimilar clinical manifestations could have different underlying genetic\netiologies. While current research interests have shifted toward uncovering\nrare variants and structural variations predisposing to human diseases, the\nimpact of heterogeneity in genetic studies of complex diseases has been largely\noverlooked. Most of the existing statistical methods assume the disease under\ninvestigation has a homogeneous genetic effect and could, therefore, have low\npower if the disease undergoes heterogeneous pathophysiological and etiological\nprocesses. In this paper, we propose a heterogeneity weighted U (HWU) method\nfor association analyses considering genetic heterogeneity. HWU can be applied\nto various types of phenotypes (e.g., binary and continuous) and is\ncomputationally effcient for high- dimensional genetic data. Through\nsimulations, we showed the advantage of HWU when the underlying genetic\netiology of a disease was heterogeneous, as well as the robustness of HWU\nagainst different model assumptions (e.g., phenotype distributions). Using HWU,\nwe conducted a genome-wide analysis of nicotine dependence from the Study of\nAddiction: Genetics and Environments (SAGE) dataset. The genome-wide analysis\nof nearly one million genetic markers took 7 hours, identifying heterogeneous\neffects of two new genes (i.e., CYP3A5 and IKBKB) on nicotine dependence.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 17:54:31 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 20:24:44 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Wei", "Changshuai", ""], ["Elston", "Robert C.", ""], ["Lu", "Qing", ""]]}, {"id": "1504.08327", "submitter": "Michele Nguyen", "authors": "Michele Nguyen and Almut E. D. Veraart", "title": "Spatio-temporal Ornstein-Uhlenbeck processes: theory, simulation and\n  statistical inference", "comments": "Supplementary material included", "journal-ref": null, "doi": "10.1111/sjos.12241", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal modelling is an increasingly popular topic in Statistics. Our\npaper contributes to this line of research by developing the theory, simulation\nand inference for a spatio-temporal Ornstein-Uhlenbeck process. We conduct\ndetailed simulation studies and demonstrate the practical relevance of these\nprocesses in an empirical study of radiation anomaly data. Finally, we describe\nhow predictions can be carried out in the Gaussian setting.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 18:19:51 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2015 12:30:13 GMT"}, {"version": "v3", "created": "Thu, 3 Mar 2016 12:41:54 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Nguyen", "Michele", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "1504.08349", "submitter": "Forrest Crawford", "authors": "Forrest W. Crawford and Jiacheng Wu and Robert Heimer", "title": "Hidden population size estimation from respondent-driven sampling: a\n  network approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the size of stigmatized, hidden, or hard-to-reach populations is a\nmajor problem in epidemiology, demography, and public health research.\nCapture-recapture and multiplier methods have become standard tools for\ninference of hidden population sizes, but they require independent random\nsampling of target population members, which is rarely possible.\nRespondent-driven sampling (RDS) is a survey method for hidden populations that\nrelies on social link tracing. The RDS recruitment process is designed to\nspread through the social network connecting members of the target population.\nIn this paper, we show how to use network data revealed by RDS to estimate\nhidden population size. The key insight is that the recruitment chain, timing\nof recruitments, and network degrees of recruited subjects provide information\nabout the number of individuals belonging to the target population who are not\nyet in the sample. We use a computationally efficient Bayesian method to\nintegrate over the missing edges in the subgraph of recruited individuals. We\nvalidate the method using simulated data and apply the technique to estimate\nthe number of people who inject drugs in St. Petersburg, Russia.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 19:17:38 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Crawford", "Forrest W.", ""], ["Wu", "Jiacheng", ""], ["Heimer", "Robert", ""]]}]