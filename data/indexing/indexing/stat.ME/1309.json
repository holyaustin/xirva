[{"id": "1309.0220", "submitter": "Zhanfeng Wang", "authors": "Kani Chen, Yuanyuan Lin, Zhanfeng Wang, Zhiliang Ying", "title": "Least Product Relative Error Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A least product relative error criterion is proposed for multiplicative\nregression models. It is invariant under scale transformation of the outcome\nand covariates. In addition, the objective function is smooth and convex,\nresulting in a simple and uniquely defined estimator of the regression\nparameter. It is shown that the estimator is asymptotically normal and that the\nsimple plugging-in variance estimation is valid. Simulation results confirm\nthat the proposed method performs well. An application to body fat calculation\nis presented to illustrate the new method.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2013 13:44:30 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Chen", "Kani", ""], ["Lin", "Yuanyuan", ""], ["Wang", "Zhanfeng", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1309.0423", "submitter": "Roland Langrock", "authors": "Roland Langrock, Thomas Kneib, Alexander Sohn, Stacy DeRuiter", "title": "Nonparametric inference in hidden Markov models using P-splines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models (HMMs) are flexible time series models in which the\ndistributions of the observations depend on unobserved serially correlated\nstates. The state-dependent distributions in HMMs are usually taken from some\nclass of parametrically specified distributions. The choice of this class can\nbe difficult, and an unfortunate choice can have serious consequences for\nexample on state estimates, on forecasts and generally on the resulting model\ncomplexity and interpretation, in particular with respect to the number of\nstates. We develop a novel approach for estimating the state-dependent\ndistributions of an HMM in a nonparametric way, which is based on the idea of\nrepresenting the corresponding densities as linear combinations of a large\nnumber of standardized B-spline basis functions, imposing a penalty term on\nnon-smoothness in order to maintain a good balance between goodness-of-fit and\nsmoothness. We illustrate the nonparametric modeling approach in a real data\napplication concerned with vertical speeds of a diving beaked whale,\ndemonstrating that compared to parametric counterparts it can lead to models\nthat are more parsimonious in terms of the number of states yet fit the data\nequally well.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2013 14:32:31 GMT"}, {"version": "v2", "created": "Tue, 17 Jun 2014 22:21:09 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Langrock", "Roland", ""], ["Kneib", "Thomas", ""], ["Sohn", "Alexander", ""], ["DeRuiter", "Stacy", ""]]}, {"id": "1309.0579", "submitter": "Galit Shmueli", "authors": "Pragya Sur, Galit Shmueli, Smarajit Bose, Paromita Dubey", "title": "Modeling Bimodal Discrete Data Using Conway-Maxwell-Poisson Mixture\n  Models", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bimodal truncated count distributions are frequently observed in aggregate\nsurvey data and in user ratings when respondents are mixed in their opinion.\nThey also arise in censored count data, where the highest category might create\nan additional mode. Modeling bimodal behavior in discrete data is useful for\nvarious purposes, from comparing shapes of different samples (or survey\nquestions) to predicting future ratings by new raters. The Poisson distribution\nis the most common distribution for fitting count data and can be modified to\nachieve mixtures of truncated Poisson distributions. However, it is suitable\nonly for modeling equi-dispersed distributions and is limited in its ability to\ncapture bimodality. The Conway-Maxwell-Poisson (CMP) distribution is a\ntwo-parameter generalization of the Poisson distribution that allows for over-\nand under-dispersion. In this work, we propose a mixture of CMPs for capturing\na wide range of truncated discrete data, which can exhibit unimodal and bimodal\nbehavior. We present methods for estimating the parameters of a mixture of two\nCMP distributions using an EM approach. Our approach introduces a special\ntwo-step optimization within the M step to estimate multiple parameters. We\nexamine computational and theoretical issues. The methods are illustrated for\nmodeling ordered rating data as well as truncated count data, using simulated\nand real examples.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 03:23:48 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2014 12:21:33 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Sur", "Pragya", ""], ["Shmueli", "Galit", ""], ["Bose", "Smarajit", ""], ["Dubey", "Paromita", ""]]}, {"id": "1309.0602", "submitter": "Aki-Hiro Sato", "authors": "Aki-Hiro Sato and Hideki Takayasu", "title": "Segmentation procedure based on Fisher's exact test and its application\n  to foreign exchange rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes the segmentation procedure of univariate time series\nbased on Fisher's exact test. We show that an adequate change point can be\ndetected as the minimum value of p-value. It is shown that the proposed\nprocedure can detect change points for an artificial time series. We apply the\nproposed method to find segments of the foreign exchange rates recursively. It\nis also applied to randomly shuffled time series. It concludes that the\nrandomly shuffled data can be used as a level to determine the null hypothesis.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 08:03:52 GMT"}], "update_date": "2013-09-04", "authors_parsed": [["Sato", "Aki-Hiro", ""], ["Takayasu", "Hideki", ""]]}, {"id": "1309.0609", "submitter": "{\\L}ukasz Kwiatkowski", "authors": "{\\L}ukasz Kwiatkowski", "title": "Coherent prior distributions in univariate finite mixture and\n  Markov-switching models", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture and Markov-switching models generalize and, therefore, nest\nspecifications featuring only one component. While specifying priors in the\ntwo: the general (mixture) model and its special (single-component) case, it\nmay be desirable to ensure that the prior assumptions introduced into both\nstructures are coherent in the sense that the prior distribution in the nested\nmodel amounts to the conditional prior in the mixture model under relevant\nparametric restriction. The study provides the rudiments of setting coherent\npriors in Bayesian univariate finite mixture and Markov-switching models. Once\nsome primary results are delivered, we derive specific conditions for coherence\nin the case of three types of continuous priors commonly engaged in Bayesian\nmodeling: the normal, inverse gamma, and gamma distributions. Further, we study\nthe consequences of introducing additional constraints into the mixture model's\nprior (such as the ones enforcing identifiability or some sort of regularity,\ne.g. second-order stationarity) on the coherence conditions. Finally, the\nmethodology is illustrated through a discussion of setting coherent priors for\na class of Markov-switching AR(2) models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 08:23:35 GMT"}], "update_date": "2013-09-04", "authors_parsed": [["Kwiatkowski", "\u0141ukasz", ""]]}, {"id": "1309.0837", "submitter": "Xiaoquan Wen", "authors": "Xiaoquan Wen", "title": "Bayesian Model Selection in Complex Linear Systems, as Illustrated in\n  Genetic Association Studies", "comments": null, "journal-ref": "Biometrics 2014 Mar; 70(1): 73-83", "doi": "10.1111/biom.12112", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by examples from genetic association studies, this paper considers\nthe model selection problem in a general complex linear model system and in a\nBayesian framework. We discuss formulating model selection problems and\nincorporating context-dependent {\\it a priori} information through different\nlevels of prior specifications. We also derive analytic Bayes factors and their\napproximations to facilitate model selection and discuss their theoretical and\ncomputational properties. We demonstrate our Bayesian approach based on an\nimplemented Markov Chain Monte Carlo (MCMC) algorithm in simulations and a real\ndata application of mapping tissue-specific eQTLs. Our novel results on Bayes\nfactors provide a general framework to perform efficient model comparisons in\ncomplex linear model systems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 20:35:07 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Wen", "Xiaoquan", ""]]}, {"id": "1309.0911", "submitter": "Mathias Drton", "authors": "Mathias Drton, Martyn Plummer", "title": "A Bayesian information criterion for singular models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider approximate Bayesian model choice for model selection problems\nthat involve models whose Fisher-information matrices may fail to be invertible\nalong other competing submodels. Such singular models do not obey the\nregularity conditions underlying the derivation of Schwarz's Bayesian\ninformation criterion (BIC) and the penalty structure in BIC generally does not\nreflect the frequentist large-sample behavior of their marginal likelihood.\nWhile large-sample theory for the marginal likelihood of singular models has\nbeen developed recently, the resulting approximations depend on the true\nparameter value and lead to a paradox of circular reasoning. Guided by examples\nsuch as determining the number of components of mixture models, the number of\nfactors in latent factor models or the rank in reduced-rank regression, we\npropose a resolution to this paradox and give a practical extension of BIC for\nsingular model selection problems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 04:32:04 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 18:30:15 GMT"}, {"version": "v3", "created": "Wed, 23 Mar 2016 13:37:43 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Drton", "Mathias", ""], ["Plummer", "Martyn", ""]]}, {"id": "1309.1287", "submitter": "Guenter Zech", "authors": "G. Bohm and G. Zech", "title": "Statistics of weighted Poisson events and its applications", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": "10.1016/j.nima.2014.02.021", "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistics of the sum of random weights where the number of weights is\nPoisson distributed has important applications in nuclear physics, particle\nphysics and astrophysics. Events are frequently weighted according to their\nacceptance or relevance to a certain type of reaction. The sum is described by\nthe compound Poisson distribution (CPD) which is shortly reviewed. It is shown\nthat the CPD can be approximated by a scaled Poisson distribution (SPD). The\nSPD is applied to parameter estimation in situations where the data are\ndistorted by resolution effects. It performs considerably better than the\nnormal approximation that is usually used. A special Poisson bootstrap\ntechnique is presented which permits to derive confidence limits for\nobservations following the CPD.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 09:18:30 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Bohm", "G.", ""], ["Zech", "G.", ""]]}, {"id": "1309.1799", "submitter": "Yajuan Si", "authors": "Yajuan Si, Natesh S. Pillai, Andrew Gelman", "title": "Bayesian Nonparametric Weighted Sampling Inference", "comments": "Published at http://dx.doi.org/10.1214/14-BA924 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 3, 605-625", "doi": "10.1214/14-BA924", "report-no": "VTeX-BA-BA924", "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has historically been a challenge to perform Bayesian inference in a\ndesign-based survey context. The present paper develops a Bayesian model for\nsampling inference in the presence of inverse-probability weights. We use a\nhierarchical approach in which we model the distribution of the weights of the\nnonsampled units in the population and simultaneously include them as\npredictors in a nonparametric Gaussian process regression. We use simulation\nstudies to evaluate the performance of our procedure and compare it to the\nclassical design-based estimator. We apply our method to the Fragile Family and\nChild Wellbeing Study. Our studies find the Bayesian nonparametric finite\npopulation estimator to be more robust than the classical design-based\nestimator without loss in efficiency, which works because we induce\nregularization for small cells and thus this is a way of automatically\nsmoothing the highly variable weights.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 01:14:31 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 21:52:49 GMT"}, {"version": "v3", "created": "Mon, 13 Oct 2014 21:50:16 GMT"}, {"version": "v4", "created": "Fri, 4 Sep 2015 05:49:15 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Si", "Yajuan", ""], ["Pillai", "Natesh S.", ""], ["Gelman", "Andrew", ""]]}, {"id": "1309.1901", "submitter": "Paul McNicholas", "authors": "Sanjeena Subedi and Paul D. McNicholas", "title": "Variational Bayes Approximations for Clustering via Mixtures of Normal\n  Inverse Gaussian Distributions", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-014-0165-7", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation for model-based clustering using a finite mixture of\nnormal inverse Gaussian (NIG) distributions is achieved through variational\nBayes approximations. Univariate NIG mixtures and multivariate NIG mixtures are\nconsidered. The use of variational Bayes approximations here is a substantial\ndeparture from the traditional EM approach and alleviates some of the\nassociated computational complexities and uncertainties. Our variational\nalgorithm is applied to simulated and real data. The paper concludes with\ndiscussion and suggestions for future work.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 20:29:26 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Subedi", "Sanjeena", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1309.1915", "submitter": "David Tyler", "authors": "Andrew F. Magyar and David E. Tyler", "title": "The asymptotic inadmissibility of the spatial sign covariance matrix for\n  elliptically symmetric distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic efficiency of the spatial sign covariance matrix (SSCM)\nrelative to affine equivariant estimates of scatter is studied in detail. In\nparticular, the SSCM is shown to be asymptoticaly inadmissible, i.e. the\nasymptotic variance-covariance matrix of the consistency corrected SSCM is\nuniformly smaller than that of its affine equivariant counterpart, namely\nTyler's scatter matrix. Although the SSCM has often been recommended when one\nis interested in principal components analysis, the degree of the inefficiency\nof the SSCM is shown to be most severe in situations where principal components\nare of most interest. A finite sample simulation shows the inefficiency of the\nSSCM also holds for small sample sizes, and that the asymptotic relative\nefficiency is a good approximation to the finite sample efficiency for\nrelatively modest sample sizes.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 22:44:54 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Magyar", "Andrew F.", ""], ["Tyler", "David E.", ""]]}, {"id": "1309.2068", "submitter": "Yi Yu", "authors": "Yi Yu and Yang Feng", "title": "Modified Cross-Validation for Penalized High-Dimensional Linear\n  Regression Models", "comments": "23 pages, 3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, for Lasso penalized linear regression models in\nhigh-dimensional settings, we propose a modified cross-validation method for\nselecting the penalty parameter. The methodology is extended to other\npenalties, such as Elastic Net. We conduct extensive simulation studies and\nreal data analysis to compare the performance of the modified cross-validation\nmethod with other methods. It is shown that the popular $K$-fold\ncross-validation method includes many noise variables in the selected model,\nwhile the modified cross-validation works well in a wide range of coefficient\nand correlation settings. Supplemental materials containing the computer code\nare available online.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 08:44:47 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Yu", "Yi", ""], ["Feng", "Yang", ""]]}, {"id": "1309.2071", "submitter": "Mark Podolskij", "authors": "Mark Podolskij and Nakahiro Yoshida", "title": "Edgeworth expansion for functionals of continuous diffusion processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new results on the Edgeworth expansion for high frequency\nfunctionals of continuous diffusion processes. We derive asymptotic expansions\nfor weighted functionals of the Brownian motion and apply them to provide the\nsecond order Edgeworth expansion for power variation of diffusion processes.\nOur methodology relies on martingale embedding, Malliavin calculus and stable\ncentral limit theorems for semimartingales. Finally, we demonstrate the density\nexpansion for studentized statistics of power variations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 08:54:22 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Podolskij", "Mark", ""], ["Yoshida", "Nakahiro", ""]]}, {"id": "1309.2324", "submitter": "Daniel Manrique-Vallier", "authors": "Daniel Manrique-Vallier", "title": "Longitudinal Mixed Membership trajectory models for disability survey\n  data", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS769 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2268-2291", "doi": "10.1214/14-AOAS769", "report-no": "IMS-AOAS-AOAS769", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods for analyzing discrete multivariate longitudinal data and\napply them to functional disability data on the U.S. elderly population from\nthe National Long Term Care Survey (NLTCS), 1982-2004. Our models build on a\nMixed Membership framework, in which individuals are allowed multiple\nmembership on a set of extreme profiles characterized by time-dependent\ntrajectories of progression into disability. We also develop an extension that\nallows us to incorporate birth-cohort effects, in order to assess\ninter-generational changes. Applying these methods, we find that most\nindividuals follow trajectories that imply a late onset of disability, and that\nyounger cohorts tend to develop disabilities at a later stage in life compared\nto their elders.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 21:10:01 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 15:11:16 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2015 10:18:07 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Manrique-Vallier", "Daniel", ""]]}, {"id": "1309.2435", "submitter": "Kara Stevens Dr", "authors": "Guy P. Nason and Kara N. Stevens", "title": "Bayesian Wavelet Shrinkage of the Haar-Fisz Transformed Wavelet\n  Periodogram", "comments": "18 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is increasingly being realised that many real world time series are not\nstationary and exhibit evolving second-order autocovariance or spectral\nstructure. This article introduces a Bayesian approach for modelling the\nevolving wavelet spectrum of a locally stationary wavelet time series. Our new\nmethod works by combining the advantages of a Haar-Fisz transformed spectrum\nwith a simple, but powerful, Bayesian wavelet shrinkage method. Our new method\nproduces excellent and stable spectral estimates and this is demonstrated via\nsimulated data and on differenced infant ECG data. A major additional benefit\nof the Bayesian paradigm is that we obtain rigorous and useful credible\nintervals of the evolving spectral structure. We show how the Bayesian credible\nintervals provide extra insight into the infant ECG data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 10:02:22 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Nason", "Guy P.", ""], ["Stevens", "Kara N.", ""]]}, {"id": "1309.2695", "submitter": "Paul McNicholas", "authors": "Sharon M. McNicholas, Paul D. McNicholas and Ryan P. Browne", "title": "Mixtures of Variance-Gamma Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of variance-gamma distributions is introduced and developed for\nmodel-based clustering and classification. The latest in a growing line of\nnon-Gaussian mixture approaches to clustering and classification, the proposed\nmixture of variance-gamma distributions is a special case of the recently\ndeveloped mixture of generalized hyperbolic distributions, and a restriction is\nrequired to ensure identifiability. Our mixture of variance-gamma distributions\nis perhaps the most useful such special case and, we will contend, may be more\nuseful than the mixture of generalized hyperbolic distributions in some cases.\nIn addition to being an alternative to the mixture of generalized hyperbolic\ndistributions, our mixture of variance-gamma distributions serves as an\nalternative to the ubiquitous mixture of Gaussian distributions, which is a\nspecial case, as well as several non-Gaussian approaches, some of which are\nspecial cases. The mathematical development of our mixture of variance-gamma\ndistributions model relies on its relationship with the generalized inverse\nGaussian distribution; accordingly, the latter is reviewed before our mixture\nof variance-gamma distributions is presented. Parameter estimation carried out\nwithin the expectation-maximization framework.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 23:54:16 GMT"}, {"version": "v2", "created": "Mon, 29 Dec 2014 17:14:54 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["McNicholas", "Sharon M.", ""], ["McNicholas", "Paul D.", ""], ["Browne", "Ryan P.", ""]]}, {"id": "1309.2961", "submitter": "Faton Merovci", "authors": "Faton Merovci and Ibrahim Elbatal", "title": "The McDonald Modified Weibull Distribution: Properties and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A six parameter distribution so-called the McDonald modified Weibull\ndistribution is defined and studied. The new distribution contains, as special\nsubmodels, several important distributions discussed in the literature, such as\nthe beta modified Weibull, Kumaraswamy modified Weibull, McDonald Weibull and\nmodified Weibull distribution,among others. The new distribution can be used\neffectively in the analysis of survival data since it accommodates monotone,\nunimodal and bathtub-shaped hazard functions. We derive the moments.We propose\nthe method of maximum likelihood for estimating the model parameters and obtain\nthe observed information matrix. A real data set is used to illustrate the\nimportance and flexibility of the new distribution.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 20:32:30 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Merovci", "Faton", ""], ["Elbatal", "Ibrahim", ""]]}, {"id": "1309.2978", "submitter": "Hugues Aschard", "authors": "Hugues Aschard, Bjarni J. Vilhj\\'almsson, Nicolas Greliche,\n  Pierre-Emmanuel Morange, David-Alexandre Tr\\'egou\\\"et, Peter Kraft", "title": "Maximizing the Power of Principal Components Analysis of Correlated\n  Phenotypes in Genome-wide Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component analysis (PCA) is a useful statistical technique that is\ncommonly used for multivariate analysis of correlated variables. It is usually\napplied as a dimension reduction method: the top principal components (PCs)\nexplaining most of total variance are tested for association with a predictor\nof interest, and the remaining PCs are ignored. This strategy has been widely\napplied in genetic epidemiology, however some of its aspects are not well\nappreciated in the context of single nucleotide polymorphisms (SNPs)\nassociation testing. In this study, we review the theoretical basis of PCA and\nits behavior when testing for association between a SNP and two correlated\ntraits under various scenarios. We then evaluate with simulations the power of\nseveral different PCA-based strategies when analyzing up to 100 correlated\ntraits. We show that contrary to widespread practice that testing the top PCs\nonly can be dramatically underpowered since PCs explaining a low amount of the\ntotal phenotypic variance can harbor substantial genetic associations.\nFurthermore, we demonstrate that PC-based strategies that use all PCs have\ngreat potential to detect negatively pleiotropic genetic variants (e.g.\nvariants with opposite effects on positively correlated traits) and genetic\nvariants that are exclusively associated with a single trait, but only achieve\na moderate gain in power to detect positive pleiotropic genetic loci. Finally,\nthe genome-wide association study of five correlated coagulation traits in 685\nsubjects from the MARTHA study confirms these results. The joint analysis of\nthe five PCs from the coagulation traits identified two new candidate SNPs,\nwhich were most strongly associated with the 5th PC that explained the smallest\namount of phenotypic variance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 21:15:50 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Aschard", "Hugues", ""], ["Vilhj\u00e1lmsson", "Bjarni J.", ""], ["Greliche", "Nicolas", ""], ["Morange", "Pierre-Emmanuel", ""], ["Tr\u00e9gou\u00ebt", "David-Alexandre", ""], ["Kraft", "Peter", ""]]}, {"id": "1309.2983", "submitter": "Tatiana Xifara Ph.D.", "authors": "Tatiana Xifara, Chris Sherlock, Samuel Livingstone, Simon Byrne, Mark\n  Girolami", "title": "Langevin diffusions and the Metropolis-adjusted Langevin algorithm", "comments": null, "journal-ref": "Statistics & Probability Letters. Volume 91, August 2014, pages\n  14-19", "doi": "10.1016/j.spl.2014.04.002", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a clarification of the description of Langevin diffusions on\nRiemannian manifolds and of the measure underlying the invariant density. As a\nresult we propose a new position-dependent Metropolis-adjusted Langevin\nalgorithm (MALA) based upon a Langevin diffusion in $\\mathbb{R}^d$ which has\nthe required invariant density with respect to Lebesgue measure. We show that\nour diffusion and the diffusion upon which a previously-proposed\nposition-dependent MALA is based are equivalent in some cases but are distinct\nin general. A simulation study illustrates the gain in efficiency provided by\nthe new position-dependent MALA.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 22:04:18 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Xifara", "Tatiana", ""], ["Sherlock", "Chris", ""], ["Livingstone", "Samuel", ""], ["Byrne", "Simon", ""], ["Girolami", "Mark", ""]]}, {"id": "1309.3145", "submitter": "Timothy Christensen", "authors": "Timothy Christensen", "title": "Nonparametric identification of positive eigenfunctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important features of certain economic models may be revealed by studying\npositive eigenfunctions of appropriately chosen linear operators. Examples\ninclude long-run risk-return relationships in dynamic asset pricing models and\ncomponents of marginal utility in external habit formation models. This paper\nprovides identification conditions for positive eigenfunctions in nonparametric\nmodels. Identification is achieved if the operator satisfies two mild\npositivity conditions and a power compactness condition. Both existence and\nidentification are achieved under a further non-degeneracy condition. The\ngeneral results are applied to obtain new identification conditions for\nexternal habit formation models and for positive eigenfunctions of pricing\noperators in dynamic asset pricing models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 13:08:39 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2013 23:15:13 GMT"}, {"version": "v3", "created": "Fri, 15 Aug 2014 15:24:37 GMT"}, {"version": "v4", "created": "Fri, 29 Aug 2014 21:32:02 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Christensen", "Timothy", ""]]}, {"id": "1309.3268", "submitter": "Faton Merovci", "authors": "Faton Merovci, Ibrahim Elbatal and Alaa Ahmed", "title": "Transmuted Generalized Inverse Weibull Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalization of the generalized inverse Weibull distribution so-called\ntransmuted generalized inverse Weibull dis- tribution is proposed and studied.\nWe will use the quadratic rank transmutation map (QRTM) in order to generate a\nflexible family of probability distributions taking generalized inverse Weibull\ndistribution as the base value distribution by introducing a new parameter that\nwould offer more distributional flexibility. Various structural properties\nincluding explicit expressions for the mo- ments, quantiles, and moment\ngenerating function of the new dis- tribution are derived.We proposed the\nmethod of maximum likelihood for estimating the model parameters and obtain the\nobserved information matrix. A real data set are used to compare the exibility\nof the transmuted version versus the generalized inverseWeibull distribution.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 22:16:50 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Merovci", "Faton", ""], ["Elbatal", "Ibrahim", ""], ["Ahmed", "Alaa", ""]]}, {"id": "1309.3295", "submitter": "Nicholas James", "authors": "Nicholas A. James and David S. Matteson", "title": "ecp: An R Package for Nonparametric Multiple Change Point Analysis of\n  Multivariate Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many different ways in which change point analysis can be\nperformed, from purely parametric methods to those that are distribution free.\nThe ecp package is designed to perform multiple change point analysis while\nmaking as few assumptions as possible. While many other change point methods\nare applicable only for univariate data, this R package is suitable for both\nunivariate and multivariate observations. Estimation can be based upon either a\nhierarchical divisive or agglomerative algorithm. Divisive estimation\nsequentially identifies change points via a bisection algorithm. The\nagglomerative algorithm estimates change point locations by determining an\noptimal segmentation. Both approaches are able to detect any type of\ndistributional change within the data. This provides an advantage over many\nexisting change point algorithms which are only able to detect changes within\nthe marginal distributions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 20:38:42 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2013 22:00:58 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["James", "Nicholas A.", ""], ["Matteson", "David S.", ""]]}, {"id": "1309.3339", "submitter": "Robert Kohn", "authors": "Minh-Ngoc Tran and Marcel Scharth and Michael K. Pitt and Robert Kohn", "title": "Importance sampling squared for Bayesian inference in latent variable\n  models", "comments": "45 pages, 5 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian inference by importance sampling when the likelihood is\nanalytically intractable but can be unbiasedly estimated. We refer to this\nprocedure as importance sampling squared (IS2), as we can often estimate the\nlikelihood itself by importance sampling. We provide a formal justification for\nimportance sampling when working with an estimate of the likelihood and study\nits convergence properties. We analyze the effect of estimating the likelihood\non the resulting inference and provide guidelines on how to set up the\nprecision of the likelihood estimate in order to obtain an optimal tradeoff?\nbetween computational cost and accuracy for posterior inference on the model\nparameters. We illustrate the procedure in empirical applications for a\ngeneralized multinomial logit model and a stochastic volatility model. The\nresults show that the IS2 method can lead to fast and accurate posterior\ninference under the optimal implementation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 00:53:20 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 00:41:34 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2014 11:42:14 GMT"}, {"version": "v4", "created": "Sun, 24 Jul 2016 01:07:30 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Scharth", "Marcel", ""], ["Pitt", "Michael K.", ""], ["Kohn", "Robert", ""]]}, {"id": "1309.3387", "submitter": "Liang Li Dr.", "authors": "Liang Li, Wei Dong, Yindong Ji, Lang Tong", "title": "A Subspace Technique for The Identification of Switched Affine Models", "comments": "13 pages, 14 figures. arXiv admin note: text overlap with\n  arXiv:1307.0326", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The problem of estimating parameters of switched affine systems with noisy\ninput-output observations is considered. The switched affine models is\ntransformed into a switched linear one by removing its intersection subspace,\nwhich is estimated from observations. A subspace technique is proposed to\nexploit the observations' permutation structure, which transforms the problem\nof associating observations with subsystems into one of de-permutating a block\ndiagonal matrix, referred as adjacency matrix. Then a normalized spectral\nclustering algorithm is presented to recover the block structure of adjacency\nmatrix, from which each observation is related to a particular subsystem. With\nthe labelled observations, parameters of the submodel are estimated via the\ntotal least squares (TLS) estimator. The proposed technique is applicable to\nswitched affine systems with arbitrarily shaped domain partitions, and it\noffers significantly improved performance and lowered computation complexity\nthan existing techniques.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 07:44:53 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Li", "Liang", ""], ["Dong", "Wei", ""], ["Ji", "Yindong", ""], ["Tong", "Lang", ""]]}, {"id": "1309.3489", "submitter": "Nicolai Meinshausen", "authors": "Nicolai Meinshausen", "title": "Group-bound: confidence intervals for groups of variables in sparse\n  high-dimensional regression without assumptions on the design", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is in general challenging to provide confidence intervals for individual\nvariables in high-dimensional regression without making strict or unverifiable\nassumptions on the design matrix. We show here that a \"group-bound\" confidence\ninterval can be derived without making any assumptions on the design matrix.\nThe lower bound for the regression coefficient of individual variables can be\nderived via linear programming. The idea also generalises naturally to groups\nof variables, where we can derive a one-sided confidence interval for the joint\neffect of a group. While the confidence intervals of individual variables are\nby the nature of the problem often very wide, it is shown to be possible to\ndetect the contribution of groups of highly correlated predictor variables even\nwhen no variable individually shows a significant effect. The assumptions\nnecessary to detect the effect of groups of variables are shown to be weaker\nthan the weakest known assumptions to detect the effect of individual\nvariables.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 15:44:49 GMT"}, {"version": "v2", "created": "Wed, 11 Jun 2014 12:30:33 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Meinshausen", "Nicolai", ""]]}, {"id": "1309.3520", "submitter": "Daniel Williamson", "authors": "Daniel Williamson and Ian Vernon", "title": "Efficient uniform designs for multi-wave computer experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the problem of generating uniform designs in very\nsmall subregions of computer model input space that have been identified in\nprevious experiments as worthy of further study. The method is capable of\nproducing uniform designs in subregions of computer model input space defined\nby a membership function that consists of a continuous function passing a\nthreshold test, and does so far more efficiently than current methods when\nthese subregions are small. Our application is designing for regions of input\nspace that are not ruled out by history matching, a statistical methodology\napplied in numerous diverse scientific applications whereby model runs are used\nto cut out regions of input space that are incompatible with real world\nobservations. History matching defines a membership function for a region of\ninput space that is not ruled out yet by observations in the form of a distance\nmetric called implausibility. We use this distance metric to drive a new type\nof Evolutionary Monte Carlo algorithm with a uniform distribution on the not\nruled out yet region as its target distribution. The algorithm can locate and\ngenerate uniform points within extremely small subspaces of the computer model\ninput space with complex and even disconnected topologies. We illustrate the\nperformance of the technique in comparison to current methods with a number of\nidealised examples. We then apply our algorithm to generating an optimal design\nfor the not ruled out yet region of a galaxy simulation model called GALFORM\nfollowing 4 previous waves of history matching where the target region is\n0.001% the volume of the input space.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 17:39:32 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Williamson", "Daniel", ""], ["Vernon", "Ian", ""]]}, {"id": "1309.3533", "submitter": "Emily Fox", "authors": "Emily B. Fox and Michael I. Jordan", "title": "Mixed Membership Models for Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we discuss some of the consequences of the mixed membership\nperspective on time series analysis. In its most abstract form, a mixed\nmembership model aims to associate an individual entity with some set of\nattributes based on a collection of observed data. Although much of the\nliterature on mixed membership models considers the setting in which\nexchangeable collections of data are associated with each member of a set of\nentities, it is equally natural to consider problems in which an entire time\nseries is viewed as an entity and the goal is to characterize the time series\nin terms of a set of underlying dynamic attributes or \"dynamic regimes\".\nIndeed, this perspective is already present in the classical hidden Markov\nmodel, where the dynamic regimes are referred to as \"states\", and the\ncollection of states realized in a sample path of the underlying process can be\nviewed as a mixed membership characterization of the observed time series. Our\ngoal here is to review some of the richer modeling possibilities for time\nseries that are provided by recent developments in the mixed membership\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 18:31:02 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Fox", "Emily B.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1309.3774", "submitter": "Faton Merovci", "authors": "Faton Merovci and Ibrahim Elbatal", "title": "Transmuted Lindley-Geometric Distribution and its applications", "comments": "20 pages, 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:1309.3268", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A functional composition of the cumulative distribution function of one\nprobability distribution with the inverse cumulative distribution function of\nanother is called the transmutation map. In this article, we will use the\nquadratic rank transmutation map (QRTM) in order to generate a flexible family\nof probability distributions taking Lindley geometric distribution as the base\nvalue distribution by introducing a new parameter that would offer more\ndistributional flexibility. It will be shown that the analytical results are\napplicable to model real world data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 21:11:14 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2013 08:30:57 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Merovci", "Faton", ""], ["Elbatal", "Ibrahim", ""]]}, {"id": "1309.3802", "submitter": "Hugh Chipman", "authors": "Shirin Golchi, Derek R. Bingham, Hugh Chipman, David A. Campbell", "title": "Monotone Function Estimation for Computer Experiments", "comments": "28 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical modeling of computer experiments sometimes prior information\nis available about the underlying function. For example, the physical system\nsimulated by the computer code may be known to be monotone with respect to some\nor all inputs. We develop a Bayesian approach to Gaussian process modelling\ncapable of incorporating monotonicity information for computer model emulation.\nMarkov chain Monte Carlo methods are used to sample from the posterior\ndistribution of the process given the simulator output and monotonicity\ninformation. The performance of the proposed approach in terms of predictive\naccuracy and uncertainty quantification is demonstrated in a number of\nsimulated examples as well as a real queueing system application.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2013 23:02:04 GMT"}, {"version": "v2", "created": "Sat, 14 Jun 2014 08:50:53 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Golchi", "Shirin", ""], ["Bingham", "Derek R.", ""], ["Chipman", "Hugh", ""], ["Campbell", "David A.", ""]]}, {"id": "1309.3895", "submitter": "Roberto Colombi", "authors": "Roberto Colombi, Sabrina Giordano", "title": "Multiple Hidden Markov Models for Categorical Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce multiple hidden Markov models (MHMMs) where an observed\nmultivariate categorical time series depends on an unobservable multivariate\nMar- kov chain. MHMMs provide an elegant framework for specifying various\nindependence relationships between multiple discrete time processes. These\nindependencies are interpreted as Markov properties of a mixed graph and a\nchain graph associated to the latent and observable components of the MHMM,\nrespectively. These Markov properties are also translated into zero\nrestrictions on the parameters of marginal models for the transition\nprobabilities and the distributions of the observable variables given the\nlatent states.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 10:46:58 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Colombi", "Roberto", ""], ["Giordano", "Sabrina", ""]]}, {"id": "1309.4054", "submitter": "Emma Persson", "authors": "Emma Persson, Jenny H\\\"aggstr\\\"om, Ingeborg Waernbaum, Xavier de Luna", "title": "Data-driven Algorithms for Dimension Reduction in Causal Inference", "comments": "27 pages, 2 figures, 11 tables", "journal-ref": "Computational Statistics and Data Analysis, 2017, Vol. 105, p.\n  280-292", "doi": "10.1016/j.csda.2016.08.012", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, the causal effect of a treatment may be confounded\nwith variables that are related to both the treatment and the outcome of\ninterest. In order to identify a causal effect, such studies often rely on the\nunconfoundedness assumption, i.e., that all confounding variables are observed.\nThe choice of covariates to control for, which is primarily based on subject\nmatter knowledge, may result in a large covariate vector in the attempt to\nensure that unconfoundedness holds. However, including redundant covariates can\naffect bias and efficiency of nonparametric causal effect estimators, e.g., due\nto the curse of dimensionality. Data-driven algorithms for the selection of\nsufficient covariate subsets are investigated. Under the assumption of\nunconfoundedness the algorithms search for minimal subsets of the covariate\nvector. Based, e.g., on the framework of sufficient dimension reduction or\nkernel smoothing, the algorithms perform a backward elimination procedure\nassessing the significance of each covariate. Their performance is evaluated in\nsimulations and an application using data from the Swedish Childhood Diabetes\nRegister is also presented.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 18:00:22 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 09:49:44 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Persson", "Emma", ""], ["H\u00e4ggstr\u00f6m", "Jenny", ""], ["Waernbaum", "Ingeborg", ""], ["de Luna", "Xavier", ""]]}, {"id": "1309.4073", "submitter": "Duncan Blythe", "authors": "Duncan A.J. Blythe", "title": "A rigorous and efficient asymptotic test for power-law cross-correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Podobnik and Stanley recently proposed a novel framework, Detrended\nCross-Correlation Analysis, for the analysis of power-law cross-correlation\nbetween two time-series, a phenomenon which occurs widely in physical,\ngeophysical, financial and numerous additional applications. While highly\npromising in these important application domains, to date no rigorous or\nefficient statistical test has been proposed which uses the information\nprovided by DCCA across time-scales for the presence of this power-law\ncross-correlation. In this paper we fill this gap by proposing a method based\non DCCA for testing the hypothesis of power-law cross-correlation; the method\nsynthesizes the information generated by DCCA across time-scales and returns\nconservative but practically relevant p-values for the null hypothesis of zero\ncorrelation, which may be efficiently calculated in software. Thus our\nproposals generate confidence estimates for a DCCA analysis in a fully\nprobabilistic fashion.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 19:17:55 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2013 11:12:43 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Blythe", "Duncan A. J.", ""]]}, {"id": "1309.4144", "submitter": "Daniel Cervone", "authors": "Daniel Cervone, Natesh S. Pillai, Debdeep Pati, Ross Berbeco, John\n  Henry Lewis", "title": "A location-mixture autoregressive model for online forecasting of lung\n  tumor motion", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS744 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1341-1371", "doi": "10.1214/14-AOAS744", "report-no": "IMS-AOAS-AOAS744", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung tumor tracking for radiotherapy requires real-time, multiple-step ahead\nforecasting of a quasi-periodic time series recording instantaneous tumor\nlocations. We introduce a location-mixture autoregressive (LMAR) process that\nadmits multimodal conditional distributions, fast approximate inference using\nthe EM algorithm and accurate multiple-step ahead predictive distributions.\nLMAR outperforms several commonly used methods in terms of out-of-sample\nprediction accuracy using clinical data from lung tumor patients. With its\nsuperior predictive performance and real-time computation, the LMAR model could\nbe effectively implemented for use in current tumor tracking systems.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 00:56:42 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2013 04:40:49 GMT"}, {"version": "v3", "created": "Wed, 9 Apr 2014 18:45:14 GMT"}, {"version": "v4", "created": "Wed, 5 Nov 2014 13:08:36 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Cervone", "Daniel", ""], ["Pillai", "Natesh S.", ""], ["Pati", "Debdeep", ""], ["Berbeco", "Ross", ""], ["Lewis", "John Henry", ""]]}, {"id": "1309.4199", "submitter": "Jan Luts", "authors": "Jan Luts and Matt P. Wand", "title": "Variational inference for count response semiparametric regression", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast variational approximate algorithms are developed for Bayesian\nsemiparametric regression when the response variable is a count, i.e. a\nnon-negative integer. We treat both the Poisson and Negative Binomial families\nas models for the response variable. Our approach utilizes recently developed\nmethodology known as non-conjugate variational message passing. For\nconcreteness, we focus on generalized additive mixed models, although our\nvariational approximation approach extends to a wide class of semiparametric\nregression models such as those containing interactions and elaborate random\neffect structure.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 06:50:41 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Luts", "Jan", ""], ["Wand", "Matt P.", ""]]}, {"id": "1309.4275", "submitter": "Eric J\\\"arpe", "authors": "Eric J\\\"arpe and Quentin Gouchet", "title": "Segmentation of Encrypted data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The retrieval of data from computer hard drives that have been seized from\npolice busts against suspected criminals are sometimes not straight forward.\nTypically the incriminating data, which may be important evidence in subsequent\ntrials, is encrypted and quick deleted. The cryptanalysis of what can be\nrecovered from such hard drives is then subject to time-consuming brute-forcing\nand password guessing. To this end methods for accurate classification of what\nis encrypted data and what is not is of the essence. Here a procedure for\ndiscriminating encrypted data from non-encrypted is derived. Several methods\nare suggested and their accuracy is evaluated in different ways. Two methods to\ndetect where encrypted data is located in a hard disk drive are detailed using\npassive change-point detection. The measures of performance of such methods are\ndiscussed and a new property for evaluation is suggested. The methods are then\nevaluated and discussed according to the new performance measure as well as the\nstandard measures.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 12:02:00 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2013 19:34:39 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 08:43:07 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["J\u00e4rpe", "Eric", ""], ["Gouchet", "Quentin", ""]]}, {"id": "1309.4686", "submitter": "Max Farrell", "authors": "Max H. Farrell", "title": "Robust Inference on Average Treatment Effects with Possibly More\n  Covariates than Observations", "comments": "48 pages, 1 figure, 1 table", "journal-ref": "Journal of Econometrics 189 (2015), pp. 1-23", "doi": "10.1016/j.jeconom.2015.06.017", "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns robust inference on average treatment effects following\nmodel selection. In the selection on observables framework, we show how to\nconstruct confidence intervals based on a doubly-robust estimator that are\nrobust to model selection errors and prove that they are valid uniformly over a\nlarge class of treatment effect models. The class allows for multivalued\ntreatments with heterogeneous effects (in observables), general\nheteroskedasticity, and selection amongst (possibly) more covariates than\nobservations. Our estimator attains the semiparametric efficiency bound under\nappropriate conditions. Precise conditions are given for any model selector to\nyield these results, and we show how to combine data-driven selection with\neconomic theory. For implementation, we give a specific proposal for selection\nbased on the group lasso, which is particularly well-suited to treatment\neffects data, and derive new results for high-dimensional, sparse multinomial\nlogistic regression. A simulation study shows our estimator performs very well\nin finite samples over a wide range of models. Revisiting the National\nSupported Work demonstration data, our method yields accurate estimates and\ntight confidence intervals.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 15:47:27 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2015 19:24:26 GMT"}, {"version": "v3", "created": "Thu, 1 Feb 2018 22:49:57 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Farrell", "Max H.", ""]]}, {"id": "1309.4740", "submitter": "Song Cai", "authors": "Song Cai, Jiahua Chen, James V. Zidek", "title": "Hypothesis testing in the presence of multiple samples under density\n  ratio models", "comments": "38 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a hypothesis testing method given independent samples\nfrom a number of connected populations. The method is motivated by a forestry\nproject for monitoring change in the strength of lumber. Traditional practice\nhas been built upon nonparametric methods which ignore the fact that these\npopulations are connected. By pooling the information in multiple samples\nthrough a density ratio model, the proposed empirical likelihood method leads\nto a more efficient inference and therefore reduces the cost in applications.\nThe new test has a classical chi-square null limiting distribution. Its power\nfunction is obtained under a class of local alternatives. The local power is\nfound increased even when some underlying populations are unrelated to the\nhypothesis of interest. Simulation studies confirm that this test has better\npower properties than potential competitors, and is robust to model\nmisspecification. An application example to lumber strength is included.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 18:36:44 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2013 13:51:17 GMT"}, {"version": "v3", "created": "Sun, 22 Dec 2013 01:50:46 GMT"}, {"version": "v4", "created": "Thu, 14 May 2015 19:54:48 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Cai", "Song", ""], ["Chen", "Jiahua", ""], ["Zidek", "James V.", ""]]}, {"id": "1309.4796", "submitter": "Lijun Peng", "authors": "Lijun Peng, Luis Carvalho", "title": "Bayesian Degree-Corrected Stochastic Blockmodels for Community Detection", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection in networks has drawn much attention in diverse fields,\nespecially social sciences. Given its significance, there has been a large body\nof literature with approaches from many fields. Here we present a statistical\nframework that is representative, extensible, and that yields an estimator with\ngood properties. Our proposed approach considers a stochastic blockmodel based\non a logistic regression formulation with node correction terms. We follow a\nBayesian approach that explicitly captures the community behavior via prior\nspecification. We further adopt a data augmentation strategy with latent\nPolya-Gamma variables to obtain posterior samples. We conduct inference based\non a principled, canonically mapped centroid estimator that formally addresses\nlabel non-identifiability and captures representative community assignments. We\ndemonstrate the proposed model and estimation on real-world as well as\nsimulated benchmark networks and show that the proposed model and estimator are\nmore flexible, representative, and yield smaller error rates when compared to\nthe MAP estimator from classical degree-corrected stochastic blockmodels.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 21:14:45 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 17:50:02 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Peng", "Lijun", ""], ["Carvalho", "Luis", ""]]}, {"id": "1309.5019", "submitter": "Ying Yuan", "authors": "Suyu Liu and Ying Yuan", "title": "Bayesian Decision-optimal Interval Designs for Phase I Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval designs are a class of phase I trial designs for which the decision\nof dose assignment is determined by comparing the observed toxicity rate at the\ncurrent dose with a prespecified (toxicity tolerance) interval. If the observed\ntoxicity rate is located within the interval, we retain the current dose; if\nthe observed toxicity rate is greater than the upper boundary of the interval,\nwe deescalate the dose; and if the observed toxicity rate is smaller than the\nlower boundary of the interval, we escalate the dose. The most critical issue\nfor the interval design is choosing an appropriate interval so that the design\nhas good operating characteristics. By casting dose finding as a Bayesian\ndecision-making problem, we propose new flexible methods to select the interval\nboundaries so as to minimize the probability of inappropriate dose assignment\nfor patients. We show, both theoretically and numerically, that the resulting\noptimal interval designs not only have desirable finite- and large-sample\nproperties, but also are particularly easy to implement in practice. Compared\nto existing designs, the proposed (local) optimal design has comparable average\nperformance, but a lower risk of yielding a poorly performing clinical trial.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 15:24:51 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Liu", "Suyu", ""], ["Yuan", "Ying", ""]]}, {"id": "1309.5050", "submitter": "Anton Korobeynikov", "authors": "Nina Golyandina, Anton Korobeynikov, Alex Shlemov, Konstantin Usevich", "title": "Multivariate and 2D Extensions of Singular Spectrum Analysis with the\n  Rssa Package", "comments": null, "journal-ref": "Journal of Statistical Software, v.67, Issue 2, 2015, p. 1-78", "doi": "10.18637/jss.v067.i02", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementation of multivariate and 2D extensions of Singular Spectrum\nAnalysis (SSA) by means of the R-package Rssa is considered. The extensions\ninclude MSSA for simultaneous analysis and forecasting of several time series\nand 2D-SSA for analysis of digital images. A new extension of 2D-SSA analysis\ncalled Shaped 2D-SSA is introduced for analysis of images of arbitrary shape,\nnot necessary rectangular. It is shown that implementation of Shaped 2D-SSA can\nserve as a base for implementation of MSSA and other generalizations. Efficient\nimplementation of operations with Hankel and Hankel-block-Hankel matrices\nthrough FFT is suggested. Examples with code fragments in R, which explain the\nmethodology and demonstrate the proper use of Rssa, are presented.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 16:55:17 GMT"}, {"version": "v2", "created": "Fri, 19 Sep 2014 12:50:08 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Golyandina", "Nina", ""], ["Korobeynikov", "Anton", ""], ["Shlemov", "Alex", ""], ["Usevich", "Konstantin", ""]]}, {"id": "1309.5109", "submitter": "Ashton Verdery", "authors": "Ashton M. Verdery, Ted Mouw, Shawn Bauldry, Peter J. Mucha", "title": "Network Structure and Biased Variance Estimation in Respondent Driven\n  Sampling", "comments": "56 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores bias in the estimation of sampling variance in Respondent\nDriven Sampling (RDS). Prior methodological work on RDS has focused on its\nproblematic assumptions and the biases and inefficiencies of its estimators of\nthe population mean. Nonetheless, researchers have given only slight attention\nto the topic of estimating sampling variance in RDS, despite the importance of\nvariance estimation for the construction of confidence intervals and hypothesis\ntests. In this paper, we show that the estimators of RDS sampling variance rely\non a critical assumption that the network is First Order Markov (FOM) with\nrespect to the dependent variable of interest. We demonstrate, through\nintuitive examples, mathematical generalizations, and computational experiments\nthat current RDS variance estimators will always underestimate the population\nsampling variance of RDS in empirical networks that do not conform to the FOM\nassumption. Analysis of 215 observed university and school networks from\nFacebook and Add Health indicates that the FOM assumption is violated in every\nempirical network we analyze, and that these violations lead to substantially\nbiased RDS estimators of sampling variance. We propose and test two alternative\nvariance estimators that show some promise for reducing biases, but which also\nillustrate the limits of estimating sampling variance with only partial\ninformation on the underlying population social network.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 22:02:23 GMT"}, {"version": "v2", "created": "Sat, 17 May 2014 20:12:23 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2015 14:39:05 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Verdery", "Ashton M.", ""], ["Mouw", "Ted", ""], ["Bauldry", "Shawn", ""], ["Mucha", "Peter J.", ""]]}, {"id": "1309.5192", "submitter": "Hamid  Zareifard Jahromi", "authors": "Hamid Zareifard, Havard Rue, Majid Jafari Khaledi, Finn Lindgren", "title": "A skew Gaussian decomposable graphical model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper propose a novel decomposable graphical model to accommodate skew\nGaussian graphical models. We encode conditional independence structure among\nthe components of the multivariate closed skew normal random vector by means of\na decomposable graph and so that the pattern of zero off-diagonal elements in\nthe precision matrix corresponds to the missing edges of the given graph. We\npresent conditions that guarantee the propriety of the posterior distributions\nunder the standard noninformative priors for mean vector and precision matrix,\nand a proper prior for skewness parameter. The identifiability of the\nparameters is investigated by a simulation study. Finally, we apply our\nmethodology to two data sets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 07:40:06 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Zareifard", "Hamid", ""], ["Rue", "Havard", ""], ["Khaledi", "Majid Jafari", ""], ["Lindgren", "Finn", ""]]}, {"id": "1309.5264", "submitter": "Gordon J Ross", "authors": "Gordon J Ross", "title": "Sequential Change Detection in the Presence of Unknown Parameters", "comments": "Statistics and Computing, 2013, forthcoming", "journal-ref": "Statistics and Computing (2014), 24:1017-1030", "doi": "10.1007/s11222-013-9417-1", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly required to detect change points in sequences of random\nvariables. In the most difficult setting of this problem, change detection must\nbe performed sequentially with new observations being constantly received over\ntime. Further, the parameters of both the pre- and post- change distributions\nmay be unknown. In a recent paper by Hawkins and Zamba (2005), the sequential\ngeneralised likelihood ratio test was introduced for detecting changes in this\ncontext, under the assumption that the observations follow a Gaussian\ndistribution. However, we show that the asymptotic approximation used in their\ntest statistic leads to it being conservative even when a large numbers of\nobservations is available. We propose an improved procedure which is more\nefficient, in the sense of detecting changes faster, in all situations. We also\nshow that similar issues arise in other parametric change detection contexts,\nwhich we illustrate by introducing a novel monitoring procedure for sequences\nof Exponentially distributed random variable, which is an important topic in\ntime-to-failure modelling.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 13:28:11 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Ross", "Gordon J", ""]]}, {"id": "1309.5352", "submitter": "Maxwell Grazier G'Sell", "authors": "Max Grazier G'Sell, Stefan Wager, Alexandra Chouldechova, Robert\n  Tibshirani", "title": "Sequential Selection Procedures and False Discovery Rate Control", "comments": "31 pages, 14 figures. Accepted to the Journal of the Royal\n  Statistical Society: Series B", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multiple hypothesis testing setting where the hypotheses are\nordered and one is only permitted to reject an initial contiguous block,\nH_1,\\dots,H_k, of hypotheses. A rejection rule in this setting amounts to a\nprocedure for choosing the stopping point k. This setting is inspired by the\nsequential nature of many model selection problems, where choosing a stopping\npoint or a model is equivalent to rejecting all hypotheses up to that point and\nnone thereafter. We propose two new testing procedures, and prove that they\ncontrol the false discovery rate in the ordered testing setting. We also show\nhow the methods can be applied to model selection using recent results on\np-values in sequential model selection settings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 19:21:01 GMT"}, {"version": "v2", "created": "Fri, 25 Apr 2014 03:35:52 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2015 20:46:11 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["G'Sell", "Max Grazier", ""], ["Wager", "Stefan", ""], ["Chouldechova", "Alexandra", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1309.5486", "submitter": "Julio C\\'esar Hern\\'andez S\\'anchez", "authors": "Julio C\\'esar Hern\\'andez S\\'anchez and Jos\\'e Luis\n  Vicente-Villard\\'on", "title": "Logistic biplot for nominal data", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical Biplot Methods allow for the simultaneous representation of\nindividuals (rows) and variables (columns) of a data matrix. For Binary data,\nLogistic biplots have been recently developed.When data are nominal, linear or\neven binary logistic biplots are not adequate and techniques as Multiple\nCorrespondence Analysis (MCA), Latent Trait Analysis (LTA) or Item Response\nTheory for nominal items should be used instead. In this paper we extend the\nbinary logistic biplot to nominal data. The resulting method is termed Nominal\nLogistic Biplot, although the variables are represented as convex prediction\nregions rather than vectors. Using the methods from Computational Geometry, the\nset of prediction regions is converted to a set of points in such a way that\nthe prediction for each individual is established by its closest category\npoint. Then interpretation is based on distances rather than on projections. We\nstudy the geometry of such a representation and construct computational\nalgorithms for the estimation of parameters and the calculation of prediction\nregions. Nominal Logistic Biplots extend both MCA and LTA in the sense that\ngives a graphical representation for LTA similar to the one obtained in MCA.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2013 15:01:15 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["S\u00e1nchez", "Julio C\u00e9sar Hern\u00e1ndez", ""], ["Vicente-Villard\u00f3n", "Jos\u00e9 Luis", ""]]}, {"id": "1309.5923", "submitter": "Mengjie Chen", "authors": "Mengjie Chen and Zhao Ren and Hongyu Zhao and Harrison Zhou", "title": "Asymptotically Normal and Efficient Estimation of Covariate-Adjusted\n  Gaussian Graphical Model", "comments": "54 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tuning-free procedure is proposed to estimate the covariate-adjusted\nGaussian graphical model. For each finite subgraph, this estimator is\nasymptotically normal and efficient. As a consequence, a confidence interval\ncan be obtained for each edge. The procedure enjoys easy implementation and\nefficient computation through parallel estimation on subgraphs or edges. We\nfurther apply the asymptotic normality result to perform support recovery\nthrough edge-wise adaptive thresholding. This support recovery procedure is\ncalled ANTAC, standing for Asymptotically Normal estimation with Thresholding\nafter Adjusting Covariates. ANTAC outperforms other methodologies in the\nliterature in a range of simulation studies. We apply ANTAC to identify\ngene-gene interactions using an eQTL dataset. Our result achieves better\ninterpretability and accuracy in comparison with CAMPE.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 19:17:48 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Chen", "Mengjie", ""], ["Ren", "Zhao", ""], ["Zhao", "Hongyu", ""], ["Zhou", "Harrison", ""]]}, {"id": "1309.5999", "submitter": "Adriano Zambom", "authors": "Adriano Zanin Zambom, Julian A. A. Collazos and Ronaldo Dias", "title": "Genetic Algorithm for Constrained Optimization with Stochastic\n  Feasibility Region with Application to Vehicle Path Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-time trajectory planning for unmanned vehicles, on-board sensors,\nradars and other instruments are used to collect information on possible\nobstacles to be avoided and pathways to be followed. Since, in practice,\nobservations of the sensors have measurement errors, the stochasticity of the\ndata has to be incorporated into the models. In this paper, we consider using a\ngenetic algorithm for the constrained optimization problem of finding the\ntrajectory with minimum length between two locations, avoiding the obstacles on\nthe way. To incorporate the variability of the sensor readings, we propose a\nmore general framework, where the feasible regions of the genetic algorithm are\nstochastic. In this way, the probability that a possible solution of the search\nspace, say x, is feasible can be derived from the random observations of\nobstacles and pathways, creating a real-time data learning algorithm. By\nbuilding a confidence region from the observed data such that its border\nintersects with the solution point x, the level of the confidence region\ndefines the probability that x is feasible. We propose using a smooth penalty\nfunction based on the Gaussian distribution, facilitating the borders of the\nfeasible regions to be reached by the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 22:24:12 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Zambom", "Adriano Zanin", ""], ["Collazos", "Julian A. A.", ""], ["Dias", "Ronaldo", ""]]}, {"id": "1309.6024", "submitter": "Zhao Ren", "authors": "Zhao Ren, Tingni Sun, Cun-Hui Zhang, Harrison H. Zhou", "title": "Asymptotic normality and optimalities in estimation of large Gaussian\n  graphical models", "comments": "Published at http://dx.doi.org/10.1214/14-AOS1286 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 3, 991-1026", "doi": "10.1214/14-AOS1286", "report-no": "IMS-AOS-AOS1286", "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian graphical model, a popular paradigm for studying relationship\namong variables in a wide range of applications, has attracted great attention\nin recent years. This paper considers a fundamental question: When is it\npossible to estimate low-dimensional parameters at parametric square-root rate\nin a large Gaussian graphical model? A novel regression approach is proposed to\nobtain asymptotically efficient estimation of each entry of a precision matrix\nunder a sparseness condition relative to the sample size. When the precision\nmatrix is not sufficiently sparse, or equivalently the sample size is not\nsufficiently large, a lower bound is established to show that it is no longer\npossible to achieve the parametric rate in the estimation of each entry. This\nlower bound result, which provides an answer to the delicate sample size\nquestion, is established with a novel construction of a subset of sparse\nprecision matrices in an application of Le Cam's lemma. Moreover, the proposed\nestimator is proven to have optimal convergence rate when the parametric rate\ncannot be achieved, under a minimal sample requirement. The proposed estimator\nis applied to test the presence of an edge in the Gaussian graphical model or\nto recover the support of the entire model, to obtain adaptive rate-optimal\nestimation of the entire precision matrix as measured by the matrix $\\ell_q$\noperator norm and to make inference in latent variables in the graphical model.\nAll of this is achieved under a sparsity condition on the precision matrix and\na side condition on the range of its spectrum. This significantly relaxes the\ncommonly imposed uniform signal strength condition on the precision matrix,\nirrepresentability condition on the Hessian tensor operator of the covariance\nmatrix or the $\\ell_1$ constraint on the precision matrix. Numerical results\nconfirm our theoretical findings. The ROC curve of the proposed algorithm,\nAsymptotic Normal Thresholding (ANT), for support recovery significantly\noutperforms that of the popular GLasso algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 01:58:23 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2013 18:16:51 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 05:08:24 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Ren", "Zhao", ""], ["Sun", "Tingni", ""], ["Zhang", "Cun-Hui", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1309.6058", "submitter": "Heng Lian", "authors": "Heng Lian, Shujie Ma", "title": "Reduced-rank Regression in Sparse Multivariate Varying-Coefficient\n  Models with High-dimensional Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genetic studies, not only can the number of predictors obtained from\nmicroarray measurements be extremely large, there can also be multiple response\nvariables. Motivated by such a situation, we consider semiparametric dimension\nreduction methods in sparse multivariate regression models. Previous studies on\njoint variable and rank selection have focused on parametric models while here\nwe consider the more challenging varying-coefficient models which make the\ninvestigation on nonlinear interactions of variables possible. Spline\napproximation, rank constraints and concave group penalties are utilized for\nmodel estimation. Asymptotic oracle properties of the estimators are presented.\nWe also propose reduced-rank independent screening to deal with the situation\nwhen the dimension is so high that penalized estimation cannot be efficiently\napplied. In simulations, we show the advantages of simultaneously performing\nvariable and rank selection. A real data set is analyzed to illustrate the good\nprediction performance when incorporating interactions between genetic\nvariables and an index variable.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 06:27:26 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Lian", "Heng", ""], ["Ma", "Shujie", ""]]}, {"id": "1309.6108", "submitter": "Francesca Condino", "authors": "Ibrahim Elbatal, Francesca Condino, Filippo Domma", "title": "Reflected Generalized Beta Inverse Weibull Distribution: definition and\n  properties", "comments": "Baseline distribution changed, according to Jones (2012) [see\n  references]", "journal-ref": null, "doi": "10.1007/s13571-015-0114-2", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a broad class of distribution functions which is\ndefined by means of reflected generalized beta distribution. This class\nincludes that of Beta-generated distribution as a special case. In particular,\nwe use this class to extend the Inverse Weibull distribution in order to obtain\nthe Reflected Generalized Beta Inverse Weibull Distribution.For this new\ndistribution, moments, entropy, order statistics and a reliability measure are\nderived. The link between the Inverse Weibull and the Dagum distribution is\ngeneralized. Then the maximum likelihood estimators of the parameters are\nexamined and the observed Fisher information matrix provided. Finally, the\nusefulness of the model is illustrated by means of an application to real data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 10:38:59 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2014 09:08:32 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Elbatal", "Ibrahim", ""], ["Condino", "Francesca", ""], ["Domma", "Filippo", ""]]}, {"id": "1309.6148", "submitter": "Jose Brito", "authors": "Jose Andre de Moura Brito, Gustavo Silva Semaan, Pedro Luis do\n  Nascimento Silva and Nelson Maculan", "title": "An Integer Programming Formulation Applied to Optimum Allocation in\n  Multivariate Stratified Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of optimal allocation of samples in surveys using a stratified\nsampling plan was first discussed by Neyman in 1934. Since then, many\nresearchers have studied the problem of the sample allocation in multivariate\nsurveys and several methods have been proposed. Basically, these methods are\ndivided into two class: The first involves forming a weighted average of the\nstratum variances and finding the optimal allocation for the average variance.\nThe second class is associated with methods that require that an acceptable\ncoefficient of variation for each of the variables on which the allocation is\nto be done. Particularly, this paper proposes a new optimization approach to\nthe second problem. This approach is based on an integer programming\nformulation. Several experiments showed that the proposed approach is efficient\nway to solve this problem, considering a comparison of this approach with the\nother approach from the literature.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 13:22:42 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Brito", "Jose Andre de Moura", ""], ["Semaan", "Gustavo Silva", ""], ["Silva", "Pedro Luis do Nascimento", ""], ["Maculan", "Nelson", ""]]}, {"id": "1309.6216", "submitter": "Ivan Vuja\\v{c}i\\'c", "authors": "Ivan Vujacic, Antonino Abbruzzo and Ernst Wit", "title": "A computationally fast alternative to cross-validation in penalized\n  Gaussian graphical models", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of selection of regularization parameter in penalized\nGaussian graphical models. When the goal is to obtain the model with good\npredicting power, cross validation is the gold standard. We present a new\nestimator of Kullback-Leibler loss in Gaussian Graphical model which provides a\ncomputationally fast alternative to cross-validation. The estimator is obtained\nby approximating leave-one-out-cross validation. Our approach is demonstrated\non simulated data sets for various types of graphs. The proposed formula\nexhibits superior performance, especially in the typical small sample size\nscenario, compared to other available alternatives to cross validation, such as\nAkaike's information criterion and Generalized approximate cross validation. We\nalso show that the estimator can be used to improve the performance of the BIC\nwhen the sample size is small.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 15:31:20 GMT"}, {"version": "v2", "created": "Wed, 5 Mar 2014 14:10:44 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Vujacic", "Ivan", ""], ["Abbruzzo", "Antonino", ""], ["Wit", "Ernst", ""]]}, {"id": "1309.6361", "submitter": "Shandong Zhao", "authors": "Shandong Zhao, David A. van Dyk, Kosuke Imai", "title": "Causal Inference in Observational Studies with Non-Binary Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score methods have become a part of the standard toolkit for\napplied researchers who wish to ascertain causal effects from observational\ndata. While they were originally developed for binary treatments, several\nresearchers have proposed generalizations of the propensity score methodology\nfor non-binary treatment regimes. Such extensions have widened the\napplicability of propensity score methods and are indeed becoming increasingly\npopular themselves. In this article, we closely examine the two main\ngeneralizations of propensity score methods, namely, the propensity function\n(P-FUNCTION) of Imai and van Dyk (2004) and the generalized propensity score\n(GPS) of Hirano and Imbens (2004), along with recent extensions of the GPS that\naim to improve its robustness. We compare the assumptions, theoretical\nproperties, and empirical performance of these alternative methodologies. On a\ntheoretical level, the GPS and its extensions are advantageous in that they can\nbe used to estimate the full dose response function rather than the simple\naverage treatment effect that is typically estimated with the P-FUNCTION.\nUnfortunately, our analysis shows that in practice response models often used\nwith the original GPS are less flexible than those typically used with\npropensity score methods and are prone to misspecification. We compare new and\nexisting methods that improve the robustness of the GPS and propose methods\nthat use the P-FUNCTION to estimate the dose response function. We illustrate\nour findings and proposals through simulation studies, including one based on\nan empirical application.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 22:34:53 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Zhao", "Shandong", ""], ["van Dyk", "David A.", ""], ["Imai", "Kosuke", ""]]}, {"id": "1309.6473", "submitter": "Pierre E. Jacob", "authors": "Pierre E. Jacob, Alexandre H. Thiery", "title": "On nonnegative unbiased estimators", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1311 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 2, 769-784", "doi": "10.1214/15-AOS1311", "report-no": "IMS-AOS-AOS1311", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the existence of algorithms generating almost surely nonnegative\nunbiased estimators. We show that given a nonconstant real-valued function $f$\nand a sequence of unbiased estimators of $\\lambda\\in\\mathbb{R}$, there is no\nalgorithm yielding almost surely nonnegative unbiased estimators of\n$f(\\lambda)\\in\\mathbb{R}^+$. The study is motivated by pseudo-marginal Monte\nCarlo algorithms that rely on such nonnegative unbiased estimators. These\nmethods allow \"exact inference\" in intractable models, in the sense that\nintegrals with respect to a target distribution can be estimated without any\nsystematic error, even though the associated probability density function\ncannot be evaluated pointwise. We discuss the consequences of our results on\nthe applicability of pseudo-marginal algorithms and thus on the possibility of\nexact inference in intractable models. We illustrate our study with particular\nchoices of functions $f$ corresponding to known challenges in statistics, such\nas exact simulation of diffusions, inference in large datasets and doubly\nintractable distributions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 11:58:04 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 08:47:05 GMT"}, {"version": "v3", "created": "Wed, 14 Jan 2015 15:19:08 GMT"}, {"version": "v4", "created": "Wed, 1 Apr 2015 12:48:39 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Jacob", "Pierre E.", ""], ["Thiery", "Alexandre H.", ""]]}, {"id": "1309.6596", "submitter": "Georgiy Shevchenko", "authors": "Yuliya Mishura, Kostiantyn Ral'chenko, Oleg Seleznev, and Georgiy\n  Shevchenko", "title": "Asymptotic Properties of Drift Parameter Estimator Based on Discrete\n  Observations of Stochastic Differential Equation Driven by Fractional\n  Brownian Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of statistical estimation of an unknown drift parameter\nfor a stochastic differential equation driven by fractional Brownian motion.\nTwo estimators based on discrete observations of solution to the stochastic\ndifferential equations are constructed. It is proved that the estimators\nconverge almost surely to the parameter value as the observation interval\nexpands and the distance between observations vanishes. A bound for the rate of\nconvergence is given and numerical simulations are presented. As an auxilliary\nresult of independent interest we establish global estimates for fractional\nderivative of fractional Brownian motion.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 18:14:41 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Mishura", "Yuliya", ""], ["Ral'chenko", "Kostiantyn", ""], ["Seleznev", "Oleg", ""], ["Shevchenko", "Georgiy", ""]]}, {"id": "1309.6609", "submitter": "Hunter Glanz", "authors": "Hunter Glanz and Luis Carvalho", "title": "An Expectation-Maximization Algorithm for the Matrix Normal Distribution", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dramatic increases in the size and dimensionality of many recent data sets\nmake crucial the need for sophisticated methods that can exploit inherent\nstructure and handle missing values. In this article we derive an\nexpectation-maximization (EM) algorithm for the matrix normal distribution, a\ndistribution well-suited for naturally structured data such as spatio-temporal\ndata. We review previously established maximum likelihood matrix normal\nestimates, and then consider the situation involving missing data. We apply our\nEM method in a simulation study exploring errors across different dimensions\nand proportions of missing data. We compare these errors and computational\nrunning times to those from two alternative methods. Finally, we implement the\nproposed EM method on a satellite image dataset to investigate land-cover\nclassification separability.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 19:00:17 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Glanz", "Hunter", ""], ["Carvalho", "Luis", ""]]}, {"id": "1309.6697", "submitter": "Jos\\'e Luis Torrecilla", "authors": "Jos\\'e R. Berrendero, Antonio Cuevas, Jos\\'e L. Torrecilla", "title": "Variable selection in functional data classification: a maxima-hunting\n  proposal", "comments": null, "journal-ref": "Statistica Sinica 26 (2016), 619-638", "doi": "10.5705/ss.202014.0014", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is considered in the setting of supervised binary\nclassification with functional data $\\{X(t),\\ t\\in[0,1]\\}$. By \"variable\nselection\" we mean any dimension-reduction method which leads to replace the\nwhole trajectory\n  $\\{X(t),\\ t\\in[0,1]\\}$, with a low-dimensional vector\n$(X(t_1),\\ldots,X(t_k))$ still keeping a similar classification error. Our\nproposal for variable selection is based on the idea of selecting the local\nmaxima $(t_1,\\ldots,t_k)$ of the function ${\\mathcal V}_X^2(t)={\\mathcal\nV}^2(X(t),Y)$, where ${\\mathcal V}$ denotes the \"distance covariance\"\nassociation measure for random variables due to Sz\\'ekely, Rizzo and Bakirov\n(2007). This method provides a simple natural way to deal with the relevance\nvs. redundancy trade-off which typically appears in variable selection. This\npaper includes\n  (a) Some theoretical motivation: a result of consistent estimation on the\nmaxima of ${\\mathcal V}_X^2$ is shown. We also show different theoretical\nmodels for the underlying process $X(t)$ under which the relevant information\nin concentrated in the maxima of ${\\mathcal V}_X^2$.\n  (b) An extensive empirical study, including about 400 simulated models and\nreal data examples, aimed at comparing our variable selection method with other\nstandard proposals for dimension reduction.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 00:32:50 GMT"}, {"version": "v2", "created": "Fri, 24 Oct 2014 10:14:33 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2015 14:34:01 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Berrendero", "Jos\u00e9 R.", ""], ["Cuevas", "Antonio", ""], ["Torrecilla", "Jos\u00e9 L.", ""]]}, {"id": "1309.6790", "submitter": "Alexander W. Blocker", "authors": "Alexander W. Blocker, Xiao-Li Meng", "title": "The potential and perils of preprocessing: Building new foundations", "comments": "Published in at http://dx.doi.org/10.3150/13-BEJSP16 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 4, 1176-1211", "doi": "10.3150/13-BEJSP16", "report-no": "IMS-BEJ-BEJSP16", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preprocessing forms an oft-neglected foundation for a wide range of\nstatistical and scientific analyses. However, it is rife with subtleties and\npitfalls. Decisions made in preprocessing constrain all later analyses and are\ntypically irreversible. Hence, data analysis becomes a collaborative endeavor\nby all parties involved in data collection, preprocessing and curation, and\ndownstream inference. Even if each party has done its best given the\ninformation and resources available to them, the final result may still fall\nshort of the best possible in the traditional single-phase inference framework.\nThis is particularly relevant as we enter the era of \"big data\". The\ntechnologies driving this data explosion are subject to complex new forms of\nmeasurement error. Simultaneously, we are accumulating increasingly massive\ndatabases of scientific analyses. As a result, preprocessing has become more\nvital (and potentially more dangerous) than ever before.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 10:50:23 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Blocker", "Alexander W.", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "1309.6867", "submitter": "Yaniv Tenzer", "authors": "Yaniv Tenzer, Gal Elidan", "title": "Speedy Model Selection (SMS) for Copula Models", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-625-634", "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the challenge of efficiently learning the structure of expressive\nmultivariate real-valued densities of copula graphical models. We start by\ntheoretically substantiating the conjecture that for many copula families the\nmagnitude of Spearman's rank correlation coefficient is monotone in the\nexpected contribution of an edge in network, namely the negative copula\nentropy. We then build on this theory and suggest a novel Bayesian approach\nthat makes use of a prior over values of Spearman's rho for learning\ncopula-based models that involve a mix of copula families. We demonstrate the\ngeneralization effectiveness of our highly efficient approach on sizable and\nvaried real-life datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:51:22 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Tenzer", "Yaniv", ""], ["Elidan", "Gal", ""]]}, {"id": "1309.6895", "submitter": "Pietro Coretto", "authors": "Pietro Coretto and Christian Hennig", "title": "Consistency, breakdown robustness, and algorithms for robust improper\n  maximum likelihood clustering", "comments": "The title of this paper was originally: \"A consistent and breakdown\n  robust model-based clustering method\"", "journal-ref": "2017, Journal of Machine Learning Research, Vol. 18(142), pp.\n  1-39. Download link: http://jmlr.org/papers/v18/16-382.html", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robust improper maximum likelihood estimator (RIMLE) is a new method for\nrobust multivariate clustering finding approximately Gaussian clusters. It\nmaximizes a pseudo-likelihood defined by adding a component with improper\nconstant density for accommodating outliers to a Gaussian mixture. A special\ncase of the RIMLE is MLE for multivariate finite Gaussian mixture models. In\nthis paper we treat existence, consistency, and breakdown theory for the RIMLE\ncomprehensively. RIMLE's existence is proved under non-smooth covariance matrix\nconstraints. It is shown that these can be implemented via a computationally\nfeasible Expectation-Conditional Maximization algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 13:51:00 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2013 07:52:06 GMT"}, {"version": "v3", "created": "Tue, 3 Jun 2014 14:35:10 GMT"}, {"version": "v4", "created": "Wed, 4 Jun 2014 16:37:41 GMT"}, {"version": "v5", "created": "Thu, 16 Apr 2015 09:12:17 GMT"}, {"version": "v6", "created": "Wed, 25 Nov 2015 14:44:19 GMT"}, {"version": "v7", "created": "Tue, 26 Jul 2016 13:40:46 GMT"}, {"version": "v8", "created": "Tue, 27 Jun 2017 07:55:58 GMT"}, {"version": "v9", "created": "Tue, 13 Feb 2018 14:23:10 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Coretto", "Pietro", ""], ["Hennig", "Christian", ""]]}, {"id": "1309.6906", "submitter": "Yuefeng Wu", "authors": "Yuefeng Wu, Giles Hooker", "title": "Hellinger Distance and Bayesian Non-Parametrics: Hierarchical Models for\n  Robust and Efficient Bayesian Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a hierarchical framework to incorporate Hellinger\ndistance methods into Bayesian analysis. We propose to modify a prior over\nnon-parametric densities with the exponential of twice the Hellinger distance\nbetween a candidate and a parametric density. By incorporating a prior over the\nparameters of the second density, we arrive at a hierarchical model in which a\nnon-parametric model is placed between parameters and the data. The parameters\nof the family can then be estimated as hyperparameters in the model. In\nfrequentist estimation, minimizing the Hellinger distance between a kernel\ndensity estimate and a parametric family has been shown to produce estimators\nthat are both robust to outliers and statistically efficient when the\nparametric model is correct. In this paper, we demonstrate that the same\nresults are applicable when a non-parametric Bayes density estimate replaces\nthe kernel density estimate. We then demonstrate that robustness and efficiency\nalso hold for the proposed hierarchical model. The finite-sample behavior of\nthe resulting estimates is investigated by simulation and on real world data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 14:11:31 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Wu", "Yuefeng", ""], ["Hooker", "Giles", ""]]}, {"id": "1309.7376", "submitter": "Hau-tieng Wu", "authors": "Jin-Ting Zhang, Ming-Yen Cheng, Chi-Jen Tseng, Hau-Tieng Wu", "title": "A New Test for One-Way ANOVA with Functional Data and Application to\n  Ischemic Heart Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a new global test, namely the $F_{\\max}$-test, for the\none-way ANOVA problem in functional data analysis. The test statistic is taken\nas the maximum value of the usual pointwise $F$-test statistics over the\ninterval the functional responses are observed. A nonparametric bootstrap\nmethod is employed to approximate the null distribution of the test statistic\nand to obtain an estimated critical value for the test. The asymptotic random\nexpression of the test statistic is derived and the asymptotic power is\nstudied. In particular, under mild conditions, the $F_{\\max}$-test\nasymptotically has the correct level and is root-$n$ consistent in detecting\nlocal alternatives. Via some simulation studies, it is found that in terms of\nboth level accuracy and power, the $F_{\\max}$-test outperforms the Globalized\nPointwise F (GPF) test of \\cite{Zhang_Liang:2013} when the functional data are\nhighly or moderately correlated, and its performance is comparable with the\nlatter otherwise. An application to an ischemic heart real dataset suggests\nthat, after proper manipulation, resting electrocardiogram (ECG) signals can be\nused as an effective tool in clinical ischemic heart screening, without the\nneed of further stress tests as in the current standard procedure.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 21:55:10 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Zhang", "Jin-Ting", ""], ["Cheng", "Ming-Yen", ""], ["Tseng", "Chi-Jen", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "1309.7501", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh, Samit Roy, Sujatro Chaklader", "title": "A Model Explaining Correlation Between Observed Values in Contingency\n  Tables", "comments": "Project done as a part of Comprehensive Statistics course in B. Stat.\n  3rd year (2010) in Indian Statistical Institute, Kolkata", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a model is proposed using Bayesian techniques to account for\nthe high correlation between many observed set of contingency tables. In many\nreal life data this high correlation is encountered. Simulation studies are\nalso given to check the effectiveness of this model.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2013 20:41:35 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Ghosh", "Abhik", ""], ["Roy", "Samit", ""], ["Chaklader", "Sujatro", ""]]}, {"id": "1309.7503", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh, Aritra Chakravorty", "title": "Estimating Copula and Test of Independence based on a generalized\n  framework of all rank-based Statistics in Bivariate Sample", "comments": "Project as a part of Multivariate Statistics Course in M. Stat. 1st\n  year in Indian Statistical Institute, Kolkata", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copulas are mathematical objects that fully capture the dependence structure\namong random variables and hence, offer a great flexibility in building\nmultivariate stochastic models. In statistics, a copula is used as a general\nway of formulating a multivariate distribution in such a way that various\ngeneral types of dependence can be represented. In case of bivariate sample,\nthe notion of estimating copula is closely related to that of testing\nindependence in a bivariate sample, as when the components of the bivariate\nsample are independent the copula becomes simply product of two uniform\ndistributions. So apart from non-parametric estimation of copulas we also\nconsidered it relevant to introduce some non-parametric tests to better\nunderstand the very essence of copula in the explanation of association between\nthe components. In fact we will develop a general multivariate statistics that\ngives rise to a much larger class of non-parametric rank based statistics. This\nclass of statistics can be used in estimation and testing for the association\npresent in the bivariate sample. We choose some representative statistics from\nthat class and compared their power in testing independence using simulation as\nan attempt to choose the best candidate in that class.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2013 21:03:59 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Ghosh", "Abhik", ""], ["Chakravorty", "Aritra", ""]]}, {"id": "1309.7604", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, F. M. Bayer, C. J. Tablada", "title": "Low-complexity 8-point DCT Approximations Based on Integer Functions", "comments": "21 pages, 4 figures, corrected typos", "journal-ref": "Signal Processing 99 (2014) 201-214", "doi": "10.1016/j.sigpro.2013.12.027", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a collection of approximations for the 8-point\ndiscrete cosine transform (DCT) based on integer functions. Approximations\ncould be systematically obtained and several existing approximations were\nidentified as particular cases. Obtained approximations were compared with the\nDCT and assessed in the context of JPEG-like image compression.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2013 15:06:58 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2013 02:11:15 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Tablada", "C. J.", ""]]}, {"id": "1309.7622", "submitter": "Fabio Rapallo", "authors": "Enrico Carlini, Fabio Rapallo", "title": "Toric ideals with linear components: an algebraic interpretation of\n  clustering the cells of a contingency table", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that the agglomeration of rows or columns of a\ncontingency table with a hierarchical clustering algorithm yields statistical\nmodels defined through toric ideals. In particular, starting from the classical\nindependence model, the agglomeration process adds a linear part to the toric\nideal generated by the $2 \\times 2$ minors.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2013 16:40:09 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Carlini", "Enrico", ""], ["Rapallo", "Fabio", ""]]}, {"id": "1309.7733", "submitter": "Madan Kundu", "authors": "Madan Gopal Kundu and Jaroslaw Harezlak", "title": "Regression Trees for Longitudinal Data", "comments": null, "journal-ref": "Biostatistics & Epidemiology, 2019", "doi": "10.1080/24709360.2018.1557797", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While studying response trajectory, often the population of interest may be\ndiverse enough to exist distinct subgroups within it and the longitudinal\nchange in response may not be uniform in these subgroups. That is, the\ntimeslope and/or influence of covariates in longitudinal profile may vary among\nthese different subgroups. For example, Raudenbush (2001) used depression as an\nexample to argue that it is incorrect to assume that all the people in a given\npopulation would be experiencing either increasing or decreasing levels of\ndepression. In such cases, traditional linear mixed effects model (assuming\ncommon parametric form for covariates and time) is not directly applicable for\nthe entire population as a group-averaged trajectory can mask important\nsubgroup differences. Our aim is to identify and characterize longitudinally\nhomogeneous subgroups based on the combination of baseline covariates in the\nmost parsimonious way. This goal can be achieved via constructing regression\ntree for longitudinal data using baseline covariates as partitioning variables.\nWe have proposed LongCART algorithm to construct regression tree for the\nlongitudinal data. In each node, the proposed LongCART algorithm determines the\nneed for further splitting (i.e. whether parameter(s) of longitudinal profile\nis influenced by any baseline attributes) via parameter instability tests and\nthus the decision of further splitting is type-I error controlled. We have\nobtained the asymptotic results for the proposed instability test and examined\nfinite sample behavior of the whole algorithm through simulation studies.\nFinally, we have applied the LongCART algorithm to study the longitudinal\nchanges in choline level among HIV patients.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 06:26:50 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 01:07:40 GMT"}, {"version": "v3", "created": "Sun, 13 Jul 2014 14:58:35 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kundu", "Madan Gopal", ""], ["Harezlak", "Jaroslaw", ""]]}, {"id": "1309.7781", "submitter": "Sonja Hahn", "authors": "Sonja Hahn, Frank Konietschke and Luigi Salmaso", "title": "A comparison of efficient permutation tests for unbalanced ANOVA in two\n  by two designs--and their behavior under heteroscedasticity", "comments": "20 pages, 9 figures, Working Paper of the Department of Management\n  And Enigineering of the University of Padova", "journal-ref": null, "doi": null, "report-no": "14", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare different permutation tests and some parametric counterparts that\nare applicable to unbalanced designs in two by two designs. First the different\napproaches are shortly summarized. Then we investigate the behavior of the\ntests in a simulation study. A special focus is on the behavior of the tests\nunder heteroscedastic variances.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 10:07:24 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Hahn", "Sonja", ""], ["Konietschke", "Frank", ""], ["Salmaso", "Luigi", ""]]}, {"id": "1309.7816", "submitter": "Nancy Reid", "authors": "Nancy Reid", "title": "Aspects of likelihood inference", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJSP03 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 4, 1404-1418", "doi": "10.3150/12-BEJSP03", "report-no": "IMS-BEJ-BEJSP03", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I review the classical theory of likelihood based inference and consider how\nit is being extended and developed for use in complex models and sampling\nschemes.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 12:15:54 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Reid", "Nancy", ""]]}, {"id": "1309.7936", "submitter": "Andrew Wey", "authors": "Andrew Wey, John Connett, Kyle Rudser", "title": "Combining parametric, semi-parametric, and non-parametric survival\n  models with stacked survival models", "comments": "Major revision since last arXiv posting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For estimating conditional survival functions, non-parametric estimators can\nbe preferred to parametric and semi-parametric estimators due to relaxed\nassumptions that enable robust estimation. Yet, even when misspecified,\nparametric and semi-parametric estimators can possess better operating\ncharacteristics in small sample sizes due to smaller variance than\nnon-parametric estimators. Fundamentally, this is a bias-variance tradeoff\nsituation in that the sample size is not large enough to take advantage of the\nlow bias of non-parametric estimation. Stacked survival models estimate an\noptimally weighted combination of models that can span parametric,\nsemi-parametric, and non-parametric models by minimizing prediction error. An\nextensive simulation study demonstrates that stacked survival models\nconsistently perform well across a wide range of scenarios by adaptively\nbalancing the strengths and weaknesses of individual candidate survival models.\nIn addition, stacked survival models perform as good as, or better than, the\nmodel selected through cross-validation. Lastly, stacked survival models are\napplied to a well-known German breast cancer study.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 17:38:02 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2013 01:15:52 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2014 20:46:25 GMT"}, {"version": "v4", "created": "Wed, 21 May 2014 20:44:02 GMT"}, {"version": "v5", "created": "Tue, 14 Oct 2014 21:07:48 GMT"}, {"version": "v6", "created": "Sat, 20 Dec 2014 01:44:01 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Wey", "Andrew", ""], ["Connett", "John", ""], ["Rudser", "Kyle", ""]]}]