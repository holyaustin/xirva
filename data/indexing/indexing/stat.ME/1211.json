[{"id": "1211.0087", "submitter": "Peter Hoff", "authors": "Peter Hoff and Jon Wakefield", "title": "Bayesian sandwich posteriors for pseudo-true parameters", "comments": "This note is part of a discussion of \"Bayesian inference with\n  misspecified models\" by Stephen Walker", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under model misspecification, the MLE generally converges to the pseudo-true\nparameter, the parameter corresponding to the distribution within the model\nthat is closest to the distribution from which the data are sampled. In many\nproblems, the pseudo-true parameter corresponds to a population parameter of\ninterest, and so a misspecified model can provide consistent estimation for\nthis parameter. Furthermore, the well-known sandwich variance formula of\nHuber(1967) provides an asymptotically accurate sampling distribution for the\nMLE, even under model misspecification. However, confidence intervals based on\na sandwich variance estimate may behave poorly for low sample sizes, partly due\nto the use of a plug-in estimate of the variance. From a Bayesian perspective,\nplug-in estimates of nuisance parameters generally underrepresent uncertainty\nin the unknown parameters, and averaging over such parameters is expected to\ngive better performance. With this in mind, we present a Bayesian sandwich\nposterior distribution, whose likelihood is based on the sandwich sampling\ndistribution of the MLE. This Bayesian approach allows for the incorporation of\nprior information about the parameter of interest, averages over uncertainty in\nthe nuisance parameter and is asymptotically robust to model misspecification.\nIn a small simulation study on estimating a regression parameter under\nheteroscedasticity, the addition of accurate prior information and the\naveraging over the nuisance parameter are both seen to improve the accuracy and\ncalibration of confidence intervals for the parameter of interest.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 04:22:03 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Hoff", "Peter", ""], ["Wakefield", "Jon", ""]]}, {"id": "1211.0152", "submitter": "Saralees Nadarajah", "authors": "C. S. Withers, S. Nadarajah", "title": "The chain rule for functionals with applications to functions of moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chain rule for derivatives of a function of a function is extended to a\nfunction of a statistical functional, and applied to obtain approximations to\nthe cumulants, distribution and quantiles of functions of sample moments, and\nso to obtain third order confidence intervals and estimates of reduced bias for\nfunctions of moments. As an example we give the distribution of the\nstandardized skewness for a normal sample to magnitude $O(n^{-2})$, where $n$\nis the sample size.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 11:34:08 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Withers", "C. S.", ""], ["Nadarajah", "S.", ""]]}, {"id": "1211.0174", "submitter": "Aki Vehtari", "authors": "Jaakko Riihim\\\"aki and Aki Vehtari", "title": "Laplace approximation for logistic Gaussian process density estimation\n  and regression", "comments": "The v2 and v3 files are the same, but the v3 metadata now has the\n  correct title and abstract", "journal-ref": "Bayesian analysis, 9(2):425-448, 2014", "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic Gaussian process (LGP) priors provide a flexible alternative for\nmodelling unknown densities. The smoothness properties of the density estimates\ncan be controlled through the prior covariance structure of the LGP, but the\nchallenge is the analytically intractable inference. In this paper, we present\napproximate Bayesian inference for LGP density estimation in a grid using\nLaplace's method to integrate over the non-Gaussian posterior distribution of\nlatent function values and to determine the covariance function parameters with\ntype-II maximum a posteriori (MAP) estimation. We demonstrate that Laplace's\nmethod with MAP is sufficiently fast for practical interactive visualisation of\n1D and 2D densities. Our experiments with simulated and real 1D data sets show\nthat the estimation accuracy is close to a Markov chain Monte Carlo\napproximation and state-of-the-art hierarchical infinite Gaussian mixture\nmodels. We also construct a reduced-rank approximation to speed up the\ncomputations for dense 2D grids, and demonstrate density regression with the\nproposed Laplace approach.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 13:31:17 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2013 14:19:51 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2013 10:57:33 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Riihim\u00e4ki", "Jaakko", ""], ["Vehtari", "Aki", ""]]}, {"id": "1211.0437", "submitter": "Jean-Patrick Baudry", "authors": "Jean-Patrick Baudry, Margarida Cardoso, Gilles Celeux, Maria Jos\\'e\n  Amorim, Ana Sousa Ferreira", "title": "Enhancing the selection of a model-based clustering with external\n  qualitative variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cluster analysis, it can be useful to interpret the partition built from\nthe data in the light of external categorical variables which were not directly\ninvolved to cluster the data. An approach is proposed in the model-based\nclustering context to select a model and a number of clusters which both fit\nthe data well and take advantage of the potential illustrative ability of the\nexternal variables. This approach makes use of the integrated joint likelihood\nof the data and the partitions at hand, namely the model-based partition and\nthe partitions associated to the external variables. It is noteworthy that each\nmixture model is fitted by the maximum likelihood methodology to the data,\nexcluding the external variables which are used to select a relevant mixture\nmodel only. Numerical experiments illustrate the promising behaviour of the\nderived criterion.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 12:36:26 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 13:07:07 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Baudry", "Jean-Patrick", ""], ["Cardoso", "Margarida", ""], ["Celeux", "Gilles", ""], ["Amorim", "Maria Jos\u00e9", ""], ["Ferreira", "Ana Sousa", ""]]}, {"id": "1211.0638", "submitter": "Aris Spanos", "authors": "Aris Spanos", "title": "Why the Decision Theoretic Perspective Misrepresents Frequentist\n  Inference: 'Nuts and Bolts' vs. Learning from Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary objective of this paper is to revisit and make a case for the\nmerits of R.A. Fisher's objections to the decision-theoretic framing of\nfrequentist inference. It is argued that this framing is congruent with the\nBayesian but incongruent with the frequentist inference. It provides the\nBayesian approach with a theory of optimal inference, but it misrepresents the\ntheory of optimal frequentist inference by framing inferences solely in terms\nof the universal quantifier `for all values of theta in the parameter space'.\nThis framing is at odds with the primary objective of model-based frequentist\ninference, which is to learn from data about the true value of theta (unknown\nparameter(s)); the one that gave rise to the particular data. The frequentist\napproach relies on factual (estimation, prediction), as well as hypothetical\n(testing) reasoning whose primary aim is to learn from data about the true\ntheta. The paper calls into question the appropriateness of admissibility and\nreassesses Stein's paradox as it relates to the capacity of frequentist\nestimators to pinpoint the true theta. The paper also compares and contrasts\nloss-based errors with traditional frequentist errors, such as coverage, type I\nand II; the former are attached to {\\theta}, but the latter to the inference\nprocedure itself.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2012 21:39:22 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 16:16:01 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 11:23:58 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Spanos", "Aris", ""]]}, {"id": "1211.0727", "submitter": "Hiroto Sekido", "authors": "Hiroto Sekido", "title": "An algorithm for calculating D-optimal designs for polynomial regression\n  with prior information and its appilications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal designs are required to make efficient statistical experiments.\nD-optimal designs for some models are calculated by using canonical moments. On\nthe other hand, integrable systems are dynamical systems whose solutions can be\nwritten down concretely. In this paper, polynomial regression models with prior\ninformation are discussed. In order to calculate D-optimal designs for these\nmodels, a useful relationship between canonical moments and discrete integrable\nsystems is used. By using canonical moments and discrete integrable systems, an\nalgorithm for calculating D-optimal designs for these models is proposed. Then\nsome examples of applications of the algorithm are introduced.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2012 22:34:36 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Sekido", "Hiroto", ""]]}, {"id": "1211.0882", "submitter": "Roland Langrock", "authors": "Roland Langrock, Ruth King", "title": "Maximum likelihood estimation of mark-recapture-recovery models in the\n  presence of continuous covariates", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS644 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 3, 1709-1732", "doi": "10.1214/13-AOAS644", "report-no": "IMS-AOAS-AOAS644", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider mark-recapture-recovery (MRR) data of animals where the model\nparameters are a function of individual time-varying continuous covariates. For\nsuch covariates, the covariate value is unobserved if the corresponding\nindividual is unobserved, in which case the survival probability cannot be\nevaluated. For continuous-valued covariates, the corresponding likelihood can\nonly be expressed in the form of an integral that is analytically intractable\nand, to date, no maximum likelihood approach that uses all the information in\nthe data has been developed. Assuming a first-order Markov process for the\ncovariate values, we accomplish this task by formulating the MRR setting in a\nstate-space framework and considering an approximate likelihood approach which\nessentially discretizes the range of covariate values, reducing the integral to\na summation. The likelihood can then be efficiently calculated and maximized\nusing standard techniques for hidden Markov models. We initially assess the\napproach using simulated data before applying to real data relating to Soay\nsheep, specifying the survival probability as a function of body mass. Models\nthat have previously been suggested for the corresponding covariate process are\ntypically of the form of diffusive random walks. We consider an alternative\nnondiffusive AR(1)-type model which appears to provide a significantly better\nfit to the Soay sheep data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 14:57:36 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2013 16:42:04 GMT"}, {"version": "v3", "created": "Fri, 29 Nov 2013 07:07:14 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langrock", "Roland", ""], ["King", "Ruth", ""]]}, {"id": "1211.0887", "submitter": "Roland Langrock", "authors": "Roland Langrock, Nils-Bastian Heidenreich and Stefan Sperlich", "title": "Kernel-based semiparametric multinomial logit modelling of political\n  party affiliation", "comments": null, "journal-ref": "Statistical Methods & Applications, 2014, Vol. 23, Issue 3, pages\n  435-449", "doi": "10.1007/s10260-014-0261-z", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional, parametric multinomial logit models are in general not\nsufficient for detecting the complex patterns voter profiles nowadays typically\nexhibit. In this manuscript, we use a semiparametric multinomial logit model to\ngive a detailed analysis of the composition of a subsample of the German\nelectorate in 2006. Germany is a particularly strong case for more flexible\nnonparametric approaches in this context, since due to the reunification and\nthe preceding different political histories the composition of the electorate\nis very complex and nuanced. Our analysis reveals strong interactions of the\ncovariates age and income, and highly nonlinear shapes of the factor impacts\nfor each party's likelihood to be voted. Notably, we develop and provide a\nsmoothed likelihood estimator for semiparametric multinomial logit models,\nwhich can be applied also in other application fields, such as, e.g.,\nmarketing.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 15:15:11 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2013 12:16:39 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Langrock", "Roland", ""], ["Heidenreich", "Nils-Bastian", ""], ["Sperlich", "Stefan", ""]]}, {"id": "1211.1144", "submitter": "Pekka Marttinen", "authors": "Pekka Marttinen, Jussi Gillberg, Aki Havulinna, Jukka Corander, and\n  Samuel Kaski", "title": "Genome-wide association studies with high-dimensional phenotypes", "comments": "33 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional phenotypes hold promise for richer findings in association\nstudies, but testing of several phenotype traits aggravates the grand challenge\nof association studies, that of multiple testing. Several methods have recently\nbeen proposed for testing jointly all traits in a high-dimensional vector of\nphenotypes, with prospect of increased power to detect small effects that would\nbe missed if tested individually. However, the methods have rarely been\ncompared to the extent of enabling assessment of their relative merits and\nsetting up guidelines on which method to use, and how to use it. We compare the\nmethods on simulated data and with a real metabolomics data set comprising 137\nhighly correlated variables and approximately 550,000 SNPs.\n  Applying the methods to genome-wide data with hundreds of thousands of\nmarkers inevitably requires division of the problem into manageable parts\nfacilitating parallel processing, parts corresponding to individual genetic\nvariants, pathways, or genes, for example. Here we utilize a straightforward\nformulation according to which the genome is divided into blocks of nearby\ncorrelated genetic markers, tested jointly for association with the phenotypes.\nThis formulation is computationally feasible, reduces the number of tests, and\nlets the methods take advantage of combining information over several\ncorrelated variables not only on the phenotype side, but also on the genotype\nside.\n  Our experiments show that canonical correlation analysis has higher power\nthan alternative methods, while remaining computationally tractable for routine\nuse in the GWAS setting, provided the number of samples is sufficient compared\nto the numbers of phenotype and genotype variables tested. Sparse canonical\ncorrelation analysis and regression models with latent confounding factors show\npromising performance when the number of samples is small.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 08:46:20 GMT"}, {"version": "v2", "created": "Mon, 13 May 2013 09:46:36 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Marttinen", "Pekka", ""], ["Gillberg", "Jussi", ""], ["Havulinna", "Aki", ""], ["Corander", "Jukka", ""], ["Kaski", "Samuel", ""]]}, {"id": "1211.1171", "submitter": "Antonio Punzo", "authors": "Salvatore Ingrassia and Simona C. Minotti and Antonio Punzo and\n  Giorgio Vittadini", "title": "Generalized Linear Gaussian Cluster-Weighted Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster-Weighted Modeling (CWM) is a flexible mixture approach for modeling\nthe joint probability of data coming from a heterogeneous population as a\nweighted sum of the products of marginal distributions and conditional\ndistributions. In this paper, we introduce a wide family of Cluster Weighted\nmodels in which the conditional distributions are assumed to belong to the\nexponential family with canonical links which will be referred to as\nGeneralized Linear Gaussian Cluster Weighted Models. Moreover, we show that, in\na suitable sense, mixtures of generalized linear models can be considered as\nnested in Generalized Linear Gaussian Cluster Weighted Models. The proposal is\nillustrated through many numerical studies based on both simulated and real\ndata sets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 10:10:22 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2012 16:17:47 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Ingrassia", "Salvatore", ""], ["Minotti", "Simona C.", ""], ["Punzo", "Antonio", ""], ["Vittadini", "Giorgio", ""]]}, {"id": "1211.1183", "submitter": "Antonio Punzo", "authors": "Angelo Mazza and Antonio Punzo and Brian McGuire", "title": "KernSmoothIRT: An R Package for Kernel Smoothing in Item Response Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item response theory (IRT) models are a class of statistical models used to\ndescribe the response behaviors of individuals to a set of items having a\ncertain number of options. They are adopted by researchers in social science,\nparticularly in the analysis of performance or attitudinal data, in psychology,\neducation, medicine, marketing and other fields where the aim is to measure\nlatent constructs. Most IRT analyses use parametric models that rely on\nassumptions that often are not satisfied. In such cases, a nonparametric\napproach might be preferable; nevertheless, there are not many software\napplications allowing to use that. To address this gap, this paper presents the\nR package KernSmoothIRT. It implements kernel smoothing for the estimation of\noption characteristic curves, and adds several plotting and analytical tools to\nevaluate the whole test/questionnaire, the items, and the subjects. In order to\nshow the package's capabilities, two real datasets are used, one employing\nmultiple-choice responses, and the other scaled responses.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 11:26:30 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 09:56:38 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Mazza", "Angelo", ""], ["Punzo", "Antonio", ""], ["McGuire", "Brian", ""]]}, {"id": "1211.1184", "submitter": "Antonio Punzo", "authors": "Angelo Mazza and Antonio Punzo", "title": "DBKGrad: An R Package for Mortality Rates Graduation by Fixed and\n  Adaptive Discrete Beta Kernel Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel smoothing represents a useful approach in the graduation of mortality\nrates. Though there exist several options for performing kernel smoothing in\nstatistical software packages, there have been very few contributions to date\nthat have focused on applications of these techniques in the graduation\ncontext. Also, although it has been shown that the use of a variable or\nadaptive smoothing parameter, based on the further information provided by the\nexposed to the risk of death, provides additional benefits, specific\ncomputational tools for this approach are essentially absent. Furthermore,\nlittle attention has been given to providing methods in available software for\nany kind of subsequent analysis with respect to the graduated mortality rates.\nTo facilitate analyses in the field, the R package DBKGrad is introduced. Among\nthe available kernel approaches, it considers a recent discrete beta kernel\nestimator, in both its fixed and adaptive variants. In this approach, boundary\nbias is automatically reduced and age is pragmatically considered as a discrete\nvariable. The bandwidth, fixed or adaptive, is allowed to be manually given by\nthe user or selected by cross-validation. Pointwise confidence intervals, for\neach considered age, are also provided. An application to mortality rates from\nthe Sicily Region (Italy) for the year 2008 is also presented to exemplify the\nuse of the package.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 11:27:08 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Mazza", "Angelo", ""], ["Punzo", "Antonio", ""]]}, {"id": "1211.1204", "submitter": "Leonie Selk", "authors": "Natalie Neumeyer, Leonie Selk", "title": "A note on nonparametric testing for Gaussian innovations in AR-ARCH\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider autoregressive models with conditional\nautoregressive variance, including the case of homoscedastic AR-models and the\ncase of ARCH models. Our aim is to test the hypothesis of normality for the\ninnovations in a completely nonparametric way, i. e. without imposing\nparametric assumptions on the conditional mean and volatility functions. To\nthis end the Cram\\'er-von Mises test based on the empirical distribution\nfunction of nonparametrically estimated residuals is shown to be asymptotically\ndistribution-free. We demonstrate its good performance for finite sample sizes\nin a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 12:45:45 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Neumeyer", "Natalie", ""], ["Selk", "Leonie", ""]]}, {"id": "1211.1208", "submitter": "Jessi Cisewski", "authors": "Jessi Cisewski, Jan Hannig", "title": "Generalized fiducial inference for normal linear mixed models", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1030 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 2102-2127", "doi": "10.1214/12-AOS1030", "report-no": "IMS-AOS-AOS1030", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While linear mixed modeling methods are foundational concepts introduced in\nany statistical education, adequate general methods for interval estimation\ninvolving models with more than a few variance components are lacking,\nespecially in the unbalanced setting. Generalized fiducial inference provides a\npossible framework that accommodates this absence of methodology. Under the\nfabric of generalized fiducial inference along with sequential Monte Carlo\nmethods, we present an approach for interval estimation for both balanced and\nunbalanced Gaussian linear mixed models. We compare the proposed method to\nclassical and Bayesian results in the literature in a simulation study of\ntwo-fold nested models and two-factor crossed designs with an interaction term.\nThe proposed method is found to be competitive or better when evaluated based\non frequentist criteria of empirical coverage and average length of confidence\nintervals for small sample sizes. A MATLAB implementation of the proposed\nalgorithm is available from the authors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 13:01:48 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Cisewski", "Jessi", ""], ["Hannig", "Jan", ""]]}, {"id": "1211.1212", "submitter": "Leonie Selk", "authors": "Leonie Selk, Natalie Neumeyer", "title": "Testing for a change of the innovation distribution in nonparametric\n  autoregression - the sequential empirical process approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a nonparametric autoregression model under conditional\nheteroscedasticity with the aim to test whether the innovation distribution\nchanges in time. To this end we develop an asymptotic expansion for the\nsequential empirical process of nonparametrically estimated innovations\n(residuals). We suggest a Kolmogorov-Smirnov statistic based on the difference\nof the estimated innovation distributions built from the first ns and the last\nn-ns residuals, respectively. Weak convergence of the underlying stochastic\nprocess to a Gaussian process is proved under the null hypothesis of no change\npoint. The result implies that the test is asymptotically distribution-free.\nConsistency against fixed alternatives is shown. The small sample performances\nof the proposed test is investigated in a simulation study and the test is\napplied to data examples.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 13:16:05 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Selk", "Leonie", ""], ["Neumeyer", "Natalie", ""]]}, {"id": "1211.1310", "submitter": "Vladislav Moltchanov", "authors": "Vladislav Moltchanov", "title": "Dynamic Modeling in Health Research as a framework for developing\n  statistical applications free of misuse of statistics", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel framework for developing statistical applications in\nhealth research, based on dynamic modeling of the investigated processes. We\nformulate the principles of dynamic modeling in health research, which are\ncoherent to those in other fields of research. Dynamic models explicitly\ndescribe causal relations which are to be adequately accounted in statistical\nmethods, making them free of misuse of statistics and statistical fallacy. We\npropose the Dynamic Model of Population Health describing temporal changes in\nhealth indicators, having nature of state variables. The Dynamic Regression\nMethod was developed as statistical method for the identification of the model.\nThis method evaluates cohort trends for state variables at each age and\ncalendar year. The method is illustrated by evaluating cohort trends for the\nBody Mass Index for men, using survey data collected in the years 1982, 1987,\n1992, in North Karelia, Finland.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 16:59:33 GMT"}, {"version": "v2", "created": "Sun, 11 Nov 2012 15:37:44 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2012 19:39:08 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Moltchanov", "Vladislav", ""]]}, {"id": "1211.1323", "submitter": "Claudia Beleites", "authors": "Claudia Beleites and Ute Neugebauer and Thomas Bocklitz and Christoph\n  Krafft and J\\\"urgen Popp", "title": "Sample Size Planning for Classification Models", "comments": "The paper is published in Analytica Chimica Acta (special issue\n  \"CAC2012\"). This is a reformatted version of the accepted manuscript with few\n  typos corrected and links to the official publicaion, including the\n  supplementary material (pages 11 - 16 and supplementary-* files in the\n  source). The slides of the presentation at Clircon (2015-04-22, Exeter, UK)\n  are available as ancillary pdf file", "journal-ref": "Analytica Chimica Acta, 760 (2013) 25 - 33", "doi": "10.1016/j.aca.2012.11.007", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biospectroscopy, suitably annotated and statistically independent samples\n(e. g. patients, batches, etc.) for classifier training and testing are scarce\nand costly. Learning curves show the model performance as function of the\ntraining sample size and can help to determine the sample size needed to train\ngood classifiers. However, building a good model is actually not enough: the\nperformance must also be proven. We discuss learning curves for typical small\nsample size situations with 5 - 25 independent samples per class. Although the\nclassification models achieve acceptable performance, the learning curve can be\ncompletely masked by the random testing uncertainty due to the equally limited\ntest sample size. In consequence, we determine test sample sizes necessary to\nachieve reasonable precision in the validation and find that 75 - 100 samples\nwill usually be needed to test a good but not perfect classifier. Such a data\nset will then allow refined sample size planning on the basis of the achieved\nperformance. We also demonstrate how to calculate necessary sample sizes in\norder to show the superiority of one classifier over another: this often\nrequires hundreds of statistically independent test samples or is even\ntheoretically impossible. We demonstrate our findings with a data set of ca.\n2550 Raman spectra of single cells (five classes: erythrocytes, leukocytes and\nthree tumour cell lines BT-20, MCF-7 and OCI-AML3) as well as by an extensive\nsimulation that allows precise determination of the actual performance of the\nmodels in question.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 17:42:00 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2013 13:28:52 GMT"}, {"version": "v3", "created": "Sun, 3 May 2015 09:09:38 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Beleites", "Claudia", ""], ["Neugebauer", "Ute", ""], ["Bocklitz", "Thomas", ""], ["Krafft", "Christoph", ""], ["Popp", "J\u00fcrgen", ""]]}, {"id": "1211.1337", "submitter": "Ana Arribas-Gil", "authors": "Ana Arribas-Gil and Hans-Georg M\\\"uller", "title": "Pairwise Dynamic Time Warping for Event Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new version of dynamic time warping for samples of observed\nevent times that are modeled as time-warped intensity processes. Our approach\nis devel- oped within a framework where for each experimental unit or subject\nin a sample, one observes a random number of event times or random locations.\nAs in our setting the number of observed events differs from subject to\nsubject, usual landmark align- ment methods that require the number of events\nto be the same across subjects are not feasible. We address this challenge by\napplying dynamic time warping, initially by aligning the event times for pairs\nof subjects, regardless of whether the numbers of observed events within the\nconsidered pair of subjects match. The information about pairwise alignments is\nthen combined to extract an overall alignment of the events for each subject\nacross the entire sample. This overall alignment provides a useful description\nof event data and can be used as a pre-processing step for subse- quent\nanalysis. The method is illustrated with a historical fertility study and with\non-line auction data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 18:15:27 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Arribas-Gil", "Ana", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1211.1344", "submitter": "Jacob Bien", "authors": "Jacob Bien, Noah Simon, Robert Tibshirani", "title": "Convex hierarchical testing of interactions", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS758 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 27-42", "doi": "10.1214/14-AOAS758", "report-no": "IMS-AOAS-AOAS758", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the testing of all pairwise interactions in a two-class problem\nwith many features. We devise a hierarchical testing framework that considers\nan interaction only when one or more of its constituent features has a nonzero\nmain effect. The test is based on a convex optimization framework that\nseamlessly considers main effects and interactions together. We show - both in\nsimulation and on a genomic data set from the SAPPHIRe study - a potential gain\nin power and interpretability over a standard (nonhierarchical) interaction\ntest.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 18:31:26 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 13:20:45 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Bien", "Jacob", ""], ["Simon", "Noah", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1211.1456", "submitter": "Cheng Wang", "authors": "Cheng Wang, Tiejun Tong, Longbing Cao and Baiqi Miao", "title": "Non-parametric shrinkage mean estimation for quadratic loss functions\n  with unknown covariance matrices", "comments": "Some technical parts of Theorem 3.1 and 3.2 were corrected in this\n  version", "journal-ref": "Journal of Multivariate Analysis, 125, 222-232, 2014", "doi": "10.1016/j.jmva.2013.12.012", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a shrinkage estimator for the population mean is proposed\nunder known quadratic loss functions with unknown covariance matrices. The new\nestimator is non-parametric in the sense that it does not assume a specific\nparametric distribution for the data and it does not require the prior\ninformation on the population covariance matrix. Analytical results on the\nimprovement of the proposed shrinkage estimator are provided and some\ncorresponding asymptotic properties are also derived. Finally, we demonstrate\nthe practical improvement of the proposed method over existing methods through\nextensive simulation studies and real data analysis. Keywords: High-dimensional\ndata; Shrinkage estimator; Large $p$ small $n$; $U$-statistic.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 05:35:37 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2012 11:36:38 GMT"}, {"version": "v3", "created": "Sat, 18 Jan 2014 06:30:25 GMT"}, {"version": "v4", "created": "Thu, 6 Nov 2014 05:26:47 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Wang", "Cheng", ""], ["Tong", "Tiejun", ""], ["Cao", "Longbing", ""], ["Miao", "Baiqi", ""]]}, {"id": "1211.1530", "submitter": "Ryan Martin", "authors": "Ryan Martin and Chuanhai Liu", "title": "Conditional inferential models: combining information for prior-free\n  probabilistic inference", "comments": "27 pages, 2 tables, 4 figures. Proposition 1 and its corollary are\n  not included in the published version", "journal-ref": "Journal of the Royal Statistical Society, Series B, volume 77,\n  pages 195--217, 2015", "doi": "10.1111/rssb.12070", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inferential model (IM) framework provides valid prior-free probabilistic\ninference by focusing on predicting unobserved auxiliary variables. But,\nefficient IM-based inference can be challenging when the auxiliary variable is\nof higher dimension than the parameter. Here we show that features of the\nauxiliary variable are often fully observed and, in such cases, a simultaneous\ndimension reduction and information aggregation can be achieved by\nconditioning. This proposed conditioning strategy leads to efficient IM\ninference, and casts new light on Fisher's notions of sufficiency,\nconditioning, and also Bayesian inference. A differential equation-driven\nselection of a conditional association is developed, and validity of the\nconditional IM is proved under some conditions. For problems that do not admit\na valid conditional IM of the standard form, we propose a more flexible class\nof conditional IMs based on localization. Examples of local conditional IMs in\na bivariate normal model and a normal variance components model are also given.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 12:53:31 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2013 00:18:18 GMT"}, {"version": "v3", "created": "Sun, 8 Sep 2013 20:01:16 GMT"}, {"version": "v4", "created": "Thu, 26 Dec 2013 13:42:28 GMT"}, {"version": "v5", "created": "Tue, 11 Feb 2014 23:34:52 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Martin", "Ryan", ""], ["Liu", "Chuanhai", ""]]}, {"id": "1211.1547", "submitter": "Ryan Martin", "authors": "Ryan Martin and Chuanhai Liu", "title": "A note on p-values interpreted as plausibilities", "comments": "13 pages, 1 figure", "journal-ref": "Statistica Sinica, volume 24, pages 1703--1716, 2014", "doi": "10.5705/ss.2013.087", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  P-values are a mainstay in statistics but are often misinterpreted. We\npropose a new interpretation of p-value as a meaningful plausibility, where\nthis is to be interpreted formally within the inferential model framework. We\nshow that, for most practical hypothesis testing problems, there exists an\ninferential model such that the corresponding plausibility function, evaluated\nat the null hypothesis, is exactly the p-value. The advantages of this\nrepresentation are that the notion of plausibility is consistent with the way\npractitioners use and interpret p-values, and the plausibility calculation\navoids the troublesome conditioning on the truthfulness of the null. This\nconnection with plausibilities also reveals a shortcoming of standard p-values\nin problems with non-trivial parameter constraints.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 13:44:23 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2013 13:34:12 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2013 20:23:23 GMT"}, {"version": "v4", "created": "Fri, 21 Mar 2014 21:18:35 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Martin", "Ryan", ""], ["Liu", "Chuanhai", ""]]}, {"id": "1211.1592", "submitter": "Ying Hung", "authors": "Ying Hung, V. Roshan Joseph, and Shreyes N. Melkote", "title": "Analysis of Computer Experiments with Functional Response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is motivated by a computer experiment conducted for optimizing\nresidual stresses in the machining of metals. Although kriging is widely used\nin the analysis of computer experiments, it cannot be easily applied to model\nthe residual stresses because they are obtained as a profile. The high\ndimensionality caused by this functional response introduces severe\ncomputational challenges in kriging. It is well known that if the functional\ndata are observed on a regular grid, the computations can be simplified using\nan application of Kronecker products. However, the case of irregular grid is\nquite complex. In this paper, we develop a Gibbs sampling-based expectation\nmaximization algorithm, which converts the irregularly spaced data into a\nregular grid so that the Kronecker product-based approach can be employed for\nefficiently fitting a kriging model to the functional data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 16:36:24 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Hung", "Ying", ""], ["Joseph", "V. Roshan", ""], ["Melkote", "Shreyes N.", ""]]}, {"id": "1211.1642", "submitter": "Stoyan Georgiev", "authors": "Stoyan Georgiev and Sayan Mukherjee", "title": "Randomized Dimension Reduction on Massive Data", "comments": "31 pages, 6 figures, Key Words:dimension reduction, generalized\n  eigendecompositon, low-rank, supervised, inverse regression, random\n  projections, randomized algorithms, Krylov subspace methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalability of statistical estimators is of increasing importance in modern\napplications and dimension reduction is often used to extract relevant\ninformation from data. A variety of popular dimension reduction approaches can\nbe framed as symmetric generalized eigendecomposition problems. In this paper\nwe outline how taking into account the low rank structure assumption implicit\nin these dimension reduction approaches provides both computational and\nstatistical advantages. We adapt recent randomized low-rank approximation\nalgorithms to provide efficient solutions to three dimension reduction methods:\nPrincipal Component Analysis (PCA), Sliced Inverse Regression (SIR), and\nLocalized Sliced Inverse Regression (LSIR). A key observation in this paper is\nthat randomization serves a dual role, improving both computational and\nstatistical performance. This point is highlighted in our experiments on real\nand simulated data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 19:21:48 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 21:17:40 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Georgiev", "Stoyan", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1211.2400", "submitter": "Cheng Wang", "authors": "Cheng Wang, Guangming Pan and Longbing Cao", "title": "A shrinkage estimation for large dimensional precision matrices using\n  random matrix theory", "comments": "This paper has been withdrawn by the author due to substantial\n  contents will be updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new ridge-type shrinkage estimator for the precision matrix\nhas been proposed. The asymptotic optimal shrinkage coefficients and the\ntheoretical loss were derived. Data-driven estimators for the shrinkage\ncoefficients were also conducted based on the asymptotic results deriving from\nrandom matrix theories. The new estimator which has a simple explicit formula\nis distribution-free and applicable to situation where the dimension of\nobservation is greater than the sample size. Further, no assumptions are\nrequired on the structure of the population covariance matrix or the precision\nmatrix. Finally, numerical studies are conducted to examine the performances of\nthe new estimator and existing methods for a wide range of settings.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2012 11:28:27 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2013 07:41:16 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 03:17:09 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wang", "Cheng", ""], ["Pan", "Guangming", ""], ["Cao", "Longbing", ""]]}, {"id": "1211.2481", "submitter": "Natesh Pillai", "authors": "Tirthankar Dasgupta, Natesh S. Pillai, Donald B. Rubin", "title": "Causal inference from $2^k$ factorial designs using the potential\n  outcomes model", "comments": "Preliminary version; comments welcome. Added figures and a table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework for causal inference from two-level factorial designs is\nproposed. The framework utilizes the concept of potential outcomes that lies at\nthe center stage of causal inference and extends Neyman's repeated sampling\napproach for estimation of causal effects and randomization tests based on\nFisher's sharp null hypothesis to the case of 2-level factorial experiments.\nThe framework allows for statistical inference from a finite population,\npermits definition and estimation of estimands other than \"average factorial\neffects\" and leads to more flexible inference procedures than those based on\nordinary least squares estimation from a linear model.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2012 23:49:09 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2012 18:13:03 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["Dasgupta", "Tirthankar", ""], ["Pillai", "Natesh S.", ""], ["Rubin", "Donald B.", ""]]}, {"id": "1211.2635", "submitter": "Silvia Bacci Dr", "authors": "Silvia Bacci, Francesco Bartolucci", "title": "A multidimensional latent class Rasch model for the assessment of the\n  Health-related Quality of Life", "comments": "In press as contributed chapter on Christensen K.B., Kreiner S.,\n  Mesbah M. (eds.), Rasch related models and methods for health science,\n  Wiley-ISTE", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work describes a multidimensional latent class Rasch model and its\napplication to data about the measurement of some aspects of Health-related\nQuality of Life and Anxiety and Depression in oncological patients.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 14:36:04 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Bacci", "Silvia", ""], ["Bartolucci", "Francesco", ""]]}, {"id": "1211.2859", "submitter": "Guenther Walther", "authors": "Camilo Rivera, Guenther Walther", "title": "Optimal detection of a jump in the intensity of a Poisson process or in\n  a density with likelihood ratio statistics", "comments": null, "journal-ref": "Scandinavian Journal of Statistics 40 (2013), 752-769", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting a `bump' in the intensity of a Poisson\nprocess or in a density. We analyze two types of likelihood ratio based\nstatistics which allow for exact finite sample inference and asymptotically\noptimal detection: The maximum of the penalized square root of log likelihood\nratios (`penalized scan') evaluated over a certain sparse set of intervals, and\na certain average of log likelihood ratios (`condensed average likelihood\nratio'). We show that penalizing the {\\sl square root} of the log likelihood\nratio - rather than the log likelihood ratio itself - leads to a simple penalty\nterm that yields optimal power. The thus derived penalty may prove useful for\nother problems that involve a Brownian bridge in the limit. The second key tool\nis an approximating set of intervals that is rich enough to allow for optimal\ndetection but which is also sparse enough to allow justifying the validity of\nthe penalization scheme simply via the union bound. This results in a\nconsiderable simplification in the theoretical treatment compared to the usual\napproach for this type of penalization technique, which requires establishing\nan exponential inequality for the variation of the test statistic. Another\nadvantage of using the sparse approximating set is that it allows fast\ncomputation in nearly linear time.\n  We present a simulation study that illustrates the superior performance of\nthe penalized scan and of the condensed average likelihood ratio compared to\nthe standard scan statistic.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 00:44:37 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2014 19:21:10 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Rivera", "Camilo", ""], ["Walther", "Guenther", ""]]}, {"id": "1211.2958", "submitter": "Juha Karvanen", "authors": "Juha Karvanen", "title": "Study design in causal models", "comments": "The example on the MORGAM Project extended is in this version", "journal-ref": "Scandinavian Journal of Statistics, Volume 42, Issue 2, pages\n  361-377, 2015", "doi": "10.1111/sjos.12110", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal assumptions, the study design and the data are the elements\nrequired for scientific inference in empirical research. The research is\nadequately communicated only if all of these elements and their relations are\ndescribed precisely. Causal models with design describe the study design and\nthe missing data mechanism together with the causal structure and allow the\ndirect application of causal calculus in the estimation of the causal effects.\nThe flow of the study is visualized by ordering the nodes of the causal diagram\nin two dimensions by their causal order and the time of the observation.\nConclusions whether a causal or observational relationship can be estimated\nfrom the collected incomplete data can be made directly from the graph. Causal\nmodels with design offer a systematic and unifying view scientific inference\nand increase the clarity and speed of communication. Examples on the causal\nmodels for a case-control study, a nested case-control study, a clinical trial\nand a two-stage case-cohort study are presented.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 11:33:47 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2013 13:32:40 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2013 11:26:41 GMT"}, {"version": "v4", "created": "Thu, 24 Apr 2014 05:13:52 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Karvanen", "Juha", ""]]}, {"id": "1211.3343", "submitter": "Bityukov Sergey", "authors": "S. I. Bitioukov, N. V. Krasnikov", "title": "The relation between frequentist confidence intervals and Bayesian\n  credible intervals", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the relation between frequentist and Bayesian approaches.\nNamely, we find the \"frequentist\" Bayes prior \\pi_{f}(\\lambda,x_{obs}) =\n-\\frac{\\int_{-\\infty}^{x_{obs}}\\frac{\\partial f(x,\\lambda)}{\\partial\n\\lambda}dx}{f(x_{obs},\\lambda)} (here f(x,\\lambda) is the probability density)\nfor which the results of frequentist and Bayes approaches to the determination\nof confidence intervals coincide. In many cases (but not always) the\n\"frequentist\" prior which reproduces frequentist results coincides with the\nJeffreys prior.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 16:03:49 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Bitioukov", "S. I.", ""], ["Krasnikov", "N. V.", ""]]}, {"id": "1211.3460", "submitter": "Gery Geenens", "authors": "Francis K.C. Hui and Gery Geenens", "title": "A Nonparametric Measure of Local Association for two-way Contingency\n  Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contingency table analysis, the odds ratio is a commonly applied measure\nused to summarize the degree of association between two categorical variables,\nsay R and S. Suppose now that for each individual in the table, a vector of\ncontinuous variables X is also observed. It is then vital to analyze whether\nand how the degree of association varies with X. In this work, we extend the\nclassical odds ratio to the conditional case, and develop nonparametric\nestimators of this \"pointwise odds ratio\" to summarize the strength of local\nassociation between R and S given X. To allow for maximum flexibility, we make\nthis extension using kernel regression. We develop confidence intervals based\non these nonparametric estimators. We demonstrate via simulation that our\npointwise odds ratio estimators can outperform model-based counterparts from\nlogistic regression and GAMs, without the need for a linearity or additivity\nassumption. Finally, we illustrate its application to a dataset of patients\nfrom an intensive care unit (ICU), offering a greater insight into how the\nassociation between survival of patients admitted for emergency versus elective\nreasons varies with the patients' ages.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 23:35:05 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Hui", "Francis K. C.", ""], ["Geenens", "Gery", ""]]}, {"id": "1211.3602", "submitter": "Geoffrey McLachlan", "authors": "Sharon X. Lee, Geoffrey J. McLachlan", "title": "On Mixtures of Skew Normal and Skew t-Distributions", "comments": null, "journal-ref": "Advances in Data Analysis and Classification 2013", "doi": "10.1007/s11634-013-0132-8", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture of skew distributions have emerged as an effective tool in\nmodelling heterogeneous data with asymmetric features. With various proposals\nappearing rapidly in the recent years, which are similar but not identical, the\nconnections between them and their relative performance becomes rather unclear.\nThis paper aims to provide a concise overview of these developments by\npresenting a systematic classification of the existing skew distributions into\nfour types, thereby clarifying their close relationships. This also aids in\nunderstanding the link between some of the proposed expectation-maximization\n(EM) based algorithms for the computation of the maximum likelihood estimates\nof the parameters of the models. The final part of this paper presents an\nillustration of the performance of these mixture models in clustering a real\ndataset, relative to other non-elliptically contoured clustering methods and\nassociated algorithms for their implementation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 13:22:50 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2012 11:28:02 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2013 12:14:36 GMT"}, {"version": "v4", "created": "Thu, 9 May 2013 12:29:04 GMT"}, {"version": "v5", "created": "Tue, 28 May 2013 13:23:28 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Lee", "Sharon X.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1211.3603", "submitter": "Francois Orieux", "authors": "F. Orieux, J.-F. Giovannelli, T. Rodet and A. Abergel", "title": "Estimating hyperparameters and instrument parameters in regularized\n  inversion. Illustration for SPIRE/Herschel map making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.NA stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We describe regularized methods for image reconstruction and focus on the\nquestion of hyperparameter and instrument parameter estimation, i.e.\nunsupervised and myopic problems. We developed a Bayesian framework that is\nbased on the \\post density for all unknown quantities, given the observations.\nThis density is explored by a Markov Chain Monte-Carlo sampling technique based\non a Gibbs loop and including a Metropolis-Hastings step. The numerical\nevaluation relies on the SPIRE instrument of the Herschel observatory. Using\nsimulated and real observations, we show that the hyperparameters and\ninstrument parameters are correctly estimated, which opens up many perspectives\nfor imaging in astrophysics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 13:37:04 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Orieux", "F.", ""], ["Giovannelli", "J. -F.", ""], ["Rodet", "T.", ""], ["Abergel", "A.", ""]]}, {"id": "1211.3671", "submitter": "Hong-Li Zeng", "authors": "Hong-Li Zeng, John Hertz and Yasser Roudi", "title": "L$_1$ Regularization for Reconstruction of a non-equilibrium Ising Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cond-mat.dis-nn physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The couplings in a sparse asymmetric, asynchronous Ising network are\nreconstructed using an exact learning algorithm. L$_1$ regularization is used\nto remove the spurious weak connections that would otherwise be found by simply\nminimizing the minus likelihood of a finite data set. In order to see how L$_1$\nregularization works in detail, we perform the calculation in several ways\nincluding (1) by iterative minimization of a cost function equal to minus the\nlog likelihood of the data plus an L$_1$ penalty term, and (2) an approximate\nscheme based on a quadratic expansion of the cost function around its minimum.\nIn these schemes, we track how connections are pruned as the strength of the\nL$_1$ penalty is increased from zero to large values. The performance of the\nmethods for various coupling strengths is quantified using ROC curves.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 17:34:11 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["Zeng", "Hong-Li", ""], ["Hertz", "John", ""], ["Roudi", "Yasser", ""]]}, {"id": "1211.3760", "submitter": "Georg M. Goerg", "authors": "Georg M. Goerg and Cosma Rohilla Shalizi", "title": "Mixed LICORS: A Nonparametric Algorithm for Predictive State\n  Reconstruction", "comments": "11 pages; AISTATS 2013", "journal-ref": "AISTATS 2013, pp. 289--297", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce 'mixed LICORS', an algorithm for learning nonlinear,\nhigh-dimensional dynamics from spatio-temporal data, suitable for both\nprediction and simulation. Mixed LICORS extends the recent LICORS algorithm\n(Goerg and Shalizi, 2012) from hard clustering of predictive distributions to a\nnon-parametric, EM-like soft clustering. This retains the asymptotic predictive\noptimality of LICORS, but, as we show in simulations, greatly improves\nout-of-sample forecasts with limited data. The new method is implemented in the\npublicly-available R package \"LICORS\"\n(http://cran.r-project.org/web/packages/LICORS/).\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 21:45:15 GMT"}, {"version": "v2", "created": "Fri, 3 May 2013 02:03:00 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Goerg", "Georg M.", ""], ["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1211.3813", "submitter": "Bailey K. Fosdick", "authors": "Bailey K. Fosdick, Peter D. Hoff", "title": "Separable factor analysis with applications to mortality data", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS694 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 120-147", "doi": "10.1214/13-AOAS694", "report-no": "IMS-AOAS-AOAS694", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human mortality data sets can be expressed as multiway data arrays, the\ndimensions of which correspond to categories by which mortality rates are\nreported, such as age, sex, country and year. Regression models for such data\ntypically assume an independent error distribution or an error model that\nallows for dependence along at most one or two dimensions of the data array.\nHowever, failing to account for other dependencies can lead to inefficient\nestimates of regression parameters, inaccurate standard errors and poor\npredictions. An alternative to assuming independent errors is to allow for\ndependence along each dimension of the array using a separable covariance\nmodel. However, the number of parameters in this model increases rapidly with\nthe dimensions of the array and, for many arrays, maximum likelihood estimates\nof the covariance parameters do not exist. In this paper, we propose a submodel\nof the separable covariance model that estimates the covariance matrix for each\ndimension as having factor analytic structure. This model can be viewed as an\nextension of factor analysis to array-valued data, as it uses a factor model to\nestimate the covariance along each dimension of the array. We discuss\nproperties of this model as they relate to ordinary factor analysis, describe\nmaximum likelihood and Bayesian estimation methods, and provide a likelihood\nratio testing procedure for selecting the factor model ranks. We apply this\nmethodology to the analysis of data from the Human Mortality Database, and show\nin a cross-validation experiment how it outperforms simpler methods.\nAdditionally, we use this model to impute mortality rates for countries that\nhave no mortality data for several years. Unlike other approaches, our\nmethodology is able to estimate similarities between the mortality rates of\ncountries, time periods and sexes, and use this information to assist with the\nimputations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 07:40:35 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2014 12:23:57 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Fosdick", "Bailey K.", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1211.3915", "submitter": "Patrick Breheny", "authors": "Yinglei Li and Patrick Breheny", "title": "Kernel-based aggregation of marker-level genetic association tests\n  involving copy-number variation", "comments": null, "journal-ref": "Microarrays, 2: 265-283 (2013)", "doi": "10.3390/microarrays2030265", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic association tests involving copy-number variants (CNVs) are\ncomplicated by the fact that CNVs span multiple markers at which measurements\nare taken. The power of an association test at a single marker is typically\nlow, and it is desirable to pool information across the markers spanned by the\nCNV. However, CNV boundaries are not known in advance, and the best way to\nproceed with this pooling is unclear. In this article, we propose a\nkernel-based method for aggregation of marker-level tests and explore several\naspects of its implementation. In addition, we explore some of the theoretical\naspects of marker-level test aggregation, proposing a permutation-based\napproach that preserves the family-wise error rate of the testing procedure,\nwhile demonstrating that several simpler alternatives fail to do so. The\nempirical power of the approach is studied in a number of simulations\nconstructed from real data involving a pharmacogenomic study of gemcitabine,\nand compares favorably with several competing approaches.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 15:10:43 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2013 17:57:04 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Li", "Yinglei", ""], ["Breheny", "Patrick", ""]]}, {"id": "1211.3946", "submitter": "David  Bolin", "authors": "David Bolin and Finn Lindgren", "title": "Excursion and contour uncertainty regions for latent Gaussian models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An interesting statistical problem is to find regions where some studied\nprocess exceeds a certain level. Estimating such regions so that the\nprobability for exceeding the level in the entire set is equal to some\npredefined value is a difficult problem that occurs in several areas of\napplications ranging from brain imaging to astrophysics. In this work, a method\nfor solving this problem, as well as the related problem of finding uncertainty\nregions for contour curves, for latent Gaussian models is proposed. The method\nis based on using a parametric family for the excursion sets in combination\nwith a sequential importance sampling method for estimating joint\nprobabilities. The accuracy of the method is investigated using simulated data\nand two environmental applications are presented. In the first application,\nareas where the air pollution in the Piemonte region in northern Italy exceeds\nthe daily limit value, set by the European Union for human health protection,\nare estimated. In the second application, regions in the African Sahel that\nexperienced an increase in vegetation after the drought period in the early\n1980s are estimated.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 16:31:27 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["Bolin", "David", ""], ["Lindgren", "Finn", ""]]}, {"id": "1211.3951", "submitter": "Andreas Joseph Mr.", "authors": "Andreas Joseph, Guanrong Chen", "title": "Composite Centrality: A Natural Scale for Complex Evolving Networks", "comments": "11 pages, 5 figures, 4 tables", "journal-ref": "Physica D, vol. 267, p. 58-67, 2014", "doi": "10.1016/j.physd.2013.08.005", "report-no": null, "categories": "stat.ME cs.SI physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a composite centrality measure for general weighted and directed\ncomplex networks, based on measure standardisation and invariant statistical\ninheritance schemes. Different schemes generate different intermediate abstract\nmeasures providing additional information, while the composite centrality\nmeasure tends to the standard normal distribution. This offers a unified scale\nto measure node and edge centralities for complex evolving networks under a\nuniform framework. Considering two real-world cases of the world trade web and\nthe world migration web, both during a time span of 40 years, we propose a\nstandard set-up to demonstrate its remarkable normative power and accuracy. We\nillustrate the applicability of the proposed framework for large and arbitrary\ncomplex systems, as well as its limitations, through extensive numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 16:48:47 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2014 01:41:36 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Joseph", "Andreas", ""], ["Chen", "Guanrong", ""]]}, {"id": "1211.4040", "submitter": "Ali Dastbaravarde", "authors": "Ali Dastbaravarde, Nasser Reza Arghami, Majid Sarmad", "title": "Some Theoretical Results Concerning non-Parametric Estimation by Using a\n  Judgment Post-stratification Sample", "comments": "30 pages, 12 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, some of the properties of non-parametric estimation of the\nexpectation of g(X) (any function of X), by using a Judgment\nPost-stratification Sample (JPS), are discussed. A class of estimators\n(including the standard JPS estimator and a JPS estimator proposed by Frey and\nFeeman (2012, Comput. Stat. Data An.)) is considered. The paper provides mean\nand variance of the members of this class, and examines their consistency and\nasymptotic distribution. Specifically, the results are for the estimation of\npopulation mean, population variance and CDF. We show that any estimators of\nthe class may be less efficient than Simple Random Sampling (SRS) estimator for\nsmall sample sizes. We prove that the relative efficiency of some estimators in\nthe class with respect to Balanced Ranked Set Sampling (BRSS) estimator tends\nto 1 as the sample size goes to infinity. Furthermore, the standard JPS mean\nestimator and, Frey and Feeman JPS mean estimator are specifically studied and\nwe show that two estimator have the same asymptotic distribution. For the\nstandard JPS mean estimator, in perfect ranking situations, optimum values of H\n(the ranking class size), for different sample sizes, are determined\nnon-parametrically for populations that are not heavily skewed or thick tailed.\nWe show that the standard JPS mean estimator may be more efficient than BRSS\nfor large sample sizes, in situations in which we can use a larger class size\nfor H in JPS set-up.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 21:16:28 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2013 11:21:44 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Dastbaravarde", "Ali", ""], ["Arghami", "Nasser Reza", ""], ["Sarmad", "Majid", ""]]}, {"id": "1211.4080", "submitter": "Heng Lian", "authors": "Heng Lian", "title": "Minimax Prediction for Functional Linear Regression with Functional\n  Responses in Reproducing Kernel Hilbert Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider convergence rates in functional linear\nregression with functional responses, where the linear coefficient lies in a\nreproducing kernel Hilbert space (RKHS). Without assuming that the reproducing\nkernel and the covariate covariance kernel are aligned, or assuming polynomial\nrate of decay of the eigenvalues of the covariance kernel, convergence rates in\nprediction risk are established. The corresponding lower bound in rates is\nderived by reducing to the scalar response case. Simulation studies and two\nbenchmark datasets are used to illustrate that the proposed approach can\nsignificantly outperform the functional PCA approach in prediction.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2012 06:41:42 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Lian", "Heng", ""]]}, {"id": "1211.4259", "submitter": "Laurent Jacob", "authors": "Laurent Jacob and Johann Gagnon-Bartsch and Terence P. Speed", "title": "Correcting gene expression data when neither the unwanted variation nor\n  the factor of interest are observed", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with large scale gene expression studies, observations are\ncommonly contaminated by unwanted variation factors such as platforms or\nbatches. Not taking this unwanted variation into account when analyzing the\ndata can lead to spurious associations and to missing important signals. When\nthe analysis is unsupervised, e.g., when the goal is to cluster the samples or\nto build a corrected version of the dataset - as opposed to the study of an\nobserved factor of interest - taking unwanted variation into account can become\na difficult task. The unwanted variation factors may be correlated with the\nunobserved factor of interest, so that correcting for the former can remove the\nlatter if not done carefully. We show how negative control genes and replicate\nsamples can be used to estimate unwanted variation in gene expression, and\ndiscuss how this information can be used to correct the expression data or\nbuild estimators for unsupervised problems. The proposed methods are then\nevaluated on three gene expression datasets. They generally manage to remove\nunwanted variation without losing the signal of interest and compare favorably\nto state of the art corrections.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2012 21:30:46 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Jacob", "Laurent", ""], ["Gagnon-Bartsch", "Johann", ""], ["Speed", "Terence P.", ""]]}, {"id": "1211.4262", "submitter": "Kushal  Dey", "authors": "Kushal Kr. Dey and Kumaresh Dhara and Bikram Karmakar and Sukalyan\n  Sengupta", "title": "Univariate and data-depth based multivariate control charts using\n  trimmed mean and winsorized standard deviation", "comments": "This paper consists of 15 pages, 2 figures, and 5 tables. This paper\n  was presented by Kumaresh Dhara at the 29th Quality Productivity and Research\n  Conference (QPRC) 2012 held at California State University, Long Beach, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, the most popularly used control chart for statistical process\ncontrol has been Shewhart's $\\bar{X}-S$ or $\\bar{X}-R$ chart along with its\nmultivariate generalizations. But, such control charts suffer from the lack of\nrobustness. In this paper, we propose a modified and improved version of\nShewhart chart, based on trimmed mean and winsorized variance that proves\nrobust and more efficient. We have generalized this approach of ours with\nsuitable modifications using depth functions for Multivariate control charts\nand EWMA charts as well. We have discussed the theoretical properties of our\nproposed statistics and have shown the efficiency of our methodology on\nunivariate and multivariate simulated datasets. We have also compared our\napproach to the other popular alternatives to Shewhart Chart already proposed\nand established the efficacy of our methodology.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2012 22:06:48 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Dey", "Kushal Kr.", ""], ["Dhara", "Kumaresh", ""], ["Karmakar", "Bikram", ""], ["Sengupta", "Sukalyan", ""]]}, {"id": "1211.4321", "submitter": "Francois Caron", "authors": "Francois Caron (INRIA Bordeaux - Sud-Ouest, IMB), Yee Whye Teh", "title": "Bayesian nonparametric models for ranked data", "comments": "NIPS - Neural Information Processing Systems (2012)", "journal-ref": null, "doi": null, "report-no": "RR-8140", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian nonparametric extension of the popular Plackett-Luce\nchoice model that can handle an infinite number of choice items. Our framework\nis based on the theory of random atomic measures, with the prior specified by a\ngamma process. We derive a posterior characterization and a simple and\neffective Gibbs sampler for posterior simulation. We develop a time-varying\nextension of our model, and apply it to the New York Times lists of weekly\nbestselling books.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 07:40:51 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Caron", "Francois", "", "INRIA Bordeaux - Sud-Ouest, IMB"], ["Teh", "Yee Whye", ""]]}, {"id": "1211.4416", "submitter": "Jean-David Fermanian", "authors": "Jean-David Fermanian", "title": "An overview of the goodness-of-fit test problem for copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the main \"omnibus procedures\" for goodness-of-fit testing for\ncopulas: tests based on the empirical copula process, on probability integral\ntransformations, on Kendall's dependence function, etc, and some corresponding\nreductions of dimension techniques. The problems of finding asymptotic\ndistribution-free test statistics and the calculation of reliable p-values are\ndiscussed. Some particular cases, like convenient tests for time-dependent\ncopulas, for Archimedean or extreme-value copulas, etc, are dealt with.\nFinally, the practical performances of the proposed approaches are briefly\nsummarized.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 13:51:39 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Fermanian", "Jean-David", ""]]}, {"id": "1211.4483", "submitter": "Nicolas Chopin", "authors": "Nicolas Chopin, Judith Rousseau and Brunero Liseo", "title": "Computational aspects of Bayesian spectral density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian time-series models are often specified through their spectral\ndensity. Such models present several computational challenges, in particular\nbecause of the non-sparse nature of the covariance matrix. We derive a fast\napproximation of the likelihood for such models. We propose to sample from the\napproximate posterior (that is, the prior times the approximate likelihood),\nand then to recover the exact posterior through importance sampling. We show\nthat the variance of the importance sampling weights vanishes as the sample\nsize goes to infinity. We explain why the approximate posterior may typically\nmulti-modal, and we derive a Sequential Monte Carlo sampler based on an\nannealing sequence in order to sample from that target distribution.\nPerformance of the overall approach is evaluated on simulated and real\ndatasets. In addition, for one real world dataset, we provide some numerical\nevidence that a Bayesian approach to semi-parametric estimation of spectral\ndensity may provide more reasonable results than its Frequentist counter-parts.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 16:24:40 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Chopin", "Nicolas", ""], ["Rousseau", "Judith", ""], ["Liseo", "Brunero", ""]]}, {"id": "1211.4675", "submitter": "Yongtao  Guan", "authors": "Yongtao Guan and Matthew Stephens", "title": "Small World MCMC with Tempering: Ergodicity and Spectral Gap", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When sampling a multi-modal distribution $\\pi(x)$, $x\\in \\rr^d$, a Markov\nchain with local proposals is often slowly mixing; while a Small-World sampler\n\\citep{guankrone} -- a Markov chain that uses a mixture of local and long-range\nproposals -- is fast mixing. However, a Small-World sampler suffers from the\ncurse of dimensionality because its spectral gap depends on the volume of each\nmode. We present a new sampler that combines tempering, Small-World sampling,\nand producing long-range proposals from samples in companion chains (e.g.\nEqui-Energy sampler). In its simplest form the sampler employs two Small-World\nchains: an exploring chain and a sampling chain. The exploring chain samples\n$\\pi_t(x) \\propto \\pi(x)^{1/t}$, $t\\in [1,\\infty)$, and builds up an empirical\ndistribution. Using this empirical distribution as its long-range proposal, the\nsampling chain is designed to have a stationary distribution $\\pi(x)$. We prove\nergodicity of the algorithm and study its convergence rate. We show that the\nspectral gap of the exploring chain is enlarged by a factor of $t^{d}$ and that\nof the sampling chain is shrunk by a factor of $t^{-d}$. Importantly, the\nspectral gap of the exploring chain depends on the \"size\" of $\\pi_t(x)$ while\nthat of sampling chain does not. Overall, the sampler enlarges a severe\nbottleneck at the cost of shrinking a mild one, hence achieves faster mixing.\nThe penalty on the spectral gap of the sampling chain can be significantly\nalleviated when extending the algorithm to multiple chains whose temperatures\n$\\{t_k\\}$ follow a geometric progression. If we allow $t_k \\rightarrow 0$, the\nsampler becomes a global optimizer.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 07:29:17 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Guan", "Yongtao", ""], ["Stephens", "Matthew", ""]]}, {"id": "1211.4763", "submitter": "Madan Kundu", "authors": "Madan G. Kundu, Jaroslaw Harezlak and Timothy W. Randolph", "title": "Longitudinal Functional Models with Structured Penalties", "comments": "23 pages, 5 figures", "journal-ref": "Statistical Modelling, 2016", "doi": "10.1177/1471082X15626291", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses estimation in a longitudinal regression model for\nassociation between a scalar outcome and a set of longitudinally-collected\nfunctional covariates or predictor curves. The framework consists of estimating\na time-varying coefficient function that is modeled as a linear combination of\ntime-invariant functions but having time-varying coefficients. The estimation\nprocedure exploits the equivalence between penalized least squares estimation\nand a linear mixed model representation. The process is empirically evaluated\nwith several simulations and it is applied to analyze the neurocognitive\nimpairment of HIV patients and its association with longitudinally-collected\nmagnetic resonance spectroscopy curves.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 14:59:29 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2014 04:23:59 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kundu", "Madan G.", ""], ["Harezlak", "Jaroslaw", ""], ["Randolph", "Timothy W.", ""]]}, {"id": "1211.4801", "submitter": "Vassilios Stathopoulos", "authors": "Vassilios Stathopoulos and Mark A. Girolami", "title": "MCMC inference for Markov Jump Processes via the Linear Noise\n  Approximation", "comments": null, "journal-ref": null, "doi": "10.1098/rsta.2011.0541", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian analysis for Markov jump processes is a non-trivial and challenging\nproblem. Although exact inference is theoretically possible, it is\ncomputationally demanding thus its applicability is limited to a small class of\nproblems. In this paper we describe the application of Riemann manifold MCMC\nmethods using an approximation to the likelihood of the Markov jump process\nwhich is valid when the system modelled is near its thermodynamic limit. The\nproposed approach is both statistically and computationally efficient while the\nconvergence rate and mixing of the chains allows for fast MCMC inference. The\nmethodology is evaluated using numerical simulations on two problems from\nchemical kinetics and one from systems biology.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 16:52:23 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Stathopoulos", "Vassilios", ""], ["Girolami", "Mark A.", ""]]}, {"id": "1211.4889", "submitter": "Greg Ver Steeg", "authors": "Greg Ver Steeg and Aram Galstyan", "title": "Statistical Tests for Contagion in Observational Social Network Studies", "comments": "9 pages, 4 figures. Appearing at AISTATS-13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current tests for contagion in social network studies are vulnerable to the\nconfounding effects of latent homophily (i.e., ties form preferentially between\nindividuals with similar hidden traits). We demonstrate a general method to\nlower bound the strength of causal effects in observational social network\nstudies, even in the presence of arbitrary, unobserved individual traits. Our\ntests require no parametric assumptions and each test is associated with an\nalgebraic proof. We demonstrate the effectiveness of our approach by correctly\ndeducing the causal effects for examples previously shown to expose defects in\nexisting methodology. Finally, we discuss preliminary results on data taken\nfrom the Framingham Heart Study.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 21:53:19 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2013 19:31:16 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1211.5037", "submitter": "Fran\\c{c}ois Caron", "authors": "Fran\\c{c}ois Caron, Yee Whye Teh, Thomas Brendan Murphy", "title": "Bayesian nonparametric Plackett-Luce models for the analysis of\n  preferences for college degree programmes", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS717 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 2, 1145-1181", "doi": "10.1214/14-AOAS717", "report-no": "IMS-AOAS-AOAS717", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a Bayesian nonparametric model for clustering\npartial ranking data. We start by developing a Bayesian nonparametric extension\nof the popular Plackett-Luce choice model that can handle an infinite number of\nchoice items. Our framework is based on the theory of random atomic measures,\nwith the prior specified by a completely random measure. We characterise the\nposterior distribution given data, and derive a simple and effective Gibbs\nsampler for posterior simulation. We then develop a Dirichlet process mixture\nextension of our model and apply it to investigate the clustering of\npreferences for college degree programmes amongst Irish secondary school\ngraduates. The existence of clusters of applicants who have similar preferences\nfor degree programmes is established and we determine that subject matter and\ngeographical location of the third level institution characterise these\nclusters.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 14:09:56 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2014 19:34:49 GMT"}, {"version": "v3", "created": "Fri, 1 Aug 2014 06:34:00 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Caron", "Fran\u00e7ois", ""], ["Teh", "Yee Whye", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1211.5092", "submitter": "Kyle Vincent Ph. D", "authors": "Kyle Vincent", "title": "A Contribution to the Theory Behind the Capture-Recapture M0 Model: An\n  Improved Estimator", "comments": "These two papers have been merged into arXiv:1401.6849", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of a sufficient statistic based on the data of samples\nthat are selected under the M_0 capture-recapture closed population model\n(Schwarz and Seber, 1999). A Rao-Blackwellized version of the estimator based\non a sufficient statistic is then presented. Though the improvements made on\nthe preliminary capture-recapture estimates are likely to be negligible, this\nbody of work is primarily intended to contribute to the theory around the\ncapture-recapture models. The code for a simulation is provided in the\nappendix.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 17:16:45 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2012 02:36:33 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2014 22:03:51 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 19:29:36 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Vincent", "Kyle", ""]]}, {"id": "1211.5290", "submitter": "Geoffrey McLachlan", "authors": "Sharon X. Lee, Geoffrey J. McLachlan", "title": "EMMIX-uskew: An R Package for Fitting Mixtures of Multivariate Skew\n  t-distributions via the EM Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an algorithm for fitting finite mixtures of unrestricted\nMultivariate Skew t (FM-uMST) distributions. The package EMMIX-uskew implements\na closed-form expectation-maximization (EM) algorithm for computing the maximum\nlikelihood (ML) estimates of the parameters for the (unrestricted) FM-MST model\nin R. EMMIX-uskew also supports visualization of fitted contours in two and\nthree dimensions, and random sample generation from a specified FM-uMST\ndistribution.\n  Finite mixtures of skew t-distributions have proven to be useful in modelling\nheterogeneous data with asymmetric and heavy tail behaviour, for example,\ndatasets from flow cytometry. In recent years, various versions of mixtures\nwith multivariate skew t (MST) distributions have been proposed. However, these\nmodels adopted some restricted characterizations of the component MST\ndistributions so that the E-step of the EM algorithm can be evaluated in closed\nform. This paper focuses on mixtures with unrestricted MST components, and\ndescribes an iterative algorithm for the computation of the ML estimates of its\nmodel parameters.\n  The usefulness of the proposed algorithm is demonstrated in three\napplications to real data sets. The first example illustrates the use of the\nmain function fmmst in the package by fitting a MST distribution to a bivariate\nunimodal flow cytometric sample. The second example fits a mixture of MST\ndistributions to the Australian Institute of Sport (AIS) data, and demonstrate\nthat EMMIX-uskew can provide better clustering results than mixtures with\nrestricted MST components. In the third example, EMMIX-uskew is applied to\nclassify cells in a trivariate flow cytometric dataset. Comparisons with other\navailable methods suggests that the EMMIX-uskew result achieved a lower\nmisclassification rate with respect to the labels given by benchmark gating\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2012 13:54:45 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2013 03:11:34 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Lee", "Sharon X.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1211.5620", "submitter": "Alexander Bauer", "authors": "Alexander Bauer, Claudia Czado", "title": "Pair-copula Bayesian networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pair-copula Bayesian networks (PCBNs) are a novel class of multivariate\nstatistical models, which combine the distributional flexibility of pair-copula\nconstructions (PCCs) with the parsimony of conditional independence models\nassociated with directed acyclic graphs (DAG). We are first to provide generic\nalgorithms for random sampling and likelihood inference in arbitrary PCBNs as\nwell as for selecting orderings of the parents of the vertices in the\nunderlying graphs. Model selection of the DAG is facilitated using a version of\nthe well-known PC algorithm which is based on a novel test for conditional\nindependence of random variables tailored to the PCC framework. A simulation\nstudy shows the PC algorithm's high aptitude for structure estimation in\nnon-Gaussian PCBNs. The proposed methods are finally applied to modelling\nfinancial return data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2012 22:28:54 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Bauer", "Alexander", ""], ["Czado", "Claudia", ""]]}, {"id": "1211.5706", "submitter": "Andy  Royle", "authors": "J. Andrew Royle, Sarah J. Converse, William A. Link", "title": "Data Augmentation for Hierarchical Capture-recapture Models", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capture-recapture studies are widely used to obtain information about\nabundance (population size or density) of animal populations. A common design\nis that in which multiple distinct populations are sampled, and the research\nobjective is modeling variation in population size $N_{s}; s=1,2,...,S$ among\nthe populations such as estimating a treatment effect or some other source of\nvariation related to landscape structure. The problem is naturally resolved\nusing hierarchical models. We provide a Bayesian formulation of such models\nusing data augmentation which preserves the individual encounter histories in\nthe model and, as such, is amenable to modeling individual effects. We\nformulate the model by conditioning on the total population size among all\npopulations. In this case, the abundance model can be formulated as a\nmultinomial model that allocates individuals among sites. MCMC is easily\ncarried out by the introduction of a categorical individual effect, $g_{i}$,\nwhich partitions the total population size. The prior distribution for the\nlatent variable $g$ is derived from the model assumed for the population sizes\n$N_{s}$.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2012 21:20:12 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Royle", "J. Andrew", ""], ["Converse", "Sarah J.", ""], ["Link", "William A.", ""]]}, {"id": "1211.5803", "submitter": "Jiashun Jin", "authors": "Jiashun Jin", "title": "Fast community detection by SCORE", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1265 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 1, 57-89", "doi": "10.1214/14-AOS1265", "report-no": "IMS-AOS-AOS1265", "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a network where the nodes split into $K$ different communities. The\ncommunity labels for the nodes are unknown and it is of major interest to\nestimate them (i.e., community detection). Degree Corrected Block Model (DCBM)\nis a popular network model. How to detect communities with the DCBM is an\ninteresting problem, where the main challenge lies in the degree heterogeneity.\nWe propose a new approach to community detection which we call the Spectral\nClustering On Ratios-of-Eigenvectors (SCORE). Compared to classical spectral\nmethods, the main innovation is to use the entry-wise ratios between the first\nleading eigenvector and each of the other leading eigenvectors for clustering.\nLet $A$ be the adjacency matrix of the network. We first obtain the $K$ leading\neigenvectors of $A$, say, $\\hat{\\eta}_1,\\ldots,\\hat{\\eta}_K$, and let $\\hat{R}$\nbe the $n\\times (K-1)$ matrix such that\n$\\hat{R}(i,k)=\\hat{\\eta}_{k+1}(i)/\\hat{\\eta}_1(i)$, $1\\leq i\\leq n$, $1\\leq\nk\\leq K-1$. We then use $\\hat{R}$ for clustering by applying the $k$-means\nmethod. The central surprise is, the effect of degree heterogeneity is largely\nancillary, and can be effectively removed by taking entry-wise ratios between\n$\\hat{\\eta}_{k+1}$ and $\\hat{\\eta}_1$, $1\\leq k\\leq K-1$. The method is\nsuccessfully applied to the web blogs data and the karate club data, with error\nrates of $58/1222$ and $1/34$, respectively. These results are more\nsatisfactory than those by the classical spectral methods. Additionally,\ncompared to modularity methods, SCORE is easier to implement, computationally\nfaster, and also has smaller error rates. We develop a theoretic framework\nwhere we show that under mild conditions, the SCORE stably yields consistent\ncommunity detection. In the core of the analysis is the recent development on\nRandom Matrix Theory (RMT), where the matrix-form Bernstein inequality is\nespecially helpful.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2012 20:32:53 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 14:36:10 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Jin", "Jiashun", ""]]}, {"id": "1211.6451", "submitter": "Paul McNicholas", "authors": "Sakyajit Bhattacharya and Paul D. McNicholas", "title": "A LASSO-Penalized BIC for Mixture Model Selection", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-013-0155-1", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficacy of family-based approaches to mixture model-based clustering and\nclassification depends on the selection of parsimonious models. Current wisdom\nsuggests the Bayesian information criterion (BIC) for mixture model selection.\nHowever, the BIC has well-known limitations, including a tendency to\noverestimate the number of components as well as a proclivity for, often\ndrastically, underestimating the number of components in higher dimensions.\nWhile the former problem might be soluble through merging components, the\nlatter is impossible to mitigate in clustering and classification applications.\nIn this paper, a LASSO-penalized BIC (LPBIC) is introduced to overcome this\nproblem. This approach is illustrated based on applications of extensions of\nmixtures of factor analyzers, where the LPBIC is used to select both the number\nof components and the number of latent factors. The LPBIC is shown to match or\noutperform the BIC in several situations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2012 21:11:41 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Bhattacharya", "Sakyajit", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1211.6537", "submitter": "Patrick J. Wolfe", "authors": "Sofia C. Olhede, Patrick J. Wolfe", "title": "Degree-based network models", "comments": "31 pages, 3 figures, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI math.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the sampling properties of random networks based on weights whose\npairwise products parameterize independent Bernoulli trials. This enables an\nunderstanding of many degree-based network models, in which the structure of\nrealized networks is governed by properties of their degree sequences. We\nprovide exact results and large-sample approximations for power-law networks\nand other more general forms. This enables us to quantify sampling variability\nboth within and across network populations, and to characterize the limiting\nextremes of variation achievable through such models. Our results highlight\nthat variation explained through expected degree structure need not be\nattributed to more complicated generative mechanisms.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 08:23:18 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2013 10:24:00 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Olhede", "Sofia C.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "1211.6688", "submitter": "Jaroslav Hlinka", "authors": "Jaroslav Hlinka, David Hartman, Martin Vejmelka, Dagmar Novotn\\'a,\n  Milan Palu\\v{s}", "title": "Non-linear dependence and teleconnections in climate data: sources,\n  relevance, nonstationarity", "comments": null, "journal-ref": null, "doi": "10.1007/s00382-013-1780-2", "report-no": null, "categories": "stat.ME physics.ao-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of relations between measured variables of interest by\nstatistical measures of dependence is a common step in analysis of climate\ndata. The term \"connectivity\" is used in the network context including the\nstudy of complex coupled dynamical systems. The choice of dependence measure is\nkey for the results of the subsequent analysis and interpretation. The use of\nlinear Pearson's correlation coefficient is widespread and convenient. On the\nother side, as the climate is widely acknowledged to be a nonlinear system,\nnonlinear connectivity quantification methods, such as those based on\ninformation-theoretical concepts, are increasingly used for this purpose.\n  In this paper we outline an approach that enables well informed choice of\nconnectivity method for a given type of data, improving the subsequent\ninterpretation of the results. The presented multi-step approach includes\nstatistical testing, quantification of the specific non-linear contribution to\nthe interaction information, localization of nodes with strongest nonlinear\ncontribution and assessment of the role of specific temporal patterns,\nincluding signal nonstationarities. In detail we study the consequences of the\nchoice of a general nonlinear connectivity measure, namely mutual information,\nfocusing on its relevance and potential alterations in the discovered\ndependence structure.\n  We document the method by applying it on monthly mean temperature data from\nthe NCEP/NCAR reanalysis dataset as well as the ERA dataset. We have been able\nto identify main sources of observed non-linearity in inter-node couplings.\nDetailed analysis suggested an important role of several sources of\nnonstationarity within the climate data. The quantitative role of genuine\nnonlinear coupling at this scale has proven to be almost negligible, providing\nquantitative support for the use of linear methods for this type of data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 18:06:06 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Hlinka", "Jaroslav", ""], ["Hartman", "David", ""], ["Vejmelka", "Martin", ""], ["Novotn\u00e1", "Dagmar", ""], ["Palu\u0161", "Milan", ""]]}, {"id": "1211.6851", "submitter": "Chiheb-Eddine Ben n'cir C.B.N'cir", "authors": "Chiheb-Eddine Ben N'Cir and Nadia Essoussi", "title": "Classification Recouvrante Bas\\'ee sur les M\\'ethodes \\`a Noyau", "comments": "Les 43\\`emes Journ\\'ees de Statistique", "journal-ref": "Les 43\\`emes Journ\\'ees de Statistique 2011", "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overlapping clustering problem is an important learning issue in which\nclusters are not mutually exclusive and each object may belongs simultaneously\nto several clusters. This paper presents a kernel based method that produces\noverlapping clusters on a high feature space using mercer kernel techniques to\nimprove separability of input patterns. The proposed method, called\nOKM-K(Overlapping $k$-means based kernel method), extends OKM (Overlapping\n$k$-means) method to produce overlapping schemes. Experiments are performed on\noverlapping dataset and empirical results obtained with OKM-K outperform\nresults obtained with OKM.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 09:22:19 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["N'Cir", "Chiheb-Eddine Ben", ""], ["Essoussi", "Nadia", ""]]}, {"id": "1211.6859", "submitter": "Chiheb-Eddine Ben n'cir C.B.N'cir", "authors": "Chiheb-Eddine Ben N'Cir and Nadia Essoussi and Patrice Bertrand", "title": "Overlapping clustering based on kernel similarity metric", "comments": "Second Meeting on Statistics and Data Mining 2010", "journal-ref": "Second Meeting on Statistics and Data Mining Second Meeting on\n  Statistics and Data Mining March 11-12, 2010", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Producing overlapping schemes is a major issue in clustering. Recent proposed\noverlapping methods relies on the search of an optimal covering and are based\non different metrics, such as Euclidean distance and I-Divergence, used to\nmeasure closeness between observations. In this paper, we propose the use of\nanother measure for overlapping clustering based on a kernel similarity metric\n.We also estimate the number of overlapped clusters using the Gram matrix.\nExperiments on both Iris and EachMovie datasets show the correctness of the\nestimation of number of clusters and show that measure based on kernel\nsimilarity metric improves the precision, recall and f-measure in overlapping\nclustering.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 09:35:30 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["N'Cir", "Chiheb-Eddine Ben", ""], ["Essoussi", "Nadia", ""], ["Bertrand", "Patrice", ""]]}, {"id": "1211.7332", "submitter": "Daniel Gervini", "authors": "Daniel Gervini", "title": "Functional robust regression for longitudinal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust regression estimator for longitudinal data, which is\nespecially suited for functional data that has been observed on sparse or\nirregular time grids. We show by simulation that the proposed estimators\npossess good outlier-resistance properties compared with the traditional\nfunctional least-squares estimator. As an example of application, we study the\nrelationship between levels of oxides of nitrogen and ozone in the city of San\nFrancisco.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 18:31:19 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Gervini", "Daniel", ""]]}, {"id": "1211.7336", "submitter": "Daniel Gervini", "authors": "Daniel Gervini", "title": "The functional singular value decomposition for bivariate stochastic\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present some statistical applications of the functional\nsingular value decomposition (FSVD). This tool allows us to decompose the\nsample mean of a bivariate stochastic process into components that are\nfunctions of separate variables. These components are sometimes interpretable\nfunctions that summarize salient features of the data. The FSVD can be used to\nvisually detect outliers, to estimate the mean of a stochastic process or to\nobtain individual smoothers of the sample surfaces. As estimators of the mean,\nwe show by simulation that FSVD estimators are competitive with tensor-product\nsplines in some situations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 18:44:57 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Gervini", "Daniel", ""]]}]