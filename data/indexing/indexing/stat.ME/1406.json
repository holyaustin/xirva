[{"id": "1406.0109", "submitter": "Leandro Pardo", "authors": "\\'Angel Felipe, Pedro Miranda and Leandro Pardo", "title": "Minimum {\\phi}-divergence estimation in constrained latent class models\n  for binary data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this paper is to introduce and study the behavior of\nminimum {\\phi}-divergence estimators as an alternative to the maximum\nlikelihood estimator in latent class models for binary items. As it will become\nclear below, minimum {\\phi}-divergence estimators are a natural extension of\nthe maximum likelihood estimator. The asymptotic properties of minimum\n{\\phi}-divergence estimators for latent class models for binary data are\ndeveloped. Finally, to compare the efficiency and robustness of these new\nestimators with those obtained through maximum likelihood when the sample size\nis not big enough to apply the asymptotic results, we have carried out a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Sat, 31 May 2014 20:28:24 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Felipe", "\u00c1ngel", ""], ["Miranda", "Pedro", ""], ["Pardo", "Leandro", ""]]}, {"id": "1406.0177", "submitter": "James Scott", "authors": "Nicholas G. Polson and James G. Scott", "title": "Mixtures, envelopes, and hierarchical duality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a connection between mixture and envelope representations of\nobjective functions that arise frequently in statistics. We refer to this\nconnection using the term \"hierarchical duality.\" Our results suggest an\ninteresting and previously under-exploited relationship between marginalization\nand profiling, or equivalently between the Fenchel--Moreau theorem for convex\nfunctions and the Bernstein--Widder theorem for Laplace transforms. We give\nseveral different sets of conditions under which such a duality result obtains.\nWe then extend existing work on envelope representations in several ways,\nincluding novel generalizations to variance-mean models and to multivariate\nGaussian location models. This turns out to provide an elegant missing-data\ninterpretation of the proximal gradient method, a widely used algorithm in\nmachine learning. We show several statistical applications in which the\nproposed framework leads to easily implemented algorithms, including a robust\nversion of the fused lasso, nonlinear quantile regression via trend filtering,\nand the binomial fused double Pareto model. Code for the examples is available\non GitHub at https://github.com/jgscott/hierduals.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 16:17:21 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 23:20:52 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""]]}, {"id": "1406.0182", "submitter": "Javier Contreras-Reyes JCR", "authors": "Reinaldo B. Arellano-Valle and Javier E. Contreras-Reyes", "title": "Discriminant functions arising from selection distributions: theory and\n  simulation", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption of normality in data has been considered in the field of\nstatistical analysis for a long time. However, in many practical situations,\nthis assumption is clearly unrealistic. It has recently been suggested that the\nuse of distributions indexed by skewness/shape parameters produce more\nexibility in the modelling of different applications. Consequently, the results\nshow a more realistic interpretation for these problems. For these reasons, the\naim of this paper is to investigate the effects of the generalisation of a\ndiscrimination function method through the class of multivariate extended\nskew-elliptical distributions, study in detail the multivariate extended\nskew-normal case and develop a quadratic approximation function for this family\nof distributions. A simulation study is reported to evaluate the adequacy of\nthe proposed classification rule as well as the performance of the EM algorithm\nto estimate the model parameters.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 17:32:08 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Arellano-Valle", "Reinaldo B.", ""], ["Contreras-Reyes", "Javier E.", ""]]}, {"id": "1406.0423", "submitter": "Iv\\'an D\\'iaz", "authors": "Iv\\'an D\\'iaz and Michael Rosenblum", "title": "Targeted Maximum Likelihood Estimation using Exponential Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeted maximum likelihood estimation (TMLE) is a general method for\nestimating parameters in semiparametric and nonparametric models. Each\niteration of TMLE involves fitting a parametric submodel that targets the\nparameter of interest. We investigate the use of exponential families to define\nthe parametric submodel. This implementation of TMLE gives a general approach\nfor estimating any smooth parameter in the nonparametric model. A computational\nadvantage of this approach is that each iteration of TMLE involves estimation\nof a parameter in an exponential family, which is a convex optimization problem\nfor which software implementing reliable and computationally efficient methods\nexists. We illustrate the method in three estimation problems, involving the\nmean of an outcome missing at random, the parameter of a median regression\nmodel, and the causal effect of a continuous exposure, respectively. We conduct\na simulation study comparing different choices for the parametric submodel,\nfocusing on the first of these problems. To the best of our knowledge, this is\nthe first study investigating robustness of TMLE to different specifications of\nthe parametric submodel. We find that the choice of submodel can have an\nimportant impact on the behavior of the estimator in finite samples.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 15:56:28 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["D\u00edaz", "Iv\u00e1n", ""], ["Rosenblum", "Michael", ""]]}, {"id": "1406.0498", "submitter": "Sachin Malik", "authors": "Sachin Malik, Rajesh Singh, SB Gupta", "title": "An almost unbiased estimator for population mean using known value of\n  population parameter(s)", "comments": "arXiv admin note: text overlap with arXiv:1405.4182", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have proposed an almost unbiased estimator using known value\nof some population parameter(s) with known population proportion of an\nauxiliary variable. A class of estimators is defined which includes [1], [2]\nand [3] estimators. Under simple random sampling without replacement (SRSWOR)\nscheme the expressions for bias and mean square error (MSE) are derived.\nNumerical illustrations are given in support of the present study. Key words:\nAuxiliary information, proportion, bias, mean square error, unbiased estimator.\n", "versions": [{"version": "v1", "created": "Sat, 31 May 2014 04:33:31 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Malik", "Sachin", ""], ["Singh", "Rajesh", ""], ["Gupta", "SB", ""]]}, {"id": "1406.0581", "submitter": "Raymond K. W. Wong", "authors": "Raymond K. W. Wong, Thomas C. M. Lee, Debashis Paul, Jie Peng, the\n  Alzheimer's Disease Neuroimaging Initiative", "title": "Fiber Direction Estimation, Smoothing and Tracking in Diffusion MRI", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion magnetic resonance imaging is an imaging technology designed to\nprobe anatomical architectures of biological samples in an in vivo and\nnon-invasive manner through measuring water diffusion. The contribution of this\npaper is threefold. First it proposes a new method to identify and estimate\nmultiple diffusion directions within a voxel through a new and identifiable\nparametrization of the widely used multi-tensor model. Unlike many existing\nmethods, this method focuses on the estimation of diffusion directions rather\nthan the diffusion tensors. Second, this paper proposes a novel direction\nsmoothing method which greatly improves direction estimation in regions with\ncrossing fibers. This smoothing method is shown to have excellent theoretical\nand empirical properties. Lastly, this paper develops a fiber tracking\nalgorithm that can handle multiple directions within a voxel. The overall\nmethodology is illustrated with simulated data and a data set collected for the\nstudy of Alzheimer's disease by the Alzheimer's Disease Neuroimaging Initiative\n(ADNI).\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 06:03:48 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 02:43:54 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Wong", "Raymond K. W.", ""], ["Lee", "Thomas C. M.", ""], ["Paul", "Debashis", ""], ["Peng", "Jie", ""], ["Initiative", "the Alzheimer's Disease Neuroimaging", ""]]}, {"id": "1406.0596", "submitter": "Nicolai Meinshausen", "authors": "Nicolai Meinshausen, Peter B\\\"uhlmann", "title": "Maximin effects in inhomogeneous large-scale data", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1325 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 4, 1801-1830", "doi": "10.1214/15-AOS1325", "report-no": "IMS-AOS-AOS1325", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale data are often characterized by some degree of inhomogeneity as\ndata are either recorded in different time regimes or taken from multiple\nsources. We look at regression models and the effect of randomly changing\ncoefficients, where the change is either smoothly in time or some other\ndimension or even without any such structure. Fitting varying-coefficient\nmodels or mixture models can be appropriate solutions but are computationally\nvery demanding and often return more information than necessary. If we just ask\nfor a model estimator that shows good predictive properties for all regimes of\nthe data, then we are aiming for a simple linear model that is reliable for all\npossible subsets of the data. We propose the concept of \"maximin effects\" and a\nsuitable estimator and look at its prediction accuracy from a theoretical point\nof view in a mixture model with known or unknown group structure. Under certain\ncircumstances the estimator can be computed orders of magnitudes faster than\nstandard penalized regression estimators, making computations on large-scale\ndata feasible. Empirical examples complement the novel methodology and theory.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 06:51:07 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 05:24:48 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Meinshausen", "Nicolai", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1406.0706", "submitter": "Tate Twinam", "authors": "Tate Twinam", "title": "Complementarity and Identification", "comments": "46 pages", "journal-ref": "Econom. Theory 33 (2017) 1154-1185", "doi": "10.1017/S0266466616000359", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the identification power of assumptions that formalize\nthe notion of complementarity in the context of a nonparametric bounds analysis\nof treatment response. I extend the literature on partial identification via\nshape restrictions by exploiting cross-dimensional restrictions on treatment\nresponse when treatments are multidimensional; the assumption of\nsupermodularity can strengthen bounds on average treatment effects in studies\nof policy complementarity. This restriction can be combined with a statistical\nindependence assumption to derive improved bounds on treatment effect\ndistributions, aiding in the evaluation of complex randomized controlled\ntrials. Complementarities arising from treatment effect heterogeneity can be\nincorporated through supermodular instrumental variables to strengthen\nidentification in studies with one or multiple treatments. An application\nexamining the long-run impact of zoning on the evolution of urban spatial\nstructure illustrates the value of the proposed identification methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 13:34:47 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 15:36:57 GMT"}, {"version": "v3", "created": "Thu, 4 Sep 2014 18:00:56 GMT"}, {"version": "v4", "created": "Tue, 13 Sep 2016 21:03:08 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Twinam", "Tate", ""]]}, {"id": "1406.0721", "submitter": "Forrest Crawford", "authors": "Forrest W. Crawford", "title": "The graphical structure of respondent-driven sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) is a chain-referral method for sampling\nmembers of a hidden or hard-to-reach population such as sex workers, homeless\npeople, or drug users via their social network. Most methodological work on RDS\nhas focused on inference of population means under the assumption that\nsubjects' network degree determines their probability of being sampled.\nCriticism of existing estimators is usually focused on missing data: the\nunderlying network is only partially observed, so it is difficult to determine\ncorrect sampling probabilities. In this paper, we show that data collected in\nordinary RDS studies contain information about the structure of the\nrespondents' social network. We construct a continuous-time model of RDS\nrecruitment that incorporates the time series of recruitment events, the\npattern of coupon use, and the network degrees of sampled subjects. Together,\nthe observed data and the recruitment model place a well-defined probability\ndistribution on the recruitment-induced subgraph of respondents. We show that\nthis distribution can be interpreted as an exponential random graph model and\ndevelop a computationally efficient method for estimating the hidden graph. We\nvalidate the method using simulated data and apply the technique to an RDS\nstudy of injection drug users in St. Petersburg, Russia.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 14:14:54 GMT"}, {"version": "v2", "created": "Fri, 3 Oct 2014 20:16:53 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2015 14:11:51 GMT"}], "update_date": "2015-08-03", "authors_parsed": [["Crawford", "Forrest W.", ""]]}, {"id": "1406.0764", "submitter": "Ashkan Ertefaie", "authors": "Ashkan Ertefaie", "title": "Constructing Dynamic Treatment Regimes in Infinite-Horizon Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of existing methods for constructing optimal dynamic\ntreatment regimes is limited to cases where investigators are interested in\noptimizing a utility function over a fixed period of time (finite horizon). In\nthis manuscript, we develop an inferential procedure based on temporal\ndifference residuals for optimal dynamic treatment regimes in infinite-horizon\nsettings, where there is no a priori fixed end of follow-up point. The proposed\nmethod can be used to determine the optimal regime in chronic diseases where\npatients are monitored and treated throughout their life. We derive large\nsample results necessary for conducting inference. We also simulate a cohort of\npatients with diabetes to mimic the third wave of the National Health and\nNutrition Examination Survey, and we examine the performance of the proposed\nmethod in controlling the level of hemoglobin A1c. Supplementary materials for\nthis article are available online.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 15:55:33 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 06:25:38 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Ertefaie", "Ashkan", ""]]}, {"id": "1406.0801", "submitter": "Scott Holan", "authors": "Scott H. Holan and Tucker S. McElroy and Guohui Wu", "title": "The Cepstral Model for Multivariate Time Series: The Vector Exponential\n  Model", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector autoregressive (VAR) models have become a staple in the analysis of\nmultivariate time series and are formulated in the time domain as difference\nequations, with an implied covariance structure. In many contexts, it is\ndesirable to work with a stable, or at least stationary, representation. To fit\nsuch models, one must impose restrictions on the coefficient matrices to ensure\nthat certain determinants are nonzero; which, except in special cases, may\nprove burdensome. To circumvent these difficulties, we propose a flexible\nfrequency domain model expressed in terms of the spectral density matrix.\nSpecifically, this paper treats the modeling of covariance stationary\nvector-valued (i.e., multivariate) time series via an extension of the\nexponential model for the spectrum of a scalar time series. We discuss the\nmodeling advantages of the vector exponential model and its computational\nfacets, such as how to obtain Wold coefficients from given cepstral\ncoefficients. Finally, we demonstrate the utility of our approach through\nsimulation as well as two illustrative data examples focusing on multi-step\nahead forecasting and estimation of squared coherence.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 17:56:26 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Holan", "Scott H.", ""], ["McElroy", "Tucker S.", ""], ["Wu", "Guohui", ""]]}, {"id": "1406.0808", "submitter": "Pietro Coretto", "authors": "Pietro Coretto and Christian Hennig", "title": "Robust improper maximum likelihood: tuning, computation, and a\n  comparison with other methods for robust Gaussian clustering", "comments": null, "journal-ref": "Journal of the American Statistical Association 111(516), pp.\n  1648--1659 (2016)", "doi": "10.1080/01621459.2015.1100996", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two main topics of this paper are the introduction of the \"optimally\ntuned improper maximum likelihood estimator\" (OTRIMLE) for robust clustering\nbased on the multivariate Gaussian model for clusters, and a comprehensive\nsimulation study comparing the OTRIMLE to Maximum Likelihood in Gaussian\nmixtures with and without noise component, mixtures of t-distributions, and the\nTCLUST approach for trimmed clustering. The OTRIMLE uses an improper constant\ndensity for modelling outliers and noise. This can be chosen optimally so that\nthe non-noise part of the data looks as close to a Gaussian mixture as\npossible. Some deviation from Gaussianity can be traded in for lowering the\nestimated noise proportion. Covariance matrix constraints and computation of\nthe OTRIMLE are also treated. In the simulation study, all methods are\nconfronted with setups in which their model assumptions are not exactly\nfulfilled, and in order to evaluate the experiments in a standardized way by\nmisclassification rates, a new model-based definition of \"true clusters\" is\nintroduced that deviates from the usual identification of mixture components\nwith clusters. In the study, every method turns out to be superior for one or\nmore setups, but the OTRIMLE achieves the most satisfactory overall\nperformance. The methods are also applied to two real datasets, one without and\none with known \"true\" clusters.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 13:25:25 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 16:48:18 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2015 09:58:15 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2015 17:43:02 GMT"}, {"version": "v5", "created": "Sat, 28 Jan 2017 18:51:12 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Coretto", "Pietro", ""], ["Hennig", "Christian", ""]]}, {"id": "1406.0812", "submitter": "James Barrett", "authors": "James E. Barrett and Anthony C. C. Coolen", "title": "Covariate dimension reduction for survival data via the Gaussian process\n  latent variable model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of high dimensional survival data is challenging, primarily due\nto the problem of overfitting which occurs when spurious relationships are\ninferred from data that subsequently fail to exist in test data. Here we\npropose a novel method of extracting a low dimensional representation of\ncovariates in survival data by combining the popular Gaussian Process Latent\nVariable Model (GPLVM) with a Weibull Proportional Hazards Model (WPHM). The\ncombined model offers a flexible non-linear probabilistic method of detecting\nand extracting any intrinsic low dimensional structure from high dimensional\ndata. By reducing the covariate dimension we aim to diminish the risk of\noverfitting and increase the robustness and accuracy with which we infer\nrelationships between covariates and survival outcomes. In addition, we can\nsimultaneously combine information from multiple data sources by expressing\nmultiple datasets in terms of the same low dimensional space. We present\nresults from several simulation studies that illustrate a reduction in\noverfitting and an increase in predictive performance, as well as successful\ndetection of intrinsic dimensionality. We provide evidence that it is\nadvantageous to combine dimensionality reduction with survival outcomes rather\nthan performing unsupervised dimensionality reduction on its own. Finally, we\nuse our model to analyse experimental gene expression data and detect and\nextract a low dimensional representation that allows us to distinguish high and\nlow risk groups with superior accuracy compared to doing regression on the\noriginal high dimensional data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 18:39:07 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 15:20:30 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Barrett", "James E.", ""], ["Coolen", "Anthony C. C.", ""]]}, {"id": "1406.0920", "submitter": "Fasheng Sun", "authors": "Fasheng Sun, Min-Qian Liu, Peter Z. G. Qian", "title": "On the construction of nested space-filling designs", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1229 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 4, 1394-1425", "doi": "10.1214/14-AOS1229", "report-no": "IMS-AOS-AOS1229", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested space-filling designs are nested designs with attractive\nlow-dimensional stratification. Such designs are gaining popularity in\nstatistics, applied mathematics and engineering. Their applications include\nmulti-fidelity computer models, stochastic optimization problems, multi-level\nfitting of nonparametric functions, and linking parameters. We propose methods\nfor constructing several new classes of nested space-filling designs. These\nmethods are based on a new group projection and other algebraic techniques. The\nconstructed designs can accommodate a nested structure with an arbitrary number\nof layers and are more flexible in run size than the existing families of\nnested space-filling designs. As a byproduct, the proposed methods can also be\nused to obtain sliced space-filling designs that are appealing for conducting\ncomputer experiments with both qualitative and quantitative factors.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 01:57:14 GMT"}, {"version": "v2", "created": "Tue, 1 Jul 2014 13:09:13 GMT"}, {"version": "v3", "created": "Thu, 28 Aug 2014 07:40:28 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Sun", "Fasheng", ""], ["Liu", "Min-Qian", ""], ["Qian", "Peter Z. G.", ""]]}, {"id": "1406.1023", "submitter": "Andr\\'e Ferrari", "authors": "I. Smith and A. Ferrari", "title": "Generalizations related to hypothesis testing with the Posterior\n  distribution of the Likelihood Ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Posterior distribution of the Likelihood Ratio (PLR) is proposed by\nDempster in 1974 for significance testing in the simple vs composite hypotheses\ncase. In this hypotheses test case, classical frequentist and Bayesian\nhypotheses tests are irreconcilable, as emphasized by Lindley's paradox, Berger\n& Selke in 1987 and many others. However, Dempster shows that the PLR (with\ninner threshold 1) is equal to the frequentist p-value in the simple Gaussian\ncase. In 1997, Aitkin extends this result by adding a nuisance parameter and\nshowing its asymptotic validity under more general distributions. Here we\nextend the reconciliation between the PLR and a frequentist p-value for a\nfinite sample, through a framework analogous to the Stein's theorem frame in\nwhich a credible (Bayesian) domain is equal to a confidence (frequentist)\ndomain.\n  This general reconciliation result only concerns simple vs composite\nhypotheses testing. The measures proposed by Aitkin in 2010 and Evans in 1997\nhave interesting properties and extend Dempster's PLR but only by adding a\nnuisance parameter. Here we propose two extensions of the PLR concept to the\ngeneral composite vs composite hypotheses test. The first extension can be\ndefined for improper priors as soon as the posterior is proper. The second\nextension appears from a new Bayesian-type Neyman-Pearson lemma and emphasizes,\nfrom a Bayesian perspective, the role of the LR as a discrepancy variable for\nhypothesis testing.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 12:20:21 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Smith", "I.", ""], ["Ferrari", "A.", ""]]}, {"id": "1406.1197", "submitter": "Diego Garlaschelli", "authors": "Tiziano Squartini, Rossana Mastrandrea, Diego Garlaschelli", "title": "Unbiased sampling of network ensembles", "comments": "MatLab code available at\n  http://www.mathworks.it/matlabcentral/fileexchange/46912-max-sam-package-zip", "journal-ref": "New J. Phys. 17, 023052 (2015)", "doi": "10.1088/1367-2630/17/2/023052", "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling random graphs with given properties is a key step in the analysis of\nnetworks, as random ensembles represent basic null models required to identify\npatterns such as communities and motifs. An important requirement is that the\nsampling process is unbiased and efficient. The main approaches are\nmicrocanonical, i.e. they sample graphs that match the enforced constraints\nexactly. Unfortunately, when applied to strongly heterogeneous networks (like\nmost real-world examples), the majority of these approaches become biased\nand/or time-consuming. Moreover, the algorithms defined in the simplest cases,\nsuch as binary graphs with given degrees, are not easily generalizable to more\ncomplicated ensembles. Here we propose a solution to the problem via the\nintroduction of a \"Maximize and Sample\" (\"Max & Sam\" for short) method to\ncorrectly sample ensembles of networks where the constraints are `soft', i.e.\nrealized as ensemble averages. Our method is based on exact maximum-entropy\ndistributions and is therefore unbiased by construction, even for strongly\nheterogeneous networks. It is also more computationally efficient than most\nmicrocanonical alternatives. Finally, it works for both binary and weighted\nnetworks with a variety of constraints, including combined degree-strength\nsequences and full reciprocity structure, for which no alternative method\nexists. Our canonical approach can in principle be turned into an unbiased\nmicrocanonical one, via a restriction to the relevant subset. Importantly, the\nanalysis of the fluctuations of the constraints suggests that the\nmicrocanonical and canonical versions of all the ensembles considered here are\nnot equivalent. We show various real-world applications and provide a code\nimplementing all our algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 20:05:22 GMT"}, {"version": "v2", "created": "Wed, 11 Jun 2014 19:54:59 GMT"}, {"version": "v3", "created": "Mon, 5 Jan 2015 14:47:04 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Squartini", "Tiziano", ""], ["Mastrandrea", "Rossana", ""], ["Garlaschelli", "Diego", ""]]}, {"id": "1406.1234", "submitter": "Lijiang Chen", "authors": "Chen Lijiang", "title": "A Geometric Method to Obtain the Generation Probability of a Sentence", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"How to generate a sentence\" is the most critical and difficult problem in\nall the natural language processing technologies. In this paper, we present a\nnew approach to explain the generation process of a sentence from the\nperspective of mathematics. Our method is based on the premise that in our\nbrain a sentence is a part of a word network which is formed by many word\nnodes. Experiments show that the probability of the entire sentence can be\nobtained by the probabilities of single words and the probabilities of the\nco-occurrence of word pairs, which indicate that human use the synthesis method\nto generate a sentence.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 23:28:51 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Lijiang", "Chen", ""]]}, {"id": "1406.1245", "submitter": "Paul McNicholas", "authors": "Amay Cheam and Paul D. McNicholas", "title": "Modelling Receiver Operating Characteristic Curves Using Gaussian\n  Mixtures", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2015.04.010", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The receiver operating characteristic curve is widely applied in measuring\nthe performance of diagnostic tests. Many direct and indirect approaches have\nbeen proposed for modelling the ROC curve, and because of its tractability, the\nGaussian distribution has typically been used to model both populations. We\npropose using a Gaussian mixture model, leading to a more flexible approach\nthat better accounts for atypical data. Monte Carlo simulation is used to\ncircumvent the issue of absence of a closed-form. We show that our method\nperforms favourably when compared to the crude binormal curve and to the\nsemi-parametric frequentist binormal ROC using the famous LABROC procedure.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 00:12:23 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Cheam", "Amay", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1406.1332", "submitter": "Paul McNicholas", "authors": "Sakyajit Bhattacharya and Paul D. McNicholas", "title": "An Adaptive LASSO-Penalized BIC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are becoming a popular tool for the clustering and\nclassification of high-dimensional data. In such high dimensional applications,\nmodel selection is problematic. The Bayesian information criterion, which is\npopular in lower dimensional applications, tends to underestimate the true\nnumber of components in high dimensions. We introduce an adaptive\nLASSO-penalized BIC (ALPBIC) to mitigate this problem. This efficacy of the\nALPBIC is illustrated via applications of parsimonious mixtures of factor\nanalyzers. The selection of the best model by ALPBIC is shown to be consistent\nwith increasing numbers of observations based on simulated and real data\nanalyses.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 10:47:16 GMT"}], "update_date": "2014-06-06", "authors_parsed": [["Bhattacharya", "Sakyajit", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1406.1381", "submitter": "Giovanni Merola", "authors": "Giovanni Maria Merola", "title": "Sparse Principal Component Analysis: a Least Squares approximation\n  approach", "comments": "25 pages, with appendix. Submitted to Australian & New Zealand\n  Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Principal Components Analysis aims to find principal components with\nfew non-zero loadings. We derive such sparse solutions by adding a genuine\nsparsity requirement to the original Principal Components Analysis (PCA)\nobjective function. This approach differs from others because it preserves\nPCA's original optimality: \\uns\\ of the components and Least Squares\napproximation of the data. To identify the best subset of non-zero loadings we\npropose a Branch-and-Bound search and an iterative elimination algorithm. This\nlast algorithm finds sparse solutions with large loadings and can be run\nwithout specifying the cardinality of the loadings and the number of components\nto compute in advance. We give thorough comparisons with the existing Sparse\nPCA methods and several examples on real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jun 2014 13:37:12 GMT"}, {"version": "v2", "created": "Sun, 17 Aug 2014 11:05:25 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Merola", "Giovanni Maria", ""]]}, {"id": "1406.1780", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Christopher R. Genovese, Larry Wasserman", "title": "A Comprehensive Approach to Mode Clustering", "comments": "34 pages, 17 figures. Accepted to the Electronic Journal of\n  Statistics. The original title is \"Enhanced Mode Clustering\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mode clustering is a nonparametric method for clustering that defines\nclusters using the basins of attraction of a density estimator's modes. We\nprovide several enhancements to mode clustering: (i) a soft variant of cluster\nassignment, (ii) a measure of connectivity between clusters, (iii) a technique\nfor choosing the bandwidth, (iv) a method for denoising small clusters, and (v)\nan approach to visualizing the clusters. Combining all these enhancements gives\nus a complete procedure for clustering in multivariate problems. We also\ncompare mode clustering to other clustering methods in several examples\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 19:34:54 GMT"}, {"version": "v2", "created": "Sun, 7 Dec 2014 15:40:02 GMT"}, {"version": "v3", "created": "Sun, 14 Dec 2014 01:06:42 GMT"}, {"version": "v4", "created": "Tue, 22 Dec 2015 15:20:50 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Genovese", "Christopher R.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1406.1803", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Christopher R. Genovese, Larry Wasserman", "title": "Generalized Mode and Ridge Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized density is a product of a density function and a weight\nfunction. For example, the average local brightness of an astronomical image is\nthe probability of finding a galaxy times the mean brightness of the galaxy. We\npropose a method for studying the geometric structure of generalized densities.\nIn particular, we show how to find the modes and ridges of a generalized\ndensity function using a modification of the mean shift algorithm and its\nvariant, subspace constrained mean shift. Our method can be used to perform\nclustering and to calculate a measure of connectivity between clusters. We\nestablish consistency and rates of convergence for our estimator and apply the\nmethods to data from two astronomical problems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 20:08:12 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Genovese", "Christopher R.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1406.1812", "submitter": "Rina Foygel Barber", "authors": "Yuancheng Zhu and Rina Foygel Barber", "title": "The log-shift penalty for adaptive estimation of multiple Gaussian\n  graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Gaussian graphical models characterize sparse dependence relationships\nbetween random variables in a network. To estimate multiple related Gaussian\ngraphical models on the same set of variables, we formulate a hierarchical\nmodel, which leads to an optimization problem with a nonconvex log-shift\npenalty function. We show that under mild conditions the optimization problem\nis convex despite the inclusion of a nonconvex penalty, and derive an efficient\noptimization algorithm. Experiments on both synthetic and real data show that\nthe proposed method is able to achieve good selection and estimation\nperformance simultaneously, because the nonconvexity of the log-shift penalty\nallows for weak signals to be thresholded to zero without excessive shrinkage\non the strong signals.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 20:43:17 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Zhu", "Yuancheng", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "1406.1882", "submitter": "Charalampos Chanialidis", "authors": "Charalampos Chanialidis, Ludger Evers, Tereza Neocleous", "title": "Bayesian density regression for count data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the increasing popularity of quantile regression models for\ncontinuous responses, models for count data have so far received little\nattention. The main quantile regression technique for count data involves\nadding uniform random noise or \"jittering\", thus overcoming the problem that\nthe conditional quantile function is not a continuous function of the\nparameters of interest. Although jittering allows estimating the conditional\nquantiles, it has the drawback that, for small values of the response variable\n$Y,$ the added noise can have a large influence on the estimated quantiles. In\naddition, quantile regression can lead to \"crossing\" quantiles. We propose a\nBayesian Dirichlet process (DP)-based approach to quantile regression for count\ndata. The approach is based on an adaptive DP mixture (DPM) of COM-Poisson\nregression models and determines the quantiles by estimating the density of the\ndata, thus eliminating all the aforementioned problems. Taking advantage of the\nexchange algorithm, the proposed MCMC algorithm can be applied to distributions\non which the likelihood can only be computed up to a normalising constant.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 10:23:17 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Chanialidis", "Charalampos", ""], ["Evers", "Ludger", ""], ["Neocleous", "Tereza", ""]]}, {"id": "1406.1939", "submitter": "Wen Zhou", "authors": "Jinyuan Chang, Chao Zheng, Wen-Xin Zhou, and Wen Zhou", "title": "Simulation-Based Hypothesis Testing of High Dimensional Means Under\n  Covariance Heterogeneity", "comments": "34 pages, 10 figures; Accepted for biometrics", "journal-ref": "Biometrics 2017, Vol. 73, No. 4, 1300-1310", "doi": "10.1111/biom.12695", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of testing the mean vectors of high\ndimensional data in both one-sample and two-sample cases. The proposed testing\nprocedures employ maximum-type statistics and the parametric bootstrap\ntechniques to compute the critical values. Different from the existing tests\nthat heavily rely on the structural conditions on the unknown covariance\nmatrices, the proposed tests allow general covariance structures of the data\nand therefore enjoy wide scope of applicability in practice. To enhance powers\nof the tests against sparse alternatives, we further propose two-step\nprocedures with a preliminary feature screening step. Theoretical properties of\nthe proposed tests are investigated. Through extensive numerical experiments on\nsynthetic datasets and an human acute lymphoblastic leukemia gene expression\ndataset, we illustrate the performance of the new tests and how they may\nprovide assistance on detecting disease-associated gene-sets. The proposed\nmethods have been implemented in an R-package HDtest and are available on CRAN.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 00:48:47 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 03:25:29 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 21:14:33 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Chang", "Jinyuan", ""], ["Zheng", "Chao", ""], ["Zhou", "Wen-Xin", ""], ["Zhou", "Wen", ""]]}, {"id": "1406.1941", "submitter": "Wagner Barreto-Souza", "authors": "Rodrigo B. Silva, Wagner Barreto-Souza", "title": "Beta and Kumaraswamy distributions as non-nested hypotheses in the\n  modeling of continuous bounded data", "comments": "17 pages. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, beta and Kumaraswamy distributions are the most popular models to\nfit continuous bounded data. These models present some characteristics in\ncommon and to select one of them in a practical situation can be of great\ninterest. With this in mind, in this paper we propose a method of selection\nbetween the beta and Kumaraswamy distributions. We use the logarithm of the\nlikelihood ratio statistic (denoted by $T_n$, where $n$ is the sample size) and\nobtain its asymptotic distribution under the hypotheses $H_{\\mathcal B}$ and\n$H_{\\mathcal K}$, where $H_{\\mathcal B}$ ($H_{\\mathcal K}$) denotes that the\ndata come from the beta (Kumaraswamy) distribution. Since both models has the\nsame number of parameters, based on the Akaike criterion, we choose the model\nthat has the greater log-likelihood value. We here propose to use the\nprobability of correct selection (given by $P(T_n>0)$ or $P(T_n<0)$ depending\non the null hypothesis) instead of only to observe the maximized log-likelihood\nvalues. We obtain an approximation for the probability of correct selection\nunder the hypotheses $H_{\\mathcal B}$ and $H_{\\mathcal K}$ and select the model\nthat maximizes it. A simulation study is presented in order to evaluate the\naccuracy of the approximated probabilities of correct selection. We illustrate\nour method of selection in two applications to real data sets involving\nproportions.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 01:43:10 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Silva", "Rodrigo B.", ""], ["Barreto-Souza", "Wagner", ""]]}, {"id": "1406.2083", "submitter": "Aaditya Ramdas", "authors": "Sashank J. Reddi, Aaditya Ramdas, Barnab\\'as P\\'oczos, Aarti Singh and\n  Larry Wasserman", "title": "On the Decreasing Power of Kernel and Distance based Nonparametric\n  Hypothesis Tests in High Dimensions", "comments": "19 pages, 9 figures, published in AAAI-15: The 29th AAAI Conference\n  on Artificial Intelligence (with author order reversed from ArXiv)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about two related decision theoretic problems, nonparametric\ntwo-sample testing and independence testing. There is a belief that two\nrecently proposed solutions, based on kernels and distances between pairs of\npoints, behave well in high-dimensional settings. We identify different sources\nof misconception that give rise to the above belief. Specifically, we\ndifferentiate the hardness of estimation of test statistics from the hardness\nof testing whether these statistics are zero or not, and explicitly discuss a\nnotion of \"fair\" alternative hypotheses for these problems as dimension\nincreases. We then demonstrate that the power of these tests actually drops\npolynomially with increasing dimension against fair alternatives. We end with\nsome theoretical insights and shed light on the \\textit{median heuristic} for\nkernel bandwidth selection. Our work advances the current understanding of the\npower of modern nonparametric hypothesis tests in high dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 05:59:21 GMT"}, {"version": "v2", "created": "Mon, 24 Nov 2014 00:23:35 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Ramdas", "Aaditya", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1406.2098", "submitter": "Jie Peng", "authors": "Ru Wang and Jie Peng", "title": "Learning directed acyclic graphs via bootstrap aggregating", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic graphical models are graphical representations of probability\ndistributions. Graphical models have applications in many fields including\nbiology, social sciences, linguistic, neuroscience. In this paper, we propose\ndirected acyclic graphs (DAGs) learning via bootstrap aggregating. The proposed\nprocedure is named as DAGBag. Specifically, an ensemble of DAGs is first\nlearned based on bootstrap resamples of the data and then an aggregated DAG is\nderived by minimizing the overall distance to the entire ensemble. A family of\nmetrics based on the structural hamming distance is defined for the space of\nDAGs (of a given node set) and is used for aggregation. Under the\nhigh-dimensional-low-sample size setting, the graph learned on one data set\noften has excessive number of false positive edges due to over-fitting of the\nnoise. Aggregation overcomes over-fitting through variance reduction and thus\ngreatly reduces false positives. We also develop an efficient implementation of\nthe hill climbing search algorithm of DAG learning which makes the proposed\nmethod computationally competitive for the high-dimensional regime. The DAGBag\nprocedure is implemented in the R package dagbag.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 07:43:22 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Wang", "Ru", ""], ["Peng", "Jie", ""]]}, {"id": "1406.2100", "submitter": "Mutsuki Kojima", "authors": "Mutsuki Kojima and Fumiyasu Komaki", "title": "Determinantal Point Process Priors for Bayesian Variable Selection in\n  Linear Regression", "comments": null, "journal-ref": "Statistica Sinica 26 (2016), 97-117", "doi": "10.5705/ss.202014.0161", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose discrete determinantal point processes (DPPs) for priors on the\nmodel parameter in Bayesian variable selection. By our variable selection\nmethod, collinear predictors are less likely to be selected simultaneously\nbecause of the repulsion property of discrete DPPs. Three types of DPP priors\nare proposed. We show the efficiency of the proposed priors through numerical\nexperiments and applications to collinear datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 07:45:17 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Kojima", "Mutsuki", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "1406.2148", "submitter": "Ville Satopaa", "authors": "Ville A. Satop\\\"a\\\"a, Robin Pemantle, and Lyle H. Ungar", "title": "Modeling Probability Forecasts via Information Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomness in scientific estimation is generally assumed to arise from\nunmeasured or uncontrolled factors. However, when combining subjective\nprobability estimates, heterogeneity stemming from people's cognitive or\ninformation diversity is often more important than measurement noise. This\npaper presents a novel framework that models the heterogeneity arising from\nexperts that use partially overlapping information sources, and applies that\nmodel to the task of aggregating the probabilities given by a group of experts\nwho forecast whether an event will occur or not. Our model describes the\ndistribution of information across experts in terms of easily interpretable\nparameters and shows how the optimal amount of extremizing of the average\nprobability forecast (shifting it closer to its nearest extreme) varies as a\nfunction of the experts' information overlap. Our model thus gives a more\nprincipled understanding of the historically ad hoc practice of extremizing\naverage forecasts.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 12:07:14 GMT"}, {"version": "v2", "created": "Thu, 6 Nov 2014 00:12:47 GMT"}, {"version": "v3", "created": "Sun, 24 May 2015 20:03:21 GMT"}, {"version": "v4", "created": "Sat, 27 Jun 2015 02:51:14 GMT"}, {"version": "v5", "created": "Thu, 10 Sep 2015 23:02:43 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Satop\u00e4\u00e4", "Ville A.", ""], ["Pemantle", "Robin", ""], ["Ungar", "Lyle H.", ""]]}, {"id": "1406.2462", "submitter": "Christian Brownlees", "authors": "Christian Brownlees, Emilien Joly, G\\'abor Lugosi", "title": "Empirical risk minimization for heavy-tailed losses", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1350 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 6, 2507-2536", "doi": "10.1214/15-AOS1350", "report-no": "IMS-AOS-AOS1350", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to discuss empirical risk minimization when the\nlosses are not necessarily bounded and may have a distribution with heavy\ntails. In such situations, usual empirical averages may fail to provide\nreliable estimates and empirical risk minimization may provide large excess\nrisk. However, some robust mean estimators proposed in the literature may be\nused to replace empirical means. In this paper, we investigate empirical risk\nminimization based on a robust estimate proposed by Catoni. We develop\nperformance bounds based on chaining arguments tailored to Catoni's mean\nestimator.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 08:22:13 GMT"}, {"version": "v2", "created": "Sun, 31 May 2015 23:13:51 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 12:32:11 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Brownlees", "Christian", ""], ["Joly", "Emilien", ""], ["Lugosi", "G\u00e1bor", ""]]}, {"id": "1406.2501", "submitter": "Jhan Rodr\\'iguez", "authors": "Rodr\\'iguez, Jhan and B\\'ardossy, Andr\\'as", "title": "Beyond correlation in spatial statistics modeling", "comments": "75 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model for spatial statistics which can account explicitly for\ninteractions among more than two field components at a time. The theoretical\naspects of the model are dealt with: cumulant and moment generating functions,\nspatial consistency and parameter estimation. On the basis of a detailed\nsynthetic example, we show the kind of inference about the (partially observed)\nspatial field that can be very wrong, if one validates his model by checking\nonly one and two dimensional marginal fit, and covariance function fit. We\nsuggest statistics that can be used additionally for model validation, which\nhelp assess interdependence among groups of variables. The implications of\nconsidering multivariate interactions for intense daily precipitation\nforecasting over a small catchment in southeastern Germany (that of the Saalach\nriver) are investigated.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 10:49:08 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 17:55:59 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Rodr\u00edguez", "", ""], ["Jhan", "", ""], ["B\u00e1rdossy", "", ""], ["Andr\u00e1s", "", ""]]}, {"id": "1406.2660", "submitter": "Christian P. Robert", "authors": "Marco Banterle (CEREMADE, Universite Paris-Dauphine), Clara Grazian\n  (CEREMADE, Universite Paris-Dauphine), Christian P. Robert (CEREMADE,\n  Universite Paris-Dauphine, and University of Warwick)", "title": "Accelerating Metropolis-Hastings algorithms: Delayed acceptance with\n  prefetching", "comments": "20 pages, 12 figures, 2 tables, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MCMC algorithms such as Metropolis-Hastings algorithms are slowed down by the\ncomputation of complex target distributions as exemplified by huge datasets. We\noffer in this paper an approach to reduce the computational costs of such\nalgorithms by a simple and universal divide-and-conquer strategy. The idea\nbehind the generic acceleration is to divide the acceptance step into several\nparts, aiming at a major reduction in computing time that outranks the\ncorresponding reduction in acceptance probability. The division decomposes the\n\"prior x likelihood\" term into a product such that some of its components are\nmuch cheaper to compute than others. Each of the components can be sequentially\ncompared with a uniform variate, the first rejection signalling that the\nproposed value is considered no further, This approach can in turn be\naccelerated as part of a prefetching algorithm taking advantage of the parallel\nabilities of the computer at hand. We illustrate those accelerating features on\na series of toy and realistic examples.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 18:48:32 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Banterle", "Marco", "", "CEREMADE, Universite Paris-Dauphine"], ["Grazian", "Clara", "", "CEREMADE, Universite Paris-Dauphine"], ["Robert", "Christian P.", "", "CEREMADE,\n  Universite Paris-Dauphine, and University of Warwick"]]}, {"id": "1406.2725", "submitter": "Qijun Fang", "authors": "Qijun Fang, Walter W. Piegorsch and Katherine Y. Barnes", "title": "Bayesian Benchmark Dose Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1402.3896", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important objective in environmental risk assessment is estimation of\nminimum exposure levels, called Benchmark Doses (BMDs) that induce a\npre-specified Benchmark Response (BMR) in a target population. Established\ninferential approaches for BMD analysis typically involve one-sided,\nfrequentist confidence limits, leading in practice to what are called Benchmark\nDose Lower Limits (BMDLs). Appeal to Bayesian modeling and credible limits for\nbuilding BMDLs is far less developed, however. Indeed, for the few existing\nforms of Bayesian BMDs, informative prior information is seldom incorporated.\nWe develop reparameterized quantal-response models that explicitly describe the\nBMD as a target parameter. Our goal is to obtain an improved estimation and\ncalculation archetype for the BMD and for the BMDL, by employing quantifiable\nprior belief to represent parameter uncertainty in the statistical model.\nImplementation is facilitated via a Monte Carlo-based adaptive Metropolis (AM)\nalgorithm to approximate the posterior distribution. An example from\nenvironmental carcinogenicity testing illustrates the calculations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 21:17:30 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Fang", "Qijun", ""], ["Piegorsch", "Walter W.", ""], ["Barnes", "Katherine Y.", ""]]}, {"id": "1406.2815", "submitter": "Jhan Rodr\\'iguez", "authors": "Rodr\\'iguez, Jhan and B\\'ardossy, Andr\\'as", "title": "Multivariate interactions modeling through their manifestations: low\n  dimensional model building via the Cumulant Generating Function", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing dimensionality of data calls for beyond-pairwise interactions\nquantification. Measures of multidimensional interactions quantification are\nhindered, among others, by two issues: 1. Interpretation difficulties, 2. the\ncurse of dimensionality. We propose to deal with multidimensional interactions\nby identifying subject-matter specific \"interaction manifestations\" and then\nbuilding a low-dimensional model that reproduces as close as possible such\nmanifestations. We argue that an adequate model building approach is to build\nthe model in the form of a cumulant generating function, i.e. to use joint\ncumulants as building blocks. The whole approach resembles that of probability\ninversion in the area of expert knowledge based risk assessment, where a\ndiscrimination is made between \"elicitation\" variables, familiar to the\nexperts, and \"target\" (or model) variables, consisting of the more abstract\nparameters of a mathematical model. A synthetic example is provided to\nillustrate these ideas.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 08:08:17 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Rodr\u00edguez", "", ""], ["Jhan", "", ""], ["B\u00e1rdossy", "", ""], ["Andr\u00e1s", "", ""]]}, {"id": "1406.2933", "submitter": "Werner M\\\"uller", "authors": "Elisa Perrone and Werner G. M\\\"uller", "title": "Optimal Designs for Copula Models", "comments": "IFAS research report 2014-66", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copula modelling has in the past decade become a standard tool in many areas\nof applied statistics. However, a largely neglected aspect concerns the design\nof related experiments. Particularly the issue of whether the estimation of\ncopula parameters can be enhanced by optimizing experimental conditions and how\nrobust all the parameter estimates for the model are with respect to the type\nof copula employed. In this paper an equivalence theorem for (bivariate) copula\nmodels is provided that allows formulation of efficient design algorithms and\nquick checks of whether designs are optimal or at least efficient. Some\nexamples illustrate that in practical situations considerable gains in design\nefficiency can be achieved. A natural comparison between different copula\nmodels with respect to design efficiency is provided as well.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 15:19:51 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Perrone", "Elisa", ""], ["M\u00fcller", "Werner G.", ""]]}, {"id": "1406.3106", "submitter": "Deepesh Bhati Mr.", "authors": "Deepesh Bhati and Mohd. Aamir Malik", "title": "On Lindley-Exponential Distribution: Properties and Application", "comments": "17 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper, we introduce a new distribution generated by Lindley random\nvariable which offers a more flexible model for modelling lifetime data.\nVarious statistical properties like distribution function, survival function,\nmoments, entropy, and limiting distribution of extreme order statistics are\nestablished. Inference for a random sample from the proposed distribution is\ninvestigated and maximum likelihood estimation method is used for estimating\nparameters of this distribution. The applicability of the proposed distribution\nis shown through real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 02:39:09 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Bhati", "Deepesh", ""], ["Malik", "Mohd. Aamir", ""]]}, {"id": "1406.3258", "submitter": "Nancy Zhang", "authors": "Nancy R. Zhang, Benjamin Yakir, Charlie L. Xia, David Siegmund", "title": "Scanning a Poisson Random Field for Local Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of local genomic signals using high-throughput DNA sequencing\ndata can be cast as a problem of scanning a Poisson random field for local\nchanges in the rate of the process. We propose a likelihood-based framework for\nfor such scans, and derive formulas for false positive rate control and power\ncalculations. The framework can also accommodate mixtures of Poisson processes\nto deal with over-dispersion. As a specific, detailed example, we consider the\ndetection of insertions and deletions by paired-end DNA-sequencing. We propose\nseveral statistics for this problem, compare their power under current\nexperimental designs, and illustrate their application on an Illumina Platinum\nGenomes data set.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 14:53:47 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Zhang", "Nancy R.", ""], ["Yakir", "Benjamin", ""], ["Xia", "Charlie L.", ""], ["Siegmund", "David", ""]]}, {"id": "1406.3521", "submitter": "Ryan Martin", "authors": "Qianshun Cheng, Xu Gao, Ryan Martin", "title": "Exact prior-free probabilistic inference on the heritability coefficient\n  in a linear mixed model", "comments": "15 pages, 1 table, 2 figures", "journal-ref": "Electronic Journal of Statistics, volume 8, pages 3062-3076, 2014", "doi": "10.1214/15-EJS984", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed-effect models with two variance components are often used when\nvariability comes from two sources. In genetics applications, variation in\nobserved traits can be attributed to biological and environmental effects, and\nthe heritability coefficient is a fundamental quantity that measures the\nproportion of total variability due to the biological effect. We propose a new\ninferential model approach which yields exact prior-free probabilistic\ninference on the heritability coefficient. In particular we construct exact\nconfidence intervals and demonstrate numerically our method's efficiency\ncompared to that of existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 12:41:32 GMT"}, {"version": "v2", "created": "Wed, 30 Jul 2014 11:40:36 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Cheng", "Qianshun", ""], ["Gao", "Xu", ""], ["Martin", "Ryan", ""]]}, {"id": "1406.3647", "submitter": "Candace Berrett", "authors": "Candace Berrett and Catherine A. Calder", "title": "Bayesian Spatial Binary Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In analyses of spatially-referenced data, researchers often have one of two\ngoals: to quantify relationships between a response variable and covariates\nwhile accounting for residual spatial dependence or to predict the value of a\nresponse variable at unobserved locations. In this second case, when the\nresponse variable is categorical, prediction can be viewed as a classification\nproblem. Many classification methods either ignore response-variable/covariate\nrelationships and rely only on spatially proximate observations for\nclassification, or they ignore spatial dependence and use only the covariates\nfor classification. The Bayesian spatial generalized linear (mixed) model\noffers a tool to accommodate both spatial and covariate sources of information\nin classification problems. In this paper, we formally define spatial\nclassification rules based on these models. We also take a close look at two of\nthese models that have been proposed in the literature, namely the probit\nversions of the spatial generalized linear model (SGLM) and the Bayesian\nspatial generalized linear mixed model (SGLMM). We describe the implications of\nthe seemingly slight differences between these models for spatial\nclassification and explore the issue of robustness to model misspecification\nthrough a simulation study. We also provide an overview of alternatives to the\nSGLM/SGLMM-based classifiers and illustrate the various methods using\nsatellite-derived land cover data from Southeast Asia.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 21:10:39 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 03:06:15 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2015 01:30:01 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2016 17:35:30 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Berrett", "Candace", ""], ["Calder", "Catherine A.", ""]]}, {"id": "1406.3704", "submitter": "Michio Yamamoto", "authors": "Michio Yamamoto and Kenichi Hayashi", "title": "Model-based clustering of multivariate binary data with dimension\n  reduction", "comments": "arXiv admin note: text overlap with arXiv:1011.3626 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering methods with dimension reduction have been receiving considerable\nwide interest in statistics lately and a lot of methods to simultaneously\nperform clustering and dimension reduction have been proposed. This work\npresents a novel procedure for simultaneously determining the optimal cluster\nstructure for multivariate binary data and the subspace to represent that\ncluster structure. The method is based on a finite mixture model of\nmultivariate Bernoulli distributions, and each component is assumed to have a\nlow-dimensional representation of the cluster structure. This method can be\nconsidered an extension of the traditional latent class analysis model.\nSparsity is introduced to the loading values, which produces the\nlow-dimensional subspace, for enhanced interpretability and more stable\nextraction of the subspace. An EM-based algorithm is developed to efficiently\nsolve the proposed optimization problem. We demonstrate the effectiveness of\nthe proposed method by applying it to a simulation study and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jun 2014 09:10:04 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Yamamoto", "Michio", ""], ["Hayashi", "Kenichi", ""]]}, {"id": "1406.3774", "submitter": "Roland Langrock", "authors": "Roland Langrock and Thomas Kneib and Richard Glennie and Th\\'eo\n  Michelot", "title": "Markov-switching generalized additive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Markov-switching regression models, i.e. models for time series\nregression analyses where the functional relationship between covariates and\nresponse is subject to regime switching controlled by an unobservable Markov\nchain. Building on the powerful hidden Markov model machinery and the methods\nfor penalized B-splines routinely used in regression analyses, we develop a\nframework for nonparametrically estimating the functional form of the effect of\nthe covariates in such a regression model, assuming an additive structure of\nthe predictor. The resulting class of Markov-switching generalized additive\nmodels is immensely flexible, and contains as special cases the common\nparametric Markov-switching regression models and also generalized additive and\ngeneralized linear models. The feasibility of the suggested maximum penalized\nlikelihood approach is demonstrated by simulation and further illustrated by\nmodelling how energy price in Spain depends on the Euro/Dollar exchange rate.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jun 2014 21:24:47 GMT"}, {"version": "v2", "created": "Sun, 10 May 2015 20:29:33 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Langrock", "Roland", ""], ["Kneib", "Thomas", ""], ["Glennie", "Richard", ""], ["Michelot", "Th\u00e9o", ""]]}, {"id": "1406.3836", "submitter": "Jianqing Fan", "authors": "Jianqing Fan, Yuan Liao, Weichen Wang", "title": "Projected principal component analysis in factor models", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1364 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 1, 219-254", "doi": "10.1214/15-AOS1364", "report-no": "IMS-AOS-AOS1364", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a Projected Principal Component Analysis\n(Projected-PCA), which employs principal component analysis to the projected\n(smoothed) data matrix onto a given linear space spanned by covariates. When it\napplies to high-dimensional factor analysis, the projection removes noise\ncomponents. We show that the unobserved latent factors can be more accurately\nestimated than the conventional PCA if the projection is genuine, or more\nprecisely, when the factor loading matrices are related to the projected linear\nspace. When the dimensionality is large, the factors can be estimated\naccurately even when the sample size is finite. We propose a flexible\nsemiparametric factor model, which decomposes the factor loading matrix into\nthe component that can be explained by subject-specific covariates and the\northogonal residual component. The covariates' effects on the factor loadings\nare further modeled by the additive model via sieve approximations. By using\nthe newly proposed Projected-PCA, the rates of convergence of the smooth factor\nloading matrices are obtained, which are much faster than those of the\nconventional factor analysis. The convergence is achieved even when the sample\nsize is finite and is particularly appealing in the\nhigh-dimension-low-sample-size situation. This leads us to developing\nnonparametric tests on whether observed covariates have explaining powers on\nthe loadings and whether they fully explain the loadings. The proposed method\nis illustrated by both simulated data and the returns of the components of the\nS&P 500 index.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 18:24:49 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 17:43:39 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2016 14:03:06 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Fan", "Jianqing", ""], ["Liao", "Yuan", ""], ["Wang", "Weichen", ""]]}, {"id": "1406.3863", "submitter": "Gabriela B. Cybis", "authors": "Gabriela B. Cybis, Janet S. Sinsheimer, Trevor Bedford, Alison E.\n  Mather, Philippe Lemey, Marc A. Suchard", "title": "Assessing phenotypic correlation through the multivariate phylogenetic\n  latent liability model", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS821 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 2, 969-991", "doi": "10.1214/15-AOAS821", "report-no": "IMS-AOAS-AOAS821", "categories": "q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding which phenotypic traits are consistently correlated throughout\nevolution is a highly pertinent problem in modern evolutionary biology. Here,\nwe propose a multivariate phylogenetic latent liability model for assessing the\ncorrelation between multiple types of data, while simultaneously controlling\nfor their unknown shared evolutionary history informed through molecular\nsequences. The latent formulation enables us to consider in a single model\ncombinations of continuous traits, discrete binary traits and discrete traits\nwith multiple ordered and unordered states. Previous approaches have\nentertained a single data type generally along a fixed history, precluding\nestimation of correlation between traits and ignoring uncertainty in the\nhistory. We implement our model in a Bayesian phylogenetic framework, and\ndiscuss inference techniques for hypothesis testing. Finally, we showcase the\nmethod through applications to columbine flower morphology, antibiotic\nresistance in Salmonella and epitope evolution in influenza.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 21:38:01 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 10:27:12 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Cybis", "Gabriela B.", ""], ["Sinsheimer", "Janet S.", ""], ["Bedford", "Trevor", ""], ["Mather", "Alison E.", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1406.3895", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Weiran Wang and Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "The Laplacian K-modes algorithm for clustering", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to finding meaningful clusters, centroid-based clustering\nalgorithms such as K-means or mean-shift should ideally find centroids that are\nvalid patterns in the input space, representative of data in their cluster.\nThis is challenging with data having a nonconvex or manifold structure, as with\nimages or text. We introduce a new algorithm, Laplacian K-modes, which\nnaturally combines three powerful ideas in clustering: the explicit use of\nassignment variables (as in K-means); the estimation of cluster centroids which\nare modes of each cluster's density estimate (as in mean-shift); and the\nregularizing effect of the graph Laplacian, which encourages similar\nassignments for nearby points (as in spectral clustering). The optimization\nalgorithm alternates an assignment step, which is a convex quadratic program,\nand a mean-shift step, which separates for each cluster centroid. The algorithm\nfinds meaningful density estimates for each cluster, even with challenging\nproblems where the clusters have manifold structure, are highly nonconvex or in\nhigh dimension. It also provides centroids that are valid patterns, truly\nrepresentative of their cluster (unlike K-means), and an out-of-sample mapping\nthat predicts soft assignments for a new point.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 03:29:48 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Wang", "Weiran", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1406.4068", "submitter": "Jeffrey Morris", "authors": "Jeffrey S. Morris", "title": "Functional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis (FDA) involves the analysis of data whose ideal\nunits of observation are functions defined on some continuous domain, and the\nobserved data consist of a sample of functions taken from some population,\nsampled on a discrete grid. Ramsay and Silverman's 1997 textbook sparked the\ndevelopment of this field, which has accelerated in the past 10 years to become\none of the fastest growing areas of statistics, fueled by the growing number of\napplications yielding this type of data. One unique characteristic of FDA is\nthe need to combine information both across and within functions, which Ramsay\nand Silverman called replication and regularization, respectively. This article\nwill focus on functional regression, the area of FDA that has received the most\nattention in applications and methodological development. First will be an\nintroduction to basis functions, key building blocks for regularization in\nfunctional regression methods, followed by an overview of functional regression\nmethods, split into three types: [1] functional predictor regression\n(scalar-on-function), [2] functional response regression (function-on-scalar)\nand [3] function-on-function regression. For each, the role of replication and\nregularization will be discussed and the methodological development described\nin a roughly chronological manner, at times deviating from the historical\ntimeline to group together similar methods. The primary focus is on modeling\nand methodology, highlighting the modeling structures that have been developed\nand the various regularization approaches employed. At the end is a brief\ndiscussion describing potential areas of future development in this field.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 16:56:35 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Morris", "Jeffrey S.", ""]]}, {"id": "1406.4151", "submitter": "Johan Segers", "authors": "Johan Segers", "title": "On the asymptotic distribution of the mean absolute deviation about the\n  mean", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean absolute deviation about the mean is an alternative to the standard\ndeviation for measuring dispersion in a sample or in a population. For\nstationary, ergodic time series with a finite first moment, an asymptotic\nexpansion for the sample mean absolute deviation is proposed. The expansion\nyields the asymptotic distribution of the sample mean absolute deviation under\na wide range of settings, allowing for serial dependence or an infinite second\nmoment.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 20:02:47 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Segers", "Johan", ""]]}, {"id": "1406.4306", "submitter": "Xiaodong Luo", "authors": "Xiaodong Luo, Rolf J. Lorentzen, Andreas S. Stordal and Geir\n  N{\\ae}vdal", "title": "Toward an enhanced Bayesian estimation framework for multiphase flow\n  soft-sensing", "comments": "To appear in Inverse Problems", "journal-ref": null, "doi": "10.1088/0266-5611/30/11/114012", "report-no": null, "categories": "stat.ME math.OC physics.data-an physics.geo-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work the authors study the multiphase flow soft-sensing problem based\non a previously established framework. There are three functional modules in\nthis framework, namely, a transient well flow model that describes the response\nof certain physical variables in a well, for instance, temperature, velocity\nand pressure, to the flow rates entering and leaving the well zones; a Markov\njump process that is designed to capture the potential abrupt changes in the\nflow rates; and an estimation method that is adopted to estimate the underlying\nflow rates based on the measurements from the physical sensors installed in the\nwell.\n  In the previous studies, the variances of the flow rates in the Markov jump\nprocess are chosen manually. To fill this gap, in the current work two\nautomatic approaches are proposed in order to optimize the variance estimation.\nThrough a numerical example, we show that, when the estimation framework is\nused in conjunction with these two proposed variance-estimation approaches, it\ncan achieve reasonable performance in terms of matching both the measurements\nof the physical sensors and the true underlying flow rates.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 10:32:30 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Luo", "Xiaodong", ""], ["Lorentzen", "Rolf J.", ""], ["Stordal", "Andreas S.", ""], ["N\u00e6vdal", "Geir", ""]]}, {"id": "1406.4421", "submitter": "Shih-Kang Chao", "authors": "Shih-Kang Chao, Katharina Proksch, Holger Dette, Wolfgang H\u007f\\\"ardle", "title": "Confidence Corridors for Multivariate Generalized Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the construction of confidence corridors for multivariate\nnonparametric generalized quantile regression functions. This construction is\nbased on asymptotic results for the maximal deviation between a suitable\nnonparametric estimator and the true function of interest which follow after a\nseries of approximation steps including a Bahadur representation, a new strong\napproximation theorem and exponential tail inequalities for Gaussian random\nfields. As a byproduct we also obtain confidence corridors for the regression\nfunction in the classical mean regression. In order to deal with the problem of\nslowly decreasing error in coverage probability of the asymptotic confidence\ncorridors, which results in meager coverage for small sample sizes, a simple\nbootstrap procedure is designed based on the leading term of the Bahadur\nrepresentation. The finite sample properties of both procedures are\ninvestigated by means of a simulation study and it is demonstrated that the\nbootstrap procedure considerably outperforms the asymptotic bands in terms of\ncoverage accuracy. Finally, the bootstrap confidence corridors are used to\nstudy the efficacy of the National Supported Work Demonstration, which is a\nrandomized employment enhancement program launched in the 1970s. This article\nhas supplementary materials.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 16:47:03 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 20:08:32 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Chao", "Shih-Kang", ""], ["Proksch", "Katharina", ""], ["Dette", "Holger", ""], ["H\u007f\u00e4rdle", "Wolfgang", ""]]}, {"id": "1406.4549", "submitter": "Art Owen", "authors": "Zhijian He and Art B. Owen", "title": "Extensible grids: uniform sampling on a space-filling curve", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the properties of points in $[0,1]^d$ generated by applying\nHilbert's space-filling curve to uniformly distributed points in $[0,1]$. For\ndeterministic sampling we obtain a discrepancy of $O(n^{-1/d})$ for $d\\ge2$.\nFor random stratified sampling, and scrambled van der Corput points, we get a\nmean squared error of $O(n^{-1-2/d})$ for integration of Lipshitz continuous\nintegrands, when $d\\ge3$. These rates are the same as one gets by sampling on\n$d$ dimensional grids and they show a deterioration with increasing $d$. The\nrate for Lipshitz functions is however best possible at that level of\nsmoothness and is better than plain IID sampling. Unlike grids, space-filling\ncurve sampling provides points at any desired sample size, and the van der\nCorput version is extensible in $n$. Additionally we show that certain\ndiscontinuous functions with infinite variation in the sense of Hardy and\nKrause can be integrated with a mean squared error of $O(n^{-1-1/d})$. It was\npreviously known only that the rate was $o(n^{-1})$. Other space-filling\ncurves, such as those due to Sierpinski and Peano, also attain these rates,\nwhile upper bounds for the Lebesgue curve are somewhat worse, as if the\ndimension were $\\log_2(3)$ times as high.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jun 2014 22:13:43 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["He", "Zhijian", ""], ["Owen", "Art B.", ""]]}, {"id": "1406.4643", "submitter": "Alfred Galichon", "authors": "Guillaume Carlier, Victor Chernozhukov, Alfred Galichon", "title": "Vector Quantile Regression: An Optimal Transport Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a notion of conditional vector quantile function and a vector\nquantile regression. A \\emph{conditional vector quantile function} (CVQF) of a\nrandom vector $Y$, taking values in $\\mathbb{R}^d$ given covariates $Z=z$,\ntaking values in $\\mathbb{R}% ^k$, is a map $u \\longmapsto Q_{Y\\mid Z}(u,z)$,\nwhich is monotone, in the sense of being a gradient of a convex function, and\nsuch that given that vector $U$ follows a reference non-atomic distribution\n$F_U$, for instance uniform distribution on a unit cube in $\\mathbb{R}^d$, the\nrandom vector $Q_{Y\\mid Z}(U,z)$ has the distribution of $Y$ conditional on\n$Z=z$. Moreover, we have a strong representation, $Y = Q_{Y\\mid Z}(U,Z)$ almost\nsurely, for some version of $U$. The \\emph{vector quantile regression} (VQR) is\na linear model for CVQF of $Y$ given $Z$. Under correct specification, the\nnotion produces strong representation, $Y=\\beta \\left(U\\right) ^\\top f(Z)$, for\n$f(Z)$ denoting a known set of transformations of $Z$, where $u \\longmapsto\n\\beta(u)^\\top f(Z)$ is a monotone map, the gradient of a convex function, and\nthe quantile regression coefficients $u \\longmapsto \\beta(u)$ have the\ninterpretations analogous to that of the standard scalar quantile regression.\nAs $f(Z)$ becomes a richer class of transformations of $Z$, the model becomes\nnonparametric, as in series modelling. A key property of VQR is the embedding\nof the classical Monge-Kantorovich's optimal transportation problem at its core\nas a special case. In the classical case, where $Y$ is scalar, VQR reduces to a\nversion of the classical QR, and CVQF reduces to the scalar conditional\nquantile function. An application to multiple Engel curve estimation is\nconsidered.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 08:59:52 GMT"}, {"version": "v2", "created": "Sun, 14 Dec 2014 02:10:57 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2015 20:59:21 GMT"}, {"version": "v4", "created": "Sun, 27 Sep 2015 20:54:09 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Carlier", "Guillaume", ""], ["Chernozhukov", "Victor", ""], ["Galichon", "Alfred", ""]]}, {"id": "1406.4676", "submitter": "Yuan-chin Chang yc.ivan.chang", "authors": "Jing Wang and Eunsik Park and Yuan-chin Ivan Chang", "title": "Active Learning Via Sequential Design and Uncertainty Sampling", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is an important task in many fields including biomedical\nresearch and machine learning. Traditionally, a classification rule is\nconstructed based a bunch of labeled data. Recently, due to technological\ninnovation and automatic data collection schemes, we easily encounter with data\nsets containing large amounts of unlabeled samples. Because to label each of\nthem is usually costly and inefficient, how to utilize these unlabeled data in\na classifier construction process becomes an important problem. In machine\nlearning literature, active learning or semi-supervised learning are popular\nconcepts discussed under this situation, where classification algorithms\nrecruit new unlabeled subjects sequentially based on the information learned\nfrom previous stages of its learning process, and these new subjects are then\nlabeled and included as new training samples. From a statistical aspect, these\nmethods can be recognized as a hybrid of the sequential design and stochastic\napproximation procedure. In this paper, we study sequential learning procedures\nfor building efficient and effective classifiers, where only the selected\nsubjects are labeled and included in its learning stage. The proposed algorithm\ncombines the ideas of Bayesian sequential optimal design and uncertainty\nsampling. Computational issues of the algorithm are discussed. Numerical\nresults using both synthesized data and real examples are reported.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 10:40:43 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Wang", "Jing", ""], ["Park", "Eunsik", ""], ["Chang", "Yuan-chin Ivan", ""]]}, {"id": "1406.4705", "submitter": "Konstantinos Themelis", "authors": "Konstantinos E. Themelis, Athanasios A. Rontogiannis, Konstantinos D.\n  Koutroumbas", "title": "Semisupervised hyperspectral image unmixing using a variational Bayes\n  algorithm", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report presents a variational Bayes algorithm for\nsemisupervised hyperspectral image unmixing. The presented Bayesian model\nemploys a heavy tailed, nonnegatively truncated Laplace prior over the\nabundance coefficients. This prior imposes both the sparsity assumption and the\nnonnegativity constraint on the abundance coefficients. Experimental results\nconducted on the Aviris Cuprite data set are presented that demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 13:13:17 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Themelis", "Konstantinos E.", ""], ["Rontogiannis", "Athanasios A.", ""], ["Koutroumbas", "Konstantinos D.", ""]]}, {"id": "1406.4784", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Improved Densification of One Permutation Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing work on densification of one permutation hashing reduces the\nquery processing cost of the $(K,L)$-parameterized Locality Sensitive Hashing\n(LSH) algorithm with minwise hashing, from $O(dKL)$ to merely $O(d + KL)$,\nwhere $d$ is the number of nonzeros of the data vector, $K$ is the number of\nhashes in each hash table, and $L$ is the number of hash tables. While that is\na substantial improvement, our analysis reveals that the existing densification\nscheme is sub-optimal. In particular, there is no enough randomness in that\nprocedure, which affects its accuracy on very sparse datasets.\n  In this paper, we provide a new densification procedure which is provably\nbetter than the existing scheme. This improvement is more significant for very\nsparse datasets which are common over the web. The improved technique has the\nsame cost of $O(d + KL)$ for query processing, thereby making it strictly\npreferable over the existing procedure. Experimental evaluations on public\ndatasets, in the task of hashing based near neighbor search, support our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 16:16:22 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1406.5062", "submitter": "Akram Shalabi", "authors": "A Shalabi, A C C Coolen, and E de Rinaldis", "title": "Overcoming computational inability to predict clinical outcome from\n  high-dimensional patient data using Bayesian methods", "comments": "Conference Paper 3rd ICCSISCT 2014 - Sydney, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical outcome prediction from high-dimensional data is problematic in the\ncommon setting where there is only a relatively small number of samples. The\nimbalance causes data overfitting, and outcome prediction becomes\ncomputationally expensive or even impossible. We propose a Bayesian outcome\nprediction method that can be applied to data of arbitrary dimension d, from 2\noutcome classes, and reduces overfitting without any approximations at\nparameter level. This is achieved by avoiding numerical integration or\napproximation, and solving the Bayesian integrals analytically. We thereby\nreduce the dimension of numerical integrals from 2d dimensions to 4, for any d.\nFor large d, this is reduced further to 3, and we obtain a simple outcome\nprediction formula without integrals in leading order for very large d. We\ncompare our method to the mclustDA method (Fraley and Raftery 2002), using\nsimulated and real data sets. Our method perform as well as or better than\nmclustDA in low dimensions d. In large dimensions d, mclustDA breaks down due\nto computational limitations, while our method provides a feasible and\ncomputationally efficient alternative.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 14:32:48 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Shalabi", "A", ""], ["Coolen", "A C C", ""], ["de Rinaldis", "E", ""]]}, {"id": "1406.5071", "submitter": "Abderrahim Halimi", "authors": "Abderrahim Halimi and Nicolas Dobigeon and Jean-Yves Tourneret", "title": "Unsupervised Unmixing of Hyperspectral Images Accounting for Endmember\n  Variability", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2471182", "report-no": null, "categories": "stat.ME physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an unsupervised Bayesian algorithm for hyperspectral\nimage unmixing accounting for endmember variability. The pixels are modeled by\na linear combination of endmembers weighted by their corresponding abundances.\nHowever, the endmembers are assumed random to take into account their\nvariability in the image. An additive noise is also considered in the proposed\nmodel generalizing the normal compositional model. The proposed algorithm\nexploits the whole image to provide spectral and spatial information. It\nestimates both the mean and the covariance matrix of each endmember in the\nimage. This allows the behavior of each material to be analyzed and its\nvariability to be quantified in the scene. A spatial segmentation is also\nobtained based on the estimated abundances. In order to estimate the parameters\nassociated with the proposed Bayesian model, we propose to use a Hamiltonian\nMonte Carlo algorithm. The performance of the resulting unmixing strategy is\nevaluated via simulations conducted on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 15:06:02 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Halimi", "Abderrahim", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1406.5392", "submitter": "Kengo Kamatani", "authors": "Kengo Kamatani", "title": "Rate optimality of Random walk Metropolis algorithm in high-dimension\n  with heavy-tailed target distribution", "comments": "12pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of the increment distribution is crucial for the random-walk\nMetropolis-Hastings (RWM) algorithm. In this paper we study the optimal choice\nin high-dimension setting among all possible increment distributions. The\nconclusion is rather counter intuitive, but the optimal rate of convergence is\nattained by the usual choice, the normal distribution as the increment\ndistribution. In particular, no heavy-tailed increment distribution can improve\nthe rate.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 14:05:24 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 03:02:04 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Kamatani", "Kengo", ""]]}, {"id": "1406.5421", "submitter": "Sandra Fortini", "authors": "Sandra Fortini, Sonia Petrone", "title": "Predictive Characterization of Mixtures of Markov Chains", "comments": "To appear in Bernoulli Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive constructions are a powerful way of characterizing the probability\nlaw of stochastic processes with certain forms of invariance, such as\nexchangeability or Markov exchangeability. When de Finetti-like representation\ntheorems are available, the predictive characterization implicitly defines the\nprior distribution, starting from assumptions on the observables; moreover, it\noften helps designing efficient computational strategies. In this paper we give\nnecessary and sufficient conditions on the sequence of predictive distributions\nsuch that they characterize a Markov exchangeable probability law for a\ndiscrete valued process X. Under recurrence, Markov exchangeable processes are\nmixtures of Markov chains. Thus, our results help checking when a predictive\nscheme characterizes a prior for Bayesian inference on the unknown transition\nmatrix of a Markov chain. Our predictive conditions are in some sense minimal\nsufficient conditions for Markov exchangeability; we also provide predictive\nconditions for recurrence. We illustrate their application in relevant examples\nfrom the literature and in novel constructions.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jun 2014 15:15:28 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 16:29:27 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2015 11:41:08 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Fortini", "Sandra", ""], ["Petrone", "Sonia", ""]]}, {"id": "1406.5663", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Christopher R. Genovese, Larry Wasserman", "title": "Asymptotic theory for density ridges", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1329 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 5, 1896-1928", "doi": "10.1214/15-AOS1329", "report-no": "IMS-AOS-AOS1329", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large sample theory of estimators for density modes is well understood.\nIn this paper we consider density ridges, which are a higher-dimensional\nextension of modes. Modes correspond to zero-dimensional, local high-density\nregions in point clouds. Density ridges correspond to $s$-dimensional, local\nhigh-density regions in point clouds. We establish three main results. First we\nshow that under appropriate regularity conditions, the local variation of the\nestimated ridge can be approximated by an empirical process. Second, we show\nthat the distribution of the estimated ridge converges to a Gaussian process.\nThird, we establish that the bootstrap leads to valid confidence sets for\ndensity ridges.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 02:16:33 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2015 20:32:14 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2015 13:25:53 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Genovese", "Christopher R.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1406.5795", "submitter": "Christopher Carter", "authors": "Christopher K. Carter and Eduardo F. Mendes and Robert Kohn", "title": "An extended space approach for particle Markov chain Monte Carlo methods", "comments": "35 pages, 2 figures, Typos corrected from Version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider fully Bayesian inference in general state space\nmodels. Existing particle Markov chain Monte Carlo (MCMC) algorithms use an\naugmented model that takes into account all the variable sampled in a\nsequential Monte Carlo algorithm. This paper describes an approach that also\nuses sequential Monte Carlo to construct an approximation to the state space,\nbut generates extra states using MCMC runs at each time point. We construct an\naugmented model for our extended space with the marginal distribution of the\nsampled states matching the posterior distribution of the state vector. We show\nhow our method may be combined with particle independent Metropolis-Hastings or\nparticle Gibbs steps to obtain a smoothing algorithm. All the Metropolis\nacceptance probabilities are identical to those obtained in existing\napproaches, so there is no extra cost in term of Metropolis-Hastings rejections\nwhen using our approach. The number of MCMC iterates at each time point is\nchosen by the used and our augmented model collapses back to the model in\nOlsson and Ryden (2011) when the number of MCMC iterations reduces. We show\nempirically that our approach works well on applied examples and can outperform\nexisting methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 02:23:45 GMT"}, {"version": "v2", "created": "Wed, 30 Jul 2014 06:32:01 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Carter", "Christopher K.", ""], ["Mendes", "Eduardo F.", ""], ["Kohn", "Robert", ""]]}, {"id": "1406.5799", "submitter": "Alexander Franks", "authors": "Alexander Franks, Florian Markowetz and Edoardo Airoldi", "title": "Estimating cellular pathways from an ensemble of heterogeneous data\n  sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building better models of cellular pathways is one of the major challenges of\nsystems biology and functional genomics. There is a need for methods to build\non established expert knowledge and reconcile it with results of\nhigh-throughput studies. Moreover, the available data sources are heterogeneous\nand need to be combined in a way specific for the part of the pathway in which\nthey are most informative. Here, we present a compartment specific strategy to\nintegrate edge, node and path data for the refinement of a network hypothesis.\nSpecifically, we use a local-move Gibbs sampler for refining pathway hypotheses\nfrom a compendium of heterogeneous data sources, including novel methodology\nfor integrating protein attributes. We demonstrate the utility of this approach\nin a case study of the pheromone response MAPK pathway in the yeast S.\ncerevisiae.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 02:45:49 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Franks", "Alexander", ""], ["Markowetz", "Florian", ""], ["Airoldi", "Edoardo", ""]]}, {"id": "1406.5881", "submitter": "Thomas Trikalinos", "authors": "Ingram Olkin, Thomas A. Trikalinos", "title": "Constructions for a bivariate beta distribution", "comments": "10 pages, 1 table, 1 figure", "journal-ref": null, "doi": null, "report-no": "tech2014_0001", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The beta distribution is a basic distribution serving several purposes. It is\nused to model data, and also, as a more flexible version of the uniform\ndistribution, it serves as a prior distribution for a binomial probability. The\nbivariate beta distribution plays a similar role for two probabilities that\nhave a bivariate binomial distribution. We provide a new multivariate\ndistribution with beta marginal distributions, positive probability over the\nunit square, and correlations over the full range. We discuss its extension to\nthree or more dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 12:11:47 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 12:24:06 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Olkin", "Ingram", ""], ["Trikalinos", "Thomas A.", ""]]}, {"id": "1406.5933", "submitter": "Jay Bartroff", "authors": "Jay Bartroff", "title": "Multiple Hypothesis Tests Controlling Generalized Error Rates for\n  Sequential Data", "comments": "35 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\gamma$-FDP and $k$-FWER multiple testing error metrics, which are tail\nprobabilities of the respective error statistics, have become popular recently\nas less-stringent alternatives to the FDR and FWER. We propose general and\nflexible stepup and stepdown procedures for testing multiple hypotheses about\nsequential (or streaming) data that simultaneously control both the type I and\nII versions of $\\gamma$-FDP, or $k$-FWER. The error control holds regardless of\nthe dependence between data streams, which may be of arbitrary size and shape.\nAll that is needed is a test statistic for each data stream that controls the\nconventional type I and II error probabilities, and no information or\nassumptions are required about the joint distribution of the statistics or data\nstreams. The procedures can be used with sequential, group sequential,\ntruncated, or other sampling schemes. We give recommendations for the\nprocedures' implementation including closed-form expressions for the needed\ncritical values in some commonly-encountered testing situations. The proposed\nsequential procedures are compared with each other and with comparable fixed\nsample size procedures in the context of strongly positively correlated\nGaussian data streams. For this setting we conclude that both the stepup and\nstepdown sequential procedures provide substantial savings over the fixed\nsample procedures in terms of expected sample size, and the stepup procedure\nperforms slightly but consistently better than the stepdown for $\\gamma$-FDP\ncontrol, with the relationship reversed for $k$-FWER control.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 14:57:17 GMT"}, {"version": "v2", "created": "Fri, 19 Sep 2014 15:14:08 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2015 20:47:53 GMT"}, {"version": "v4", "created": "Mon, 19 Dec 2016 16:41:09 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Bartroff", "Jay", ""]]}, {"id": "1406.5954", "submitter": "Catherine Calder", "authors": "Yanan Jia, Catherine A. Calder, and Christopher R. Browning", "title": "Bilinear Mixed-Effects Models for Affiliation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An affiliation network is a particular type of two-mode social network that\nconsists of a set of `actors' and a set of `events' where ties indicate an\nactor's participation in an event. Although networks describe a variety of\nconsequential social structures, statistical methods for studying affiliation\nnetworks are less well developed than methods for studying one-mode, or\nactor-actor, networks. One way to analyze affiliation networks is to consider\none-mode network matrices that are derived from an affiliation network, but\nthis approach may lead to the loss of important structural features of the\ndata. The most comprehensive approach is to study both actors and events\nsimultaneously. In this paper, we extend the bilinear mixed-effects model, a\ntype of latent space model developed for one-mode networks, to the affiliation\nnetwork setting by considering the dependence patterns in the interactions\nbetween actors and events and describe a Markov chain Monte Carlo algorithm for\nBayesian inference. We use our model to explore patterns in extracurricular\nactivity membership of students in a racially-diverse high school in a\nMidwestern metropolitan area. Using techniques from spatial point pattern\nanalysis, we show how our model can provide insight into patterns of racial\nsegregation in the voluntary extracurricular activity participation profiles of\nadolescents.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 15:56:03 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 19:28:01 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Jia", "Yanan", ""], ["Calder", "Catherine A.", ""], ["Browning", "Christopher R.", ""]]}, {"id": "1406.5958", "submitter": "Matthew Reimherr", "authors": "Matthew Reimherr, Xiao-Li Meng, and Dan L. Nicolae", "title": "Prior sample size extensions for assessing prior impact and\n  prior--likelihood discordance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines a framework for quantifying the prior's contribution to\nposterior inference in the presence of prior-likelihood discordance, a broader\nconcept than the usual notion of prior-likelihood conflict. We achieve this\ndual purpose by extending the classic notion of \\textit{prior sample size},\n$M$, in three directions: (I) estimating $M$ beyond conjugate families; (II)\nformulating $M$ as a relative notion, i.e., as a function of the likelihood\nsample size $k, M(k),$ which also leads naturally to a graphical diagnosis; and\n(III) permitting negative $M$, as a measure of prior-likelihood conflict, i.e.,\nharmful discordance. Our asymptotic regime permits the prior sample size to\ngrow with the likelihood data size, hence making asymptotic arguments\nmeaningful for investigating the impact of the prior relative to that of\nlikelihood. It leads to a simple asymptotic formula for quantifying the impact\nof a proper prior that only involves computing a centrality and a spread\nmeasure of the prior and the posterior. We use simulated and real data to\nillustrate the potential of the proposed framework, including quantifying how\nweak is a \"weakly informative\" prior adopted in a study of lupus nephritis.\nWhereas we take a pragmatic perspective in assessing the impact of a prior on a\ngiven inference problem under a specific evaluative metric, we also touch upon\nconceptual and theoretical issues such as using improper priors and permitting\npriors with asymptotically non-vanishing influence.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 16:06:21 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 15:33:23 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 14:43:41 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Reimherr", "Matthew", ""], ["Meng", "Xiao-Li", ""], ["Nicolae", "Dan L.", ""]]}, {"id": "1406.6017", "submitter": "Ngom Papa", "authors": "Papa Ngom and Hamza Dhaker and Pierre Mendy and El Hadji Deme", "title": "Uniform-in-bandwidth consistency for nonparametric estimation of\n  divergence measures", "comments": "17 pages. arXiv admin note: text overlap with arXiv:0807.2153 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We propose nonparametric estimation of divergence measures between continuous\ndistributions. Our approach is based on a plug-in kernel- type estimators of\ndensity functions. We give the uniform in bandwidth consistency for the\nproposal estimators. As a consequence, their asymp- totic 100% confidence\nintervals are also provided.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 18:43:11 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Ngom", "Papa", ""], ["Dhaker", "Hamza", ""], ["Mendy", "Pierre", ""], ["Deme", "El Hadji", ""]]}, {"id": "1406.6200", "submitter": "Thijs van Ommen", "authors": "Thijs van Ommen", "title": "Combining predictions from linear models when training and test inputs\n  differ", "comments": "12 pages, 2 figures. To appear in Proceedings of the 30th Conference\n  on Uncertainty in Artificial Intelligence (UAI2014). This version includes\n  the supplementary material (regularity assumptions, proofs)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for combining predictions from different models in a supervised\nlearning setting must somehow estimate/predict the quality of a model's\npredictions at unknown future inputs. Many of these methods (often implicitly)\nmake the assumption that the test inputs are identical to the training inputs,\nwhich is seldom reasonable. By failing to take into account that prediction\nwill generally be harder for test inputs that did not occur in the training\nset, this leads to the selection of too complex models. Based on a novel,\nunbiased expression for KL divergence, we propose XAIC and its special case\nFAIC as versions of AIC intended for prediction that use different degrees of\nknowledge of the test inputs. Both methods substantially differ from and may\noutperform all the known versions of AIC even when the training and test inputs\nare iid, and are especially useful for deterministic inputs and under covariate\nshift. Our experiments on linear models suggest that if the test and training\ninputs differ substantially, then XAIC and FAIC predictively outperform AIC,\nBIC and several other methods including Bayesian model averaging.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 10:56:13 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["van Ommen", "Thijs", ""]]}, {"id": "1406.6288", "submitter": "Jean-Michel Marin", "authors": "Pierre Pudlo, Jean-Michel Marin (IMAG and IBC, Universite de\n  Montpellier), Arnaud Estoup, Jean-Marie Cornuet, Mathieu Gautier (CBGP, INRA,\n  Montpellier), Christian P. Robert (Universite Paris-Dauphine and University\n  of Warwick)", "title": "Reliable ABC model choice via random forests", "comments": "39 pages, 15 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.PE stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) methods provide an elaborate approach\nto Bayesian inference on complex models, including model choice. Both\ntheoretical arguments and simulation experiments indicate, however, that model\nposterior probabilities may be poorly evaluated by standard ABC techniques. We\npropose a novel approach based on a machine learning tool named random forests\nto conduct selection among the highly complex models covered by ABC algorithms.\nWe thus modify the way Bayesian model selection is both understood and\noperated, in that we rephrase the inferential goal as a classification problem,\nfirst predicting the model that best fits the data with random forests and\npostponing the approximation of the posterior probability of the predicted MAP\nfor a second stage also relying on random forests. Compared with earlier\nimplementations of ABC model choice, the ABC random forest approach offers\nseveral potential improvements: (i) it often has a larger discriminative power\namong the competing models, (ii) it is more robust against the number and\nchoice of statistics summarizing the data, (iii) the computing effort is\ndrastically reduced (with a gain in computation efficiency of at least fifty),\nand (iv) it includes an approximation of the posterior probability of the\nselected model. The call to random forests will undoubtedly extend the range of\nsize of datasets and complexity of models that ABC can handle. We illustrate\nthe power of this novel methodology by analyzing controlled experiments as well\nas genuine population genetics datasets. The proposed methodologies are\nimplemented in the R package abcrf available on the CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 16:03:32 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 09:45:03 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2015 15:19:41 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Pudlo", "Pierre", "", "IMAG and IBC, Universite de\n  Montpellier"], ["Marin", "Jean-Michel", "", "IMAG and IBC, Universite de\n  Montpellier"], ["Estoup", "Arnaud", "", "CBGP, INRA,\n  Montpellier"], ["Cornuet", "Jean-Marie", "", "CBGP, INRA,\n  Montpellier"], ["Gautier", "Mathieu", "", "CBGP, INRA,\n  Montpellier"], ["Robert", "Christian P.", "", "Universite Paris-Dauphine and University\n  of Warwick"]]}, {"id": "1406.6348", "submitter": "Simon Nanty", "authors": "Vincent Moutoussamy (EDF R&D, IMT), Simon Nanty (DER, Grenoble 1 UJF),\n  Beno\\^it Pauwels (IFPEN, UPS)", "title": "Emulators for stochastic simulation codes", "comments": null, "journal-ref": null, "doi": "10.1051/proc/201448005", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical simulation codes are very common tools to study complex phenomena,\nbut they are often time-consuming and considered as black boxes. For some\nstatistical studies (e.g. asset management, sensitivity analysis) or\noptimization problems (e.g. tuning of a molecular model), a high number of runs\nof such codes is needed. Therefore it is more convenient to build a\nfast-running approximation - or metamodel - of this code based on a design of\nexperiments. The topic of this paper is the definition of metamodels for\nstochastic codes. Contrary to deterministic codes, stochastic codes can give\ndifferent results when they are called several times with the same input. In\nthis paper, two approaches are proposed to build a metamodel of the probability\ndensity function of a stochastic code output. The first one is based on kernel\nregression and the second one consists in decomposing the output density on a\nbasis of well-chosen probability density functions, with a metamodel linking\nthe coefficients and the input parameters. For the second approach, two types\nof decomposition are proposed, but no metamodel has been designed for the\ncoefficients yet. This is a topic of future research. These methods are applied\nto two analytical models and three industrial cases.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 19:37:56 GMT"}, {"version": "v2", "created": "Thu, 26 Jun 2014 06:57:23 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Moutoussamy", "Vincent", "", "EDF R&D, IMT"], ["Nanty", "Simon", "", "DER, Grenoble 1 UJF"], ["Pauwels", "Beno\u00eet", "", "IFPEN, UPS"]]}, {"id": "1406.6371", "submitter": "Ewan Cameron Dr", "authors": "Ewan Cameron", "title": "What we talk about when we talk about fields", "comments": "4 pages, no figures, proceedings from the IAUS306 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In astronomical and cosmological studies one often wishes to infer some\nproperties of an infinite-dimensional field indexed within a finite-dimensional\nmetric space given only a finite collection of noisy observational data.\nBayesian inference offers an increasingly-popular strategy to overcome the\ninherent ill-posedness of this signal reconstruction challenge. However, there\nremains a great deal of confusion within the astronomical community regarding\nthe appropriate mathematical devices for framing such analyses and the\ndiversity of available computational procedures for recovering posterior\nfunctionals. In this brief research note I will attempt to clarify both these\nissues from an \"applied statistics\" perpective, with insights garnered from my\npost-astronomy experiences as a computational Bayesian / epidemiological\ngeostatistician.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 20:00:20 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Cameron", "Ewan", ""]]}, {"id": "1406.6419", "submitter": "Agniva Som", "authors": "Agniva Som, Christopher M. Hans, Steven N. MacEachern", "title": "Block Hyper-g Priors in Bayesian Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of prior distributions for Bayesian regression has\ntraditionally been driven by the goal of achieving sensible model selection and\nparameter estimation. The formalization of properties that characterize good\nperformance has led to the development and popularization of thick tailed\nmixtures of g priors such as the Zellner--Siow and hyper-g priors. The\nproperties of a particular prior are typically illuminated under limits on the\nlikelihood or the prior. In this paper we introduce a new, conditional\ninformation asymptotic that is motivated by the common data analysis setting\nwhere at least one regression coefficient is much larger than others. We\nanalyze existing mixtures of g priors under this limit and reveal two new\nbehaviors, Essentially Least Squares (ELS) estimation and the Conditional\nLindley's Paradox (CLP), and argue that these behaviors are, in general,\nundesirable. As the driver behind both of these behaviors is the use of a\nsingle, latent scale parameter that is common to all coefficients, we propose a\nblock hyper-g prior, defined by first partitioning the covariates into groups\nand then placing independent hyper-g priors on the corresponding blocks of\ncoefficients. We provide conditions under which ELS and the CLP are avoided by\nthe new class of priors, and provide consistency results under traditional\nsample size asymptotics.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 00:04:50 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 16:49:59 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Som", "Agniva", ""], ["Hans", "Christopher M.", ""], ["MacEachern", "Steven N.", ""]]}, {"id": "1406.6519", "submitter": "Nirian Mart\\'in", "authors": "Abhik Ghosh, Abhijit Mandal, Nirian Martin, Leandro Pardo", "title": "Influence Analysis of Robust Wald-type Tests", "comments": "36 pages", "journal-ref": "Journal of Multivariate Analysis, 2016, 147, 102 - 126", "doi": "10.1016/j.jmva.2016.01.004", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a robust version of the classical Wald test statistics for\ntesting simple and composite null hypotheses for general parametric models.\nThese test statistics are based on the minimum density power divergence\nestimators instead of the maximum likelihood estimators. An extensive study of\ntheir robustness properties is given though the influence functions as well as\nthe chi-square inflation factors. It is theoretically established that the\nlevel and power of these robust tests are stable against outliers, whereas the\nclassical Wald test breaks down. Some numerical examples confirm the validity\nof the theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 10:38:54 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 08:02:58 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2015 09:00:37 GMT"}, {"version": "v4", "created": "Mon, 16 Nov 2015 17:09:43 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Ghosh", "Abhik", ""], ["Mandal", "Abhijit", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1406.6627", "submitter": "Emilie Lebarbier", "authors": "Karine Bertin and Xavier Collilieux and Emilie Lebarbier and Cristian\n  Meza", "title": "Segmentation of multiple series using a Lasso strategy", "comments": "21 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new semi-parametric approach to the joint segmentation of\nmultiple series corrupted by a functional part. This problem appears in\nparticular in geodesy where GPS permanent station coordinate series are\naffected by undocumented artificial abrupt changes and additionally show\nprominent periodic variations. Detecting and estimating them are crucial, since\nthose series are used to determine averaged reference coordinates in\ngeosciences and to infer small tectonic motions induced by climate change. We\npropose an iterative procedure based on Dynamic Programming for the\nsegmentation part and Lasso estimators for the functional part. Our Lasso\nprocedure, based on the dictionary approach, allows us to both estimate smooth\nfunctions and functions with local irregularity, which permits more flexibility\nthan previous proposed methods. This yields to a better estimation of the bias\npart and improvements in the segmentation. The performance of our method is\nassessed using simulated and real data. In particular, we apply our method to\ndata from four GPS stations in Yarragadee, Australia. Our estimation procedure\nresults to be a reliable tool to assess series in terms of change detection and\nperiodic variations estimation giving an interpretable estimation of the\nfunctional part of the model in terms of known functions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 16:20:03 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Bertin", "Karine", ""], ["Collilieux", "Xavier", ""], ["Lebarbier", "Emilie", ""], ["Meza", "Cristian", ""]]}, {"id": "1406.7117", "submitter": "Anish Acharya", "authors": "Anish Acharya", "title": "A Complete Review of Controlling the FDR in a Multiple Comparison\n  Problem Framework -- The Benjamini-Hochberg Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a review of the popular Benjamini Hochberg Method and other\nrelated useful methods of Multiple Hypothesis testing. This is written with the\npurpose of serving a short but complete easy to understand review of the main\narticle with proper background. The paper titled 'Controlling the False\nDiscovery Rate-a practical and powerful Approach to multiple Testing' by\nbenjamini et. al.[1] proposes a new framework of controlling the False\nDiscovery Rate in a Multiple Hypothesis testing problem. It has been claimed\nthat the procedure proposed in the paper results in a substantial gain in power\nmore applicable in case of problems which call for False discovery rate (FDR)\ncontrol rather than Familywise Error Rate (FWER). The proposed method uses a\nsimple Bonferroni type procedure for FDR control.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 09:03:33 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Acharya", "Anish", ""]]}, {"id": "1406.7143", "submitter": "Satyaki Mazumder", "authors": "Satyaki Mazumder", "title": "Single step and multiple step forecasting in one dimensional single\n  chirp signal using MCMC based Bayesian analysis", "comments": "This paper needs some final touch. Final version will be uploaded\n  soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chirp signals are frequently used in different areas of science and\nengineering. MCMC based Bayesian inference is done here for purpose of one step\nand multiple step prediction in case of one dimensional single chirp signal\nwith i.\\ i.\\ d.\\ error structure as well as dependent error structure with\nexponentially decaying covariances. We use Gibbs sampling technique and random\nwalk MCMC to update the parameters. We perform total five simulation studies\nfor illustration purpose. We also do some real data analysis to show how the\nmethod is working in practice.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 10:36:58 GMT"}, {"version": "v2", "created": "Tue, 1 Jul 2014 15:02:47 GMT"}, {"version": "v3", "created": "Wed, 2 Jul 2014 06:42:55 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Mazumder", "Satyaki", ""]]}, {"id": "1406.7343", "submitter": "Abhirup Datta", "authors": "Abhirup Datta, Sudipto Banerjee, Andrew O. Finley and Alan E. Gelfand", "title": "Hierarchical Nearest-Neighbor Gaussian Process Models for Large\n  Geostatistical Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial process models for analyzing geostatistical data entail computations\nthat become prohibitive as the number of spatial locations become large. This\nmanuscript develops a class of highly scalable Nearest Neighbor Gaussian\nProcess (NNGP) models to provide fully model-based inference for large\ngeostatistical datasets. We establish that the NNGP is a well-defined spatial\nprocess providing legitimate finite-dimensional Gaussian densities with sparse\nprecision matrices. We embed the NNGP as a sparsity-inducing prior within a\nrich hierarchical modeling framework and outline how computationally efficient\nMarkov chain Monte Carlo (MCMC) algorithms can be executed without storing or\ndecomposing large matrices. The floating point operations (flops) per iteration\nof this algorithm is linear in the number of spatial locations, thereby\nrendering substantial scalability. We illustrate the computational and\ninferential benefits of the NNGP over competing methods using simulation\nstudies and also analyze forest biomass from a massive United States Forest\nInventory dataset at a scale that precludes alternative dimension-reducing\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 01:24:50 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2016 06:13:51 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Datta", "Abhirup", ""], ["Banerjee", "Sudipto", ""], ["Finley", "Andrew O.", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "1406.7427", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo", "title": "Gaussian Approximations and Related Questions for the Spacings process", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": "Technical report 48BIS, 1986, LSTA, Universit\\'e Pierre et Marie\n  Curie", "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All the available results on the approximation of the k-spacings process to\nGaussian processes have only used one approach, that is the Shorack and Pyke's\none. Here, it is shown that this approach cannot yield a rate better than $%\n\\left( N/\\log \\log N\\right) ^{-\\frac{1}{4}}\\left( \\log N\\right) ^{\\frac{1}{2}%\n}$. Strong and weak bounds for that rate are specified both where k is fixed\nand where $k\\rightarrow +\\infty $. A Glivenko-Cantelli Theorem is given while\nStute's result for the increments of the empirical process based on independent\nand indentically distributed random variables is extended to the spacings\nprocess. One of the Mason-Wellner-Shorack cases is also obtained.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 18:09:41 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Lo", "Gane Samb", ""]]}, {"id": "1406.7434", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo", "title": "Strong limits related to the oscillation modulus of the empirical\n  process based on the k-spacing process", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": "Rapport technique, 48, 1986, LSTA, Universit\\'e Pierre et Marie\n  Curie", "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several strong limit theorems for the oscillation moduli of the\nempirical process have been given in the iid-case. We show that, with very\nslight differences, those strong results are also obtained for some\nrepresentation of the reduced empirical process based on the (non-overlapping)\nk-spacings generated by a sequence of independent random variables (rv's)\nuniformly distributed on $(0,1)$. This yields weak limits for the mentioned\nprocess. Our study includes the case where the step k is unbounded. The results\nare mainly derived from several properties concerning the increments of gamma\nfunctions with parameters k and one.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jun 2014 19:39:38 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Lo", "Gane Samb", ""]]}, {"id": "1406.7648", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Bayesian Network Constraint-Based Structure Learning Algorithms:\n  Parallel and Optimised Implementations in the bnlearn R Package", "comments": "20 pages, 4 figures", "journal-ref": "Journal of Statistical Software (2017), 77(2), 1-20", "doi": null, "report-no": null, "categories": "stat.CO cs.AI cs.MS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known in the literature that the problem of learning the structure\nof Bayesian networks is very hard to tackle: its computational complexity is\nsuper-exponential in the number of nodes in the worst case and polynomial in\nmost real-world scenarios.\n  Efficient implementations of score-based structure learning benefit from past\nand current research in optimisation theory, which can be adapted to the task\nby using the network score as the objective function to maximise. This is not\ntrue for approaches based on conditional independence tests, called\nconstraint-based learning algorithms. The only optimisation in widespread use,\nbacktracking, leverages the symmetries implied by the definitions of\nneighbourhood and Markov blanket.\n  In this paper we illustrate how backtracking is implemented in recent\nversions of the bnlearn R package, and how it degrades the stability of\nBayesian network structure learning for little gain in terms of speed. As an\nalternative, we describe a software architecture and framework that can be used\nto parallelise constraint-based structure learning algorithms (also implemented\nin bnlearn) and we demonstrate its performance using four reference networks\nand two real-world data sets from genetics and systems biology. We show that on\nmodern multi-core or multiprocessor hardware parallel implementations are\npreferable over backtracking, which was developed when single-processor\nmachines were the norm.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 09:56:20 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2015 10:27:23 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1406.7674", "submitter": "Francisco Javier Rubio", "authors": "F. J. Rubio and M. F. J. Steel", "title": "Bayesian modelling of skewness and kurtosis with two-piece scale and\n  shape distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalise and generalise the definition of the family of univariate double\ntwo--piece distributions, obtained by using a density--based transformation of\nunimodal symmetric continuous distributions with a shape parameter. The\nresulting distributions contain five interpretable parameters that control the\nmode, as well as the scale and shape in each direction. Four-parameter\nsubfamilies of this class of distributions that capture different types of\nasymmetry are discussed. We propose interpretable scale and location-invariant\nbenchmark priors and derive conditions for the propriety of the corresponding\nposterior distribution. The prior structures used allow for meaningful\ncomparisons through Bayes factors within flexible families of distributions.\nThese distributions are applied to data from finance, internet traffic and\nmedicine, comparing them with appropriate competitors.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 11:13:10 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2015 17:04:21 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Rubio", "F. J.", ""], ["Steel", "M. F. J.", ""]]}, {"id": "1406.7686", "submitter": "Camelia Goga", "authors": "H. Cardot and C. Goga and M.-A Shehzad", "title": "Calibration and partial calibration on principal components when the\n  number of auxiliary variables is large", "comments": "in revision for Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In survey sampling, calibration is a very popular tool used to make total\nestimators consistent with known totals of auxiliary variables and to reduce\nvariance. When the number of auxiliary variables is large, calibration on all\nthe variables may lead to estimators of totals whose mean squared error (MSE)\nis larger than the MSE of the Horvitz-Thompson estimator even if this simple\nestimator does not take account of the available auxiliary information. We\nstudy in this paper a new technique based on dimension reduction through\nprincipal components that can be useful in this large dimension context.\nCalibration is performed on the first principal components, which can be viewed\nas the synthetic variables containing the most important part of the\nvariability of the auxiliary variables. When some auxiliary variables play a\nmore important role than the others, the method can be adapted to provide an\nexact calibration on these important variables. Some asymptotic properties are\ngiven in which the number of variables is allowed to tend to infinity with the\npopulation size. A data driven selection criterion of the number of principal\ncomponents ensuring that all the sampling weights remain positive is discussed.\nThe methodology of the paper is illustrated, in a multipurpose context, by an\napplication to the estimation of electricity consumption for each day of a week\nwith the help of 336 auxiliary variables consisting of the past consumption\nmeasured every half an hour over the previous week.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 11:58:17 GMT"}, {"version": "v2", "created": "Wed, 26 Nov 2014 08:27:46 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2015 09:36:02 GMT"}, {"version": "v4", "created": "Mon, 14 Dec 2015 10:37:42 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Cardot", "H.", ""], ["Goga", "C.", ""], ["Shehzad", "M. -A", ""]]}, {"id": "1406.7691", "submitter": "Camelia Goga", "authors": "C. Goga and A Ruiz-Gazen", "title": "Improving the estimation of the odds-ratio using auxiliary information", "comments": "to appear in the Mathematical Population Studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The odds ratio measure is used in health and social surveys where the odds of\na certain event is to be compared between two populations. It is defined using\nlogistic regression, and requires that data from surveys are accompanied by\ntheir weights. A nonparametric estimation method that incorporates survey\nweights and auxiliary information may improve the precision of the odds ratio\nestimator. It consists in $B$-spline calibration which can handle the nonlinear\nstructure of the parameter. The variance is estimated through linearization.\nImplementation is possible through standard survey softwares. The gain in\nprecision depends on the data as shown on two examples.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 12:12:35 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Goga", "C.", ""], ["Ruiz-Gazen", "A", ""]]}, {"id": "1406.7718", "submitter": "Ryan Martin", "authors": "Ryan Martin, Raymond Mess and Stephen G. Walker", "title": "Empirical Bayes posterior concentration in sparse high-dimensional\n  linear models", "comments": "24 pages, 3 tables, and 3 extra pages to correct a couple minor\n  mistakes in the published version", "journal-ref": "Bernoulli, 2017, volume 23, number 3, pages 1822--1847", "doi": "10.3150/15-BEJ797", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new empirical Bayes approach for inference in the $p \\gg n$\nnormal linear model. The novelty is the use of data in the prior in two ways,\nfor centering and regularization. Under suitable sparsity assumptions, we\nestablish a variety of concentration rate results for the empirical Bayes\nposterior distribution, relevant for both estimation and model selection.\nComputation is straightforward and fast, and simulation results demonstrate the\nstrong finite-sample performance of the empirical Bayes model selection\nprocedure.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 13:11:37 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 19:46:21 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2015 17:16:00 GMT"}, {"version": "v4", "created": "Fri, 8 Sep 2017 13:21:49 GMT"}, {"version": "v5", "created": "Wed, 5 Dec 2018 18:26:51 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Martin", "Ryan", ""], ["Mess", "Raymond", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1406.7732", "submitter": "Giles Hooker", "authors": "Peter Hall and Giles Hooker", "title": "Truncated Linear Models for Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conventional linear model for functional data involves expressing a\nresponse variable $Y$ in terms of the explanatory function $X(t)$, via the\nmodel: $Y=a+\\int_I b(t)X(t)dt+\\hbox{error}$, where $a$ is a scalar, $b$ is an\nunknown function and $I=[0, \\alpha]$ is a compact interval. However, in some\nproblems the support of $b$ or $X$, $I_1$ say, is a proper and unknown subset\nof $I$, and is a quantity of particular practical interest. In this paper,\nmotivated by a real-data example involving particulate emissions, we develop\nmethods for estimating $I_1$. We give particular emphasis to the case\n$I_1=[0,\\theta]$, where $\\theta \\in(0,\\alpha]$, and suggest two methods for\nestimating $a$, $b$ and $\\theta$ jointly; we introduce techniques for selecting\ntuning parameters; and we explore properties of our methodology using both\nsimulation and the real-data example mentioned above. Additionally, we derive\ntheoretical properties of the methodology, and discuss implications of the\ntheory. Our theoretical arguments give particular emphasis to the problem of\nidentifiability.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 13:41:26 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Hall", "Peter", ""], ["Hooker", "Giles", ""]]}, {"id": "1406.7851", "submitter": "Daniele Durante", "authors": "Daniele Durante, David B. Dunson, Joshua T. Vogelstein", "title": "Nonparametric Bayes Modeling of Populations of Networks", "comments": null, "journal-ref": "Journal of the American Statistical Association (2017). 112,\n  1516-1530", "doi": "10.1080/01621459.2016.1219260", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replicated network data are increasingly available in many research fields.\nIn connectomic applications, inter-connections among brain regions are\ncollected for each patient under study, motivating statistical models which can\nflexibly characterize the probabilistic generative mechanism underlying these\nnetwork-valued data. Available models for a single network are not designed\nspecifically for inference on the entire probability mass function of a\nnetwork-valued random variable and therefore lack flexibility in characterizing\nthe distribution of relevant topological structures. We propose a flexible\nBayesian nonparametric approach for modeling the population distribution of\nnetwork-valued data. The joint distribution of the edges is defined via a\nmixture model which reduces dimensionality and efficiently incorporates network\ninformation within each mixture component by leveraging latent space\nrepresentations. The formulation leads to an efficient Gibbs sampler and\nprovides simple and coherent strategies for inference and goodness-of-fit\nassessments. We provide theoretical results on the flexibility of our model and\nillustrate improved performance --- compared to state-of-the-art models --- in\nsimulations and application to human brain networks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 18:52:44 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2015 09:14:21 GMT"}, {"version": "v3", "created": "Sun, 5 Jun 2016 19:47:36 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Durante", "Daniele", ""], ["Dunson", "David B.", ""], ["Vogelstein", "Joshua T.", ""]]}]