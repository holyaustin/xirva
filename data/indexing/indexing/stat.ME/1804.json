[{"id": "1804.00096", "submitter": "Prabhashi Withana Gamage", "authors": "Prabhashi W. Withana Gamage, Monica Chaudari, Christopher S. McMahan,\n  Michael R. Kosorok", "title": "A proportional hazards model for interval-censored data subject to\n  instantaneous failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proportional hazards (PH) model is arguably one of the most popular\nmodels used to analyze time to event data arising from clinical trials and\nlongitudinal studies, among many others. In many such studies, the event time\nof interest is not directly observed but is known relative to periodic\nexamination times; i.e., practitioners observe either current status or\ninterval-censored data. The analysis of data of this structure is often fraught\nwith many difficulties. Further exacerbating this issue, in some such studies\nthe observed data also consists of instantaneous failures; i.e., the event\ntimes for several study units coincide exactly with the time at which the study\nbegins. In light of these difficulties, this work focuses on developing a\nmixture model, under the PH assumptions, which can be used to analyze\ninterval-censored data subject to instantaneous failures. To allow for modeling\nflexibility, two methods of estimating the unknown cumulative baseline hazard\nfunction are proposed; a fully parametric and a monotone spline representation\nare considered. Through a novel data augmentation procedure involving latent\nPoisson random variables, an expectation-maximization (EM) algorithm was\ndeveloped to complete model fitting. The resulting EM algorithm is easy to\nimplement and is computationally efficient. Moreover, through extensive\nsimulation studies the proposed approach is shown to provide both reliable\nestimation and inference.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 00:48:49 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 12:49:37 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Gamage", "Prabhashi W. Withana", ""], ["Chaudari", "Monica", ""], ["McMahan", "Christopher S.", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1804.00102", "submitter": "Cheng Ju", "authors": "Cheng Ju and Antoine Chambaz and Mark J. van der Laan", "title": "Collaborative targeted inference from continuously indexed nuisance\n  parameter estimators", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We wish to infer the value of a parameter at a law from which we sample\nindependent observations. The parameter is smooth and we can define two\nvariation-independent features of the law, its $Q$- and $G$-components, such\nthat estimating them consistently at a fast enough product of rates allows to\nbuild a confidence interval (CI) with a given asymptotic level from a plain\ntargeted minimum loss estimator (TMLE). Say that the above product is not fast\nenough and the algorithm for the $G$-component is fine-tuned by a real-valued\n$h$. A plain TMLE with an $h$ chosen by cross-validation would typically not\nyield a CI. We construct a collaborative TMLE (C-TMLE) and show under mild\nconditions that, if there exists an oracle $h$ that makes a bulky remainder\nterm asymptotically Gaussian, then the C-TMLE yields a CI. We illustrate our\nfindings with the inference of the average treatment effect. We conduct a\nsimulation study where the $G$-component is estimated by the LASSO and $h$ is\nthe bound on the coefficients' norms. It sheds light on small sample\nproperties, in the face of low- to high-dimensional baseline covariates, and\npossibly positivity violation.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 01:30:36 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 18:29:43 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Ju", "Cheng", ""], ["Chambaz", "Antoine", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1804.00160", "submitter": "Abhik Ghosh PhD", "authors": "Ayanendranath Basu, Abhik Ghosh, Abhijit Mandal, Nirian Martin,\n  Leandro Pardo", "title": "Robust Wald-type test in GLM with random design based on minimum density\n  power divergence estimators", "comments": "Pre-print, Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of robust inference under the generalized linear\nmodel (GLM) with stochastic covariates. We derive the properties of the minimum\ndensity power divergence estimator of the parameters in GLM with random design\nand use this estimator to propose robust Wald-type tests for testing any\ngeneral composite null hypothesis about the GLM. The asymptotic and robustness\nproperties of the proposed tests are also examined for the GLM with random\ndesign. Application of the proposed robust inference procedures to the popular\nPoisson regression model for analyzing count data is discussed in detail both\ntheoretically and numerically through simulation studies and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 11:24:35 GMT"}, {"version": "v2", "created": "Sun, 16 Jun 2019 21:23:07 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2020 17:49:58 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Basu", "Ayanendranath", ""], ["Ghosh", "Abhik", ""], ["Mandal", "Abhijit", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1804.00195", "submitter": "David Cheng", "authors": "David Cheng, Ashwin Ananthakrishnan, Tianxi Cai", "title": "Robust and Efficient Semi-Supervised Estimation of Average Treatment\n  Effects with Application to Electronic Health Records Data", "comments": "47 pages, 2 figures; Revised version - To appear in Biometrics", "journal-ref": null, "doi": "10.1111/biom.13298", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the average treatment effect (ATE) in a\nsemi-supervised learning setting, where a very small proportion of the entire\nset of observations are labeled with the true outcome but features predictive\nof the outcome are available among all observations. This problem arises, for\nexample, when estimating treatment effects in electronic health records (EHR)\ndata because gold-standard outcomes are often not directly observable from the\nrecords but are observed for a limited number of patients through small-scale\nmanual chart review. We develop an imputation-based approach for estimating the\nATE that is robust to misspecification of the imputation model. This\neffectively allows information from the predictive features to be safely\nleveraged to improve efficiency in estimating the ATE. The estimator is\nadditionally doubly-robust in that it is consistent under correct specification\nof either an initial propensity score model or a baseline outcome model. It is\nalso locally semiparametric efficient under an ideal semi-supervised model\nwhere the distribution of the unlabeled data is known. Simulations exhibit the\nefficiency and robustness of the proposed method compared to existing\napproaches in finite samples.We illustrate the method by comparing rates of\ntreatment response to two biologic agents for treatment inflammatory bowel\ndisease using EHR data from Partner's Healthcare.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 17:21:13 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 20:49:39 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Cheng", "David", ""], ["Ananthakrishnan", "Ashwin", ""], ["Cai", "Tianxi", ""]]}, {"id": "1804.00230", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Rosa M. Crujeiras, Wenceslao\n  Gonz\\'alez-Manteiga", "title": "Smoothing-based tests with directional random variables", "comments": "8 pages, 2 figures", "journal-ref": "In Gil, E., Gil, E., Gil, J. and Gil, M. \\'A, editors, The\n  Mathematics of the Uncertain, pages 175-184. Springer, 2018", "doi": "10.1007/978-3-319-73848-2_17", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Testing procedures for assessing specific parametric model forms, or for\nchecking the plausibility of simplifying assumptions, play a central role in\nthe mathematical treatment of the uncertain. No certain answers are obtained by\ntesting methods, but at least the uncertainty of these answers is properly\nquantified. This is the case for tests designed on the two most general data\ngenerating mechanisms in practice: distribution/density and regression models.\nTesting proposals are usually formulated on the Euclidean space, but important\nchallenges arise in non-Euclidean settings, such as when directional variables\n(i.e., random vectors on the hypersphere) are involved. This work reviews some\nof the smoothing-based testing procedures for density and regression models\nthat comprise directional variables. The asymptotic distributions of the\nrevised proposals are presented, jointly with some numerical illustrations\njustifying the need of employing resampling mechanisms for effective test\ncalibration.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 00:01:48 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 10:04:47 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Crujeiras", "Rosa M.", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""]]}, {"id": "1804.00237", "submitter": "Sixing Chen", "authors": "Sixing Chen, Antonietta Mira, Jukka-Pekka Onnela", "title": "Flexible model selection for mechanistic network models", "comments": "17 pages, 4 figures, 2 tables, accepted at Journal of Complex\n  Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network models are applied across many domains where data can be represented\nas a network. Two prominent paradigms for modeling networks are statistical\nmodels (probabilistic models for the observed network) and mechanistic models\n(models for network growth and/or evolution). Mechanistic models are better\nsuited for incorporating domain knowledge, to study effects of interventions\n(such as changes to specific mechanisms) and to forward simulate, but they\ntypically have intractable likelihoods. As such, and in a stark contrast to\nstatistical models, there is a relative dearth of research on model selection\nfor such models despite the otherwise large body of extant work. In this paper,\nwe propose a simulator-based procedure for mechanistic network model selection\nthat borrows aspects from Approximate Bayesian Computation (ABC) along with a\nmeans to quantify the uncertainty in the selected model. To select the most\nsuitable network model, we consider and assess the performance of several\nlearning algorithms, most notably the so-called Super Learner, which makes our\nframework less sensitive to the choice of a particular learning algorithm. Our\napproach takes advantage of the ease to forward simulate from mechanistic\nnetwork models to circumvent their intractable likelihoods. The overall process\nis flexible and widely applicable. Our simulation results demonstrate the\napproach's ability to accurately discriminate between competing mechanistic\nmodels. Finally, we showcase our approach with a protein-protein interaction\nnetwork model from the literature for yeast (Saccharomyces cerevisiae).\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 00:57:06 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 17:05:51 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Chen", "Sixing", ""], ["Mira", "Antonietta", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1804.00285", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Michael Golden, Michael S{\\o}rensen,\n  Kanti V. Mardia, Thomas Hamelryck, Jotun Hein", "title": "Toroidal diffusions and protein structure evolution", "comments": "26 pages, 13 figures", "journal-ref": "In Ley, C. and Verdebout, T., editors, Applied Directional\n  Statistics, pages 61-90. CRC Press, 2018", "doi": "10.1201/9781315228570", "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This chapter shows how toroidal diffusions are convenient methodological\ntools for modelling protein evolution in a probabilistic framework. The chapter\naddresses the construction of ergodic diffusions with stationary distributions\nequal to well-known directional distributions, which can be regarded as\ntoroidal analogues of the Ornstein-Uhlenbeck process. The important challenges\nthat arise in the estimation of the diffusion parameters require the\nconsideration of tractable approximate likelihoods and, among the several\napproaches introduced, the one yielding a specific approximation to the\ntransition density of the wrapped normal process is shown to give the best\nempirical performance on average. This provides the methodological building\nblock for Evolutionary Torus Dynamic Bayesian Network (ETDBN), a hidden Markov\nmodel for protein evolution that emits a wrapped normal process and two\ncontinuous-time Markov chains per hidden state. The chapter describes the main\nfeatures of ETDBN, which allows for both \"smooth\" conformational changes and\n\"catastrophic\" conformational jumps, and several empirical benchmarks. The\ninsights into the relationship between sequence and structure evolution that\nETDBN provides are illustrated in a case study.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 11:51:12 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 10:03:56 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Golden", "Michael", ""], ["S\u00f8rensen", "Michael", ""], ["Mardia", "Kanti V.", ""], ["Hamelryck", "Thomas", ""], ["Hein", "Jotun", ""]]}, {"id": "1804.00286", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Thomas Verdebout", "title": "An overview of uniformity tests on the hypersphere", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When modeling directional data, that is, unit-norm multivariate vectors, a\nfirst natural question is to ask whether the directions are uniformly\ndistributed or, on the contrary, whether there exist modes of variation\nsignificantly different from uniformity. We review in this article a reasonably\nexhaustive collection of uniformity tests for assessing uniformity in the\nhypersphere. Specifically, we review the classical circular-specific tests, the\nlarge class of Sobolev tests with its many notable particular cases, some\nrecent alternative tests, and novel results in the high-dimensional low-sample\nsize case. A reasonably comprehensive bibliography on the topic is provided.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 11:52:36 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 07:37:00 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Verdebout", "Thomas", ""]]}, {"id": "1804.00353", "submitter": "Ye Wang", "authors": "Ye Wang, David Dunson", "title": "Bayesian Mosaic: Parallelizable Composite Posterior", "comments": "45 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Bayesian mosaic, a parallelizable composite posterior,\nfor scalable Bayesian inference on a broad class of multivariate discrete data\nmodels. Sampling is embarrassingly parallel since Bayesian mosaic is a\nmultiplication of component posteriors that can be independently sampled from.\nAnalogous to composite likelihood methods, these component posteriors are based\non univariate or bivariate marginal densities. Utilizing the fact that the\nscore functions of these densities are unbiased, we show that Bayesian mosaic\nis consistent and asymptotically normal under mild conditions. Since the\nevaluation of univariate or bivariate marginal densities can rely on numerical\nintegration, sampling from Bayesian mosaic bypasses the traditional data\naugmented Markov chain Monte Carlo (DA-MCMC) method, which has a provably slow\nmixing rate when data are imbalanced. Moreover, we show that sampling from\nBayesian mosaic has better scalability to large sample size than DA-MCMC. The\nmethod is evaluated via simulation studies and an application on a citation\ncount dataset.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 22:27:03 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Wang", "Ye", ""], ["Dunson", "David", ""]]}, {"id": "1804.00541", "submitter": "Krzysztof Domino", "authors": "Krzysztof Domino", "title": "Multivariate cumulants in outlier detection for financial data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many research papers yielding the financial data models, where\nreturns are tied either to the fundamental analysis or to the individual, often\nirrational, behaviour of investors. In the second case the bubble followed by\nthe crisis is possible on the market. Such bubble or crisis is reflected by the\ncross-correlated extreme positive or negative returns of many assets. Such\nreturns are modelled by the copula with the meaningful tail dependencies. The\ntypical model of such cross-correlation provides the t-Student copula. The\nauthor demonstrates that the mutual information tied to this copula can be\nmeasured by the 4th order multivariate cumulants. Tested on the artificial\ndata, the 4th order multivariate cumulant approach was used successfully for\nthe financial crisis detection. For this end the author introduces the outliers\ndetection algorithm. In addition this algorithm displays the potential\napplication for the crisis prediction, where the cross-correlated extreme\nevents may appear before the crisis in the analogy to the auto-correlated ones\nmeasured by the Hurst Exponent.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 11:46:00 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 10:59:31 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 08:41:04 GMT"}, {"version": "v4", "created": "Fri, 5 Apr 2019 09:59:55 GMT"}, {"version": "v5", "created": "Mon, 27 May 2019 09:40:09 GMT"}, {"version": "v6", "created": "Wed, 26 Feb 2020 13:48:44 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Domino", "Krzysztof", ""]]}, {"id": "1804.00545", "submitter": "Lynn Roy LaMotte", "authors": "Lynn Roy LaMotte", "title": "A Formula for Type III Sums of Squares", "comments": "arXiv admin note: text overlap with arXiv:1708.06483", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type III methods were introduced by SAS to address difficulties in\ndummy-variable models for effects of multiple factors and covariates. They are\nwidely used in practice; they are the default method in several statistical\ncomputing packages. Type III sums of squares (SSs) are defined by a set of\ninstructions; an explicit mathematical formulation does not seem to exist.\n  An explicit formulation is derived in this paper. It is used to illustrate\nType III SSs and their properties in the two-factor ANOVA model.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 22:24:20 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["LaMotte", "Lynn Roy", ""]]}, {"id": "1804.00609", "submitter": "Fekadu L. Bayisa Dr.", "authors": "Fekadu L. Bayisa, Zhiyong Zhou, Ottmar Cronie, Jun Yu", "title": "Adaptive Algorithm for Sparse Signal Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike and slab priors play a key role in inducing sparsity for sparse signal\nrecovery. The use of such priors results in hard non-convex and mixed integer\nprogramming problems. Most of the existing algorithms to solve the optimization\nproblems involve either simplifying assumptions, relaxations or high\ncomputational expenses. We propose a new adaptive alternating direction method\nof multipliers (AADMM) algorithm to directly solve the presented optimization\nproblem. The algorithm is based on the one-to-one mapping property of the\nsupport and non-zero element of the signal. At each step of the algorithm, we\nupdate the support by either adding an index to it or removing an index from it\nand use the alternating direction method of multipliers to recover the signal\ncorresponding to the updated support. Experiments on synthetic data and\nreal-world images show that the proposed AADMM algorithm provides superior\nperformance and is computationally cheaper, compared to the recently developed\niterative convex refinement (ICR) algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 16:10:33 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2019 17:23:59 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Bayisa", "Fekadu L.", ""], ["Zhou", "Zhiyong", ""], ["Cronie", "Ottmar", ""], ["Yu", "Jun", ""]]}, {"id": "1804.00631", "submitter": "Gongkai Li", "authors": "Gongkai Li, Minh Tang, Nichlas Charon, Carey E Priebe", "title": "Central Limit Theorems for Classical Multidimensional Scaling", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical multidimensional scaling is a widely used method in dimensionality\nreduction and manifold learning. The method takes in a dissimilarity matrix and\noutputs a low-dimensional configuration matrix based on a spectral\ndecomposition. In this paper, we present three noise models and analyze the\nresulting configuration matrices, or embeddings. In particular, we show that\nunder each of the three noise models the resulting embedding gives rise to a\ncentral limit theorem. We also provide compelling simulations and real data\nillustrations of these central limit theorems. This perturbation analysis\nrepresents a significant advancement over previous results regarding classical\nmultidimensional scaling behavior under randomness.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 17:22:31 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 18:00:13 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 23:49:57 GMT"}, {"version": "v4", "created": "Tue, 14 May 2019 14:42:54 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Li", "Gongkai", ""], ["Tang", "Minh", ""], ["Charon", "Nichlas", ""], ["Priebe", "Carey E", ""]]}, {"id": "1804.00636", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias, Warren B. Powell", "title": "Recursive Optimization of Convex Risk Measures: Mean-Semideviation\n  Models", "comments": "90 pages, 3 figures. Update: Substantial revision of the technical\n  content, with an additional fully detailed analysis in regard to the rate of\n  convergence of the MESSAGEp algorithm. NOTE: Please open in browser to see\n  the math in the abstract!", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop recursive, data-driven, stochastic subgradient methods for\noptimizing a new, versatile, and application-driven class of convex risk\nmeasures, termed here as mean-semideviations, strictly generalizing the\nwell-known and popular mean-upper-semideviation. We introduce the MESSAGEp\nalgorithm, which is an efficient compositional subgradient procedure for\niteratively solving convex mean-semideviation risk-averse problems to\noptimality. We analyze the asymptotic behavior of the MESSAGEp algorithm under\na flexible and structure-exploiting set of problem assumptions. In particular:\n1) Under appropriate stepsize rules, we establish pathwise convergence of the\nMESSAGEp algorithm in a strong technical sense, confirming its asymptotic\nconsistency. 2) Assuming a strongly convex cost, we show that, for fixed\nsemideviation order $p>1$ and for $\\epsilon\\in\\left[0,1\\right)$, the MESSAGEp\nalgorithm achieves a squared-${\\cal L}_{2}$ solution suboptimality rate of the\norder of ${\\cal O}(n^{-\\left(1-\\epsilon\\right)/2})$ iterations, where, for\n$\\epsilon>0$, pathwise convergence is simultaneously guaranteed. This result\nestablishes a rate of order arbitrarily close to ${\\cal O}(n^{-1/2})$, while\nensuring strongly stable pathwise operation. For $p\\equiv1$, the rate order\nimproves to ${\\cal O}(n^{-2/3})$, which also suffices for pathwise convergence,\nand matches previous results. 3) Likewise, in the general case of a convex\ncost, we show that, for any $\\epsilon\\in\\left[0,1\\right)$, the MESSAGEp\nalgorithm with iterate smoothing achieves an ${\\cal L}_{1}$ objective\nsuboptimality rate of the order of ${\\cal\nO}(n^{-\\left(1-\\epsilon\\right)/\\left(4\\bf{1}_{\\left\\{ p>1\\right\\} }+4\\right)})$\niterations. This result provides maximal rates of ${\\cal O}(n^{-1/4})$, if\n$p\\equiv1$, and ${\\cal O}(n^{-1/8})$, if $p>1$, matching the state of the art,\nas well.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 17:27:52 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 02:36:03 GMT"}, {"version": "v3", "created": "Mon, 4 Jun 2018 03:15:25 GMT"}, {"version": "v4", "created": "Mon, 25 Jun 2018 00:47:36 GMT"}, {"version": "v5", "created": "Mon, 29 Oct 2018 15:49:58 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Powell", "Warren B.", ""]]}, {"id": "1804.00778", "submitter": "Yuhao Wang", "authors": "Yuhao Wang, Santiago Segarra, Caroline Uhler", "title": "High-Dimensional Joint Estimation of Multiple Directed Gaussian\n  Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of jointly estimating multiple related directed\nacyclic graph (DAG) models based on high-dimensional data from each graph. This\nproblem is motivated by the task of learning gene regulatory networks based on\ngene expression data from different tissues, developmental stages or disease\nstates. We prove that under certain regularity conditions, the proposed\n$\\ell_0$-penalized maximum likelihood estimator converges in Frobenius norm to\nthe adjacency matrices consistent with the data-generating distributions and\nhas the correct sparsity. In particular, we show that this joint estimation\nprocedure leads to a faster convergence rate than estimating each DAG model\nseparately. As a corollary, we also obtain high-dimensional consistency results\nfor causal inference from a mix of observational and interventional data. For\npractical purposes, we propose \\emph{jointGES} consisting of Greedy Equivalence\nSearch (GES) to estimate the union of all DAG models followed by variable\nselection using lasso to obtain the different DAGs, and we analyze its\nconsistency guarantees. The proposed method is illustrated through an analysis\nof simulated data as well as epithelial ovarian cancer gene expression data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 01:29:23 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 11:57:46 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 13:18:26 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Wang", "Yuhao", ""], ["Segarra", "Santiago", ""], ["Uhler", "Caroline", ""]]}, {"id": "1804.00808", "submitter": "Steven Thompson", "authors": "Steve Thompson", "title": "Simple estimators for network sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new estimation method is presented for network sampling designs, including\nRespondent Driven Sampling (RDS) and Snowball (SB) sampling. These types of\nlink-tracing designs are essential for studies of hidden populations, such as\npeople at risk for HIV. The simple idea behind the new method is to run a\nfast-sampling process on the sample network data to estimate the inclusion\nprobabilities of the actual survey, and incorporate those in unequal\nprobability estimators of population means and proportions. Improved versions\nof the usual RDS and SB designs are also proposed, termed RDS+ and SB+, to\nobtain information on more of the within-sample links. In simulations using the\nnetwork from the Colorado Springs study on the heterosexual spread of HIV, the\nnew estimators produce in most cases lower bias and lower mean square than\ncurrent methods. For the variables having the largest mean square errors with\ncurrent estimators, the improvement with the new estimator is dramatic. The\nestimates are improved even more with the enhanced design versions. For\nestimating the population mean degree, the efficiency gains using he new method\nare 29 for RDS, 54 for RDS+, 26 for SB and 80 for SB+. This means for example,\nwith the ordinary RDS design, the mean square error with the new estimator,\nsame data, is 1/29 that of currently used estimators. The new method is\ncomputationally intricate but is fast and scales up well. The new estimation\nmethod can be used to re-analyze existing network survey data. For new network\nsampling studies, it is recommended to use the improved designs as well as the\nnew estimators.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 03:53:52 GMT"}, {"version": "v2", "created": "Sun, 14 Oct 2018 11:11:29 GMT"}, {"version": "v3", "created": "Sun, 14 Apr 2019 16:28:35 GMT"}, {"version": "v4", "created": "Mon, 22 Apr 2019 19:50:43 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Thompson", "Steve", ""]]}, {"id": "1804.00888", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa", "title": "Grouped Heterogeneous Mixture Modeling for Clustered Data", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustered data is ubiquitous in a variety of scientific fields. In this\npaper, we propose a flexible and interpretable modeling approach, called\ngrouped heterogenous mixture modeling, for clustered data, which models\ncluster-wise conditional distributions by mixtures of latent conditional\ndistributions common to all the clusters. In the model, we assume that clusters\nare divided into a finite number of groups and mixing proportions are the same\nwithin the same group. We provide a simple generalized EM algorithm for\ncomputing the maximum likelihood estimator, and an information criterion to\nselect the numbers of groups and latent distributions. We also propose\nstructured grouping strategies by introducing penalties on grouping parameters\nin the likelihood function. Under the settings where both the number of\nclusters and cluster sizes tend to infinity, we present asymptotic properties\nof the maximum likelihood estimator and the information criterion. We\ndemonstrate the proposed method through simulation studies and an application\nto crime risk modeling in Tokyo.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 09:50:56 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 12:29:32 GMT"}, {"version": "v3", "created": "Fri, 7 Feb 2020 04:39:10 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Sugasawa", "Shonosuke", ""]]}, {"id": "1804.01049", "submitter": "Madeline Ausdemore", "authors": "Madeline Ausdemore and Cedric Neumann and Christopher Saunders and\n  Douglas Armstrong and Cyril Muehlethaler", "title": "Two-stage approach for the inference of the source of high-dimension and\n  complex chemical data in forensic science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forensic scientists are often criticised for the lack of quantitative support\nfor the conclusions of their examinations. While scholars advocate for the use\nof a Bayes factor to quantify the weight of forensic evidence, it is often\nimpossible to assign the necessary probability measures to perform\nlikelihood-based inference for high-dimensional and complex data. To address\nthis issue, we revisit a two-stage inference framework and leverage the\nproperties of kernel functions to offer a method that allows for statistically\nsupporting the inference of the identity of source of sets of trace and control\nobjects by way of a single test. Our method is generic in that it can be easily\ntailored to any type of data encountered in forensic science or pattern\nrecognition, and our method does not depend on the dimension or the type of the\nconsidered data. The application of our method to paint evidence shows that\nthis type of evidence carries substantial probative value. Finally, our\napproach can easily be extended to other evidence types such as glass, fibres\nand dust.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 16:04:07 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 16:40:20 GMT"}, {"version": "v3", "created": "Fri, 11 Jan 2019 03:58:10 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Ausdemore", "Madeline", ""], ["Neumann", "Cedric", ""], ["Saunders", "Christopher", ""], ["Armstrong", "Douglas", ""], ["Muehlethaler", "Cyril", ""]]}, {"id": "1804.01054", "submitter": "Kengo Nagashima", "authors": "Kengo Nagashima, Hisashi Noma, Toshi A. Furukawa", "title": "Prediction intervals for random-effects meta-analysis: a confidence\n  distribution approach", "comments": "14 pages, 3 figures", "journal-ref": "Statistical Methods in Medical Research 2019; 28(6): 1689-1702", "doi": "10.1177/0962280218773520", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the inference of random-effects models in meta-analysis, the prediction\ninterval was proposed as a summary measure of the treatment effects that\nexplains the heterogeneity in the target population. While the\nHiggins-Thompson-Spiegelhalter (HTS) plug-in-type prediction interval has been\nwidely used, in which the heterogeneity parameter is replaced with its point\nestimate, its validity depends on a large sample approximation. Most\nmeta-analyses, however, include less than 20 studies. It has been revealed that\nthe validity of the HTS method is not assured under realistic situations, but\nno solution to this problem has been proposed in literature. Therefore, in this\narticle, we describe our proposed prediction interval. Instead of using the\nplug-in scheme, we developed a bootstrap approach using an exact confidence\ndistribution to account for the uncertainty in estimation of the heterogeneity\nparameter. Compared to the HTS method, the proposed method provides an accurate\nprediction interval that adequately explains the heterogeneity of treatment\neffects and the statistical error. Simulation studies demonstrated that the HTS\nmethod had poor coverage performance; by contrast, the coverage probabilities\nfor the proposed method satisfactorily retained the nominal level. Applications\nto three published random-effects meta-analyses are presented.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 16:12:32 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 15:00:42 GMT"}, {"version": "v3", "created": "Thu, 10 May 2018 18:09:01 GMT"}, {"version": "v4", "created": "Thu, 13 Jun 2019 03:45:47 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Nagashima", "Kengo", ""], ["Noma", "Hisashi", ""], ["Furukawa", "Toshi A.", ""]]}, {"id": "1804.01201", "submitter": "Wenhao Hu", "authors": "Wenhao Hu, Eric Laber, Leonard Stefanski", "title": "Variable selection using pseudo-variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression has become a standard tool for model building across a\nwide range of application domains. Common practice is to tune the amount of\npenalization to tradeoff bias and variance or to optimize some other measure of\nperformance of the estimated model. An advantage of such automated\nmodel-building procedures is that their operating characteristics are\nwell-defined, i.e., completely data-driven, and thereby they can be\nsystematically studied. However, in many applications it is desirable to\nincorporate domain knowledge into the model building process; one way to do\nthis is to characterize each model along the solution path of a penalized\nregression estimator in terms of an operating characteristic that is meaningful\nwithin a domain context and then to allow domain experts to choose from among\nthese models using these operating characteristics as well as other factors not\navailable to the estimation algorithm. We derive an estimator of the false\nselection rate for each model along the solution path using a novel variable\naddition method. The proposed estimator applies to both fixed and random\ndesigns and allows for $p \\gg n$. The proposed estimator can be used to\nestimate a model with a pre-specified false selection rate or can be overlaid\non the solution path to facilitate interactive model exploration. We\ncharacterize the asymptotic behavior of the proposed estimator in the case of a\nlinear model under a fixed design; however, simulation experiments show that\nthe proposed estimator provides consistently more accurate estimates of the\nfalse selection rate than competing methods across a wide range of models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 01:04:40 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Hu", "Wenhao", ""], ["Laber", "Eric", ""], ["Stefanski", "Leonard", ""]]}, {"id": "1804.01208", "submitter": "Jonathan Roth", "authors": "Jonathan Roth", "title": "Should We Adjust for the Test for Pre-trends in Difference-in-Difference\n  Designs?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common practice in difference-in-difference (DiD) designs is to check for\nparallel trends prior to treatment assignment, yet typical estimation and\ninference does not account for the fact that this test has occurred. I analyze\nthe properties of the traditional DiD estimator conditional on having passed\n(i.e. not rejected) the test for parallel pre-trends. When the DiD design is\nvalid and the test for pre-trends confirms it, the typical DiD estimator is\nunbiased, but traditional standard errors are overly conservative.\nAdditionally, there exists an alternative unbiased estimator that is more\nefficient than the traditional DiD estimator under parallel trends. However,\nwhen in population there is a non-zero pre-trend but we fail to reject the\nhypothesis of parallel pre-trends, the DiD estimator is generally biased\nrelative to the population DiD coefficient. Moreover, if the trend is monotone,\nthen under reasonable assumptions the bias from conditioning exacerbates the\nbias relative to the true treatment effect. I propose new estimation and\ninference procedures that account for the test for parallel trends, and compare\ntheir performance to that of the traditional estimator in a Monte Carlo\nsimulation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 01:54:37 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 00:01:57 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Roth", "Jonathan", ""]]}, {"id": "1804.01268", "submitter": "Carina Gerstenberger", "authors": "Carina Gerstenberger", "title": "Robust Discrimination between Long-Range Dependence and a Change in Mean", "comments": "34 pages, 5 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a robust to outliers Wilcoxon change-point testing\nprocedure, for distinguishing between short-range dependent time series with a\nchange in mean at unknown time and stationary long-range dependent time series.\nWe establish the asymptotic distribution of the test statistic under the null\nhypothesis for $L_1$ near epoch dependent processes and show its consistency\nunder the alternative. The Wilcoxon-type testing procedure similarly as the\nCUSUM-type testing procedure of Berkes, Horv\\'ath, Kokoszka and Shao (2006),\nrequires estimation of the location of a possible change-point, and then using\npre- and post-break subsamples to discriminate between short and long-range\ndependence. A simulation study examines the empirical size and power of the\nWilcoxon-type testing procedure in standard cases and with disturbances by\noutliers. It shows that in standard cases the Wilcoxon-type testing procedure\nbehaves equally well as the CUSUM-type testing procedure but outperforms it in\npresence of outliers. We also apply both testing procedure to hydrologic data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 07:29:43 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 10:43:21 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Gerstenberger", "Carina", ""]]}, {"id": "1804.01269", "submitter": "Debasis Kundu Professor", "authors": "Rhythm Grover and Debasis Kundu and Amit Mitra", "title": "On approximate least squares estimators of parameters on one-dimensional\n  chirp signal", "comments": "Going to appear in Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chirp signals are quite common in many natural and man-made systems like\naudio signals, sonar, radar etc. Estimation of the unknown parameters of a\nsignal is a fundamental problem in statistical signal processing. Recently,\nKundu and Nandi \\cite{2008} studied the asymptotic properties of least squares\nestimators of the unknown parameters of a simple chirp signal model under the\nassumption of stationary noise. In this paper, we propose periodogram-type\nestimators called the approximate least squares estimators to estimate the\nunknown parameters and study the asymptotic properties of these estimators\nunder the same error assumptions. It is observed that the approximate least\nsquares estimators are strongly consistent and asymptotically equivalent to the\nleast squares estimators. Similar to the periodogram estimators, these\nestimators can also be used as initial guesses to find the least squares\nestimators of the unknown parameters. We perform some numerical simulations to\nsee the performance of the proposed estimators and compare them with the least\nsquares estimators and the estimators proposed by Lahiri et al., \\cite{2013}.\nWe have analysed two real data sets for illustrative purposes.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 07:30:05 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Grover", "Rhythm", ""], ["Kundu", "Debasis", ""], ["Mitra", "Amit", ""]]}, {"id": "1804.01440", "submitter": "Tobias Kley", "authors": "Stefan Birr, Tobias Kley, Stanislav Volgushev", "title": "Model assessment for time series dynamics using copula spectral\n  densities: a graphical tool", "comments": "paper (20 pages) and online supplement (9 pages), 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding parametric models that accurately describe the dependence structure\nof observed data is a central task in the analysis of time series. Classical\nfrequency domain methods provide a popular set of tools for fitting and\ndiagnostics of time series models, but their applicability is seriously\nimpacted by the limitations of covariances as a measure of dependence.\nMotivated by recent developments of frequency domain methods that are based on\ncopulas instead of covariances, we propose a novel graphical tool that allows\nto access the quality of time series models for describing dependencies that go\nbeyond linearity. We provide a thorough theoretical justification of our\napproach and show in simulations that it can successfully distinguish between\nsubtle differences of time series dynamics, including non-linear dynamics which\nresult from GARCH and EGARCH models. We also demonstrate the utility of the\nproposed tools through an application to modeling returns of the S&P 500 stock\nmarket index.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 14:38:16 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 09:45:56 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Birr", "Stefan", ""], ["Kley", "Tobias", ""], ["Volgushev", "Stanislav", ""]]}, {"id": "1804.01454", "submitter": "Fabio M. Bayer Ph.D", "authors": "F\\'abio Mariano Bayer, Catia Michele Tondolo, Fernanda Maria M\\\"uller", "title": "Beta regression control chart for monitoring fractions and proportions", "comments": "Accepted paper", "journal-ref": "Computers \\& Industrial Engineering, 2018", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression control charts are usually used to monitor variables of interest\nthat are related to control variables. However, for fraction and/or proportion\ndata, the use of standard regression control charts may not be adequate, since\nthe linear regression model assumes the normality of the interest variable. To\nwork around this problem, we propose the beta regression control chart (BRCC).\nThe BRCC is useful for monitoring fraction, rate and/or proportion data sets\nwhen they are related to control variables. The proposed control chart assumes\nthat the mean and dispersion parameters of beta distributed variables are\nrelated to the exogenous variables, being modeled using regression structures.\nThe BRCC is numerically assessed through an extensive Monte Carlo simulation\nstudy, showing good performance in terms of average run length (ARL). Two\napplications to real data are presented, evidencing the practical applicability\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 15:05:36 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Bayer", "F\u00e1bio Mariano", ""], ["Tondolo", "Catia Michele", ""], ["M\u00fcller", "Fernanda Maria", ""]]}, {"id": "1804.01458", "submitter": "Sutanoy Dasgupta", "authors": "Sutanoy Dasgupta, Debdeep Pati, Ian H. Jermyn and Anuj Srivastava", "title": "Shape-Constrained Univariate Density Estimation", "comments": "31 pages, Initial version. Presented at IISA 2017, Shape Constrained\n  Methods Workshp, BIRS,2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the problem of estimating a probability density function (pdf) from its\nobservations is classical, the estimation under additional shape constraints is\nboth important and challenging. We introduce an efficient, geometric approach\nfor estimating pdfs given the number of its modes. This approach explores the\nspace of constrained pdf's using an action of the diffeomorphism group that\npreserves their shapes. It starts with an initial template, with the desired\nnumber of modes and arbitrarily chosen heights at the critical points, and\ntransforms it via: (1) composition by diffeomorphisms and (2) normalization to\nobtain the final density estimate. The search for optimal diffeomorphism is\nperformed under the maximum-likelihood criterion and is accomplished by mapping\ndiffeomorphisms to the tangent space of a Hilbert sphere, a vector space whose\nelements can be expressed using an orthogonal basis. This framework is first\napplied to shape-constrained univariate, unconditional pdf estimation and then\nextended to conditional pdf estimation. We derive asymptotic convergence rates\nof the estimator and demonstrate this approach using a synthetic dataset\ninvolving speed distribution for different traffic flow on Californian\ndriveways.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 15:13:24 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Dasgupta", "Sutanoy", ""], ["Pati", "Debdeep", ""], ["Jermyn", "Ian H.", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1804.01528", "submitter": "Mikael Escobar-Bach", "authors": "Mikael Escobar-Bach and Ingrid Van Keilegom", "title": "Non-parametric cure rate estimation under insufficient follow-up using\n  extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important research topic in survival analysis is related to the modeling\nand estimation of the cure rate, i.e. the proportion of subjects that will\nnever experience the event of interest. However, most estimation methods\nproposed so far in the literature do not handle the case of insufficient\nfollow-up, that is when the right end point of the support of the censoring\ntime is strictly less than that of the survival time of the susceptible\nsubjects, and consequently these estimators overestimate the cure rate in that\ncase. We fill this gap by proposing a new estimator of the cure rate that makes\nuse of extrapolation techniques from the area of extreme value theory. We\nestablish the asymptotic normality of the proposed estimator, and show how the\nestimator works for small samples by means of a simulation study. We also\nillustrate its practical applicability through the analysis of data on the\nsurvival of breast cancer patients.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 14:39:29 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Escobar-Bach", "Mikael", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "1804.01618", "submitter": "Brittany Terese Fasy", "authors": "Eric Berry, Yen-Chi Chen, Jessi Cisewski-Kehe, Brittany Terese Fasy", "title": "Functional Summaries of Persistence Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary areas of interest in applied algebraic topology is\npersistent homology, and, more specifically, the persistence diagram.\nPersistence diagrams have also become objects of interest in topological data\nanalysis. However, persistence diagrams do not naturally lend themselves to\nstatistical goals, such as inferring certain population characteristics,\nbecause their complicated structure makes common algebraic operations--such as\naddition, division, and multiplication-- challenging (e.g., the mean might not\nbe unique). To bypass these issues, several functional summaries of persistence\ndiagrams have been proposed in the literature (e.g. landscape and silhouette\nfunctions). The problem of analyzing a set of persistence diagrams then becomes\nthe problem of analyzing a set of functions, which is a topic that has been\nstudied for decades in statistics. First, we review the various functional\nsummaries in the literature and propose a unified framework for the functional\nsummaries. Then, we generalize the definition of persistence landscape\nfunctions, establish several theoretical properties of the persistence\nfunctional summaries, and demonstrate and discuss their performance in the\ncontext of classification using simulated prostate cancer histology data, and\ntwo-sample hypothesis tests comparing human and monkey fibrin images, after\ndeveloping a simulation study using a new data generator we call the Pickup\nSticks Simulator (STIX).\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 22:18:39 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Berry", "Eric", ""], ["Chen", "Yen-Chi", ""], ["Cisewski-Kehe", "Jessi", ""], ["Fasy", "Brittany Terese", ""]]}, {"id": "1804.01631", "submitter": "Sami Stouli", "authors": "Richard Spady, Sami Stouli", "title": "Simultaneous Mean-Variance Regression", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose simultaneous mean-variance regression for the linear estimation\nand approximation of conditional mean functions. In the presence of\nheteroskedasticity of unknown form, our method accounts for varying dispersion\nin the regression outcome across the support of conditioning variables by using\nweights that are jointly determined with the mean regression parameters.\nSimultaneity generates outcome predictions that are guaranteed to improve over\nordinary least-squares prediction error, with corresponding parameter standard\nerrors that are automatically valid. Under shape misspecification of the\nconditional mean and variance functions, we establish existence and uniqueness\nof the resulting approximations and characterize their formal interpretation\nand robustness properties. In particular, we show that the corresponding\nmean-variance regression location-scale model weakly dominates the ordinary\nleast-squares location model under a Kullback-Leibler measure of divergence,\nwith strict improvement in the presence of heteroskedasticity. The simultaneous\nmean-variance regression loss function is globally convex and the corresponding\nestimator is easy to implement. We establish its consistency and asymptotic\nnormality under misspecification, provide robust inference methods, and present\nnumerical simulations that show large improvements over ordinary and weighted\nleast-squares in terms of estimation and inference in finite samples. We\nfurther illustrate our method with two empirical applications to the estimation\nof the relationship between economic prosperity in 1500 and today, and demand\nfor gasoline in the United States.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 00:23:02 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 23:36:59 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Spady", "Richard", ""], ["Stouli", "Sami", ""]]}, {"id": "1804.01864", "submitter": "Shogo Nakakita", "authors": "Shogo H. Nakakita, Masayuki Uchida", "title": "Adaptive test for ergodic diffusions plus noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose some parametric tests for ergodic diffusion-plus-noise model,\nwhich is a version of state-space modelling in statistics for stochastic\ndiffusion equations. The test statistics are classified into three types:\nlikelihood-ratio-type test statistic; Wald-type one; and Rao-type one. All the\ntest statistics are constructed with quasi-likelihood-functions for local mean\nsequence of noised observation. We also simulate the behaviour of them for\nseveral practical hypothesis tests and check the convergence in law of test\nstatistics under null hypotheses and consistency of the test under alternative\nones. We apply the method for real data analysis of wind data, and examine some\nsets of the hypotheses mainly with respect to the structure of diffusion\ncoefficient.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 14:10:03 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Nakakita", "Shogo H.", ""], ["Uchida", "Masayuki", ""]]}, {"id": "1804.01932", "submitter": "Justin Kinney", "authors": "Wei-Chia Chen, Ammar Tareen, Justin B. Kinney", "title": "Density estimation on small datasets", "comments": "Includes main text (5 pages, 3 figures) and Supplemental Information\n  (10 pages, 4 figures). Same as version 3 but with Feynman diagrams properly\n  rendered", "journal-ref": "Phys. Rev. Lett. 121, 160605 (2018)", "doi": "10.1103/PhysRevLett.121.160605", "report-no": null, "categories": "physics.data-an cs.NA physics.comp-ph q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How might a smooth probability distribution be estimated, with accurately\nquantified uncertainty, from a limited amount of sampled data? Here we describe\na field-theoretic approach that addresses this problem remarkably well in one\ndimension, providing an exact nonparametric Bayesian posterior without relying\non tunable parameters or large-data approximations. Strong non-Gaussian\nconstraints, which require a non-perturbative treatment, are found to play a\nmajor role in reducing distribution uncertainty. A software implementation of\nthis method is provided.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 16:12:37 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 13:30:49 GMT"}, {"version": "v3", "created": "Tue, 28 Aug 2018 00:57:03 GMT"}, {"version": "v4", "created": "Thu, 30 Aug 2018 01:04:09 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Chen", "Wei-Chia", ""], ["Tareen", "Ammar", ""], ["Kinney", "Justin B.", ""]]}, {"id": "1804.02089", "submitter": "Matthew Pratola", "authors": "Matthew T. Pratola, C. Devon Lin and Peter F. Craigmile", "title": "Optimal Design Emulators: A Point Process Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design of experiments is a fundamental topic in applied statistics with a\nlong history. Yet its application is often limited by the complexity and\ncostliness of constructing experimental designs in the first place, which\ninvolve searching a high-dimensional input space and evaluating computationally\nexpensive criterion functions. In this work, we introduce a novel approach to\nthe challenging design problem. We will take a probabilistic view of the\nproblem by representing the optimal design as being one element (or a subset of\nelements) of a probability space. Given a suitable distribution on this space,\na generative point process can be specified from which stochastic design\nrealizations can be drawn. The appropriate class of point processes is\nmotivated by exploring a connection to Latin Hypercube designs. We then\ndescribe a scenario where the classical (point estimate) entropy-optimal design\nfor Gaussian Process regression coincides with the mode of a particular point\nprocess. We conclude with outlining an algorithm for drawing such design\nrealizations, its extension to sequential design, and applying the techniques\ndeveloped to constructing space-filling designs for Stochastic Gradient Descent\nand entropy designs for Gaussian process regression.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 00:29:42 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 17:45:37 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Pratola", "Matthew T.", ""], ["Lin", "C. Devon", ""], ["Craigmile", "Peter F.", ""]]}, {"id": "1804.02090", "submitter": "Maria DeYoreo", "authors": "Carolyn Rutter, Jonathan Ozik, Maria DeYoreo, Nicholson Collier", "title": "Microsimulation Model Calibration using Incremental Mixture Approximate\n  Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microsimulation models (MSMs) are used to predict population-level effects of\nhealth care policies by simulating individual-level outcomes. Simulated\noutcomes are governed by unknown parameters that are chosen so that the model\naccurately predicts specific targets, a process referred to as model\ncalibration. Calibration targets can come from randomized controlled trials,\nobservational studies, and expert opinion, and are typically summary\nstatistics. A well calibrated model can reproduce a wide range of targets. MSM\ncalibration generally involves searching a high dimensional parameter space and\npredicting many targets through model simulation. This requires efficient\nmethods for exploring the parameter space and sufficient computational\nresources. We develop Incremental Mixture Approximate Bayesian Computation\n(IMABC) as a method for MSM calibration and implement it via a high-performance\ncomputing workflow, which provides the necessary computational scale. IMABC\nbegins with a rejection-based approximate Bayesian computation (ABC) step,\ndrawing a sample of parameters from the prior distribution and simulating\ncalibration targets. Next, the sample is iteratively updated by drawing\nadditional points from a mixture of multivariate normal distributions, centered\nat the points that yield simulated targets that are near observed targets.\nPosterior estimates are obtained by weighting sampled parameter vectors to\naccount for the adaptive sampling scheme. We demonstrate IMABC by calibrating a\nMSM for the natural history of colorectal cancer to obtain simulated draws from\nthe joint posterior distribution of model parameters.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 00:31:11 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 20:10:36 GMT"}, {"version": "v3", "created": "Mon, 13 Aug 2018 16:40:48 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Rutter", "Carolyn", ""], ["Ozik", "Jonathan", ""], ["DeYoreo", "Maria", ""], ["Collier", "Nicholson", ""]]}, {"id": "1804.02097", "submitter": "Luwan Zhang", "authors": "Luwan Zhang, Katherine Liao, Issac Kohane, Tianxi Cai", "title": "Multi-view Banded Spectral Clustering with Application to ICD9\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent development in methodology, community detection remains a\nchallenging problem. Existing literature largely focuses on the standard\nsetting where a network is learned using an observed adjacency matrix from a\nsingle data source. Constructing a shared network from multiple data sources is\nmore challenging due to the heterogeneity across populations. Additionally, no\nexisting method leverages the prior distance knowledge available in many\ndomains to help the discovery of the network structure. To bridge this gap, in\nthis paper we propose a novel spectral clustering method that optimally\ncombines multiple data sources while leveraging the prior distance knowledge.\nThe proposed method combines a banding step guided by the distance knowledge\nwith a subsequent weighting step to maximize consensus across multiple sources.\nIts statistical performance is thoroughly studied under a multi-view stochastic\nblock model. We also provide a simple yet optimal rule of choosing weights in\npractice. The efficacy and robustness of the method is fully demonstrated\nthrough extensive simulations. Finally, we apply the method to cluster the\nInternational classification of diseases, ninth revision (ICD9), codes and\nyield a very insightful clustering structure by integrating information from a\nlarge claim database and two healthcare systems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 01:02:25 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 15:03:05 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Zhang", "Luwan", ""], ["Liao", "Katherine", ""], ["Kohane", "Issac", ""], ["Cai", "Tianxi", ""]]}, {"id": "1804.02253", "submitter": "Konstantin Eckle", "authors": "Konstantin Eckle, Johannes Schmidt-Hieber", "title": "A comparison of deep networks with ReLU activation function and linear\n  spline-type methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) generate much richer function spaces than shallow\nnetworks. Since the function spaces induced by shallow networks have several\napproximation theoretic drawbacks, this explains, however, not necessarily the\nsuccess of deep networks. In this article we take another route by comparing\nthe expressive power of DNNs with ReLU activation function to piecewise linear\nspline methods. We show that MARS (multivariate adaptive regression splines) is\nimproper learnable by DNNs in the sense that for any given function that can be\nexpressed as a function in MARS with $M$ parameters there exists a multilayer\nneural network with $O(M \\log (M/\\varepsilon))$ parameters that approximates\nthis function up to sup-norm error $\\varepsilon.$ We show a similar result for\nexpansions with respect to the Faber-Schauder system. Based on this, we derive\nrisk comparison inequalities that bound the statistical risk of fitting a\nneural network by the statistical risk of spline-based methods. This shows that\ndeep networks perform better or only slightly worse than the considered spline\nmethods. We provide a constructive proof for the function approximations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 13:28:15 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 09:11:59 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Eckle", "Konstantin", ""], ["Schmidt-Hieber", "Johannes", ""]]}, {"id": "1804.02274", "submitter": "Riccardo Rastelli", "authors": "Riccardo Rastelli, Florian Maire and Nial Friel", "title": "Computationally efficient inference for latent position network models", "comments": "39 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent position models are widely used for the analysis of networks in a\nvariety of research fields. In fact, these models possess a number of desirable\ntheoretical properties, and are particularly easy to interpret. However,\nstatistical methodologies to fit these models generally incur a computational\ncost which grows with the square of the number of nodes in the graph. This\nmakes the analysis of large social networks impractical. In this paper, we\npropose a new method characterised by a linear computational complexity, which\ncan be used to fit latent position models on networks of several tens of\nthousands nodes. Our approach relies on an approximation of the likelihood\nfunction, where the amount of noise introduced by the approximation can be\narbitrarily reduced at the expense of computational efficiency. We establish\nseveral theoretical results that show how the likelihood error propagates to\nthe invariant distribution of the Markov chain Monte Carlo sampler. In\nparticular, we demonstrate that one can achieve a substantial reduction in\ncomputing time and still obtain a good estimate of the latent structure.\nFinally, we propose applications of our method to simulated networks and to a\nlarge coauthorships network, highlighting the usefulness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 13:55:44 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 14:45:42 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Rastelli", "Riccardo", ""], ["Maire", "Florian", ""], ["Friel", "Nial", ""]]}, {"id": "1804.02348", "submitter": "Ke Zhu", "authors": "Ke Zhu", "title": "Statistical inference for autoregressive models under heteroscedasticity\n  of unknown form", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an entire inference procedure for the autoregressive\nmodel under (conditional) heteroscedasticity of unknown form with a finite\nvariance. We first establish the asymptotic normality of the weighted least\nabsolute deviations estimator (LADE) for the model. Second, we develop the\nrandom weighting (RW) method to estimate its asymptotic covariance matrix,\nleading to the implementation of the Wald test. Third, we construct a\nportmanteau test for model checking, and use the RW method to obtain its\ncritical values. As a special weighted LADE, the feasible adaptive LADE (ALADE)\nis proposed and proved to have the same efficiency as its infeasible\ncounterpart. The importance of our entire methodology based on the feasible\nALADE is illustrated by simulation results and the real data analysis on three\nU.S. economic data sets.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 16:33:08 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 03:36:34 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Zhu", "Ke", ""]]}, {"id": "1804.02461", "submitter": "Riccardo Rastelli", "authors": "Nial Friel and Riccardo Rastelli", "title": "Discussion of the article \"Bayesian cluster analysis: point estimation\n  and credible balls\" by Wade and Ghahramani", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a discussion of the paper \"Bayesian cluster analysis: point\nestimation and credible balls\" by Wade and Ghahramani. We believe that this\npaper contributes substantially to the literature on Bayesian clustering by\nfilling in an important methodological gap, by providing a means to assess the\nuncertainty around a point estimate of the optimal clustering solution based on\na given loss function. In our discussion we reflect on the characterisation of\nuncertainty around the Bayesian optimal partition, revealing other possible\nalternatives that may be viable. In addition, we suggest other important\nextensions of the approach proposed which may lead to wider applicability.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 21:27:42 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Friel", "Nial", ""], ["Rastelli", "Riccardo", ""]]}, {"id": "1804.02499", "submitter": "Min Tsao Dr.", "authors": "Min Tsao", "title": "A group-based approach to the least squares regression for handling\n  multicollinearity from strongly correlated variables", "comments": "36 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multicollinearity due to strongly correlated predictor variables is a\nlong-standing problem in regression analysis. It leads to difficulties in\nparameter estimation, inference, variable selection and prediction for the\nleast squares regression. To deal with these difficulties, we propose a\ngroup-based approach to the least squares regression centered on the collective\nimpact of the strongly correlated variables. We discuss group effects of such\nvariables that represent their collective impact, and present the group-based\napproach through real and simulated data examples. We also give a condition\nmore precise than what is available in the literature under which predictions\nby the least squares estimated model are accurate. This approach is a natural\nway of working with multicollinearity which resolves the difficulties without\naltering the least squares method. It has several advantages over alternative\nmethods such as ridge regression and principal component regression.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 03:16:41 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 09:15:38 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Tsao", "Min", ""]]}, {"id": "1804.02502", "submitter": "Cesar H Comin Prof.", "authors": "Felipe L. Gewers, Gustavo R. Ferreira, Henrique F. de Arruda, Filipi\n  N. Silva, Cesar H. Comin, Diego R. Amancio, Luciano da F. Costa", "title": "Principal Component Analysis: A Natural Approach to Data Exploration", "comments": null, "journal-ref": "ACM Computing Surveys (CSUR), 54(4), pp.1-34 (2021)", "doi": "10.1145/3447755", "report-no": null, "categories": "cs.CE stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is often used for analyzing data in the\nmost diverse areas. In this work, we report an integrated approach to several\ntheoretical and practical aspects of PCA. We start by providing, in an\nintuitive and accessible manner, the basic principles underlying PCA and its\napplications. Next, we present a systematic, though no exclusive, survey of\nsome representative works illustrating the potential of PCA applications to a\nwide range of areas. An experimental investigation of the ability of PCA for\nvariance explanation and dimensionality reduction is also developed, which\nconfirms the efficacy of PCA and also shows that standardizing or not the\noriginal data can have important effects on the obtained results. Overall, we\nbelieve the several covered issues can assist researchers from the most diverse\nareas in using and interpreting PCA.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 03:48:49 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 13:20:08 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gewers", "Felipe L.", ""], ["Ferreira", "Gustavo R.", ""], ["de Arruda", "Henrique F.", ""], ["Silva", "Filipi N.", ""], ["Comin", "Cesar H.", ""], ["Amancio", "Diego R.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1804.02526", "submitter": "Anthony Ebert", "authors": "Anthony Ebert, Ritabrata Dutta, Kerrie Mengersen, Antonietta Mira,\n  Fabrizio Ruggeri, Paul Wu", "title": "Likelihood-free parameter estimation for dynamic queueing networks: case\n  study of passenger flow in an international airport terminal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic queueing networks (DQN) model queueing systems where demand varies\nstrongly with time, such as airport terminals. With rapidly rising global air\npassenger traffic placing increasing pressure on airport terminals, efficient\nallocation of resources is more important than ever. Parameter inference and\nquantification of uncertainty are key challenges for developing decision\nsupport tools. The DQN likelihood function is, in general, intractable and\ncurrent approaches to simulation make likelihood-free parameter inference\nmethods, such as approximate Bayesian computation (ABC), infeasible since\nsimulating from these models is computationally expensive. By leveraging a\nrecent advance in computationally efficient queueing simulation, we develop the\nfirst parameter inference approach for DQNs. We demonstrate our approach with\ndata of passenger flows in a real airport terminal, and we show that our model\naccurately recreates the behaviour of the system and is useful for decision\nsupport. Special care must be taken in developing the distance for ABC since\nany useful output must vary with time. We use maximum mean discrepancy, a\nmetric on probability measures, as the distance function for ABC. Prediction\nintervals of performance measures for decision support tools are easily\nconstructed using draws from posterior samples, which we demonstrate with a\nscenario of a delayed flight.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 07:51:23 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 05:33:37 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Ebert", "Anthony", ""], ["Dutta", "Ritabrata", ""], ["Mengersen", "Kerrie", ""], ["Mira", "Antonietta", ""], ["Ruggeri", "Fabrizio", ""], ["Wu", "Paul", ""]]}, {"id": "1804.02592", "submitter": "Jonas Wallin", "authors": "\\\"Ozg\\\"ur Asar, David Bolin, Peter J. Diggle and Jonas Wallin", "title": "Linear Mixed-Effects Models for Non-Gaussian Repeated Measurement Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the analysis of continuous repeated measurement outcomes that are\ncollected through time, also known as longitudinal data. A standard framework\nfor analysing data of this kind is a linear Gaussian mixed-effects model within\nwhich the outcome variable can be decomposed into fixed-effects, time-invariant\nand time-varying random-effects, and measurement noise. We develop methodology\nthat, for the first time, allows any combination of these stochastic components\nto be non-Gaussian, using multivariate Normal variance-mean mixtures. We\nestimate parameters by max- imum likelihood, implemented with a novel,\ncomputationally efficient stochastic gradient algorithm. We obtain standard\nerror estimates by inverting the observed Fisher-information matrix, and obtain\nthe predictive distributions for the random-effects in both filtering\n(conditioning on past and current data) and smoothing (conditioning on all\ndata) contexts. To implement these procedures, we intro- duce an R package,\nngme. We re-analyse two data-sets, from cystic fibrosis and nephrology\nresearch, that were previously analysed using Gaussian linear mixed effects\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 20:50:42 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Asar", "\u00d6zg\u00fcr", ""], ["Bolin", "David", ""], ["Diggle", "Peter J.", ""], ["Wallin", "Jonas", ""]]}, {"id": "1804.02605", "submitter": "Abhishek Chakrabortty", "authors": "Arun Kumar Kuchibhotla and Abhishek Chakrabortty", "title": "Moving Beyond Sub-Gaussianity in High-Dimensional Statistics:\n  Applications in Covariance Estimation and Linear Regression", "comments": "64 pages; Revised version (discussions added and some results\n  modified in Section 4, minor changes made throughout)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concentration inequalities form an essential toolkit in the study of high\ndimensional (HD) statistical methods. Most of the relevant statistics\nliterature in this regard is based on sub-Gaussian or sub-exponential tail\nassumptions. In this paper, we first bring together various probabilistic\ninequalities for sums of independent random variables under much weaker\nexponential type (namely sub-Weibull) tail assumptions. These results extract a\npart sub-Gaussian tail behavior in finite samples, matching the asymptotics\ngoverned by the central limit theorem, and are compactly represented in terms\nof a new Orlicz quasi-norm - the Generalized Bernstein-Orlicz norm - that\ntypifies such tail behaviors.\n  We illustrate the usefulness of these inequalities through the analysis of\nfour fundamental problems in HD statistics. In the first two problems, we study\nthe rate of convergence of the sample covariance matrix in terms of the maximum\nelementwise norm and the maximum k-sub-matrix operator norm which are key\nquantities of interest in bootstrap, HD covariance matrix estimation and HD\ninference. The third example concerns the restricted eigenvalue condition,\nrequired in HD linear regression, which we verify for all sub-Weibull random\nvectors through a unified analysis, and also prove a more general result\nrelated to restricted strong convexity in the process. In the final example, we\nconsider the Lasso estimator for linear regression and establish its rate of\nconvergence under much weaker than usual tail assumptions (on the errors as\nwell as the covariates), while also allowing for misspecified models and both\nfixed and random design. To our knowledge, these are the first such results for\nLasso obtained in this generality. The common feature in all our results over\nall the examples is that the convergence rates under most exponential tails\nmatch the usual ones under sub-Gaussian assumptions.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 00:27:45 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 01:40:10 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 20:56:42 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""], ["Chakrabortty", "Abhishek", ""]]}, {"id": "1804.02784", "submitter": "Jingchen Hu", "authors": "Jingchen Hu", "title": "Bayesian Estimation of Attribute and Identification Disclosure Risks in\n  Synthetic Data", "comments": null, "journal-ref": "Transactions on Data Privacy, 12:1, 61-89 (2019)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The synthetic data approach to data confidentiality has been actively\nresearched on, and for the past decade or so, a good number of high quality\nwork on developing innovative synthesizers, creating appropriate utility\nmeasures and risk measures, among others, have been published. Comparing to a\nlarge volume of work on synthesizers development and utility measures creation,\nmeasuring risks has overall received less attention. This paper focuses on the\ndetailed construction of some Bayesian methods proposed for estimating\ndisclosure risks in synthetic data. In the processes of presenting attribute\nand identification disclosure risks evaluation methods, we highlight key steps,\nemphasize Bayesian thinking, illustrate with real application examples, and\ndiscuss challenges and future research directions. We hope to give the readers\na comprehensive view of the Bayesian estimation procedures, enable synthetic\ndata researchers and producers to use these procedures to evaluate disclosure\nrisks, and encourage more researchers to work in this important growing field.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 01:17:47 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 17:47:53 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 18:36:47 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Hu", "Jingchen", ""]]}, {"id": "1804.02905", "submitter": "Juan A. Cuesta-Albertos", "authors": "E. del Barrio, J.A. Cuesta-Albertos and C. Matran", "title": "Some indices to measure departures from stochastic order", "comments": "38 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with three famous statistics involved on two-sample\nproblems. The Mann-Whitney, the one-sided Kolmogorov-Smirnov, and the Galton\nrank order statistics are invoked here in an unusual way. Looking for indices\nto capture the disagreement of stochastic dominance of a distribution function\n$G$ over another $F$, we resort to suitable couplings $(X,Y)$ of random\nvariables with marginal distribution functions $F$ and $G$. We show as, the\ncommon representation, $P(X>Y)$ under the independent, the contamination and\nthe quantile frameworks give interpretable indices, whose plugin sample-based\nversions lead to these widely known statistics and can be used for statistical\nvalidation of approximate stochastic dominance. This supplies a workaround to\nthe non-viable statistical problem of validating stochastic dominance on the\nbasis of two samples. While the available literature on the asymptotics for the\nfirst and second statistics justifies their use for this task, for the Galton\nstatistic the existent results just cover the case where both distributions\ncoincide at a large extent or the case of distribution functions with only one\ncrossing point. In the paper we provide new findings, giving a full picture of\nthe complex behaviour of the Galton statistic: the time that a sample quantile\nfunction spent below another. We illustrate the performance of this index\nthrough simulations and discuss its application in a case study on the\nimprovement of household wealth distribution in Spain over the period around\nthe recent financial crisis\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 10:46:02 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["del Barrio", "E.", ""], ["Cuesta-Albertos", "J. A.", ""], ["Matran", "C.", ""]]}, {"id": "1804.02921", "submitter": "Lisa Schlosser", "authors": "Lisa Schlosser, Torsten Hothorn, Reto Stauffer, Achim Zeileis", "title": "Distributional Regression Forests for Probabilistic Precipitation\n  Forecasting in Complex Terrain", "comments": null, "journal-ref": "The Annals of Applied Statistics, Volume 13, Number 3 (2019),\n  1564-1589", "doi": "10.1214/19-AOAS1247", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To obtain a probabilistic model for a dependent variable based on some set of\nexplanatory variables, a distributional approach is often adopted where the\nparameters of the distribution are linked to regressors. In many classical\nmodels this only captures the location of the distribution but over the last\ndecade there has been increasing interest in distributional regression\napproaches modeling all parameters including location, scale, and shape.\nNotably, so-called non-homogeneous Gaussian regression (NGR) models both mean\nand variance of a Gaussian response and is particularly popular in weather\nforecasting. Moreover, generalized additive models for location, scale, and\nshape (GAMLSS) provide a framework where each distribution parameter is modeled\nseparately capturing smooth linear or nonlinear effects. However, when variable\nselection is required and/or there are non-smooth dependencies or interactions\n(especially unknown or of high-order), it is challenging to establish a good\nGAMLSS. A natural alternative in these situations would be the application of\nregression trees or random forests but, so far, no general distributional\nframework is available for these. Therefore, a framework for distributional\nregression trees and forests is proposed that blends regression trees and\nrandom forests with classical distributions from the GAMLSS framework as well\nas their censored or truncated counterparts. To illustrate these novel\napproaches in practice, they are employed to obtain probabilistic precipitation\nforecasts at numerous sites in a mountainous region based on a large number of\nnumerical weather prediction quantities. It is shown that the novel\ndistributional regression forests automatically select variables and\ninteractions, performing on par or often even better than GAMLSS specified\neither through prior meteorological knowledge or a computationally more\ndemanding boosting approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 11:36:30 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 09:54:36 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 14:45:49 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Schlosser", "Lisa", ""], ["Hothorn", "Torsten", ""], ["Stauffer", "Reto", ""], ["Zeileis", "Achim", ""]]}, {"id": "1804.03015", "submitter": "German A. Schnaidt Grez", "authors": "German A. Schnaidt Grez and Brani Vidakovic", "title": "Least Squares Wavelet-based Estimation for Additive Regression Models\n  using Non Equally-Spaced Designs", "comments": "arXiv admin note: text overlap with arXiv:1803.04558", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive regression models are actively researched in the statistical field\nbecause of their usefulness in the analysis of responses determined by\nnon-linear relationships with multivariate predictors. In this kind of\nstatistical models, the response depends linearly on unknown functions of\npredictor variables and typically, the goal of the analysis is to make\ninference about these functions.\n  In this paper, we study the problem of additive regression using a least\nsquares approach based on periodic orthogonal wavelets on the interval [0,1].\nFor this estimator, we obtain strong consistency (with respect to the\n$\\mathbb{L}_{2}$ norm) characterized by optimal convergence rates up to a\nlogarithmic factor, independent of the dimensionality of the problem. This is\nachieved by truncating the model estimates by a properly chosen parameter, and\nselecting the multiresolution level $J$ used for the wavelet expansion, as a\nfunction of the sample size. In this approach, we obtain these results without\nthe assumption of an equispaced design, a condition that is typically assumed\nin most wavelet-based procedures.\n  Finally, we show practical results obtained from a simulation study and a\nreal life application, demonstrating the applicability of the proposed methods\nfor the problem of non-linear robust additive regression models.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 15:29:00 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Grez", "German A. Schnaidt", ""], ["Vidakovic", "Brani", ""]]}, {"id": "1804.03016", "submitter": "Toni Karvonen", "authors": "Toni Karvonen, Chris J. Oates, Simo S\\\"arkk\\\"a", "title": "A Bayes-Sard Cubature Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focusses on the formulation of numerical integration as an\ninferential task. To date, research effort has largely focussed on the\ndevelopment of Bayesian cubature, whose distributional output provides\nuncertainty quantification for the integral. However, the point estimators\nassociated to Bayesian cubature can be inaccurate and acutely sensitive to the\nprior when the domain is high-dimensional. To address these drawbacks we\nintroduce Bayes-Sard cubature, a probabilistic framework that combines the\nflexibility of Bayesian cubature with the robustness of classical cubatures\nwhich are well-established. This is achieved by considering a Gaussian process\nmodel for the integrand whose mean is a parametric regression model, with an\nimproper flat prior on each regression coefficient. The features in the\nregression model consist of test functions which are guaranteed to be exactly\nintegrated, with remaining degrees of freedom afforded to the non-parametric\npart. The asymptotic convergence of the Bayes-Sard cubature method is\nestablished and the theoretical results are numerically verified. In\nparticular, we report two orders of magnitude reduction in error compared to\nBayesian cubature in the context of a high-dimensional financial integral.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 14:21:08 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 12:27:59 GMT"}, {"version": "v3", "created": "Fri, 18 May 2018 14:04:25 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Karvonen", "Toni", ""], ["Oates", "Chris J.", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1804.03018", "submitter": "Linjun Zhang", "authors": "T. Tony Cai and Linjun Zhang", "title": "High-dimensional Linear Discriminant Analysis: Optimality, Adaptive\n  Algorithm, and Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to develop an optimality theory for linear discriminant\nanalysis in the high-dimensional setting. A data-driven and tuning free\nclassification rule, which is based on an adaptive constrained $\\ell_1$\nminimization approach, is proposed and analyzed. Minimax lower bounds are\nobtained and this classification rule is shown to be simultaneously rate\noptimal over a collection of parameter spaces. In addition, we consider\nclassification with incomplete data under the missing completely at random\n(MCR) model. An adaptive classifier with theoretical guarantees is introduced\nand optimal rate of convergence for high-dimensional linear discriminant\nanalysis under the MCR model is established. The technical analysis for the\ncase of missing data is much more challenging than that for the complete data.\nWe establish a large deviation result for the generalized sample covariance\nmatrix, which serves as a key technical tool and can be of independent\ninterest. An application to lung cancer and leukemia studies is also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 14:23:07 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Linjun", ""]]}, {"id": "1804.03105", "submitter": "Alex Chin", "authors": "Alex Chin", "title": "Central limit theorems via Stein's method for randomized experiments\n  under interference", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study conditions under which treatment effect estimators constructed under\nthe no-interference assumption in randomized experiments are asymptotically\nnormal in the presence of interference. We prove that the standard\nHorvitz-Thompson estimator is asymptotically normal under a restricted\ninterference condition characterized by limiting the degree of the dependency\ngraph. The amount of interference is allowed to grow with the population size.\nWe then provide a central limit theorem for the difference-in-means estimator\nthat can handle interference that exists between all pairs of units, provided\nmost of the interference is captured by a restricted-degree dependency graph.\nThe asymptotic variance admits a decomposition into two terms: (a) the variance\nthat is expected under no-interference and (b) the additional variance\ncontributed by interference. We propose a conservative variance estimator based\non this variance decomposition. The results arise as an application of Stein's\nmethod. For practitioners, our results show that standard estimators continue\nto exhibit normality in large sample sizes and that inference can be made\nrobust to mild forms of interference.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 17:11:37 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 23:40:51 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Chin", "Alex", ""]]}, {"id": "1804.03109", "submitter": "Xiaowei Yue", "authors": "Xiaowei Yue, Jin Gyu Park, Zhiyong Liang, Jianjun Shi", "title": "Tensor Mixed Effects Model with Applications in Nanomanufacturing\n  Inspection", "comments": "29 pages, 8 figures", "journal-ref": "Technometrics, 2019", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raman mapping technique has been used to perform in-line quality inspections\nof nanomanufacturing processes. In such an application, massive\nhigh-dimensional Raman mapping data with mixed effects is generated. In\ngeneral, fixed effects and random effects in the multi-array Raman data are\nassociated with different quality characteristics such as fabrication\nconsistency, uniformity, defects, et al. The existing tensor decomposition\nmethods cannot separate mixed effects, and existing mixed effects model can\nonly handle matrix data but not high-dimensional multi-array data. In this\npaper, we propose a tensor mixed effects (TME) model to analyze massive\nhigh-dimensional Raman mapping data with complex structure. The proposed TME\nmodel can (i) separate fixed effects and random effects in a tensor domain;\n(ii) explore the correlations along different dimensions; and (iii) realize\nefficient parameter estimation by a proposed iterative double Flip-Flop\nalgorithm. We also investigate the properties of the TME model, existence and\nidentifiability of parameter estimation. The numerical analysis demonstrates\nthe efficiency and accuracy of the parameter estimation in the TME model.\nConvergence and asymptotic properties are discussed in the simulation and\nsurrogate data analysis. The case study shows an application of the TME model\nin quantifying the influence of alignment on carbon nanotubes buckypaper.\nMoreover, the TME model can be applied to provide potential solutions for a\nfamily of tensor data analytics problems with mixed effects.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 17:16:16 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 22:40:56 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Yue", "Xiaowei", ""], ["Park", "Jin Gyu", ""], ["Liang", "Zhiyong", ""], ["Shi", "Jianjun", ""]]}, {"id": "1804.03122", "submitter": "Lu Chen", "authors": "Junheng Ma, Joe Sedransk, Balgobin Nandram and Lu Chen", "title": "Bayesian Predictive Inference For Finite Population Quantities Under\n  Informative Sampling", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate Bayesian predictive inference for finite population quantities\nwhen there are unequal probabilities of selection. Only limited information\nabout the sample design is available; i.e., only the first-order selection\nprobabilities corresponding to the sample units are known. Our methodology,\nunlike that of Chambers, Dorfman and Wang (1998), can be used to make inference\nfor finite population quantities and provides measures of precision and\nintervals. Moreover, our methodology, using Markov chain Monte Carlo methods,\navoids the necessity of using asymptotic closed form approximations, necessary\nfor the other approaches that have been proposed. A set of simulated examples\nshows that the informative model provides improved precision over a standard\nignorable model, and corrects for the selection bias.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 17:40:36 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Ma", "Junheng", ""], ["Sedransk", "Joe", ""], ["Nandram", "Balgobin", ""], ["Chen", "Lu", ""]]}, {"id": "1804.03255", "submitter": "Ozan S\\\"onmez", "authors": "Alexander Aue, Gregory Rice, Ozan S\\\"onmez", "title": "Structural break analysis for spectrum and trace of covariance\n  operators?", "comments": "33 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with analyzing structural breaks in the covariance operator\nof sequentially observed functional data. For this purpose, procedures are\ndeveloped to segment an observed stretch of curves into periods for which\nsecond-order stationarity may be reasonably assumed. The proposed methods are\nbased on measuring the fluctuations of sample eigenvalues, either individually\nor jointly, and traces of the sample covariance operator computed from segments\nof the data. To implement the tests, new limit results are introduced that deal\nwith the large-sample behavior of vector-valued processes built from partial\nsample eigenvalue estimates. These results in turn enable the calibration of\nthe tests to a prescribed asymptotic level. A simulation study and an\napplication to Australian annual minimum temperature curves confirm that the\nproposed methods work well in finite samples. The application suggests that the\nvariation in annual minimum temperature underwent a structural break in the\n1950s, after which typical fluctuations from the generally increasing\ntrendstarted to be significantly smaller.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 22:07:19 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Aue", "Alexander", ""], ["Rice", "Gregory", ""], ["S\u00f6nmez", "Ozan", ""]]}, {"id": "1804.03274", "submitter": "Xiongzhi Chen", "authors": "X. Jessie Jeng and Xiongzhi Chen", "title": "Efficient Predictor Ranking and False Discovery Proportion Control in\n  High-Dimensional Regression", "comments": "16 pages; 3 rigures; this version accepted by Journal of Multivariate\n  Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a ranking and selection procedure to prioritize relevant\npredictors and control false discovery proportion (FDP) of variable selection.\nOur procedure utilizes a new ranking method built upon the de-sparsified Lasso\nestimator. We show that the new ranking method achieves the optimal order of\nminimum non-zero effects in ranking relevant predictors ahead of irrelevant\nones. Adopting the new ranking method, we develop a variable selection\nprocedure to asymptotically control FDP at a user-specified level. We show that\nour procedure can consistently estimate the FDP of variable selection as long\nas the de-sparsified Lasso estimator is asymptotically normal. In numerical\nanalyses, our procedure compares favorably to existing methods in ranking\nefficiency and FDP control when the regression model is relatively sparse.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 23:54:57 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 02:47:49 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Jeng", "X. Jessie", ""], ["Chen", "Xiongzhi", ""]]}, {"id": "1804.03366", "submitter": "Efstathios Paparoditis", "authors": "Anne Leucht, Efstathios Paparoditis and Theofanis Sapatinas", "title": "Testing equality of spectral density operators for functional linear\n  processes", "comments": "superseded by arXiv:2004.03412", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of testing equality of the entire second order structure of two\nindependent functional linear processes is considered. A fully functional\n$L^2$-type test is developed which evaluates, over all frequencies, the\nHilbert-Schmidt distance between the estimated spectral density operators of\nthe two processes. The asymptotic behavior of the test statistic is\ninvestigated and its limiting distribution under the null hypothesis is\nderived. Furthermore, a novel frequency domain bootstrap method is developed\nwhich approximates more accurately the distribution of the test statistic under\nthe null than the large sample Gaussian approximation obtained. Asymptotic\nvalidity of the bootstrap procedure is established and consistency of the\nbootstrap-based test under the alternative is proved. Numerical simulations\nshow that, even for small samples, the bootstrap-based test has very good size\nand power behavior. An application to meteorological functional time series is\nalso presented.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 06:56:03 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 07:04:06 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Leucht", "Anne", ""], ["Paparoditis", "Efstathios", ""], ["Sapatinas", "Theofanis", ""]]}, {"id": "1804.03616", "submitter": "Shota Gugushvili", "authors": "Shota Gugushvili, Frank van der Meulen, Moritz Schauer, Peter Spreij", "title": "Fast and scalable non-parametric Bayesian inference for Poisson point\n  processes", "comments": "45 pages, 22 figures", "journal-ref": "RESEARCHERS.ONE (2019),\n  https://www.researchers.one/article/2019-06-6", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of non-parametric Bayesian estimation of the intensity\nfunction of a Poisson point process. The observations are $n$ independent\nrealisations of a Poisson point process on the interval $[0,T]$. We propose two\nrelated approaches. In both approaches we model the intensity function as\npiecewise constant on $N$ bins forming a partition of the interval $[0,T]$. In\nthe first approach the coefficients of the intensity function are assigned\nindependent gamma priors, leading to a closed form posterior distribution. On\nthe theoretical side, we prove that as $n\\rightarrow\\infty,$ the posterior\nasymptotically concentrates around the \"true\", data-generating intensity\nfunction at an optimal rate for $h$-H\\\"older regular intensity functions ($0 <\nh\\leq 1$). In the second approach we employ a gamma Markov chain prior on the\ncoefficients of the intensity function. The posterior distribution is no longer\navailable in closed form, but inference can be performed using a\nstraightforward version of the Gibbs sampler. Both approaches scale well with\nsample size, but the second is much less sensitive to the choice of $N$.\nPractical performance of our methods is first demonstrated via synthetic data\nexamples. We compare our second method with other existing approaches on the UK\ncoal mining disasters data. Furthermore, we apply it to the US mass shootings\ndata and Donald Trump's Twitter data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 16:20:30 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 13:05:00 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 10:10:37 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Gugushvili", "Shota", ""], ["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""], ["Spreij", "Peter", ""]]}, {"id": "1804.03637", "submitter": "Yeqing Zhou", "authors": "Yeqing Zhou, Jingyuan Liu, Zhihui Hao and Liping Zhu", "title": "Model-Free Conditional Feature Screening with Exposure Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensional analysis, effects of explanatory variables on responses\nsometimes rely on certain exposure variables, such as time or environmental\nfactors. In this paper, to characterize the importance of each predictor, we\nutilize its conditional correlation given exposure variables with the empirical\ndistribution function of response. A model-free conditional screening method is\nsubsequently advocated based on this idea, aiming to identify significant\npredictors whose effects may vary with the exposure variables. The proposed\nscreening procedure is applicable to any model form, including that with\nheteroscedasticity where the variance component may also vary with exposure\nvariables. It is also robust to extreme values or outlier. Under some mild\nconditions, we establish the desirable sure screening and the ranking\nconsistency properties of the screening method. The finite sample performances\nare illustrated by simulation studies and an application to the breast cancer\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 17:30:32 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Zhou", "Yeqing", ""], ["Liu", "Jingyuan", ""], ["Hao", "Zhihui", ""], ["Zhu", "Liping", ""]]}, {"id": "1804.03714", "submitter": "Tullia Padellini", "authors": "Tullia Padellini and Haavard Rue", "title": "Model-aware Quantile Regression for Discrete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression relates the quantile of the response to a linear\npredictor. For a discrete response distributions, like the Poission, Binomial\nand the negative Binomial, this approach is not feasible as the quantile\nfunction is not bijective. We argue to use a continuous model-aware\ninterpolation of the quantile function, allowing for proper quantile inference\nwhile retaining model interpretation. This approach allows for proper\nuncertainty quantification and mitigates the issue of quantile crossing. Our\nreanalysis of hospitalisation data considered in Congdon (2017) shows the\nadvantages of our proposal as well as introducing a novel method to exploit\nquantile regression in the context of disease mapping.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 20:48:39 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 08:52:11 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Padellini", "Tullia", ""], ["Rue", "Haavard", ""]]}, {"id": "1804.03942", "submitter": "Chitradipa Chakraborty", "authors": "Chitradipa Chakraborty and Subhra Sankar Dhar", "title": "A Test for Multivariate Location Parameter in Elliptical Model based on\n  Forward Search Method", "comments": "32 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we develop a test for multivariate location parameter in\nelliptical model based on the forward search estimator for a specified scatter\nmatrix. Here, we study the asymptotic power of the test under contiguous\nalternatives based on the asymptotic distribution of the test statistics under\nsuch alternatives. Moreover, the performances of the test have been carried out\nfor different simulated data and real data, and compared the performances with\nmore classical ones.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 11:44:14 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Chakraborty", "Chitradipa", ""], ["Dhar", "Subhra Sankar", ""]]}, {"id": "1804.03963", "submitter": "Simon Taylor", "authors": "Simon Taylor, Chris Sherlock, Gareth Ridall and Paul Fearnhead", "title": "Motor Unit Number Estimation via Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A change in the number of motor units that operate a particular muscle is an\nimportant indicator for the progress of a neuromuscular disease and the\nefficacy of a therapy. Inference for realistic statistical models of the\ntypical data produced when testing muscle function is difficult, and estimating\nthe number of motor units from these data is an ongoing statistical challenge.\nWe consider a set of models for the data, each with a different number of\nworking motor units, and present a novel method for Bayesian inference, based\non sequential Monte Carlo, which provides estimates of the marginal likelihood\nand, hence, a posterior probability for each model. To implement this approach\nin practice we require sequential Monte Carlo methods that have excellent\ncomputational and Monte Carlo properties. We achieve this by leveraging the\nconditional independence structure in the model, where given knowledge of which\nmotor units fired as a result of a particular stimulus, parameters that specify\nthe size of each unit's response are independent of the parameters defining the\nprobability that a unit will respond at all. The scalability of our methodology\nrelies on the natural conjugacy structure that we create for the former and an\nenforced, approximate conjugate structure for the latter. A simulation study\ndemonstrates the accuracy of our method, and inferences are consistent across\ntwo different datasets arising from the same rat tibial muscle.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 12:58:04 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Taylor", "Simon", ""], ["Sherlock", "Chris", ""], ["Ridall", "Gareth", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1804.03981", "submitter": "Muhammad Naveed Tabassum", "authors": "Muhammad Naveed Tabassum and Esa Ollila", "title": "Compressive Regularized Discriminant Analysis of High-Dimensional Data\n  with Applications to Microarray Studies", "comments": "5 pages, 2018 IEEE International Conference on Acoustics, Speech and\n  Signal Processing 15-20 April 2018 | Calgary, Alberta, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a modification of linear discriminant analysis, referred to as\ncompressive regularized discriminant analysis (CRDA), for analysis of\nhigh-dimensional datasets. CRDA is specially designed for feature elimination\npurpose and can be used as gene selection method in microarray studies. CRDA\nlends ideas from $\\ell_{q,1}$ norm minimization algorithms in the multiple\nmeasurement vectors (MMV) model and utilizes joint-sparsity promoting hard\nthresholding for feature elimination. A regularization of the sample covariance\nmatrix is also needed as we consider the challenging scenario where the number\nof features (variables) is comparable or exceeding the sample size of the\ntraining dataset. A simulation study and four examples of real-life microarray\ndatasets evaluate the performances of CRDA based classifiers. Overall, the\nproposed method gives fewer misclassification errors than its competitors,\nwhile at the same time achieving accurate feature selection.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 13:48:56 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Tabassum", "Muhammad Naveed", ""], ["Ollila", "Esa", ""]]}, {"id": "1804.03989", "submitter": "Kenric Nelson Ph.D.", "authors": "Kenric P. Nelson, Mark A. Kon and Sabir R. Umarov", "title": "Use of the geometric mean as a statistic for the scale of the coupled\n  Gaussian distributions", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.physa.2018.09.049", "report-no": null, "categories": "stat.ME cond-mat.stat-mech math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The geometric mean is shown to be an appropriate statistic for the scale of a\nheavy-tailed coupled Gaussian distribution or equivalently the Student's t\ndistribution. The coupled Gaussian is a member of a family of distributions\nparameterized by the nonlinear statistical coupling which is the reciprocal of\nthe degree of freedom and is proportional to fluctuations in the inverse scale\nof the Gaussian. Existing estimators of the scale of the coupled Gaussian have\nrelied on estimates of the full distribution, and they suffer from problems\nrelated to outliers in heavy-tailed distributions. In this paper, the scale of\na coupled Gaussian is proven to be equal to the product of the generalized mean\nand the square root of the coupling. From our numerical computations of the\nscales of coupled Gaussians using the generalized mean of random samples, it is\nindicated that only samples from a Cauchy distribution (with coupling parameter\none) form an unbiased estimate with diminishing variance for large samples.\nNevertheless, we also prove that the scale is a function of the geometric mean,\nthe coupling term and a harmonic number. Numerical experiments show that this\nestimator is unbiased with diminishing variance for large samples for a broad\nrange of coupling values.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 13:31:46 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 13:06:34 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Nelson", "Kenric P.", ""], ["Kon", "Mark A.", ""], ["Umarov", "Sabir R.", ""]]}, {"id": "1804.04034", "submitter": "Daniel Rudolf", "authors": "Manuel Diehn, Axel Munk, Daniel Rudolf", "title": "Maximum likelihood estimation in hidden Markov models with inhomogeneous\n  noise", "comments": "31 pages, 6 figures, Accepted for publication in ESAIM Probab. Stat", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parameter estimation in finite hidden state space Markov models\nwith time-dependent inhomogeneous noise, where the inhomogeneity vanishes\nsufficiently fast. Based on the concept of asymptotic mean stationary processes\nwe prove that the maximum likelihood and a quasi-maximum likelihood estimator\n(QMLE) are strongly consistent. The computation of the QMLE ignores the\ninhomogeneity, hence, is much simpler and robust. The theory is motivated by an\nexample from biophysics and applied to a Poisson- and linear Gaussian model.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 15:03:03 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 20:13:00 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Diehn", "Manuel", ""], ["Munk", "Axel", ""], ["Rudolf", "Daniel", ""]]}, {"id": "1804.04085", "submitter": "Ioannis Kosmidis", "authors": "Ioannis Kosmidis and Euloge Clovis Kenne Pagui and Nicola Sartori", "title": "Mean and median bias reduction in generalized linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an integrated framework for estimation and inference from\ngeneralized linear models using adjusted score equations that result in mean\nand median bias reduction. The framework unifies theoretical and methodological\naspects of past research on mean bias reduction and accommodates, in a natural\nway, new advances on median bias reduction. General expressions for the\nadjusted score functions are derived in terms of quantities that are readily\navailable in standard software for fitting generalized linear models. The\nresulting estimating equations are solved using a unifying quasi-Fisher scoring\nalgorithm that is shown to be equivalent to iteratively re-weighted least\nsquares with appropriately adjusted working variates. Formal links between the\niterations for mean and median bias reduction are established. Core model\ninvariance properties are used to develop a novel mixed adjustment strategy\nwhen the estimation of a dispersion parameter is necessary. It is also shown\nhow median bias reduction in multinomial logistic regression can be done using\nthe equivalent Poisson log-linear model. The estimates coming out from mean and\nmedian bias reduction are found to overcome practical issues related to\ninfinite estimates that can occur with positive probability in generalized\nlinear models with multinomial or discrete responses, and can result in valid\ninferences even in the presence of a high-dimensional nuisance parameter\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 16:50:13 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 09:34:12 GMT"}, {"version": "v3", "created": "Sun, 17 Jun 2018 19:06:38 GMT"}, {"version": "v4", "created": "Sat, 12 Jan 2019 18:53:49 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Kosmidis", "Ioannis", ""], ["Pagui", "Euloge Clovis Kenne", ""], ["Sartori", "Nicola", ""]]}, {"id": "1804.04103", "submitter": "Shovan Chowdhury", "authors": "Shovan Chowdhury and Amarjit Kundu", "title": "Stochastic Comparison of Parallel Systems with Log-Lindley Distributed\n  Components under Random Shocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Chowdhury and Kundu [6] compared two parallel systems of\nheterogeneous independent log-Lindley distributed components using the concept\nof vector majorization and related orders. Under the same set-up, this paper\nderives some results related to usual stochastic ordering between two parallel\nsystems when each component receives a random shock.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 17:19:03 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 14:27:12 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Chowdhury", "Shovan", ""], ["Kundu", "Amarjit", ""]]}, {"id": "1804.04231", "submitter": "Hedibert Lopes", "authors": "Sylvia Fruehwirth-Schnatter and Hedibert Freitas Lopes", "title": "Sparse Bayesian Factor Analysis when the Number of Factors is Unknown", "comments": "62 pages, 7 figures, 7 tables,", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the popularity of sparse factor models, little attention has been\ngiven to formally address identifiability of these models beyond standard\nrotation-based identification such as the positive lower triangular constraint.\nTo fill this gap, we provide a counting rule on the number of nonzero factor\nloadings that is sufficient for achieving uniqueness of the variance\ndecomposition in the factor representation. Furthermore, we introduce the\ngeneralised lower triangular representation to resolve rotational invariance\nand show that within this model class the unknown number of common factors can\nbe recovered in an overfitting sparse factor model. By combining point-mass\nmixture priors with a highly efficient and customised MCMC scheme, we obtain\nposterior summaries regarding the number of common factors as well as the\nfactor loadings via postprocessing. Our methodology is illustrated for monthly\nexchange rates of 22 currencies with respect to the euro over a period of eight\nyears and for monthly log returns of 73 firms from the NYSE100 over a period of\n20 years.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 21:32:32 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Fruehwirth-Schnatter", "Sylvia", ""], ["Lopes", "Hedibert Freitas", ""]]}, {"id": "1804.04255", "submitter": "Rong Zhu", "authors": "Xianpeng Zong and Rong Zhu and Guohua Zou", "title": "Improved Horvitz-Thompson Estimator in Survey Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Horvitz-Thompson (HT) estimator is widely used in survey sampling.\nHowever, the variance of the HT estimator becomes large when the inclusion\nprobabilities are highly heterogeneous. To overcome this shortcoming, in this\npaper, a hard-threshold method is used for the first-order inclusion\nprobabilities, that is, we carefully choose a threshold value, then replace the\ninclusion probabilities smaller than the threshold by the threshold. By this\nshrinkage strategy, we propose a new estimator called improved Horvitz-Thompson\n(IHT) estimator to estimate the population total. The IHT estimator increases\nthe estimation accuracy although it brings bias which is relatively small. We\nderive the IHT estimator's MSE and its unbiased estimator, and theoretically\ncompare the IHT estimator with the HT estimator. We also apply our idea to\nconstruct the improved ratio estimator. We numerically analyze simulated and\nreal data sets to illustrate that the proposed estimators are more efficient\nand robust than the classical estimators.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 23:20:41 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Zong", "Xianpeng", ""], ["Zhu", "Rong", ""], ["Zou", "Guohua", ""]]}, {"id": "1804.04299", "submitter": "Wai Hoh Tang", "authors": "Wai Hoh Tang and Adrian R\\\"ollin", "title": "Model identification for ARMA time series through convolutional neural\n  networks", "comments": "17 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use convolutional neural networks to address the problem of\nmodel identification for autoregressive moving average time series models. We\ncompare the performance of several neural network architectures, trained on\nsimulated time series, with likelihood based methods, in particular the Akaike\nand Bayesian information criteria. We find that our neural networks can\nsignificantly outperform these likelihood based methods in terms of accuracy\nand, by orders of magnitude, in terms of speed.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 03:33:27 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 03:35:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Tang", "Wai Hoh", ""], ["R\u00f6llin", "Adrian", ""]]}, {"id": "1804.04306", "submitter": "Paul Kabaila", "authors": "Paul Kabaila, Samer Alhelli, Davide Farchione and Nathan Bragg", "title": "The effect of a Durbin-Watson pretest on confidence intervals in\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a linear regression model and suppose that our aim is to find a\nconfidence interval for a specified linear combination of the regression\nparameters. In practice, it is common to perform a Durbin-Watson pretest of the\nnull hypothesis of zero first-order autocorrelation of the random errors\nagainst the alternative hypothesis of positive first-order autocorrelation. If\nthis null hypothesis is accepted then the confidence interval centred on the\nOrdinary Least Squares estimator is used; otherwise the confidence interval\ncentred on the Feasible Generalized Least Squares estimator is used. We provide\nnew tools for the computation, for any given design matrix and parameter of\ninterest, of graphs of the coverage probability functions of the confidence\ninterval resulting from this two-stage procedure and the confidence interval\nthat is always centred on the Feasible Generalized Least Squares estimator.\nThese graphs are used to choose the better confidence interval, prior to any\nexamination of the observed response vector.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 03:58:48 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 23:04:39 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Kabaila", "Paul", ""], ["Alhelli", "Samer", ""], ["Farchione", "Davide", ""], ["Bragg", "Nathan", ""]]}, {"id": "1804.04359", "submitter": "David Gunawan", "authors": "David Gunawan and Chris Carter and Robert Kohn", "title": "On Scalable Particle Markov Chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Markov Chain Monte Carlo (PMCMC) is a general approach to carry out\nBayesian inference in non-linear and non-Gaussian state space models. Our\narticle shows how to scale up PMCMC in terms of the number of observations and\nparameters by expressing the target density of the PMCMC in terms of the basic\nuniform or standard normal random numbers, instead of the particles, used in\nthe sequential Monte Carlo algorithm. Parameters that can be drawn efficiently\nconditional on the particles are generated by particle Gibbs. All the other\nparameters are drawn by conditioning on the basic uniform or standard normal\nrandom variables; e.g. parameters that are highly correlated with the states,\nor parameters whose generation is expensive when conditioning on the states.\nThe performance of this hybrid sampler is investigated empirically by applying\nit to univariate and multivariate stochastic volatility models having both a\nlarge number of parameters and a large number of latent states and shows that\nit is much more efficient than competing PMCMC methods. We also show that the\nproposed hybrid sampler is ergodic.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 07:32:18 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 06:02:54 GMT"}, {"version": "v3", "created": "Sun, 9 Aug 2020 07:10:30 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Gunawan", "David", ""], ["Carter", "Chris", ""], ["Kohn", "Robert", ""]]}, {"id": "1804.04541", "submitter": "Matei Tene", "authors": "Matei Tene, Dana E. Stuparu, Dorota Kurowicka, Ghada Y. El Serafy", "title": "A copula-based sensitivity analysis method and its application to a\n  North Sea sediment transport model", "comments": null, "journal-ref": "Environmental Modelling & Software, Volume 104, June 2018, Pages\n  1-12", "doi": "10.1016/j.envsoft.2018.03.002", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes a novel sensitivity analysis method, able to handle\ndependency relationships between model parameters. The starting point is the\npopular Morris (1991) algorithm, which was initially devised under the\nassumption of parameter independence. This important limitation is tackled by\nallowing the user to incorporate dependency information through a copula. The\nset of model runs obtained using latin hypercube sampling, are then used for\nderiving appropriate sensitivity measures.\n  Delft3D-WAQ (Deltares, 2010) is a sediment transport model with strong\ncorrelations between input parameters. Despite this, the parameter ranking\nobtained with the newly proposed method is in accordance with the knowledge\nobtained from expert judgment. However, under the same conditions, the classic\nMorris method elicits its results from model runs which break the assumptions\nof the underlying physical processes. This leads to the conclusion that the\nproposed extension is superior to the classic Morris algorithm and can\naccommodate a wide range of use cases.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 12:10:02 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Tene", "Matei", ""], ["Stuparu", "Dana E.", ""], ["Kurowicka", "Dorota", ""], ["Serafy", "Ghada Y. El", ""]]}, {"id": "1804.04557", "submitter": "Yongqiang Tang", "authors": "Yongqiang Tang", "title": "A noniterative sample size procedure for tests based on t distributions", "comments": "18 pages", "journal-ref": "Statistics in medicine 2018", "doi": "10.1002/sim.7807", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A noniterative sample size procedure is proposed for a general hypothesis\ntest based on the t distribution by modifying and extending Guenther's (1981)\napproach for the one sample and two sample t tests. The generalized procedure\nis employed to determine the sample size for treatment comparisons using the\nanalysis of covariance (ANCOVA) and the mixed effects model for repeated\nmeasures (MMRM) in randomized clinical trials. The sample size is calculated by\nadding a few simple correction terms to the sample size from the normal\napproximation to account for the nonnormality of the t statistic and lower\norder variance terms, which are functions of the covariates in the model. But\nit does not require specifying the covariate distribution. The noniterative\nprocedure is suitable for superiority tests, noninferiority tests and a special\ncase of the tests for equivalence or bioequivalence, and generally yields the\nexact or nearly exact sample size estimate after rounding to an integer. The\nmethod for calculating the exact power of the two sample t test with unequal\nvariance in superiority trials is extended to equivalence trials. We also\nderive accurate power formulae for ANCOVA and MMRM, and the formula for ANCOVA\nis exact for normally distributed covariates. Numerical examples demonstrate\nthe accuracy of the proposed methods particularly in small samples.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 15:08:16 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Tang", "Yongqiang", ""]]}, {"id": "1804.04583", "submitter": "Ian Fellows", "authors": "Ian E. Fellows", "title": "A New Generative Statistical Model for Graphs: The Latent Order Logistic\n  (LOLOG) Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full probability models are critical for the statistical modeling of complex\nnetworks, and yet there are few general, flexible and widely applicable\ngenerative methods. We propose a new family of probability models motivated by\nthe idea of network growth, which we call the Latent Order Logistic (LOLOG)\nmodel. LOLOG is a fully general framework capable of describing any probability\ndistribution over graph configurations, though not all distributions are easily\nexpressible or estimable as a LOLOG. We develop inferential procedures based on\nMonte Carlo Method of Moments, Generalized Method of Moments and variational\ninference. To show the flexibility of the model framework, we show how\nso-called scale-free networks can be modeled as LOLOGs via preferential\nattachment. The advantages of LOLOG in terms of avoidance of degeneracy, ease\nof sampling, and model flexibility are illustrated. Connections with the\npopular Exponential-family Random Graph model (ERGM) are also explored, and we\nfind that they are identical in the case of dyadic independence. Finally, we\napply the model to a social network of collaboration within a corporate law\nfirm, a friendship network among adolescent students, and the friendship\nrelations in an online social network.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 15:59:33 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Fellows", "Ian E.", ""]]}, {"id": "1804.04622", "submitter": "Jovana Mitrovic", "authors": "Jovana Mitrovic, Dino Sejdinovic, Yee Whye Teh", "title": "Causal Inference via Kernel Deviance Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering the causal structure among a set of variables is a fundamental\nproblem in many areas of science. In this paper, we propose Kernel Conditional\nDeviance for Causal Inference (KCDC) a fully nonparametric causal discovery\nmethod based on purely observational data. From a novel interpretation of the\nnotion of asymmetry between cause and effect, we derive a corresponding\nasymmetry measure using the framework of reproducing kernel Hilbert spaces.\nBased on this, we propose three decision rules for causal discovery. We\ndemonstrate the wide applicability of our method across a range of diverse\nsynthetic datasets. Furthermore, we test our method on real-world time series\ndata and the real-world benchmark dataset Tubingen Cause-Effect Pairs where we\noutperform existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 16:51:04 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Mitrovic", "Jovana", ""], ["Sejdinovic", "Dino", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1804.04977", "submitter": "Vincent Briane", "authors": "Vincent Briane, Charles Kervrann and Myriam Vimond", "title": "A Sequential Algorithm to Detect Diffusion Switching along Intracellular\n  Particle Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-particle tracking allows to infer the motion of single molecules in\nliving cells. When we observe a long trajectory (more than 100 points), it is\npossible that the particle switches mode of motion over time. Then, fitting a\nsingle model to the trajectory can be misleading. In this paper, we propose a\nmethod to detect the temporal change points : the times at which a change of\ndynamics occurs. More specifically, we consider that the particle switches\nbetween three main modes of motion : Brownian motion, subdiffusion and\nsuperdiffusion. We use an algorithm based on a statistic (Briane et al. 2016)\ncomputed on local windows along the trajectory. The method is non parametric as\nthe statistic is not related to any particular model. This algorithm controls\nthe number of false change point detections in the case where the trajectory is\nfully Brownian. A Monte Carlo study is proposed to demonstrate the performances\nof the method and also to compare the procedure to two competitive algorithms.\nAt the end, we illustrate the utility of the method on real data depicting the\nmotion of mRNA complexes (called mRNP) in neuronal dendrites.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 15:00:19 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Briane", "Vincent", ""], ["Kervrann", "Charles", ""], ["Vimond", "Myriam", ""]]}, {"id": "1804.05061", "submitter": "Gong Lun", "authors": "Lun Gong, Cheng Zhang, Luwen Duan, Xueying Du, Hanqiu Liu, Xinjian\n  Chen, Jian Zheng", "title": "Non-rigid image registration using spatially region-weighted correlation\n  ratio and GPU-acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Non-rigid image registration with high accuracy and efficiency is\nstill a challenging task for medical image analysis. In this work, we present\nthe spatially region-weighted correlation ratio (SRWCR) as a novel similarity\nmeasure to improve the registration performance. Methods: SRWCR is rigorously\ndeduced from a three-dimension joint probability density function combining the\nintensity channels with an extra spatial information channel. SRWCR estimates\nthe optimal functional dependence between the intensities for each spatial bin,\nin which the spatial distribution modeled by a cubic B-spline function is used\nto differentiate the contribution of voxels. We also analytically derive the\ngradient of SRWCR with respect to the transformation parameters and optimize it\nusing a quasi-Newton approach. Furthermore, we propose a GPU-based parallel\nmechanism to accelerate the computation of SRWCR and its derivatives. Results:\nThe experiments on synthetic images, public 4-D thoracic computed tomography\n(CT) dataset, retinal optical coherence tomography (OCT) data, and clinical CT\nand positron emission tomography (PET) images confirm that SRWCR significantly\noutperforms some state-of-the-art techniques such as spatially encoded mutual\ninformation and Robust PaTch-based cOrrelation Ration. Conclusion: This study\ndemonstrates the advantages of SRWCR in tackling the practical difficulties due\nto distinct intensity changes, serious speckle noise, or different imaging\nmodalities. Significance: The proposed registration framework might be more\nreliable to correct the non-rigid deformations and more potential for clinical\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 09:13:41 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Gong", "Lun", ""], ["Zhang", "Cheng", ""], ["Duan", "Luwen", ""], ["Du", "Xueying", ""], ["Liu", "Hanqiu", ""], ["Chen", "Xinjian", ""], ["Zheng", "Jian", ""]]}, {"id": "1804.05079", "submitter": "Yebin Tao", "authors": "Yebin Tao and Haoda Fu", "title": "Robust Estimation of the Weighted Average Treatment Effect for A Target\n  Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weighted average treatment effect (WATE) is a causal measure for the\ncomparison of interventions in a specific target population, which may be\ndifferent from the population where data are sampled from. For instance, when\nthe goal is to introduce a new treatment to a target population, the question\nis what efficacy (or effectiveness) can be gained by switching patients from a\nstandard of care (control) to this new treatment, for which the average\ntreatment effect for the control (ATC) estimand can be applied. In this paper,\nwe propose two estimators based on augmented inverse probability weighting to\nestimate the WATE for a well defined target population (i.e., there exists a\ntarget function that describes the population of interest), using observational\ndata. The first proposed estimator is doubly robust if the target function is\nknown or can be correctly specified. The second proposed estimator is doubly\nrobust if the target function has a linear dependence on the propensity score,\nwhich can be used to estimate the average treatment effect for the treated\n(ATT) and ATC. We demonstrate the properties of the proposed estimators through\ntheoretical proof and simulation studies. We also apply our proposed methods in\na comparison of glucagon-like peptide-1 receptor agonists therapy and insulin\ntherapy among patients with type 2 diabetes, using the UK clinical practice\nresearch datalink data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 18:16:53 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Tao", "Yebin", ""], ["Fu", "Haoda", ""]]}, {"id": "1804.05133", "submitter": "Paul McNicholas", "authors": "Vanessa S.E. Bierling and Paul D. McNicholas", "title": "A Latent Gaussian Mixture Model for Clustering Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture models have become a popular tool for clustering. Amongst\nother uses, they have been applied for clustering longitudinal data and\nclustering high-dimensional data. In the latter case, a latent Gaussian mixture\nmodel is sometimes used. Although there has been much work on clustering using\nlatent variables and on clustering longitudinal data, respectively, there has\nbeen a paucity of work that combines these features. An approach is developed\nfor clustering longitudinal data with many time points based on an extension of\nthe mixture of common factor analyzers model. A variation of the\nexpectation-maximization algorithm is used for parameter estimation and the\nBayesian information criterion is used for model selection. The approach is\nillustrated using real and simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 22:40:00 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Bierling", "Vanessa S. E.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1804.05144", "submitter": "Olanrewaju Akande", "authors": "Olanrewaju Akande, Andr\\'es Barrientos and Jerome P. Reiter", "title": "Simultaneous Edit and Imputation for Household Data with Structural\n  Zeros", "comments": null, "journal-ref": null, "doi": "10.1093/jssam/smy022", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate categorical data nested within households often include reported\nvalues that fail edit constraints---for example, a participating household\nreports a child's age as older than his biological parent's age---as well as\nmissing values. Generally, agencies prefer datasets to be free from erroneous\nor missing values before analyzing them or disseminating them to secondary data\nusers. We present a model-based engine for editing and imputation of household\ndata based on a Bayesian hierarchical model that includes (i) a nested data\nDirichlet process mixture of products of multinomial distributions as the model\nfor the true latent values of the data, truncated to allow only households that\nsatisfy all edit constraints, (ii) a model for the location of errors, and\n(iii) a reporting model for the observed responses in error. The approach\npropagates uncertainty due to unknown locations of errors and missing values,\ngenerates plausible datasets that satisfy all edit constraints, and can\npreserve multivariate relationships within and across individuals in the same\nhousehold. We illustrate the approach using data from the 2012 American\nCommunity Survey.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 01:17:56 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 15:47:47 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Akande", "Olanrewaju", ""], ["Barrientos", "Andr\u00e9s", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1804.05163", "submitter": "Chenyang Gu", "authors": "Chenyang Gu and Roee Gutman", "title": "Development of a Common Patient Assessment Scale across the Continuum of\n  Care: A Nested Multiple Imputation Approach", "comments": null, "journal-ref": null, "doi": "10.1214/18-AOAS1202", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating and tracking patients' functional status through the post-acute\ncare continuum requires a common instrument. However, different post-acute\nservice providers such as nursing homes, inpatient rehabilitation facilities\nand home health agencies rely on different instruments to evaluate patients'\nfunctional status. These instruments assess similar functional status domains,\nbut they comprise different activities, rating scales and scoring instructions.\nThese differences hinder the comparison of patients' assessments across health\ncare settings. We propose a two-step procedure that combines nested multiple\nimputation with the multivariate ordinal probit (MVOP) model to obtain a common\npatient assessment scale across the post-acute care continuum. Our procedure\nimputes the unmeasured assessments at multiple assessment dates and enables\nevaluation and comparison of the rates of functional improvement experienced by\npatients treated in different health care settings using a common measure. To\ngenerate multiple imputations of the unmeasured assessments using the MVOP\nmodel, a likelihood-based approach that combines the EM algorithm and the\nbootstrap method as well as a fully Bayesian approach using the data\naugmentation algorithm are developed. Using a dataset on patients who suffered\na stroke, we simulate missing assessments and compare the MVOP model to\nexisting methods for imputing incomplete multivariate ordinal variables. We\nshow that, for all of the estimands considered, and in most of the experimental\nconditions that were examined, the MVOP model appears to be superior. The\nproposed procedure is then applied to patients who suffered a stroke and were\nreleased from rehabilitation facilities either to skilled nursing facilities or\nto their homes.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 04:25:53 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 02:11:48 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2018 01:11:31 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Gu", "Chenyang", ""], ["Gutman", "Roee", ""]]}, {"id": "1804.05373", "submitter": "Muxuan Liang", "authors": "Muxuan Liang, Menggang Yu", "title": "A Semiparametric Approach to Model Effect Modification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One fundamental statistical question for research areas such as precision\nmedicine and health disparity is about discovering effect modification of\ntreatment or exposure by observed covariates. We propose a semiparametric\nframework for identifying such effect modification. Instead of using the\ntraditional outcome models, we directly posit semiparametric models on\ncontrasts, or expected differences of the outcome under different treatment\nchoices or exposures. Through semiparametric estimation theory, all valid\nestimating equations, including the efficient scores, are derived. Besides\ndoubly robust loss functions, our approach also enables dimension reduction in\npresence of many covariates. The asymptotic and non-asymptotic properties of\nthe proposed methods are explored via a unified statistical and algorithmic\nanalysis. Comparison with existing methods in both simulation and real data\nanalysis demonstrates the superiority of our estimators especially for an\nefficiency improved version.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 15:43:19 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 03:08:13 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Liang", "Muxuan", ""], ["Yu", "Menggang", ""]]}, {"id": "1804.05430", "submitter": "Young-Geun Choi", "authors": "Young-Geun Choi, Lawrence P. Hanrahan, Derek Norton and Ying-Qi Zhao", "title": "Simultaneous disease mapping and hot spot detection with application to\n  childhood obesity surveillance from electronic health records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHRs) have become a platform for data-driven\nsurveillance on a granular level in recent years. In this paper, we make use of\nEHRs for early prevention of childhood obesity. The proposed method\nsimultaneously provides smooth disease mapping and outlier information for\nobesity prevalence, which are useful for raising public awareness and\nfacilitating targeted intervention. More precisely, we consider a penalized\nmultilevel generalized linear model. We decompose regional contribution into\nsmooth and sparse signals, which are automatically identified by a combination\nof fusion and sparse penalties imposed on the likelihood function. In addition,\nwe weigh the proposed likelihood to account for the missingness and potential\nnon-representativeness arising from the EHR data. We develop a novel\nalternating minimization algorithm, which is computationally efficient, easy to\nimplement, and guarantees convergence. Simulation studies demonstrate superior\nperformance of the proposed method. Finally, we apply our method to the\nUniversity of Wisconsin Population Health Information Exchange database.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 21:17:20 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 15:29:54 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Choi", "Young-Geun", ""], ["Hanrahan", "Lawrence P.", ""], ["Norton", "Derek", ""], ["Zhao", "Ying-Qi", ""]]}, {"id": "1804.05545", "submitter": "Stephen Burgess", "authors": "Stephen Burgess and Jeremy A Labrecque", "title": "Mendelian randomization with a binary exposure variable: interpretation\n  and presentation of causal estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mendelian randomization uses genetic variants to make causal inferences about\na modifiable exposure. Subject to a genetic variant satisfying the instrumental\nvariable assumptions, an association between the variant and outcome implies a\ncausal effect of the exposure on the outcome. Complications arise with a binary\nexposure that is a dichotomization of a continuous risk factor (for example,\nhypertension is a dichotomization of blood pressure). This can lead to\nviolation of the exclusion restriction assumption: the genetic variant can\ninfluence the outcome via the continuous risk factor even if the binary\nexposure does not change. Provided the instrumental variable assumptions are\nsatisfied for the underlying continuous risk factor, causal inferences for the\nbinary exposure are valid for the continuous risk factor. Causal estimates for\nthe binary exposure assume the causal effect is a stepwise function at the\npoint of dichotomization. Even then, estimation requires further parametric\nassumptions. Under monotonicity, the causal estimate represents the average\ncausal effect in `compliers', individuals for whom the binary exposure would be\npresent if they have the genetic variant and absent otherwise. Unlike in\nrandomized trials, genetic compliers are unlikely to be a large or\nrepresentative subgroup of the population. Under homogeneity, the causal effect\nof the exposure on the outcome is assumed constant in all individuals; often an\nunrealistic assumption. We here provide methods for causal estimation with a\nbinary exposure (although subject to all the above caveats). Mendelian\nrandomization investigations with a dichotomized binary exposure should be\nconceptualized in terms of an underlying continuous variable.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 08:26:05 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Burgess", "Stephen", ""], ["Labrecque", "Jeremy A", ""]]}, {"id": "1804.05591", "submitter": "Jakob Knollm\\\"uller", "authors": "Jakob Knollm\\\"uller, Philipp Frank, and Torsten A. En{\\ss}lin", "title": "Separating diffuse from point-like sources - a Bayesian approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the starblade algorithm, a method to separate superimposed point\nsources from auto-correlated, diffuse flux using a Bayesian model. Point\nsources are assumed to be independent from each other and to follow a power-law\nbrightness distribution. The diffuse emission is described as a non-parametric\nlog-normal model with a priori unknown correlation structure. This model\nenforces positivity of the underlying emission and allows for variation in the\norder of magnitudes. The correlation structure is recovered non-parametrically\nin addition to the diffuse flux and is used for the separation of the point\nsources. Additionally many measurement artifacts appear as point-like or\nquasi-point-like effects, not compatible with superimposed diffuse emission. An\nestimate of the separation uncertainty can be provided as well. We demonstrate\nthe capabilities of the derived method on synthetic data and data obtained by\nthe Hubble Space Telescope, emphasizing its effect on instrumental artifacts as\nwell as physical sources. The performance of this method is compared to the\nbackground estimation of the SExtractor method, as well as to a denoising\nauto-encoder.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 10:16:48 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 15:53:29 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 13:43:23 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Knollm\u00fcller", "Jakob", ""], ["Frank", "Philipp", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1804.05809", "submitter": "Maxime Vono", "authors": "Maxime Vono, Nicolas Dobigeon and Pierre Chainais", "title": "Split-and-augmented Gibbs sampler - Application to large-scale inference\n  problems", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2894825", "report-no": null, "categories": "stat.ME eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives two new optimization-driven Monte Carlo algorithms\ninspired from variable splitting and data augmentation. In particular, the\nformulation of one of the proposed approaches is closely related to the\nalternating direction method of multipliers (ADMM) main steps. The proposed\nframework enables to derive faster and more efficient sampling schemes than the\ncurrent state-of-the-art methods and can embed the latter. By sampling\nefficiently the parameter to infer as well as the hyperparameters of the\nproblem, the generated samples can be used to approximate Bayesian estimators\nof the parameters to infer. Additionally, the proposed approach brings\nconfidence intervals at a low cost contrary to optimization methods.\nSimulations on two often-studied signal processing problems illustrate the\nperformance of the two proposed samplers. All results are compared to those\nobtained by recent state-of-the-art optimization and MCMC algorithms used to\nsolve these problems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 17:28:42 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 17:01:35 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Vono", "Maxime", ""], ["Dobigeon", "Nicolas", ""], ["Chainais", "Pierre", ""]]}, {"id": "1804.05923", "submitter": "Tom Chen", "authors": "Tom Chen, Eric Tchetgen Tchetgen, Rui Wang", "title": "A stochastic second-order generalized estimating equations approach for\n  estimating intraclass correlation coefficient in the presence of informative\n  missing data", "comments": "44 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design and analysis of cluster randomized trials must take into account\ncorrelation among outcomes from the same clusters. When applying standard\ngeneralized estimating equations (GEE), the first-order (e.g. treatment)\neffects can be estimated consistently even with a misspecified correlation\nstructure. In settings for which the correlation is of interest, one could\nestimate this quantity via second-order generalized estimating equations\n(GEE2). We build upon GEE2 in the setting of missing data, for which we\nincorporate a \"second-order\" inverse-probability weighting (IPW) scheme and\n\"second-order\" doubly robust (DR) estimating equations that guard against\npartial model misspecification. We highlight the need to model correlation\namong missing indicators in such settings. In addition, the computational\ndifficulties in solving these second-order equations have motivated our\ndevelopment of more computationally efficient algorithms for solving GEE2,\nwhich alleviates reliance on parameter starting values and provides\nsubstantially faster and higher convergence rates than the more widely used\ndeterministic root-solving methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 20:07:58 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Chen", "Tom", ""], ["Tchetgen", "Eric Tchetgen", ""], ["Wang", "Rui", ""]]}, {"id": "1804.06490", "submitter": "David Barajas-Solano", "authors": "David A. Barajas-Solano, Alexandre M. Tartakovsky", "title": "Multivariate Gaussian Process Regression for Multiscale Data\n  Assimilation and Uncertainty Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multivariate Gaussian process regression approach for parameter\nfield reconstruction based on the field's measurements collected at two\ndifferent scales, the coarse and fine scales. The proposed approach treats the\nparameter field defined at fine and coarse scales as a bivariate Gaussian\nprocess with a parameterized multiscale covariance model. We employ a full\nbivariate Mat\\'{e}rn kernel as multiscale covariance model, with shape and\nsmoothness hyperparameters that account for the coarsening relation between\nfine and coarse fields. In contrast to similar multiscale kriging approaches\nthat assume a known coarsening relation between scales, the hyperparameters of\nthe multiscale covariance model are estimated directly from data via\npseudo-likelihood maximization.\n  We illustrate the proposed approach with a predictive simulation application\nfor saturated flow in porous media. Multiscale Gaussian process regression is\nemployed to estimate two-dimensional log-saturated hydraulic conductivity\ndistributions from synthetic multiscale measurements. The resulting stochastic\nmodel for coarse saturated conductivity is employed to quantify and reduce\nuncertainty in pressure predictions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 22:38:39 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Barajas-Solano", "David A.", ""], ["Tartakovsky", "Alexandre M.", ""]]}, {"id": "1804.06699", "submitter": "Vincent Runge", "authors": "Vincent Runge", "title": "Is a Finite Intersection of Balls Covered by a Finite Union of Balls in\n  Euclidean Spaces ?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering a finite intersection of balls and a finite union of other balls\nin an Euclidean space, we propose an exact method to test whether the\nintersection is covered by the union. We reformulate this problem into\nquadratic programming problems. For each problem, we study the intersection\nbetween a sphere and a Voronoi-like polyhedron. That way we get information\nabout a possible overlap between the frontier of the union and the intersection\nof balls. If the polyhedra are non-degenerate, the initial nonconvex geometric\nproblem, which is NP-hard in general, is tractable in polynomial time by convex\noptimization tools and vertex enumeration. Under some mild conditions the\nvertex enumeration can be skipped. Simulations highlight the accuracy and\nefficiency of our approach compared with competing algorithms in Python for\nnonconvex quadratically constrained quadratic programming. This work is\nmotivated by an application in statistics to the problem of multidimensional\nchangepoint detection using pruned dynamic programming algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 13:08:07 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 19:59:59 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Runge", "Vincent", ""]]}, {"id": "1804.06788", "submitter": "Michael Betancourt", "authors": "Sean Talts, Michael Betancourt, Daniel Simpson, Aki Vehtari, Andrew\n  Gelman", "title": "Validating Bayesian Inference Algorithms with Simulation-Based\n  Calibration", "comments": "19 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying the correctness of Bayesian computation is challenging. This is\nespecially true for complex models that are common in practice, as these\nrequire sophisticated model implementations and algorithms. In this paper we\nintroduce \\emph{simulation-based calibration} (SBC), a general procedure for\nvalidating inferences from Bayesian algorithms capable of generating posterior\nsamples. This procedure not only identifies inaccurate computation and\ninconsistencies in model implementations but also provides graphical summaries\nthat can indicate the nature of the problems that arise. We argue that SBC is a\ncritical part of a robust Bayesian workflow, as well as being a useful tool for\nthose developing computational algorithms and statistical software.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 15:29:42 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 17:50:23 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Talts", "Sean", ""], ["Betancourt", "Michael", ""], ["Simpson", "Daniel", ""], ["Vehtari", "Aki", ""], ["Gelman", "Andrew", ""]]}, {"id": "1804.07065", "submitter": "Paul Jenkins", "authors": "Stefano Favaro, Shui Feng and Paul A. Jenkins", "title": "Bayesian nonparametric analysis of Kingman's coalescent", "comments": "37 pages, 2 figures. To appear in Annales de l'Institut Henri\n  Poincar\\'e - Probabilit\\'es et Statistiques", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kingman's coalescent is one of the most popular models in population\ngenetics. It describes the genealogy of a population whose genetic composition\nevolves in time according to the Wright-Fisher model, or suitable\napproximations of it belonging to the broad class of Fleming-Viot processes.\nAncestral inference under Kingman's coalescent has had much attention in the\nliterature, both in practical data analysis, and from a theoretical and\nmethodological point of view. Given a sample of individuals taken from the\npopulation at time $t>0$, most contributions have aimed at making frequentist\nor Bayesian parametric inference on quantities related to the genealogy of the\nsample. In this paper we propose a Bayesian nonparametric predictive approach\nto ancestral inference. That is, under the prior assumption that the\ncomposition of the population evolves in time according to a neutral\nFleming-Viot process, and given the information contained in an initial sample\nof $m$ individuals taken from the population at time $t>0$, we estimate\nquantities related to the genealogy of an additional unobservable sample of\nsize $m^{\\prime}\\geq1$. As a by-product of our analysis we introduce a class of\nBayesian nonparametric estimators (predictors) which can be thought of as\nGood-Turing type estimators for ancestral inference. The proposed approach is\nillustrated through an application to genetic data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 10:12:35 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Favaro", "Stefano", ""], ["Feng", "Shui", ""], ["Jenkins", "Paul A.", ""]]}, {"id": "1804.07079", "submitter": "Andr\\'e Beauducel", "authors": "Andr\\'e Beauducel and Norbert Hilger", "title": "On optimal allocation of treatment/condition variance in principal\n  component analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The allocation of a (treatment) condition-effect on the wrong principal\ncomponent (misallocation of variance) in principal component analysis (PCA) has\nbeen addressed in research on event-related potentials of the\nelectroencephalogram. However, the correct allocation of condition-effects on\nPCA components might be relevant in several domains of research. The present\npaper investigates whether different loading patterns at each condition-level\nare a basis for an optimal allocation of between-condition variance on\nprincipal components. It turns out that a similar loading shape at each\ncondition-level is a necessary condition for an optimal allocation of\nbetween-condition variance, whereas a similar loading magnitude is not\nnecessary.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 10:52:28 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Beauducel", "Andr\u00e9", ""], ["Hilger", "Norbert", ""]]}, {"id": "1804.07117", "submitter": "Jeremie Houssineau", "authors": "Jeremie Houssineau and Ajay Jasra and Sumeetpal S. Singh", "title": "On Large Lag Smoothing for Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider the smoothing problem for hidden Markov models\n(HMM). Given a hidden Markov chain $\\{X_n\\}_{n\\geq 0}$ and observations\n$\\{Y_n\\}_{n\\geq 0}$, our objective is to compute\n$\\mathbb{E}[\\varphi(X_0,\\dots,X_k)|y_{0},\\dots,y_n]$ for some real-valued,\nintegrable functional $\\varphi$ and $k$ fixed, $k \\ll n$ and for some\nrealisation $(y_0,\\dots,y_n)$ of $(Y_0,\\dots,Y_n)$. We introduce a novel\napplication of the multilevel Monte Carlo (MLMC) method with a coupling based\non the Knothe-Rosenblatt rearrangement. We prove that this method can\napproximate the afore-mentioned quantity with a mean square error (MSE) of\n$\\mathcal{O}(\\epsilon^2)$, for arbitrary $\\epsilon>0$ with a cost of\n$\\mathcal{O}(\\epsilon^{-2})$. This is in contrast to the same direct Monte\nCarlo method, which requires a cost of $\\mathcal{O}(n\\epsilon^{-2})$ for the\nsame MSE. The approach we suggest is, in general, not possible to implement, so\nthe optimal transport methodology of \\cite{span} is used, which directly\napproximates our strategy. We show that our theoretical improvements are\nachieved, even under approximation, in several numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 12:41:28 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Houssineau", "Jeremie", ""], ["Jasra", "Ajay", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1804.07192", "submitter": "Antonio Irpino PhD", "authors": "Rosanna Verde and Antonio Irpino", "title": "Multiple factor analysis of distributional data", "comments": "Accepted from STATSTICA APPLICATA: Italian Journal of Applied\n  Statistics on 12/2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of Symbolic Data Analysis (SDA), distribution-variables are\na particular case of multi-valued variables: each unit is represented by a set\nof distributions (e.g. histograms, density functions or quantile functions),\none for each variable. Factor analysis (FA) methods are primary exploratory\ntools for dimension reduction and visualization. In the present work, we use\nMultiple Factor Analysis (MFA) approach for the analysis of data described by\ndistributional variables. Each distributional variable induces a set new\nnumeric variable related to the quantiles of each distribution. We call these\nnew variables as \\textit{quantile variables} and the set of quantile variables\nrelated to a distributional one is a block in the MFA approach. Thus, MFA is\nperformed on juxtaposed tables of quantile variables. \\\\ We show that the\ncriterion decomposed in the analysis is an approximation of the variability\nbased on a suitable metrics between distributions: the squared $L_2$\nWasserstein distance. \\\\ Applications on simulated and real distributional data\ncorroborate the method. The interpretation of the results on the factorial\nplanes is performed by new interpretative tools that are related to the several\ncharacteristics of the distributions (location, scale and shape).\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 14:29:21 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Verde", "Rosanna", ""], ["Irpino", "Antonio", ""]]}, {"id": "1804.07430", "submitter": "Chixiang Chen", "authors": "Chixiang Chen, Biyi Shen, Lijun Zhang, Yuan Xue, Ming Wang", "title": "Empirical-likelihood-based criteria for model selection on marginal\n  analysis of longitudinal data with dropout missingness", "comments": "Earlier version won the Student Paper Award at the 2018 International\n  Chinese Statistical Association (ICSA) Applied Statistics Symposium", "journal-ref": null, "doi": "10.1111/biom.13060", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal data are common in clinical trials and observational studies,\nwhere missing outcomes due to dropouts are always encountered. Under such\ncontext with the assumption of missing at random, the weighted generalized\nestimating equations (WGEE) approach is widely adopted for marginal analysis.\nModel selection on marginal mean regression is a crucial aspect of data\nanalysis, and identifying an appropriate correlation structure for model\nfitting may also be of interest and importance. However, the existing\ninformation criteria for model selection in WGEE have limitations, such as\nseparate criteria for the selection of marginal mean and correlation\nstructures, unsatisfactory selection performance in small-sample set-ups and so\non. In particular, there are few studies to develop joint information criteria\nfor selection of both marginal mean and correlation structures. In this work,\nby embedding empirical likelihood into the WGEE framework, we propose two\ninnovative information criteria named a joint empirical Akaike information\ncriterion (JEAIC) and a joint empirical Bayesian information criterion (JEBIC),\nwhich can simultaneously select the variables for marginal mean regression and\nalso correlation structure. Through extensive simulation studies, these\nempirical-likelihood-based criteria exhibit robustness, flexibility, and\noutperformance compared to the other criteria including the weighted\nquasi-likelihood under the independence model criterion, the missing\nlongitudinal information criterion and the joint longitudinal information\ncriterion. In addition, we provide a theoretical justification of our proposed\ncriteria, and present two real data examples in practice for further\nillustration.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 02:34:02 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 22:12:01 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Chen", "Chixiang", ""], ["Shen", "Biyi", ""], ["Zhang", "Lijun", ""], ["Xue", "Yuan", ""], ["Wang", "Ming", ""]]}, {"id": "1804.07483", "submitter": "Chau Thi Tuyet Trang", "authors": "Thi Tuyet Trang Chau (IRMAR), Pierre Ailliot (LMBA), Val\\'erie Monbet\n  (IRMAR, SIMSMART), Pierre Tandeo (IMT Atlantique, Lab-STICC)", "title": "An efficient particle-based method for maximum likelihood estimation in\n  nonlinear state-space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation methods aim at estimating the state of a system by\ncombining observations with a physical model. When sequential data assimilation\nis considered, the joint distribution of the latent state and the observations\nis described mathematically using a state-space model, and filtering or\nsmoothing algorithms are used to approximate the conditional distribution of\nthe state given the observations. The most popular algorithms in the data\nassimilation community are based on the Ensemble Kalman Filter and Smoother\n(EnKF/EnKS) and its extensions. In this paper we investigate an alternative\napproach where a Conditional Particle Filter (CPF) is combined with Backward\nSimulation (BS). This allows to explore efficiently the latent space and\nsimulate quickly relevant trajectories of the state conditionally to the\nobservations. We also tackle the difficult problem of parameter estimation.\nIndeed, the models generally involve statistical parameters in the physical\nmodels and/or in the stochastic models for the errors. These parameters\nstrongly impact the results of the data assimilation algorithm and there is a\nneed for an efficient method to estimate them. Expectation-Maximization (EM) is\nthe most classical algorithm in the statistical literature to estimate the\nparameters in models with latent variables. It consists in updating\nsequentially the parameters by maximizing a likelihood function where the state\nis approximated using a smoothing algorithm. In this paper, we propose an\noriginal Stochastic Expectation-Maximization (SEM) algorithm combined to the\nCPF-BS smoother to estimate the statistical parameters. We show on several toy\nmodels that this algorithm provides, with reasonable computational cost,\naccurate estimations of the statistical parameters and the state in highly\nnonlinear state-space models, where the application of EM algorithms using EnKS\nis limited. We also provide a Python source code of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 08:10:34 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Chau", "Thi Tuyet Trang", "", "IRMAR"], ["Ailliot", "Pierre", "", "LMBA"], ["Monbet", "Val\u00e9rie", "", "IRMAR, SIMSMART"], ["Tandeo", "Pierre", "", "IMT Atlantique, Lab-STICC"]]}, {"id": "1804.07562", "submitter": "Gael Sent\\'is", "authors": "Gael Sent\\'is, Johannes N. Greiner, Jiangwei Shang, Jens Siewert,\n  Matthias Kleinmann", "title": "Bound entangled states fit for robust experimental verification", "comments": "Accepted version in Quantum", "journal-ref": "Quantum 2, 113 (2018)", "doi": "10.22331/q-2018-12-18-113", "report-no": null, "categories": "quant-ph stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Preparing and certifying bound entangled states in the laboratory is an\nintrinsically hard task, due to both the fact that they typically form narrow\nregions in the state space, and that a certificate requires a tomographic\nreconstruction of the density matrix. Indeed, the previous experiments that\nhave reported the preparation of a bound entangled state relied on such\ntomographic reconstruction techniques. However, the reliability of these\nresults crucially depends on the extra assumption of an unbiased\nreconstruction. We propose an alternative method for certifying the bound\nentangled character of a quantum state that leads to a rigorous claim within a\ndesired statistical significance, while bypassing a full reconstruction of the\nstate. The method is comprised by a search for bound entangled states that are\nrobust for experimental verification, and a hypothesis test tailored for the\ndetection of bound entanglement that is naturally equipped with a measure of\nstatistical significance. We apply our method to families of states of $3\\times\n3$ and $4\\times 4$ systems, and find that the experimental certification of\nbound entangled states is well within reach.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 11:46:17 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 17:18:29 GMT"}, {"version": "v3", "created": "Fri, 14 Dec 2018 12:13:16 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Sent\u00eds", "Gael", ""], ["Greiner", "Johannes N.", ""], ["Shang", "Jiangwei", ""], ["Siewert", "Jens", ""], ["Kleinmann", "Matthias", ""]]}, {"id": "1804.07648", "submitter": "Abhishek Shah", "authors": "Abhishek Shah, Mohamad El Gharamti and Laurent Bertino", "title": "Assimilation of semi-qualitative observations with a stochastic Ensemble\n  Kalman Filter", "comments": null, "journal-ref": null, "doi": "10.1002/qj.3381", "report-no": null, "categories": "math.OC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ensemble Kalman filter assumes the observations to be Gaussian random\nvariables with a pre-specified mean and variance. In practice, observations may\nalso have detection limits, for instance when a gauge has a minimum or maximum\nvalue. In such cases most data assimilation schemes discard out-of-range\nvalues, treating them as \"not a number\", at a loss of possibly useful\nqualitative information.\n  The current work focuses on the development of a data assimilation scheme\nthat tackles observations with a detection limit. We present the Ensemble\nKalman Filter Semi-Qualitative (EnKF-SQ) and test its performance against the\nPartial Deterministic Ensemble Kalman Filter (PDEnKF) of Borup et al. (2015).\nBoth are designed to explicitly assimilate the out-of-range observations: the\nout-of-range values are qualitative by nature (inequalities), but one can\npostulate a probability distribution for them and then update the ensemble\nmembers accordingly. The EnKF-SQ is tested within the framework of twin\nexperiments, using both linear and non-linear toy models. Different sensitivity\nexperiments are conducted to assess the influence of the ensemble size,\nobservation detection limit and a number of observations on the performance of\nthe filter. Our numerical results show that assimilating qualitative\nobservations using the proposed scheme improves the overall forecast mean,\nmaking it viable for testing on more realistic applications such as sea-ice\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 14:45:12 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Shah", "Abhishek", ""], ["Gharamti", "Mohamad El", ""], ["Bertino", "Laurent", ""]]}, {"id": "1804.07734", "submitter": "Manoel Santos Neto", "authors": "Marcelo Bourguignon, Manoel Santos-Neto and M\\'ario de Castro", "title": "A new regression model for positive data", "comments": "20 pages and 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a regression model where the response variable is\nbeta prime distributed using a new parameterization of this distribution that\nis indexed by mean and precision parameters. The proposed regression model is\nuseful for situations where the variable of interest is continuous and\nrestricted to the positive real line and is related to other variables through\nthe mean and precision parameters. The variance function of the proposed model\nhas a quadratic form. In addition, the beta prime model has properties that its\ncompetitor distributions of the exponential family do not have. Estimation is\nperformed by maximum likelihood. Furthermore, we discuss residuals and\ninfluence diagnostic tools. Finally, we also carry out an application to real\ndata that demonstrates the usefulness of the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 17:22:30 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Bourguignon", "Marcelo", ""], ["Santos-Neto", "Manoel", ""], ["de Castro", "M\u00e1rio", ""]]}, {"id": "1804.07780", "submitter": "Xin Chen", "authors": "Xin Chen, Aleksandr Y. Aravkin, and R. Douglas Martin", "title": "Generalized Linear Model for Gamma Distributed Variables via Elastic Net\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Generalized Linear Model (GLM) for the Gamma distribution (glmGamma) is\nwidely used in modeling continuous, non-negative and positive-skewed data, such\nas insurance claims and survival data. However, model selection for GLM depends\non AIC/BIC criteria, which is computationally impractical for even a moderate\nnumber of variables. In this paper, we develop variable selection for glmGamma\nusing elastic net regularization (glmGammaNet), for which we provide an\nalgorithm and implementation. The glmGammaNet model is more challening than\nother more common GLMs as the likelihood function has no global quadratic upper\nbound, and we develop an efficient accelerated proximal gradient algorithm\nusing a local model. We report simulation study results and discuss the choice\nof regularization parameter. The method is implemented in the R package\nglmGammaNet.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 18:17:30 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Chen", "Xin", ""], ["Aravkin", "Aleksandr Y.", ""], ["Martin", "R. Douglas", ""]]}, {"id": "1804.07863", "submitter": "Evan Rosenman", "authors": "Evan Rosenman, Art B. Owen, Michael Baiocchi, Hailey Banack", "title": "Propensity Score Methods for Merging Observational and Experimental\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project considers how one might augment a limited amount of data from\nrandomized controlled trial (RCT) with more plentiful data from an\nobservational database (ODB), in order to estimate a causal effect. In our\nmotivating setting, the ODB has better external validity, while the RCT has\ngenuine randomization. We work with strata defined by the propensity score in\nthe ODB. Subjects from the RCT are placed in strata defined by the propensity\nthey would have had, had they been in the ODB. Our first method simply spikes\nthe RCT data into their corresponding ODB strata. Our second method takes a\ndata-driven convex combination of the ODB and RCT treatment effect estimates\nwithin each stratum. Using the delta method and simulations we show that the\nspike-in method works best when the RCT covariates are drawn from the same\ndistribution as in the ODB. Our convex combination method is more robust than\nthe spike-in to covariate-based inclusion criteria that bias the RCT data. We\napply our methods to data from the Women's Health Initiative, a study of\nthousands of postmenopausal women which has both observational and experimental\ndata on hormone therapy (HT). Using half of the RCT to define a gold standard,\nwe find that a version of the spiked-in estimate yields stable estimates of the\ncausal impact of HT on coronary heart disease.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 23:41:35 GMT"}, {"version": "v2", "created": "Sun, 21 Oct 2018 19:21:37 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Rosenman", "Evan", ""], ["Owen", "Art B.", ""], ["Baiocchi", "Michael", ""], ["Banack", "Hailey", ""]]}, {"id": "1804.07935", "submitter": "Ali Eshragh", "authors": "Hadi Charkhgard and Ali Eshragh", "title": "Best subset selection in linear regression via bi-objective mixed\n  integer linear programming", "comments": "13 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of choosing the best subset of p features in linear\nregression given n observations. This problem naturally contains two objective\nfunctions including minimizing the amount of bias and minimizing the number of\npredictors. The existing approaches transform the problem into a\nsingle-objective optimization problem. We explain the main weaknesses of\nexisting approaches, and to overcome their drawbacks, we propose a bi-objective\nmixed integer linear programming approach. A computational study shows the\nefficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 10:27:32 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Charkhgard", "Hadi", ""], ["Eshragh", "Ali", ""]]}, {"id": "1804.07941", "submitter": "Priyantha Wijayatunga", "authors": "Priyantha Wijayatunga", "title": "On Associative Confounder Bias", "comments": "10 pages; The Thirteenth Scandinavian Conference on Artificial\n  Intelligence, Halmstad Sweden. pp. 157-166", "journal-ref": null, "doi": "10.3233/978-1-61499-589-0-157", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditioning on some set of confounders that causally affect both treatment\nand outcome variables can be sufficient for eliminating bias introduced by all\nsuch confounders when estimating causal effect of the treatment on the outcome\nfrom observational data. It is done by including them in propensity score model\nin so-called potential outcome framework for causal inference whereas in causal\ngraphical modeling framework usual conditioning on them is done. However in the\nformer framework, it is confusing when modeler finds a variable that is\nnon-causally associated with both the treatment and the outcome. Some argue\nthat such variables should also be included in the analysis for removing bias.\nBut others argue that they introduce no bias so they should be excluded and\nconditioning on them introduces spurious dependence between the treatment and\nthe outcome, thus resulting extra bias in the estimation. We show that there\nmay be errors in both the arguments in different contexts. When such a variable\nis found neither of the actions may give the correct causal effect estimate.\nSelecting one action over the other is needed in order to be less wrong. We\ndiscuss how to select the better action.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 10:44:20 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Wijayatunga", "Priyantha", ""]]}, {"id": "1804.08222", "submitter": "Kun He", "authors": "Kun He, Mengjie Li, Yan Fu, Fuzhou Gong and Xiaoming Sun", "title": "Null-free False Discovery Rate Control Using Decoy Permutations", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional approaches to false discovery rate (FDR) control in multiple\nhypothesis testing are usually based on the null distribution of a test\nstatistic. However, all types of null distributions, including the theoretical,\npermutation-based and empirical ones, have some inherent drawbacks. For\nexample, the theoretical null might fail because of improper assumptions on the\nsample distribution. Here, we propose a null distribution-free approach to FDR\ncontrol for multiple hypothesis testing. This approach, named target-decoy\nprocedure, simply builds on the ordering of tests by some statistic or score,\nthe null distribution of which is not required to be known. Competitive decoy\ntests are constructed from permutations of original samples and are used to\nestimate the false target discoveries. We prove that this approach controls the\nFDR when the statistics are independent between different tests. Simulation\ndemonstrates that it is more stable and powerful than two existing popular\napproaches. Evaluation is also made on a real dataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 02:05:09 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 04:42:24 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 15:57:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["He", "Kun", ""], ["Li", "Mengjie", ""], ["Fu", "Yan", ""], ["Gong", "Fuzhou", ""], ["Sun", "Xiaoming", ""]]}, {"id": "1804.08326", "submitter": "Gopal Basak", "authors": "Gopal K Basak and Samarjit Das", "title": "Understanding Cross-sectional Dependence in Panel Data", "comments": "There are 27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide various norm-based definitions of different types of\ncross-sectional dependence and the relations between them. These definitions\nfacilitate to comprehend and to characterize the various forms of\ncross-sectional dependence, such as strong, semi-strong, and weak dependence.\nThen we examine the asymptotic properties of parameter estimators both for\nfixed (within) effect estimator and random effect (pooled) estimator for linear\npanel data models incorporating various forms of cross-sectional dependence.\nThe asymptotic properties are also derived when both cross-sectional and\ntemporal dependence are present. Subsequently, we develop consistent and robust\nstandard error of the parameter estimators both for fixed effect and random\neffect model separately. Robust standard errors are developed (i) for pure\ncross-sectional dependence; and (ii) also for cross-sectional and time series\ndependence. Under strong or semi-strong cross-sectional dependence, it is\nestablished that when the time dependence comes through the idiosyncratic\nerrors, such time dependence does not have any influence in the asymptotic\nvariance of $(\\hat{\\beta}_{FE/RE}). $ Hence, it is argued that in estimating\n$Var(\\hat{\\beta}_{FE/RE}),$ Newey-West kind of correction injects bias in the\nvariance estimate. Furthermore, this article lay down conditions under which\n$t$, $F$ and the $Wald$ statistics based on the robust covariance matrix\nestimator give valid inference.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 10:34:05 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Basak", "Gopal K", ""], ["Das", "Samarjit", ""]]}, {"id": "1804.08345", "submitter": "Yesim Guney", "authors": "Ye\\c{s}im G\\\"uney, \\c{S}enay \\\"Ozdemir, Yetkin Tua\\c{c} and Olcay\n  Arslan", "title": "Optimal B-Robust Estimation for the Parameters of Marshall-Olkin\n  Extended Burr XII Distribution and Application for Modeling Data from\n  Pharmacokinetics Study", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marshall-Olkin Extended Burr XII (MOEBXII) distribution family, which is a\ngeneralization of Burr XII distribution proposed by Al-Saiari et al. [1] , is a\nflexible distribution that can be used in many fields such as actuarial\nscience, economics, life testing, reliability and failure time modeling. The\nparameters of the MOEBXII distribution are usually estimated by the maximum\nlikelihood (ML) and least squares (LS) estimation methods. However, these\nestimators are not robust to the outliers which are often encountered in\npractice. There are two main purposes of this paper. The first one is to find\nthe robust estimators for the parameters of the MOEBXII distribution. The\nsecond one is to use this distribution for modeling data from pharmacokinetics\nstudy. To obtain the robust estimators we use the optimal B robust estimator\nproposed by Hampel et al. [2]. We provide a simulation study to show the\nperformance of the proposed estimators for the ML, LS and robust M estimators.\nWe also give a real data example to illustrate the modeling capacity of the\nMOEBXII distribution for data from pharmacokinetics study.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 11:28:37 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["G\u00fcney", "Ye\u015fim", ""], ["\u00d6zdemir", "\u015eenay", ""], ["Tua\u00e7", "Yetkin", ""], ["Arslan", "Olcay", ""]]}, {"id": "1804.08650", "submitter": "Kyoungjae Lee", "authors": "Kyoungjae Lee and Lizhen Lin", "title": "Bayesian Bandwidth Test and Selection for High-dimensional Banded\n  Precision Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assuming a banded structure is one of the common practice in the estimation\nof high-dimensional precision matrix. In this case, estimating the bandwidth of\nthe precision matrix is a crucial initial step for subsequent analysis.\nAlthough there exist some consistent frequentist tests for the bandwidth\nparameter, bandwidth selection consistency for precision matrices has not been\nestablished in a Bayesian framework. In this paper, we propose a prior\ndistribution tailored to the bandwidth estimation of high-dimensional precision\nmatrices. The banded structure is imposed via the Cholesky factor from the\nmodified Cholesky decomposition. We establish the strong model selection\nconsistency for the bandwidth as well as the consistency of the Bayes factor.\nThe convergence rates for Bayes factors under both the null and alternative\nhypotheses are derived which yield similar order of rates. As a by-product, we\nalso proposed an estimation procedure for the Cholesky factors yielding an\nalmost optimal order of convergence rates. Two-sample bandwidth test is also\nconsidered, and it turns out that our method is able to consistently detect the\nequality of bandwidths between two precision matrices. The simulation study\nconfirms that our method in general outperforms the existing frequentist and\nBayesian methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 18:21:40 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 00:19:14 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Lee", "Kyoungjae", ""], ["Lin", "Lizhen", ""]]}, {"id": "1804.08665", "submitter": "Annamaria Guolo Dr.", "authors": "Annamaria Guolo and Duc Khanh To", "title": "A pseudo-likelihood approach for multivariate meta-analysis of test\n  accuracy studies with multiple thresholds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate meta-analysis of test accuracy studies when tests are evaluated\nin terms of sensitivity and specificity at more than one threshold represents\nan effective way to synthesize results by fully exploiting the data, if\ncompared to univariate meta-analyses performed at each threshold independently.\nThe approximation of logit transformations of sensitivities and specificities\nat different thresholds through a normal multivariate random-effects model is a\nrecent proposal, that straightforwardly extends the bivariate models well\nrecommended for the one threshold case. However, drawbacks of the approach,\nsuch as poor estimation of the within-study correlations between sensitivities\nand between specificities and severe computational issues, can make it\nunappealing. We propose an alternative method for inference on common\ndiagnostic measures using a pseudo-likelihood constructed under a working\nindependence assumption between sensitivities and between specificities at\ndifferent thresholds in the same study. The method does not require\nwithin-study correlations, overcomes the convergence issues and can be\neffortlessly implemented. Simulation studies highlight a satisfactory\nperformance of the method, remarkably improving the results from the\nmultivariate normal counterpart under different scenarios. The\npseudo-likelihood approach is illustrated in the evaluation of a test used for\ndiagnosis of pre-eclampsia as a cause of maternal and perinatal morbidity and\nmortality.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 18:52:47 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 05:16:11 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 09:13:09 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Guolo", "Annamaria", ""], ["To", "Duc Khanh", ""]]}, {"id": "1804.08694", "submitter": "Natalie Karavarsamis", "authors": "Natalie Karavarsamis and Richard M. huggins", "title": "Two-stage approaches to the analysis of occupancy data I: The\n  homogeneous case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occupancy models are used in statistical ecology to estimate species\ndispersion. The two components of an occupancy model are the detection and\noccupancy probabilities, with the main interest being in the occupancy\nprobabilities.\n  We show that for the homogeneous occupancy model there is an orthogonal\ntransformation of the parameters that gives a natural two-stage inference\nprocedure based on a conditional likelihood. We then extend this to a partial\nlikelihood that gives explicit estimators of the model parameters. By allowing\nthe separate modelling of the detection and occupancy probabilities, the\nextension of the two-stage approach to more general models has the potential to\nsimplify the computational routines used there.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 03:51:43 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Karavarsamis", "Natalie", ""], ["huggins", "Richard M.", ""]]}, {"id": "1804.08715", "submitter": "Javier Espinosa", "authors": "Javier Espinosa, Christian Hennig", "title": "A constrained regression model for an ordinal response with ordinal\n  predictors", "comments": "33 pages, 7 figures, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regression model is proposed for the analysis of an ordinal response\nvariable depending on a set of multiple covariates containing ordinal and\npotentially other variables. The proportional odds model (McCullagh (1980)) is\nused for the ordinal response, and constrained maximum likelihood estimation is\nused to account for the ordinality of covariates.\n  Ordinal predictors are coded by dummy variables. The parameters associated to\nthe categories of the ordinal predictor(s) are constrained, enforcing them to\nbe monotonic (isotonic or antitonic). A decision rule is introduced for\nclassifying the ordinal predictors' monotonicity directions, also providing\ninformation whether observations are compatible with both or no monotonicity\ndirection. In addition, a monotonicity test for the parameters of any ordinal\npredictor is proposed. The monotonicity constrained model is proposed together\nwith three estimation methods and compared to the unconstrained one based on\nsimulations.\n  The model is applied to real data explaining a 10-Points Likert scale quality\nof life self-assessment variable from ordinal and other predictors.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 20:17:00 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Espinosa", "Javier", ""], ["Hennig", "Christian", ""]]}, {"id": "1804.08760", "submitter": "Zach Branson", "authors": "Zach Branson", "title": "Randomization Tests to Assess Covariate Balance When Designing and\n  Analyzing Matched Datasets", "comments": "28 pages, 3 figures", "journal-ref": "Observational Studies 7 (2021) 1-36", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal analyses for observational studies are often complicated by covariate\nimbalances among treatment groups, and matching methodologies alleviate this\ncomplication by finding subsets of treatment groups that exhibit covariate\nbalance. It is widely agreed upon that covariate balance can serve as evidence\nthat a matched dataset approximates a randomized experiment, but what kind of\nexperiment does a matched dataset approximate? In this work, we develop a\nrandomization test for the hypothesis that a matched dataset approximates a\nparticular experimental design, such as complete randomization, block\nrandomization, or rerandomization. Our test can incorporate any experimental\ndesign, and it allows for a graphical display that puts several designs on the\nsame univariate scale, thereby allowing researchers to pinpoint which design --\nif any -- is most appropriate for a matched dataset. After researchers\ndetermine a plausible design, we recommend a randomization-based approach for\nanalyzing the matched data, which can incorporate any design and treatment\neffect estimator. Through simulation, we find that our test can frequently\ndetect violations of randomized assignment that harm inferential results.\nFurthermore, through simulation and a real application in political science, we\nfind that matched datasets with high levels of covariate balance tend to\napproximate balance-constrained designs like rerandomization, and analyzing\nthem as such can lead to precise causal analyses. However, assuming a precise\ndesign should be proceeded with caution, because it can harm inferential\nresults if there are still substantial biases due to remaining imbalances after\nmatching. Our approach is implemented in the randChecks R package, available on\nCRAN.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 22:09:00 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 04:09:57 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 22:41:18 GMT"}, {"version": "v4", "created": "Mon, 15 Feb 2021 15:20:39 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Branson", "Zach", ""]]}, {"id": "1804.08841", "submitter": "Haoyang Liu", "authors": "Haoyang Liu and Rina Foygel Barber", "title": "Between hard and soft thresholding: optimal iterative thresholding\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative thresholding algorithms seek to optimize a differentiable objective\nfunction over a sparsity or rank constraint by alternating between gradient\nsteps that reduce the objective, and thresholding steps that enforce the\nconstraint. This work examines the choice of the thresholding operator, and\nasks whether it is possible to achieve stronger guarantees than what is\npossible with hard thresholding. We develop the notion of relative concavity of\na thresholding operator, a quantity that characterizes the worst-case\nconvergence performance of any thresholding operator on the target optimization\nproblem. Surprisingly, we find that commonly used thresholding operators, such\nas hard thresholding and soft thresholding, are suboptimal in terms of\nworst-case convergence guarantees. Instead, a general class of thresholding\noperators, lying between hard thresholding and soft thresholding, is shown to\nbe optimal with the strongest possible convergence guarantee among all\nthresholding operators. Examples of this general class includes $\\ell_q$\nthresholding with appropriate choices of $q$, and a newly defined {\\em\nreciprocal thresholding} operator. We also investigate the implications of the\nimproved optimization guarantee in the statistical setting of sparse linear\nregression, and show that this new class of thresholding operators attain the\noptimal rate for computationally efficient estimators, matching the Lasso.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 05:19:44 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 17:04:50 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 01:06:22 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 19:54:59 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Liu", "Haoyang", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "1804.08862", "submitter": "Yongxiang Li", "authors": "Yongxiang Li, Qiang Zhou, Kwok Leung Tsui, and Javier Cabrera", "title": "Composite Inference for Gaussian Processes", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale Gaussian process models are becoming increasingly important and\nwidely used in many areas, such as, computer experiments, stochastic\noptimization via simulation, and machine learning using Gaussian processes. The\nstandard methods, such as maximum likelihood estimation (MLE) for parameter\nestimation and the best linear unbiased predictor (BLUP) for prediction, are\ngenerally the primary choices in many applications. In spite of their merits,\nthose methods are not feasible due to intractable computation when the sample\nsize is huge. A novel method for the purposes of parameter estimation and\nprediction is proposed to solve the computational problems of large-scale\nGaussian process based models, by separating the original dataset into\ntractable subsets. This method consistently combines parameter estimation and\nprediction by making full use of the dependence among conditional densities: a\nstatistically efficient composite likelihood based on joint distributions of\nsome well selected conditional densities is developed to estimate parameters\nand then \"composite inference\" is coined to make prediction for an unknown\ninput point, based on its distributions conditional on each block subset. The\nproposed method transforms the intractable BLUP into a tractable convex\noptimization problem. It is also shown that the prediction given by the\nproposed method, called the best linear unbiased block predictor, has a minimum\nvariance for a given separation of the dataset.\n  Keywords: Large scale, Parallel computing, Composite likelihood, Spatial\nprocess\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 06:53:43 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 02:47:58 GMT"}, {"version": "v3", "created": "Wed, 1 Aug 2018 16:19:50 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Li", "Yongxiang", ""], ["Zhou", "Qiang", ""], ["Tsui", "Kwok Leung", ""], ["Cabrera", "Javier", ""]]}, {"id": "1804.08901", "submitter": "Xavier Bry", "authors": "Xavier Bry (UM), Lionel Cucala (UM)", "title": "Classifying variable-structures: a general framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we unify recent variable-clustering techniques within a common\ngeometric framework which allows to extend clustering to variable-structures,\ni.e. variable-subsets within which links between variables are taken into\nconsideration in a given way. All variables being measured on the same n\nstatistical units, we first represent every variable-structure with a unit-norm\noperator in $\\mathbb{R}^{n\\times n}$. We consider either the euclidean\nchord-distance or the geodesic distance on the unit-sphere of\n$\\mathbb{R}^{n\\times n}$. Then, we introduce the notion of rank-H average of\nsuch operators as the rank-H solution of a compound distance-minimisation\nprogram. Finally, we propose a K-means-type algorithm using the rank-H average\nas centroid to perform variable-structure clustering. The method is tested on\nsimulated data and applied to wine data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 08:47:56 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Bry", "Xavier", "", "UM"], ["Cucala", "Lionel", "", "UM"]]}, {"id": "1804.08962", "submitter": "Elsa Cazelles", "authors": "J\\'er\\'emie Bigot and Elsa Cazelles and Nicolas Papadakis", "title": "Data-driven regularization of Wasserstein barycenters with an\n  application to multivariate density registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to simultaneously align and smooth data in the form of\nmultiple point clouds sampled from unknown densities with support in a\nd-dimensional Euclidean space. This work is motivated by applications in\nbioinformatics where researchers aim to automatically homogenize large datasets\nto compare and analyze characteristics within a same cell population.\nInconveniently, the information acquired is most certainly noisy due to\nmis-alignment caused by technical variations of the environment. To overcome\nthis problem, we propose to register multiple point clouds by using the notion\nof regularized barycenters (or Fr\\'{e}chet mean) of a set of probability\nmeasures with respect to the Wasserstein metric. A first approach consists in\npenalizing a Wasserstein barycenter with a convex functional as recently\nproposed in Bigot and al. (2018). A second strategy is to transform the\nWasserstein metric itself into an entropy regularized transportation cost\nbetween probability measures as introduced in Cuturi (2013). The main\ncontribution of this work is to propose data-driven choices for the\nregularization parameters involved in each approach using the\nGoldenshluger-Lepski's principle. Simulated data sampled from Gaussian mixtures\nare used to illustrate each method, and an application to the analysis of flow\ncytometry data is finally proposed. This way of choosing of the regularization\nparameter for the Sinkhorn barycenter is also analyzed through the prism of an\noracle inequality that relates the error made by such data-driven estimators to\nthe one of an ideal estimator.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 11:34:06 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 09:17:15 GMT"}, {"version": "v3", "created": "Sat, 4 May 2019 15:26:17 GMT"}, {"version": "v4", "created": "Tue, 27 Aug 2019 13:48:40 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Bigot", "J\u00e9r\u00e9mie", ""], ["Cazelles", "Elsa", ""], ["Papadakis", "Nicolas", ""]]}, {"id": "1804.09216", "submitter": "Elizabeth Hou", "authors": "Elizabeth Hou, Yasin Yilmaz, and Alfred Hero", "title": "Anomaly Detection in Partially Observed Traffic Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2892026", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of detecting anomalous activity in traffic\nnetworks where the network is not directly observed. Given knowledge of what\nthe node-to-node traffic in a network should be, any activity that differs\nsignificantly from this baseline would be considered anomalous. We propose a\nBayesian hierarchical model for estimating the traffic rates and detecting\nanomalous changes in the network. The probabilistic nature of the model allows\nus to perform statistical goodness-of-fit tests to detect significant\ndeviations from a baseline network. We show that due to the more defined\nstructure of the hierarchical Bayesian model, such tests perform well even when\nthe empirical models estimated by the EM algorithm are misspecified. We apply\nour model to both simulated and real datasets to demonstrate its superior\nperformance over existing alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 19:03:17 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 19:07:20 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Hou", "Elizabeth", ""], ["Yilmaz", "Yasin", ""], ["Hero", "Alfred", ""]]}, {"id": "1804.09252", "submitter": "Geir Drage Berentsen Dr.", "authors": "Geir D. Berentsen, Jan Bulla, Antonello Maruotti and B{\\aa}rd St{\\o}ve", "title": "Modelling corporate defaults: A Markov-switching Poisson log-linear\n  autoregressive model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article extends the autoregressive count time series model class by\nallowing for a model with regimes, that is, some of the parameters in the model\ndepend on the state of an unobserved Markov chain. We develop a quasi-maximum\nlikelihood estimator by adapting the extended Hamilton-Grey algorithm for the\nPoisson log-linear autoregressive model, and we perform a simulation study to\ncheck the finite sample behaviour of the estimator. The motivation for the\nmodel comes from the study of corporate defaults, in particular the study of\ndefault clustering. We provide evidence that time series of counts of US\nmonthly corporate defaults consists of two regimes and that the so-called\ncontagion effect, that is current defaults affect the probability of other\nfirms defaulting in the future, is present in one of these regimes, even after\ncontrolling for financial and economic covariates. We further find evidence for\nthat the covariate effects are different in each of the two regimes. Our\nresults imply that the notion of contagion in the default count process is\ntime-dependent, and thus more dynamic than previously believed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 20:46:59 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Berentsen", "Geir D.", ""], ["Bulla", "Jan", ""], ["Maruotti", "Antonello", ""], ["St\u00f8ve", "B\u00e5rd", ""]]}, {"id": "1804.09285", "submitter": "Cristian Oliva", "authors": "Cristian Oliva-Aviles and Mary C. Meyer and Jean D. Opsomer", "title": "Estimation and inference of domain means subject to shape constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population domain means are frequently expected to respect shape or order\nconstraints that arise naturally with survey data. For example, given a job\ncategory, mean salaries in big cities might be expected to be higher than those\nin small cities, but no order might be available to be imposed within big or\nsmall cities. A design-based estimator of domain means that imposes constraints\non the most common survey estimators is proposed. Inequality restrictions that\ncan be expressed with irreducible matrices are considered, as these cover a\nbroad class of shapes and partial orderings. The constrained estimator is shown\nto be consistent and asymptotically normally distributed under mild conditions,\ngiven that the shape is a reasonable assumption for the population. Further,\nsimulation experiments demonstrate that both estimation and variability of\ndomain means are improved by the constrained estimator in comparison with usual\nunconstrained estimators, especially for small domains. An application of the\nproposed estimator to the 2015 U.S. National Survey of College Graduates is\nshown.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 22:50:26 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Oliva-Aviles", "Cristian", ""], ["Meyer", "Mary C.", ""], ["Opsomer", "Jean D.", ""]]}, {"id": "1804.09305", "submitter": "Quoc Dung Cao", "authors": "Quoc Dung Cao, Youngjun Choe", "title": "Cross-Entropy Based Importance Sampling for Stochastic Simulation Models", "comments": null, "journal-ref": null, "doi": "10.1016/j.ress.2019.106526", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To efficiently evaluate system reliability based on Monte Carlo simulation,\nimportance sampling is used widely. The optimal importance sampling density was\nderived in 1950s for the deterministic simulation model, which maps an input to\nan output deterministically, and is approximated in practice using various\nmethods. For the stochastic simulation model whose output is random given an\ninput, the optimal importance sampling density was derived only recently. In\nthe existing literature, metamodel-based approaches have been used to\napproximate this optimal density. However, building a satisfactory metamodel is\noften difficult or time-consuming in practice. This paper proposes a\ncross-entropy based method, which is automatic and does not require specific\ndomain knowledge. The proposed method uses an expectation-maximization\nalgorithm to guide the choice of a mixture distribution model for approximating\nthe optimal density. The method iteratively updates the approximated density to\nminimize its estimated discrepancy, measured by estimated cross-entropy, from\nthe optimal density. The mixture model's complexity is controlled using the\ncross-entropy information criterion. The method is empirically validated using\nextensive numerical studies and applied to a case study of evaluating the\nreliability of wind turbine using a stochastic simulation model.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 01:04:04 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 15:31:01 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 20:49:56 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Cao", "Quoc Dung", ""], ["Choe", "Youngjun", ""]]}, {"id": "1804.09329", "submitter": "Mengyang Gu", "authors": "Mengyang Gu", "title": "Jointly Robust Prior for Gaussian Stochastic Process in Emulation,\n  Calibration and Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian stochastic process (GaSP) has been widely used in two fundamental\nproblems in uncertainty quantification, namely the emulation and calibration of\nmathematical models. Some objective priors, such as the reference prior, are\nstudied in the context of emulating (approximating) computationally expensive\nmathematical models. In this work, we introduce a new class of priors, called\nthe jointly robust prior, for both the emulation and calibration. This prior is\ndesigned to maintain various advantages from the reference prior. In emulation,\nthe jointly robust prior has an appropriate tail decay rate as the reference\nprior, and is computationally simpler than the reference prior in parameter\nestimation. Moreover, the marginal posterior mode estimation with the jointly\nrobust prior can separate the influential and inert inputs in mathematical\nmodels, while the reference prior does not have this property. We establish the\nposterior propriety for a large class of priors in calibration, including the\nreference prior and jointly robust prior in general scenarios, but the jointly\nrobust prior is preferred because the calibrated mathematical model typically\npredicts the reality well. The jointly robust prior is used as the default\nprior in two new R packages, called \"RobustGaSP\" and \"RobustCalibration\",\navailable on CRAN for emulation and calibration, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 02:56:37 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 21:00:36 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Gu", "Mengyang", ""]]}, {"id": "1804.09567", "submitter": "Lu Wang", "authors": "Lu Wang, Zhengwu Zhang and David Dunson", "title": "Symmetric Bilinear Regression for Signal Subgraph Estimation", "comments": "12 pages, double columns, 18 figures", "journal-ref": null, "doi": "10.1109/TSP.2019.2899818", "report-no": null, "categories": "stat.ME eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing interest in learning a set of small outcome-relevant\nsubgraphs in network-predictor regression. The extracted signal subgraphs can\ngreatly improve the interpretation of the association between the network\npredictor and the response. In brain connectomics, the brain network for an\nindividual corresponds to a set of interconnections among brain regions and\nthere is a strong interest in linking the brain connectome to human cognitive\ntraits. Modern neuroimaging technology allows a very fine segmentation of the\nbrain, producing very large structural brain networks. Therefore, accurate and\nefficient methods for identifying a set of small predictive subgraphs become\ncrucial, leading to discovery of key interconnected brain regions related to\nthe trait and important insights on the mechanism of variation in human\ncognitive traits. We propose a symmetric bilinear model with $L_1$ penalty to\nsearch for small clique subgraphs that contain useful information about the\nresponse. A coordinate descent algorithm is developed to estimate the model\nwhere we derive analytical solutions for a sequence of conditional convex\noptimizations. Application of this method on human connectome and language\ncomprehension data shows interesting discovery of relevant interconnections\namong several small sets of brain regions and better predictive performance\nthan competitors.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 13:47:00 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 04:36:43 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Wang", "Lu", ""], ["Zhang", "Zhengwu", ""], ["Dunson", "David", ""]]}, {"id": "1804.09590", "submitter": "Anna Heath", "authors": "Anna Heath, Ioanna Manolopoulou and Gianluca Baio", "title": "Estimating the Expected Value of Sample Information across Different\n  Sample Sizes using Moment Matching and Non-Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The Expected Value of Sample Information (EVSI) determines the\neconomic value of any future study with a specific design aimed at reducing\nuncertainty in a health economic model. This has potential as a tool for trial\ndesign; the cost and value of different designs could be compared to find the\ntrial with the greatest net benefit. However, despite recent developments, EVSI\nanalysis can be slow especially when optimising over a large number of\ndifferent designs. Methods: This paper develops a method to reduce the\ncomputation time required to calculate the EVSI across different sample sizes.\nOur method extends the moment matching approach to EVSI estimation to optimise\nover different sample sizes for the underlying trial with a similar\ncomputational cost to a single EVSI estimate. This extension calculates\nposterior variances across the alternative sample sizes and then uses Bayesian\nnon-linear regression to calculate the EVSI. Results: A health economic model\ndeveloped to assess the cost-effectiveness of interventions for chronic pain\ndemonstrates that this EVSI calculation method is fast and accurate for\nrealistic models. This example also highlights how different trial designs can\nbe compared using the EVSI. Conclusion: The proposed estimation method is fast\nand accurate when calculating the EVSI across different sample sizes. This will\nallow researchers to realise the potential of using the EVSI to determine an\neconomically optimal trial design for reducing uncertainty in health economic\nmodels. Limitations: Our method relies on some additional simulation, which can\nbe expensive in models with very large computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 14:23:43 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Heath", "Anna", ""], ["Manolopoulou", "Ioanna", ""], ["Baio", "Gianluca", ""]]}, {"id": "1804.09753", "submitter": "Emmanuel Candes", "authors": "Emmanuel J. Candes and Pragya Sur", "title": "The phase transition for the existence of the maximum likelihood\n  estimate in high-dimensional logistic regression", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper rigorously establishes that the existence of the maximum\nlikelihood estimate (MLE) in high-dimensional logistic regression models with\nGaussian covariates undergoes a sharp `phase transition'. We introduce an\nexplicit boundary curve $h_{\\text{MLE}}$, parameterized by two scalars\nmeasuring the overall magnitude of the unknown sequence of regression\ncoefficients, with the following property: in the limit of large sample sizes\n$n$ and number of features $p$ proportioned in such a way that $p/n \\rightarrow\n\\kappa$, we show that if the problem is sufficiently high dimensional in the\nsense that $\\kappa > h_{\\text{MLE}}$, then the MLE does not exist with\nprobability one. Conversely, if $\\kappa < h_{\\text{MLE}}$, the MLE\nasymptotically exists with probability one.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 18:53:11 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Candes", "Emmanuel J.", ""], ["Sur", "Pragya", ""]]}, {"id": "1804.09804", "submitter": "Russell Bowater", "authors": "Russell J. Bowater", "title": "Multivariate subjective fiducial inference", "comments": "Final version with corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to firmly establish subjective fiducial inference as\na rival to the more conventional schools of statistical inference, and to show\nthat Fisher's intuition concerning the importance of the fiducial argument was\ncorrect. In this regard, methodology outlined in an earlier paper is modified,\nenhanced and extended to deal with general inferential problems in which\nvarious parameters are unknown. As a key part of what is put forward, the joint\nfiducial distribution of all the parameters of a given model is determined on\nthe basis of the full conditional fiducial distributions of these parameters by\nusing an analytical approach or a Gibbs sampling method, the latter of which\ndoes not require these conditional distributions to be compatible. Although the\nresulting theory is classified as being \"subjective\", this is essentially due\nto the argument that all probability statements made about fixed but unknown\nparameters must be inherently subjective. In particular, it is systematically\nargued that, in general, there is no need to place a great emphasis on the\ndifference between the fiducial probabilities derived by using this theory of\ninference and objective probabilities. Some important examples of the\napplication of this theory are presented.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 21:25:26 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 16:38:29 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2019 16:50:28 GMT"}, {"version": "v4", "created": "Thu, 4 Jun 2020 16:27:30 GMT"}, {"version": "v5", "created": "Wed, 7 Apr 2021 16:35:26 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Bowater", "Russell J.", ""]]}, {"id": "1804.09866", "submitter": "Ke Zhu", "authors": "Guochang Wang, Wai Keung Li, Ke Zhu", "title": "New HSIC-based tests for independence between two stationary\n  multivariate time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes some novel one-sided omnibus tests for independence\nbetween two multivariate stationary time series. These new tests apply the\nHilbert-Schmidt independence criterion (HSIC) to test the independence between\nthe innovations of both time series. Under regular conditions, the limiting\nnull distributions of our HSIC-based tests are established. Next, our\nHSIC-based tests are shown to be consistent. Moreover, a residual bootstrap\nmethod is used to obtain the critical values for our HSIC-based tests, and its\nvalidity is justified. Compared with the existing cross-correlation-based tests\nfor linear dependence, our tests examine the general (including both linear and\nnon-linear) dependence to give investigators more complete information on the\ncausal relationship between two multivariate time series. The merits of our\ntests are illustrated by some simulation results and a real example.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 02:57:00 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Wang", "Guochang", ""], ["Li", "Wai Keung", ""], ["Zhu", "Ke", ""]]}, {"id": "1804.10098", "submitter": "Marius Thomas", "authors": "Marius Thomas, Bj\\\"orn Bornkamp and Heidi Seibold", "title": "Subgroup identification in dose-finding trials via model-based recursive\n  partitioning", "comments": "23 pages, 6 figures", "journal-ref": "Statistics in medicine, 37(10), 1608-1624 (2018)", "doi": "10.1002/sim.7594", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in early phase drug development is to identify patients,\nwhich respond better or worse to an experimental treatment. While a variety of\ndifferent subgroup identification methods have been developed for the situation\nof trials that study an experimental treatment and control, much less work has\nbeen done in the situation when patients are randomized to different dose\ngroups. In this article we propose new strategies to perform subgroup analyses\nin dose-finding trials and discuss the challenges, which arise in this new\nsetting. We consider model-based recursive partitioning, which has recently\nbeen applied to subgroup identification in two arm trials, as a promising\nmethod to tackle these challenges and assess its viability using a real trial\nexample and simulations. Our results show that model-based recursive\npartitioning can be used to identify subgroups of patients with different\ndose-response curves and improves estimation of treatment effects and minimum\neffective doses, when heterogeneity among patients is present.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 14:57:01 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Thomas", "Marius", ""], ["Bornkamp", "Bj\u00f6rn", ""], ["Seibold", "Heidi", ""]]}, {"id": "1804.10118", "submitter": "Takuya Ura", "authors": "Luis E. Candelaria and Takuya Ura", "title": "Identification and Inference of Network Formation Games with\n  Misclassified Links", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a network formation model when links are potentially\nmeasured with error. We focus on a game-theoretical model of strategic network\nformation with incomplete information, in which the linking decisions depend on\nagents' exogenous attributes and endogenous network characteristics. In the\npresence of link misclassification, we derive moment conditions that\ncharacterize the identified set for the preference parameters associated with\nhomophily and network externalities. Based on the moment equality conditions,\nwe provide an inference method that is asymptotically valid when a single\nnetwork of many agents is observed. Finally, we apply our proposed method to\nstudy trust networks in rural villages in southern India.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 15:45:16 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 16:29:16 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 17:14:52 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Candelaria", "Luis E.", ""], ["Ura", "Takuya", ""]]}, {"id": "1804.10255", "submitter": "Peter Bubenik", "authors": "Vic Patrangenaru, Peter Bubenik, Robert L. Paige, and Daniel Osborne", "title": "Topological Data Analysis for Object Data", "comments": "16 pages, 12 figures", "journal-ref": "Sankhya A, 2019, Volume 81-A, Part 1, pp. 244-271", "doi": "10.1007/s13171-018-0137-7", "report-no": null, "categories": "stat.ME math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis on object data presents many challenges. Basic summaries\nsuch as means and variances are difficult to compute. We apply ideas from\ntopology to study object data. We present a framework for using persistence\nlandscapes to vectorize object data and perform statistical analysis. We apply\nto this pipeline to some biological images that were previously shown to be\nchallenging to study using shape theory. Surprisingly, the most persistent\nfeatures are shown to be \"topological noise\" and the statistical analysis\ndepends on the less persistent features which we refer to as the \"geometric\nsignal\". We also describe the first steps to a new approach to using topology\nfor object data analysis, which applies topology to distributions on object\nspaces.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 19:30:50 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Patrangenaru", "Vic", ""], ["Bubenik", "Peter", ""], ["Paige", "Robert L.", ""], ["Osborne", "Daniel", ""]]}, {"id": "1804.10256", "submitter": "Saharon Rosset", "authors": "Saharon Rosset, Ruth Heller, Amichai Painsky, Ehud Aharoni", "title": "Optimal and Maximin Procedures for Multiple Testing Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple testing problems are a staple of modern statistical analysis. The\nfundamental objective of multiple testing procedures is to reject as many false\nnull hypotheses as possible (that is, maximize some notion of power), subject\nto controlling an overall measure of false discovery, like family-wise error\nrate (FWER) or false discovery rate (FDR). In this paper we formulate multiple\ntesting of simple hypotheses as an infinite-dimensional optimization problem,\nseeking the most powerful rejection policy which guarantees strong control of\nthe selected measure. In that sense, our approach is a generalization of the\noptimal Neyman-Pearson test for a single hypothesis. We show that for\nexchangeable hypotheses, for both FWER and FDR and relevant notions of power,\nthese problems can be formulated as infinite linear programs and can in\nprinciple be solved for any number of hypotheses. We also characterize maximin\nrules for complex alternatives, and demonstrate that such rules can be found in\npractice, leading to improved practical procedures compared to existing\nalternatives. We derive explicit optimal tests for FWER or FDR control for\nthree independent normal means. We find that the power gain over natural\ncompetitors is substantial in all settings examined. Finally, we apply our\noptimal maximin rule to subgroup analyses in systematic reviews from the\nCochrane library, leading to an increase in the number of findings while\nguaranteeing strong FWER control against the one sided alternative.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 19:34:11 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 16:22:52 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 19:14:47 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Rosset", "Saharon", ""], ["Heller", "Ruth", ""], ["Painsky", "Amichai", ""], ["Aharoni", "Ehud", ""]]}, {"id": "1804.10397", "submitter": "Nadja Klein Dr.", "authors": "Nadja Klein and Michael Stanley Smith", "title": "Implicit Copulas from Bayesian Regularized Regression Smoothers", "comments": null, "journal-ref": "Bayesian Anal. 14 (2019), no. 4, 1143--1171", "doi": "10.1214/18-BA1138", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to extract the implicit copula of a response vector from a\nBayesian regularized regression smoother with Gaussian disturbances. The copula\ncan be used to compare smoothers that employ different shrinkage priors and\nfunction bases. We illustrate with three popular choices of shrinkage priors\n--- a pairwise prior, the horseshoe prior and a g prior augmented with a point\nmass as employed for Bayesian variable selection --- and both univariate and\nmultivariate function bases. The implicit copulas are high-dimensional, have\nflexible dependence structures that are far from that of a Gaussian copula, and\nare unavailable in closed form. However, we show how they can be evaluated by\nfirst constructing a Gaussian copula conditional on the regularization\nparameters, and then integrating over these. Combined with non-parametric\nmargins the regularized smoothers can be used to model the distribution of\nnon-Gaussian univariate responses conditional on the covariates. Efficient\nMarkov chain Monte Carlo schemes for evaluating the copula are given for this\ncase. Using both simulated and real data, we show how such copula smoothing\nmodels can improve the quality of resulting function estimates and predictive\ndistributions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 08:54:45 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 11:47:03 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Klein", "Nadja", ""], ["Smith", "Michael Stanley", ""]]}, {"id": "1804.10527", "submitter": "Nazih Benoumechiara", "authors": "Nazih Benoumechiara (LPSM, EDF R and D PRISME), Bertrand Michel\n  (LMJL), Philippe Saint-Pierre (IMT), Nicolas Bousquet (LPSM)", "title": "Detecting and modeling worst-case dependence structures between random\n  inputs of computational reliability models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertain information on input parameters of reliability models is usually\nmodeled by considering these parameters as random, and described by marginal\ndistributions and a dependence structure of these variables. In numerous\nreal-world applications, while information is mainly provided by marginal\ndistributions, typically from samples , little is really known on the\ndependence structure itself. Faced with this problem of incomplete or missing\ninformation, risk studies are often conducted by considering independence of\ninput variables, at the risk of including irrelevant situations. This approach\nis especially used when reliability functions are considered as black-box\ncomputational models. Such analyses remain weakened in absence of in-depth\nmodel exploration, at the possible price of a strong risk misestimation.\nConsidering the frequent case where the reliability output is a quantile, this\narticle provides a methodology to improve risk assessment, by exploring a set\nof pessimistic dependencies using a copula-based strategy. In dimension greater\nthan two, a greedy algorithm is provided to build input regular vine copulas\nreaching a minimum quantile to which a reliability admissible limit value can\nbe compared, by selecting pairwise components of sensitive influence on the\nresult. The strategy is tested over toy models and a real industrial\ncase-study. The results highlight that current approaches can provide\nnon-conservative results, and that a nontrivial dependence structure can be\nexhibited to define a worst-case scenario.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 14:15:42 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Benoumechiara", "Nazih", "", "LPSM, EDF R and D PRISME"], ["Michel", "Bertrand", "", "LMJL"], ["Saint-Pierre", "Philippe", "", "IMT"], ["Bousquet", "Nicolas", "", "LPSM"]]}, {"id": "1804.10671", "submitter": "Munir Winkel", "authors": "Munir A. Winkel and Jonathan W. Stallings and Curt B. Storlie and\n  Brian J. Reich", "title": "Sequential Optimization in Locally Important Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing an expensive, black-box function $f(\\cdot)$ is challenging when\nits input space is high-dimensional. Sequential design frameworks first model\n$f(\\cdot)$ with a surrogate function and then optimize an acquisition function\nto determine input settings to evaluate next. Optimization of both $f(\\cdot)$\nand the acquisition function benefit from effective dimension reduction. Global\nvariable selection detects and removes input variables that do not affect\n$f(\\cdot)$ across the input space. Further dimension reduction may be possible\nif we consider local variable selection around the current optimum estimate. We\ndevelop a sequential design algorithm called Sequential Optimization in Locally\nImportant Dimensions (SOLID) that incorporates global and local variable\nselection to optimize a continuous, differentiable function. SOLID performs\nlocal variable selection by comparing the surrogate's predictions in a\nlocalized region around the estimated optimum with the $p$ alternative\npredictions made by removing each input variable. The search space of the\nacquisition function is further restricted to focus only on the variables that\nare deemed locally active, leading to greater emphasis on refining the\nsurrogate model in locally active dimensions. A simulation study across three\ntest functions and an application to the Sarcos robot dataset show that SOLID\noutperforms conventional approaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 20:17:02 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2019 15:46:23 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Winkel", "Munir A.", ""], ["Stallings", "Jonathan W.", ""], ["Storlie", "Curt B.", ""], ["Reich", "Brian J.", ""]]}, {"id": "1804.10675", "submitter": "Hyo Young Choi", "authors": "Hyo Young Choi and J. S. Marron", "title": "Estimation of the number of spikes using a generalized spike population\n  model and application to RNA-seq data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a generalized spike population model has been actively studied in\nrandom matrix theory, its application to real data has been rarely explored. We\nfind that most methods for determining the number of spikes based on the\nJohnstone's spike population model choose far too many spikes in RNA-seq gene\nexpression data or often fail to determine the number of spikes by indicating\nthat all components are spikes. In this paper, we propose a new algorithm for\nthe estimation of the number of spikes based on a generalized spike population\nmodel. Also, we suggest a new noise model for RNA-seq data based on population\nspectral distribution ideas, which provides a biologically reasonable number of\nspikes using the proposed algorithm. Furthermore, we propose a graphical tool\nfor assessing the performance of the underlying noise model.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 20:23:04 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Choi", "Hyo Young", ""], ["Marron", "J. S.", ""]]}, {"id": "1804.10820", "submitter": "Helton Saulo", "authors": "Helton Saulo, Jeremias Le\\~ao, Roberto Vila, Victor Leiva, Vera\n  Tomazella", "title": "On a bivariate Birnbaum-Saunders distribution parameterized by its\n  means: features, reliability analysis and application", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Birnbaum-Saunders models have been widely used to model positively skewed\ndata. In this paper, we introduce a bivariate Birnbaum-Saunders distribution\nwhich has the means as parameters. We present some properties of the univariate\nand bivariate Birnbaum-Saunders models. We discuss the maximum likelihood and\nmodified moment estimation of the model parameters and associated inference. A\nsimulation study is conducted to evaluate the performance of the maximum\nlikelihood and modified moment estimators. The probability coverages of\nconfidence intervals are also discussed. Finally, a real-world data analysis is\ncarried out for illustrating the proposed model.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 15:25:34 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Saulo", "Helton", ""], ["Le\u00e3o", "Jeremias", ""], ["Vila", "Roberto", ""], ["Leiva", "Victor", ""], ["Tomazella", "Vera", ""]]}, {"id": "1804.10887", "submitter": "Yuchao Liu", "authors": "Yuchao Liu, Jiaqi Guo", "title": "Distribution-Free, Size Adaptive Submatrix Detection with Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large matrix containing independent data entries, we consider the\nproblem of detecting a submatrix inside the data matrix that contains\nlarger-than-usual values. Different from previous literature, we do not have\nexact information about the dimension of the potential elevated submatrix. We\npropose a Bonferroni type testing procedure based on permutation tests, and\nshow that our proposed test loses no first-order asymptotic power compared to\ntests with full knowledge of potential elevated submatrix. In order to speed up\nthe calculation during the test, an approximation net is constructed and we\nshow that Bonferroni type permutation test on the approximation net loses no\npower on the first order asymptotically.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 08:00:53 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 07:16:54 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Liu", "Yuchao", ""], ["Guo", "Jiaqi", ""]]}, {"id": "1804.10928", "submitter": "Tanujit Chakraborty", "authors": "Tanujit Chakraborty and Ashis Kumar Chakraborty and C.A. Murthy", "title": "A Nonparametric Ensemble Binary Classifier and its Statistical\n  Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an ensemble of classification trees (CT) and\nartificial neural networks (ANN). Several statistical properties including\nuniversal consistency and upper bound of an important parameter of the proposed\nclassifier are shown. Numerical evidence is also provided using various real\nlife data sets to assess the performance of the model. Our proposed\nnonparametric ensemble classifier doesn't suffer from the `curse of\ndimensionality' and can be used in a wide variety of feature selection cum\nclassification problems. Performance of the proposed model is quite better when\ncompared to many other state-of-the-art models used for similar situations.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 13:41:58 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 12:01:02 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chakraborty", "Tanujit", ""], ["Chakraborty", "Ashis Kumar", ""], ["Murthy", "C. A.", ""]]}, {"id": "1804.10950", "submitter": "Abhijit Mandal", "authors": "Ayanendranath Basu, Abhijit Mandal, Nirian Martin, Leandro Pardo", "title": "A Robust Wald-type Test for Testing the Equality of Two Means from\n  Log-Normal Samples", "comments": null, "journal-ref": null, "doi": "10.1007/s11009-018-9639-y", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log-normal distribution is one of the most common distributions used for\nmodeling skewed and positive data. It frequently arises in many disciplines of\nscience, specially in the biological and medical sciences. The statistical\nanalysis for comparing the means of two independent log-normal distributions is\nan issue of significant interest. In this paper we present a robust test for\nthis problem. The unknown parameters of the model are estimated by minimum\ndensity power divergence estimators (Basu et al 1998, Biometrika, 85(3),\n549-559). The robustness as well as the asymptotic properties of the proposed\ntest statistics are rigorously established. The performance of the test is\nexplored through simulations and real data analysis. The test is compared with\nsome existing methods, and it is demonstrated that the proposed test\noutperforms the others in the presence of outliers.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 14:59:26 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Basu", "Ayanendranath", ""], ["Mandal", "Abhijit", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1804.10957", "submitter": "Matthew Masten", "authors": "Matthew A. Masten and Alexandre Poirier", "title": "Interpreting Quantile Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should one assess the credibility of assumptions weaker than statistical\nindependence, like quantile independence? In the context of identifying causal\neffects of a treatment variable, we argue that such deviations should be chosen\nbased on the form of selection on unobservables they allow. For quantile\nindependence, we characterize this form of treatment selection. Specifically,\nwe show that quantile independence is equivalent to a constraint on the average\nvalue of either a latent propensity score (for a binary treatment) or the cdf\nof treatment given the unobservables (for a continuous treatment). In both\ncases, this average value constraint requires a kind of non-monotonic treatment\nselection. Using these results, we show that several common treatment selection\nmodels are incompatible with quantile independence. We introduce a class of\nassumptions which weakens quantile independence by removing the average value\nconstraint, and therefore allows for monotonic treatment selection. In a\npotential outcomes model with a binary treatment, we derive identified sets for\nthe ATT and QTT under both classes of assumptions. In a numerical example we\nshow that the average value constraint inherent in quantile independence has\nsubstantial identifying power. Our results suggest that researchers should\ncarefully consider the credibility of this non-monotonicity property when using\nquantile independence to weaken full independence.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 16:09:40 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Masten", "Matthew A.", ""], ["Poirier", "Alexandre", ""]]}, {"id": "1804.11011", "submitter": "Mingwei Dai", "authors": "Mingwei Dai, Xiang Wan, Hao Peng, Yao Wang, Yue Liu, Jin Liu, Zongben\n  Xu and Can Yang", "title": "Joint Analysis of Individual-level and Summary-level GWAS Data by\n  Leveraging Pleiotropy", "comments": "32 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of recent genome-wide association studies (GWASs) for complex\nphenotypes confirm the early conjecture for polygenicity, suggesting the\npresence of large number of variants with only tiny or moderate effects.\nHowever, due to the limited sample size of a single GWAS, many associated\ngenetic variants are too weak to achieve the genome-wide significance. These\nundiscovered variants further limit the prediction capability of GWAS.\nRestricted access to the individual-level data and the increasing availability\nof the published GWAS results motivate the development of methods integrating\nboth the individual-level and summary-level data. How to build the connection\nbetween the individual-level and summary-level data determines the efficiency\nof using the existing abundant summary-level resources with limited\nindividual-level data, and this issue inspires more efforts in the existing\narea.\n  In this study, we propose a novel statistical approach, LEP, which provides a\nnovel way of modeling the connection between the individual-level data and\nsummary-level data. LEP integrates both types of data by \\underline{LE}veraing\n\\underline{P}leiotropy to increase the statistical power of risk variants\nidentification and the accuracy of risk prediction. The algorithm for parameter\nestimation is developed to handle genome-wide-scale data. Through comprehensive\nsimulation studies, we demonstrated the advantages of LEP over the existing\nmethods. We further applied LEP to perform integrative analysis of Crohn's\ndisease from WTCCC and summary statistics from GWAS of some other diseases,\nsuch as Type 1 diabetes, Ulcerative colitis and Primary biliary cirrhosis. LEP\nwas able to significantly increase the statistical power of identifying risk\nvariants and improve the risk prediction accuracy from 63.39\\% ($\\pm$ 0.58\\%)\nto 68.33\\% ($\\pm$ 0.32\\%) using about 195,000 variants.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 01:17:46 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Dai", "Mingwei", ""], ["Wan", "Xiang", ""], ["Peng", "Hao", ""], ["Wang", "Yao", ""], ["Liu", "Yue", ""], ["Liu", "Jin", ""], ["Xu", "Zongben", ""], ["Yang", "Can", ""]]}, {"id": "1804.11072", "submitter": "Eric Klopp", "authors": "Stefan Kl\\\"o{\\ss}ner and Eric Klopp", "title": "Explaining Constraint Interaction: How to Interpret Estimated Model\n  Parameters under Alternative Scaling Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explain the reasons behind constraint interaction, which is\nthe phenomenon that the results of testing equality constraints may depend\nheavily on the scaling method used. We find that the scaling methods interfere\nwith the testing procedures because scaling methods determine which\ntransformations of population quantities model parameters actually estimate. We\ntherefore also develop rules on how to correctly interpret estimates of model\nparameters under alternative scaling methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 08:01:42 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Kl\u00f6\u00dfner", "Stefan", ""], ["Klopp", "Eric", ""]]}, {"id": "1804.11205", "submitter": "Debasis Kundu Professor", "authors": "Debasis Kundu and Vahid Nekoukhou", "title": "On Bivariate Discrete Weibull Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Lee and Cha (2015, `On two generalized classes of discrete\nbivariate distributions', {\\it American Statistician}, 221 - 230) proposed two\ngeneral classes of discrete bivariate distributions. They have discussed some\ngeneral properties and some specific cases of their proposed distributions. In\nthis paper we have considered one model, namely bivariate discrete Weibull\ndistribution, which has not been considered in the literature yet. The proposed\nbivariate discrete Weibull distribution is a discrete analogue of the\nMarshall-Olkin bivariate Weibull distribution. We study various properties of\nthe proposed distribution and discuss its interesting physical interpretations.\nThe proposed model has four parameters, and because of that it is a very\nflexible distribution. The maximum likelihood estimators of the parameters\ncannot be obtained in closed forms, and we have proposed a very efficient\nnested EM algorithm which works quite well for discrete data. We have also\nproposed augmented Gibbs sampling procedure to compute Bayes estimates of the\nunknown parameters based on a very flexible set of priors. Two data sets have\nbeen analyzed to show how the proposed model and the method work in practice.\nWe will see that the performances are quite satisfactory. Finally, we conclude\nthe paper.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 18:50:07 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Kundu", "Debasis", ""], ["Nekoukhou", "Vahid", ""]]}, {"id": "1804.11267", "submitter": "Moritz Schauer", "authors": "Denis Belomestny, Shota Gugushvili, Moritz Schauer, Peter Spreij", "title": "Nonparametric Bayesian inference for Gamma-type L\\'evy subordinators", "comments": null, "journal-ref": "Communications in Mathematical Sciences, Volume 17, Number 3, 2019", "doi": "10.4310/CMS.2019.v17.n3.a8", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given discrete time observations over a growing time interval, we consider a\nnonparametric Bayesian approach to estimation of the L\\'evy density of a L\\'evy\nprocess belonging to a flexible class of infinite activity subordinators.\nPosterior inference is performed via MCMC, and we circumvent the problem of the\nintractable likelihood via the data augmentation device, that in our case\nrelies on bridge process sampling via Gamma process bridges. Our approach also\nrequires the use of a new infinite-dimensional form of a reversible jump MCMC\nalgorithm. We show that our method leads to good practical results in\nchallenging simulation examples. On the theoretical side, we establish that our\nnonparametric Bayesian procedure is consistent: in the low frequency data\nsetting, with equispaced in time observations and intervals between successive\nobservations remaining fixed, the posterior asymptotically, as the sample size\n$n\\rightarrow\\infty$, concentrates around the L\\'evy density under which the\ndata have been generated. Finally, we test our method on a classical insurance\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 15:12:23 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 17:59:48 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Belomestny", "Denis", ""], ["Gugushvili", "Shota", ""], ["Schauer", "Moritz", ""], ["Spreij", "Peter", ""]]}]