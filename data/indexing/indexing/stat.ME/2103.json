[{"id": "2103.00069", "submitter": "Edouard Ollier", "authors": "Edouard Ollier, Pierre Blanchard, Gw\\'ena\\\"el Le Teuff, Stefan\n  Michiels", "title": "Penalized Poisson model for network meta-analysis of individual patient\n  time-to-event data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network meta-analysis (NMA) allows the combination of direct and indirect\nevidence from a set of randomized clinical trials. Performing NMA using\nindividual patient data (IPD) is considered as a gold standard approach as it\nprovides several advantages over NMA based on aggregate data. For example, it\nallows to perform advanced modelling of covariates or covariate-treatment\ninteractions. An important issue in IPD NMA is the selection of influential\nparameters among terms that account for inconsistency, covariates,\ncovariate-by-treatment interactions or non-proportionality of treatments effect\nfor time to event data. This issue has not been deeply studied in the\nliterature yet and in particular not for time-to-event data. A major difficulty\nis to jointly account for between-trial heterogeneity which could have a major\ninfluence on the selection process. The use of penalized generalized mixed\neffect model is a solution, but existing implementations have several\nshortcomings and an important computational cost that precludes their use for\ncomplex IPD NMA. In this article, we propose a penalized Poisson regression\nmodel to perform IPD NMA of time-to-event data. It is based only on fixed\neffect parameters which improve its computational cost over the use of random\neffects. It could be easily implemented using existing penalized regression\npackage. Computer code is shared for implementation. The methods were applied\non simulated data to illustrate the importance to take into account between\ntrial heterogeneity during the selection procedure. Finally, it was applied to\nan IPD NMA of overall survival of chemotherapy and radiotherapy in\nnasopharyngeal carcinoma.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 22:18:04 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ollier", "Edouard", ""], ["Blanchard", "Pierre", ""], ["Teuff", "Gw\u00e9na\u00ebl Le", ""], ["Michiels", "Stefan", ""]]}, {"id": "2103.00117", "submitter": "Xiaojun Zheng", "authors": "Xiaojun Zheng, Simon Mak, Yao Xie", "title": "Online High-Dimensional Change-Point Detection using Topological Data\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.AT stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological Data Analysis (TDA) is a rapidly growing field, which studies\nmethods for learning underlying topological structures present in complex data\nrepresentations. TDA methods have found recent success in extracting useful\ngeometric structures for a wide range of applications, including protein\nclassification, neuroscience, and time-series analysis. However, in many such\napplications, one is also interested in sequentially detecting changes in this\ntopological structure. We propose a new method called Persistence Diagram based\nChange-Point (PD-CP), which tackles this problem by integrating the widely-used\npersistence diagrams in TDA with recent developments in nonparametric\nchange-point detection. The key novelty in PD-CP is that it leverages the\ndistribution of points on persistence diagrams for online detection of\ntopological changes. We demonstrate the effectiveness of PD-CP in an\napplication to solar flare monitoring.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 03:43:57 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 22:10:54 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zheng", "Xiaojun", ""], ["Mak", "Simon", ""], ["Xie", "Yao", ""]]}, {"id": "2103.00209", "submitter": "Yan Liu", "authors": "Yan Liu, Masanobu Taniguchi and Hernando Ombao", "title": "Statistical Inference for Local Granger Causality", "comments": "75 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Granger causality has been employed to investigate causality relations\nbetween components of stationary multiple time series. Here, we generalize this\nconcept by developing statistical inference for local Granger causality for\nmultivariate locally stationary processes. Thus, our proposed local Granger\ncausality approach captures time-evolving causality relationships in\nnonstationary processes. The proposed local Granger causality is well\nrepresented in the frequency domain and estimated based on the parametric\ntime-varying spectral density matrix using the local Whittle likelihood. Under\nregularity conditions, we demonstrate that the estimators converge weakly to a\nGaussian process. Additionally, the test statistic for the local Granger\ncausality is shown to be asymptotically distributed as a quadratic form of a\nmultivariate normal distribution. The finite sample performance is confirmed\nwith several simulation studies for multivariate time-varying VAR models. For\npractical demonstration, the proposed local Granger causality method uncovered\nnew functional connectivity relationships between channels in brain signals.\nMoreover, the method was able to identify structural changes of Granger\ncausality in financial data.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 13:08:56 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Liu", "Yan", ""], ["Taniguchi", "Masanobu", ""], ["Ombao", "Hernando", ""]]}, {"id": "2103.00290", "submitter": "Jin Liu", "authors": "Jin Liu", "title": "Jenss-Bayley Latent Change Score Model with Individual Ratio of Growth\n  Acceleration in the Framework of Individual Measurement Occasions", "comments": "Draft version 1.0, 02/27/2021. This paper has not been peer reviewed.\n  Please do not copy or cite without author's permission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal analysis has been widely employed to examine between-individual\ndifferences in within-individual change. One challenge for such analyses lies\nin that the rate-of-change is only available indirectly when change patterns\nare nonlinear with respect to time. Latent change score models (LCSMs), which\ncan be employed to investigate the change in growth rate at the individual\nlevel, have been developed to address this challenge. We extend an existing\nLCSM with the Jenss-Bayley growth curve (Grimm et al., 2016c) and propose a\nnovel expression of change scores that allows for (1) unequally-spaced study\nwaves and (2) individual measurement occasions around each wave. We also extend\nthe existing model to estimate the individual ratio of growth acceleration\n(that largely determines the trajectory shape and then is viewed as the most\nimportant parameter in the Jenss-Bayley model). We present the proposed model\nby simulation studies and a real-world data analysis. Our simulation studies\ndemonstrate that the proposed model generally estimates the parameters of\ninterest unbiasedly, precisely and exhibits appropriate confidence interval\ncoverage. More importantly, the proposed model with the novel expression of\nchange scores performed better than the existing model shown by simulation\nstudies. An empirical example using longitudinal reading scores shows that the\nmodel can estimate the individual ratio of growth acceleration and generate\nindividual growth rate in practice. We also provide the corresponding code for\nthe proposed model.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 18:38:16 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Liu", "Jin", ""]]}, {"id": "2103.00304", "submitter": "Andrew Giffin", "authors": "Andrew Giffin, Brian J. Reich, Shu Yang, Ana G. Rappold", "title": "Instrumental variables, spatial confounding and interference", "comments": "27 pages, 5 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unobserved spatial confounding variables are prevalent in environmental and\necological applications where the system under study is complex and the data\nare often observational. Instrumental variables (IVs) are a common way to\naddress unobserved confounding; however, the efficacy of using IVs on spatial\nconfounding is largely unknown. This paper explores the effectiveness of IVs in\nthis situation -- with particular attention paid to the spatial scale of the\ninstrument. We show that, in case of spatially-dependent treatments, IVs are\nmost effective when they vary at a finer spatial resolution than the treatment.\nWe investigate IV performance in extensive simulations and apply the model in\nthe example of long term trends in the air pollution and cardiovascular\nmortality in the United States over 1990-2010. Finally, the IV approach is also\nextended to the spatial interference setting, in which treatments can affect\nnearby responses.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 19:38:06 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Giffin", "Andrew", ""], ["Reich", "Brian J.", ""], ["Yang", "Shu", ""], ["Rappold", "Ana G.", ""]]}, {"id": "2103.00315", "submitter": "Juan Sosa", "authors": "Juan Sosa and Lina Buitrago", "title": "Time-Varying Coefficient Model Estimation Through Radial Basis Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we estimate the dynamic parameters of a time-varying\ncoefficient model through radial kernel functions in the context of a\nlongitudinal study. Our proposal is based on a linear combination of weighted\nkernel functions involving a bandwidth, centered around a given set of time\npoints. In addition, we study different alternatives of estimation and\ninference including a Frequentist approach using weighted least squares along\nwith bootstrap methods, and a Bayesian approach through both Markov chain Monte\nCarlo and variational methods. We compare the estimation strategies mention\nabove with each other, and our radial kernel functions proposal with an\nexpansion based on regression spline, by means of an extensive simulation study\nconsidering multiples scenarios in terms of sample size, number of repeated\nmeasurements, and subject-specific correlation. Our experiments show that the\ncapabilities of our proposal based on radial kernel functions are indeed\ncomparable with or even better than those obtained from regression splines. We\nillustrate our methodology by analyzing data from two AIDS clinical studies.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 20:36:05 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sosa", "Juan", ""], ["Buitrago", "Lina", ""]]}, {"id": "2103.00486", "submitter": "Mark He", "authors": "Mark He, Dylan Lu, Jason Xu, Rose Mary Xavier", "title": "Community Detection in Weighted Multilayer Networks with Ambient Noise", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel class of stochastic blockmodel for multilayer weighted\nnetworks that accounts for the presence of a global ambient noise governing\nbetween-block interactions. We induce a hierarchy of classifications in\nweighted multilayer networks by assuming that all but one cluster (block) are\ngoverned by unique local signals, while a single block behaves identically as\ninteractions across differing blocks (ambient noise). Hierarchical variational\ninference is employed to jointly detect and typologize blocks as signal or\nnoise. We call this model for multilayer weighted networks the Stochastic Block\n(with) Ambient Noise Model(SBANM) and develop an associated community detection\nalgorithm. Then we apply this method to subjects in the Philadelphia\nNeurodevelopmental Cohort to discover communities of subjects with similar\npsychopathological symptoms in relation to psychosis.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 07:47:28 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 05:05:35 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 15:08:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["He", "Mark", ""], ["Lu", "Dylan", ""], ["Xu", "Jason", ""], ["Xavier", "Rose Mary", ""]]}, {"id": "2103.00512", "submitter": "Benjamin Eltzner", "authors": "Benjamin Eltzner, Shayan Hundrieser, Stephan F. Huckemann", "title": "Finite Sample Smeariness on Spheres", "comments": "8 pages, 4 figures, conference paper, GSI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite Sample Smeariness (FSS) has been recently discovered. It means that\nthe distribution of sample Fr\\'echet means of underlying rather unsuspicious\nrandom variables can behave as if it were smeary for quite large regimes of\nfinite sample sizes. In effect classical quantile-based statistical testing\nprocedures do not preserve nominal size, they reject too often under the null\nhypothesis. Suitably designed bootstrap tests, however, amend for FSS. On the\ncircle it has been known that arbitrarily sized FSS is possible, and that all\ndistributions with a nonvanishing density feature FSS. These results are\nextended to spheres of arbitrary dimension. In particular all rotationally\nsymmetric distributions, not necessarily supported on the entire sphere feature\nFSS of Type I. While on the circle there is also FSS of Type II it is\nconjectured that this is not possible on higher-dimensional spheres.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 13:42:10 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Eltzner", "Benjamin", ""], ["Hundrieser", "Shayan", ""], ["Huckemann", "Stephan F.", ""]]}, {"id": "2103.00518", "submitter": "Yasuyuki Hamura", "authors": "Yasuyuki Hamura", "title": "Bayesian Point Estimation and Predictive Density Estimation for the\n  Binomial Distribution with a Restricted Probability Parameter", "comments": "29 pages, 4 figures; Theorem 3.3 and Sections 4 and 6 have been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider Bayesian point estimation and predictive density\nestimation in the binomial case. After presenting preliminary results on these\nproblems, we compare the risk functions of the Bayes estimators based on the\ntruncated and untruncated beta priors and obtain dominance conditions when the\nprobability parameter is less than or equal to a known constant. The case where\nthere are both a lower bound restriction and an upper bound restriction is also\ntreated. Then our problems are shown to be related to similar problems in the\nPoisson case. Finally, numerical studies are presented.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 14:08:41 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 10:27:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Hamura", "Yasuyuki", ""]]}, {"id": "2103.00527", "submitter": "Xavier de Luna", "authors": "Seong-ho Lee, Yanyuan Ma and Xavier de Luna", "title": "Covariate balancing for causal inference on categorical and continuous\n  treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose novel estimators for categorical and continuous treatments by\nusing an optimal covariate balancing strategy for inverse probability\nweighting. The resulting estimators are shown to be consistent and\nasymptotically normal for causal contrasts of interest, either when the model\nexplaining treatment assignment is correctly specified, or when the correct set\nof bases for the outcome models has been chosen and the assignment model is\nsufficiently rich. For the categorical treatment case, we show that the\nestimator attains the semiparametric efficiency bound when all models are\ncorrectly specified. For the continuous case, the causal parameter of interest\nis a function of the treatment dose. The latter is not parametrized and the\nestimators proposed are shown to have bias and variance of the classical\nnonparametric rate. Asymptotic results are complemented with simulations\nillustrating the finite sample properties. Our analysis of a data set suggests\na nonlinear effect of BMI on the decline in self reported health.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 14:52:16 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Lee", "Seong-ho", ""], ["Ma", "Yanyuan", ""], ["de Luna", "Xavier", ""]]}, {"id": "2103.00533", "submitter": "Raphael Huser", "authors": "Peng Zhong, Rapha\\\"el Huser and Thomas Opitz", "title": "Exact Simulation of Max-Infinitely Divisible Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-infinitely divisible (max-id) processes play a central role in\nextreme-value theory and include the subclass of all max-stable processes. They\nallow for a constructive representation based on the componentwise maximum of\nrandom functions drawn from a Poisson point process defined on a suitable\nfunctions space. Simulating from a max-id process is often difficult due to its\ncomplex stochastic structure, while calculating its joint density in high\ndimensions is often numerically infeasible. Therefore, exact and efficient\nsimulation techniques for max-id processes are useful tools for studying the\ncharacteristics of the process and for drawing statistical inferences. Inspired\nby the simulation algorithms for max-stable processes, we here develop theory\nand algorithms to generalize simulation approaches tailored for certain\nflexible (existing or new) classes of max-id processes. Efficient simulation\nfor a large class of models can be achieved by implementing an adaptive\nrejection sampling scheme to sidestep a numerical integration step in the\nalgorithm. We present the results of a simulation study highlighting that our\nsimulation algorithm works as expected and is highly accurate and efficient,\nsuch that it clearly outperforms customary approximate sampling schemes. As a\nbyproduct, we also develop here new max-id models, which can be represented as\npointwise maxima of general location scale mixtures, and which possess flexible\ntail dependence structures capturing a wide range of asymptotic dependence\nscenarios.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 15:17:35 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhong", "Peng", ""], ["Huser", "Rapha\u00ebl", ""], ["Opitz", "Thomas", ""]]}, {"id": "2103.00553", "submitter": "Guillaume Basse", "authors": "Kevin Wu Han, Iavor Bojinov and Guillaume Basse", "title": "Population Interference in Panel Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phenomenon of population interference, where a treatment assigned to one\nexperimental unit affects another experimental unit's outcome, has received\nconsiderable attention in standard randomized experiments. The complications\nproduced by population interference in this setting are now readily recognized,\nand partial remedies are well known. Much less understood is the impact of\npopulation interference in panel experiments where treatment is sequentially\nrandomized in the population, and the outcomes are observed at each time step.\nThis paper proposes a general framework for studying population interference in\npanel experiments and presents new finite population estimation and inference\nresults. Our findings suggest that, under mild assumptions, the addition of a\ntemporal dimension to an experiment alleviates some of the challenges of\npopulation interference for certain estimands. In contrast, we show that the\npresence of carryover effects -- that is, when past treatments may affect\nfuture outcomes -- exacerbates the problem. Revisiting the special case of\nstandard experiments with population interference, we prove a central limit\ntheorem under weaker conditions than previous results in the literature and\nhighlight the trade-off between flexibility in the design and the interference\nstructure.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 16:24:35 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Han", "Kevin Wu", ""], ["Bojinov", "Iavor", ""], ["Basse", "Guillaume", ""]]}, {"id": "2103.00567", "submitter": "Guillaume Basse", "authors": "Hui Xu and Guillaume Basse", "title": "Randomization Inference for Composite Experiments with Spillovers and\n  Peer Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group-formation experiments, in which experimental units are randomly\nassigned to groups, are a powerful tool for studying peer effects in the social\nsciences. Existing design and analysis approaches allow researchers to draw\ninference from such experiments without relying on parametric assumptions. In\npractice, however, group-formation experiments are often coupled with a second,\nexternal intervention, that is not accounted for by standard nonparametric\napproaches. This note shows how to construct Fisherian randomization tests and\nNeymanian asymptotic confidence intervals for such composite experiments,\nincluding in settings where the second intervention exhibits spillovers. We\nalso propose an approach for designing optimal composite experiments.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 17:20:59 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xu", "Hui", ""], ["Basse", "Guillaume", ""]]}, {"id": "2103.00569", "submitter": "Shuoyang Wang", "authors": "Shuoyang Wang, Zuofeng Shang, Guanqun Cao", "title": "Optimal Imperfect Classification for Gaussian Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works on functional data classification focus on the construction of\nclassifiers that achieve perfect classification in the sense that\nclassification risk converges to zero asymptotically. In practical\napplications, perfect classification is often impossible since the optimal\nBayes classifier may have asymptotically nonzero risk. Such a phenomenon is\ncalled as imperfect classification. In the case of Gaussian functional data, we\nexploit classification problem in imperfect classification scenario. Sharp\nconvergence rates for minimax excess risk are derived when data functions are\neither fully observed or discretely observed. Easily implementable classifiers\nbased on discriminant analysis are proposed which are proven to achieve minimax\noptimality. In discretely observed case, we discover a critical sampling\nfrequency that governs the sharp convergence rates. The proposed classifiers\nperform favorably in finite-sample applications, as we demonstrate through\ncomparisons with other functional classifiers in simulations and one real data\napplication.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 17:32:13 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Shuoyang", ""], ["Shang", "Zuofeng", ""], ["Cao", "Guanqun", ""]]}, {"id": "2103.00580", "submitter": "Wenkai Xu", "authors": "Wenkai Xu and Gesine Reinert", "title": "A Stein Goodness of fit Test for Exponential Random Graph Models", "comments": null, "journal-ref": "Proceedings of the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2021", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyse a novel nonparametric goodness of fit testing\nprocedure for exchangeable exponential random graph models (ERGMs) when a\nsingle network realisation is observed. The test determines how likely it is\nthat the observation is generated from a target unnormalised ERGM density. Our\ntest statistics are derived from a kernel Stein discrepancy, a divergence\nconstructed via Steins method using functions in a reproducing kernel Hilbert\nspace, combined with a discrete Stein operator for ERGMs. The test is a Monte\nCarlo test based on simulated networks from the target ERGM. We show\ntheoretical properties for the testing procedure for a class of ERGMs.\nSimulation studies and real network applications are presented.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 18:16:41 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xu", "Wenkai", ""], ["Reinert", "Gesine", ""]]}, {"id": "2103.00588", "submitter": "Benjamin Eltzner", "authors": "Pernille Hansen, Benjamin Eltzner, Stefan Sommer", "title": "Diffusion Means and Heat Kernel on Manifolds", "comments": "8 pages, 1 figure, conference paper submitted to GSI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce diffusion means as location statistics on manifold data spaces.\nA diffusion mean is defined as the starting point of an isotropic diffusion\nwith a given diffusivity. They can therefore be defined on all spaces on which\na Brownian motion can be defined and numerical calculation of sample diffusion\nmeans is possible on a variety of spaces using the heat kernel expansion. We\npresent several classes of spaces, for which the heat kernel is known and\nsample diffusion means can therefore be calculated. As an example, we\ninvestigate a classic data set from directional statistics, for which the\nsample Fr\\'echet mean exhibits finite sample smeariness.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 19:01:34 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hansen", "Pernille", ""], ["Eltzner", "Benjamin", ""], ["Sommer", "Stefan", ""]]}, {"id": "2103.00605", "submitter": "Shuxi Zeng", "authors": "Shuxi Zeng, Fan Li, Liangyuan Hu, Fan Li", "title": "Propensity Score Weighting Analysis of Survival Outcomes Using\n  Pseudo-observations", "comments": "63 pages, 12 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival outcomes are common in comparative effectiveness studies and require\nunique handling because they are usually incompletely observed due to\nright-censoring. A ``once for all'' approach for causal inference with survival\noutcomes constructs pseudo-observations and allows standard methods such as\npropensity score weighting to proceed as if the outcomes are completely\nobserved. We propose a general class of model-free causal estimands with\nsurvival outcomes on user-specified target populations. We develop\ncorresponding propensity score weighting estimators based on the\npseudo-observations and establish their asymptotic properties. In particular,\nutilizing the functional delta-method and the von Mises expansion, we derive a\nnew closed-form variance of the weighting estimator that takes into account the\nuncertainty due to both pseudo-observation calculation and propensity score\nestimation. This allows valid and computationally efficient inference without\nresampling. We also prove the optimal efficiency property of the overlap\nweights within the class of balancing weights for survival outcomes. The\nproposed methods are applicable to both binary and multiple treatments.\nExtensive simulations are conducted to explore the operating characteristics of\nthe proposed method versus other commonly used alternatives. We apply the\nproposed method to compare the causal effects of three popular treatment\napproaches for prostate cancer patients.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 20:07:25 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zeng", "Shuxi", ""], ["Li", "Fan", ""], ["Hu", "Liangyuan", ""], ["Li", "Fan", ""]]}, {"id": "2103.00627", "submitter": "Aldo Solari", "authors": "Aldo Solari and Vera Djordjilovi\\'c", "title": "Multi Split Conformal Prediction", "comments": "12 pages, 1 figure, 2 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Split conformal prediction is a computationally efficient method for\nperforming distribution-free predictive inference in regression. It involves,\nhowever, a one-time random split of the data, and the result depends on the\nparticular split. To address this problem, we propose multi split conformal\nprediction, a simple method based on Markov's inequality to aggregate single\nsplit conformal prediction intervals across multiple splits.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 21:31:42 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 12:48:22 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Solari", "Aldo", ""], ["Djordjilovi\u0107", "Vera", ""]]}, {"id": "2103.00631", "submitter": "Tao Zou", "authors": "Tao Zou, Xian Li, Xuan Liang, Hansheng Wang", "title": "On the Subbagging Estimation for Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces subbagging (subsample aggregating) estimation\napproaches for big data analysis with memory constraints of computers.\nSpecifically, for the whole dataset with size $N$, $m_N$ subsamples are\nrandomly drawn, and each subsample with a subsample size $k_N\\ll N$ to meet the\nmemory constraint is sampled uniformly without replacement. Aggregating the\nestimators of $m_N$ subsamples can lead to subbagging estimation. To analyze\nthe theoretical properties of the subbagging estimator, we adapt the incomplete\n$U$-statistics theory with an infinite order kernel to allow overlapping drawn\nsubsamples in the sampling procedure. Utilizing this novel theoretical\nframework, we demonstrate that via a proper hyperparameter selection of $k_N$\nand $m_N$, the subbagging estimator can achieve $\\sqrt{N}$-consistency and\nasymptotic normality under the condition $(k_Nm_N)/N\\to \\alpha \\in (0,\\infty]$.\nCompared to the full sample estimator, we theoretically show that the\n$\\sqrt{N}$-consistent subbagging estimator has an inflation rate of $1/\\alpha$\nin its asymptotic variance. Simulation experiments are presented to demonstrate\nthe finite sample performances. An American airline dataset is analyzed to\nillustrate that the subbagging estimate is numerically close to the full sample\nestimate, and can be computationally fast under the memory constraint.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 21:38:22 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zou", "Tao", ""], ["Li", "Xian", ""], ["Liang", "Xuan", ""], ["Wang", "Hansheng", ""]]}, {"id": "2103.00648", "submitter": "Zhong Guan", "authors": "Zhong Guan", "title": "Maximum Approximate Bernstein Likelihood Estimation of Densities in a\n  Two-sample Semiparametric Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Maximum likelihood estimators are proposed for the parameters and the\ndensities in a semiparametric density ratio model in which the nonparametric\nbaseline density is approximated by the Bernstein polynomial model. The EM\nalgorithm is used to obtain the maximum approximate Bernstein likelihood\nestimates. Simulation study shows that the performance of the proposed method\nis much better than the existing ones. The proposed method is illustrated by\nreal data examples. Some asymptotic results are also presented and proved.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 22:33:22 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Guan", "Zhong", ""]]}, {"id": "2103.00674", "submitter": "Kai Zhang", "authors": "Kai Zhang, Zhigen Zhao, Wen Zhou", "title": "BEAUTY Powered BEAST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.AP stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study inference about the uniform distribution with the proposed binary\nexpansion approximation of uniformity (BEAUTY) approach. Through an extension\nof the celebrated Euler's formula, we approximate the characteristic function\nof any copula distribution with a linear combination of means of binary\ninteractions from marginal binary expansions. This novel characterization\nenables a unification of many important existing tests through an approximation\nfrom some quadratic form of symmetry statistics, where the deterministic weight\nmatrix characterizes the power properties of each test. To achieve a uniformly\nhigh power, we study test statistics with data-adaptive weights through an\noracle approach, referred to as the binary expansion adaptive symmetry test\n(BEAST). By utilizing the properties of the binary expansion filtration, we\nshow that the Neyman-Pearson test of uniformity can be approximated by an\noracle weighted sum of symmetry statistics. The BEAST with this oracle leads\nall existing tests we considered in empirical power against all complex forms\nof alternatives. This oracle therefore sheds light on the potential of\nsubstantial improvements in power and on the form of optimal weights under each\nalternative. By approximating this oracle with data-adaptive weights, we\ndevelop the BEAST that improves the empirical power of many existing tests\nagainst a wide spectrum of common alternatives while providing clear\ninterpretation of the form of non-uniformity upon rejection. We illustrate the\nBEAST with a study of the relationship between the location and brightness of\nstars.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 00:36:15 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 18:08:13 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhang", "Kai", ""], ["Zhao", "Zhigen", ""], ["Zhou", "Wen", ""]]}, {"id": "2103.00772", "submitter": "Michael Evans", "authors": "Luai Al Labadi, Michael Evans and Qiaoyu Liang", "title": "ROC Analyses Based on Measuring Evidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  ROC analyses are considered under a variety of assumptions concerning the\ndistributions of a measurement $X$ in two populations. These include the\nbinormal model as well as nonparametric models where little is assumed about\nthe form of distributions. The methodology is based on a characterization of\nstatistical evidence which is dependent on the specification of prior\ndistributions for the unknown population distributions as well as for the\nrelevant prevalence $w$ of the disease in a given population. In all cases,\nelicitation algorithms are provided to guide the selection of the priors.\nInferences are derived for the AUC as well as the cutoff $c$ used for\nclassification and the associated error characteristics.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 05:57:14 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Labadi", "Luai Al", ""], ["Evans", "Michael", ""], ["Liang", "Qiaoyu", ""]]}, {"id": "2103.00807", "submitter": "Yan Wang", "authors": "Yan Wang", "title": "Penalized Projected Kernel Calibration for Computer Models", "comments": "28 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projected kernel calibration is known to be theoretically superior, its loss\nfunction is abbreviated as PK loss function. In this work, we prove the uniform\nconvergence of PK loss function and show that (1) when the sample size is\nlarge, any local minimum point and local maximum point of the $L_2$ loss\nbetween the true process and the computer models is a local minimum point of\nthe PK loss function; (2) all the local minimum values of the PK loss function\nconverge to the same value. These theoretical results imply that it is\nextremely hard for the projected kernel calibration to identify the global\nminimum point of the $L_2$ loss which is defined as the optimal value of the\ncalibration parameters. To solve this problem, a frequentist method, called the\npenalized projected kernel calibration method is proposed. As a frequentist\nmethod, the proposed method is proved to be semi-parametric efficient. On the\nother hand, the proposed method has a natural bayesian version, which allows\nusers to calculate the credible region of the calibration parameters without\nusing a large sample approximation. Through extensive simulation studies and a\nreal-world case study, we show that the proposed calibration can accurately\nestimate the calibration parameters, and compare favorably to alternative\ncalibration methods regardless of the sample size.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:05:43 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Yan", ""]]}, {"id": "2103.00808", "submitter": "Sebastian Engelke", "authors": "Jasper Velthoen, Cl\\'ement Dombry, Juan-Juan Cai, Sebastian Engelke", "title": "Gradient boosting for extreme quantile regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extreme quantile regression provides estimates of conditional quantiles\noutside the range of the data. Classical methods such as quantile random\nforests perform poorly in such cases since data in the tail region are too\nscarce. Extreme value theory motivates to approximate the conditional\ndistribution above a high threshold by a generalized Pareto distribution with\ncovariate dependent parameters. This model allows for extrapolation beyond the\nrange of observed values and estimation of conditional extreme quantiles. We\npropose a gradient boosting procedure to estimate a conditional generalized\nPareto distribution by minimizing its deviance. Cross-validation is used for\nthe choice of tuning parameters such as the number of trees and the tree\ndepths. We discuss diagnostic plots such as variable importance and partial\ndependence plots, which help to interpret the fitted models. In simulation\nstudies we show that our gradient boosting procedure outperforms classical\nmethods from quantile regression and extreme value theory, especially for\nhigh-dimensional predictor spaces and complex parameter response surfaces. An\napplication to statistical post-processing of weather forecasts with\nprecipitation data in the Netherlands is proposed.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:11:19 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Velthoen", "Jasper", ""], ["Dombry", "Cl\u00e9ment", ""], ["Cai", "Juan-Juan", ""], ["Engelke", "Sebastian", ""]]}, {"id": "2103.00834", "submitter": "Quinten Meertens", "authors": "Quinten Meertens and Cees Diks and Jaap van den Herik and Frank Takes", "title": "Improving the output quality of official statistics based on machine\n  learning algorithms", "comments": "19 pages, 3 figures, submitted to the Journal of Official Statistics\n  on 14 December 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  National statistical institutes currently investigate how to improve the\noutput quality of official statistics based on machine learning algorithms. A\nkey obstacle is concept drift, i.e., when the joint distribution of independent\nvariables and a dependent (categorical) variable changes over time. Under\nconcept drift, a statistical model requires regular updating to prevent it from\nbecoming biased. However, updating a model asks for additional data, which are\nnot always available. In the literature, we find a variety of bias correction\nmethods as a promising solution. In the paper, we will compare two popular\ncorrection methods: the misclassification estimator and the calibration\nestimator. For prior probability shift (a specific type of concept drift), we\ninvestigate the two correction methods theoretically as well as experimentally.\nOur theoretical results are expressions for the bias and variance of both\nmethods. As experimental result, we present a decision boundary (as a function\nof (a) model accuracy, (b) class distribution and (c) test set size) for the\nrelative performance of the two methods. Close inspection of the results will\nprovide a deep insight into the effect of prior probability shift on output\nquality, leading to practical recommendations on the use of machine learning\nalgorithms in official statistics.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 08:05:38 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Meertens", "Quinten", ""], ["Diks", "Cees", ""], ["Herik", "Jaap van den", ""], ["Takes", "Frank", ""]]}, {"id": "2103.00895", "submitter": "Wenkai Xu", "authors": "Wenkai Xu and Takeru Matsuda", "title": "Interpretable Stein Goodness-of-fit Tests on Riemannian Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, we encounter data on Riemannian manifolds such as torus\nand rotation groups. Standard statistical procedures for multivariate data are\nnot applicable to such data. In this study, we develop goodness-of-fit testing\nand interpretable model criticism methods for general distributions on\nRiemannian manifolds, including those with an intractable normalization\nconstant. The proposed methods are based on extensions of kernel Stein\ndiscrepancy, which are derived from Stein operators on Riemannian manifolds. We\ndiscuss the connections between the proposed tests with existing ones and\nprovide a theoretical analysis of their asymptotic Bahadur efficiency.\nSimulation results and real data applications show the validity of the proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 10:42:07 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xu", "Wenkai", ""], ["Matsuda", "Takeru", ""]]}, {"id": "2103.00977", "submitter": "Helga Wagner Dr.", "authors": "Helga Wagner, Sylvia Fr\\\"uhwirth-Schnatter and Liana Jacobi", "title": "Factor-augmented Bayesian treatment effects models for panel outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new, flexible model for inference of the effect of a binary\ntreatment on a continuous outcome observed over subsequent time periods. The\nmodel allows to seperate association due to endogeneity of treatment selection\nfrom additional longitudinal association of the outcomes and hence unbiased\nestimation of dynamic treatment effects.\n  We investigate the performance of the proposed method on simulated data and\nemploy it to reanalyse data on the longitudinal effects of a long maternity\nleave\n  on mothers' earnings after their return to the labour market.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 13:12:40 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wagner", "Helga", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""], ["Jacobi", "Liana", ""]]}, {"id": "2103.01060", "submitter": "Tijana Levajkovic", "authors": "Tijana Levajkovic and Michael Messer", "title": "Multiscale change point detection via gradual bandwidth adjustment in\n  moving sum processes", "comments": "24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for the detection of changes in the expectation in univariate\nsequences is provided. Moving sum processes are studied. These rely on the\nselection of a tuning bandwidth. Here, a framework to overcome bandwidth\nselection is presented - the bandwidth adjusts gradually. For that, moving sum\nprocesses are made dependent on both time and the bandwidth: the domain becomes\na triangle. On the triangle, paths are constructed which systematically lead to\nchange points. An algorithm is provided that estimates change points by\nsubsequent consideration of paths. Strong consistency for the number and\nlocation of change points is shown. Simulations support estimation precision. A\ncompanion R-package mscp is made available on CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 15:15:46 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Levajkovic", "Tijana", ""], ["Messer", "Michael", ""]]}, {"id": "2103.01085", "submitter": "Akash Kumar Dhaka", "authors": "Akash Kumar Dhaka, Alejandro Catalina, Manushi Welandawe, Michael Riis\n  Andersen, Jonathan Huggins, Aki Vehtari", "title": "Challenges and Opportunities in High-dimensional Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current black-box variational inference (BBVI) methods require the user to\nmake numerous design choices -- such as the selection of variational objective\nand approximating family -- yet there is little principled guidance on how to\ndo so. We develop a conceptual framework and set of experimental tools to\nunderstand the effects of these choices, which we leverage to propose best\npractices for maximizing posterior approximation accuracy. Our approach is\nbased on studying the pre-asymptotic tail behavior of the density ratios\nbetween the joint distribution and the variational approximation, then\nexploiting insights and tools from the importance sampling literature. Our\nframework and supporting experiments help to distinguish between the behavior\nof BBVI methods for approximating low-dimensional versus\nmoderate-to-high-dimensional posteriors. In the latter case, we show that\nmass-covering variational objectives are difficult to optimize and do not\nimprove accuracy, but flexible variational families can improve accuracy and\nthe effectiveness of importance sampling -- at the cost of additional\noptimization challenges. Therefore, for moderate-to-high-dimensional posteriors\nwe recommend using the (mode-seeking) exclusive KL divergence since it is the\neasiest to optimize, and improving the variational family or using model\nparameter transformations to make the posterior and optimal variational\napproximation more similar. On the other hand, in low-dimensional settings, we\nshow that heavy-tailed variational families and mass-covering divergences are\neffective and can increase the chances that the approximation can be improved\nby importance sampling.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 15:53:34 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 16:48:09 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Dhaka", "Akash Kumar", ""], ["Catalina", "Alejandro", ""], ["Welandawe", "Manushi", ""], ["Andersen", "Michael Riis", ""], ["Huggins", "Jonathan", ""], ["Vehtari", "Aki", ""]]}, {"id": "2103.01097", "submitter": "Karthik Bharath", "authors": "Min Ho Cho, Sebastian Kurtek, Karthik Bharath", "title": "Tangent functional canonical correlation analysis for densities and\n  shapes, with applications to multimodal imaging data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is quite common for functional data arising from imaging data to assume\nvalues in infinite-dimensional manifolds. Uncovering associations between two\nor more such nonlinear functional data extracted from the same object across\nmedical imaging modalities can assist development of personalized treatment\nstrategies. We propose a method for canonical correlation analysis between\npaired probability densities or shapes of closed planar curves, routinely used\nin biomedical studies, which combines a convenient linearization and dimension\nreduction of the data using tangent space coordinates. Leveraging the fact that\nthe corresponding manifolds are submanifolds of unit Hilbert spheres, we\ndescribe how finite-dimensional representations of the functional data objects\ncan be easily computed, which then facilitates use of standard multivariate\ncanonical correlation analysis methods. We further construct and visualize\ncanonical variate directions directly on the space of densities or shapes.\nUtility of the method is demonstrated through numerical simulations and\nperformance on a magnetic resonance imaging dataset of Glioblastoma Multiforme\nbrain tumors.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:05:01 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Cho", "Min Ho", ""], ["Kurtek", "Sebastian", ""], ["Bharath", "Karthik", ""]]}, {"id": "2103.01132", "submitter": "Antony Overstall", "authors": "Antony M. Overstall, James M. McGree", "title": "General Bayesian $L^2$ calibration of mathematical models", "comments": "First draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A general Bayesian method for $L^2$ calibration of a mathematical model is\npresented. General Bayesian inference starts with the specification of a loss\nfunction. Then, the log-likelihood in Bayes' theorem is replaced by the\nnegative loss. While the minimiser of the loss function is unchanged by, for\nexample, multiplying the loss by a constant, the same is not true of the\nresulting general posterior distribution. To address this problem in the\ncontext of $L^2$ calibration of mathematical models, different automatic\nscalings of the general Bayesian posterior are proposed. These are based on\nequating asymptotic properties of the general Bayesian posterior and the\nminimiser of the $L^2$ loss.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 17:06:43 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Overstall", "Antony M.", ""], ["McGree", "James M.", ""]]}, {"id": "2103.01218", "submitter": "Luis Nieto-Barajas Dr.", "authors": "Luis Nieto-Barajas and Eduardo Guti\\'errez-Pe\\~na", "title": "General dependence structures for some models based on exponential\n  families with quadratic variance functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a procedure to introduce general dependence structures on a set\nof random variables. These include order-$q$ moving average-type structures, as\nwell as seasonal, periodic and spatial dependences. The invariant marginal\ndistribution can be in any family that is conjugate to an exponential family\nwith quadratic variance functions. Dependence is induced via latent variables\nwhose conditional distribution mirrors the sampling distribution in a Bayesian\nconjugate analysis of such exponential families. We obtain strict stationarity\nas a special case.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 20:00:40 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Nieto-Barajas", "Luis", ""], ["Guti\u00e9rrez-Pe\u00f1a", "Eduardo", ""]]}, {"id": "2103.01252", "submitter": "Christopher Hans", "authors": "Christopher M. Hans, Mario Peruggia and Junyan Wang", "title": "Empirical Bayes Model Averaging with Influential Observations: Tuning\n  Zellner's g Prior for Predictive Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the behavior of Bayesian model averaging (BMA) for the normal\nlinear regression model in the presence of influential observations that\ncontribute to model misfit, and propose remedies to attenuate the potential\nnegative impacts of such observations on inference and prediction. The\nmethodology is motivated by the view that well-behaved residuals and good\npredictive performance often go hand-in-hand. The study focuses on regression\nmodels that use variants on Zellner's g prior. By studying the impact of\nvarious forms of model misfit on BMA predictions in simple situations we\nidentify prescriptive guidelines for \"tuning\" Zellner's g prior to obtain\noptimal predictions. The tuning of the prior distribution is obtained by\nconsidering theoretical properties that should be enjoyed by the optimal fits\nof the various models in the BMA ensemble. The methodology can be thought of as\nan \"empirical Bayes\" approach to modeling, as the data help to inform the\nspecification of the prior in an attempt to attenuate the negative impact of\ninfluential cases.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:05:42 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Hans", "Christopher M.", ""], ["Peruggia", "Mario", ""], ["Wang", "Junyan", ""]]}, {"id": "2103.01254", "submitter": "Vincenzo Nardelli", "authors": "Giorgio Alleva, Giuseppe Arbia, Piero Demetrio Falorsi, Vincenzo\n  Nardelli, Alberto Zuliani", "title": "Spatial sampling design to improve the efficiency of the estimation of\n  the critical parameters of the SARS-CoV-2 epidemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pandemic linked to COVID-19 infection represents an unprecedented\nclinical and healthcare challenge for many medical researchers attempting to\nprevent its worldwide spread. This pandemic also represents a major challenge\nfor statisticians involved in quantifying the phenomenon and in offering timely\ntools for the monitoring and surveillance of critical pandemic parameters. In a\nrecent paper, Alleva et al. (2020) proposed a two-stage sample design to build\na continuous-time surveillance system designed to correctly quantify the number\nof infected people through an indirect sampling mechanism that could be\nrepeated in several waves over time to capture different target variables in\nthe different stages of epidemic development. The proposed method exploits the\nindirect sampling (Lavalle, 2007; Kiesl, 2016) method employed in the\nestimation of rare and elusive populations (Borchers, 2009; Lavall\\'ee and\nRivest, 2012) and a capture/recapture mechanism (Sudman, 1988; Thompson and\nSeber, 1996). In this paper, we extend the proposal of Alleva et al. (2020) to\ninclude a spatial sampling mechanism (M\\\"uller, 1998; Grafstr\\\"om et al., 2012,\nJauslin and Till\\`e, 2020) in the process of data collection to achieve the\nsame level of precision with fewer sample units, thereby facilitating the\nprocess of data collection in a situation where timeliness and costs are\ncrucial elements. We present the basic idea of the new sample design,\nanalytically prove the theoretical properties of the associated estimators and\nshow the relative advantages through a systematic simulation study where all\nthe typical elements of an epidemic are accounted for.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:08:44 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Alleva", "Giorgio", ""], ["Arbia", "Giuseppe", ""], ["Falorsi", "Piero Demetrio", ""], ["Nardelli", "Vincenzo", ""], ["Zuliani", "Alberto", ""]]}, {"id": "2103.01280", "submitter": "Davide Viviano Mr.", "authors": "Davide Viviano, Jelena Bradic", "title": "Dynamic covariate balancing: estimating treatment effects over time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the problem of estimation and inference on the effects\nof time-varying treatment. We propose a method for inference on the effects\ntreatment histories, introducing a dynamic covariate balancing method combined\nwith penalized regression. Our approach allows for (i) treatments to be\nassigned based on arbitrary past information, with the propensity score being\nunknown; (ii) outcomes and time-varying covariates to depend on treatment\ntrajectories; (iii) high-dimensional covariates; (iv) heterogeneity of\ntreatment effects. We study the asymptotic properties of the estimator, and we\nderive the parametric convergence rate of the proposed procedure. Simulations\nand an empirical application illustrate the advantage of the method over\nstate-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:53:32 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 17:54:38 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Viviano", "Davide", ""], ["Bradic", "Jelena", ""]]}, {"id": "2103.01281", "submitter": "Theresa Ullmann", "authors": "Theresa Ullmann (1), Christian Hennig (2), Anne-Laure Boulesteix (1)\n  ((1) Institute for Medical Information Processing, Biometry and Epidemiology,\n  LMU Munich, Germany, (2) Dipartimento di Scienze Statistiche \"Paolo\n  Fortunati\", Universita di Bologna, Italy)", "title": "Validation of cluster analysis results on validation data: A systematic\n  framework", "comments": "32 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cluster analysis refers to a wide range of data analytic techniques for class\ndiscovery and is popular in many application fields. To judge the quality of a\nclustering result, different cluster validation procedures have been proposed\nin the literature. While there is extensive work on classical validation\ntechniques, such as internal and external validation, less attention has been\ngiven to validating and replicating a clustering result using a validation\ndataset. Such a dataset may be part of the original dataset, which is separated\nbefore analysis begins, or it could be an independently collected dataset. We\npresent a systematic structured framework for validating clustering results on\nvalidation data that includes most existing validation approaches. In\nparticular, we review classical validation techniques such as internal and\nexternal validation, stability analysis, hypothesis testing, and visual\nvalidation, and show how they can be interpreted in terms of our framework. We\nprecisely define and formalise different types of validation of clustering\nresults on a validation dataset and explain how each type can be implemented in\npractice. Furthermore, we give examples of how clustering studies from the\napplied literature that used a validation dataset can be classified into the\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:53:59 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ullmann", "Theresa", ""], ["Hennig", "Christian", ""], ["Boulesteix", "Anne-Laure", ""]]}, {"id": "2103.01327", "submitter": "Trong-Nghia Nguyen", "authors": "Minh-Ngoc Tran, Trong-Nghia Nguyen, and Viet-Hung Dao", "title": "A practical tutorial on Variational Bayes", "comments": "43 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This tutorial gives a quick introduction to Variational Bayes (VB), also\ncalled Variational Inference or Variational Approximation, from a practical\npoint of view. The paper covers a range of commonly used VB methods and an\nattempt is made to keep the materials accessible to the wide community of data\nanalysis practitioners. The aim is that the reader can quickly derive and\nimplement their first VB algorithm for Bayesian inference with their data\nanalysis problem. An end-user software package in Matlab together with the\ndocumentation can be found at https://vbayeslab.github.io/VBLabDocs/\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 22:11:42 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Nguyen", "Trong-Nghia", ""], ["Dao", "Viet-Hung", ""]]}, {"id": "2103.01355", "submitter": "Weichi Yao", "authors": "Hoora Moradian and Weichi Yao and Denis Larocque and Jeffrey S.\n  Simonoff and Halina Frydman", "title": "Dynamic estimation with random forests for discrete-time survival data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-varying covariates are often available in survival studies and\nestimation of the hazard function needs to be updated as new information\nbecomes available. In this paper, we investigate several different\neasy-to-implement ways that random forests can be used for dynamic estimation\nof the survival or hazard function from discrete-time survival data. The\nresults from a simulation study indicate that all methods can perform well, and\nthat none dominates the others. In general, situations that are more difficult\nfrom an estimation point of view (such as weaker signals and less data) favour\na global fit, pooling over all time points, while situations that are easier\nfrom an estimation point of view (such as stronger signals and more data) favor\nlocal fits.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:46:46 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 17:40:10 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Moradian", "Hoora", ""], ["Yao", "Weichi", ""], ["Larocque", "Denis", ""], ["Simonoff", "Jeffrey S.", ""], ["Frydman", "Halina", ""]]}, {"id": "2103.01356", "submitter": "Ottmar Cronie", "authors": "Ottmar Cronie, Mehdi Moradi, Christophe A.N. Biscio", "title": "Statistical learning and cross-validation for point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the first general (supervised) statistical learning\nframework for point processes in general spaces. Our approach is based on the\ncombination of two new concepts, which we define in the paper: i) bivariate\ninnovations, which are measures of discrepancy/prediction-accuracy between two\npoint processes, and ii) point process cross-validation (CV), which we here\ndefine through point process thinning. The general idea is to carry out the\nfitting by predicting CV-generated validation sets using the corresponding\ntraining sets; the prediction error, which we minimise, is measured by means of\nbivariate innovations. Having established various theoretical properties of our\nbivariate innovations, we study in detail the case where the CV procedure is\nobtained through independent thinning and we apply our statistical learning\nmethodology to three typical spatial statistical settings, namely parametric\nintensity estimation, non-parametric intensity estimation and Papangelou\nconditional intensity fitting. Aside from deriving theoretical properties\nrelated to these cases, in each of them we numerically show that our\nstatistical learning approach outperforms the state of the art in terms of mean\n(integrated) squared error.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:47:48 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Cronie", "Ottmar", ""], ["Moradi", "Mehdi", ""], ["Biscio", "Christophe A. N.", ""]]}, {"id": "2103.01470", "submitter": "Michael Leung", "authors": "Michael P. Leung", "title": "Network Cluster-Robust Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since network data commonly consists of observations on a single large\nnetwork, researchers often partition the network into clusters in order to\napply cluster-robust inference methods. All existing such methods require\nclusters to be asymptotically independent. We prove under mild conditions that,\nin order for this requirement to hold for network-dependent data, it is\nnecessary and sufficient for clusters to have low conductance, the ratio of\nedge boundary size to volume. This yields a simple measure of cluster quality.\nWe find in simulations that, when clusters have low conductance, cluster-robust\nmethods outperform HAC estimators in terms of size control. However, for\nimportant classes of networks lacking low-conductance clusters, the methods can\nexhibit substantial size distortion. To assess the existence of low-conductance\nclusters and construct them, we draw on results in spectral graph theory that\nconnect conductance to the spectrum of the graph Laplacian. Based on these\nresults, we propose to use the spectrum to determine the number of\nlow-conductance clusters and spectral clustering to construct them.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 04:47:47 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 03:48:16 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Leung", "Michael P.", ""]]}, {"id": "2103.01526", "submitter": "Oswaldo Gressani", "authors": "Oswaldo Gressani, Christel Faes, Niel Hens", "title": "Laplacian-P-splines for Bayesian inference in the mixture cure model", "comments": "36 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixture cure model for analyzing survival data is characterized by the\nassumption that the population under study is divided into a group of subjects\nwho will experience the event of interest over some finite time horizon and\nanother group of cured subjects who will never experience the event\nirrespective of the duration of follow-up. When using the Bayesian paradigm for\ninference in survival models with a cure fraction, it is common practice to\nrely on Markov chain Monte Carlo (MCMC) methods to sample from posterior\ndistributions. Although computationally feasible, the iterative nature of MCMC\noften implies long sampling times to explore the target space with chains that\nmay suffer from slow convergence and poor mixing. An alternative strategy for\nfast and flexible sampling-free Bayesian inference in the mixture cure model is\nsuggested in this paper by combining Laplace approximations and penalized\nB-splines. A logistic regression model is assumed for the cure proportion and a\nCox proportional hazards model with a P-spline approximated baseline hazard is\nused to specify the conditional survival function of susceptible subjects.\nLaplace approximations to the conditional latent vector are based on analytical\nformulas for the gradient and Hessian of the log-likelihood, resulting in a\nsubstantial speed-up in approximating posterior distributions. The statistical\nperformance and computational efficiency of the proposed Laplacian-P-splines\nmixture cure (LPSMC) model is assessed in a simulation study. Results show that\nLPSMC is an appealing alternative to classic MCMC for approximate Bayesian\ninference in standard mixture cure models. Finally, the novel LPSMC approach is\nillustrated on three applications involving real survival data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 07:15:27 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 10:01:00 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Gressani", "Oswaldo", ""], ["Faes", "Christel", ""], ["Hens", "Niel", ""]]}, {"id": "2103.01621", "submitter": "Edouard Ollier", "authors": "Edouard Ollier", "title": "Fast selection of nonlinear mixed effect models using penalized\n  likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear Mixed effects models are hidden variables models that are widely\nused in many field such as pharmacometrics. In such models, the distribution\ncharacteristics of hidden variables can be specified by including several\nparameters such as covariates or correlations which must be selected. Recent\ndevelopment of pharmacogenomics has brought averaged/high dimensional problems\nto the field of nonlinear mixed effects modeling for which standard covariates\nselection techniques like stepwise methods are not well suited. This work\nproposes to select covariates and correlation parameters using a penalized\nlikelihood approach. The penalized likelihood problem is solved using a\nstochastic proximal gradient algorithm to avoid inner-outer iterations. Speed\nof convergence of the proximal gradient algorithm is improved by the use of\ncomponent-wise adaptive gradient step sizes. The practical implementation and\ntuning of the proximal gradient algorithm is explored using simulations.\nCalibration of regularization parameters is performed by minimizing the\nBayesian Information Criterion using particle swarm optimization, a zero order\noptimization procedure. The use of warm restart and parallelization allows to\nreduce significantly computing time. The performance of the proposed method\ncompared to the traditional grid search strategy is explored using simulated\ndata. Finally, an application to real data from two pharmacokinetics studies is\nprovided, one studying an antifibrinolitic and the other studying an\nantibiotic.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:24:33 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ollier", "Edouard", ""]]}, {"id": "2103.01650", "submitter": "Sugata Ghosh", "authors": "Sugata Ghosh and Asok K. Nanda", "title": "Conditional Precedence Orders for Stochastic Comparison of Random\n  Variables", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the stochastic orders for comparing random variables, considered in\nthe literature, are afflicted with two main drawbacks: (i) lack of connex\nproperty and (ii) lack of consideration of any dependence structure between the\nrandom variables. Both these drawbacks can be overcome at the cost of\ntransitivity with the stochastic precedence order, which may seem to be a good\nchoice in particular when only two random variables are under consideration, a\nsituation where the question of transitivity does not arise. In this paper, we\nshow that even under such favorable conditions, stochastic precedence order may\ndirect to misleading conclusion in certain situations and develop variations of\nthe order to address the phenomenon.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:23:56 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ghosh", "Sugata", ""], ["Nanda", "Asok K.", ""]]}, {"id": "2103.01727", "submitter": "Sugata Ghosh", "authors": "Sugata Ghosh and Asok K. Nanda", "title": "Departure-based Asymptotic Stochastic Order for Random Processes", "comments": "33 pages, reference hyperlink added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a specific asymptotic stochastic order for random\nprocesses based on the measure of departure discussed in the literature. As\napplications, we stochastically compare mixtures of order statistics and record\nvalues coming from two different homogeneous samples, as the sample size\nbecomes large.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:59:30 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 18:26:22 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Ghosh", "Sugata", ""], ["Nanda", "Asok K.", ""]]}, {"id": "2103.01750", "submitter": "Thong Pham", "authors": "Thong Pham, Paul Sheridan, Hidetoshi Shimodaira", "title": "Nonparametric estimation of the preferential attachment function from\n  one network snapshot", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.data-an physics.soc-ph stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Preferential attachment is commonly invoked to explain the emergence of those\nheavy-tailed degree distributions characteristic of growing network\nrepresentations of diverse real-world phenomena. Experimentally confirming this\nhypothesis in real-world growing networks is an important frontier in network\nscience research. Conventional preferential attachment estimation methods\nrequire that a growing network be observed across at least two snapshots in\ntime. Numerous publicly available growing network datasets are, however, only\navailable as single snapshots, leaving the applied network scientist with no\nmeans of measuring preferential attachment in these cases. We propose a\nnonparametric method, called PAFit-oneshot, for estimating preferential\nattachment in a growing network from one snapshot. PAFit-oneshot corrects for a\npreviously unnoticed bias that arises when estimating preferential attachment\nvalues only for degrees observed in the single snapshot. Our work provides a\nmeans of measuring preferential attachment in a large number of publicly\navailable one-snapshot networks. As a demonstration, we estimated preferential\nattachment in three such networks, and found sublinear preferential attachment\nin all cases. PAFit-oneshot is implemented in the R package PAFit.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 15:23:35 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 06:26:38 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 12:58:48 GMT"}, {"version": "v4", "created": "Mon, 21 Jun 2021 15:07:31 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Pham", "Thong", ""], ["Sheridan", "Paul", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "2103.01802", "submitter": "Liu Leqi", "authors": "Liu Leqi, Edward H. Kennedy", "title": "Median Optimal Treatment Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal treatment regimes are personalized policies for making a treatment\ndecision based on subject characteristics, with the policy chosen to maximize\nsome value. It is common to aim to maximize the mean outcome in the population,\nvia a regime assigning treatment only to those whose mean outcome is higher\nunder treatment versus control. However, the mean can be an unstable measure of\ncentrality, resulting in imprecise statistical procedures, as well as unfair\ndecisions that can be overly influenced by a small fraction of subjects. In\nthis work, we propose a new median optimal treatment regime that instead treats\nindividuals whose conditional median is higher under treatment. This ensures\nthat optimal decisions for individuals from the same group are not overly\ninfluenced either by (i) a small fraction of the group (unlike the mean\ncriterion), or (ii) unrelated subjects from different groups (unlike marginal\nmedian/quantile criteria). We introduce a new measure of value, the Average\nConditional Median Effect (ACME), which summarizes across-group median\ntreatment outcomes of a policy, and which the optimal median treatment regime\nmaximizes. After developing key motivating examples that distinguish median\noptimal treatment regimes from mean and marginal median optimal treatment\nregimes, we give a nonparametric efficiency bound for estimating the ACME of a\npolicy, and propose a new doubly robust-style estimator that achieves the\nefficiency bound under weak conditions. Finite-sample properties of the\nestimator are explored via numerical simulations and the proposed algorithm is\nillustrated using data from a randomized clinical trial in patients with HIV.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 15:26:20 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Leqi", "Liu", ""], ["Kennedy", "Edward H.", ""]]}, {"id": "2103.01992", "submitter": "Mohammadhossein Toutiaee", "authors": "Indrajeet Y. Javeri, Mohammadhossein Toutiaee, Ismailcem B. Arpinar,\n  Tom W. Miller, John A. Miller", "title": "Improving Neural Networks for Time Series Forecasting using Data\n  Augmentation and AutoML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods such as the Box-Jenkins method for time-series\nforecasting have been prominent since their development in 1970. Many\nresearchers rely on such models as they can be efficiently estimated and also\nprovide interpretability. However, advances in machine learning research\nindicate that neural networks can be powerful data modeling techniques, as they\ncan give higher accuracy for a plethora of learning problems and datasets. In\nthe past, they have been tried on time-series forecasting as well, but their\noverall results have not been significantly better than the statistical models\nespecially for intermediate length times series data. Their modeling capacities\nare limited in cases where enough data may not be available to estimate the\nlarge number of parameters that these non-linear models require. This paper\npresents an easy to implement data augmentation method to significantly improve\nthe performance of such networks. Our method, Augmented-Neural-Network, which\ninvolves using forecasts from statistical models, can help unlock the power of\nneural networks on intermediate length time-series and produces competitive\nresults. It shows that data augmentation, when paired with Automated Machine\nLearning techniques such as Neural Architecture Search, can help to find the\nbest neural architecture for a given time-series. Using the combination of\nthese, demonstrates significant enhancement in the forecasting accuracy of\nthree neural network-based models for a COVID-19 dataset, with a maximum\nimprovement in forecasting accuracy by 21.41%, 24.29%, and 16.42%,\nrespectively, over the neural networks that do not use augmented data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 19:20:49 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 18:18:04 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 04:31:10 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Javeri", "Indrajeet Y.", ""], ["Toutiaee", "Mohammadhossein", ""], ["Arpinar", "Ismailcem B.", ""], ["Miller", "Tom W.", ""], ["Miller", "John A.", ""]]}, {"id": "2103.02033", "submitter": "Lauren J Beesley", "authors": "Lauren J Beesley, Irina Bondarenko, Michael R Elliott, Allison W\n  Kurian, Steven J Katz, and Jeremy M G Taylor", "title": "Multiple imputation with missing data indicators", "comments": "See also: Supplemental Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multiple imputation is a well-established general technique for analyzing\ndata with missing values. A convenient way to implement multiple imputation is\nsequential regression multiple imputation (SRMI), also called chained equations\nmultiple imputation. In this approach, we impute missing values using\nregression models for each variable, conditional on the other variables in the\ndata. This approach, however, assumes that the missingness mechanism is missing\nat random, and it is not well-justified under not-at-random missingness without\nadditional modification. In this paper, we describe how we can generalize the\nSRMI imputation procedure to handle not-at-random missingness (MNAR) in the\nsetting where missingness may depend on other variables that are also missing.\nWe provide algebraic justification for several generalizations of standard SRMI\nusing Taylor series and other approximations of the target imputation\ndistribution under MNAR. Resulting regression model approximations include\nindicators for missingness, interactions, or other functions of the MNAR\nmissingness model and observed data. In a simulation study, we demonstrate that\nthe proposed SRMI modifications result in reduced bias in the final analysis\ncompared to standard SRMI, with an approximation strategy involving inclusion\nof an offset in the imputation model performing the best overall. The method is\nillustrated in a breast cancer study, where the goal is to estimate the\nprevalence of a specific genetic pathogenic variant.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 21:26:51 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Beesley", "Lauren J", ""], ["Bondarenko", "Irina", ""], ["Elliott", "Michael R", ""], ["Kurian", "Allison W", ""], ["Katz", "Steven J", ""], ["Taylor", "Jeremy M G", ""]]}, {"id": "2103.02089", "submitter": "Chudamani Poudyal", "authors": "Chudamani Poudyal", "title": "Robust Estimation of Loss Models for Lognormal Insurance Payment\n  Severity Data", "comments": "32 pages", "journal-ref": null, "doi": "10.1017/asb.2021.4", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary objective of this scholarly work is to develop two estimation\nprocedures - maximum likelihood estimator (MLE) and method of trimmed moments\n(MTM) - for the mean and variance of lognormal insurance payment severity data\nsets affected by different loss control mechanism, for example, truncation (due\nto deductibles), censoring (due to policy limits), and scaling (due to\ncoinsurance proportions), in insurance and financial industries. Maximum\nlikelihood estimating equations for both payment-per-payment and\npayment-per-loss data sets are derived which can be solved readily by any\nexisting iterative numerical methods. The asymptotic distributions of those\nestimators are established via Fisher information matrices. Further, with a\ngoal of balancing efficiency and robustness and to remove point masses at\ncertain data points, we develop a dynamic MTM estimation procedures for\nlognormal claim severity models for the above-mentioned transformed data\nscenarios. The asymptotic distributional properties and the comparison with the\ncorresponding MLEs of those MTM estimators are established along with extensive\nsimulation studies. Purely for illustrative purpose, numerical examples for\n1500 US indemnity losses are provided which illustrate the practical\nperformance of the established results in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 23:31:23 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 01:23:37 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Poudyal", "Chudamani", ""]]}, {"id": "2103.02156", "submitter": "Zhaoxia Yu", "authors": "Dustin Pluta, Tong Shen, Gui Xue, Chuansheng Chen, Hernando Ombao,\n  Zhaoxia Yu", "title": "Ridge-penalized adaptive Mantel test and its application in imaging\n  genetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a ridge-penalized adaptive Mantel test (AdaMant) for evaluating\nthe association of two high-dimensional sets of features. By introducing a\nridge penalty, AdaMant tests the association across many metrics\nsimultaneously. We demonstrate how ridge penalization bridges Euclidean and\nMahalanobis distances and their corresponding linear models from the\nperspective of association measurement and testing. This result is not only\ntheoretically interesting but also has important implications in penalized\nhypothesis testing, especially in high dimensional settings such as imaging\ngenetics. Applying the proposed method to an imaging genetic study of visual\nworking memory in health adults, we identified interesting associations of\nbrain connectivity (measured by EEG coherence) with selected genetic features.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 03:42:34 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 05:04:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pluta", "Dustin", ""], ["Shen", "Tong", ""], ["Xue", "Gui", ""], ["Chen", "Chuansheng", ""], ["Ombao", "Hernando", ""], ["Yu", "Zhaoxia", ""]]}, {"id": "2103.02323", "submitter": "F. Richard Guo", "authors": "F. Richard Guo, Thomas S. Richardson and James M. Robins", "title": "Discussion of 'Estimating time-varying causal excursion effect in mobile\n  health with binary outcomes' by T. Qian et al", "comments": "Submitted to Biometrika as an invited discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the recent paper on \"excursion effect\" by T. Qian et al. (2020).\nWe show that the methods presented have close relationships to others in the\nliterature, in particular to a series of papers by Robins, Hern\\'{a}n and\ncollaborators on analyzing observational studies as a series of randomized\ntrials. There is also a close relationship to the history-restricted and the\nhistory-adjusted marginal structural models (MSM). Important differences and\ntheir methodological implications are clarified. We also demonstrate that the\nexcursion effect can depend on the design and discuss its suitability for\nmodifying the treatment protocol.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 10:59:42 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Guo", "F. Richard", ""], ["Richardson", "Thomas S.", ""], ["Robins", "James M.", ""]]}, {"id": "2103.02407", "submitter": "Christopher Drovandi Dr", "authors": "Christopher Drovandi and David T Frazier", "title": "A Comparison of Likelihood-Free Methods With and Without Summary\n  Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-free methods are useful for parameter estimation of complex models\nwith intractable likelihood functions for which it is easy to simulate data.\nSuch models are prevalent in many disciplines including genetics, biology,\necology and cosmology. Likelihood-free methods avoid explicit likelihood\nevaluation by finding parameter values of the model that generate data close to\nthe observed data. The general consensus has been that it is most efficient to\ncompare datasets on the basis of a low dimensional informative summary\nstatistic, incurring information loss in favour of reduced dimensionality. More\nrecently, researchers have explored various approaches for efficiently\ncomparing empirical distributions in the likelihood-free context in an effort\nto avoid data summarisation. This article provides a review of these full data\ndistance based approaches, and conducts the first comprehensive comparison of\nsuch methods, both qualitatively and empirically. We also conduct a substantive\nempirical comparison with summary statistic based likelihood-free methods. The\ndiscussion and results offer guidance to practitioners considering a\nlikelihood-free approach. Whilst we find the best approach to be problem\ndependent, we also find that the full data distance based approaches are\npromising and warrant further development. We discuss some opportunities for\nfuture research in this space.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:56:43 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Drovandi", "Christopher", ""], ["Frazier", "David T", ""]]}, {"id": "2103.02413", "submitter": "Vincenzo Gioia", "authors": "Vincenzo Gioia and Euloge Clovis Kenne Pagui", "title": "Estimation of Dirichlet distribution parameters with bias-reducing\n  adjusted score functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dirichlet distribution, also known as multivariate beta, is the most used\nto analyse frequencies or proportions data. Maximum likelihood is widespread\nfor estimation of Dirichlet's parameters. However, for small sample sizes, the\nmaximum likelihood estimator may shows a significant bias. In this paper,\nDirchlet's parameters estimation is obtained through modified score functions\naiming at mean and median bias reduction of the maximum likelihood estimator,\nrespectively. A simulation study and an application compare the adjusted score\napproaches with maximum likelihood.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 14:02:09 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Gioia", "Vincenzo", ""], ["Pagui", "Euloge Clovis Kenne", ""]]}, {"id": "2103.02470", "submitter": "Thiago dos Santos Rezende", "authors": "Victor S. Comitti, F\\'abio N. Demarqui, Thiago R. dos Santos,\n  J\\'essica da Assun\\c{c}\\~ao Almeida", "title": "Product Partition Dynamic Generalized Linear Models", "comments": "20 pages, 10 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection and modeling of change-points in time-series can be considerably\nchallenging. In this paper we approach this problem by incorporating the class\nof Dynamic Generalized Linear Models (DGLM) into the well know class of Product\nPartition Models (PPM). This new methodology, that we call DGLM-PPM, extends\nthe PPM to distributions within the Exponential Family while also retaining the\nflexibility of the DGLM class. It also provides a framework for Bayesian\nmultiple change-point detection in dynamic regression models. Inference on the\nDGLM-PPM follow the steps of evolution and updating of the DGLM class. A Gibbs\nSampler scheme with an Adaptive Rejection Metropolis Sampling (ARMS) step\nappended is used to compute posterior estimates of the relevant quantities. A\nsimulation study shows that the proposed model provides reasonable estimates of\nthe dynamic parameters and also assigns high change-point probabilities to the\nbreaks introduced in the artificial data generated for this work. We also\npresent a real life data example that highlights the superiority of the\nDGLM-PPM over the conventional DGLM in both in-sample and out-of-sample\ngoodness of fit measures.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:28:35 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Comitti", "Victor S.", ""], ["Demarqui", "F\u00e1bio N.", ""], ["Santos", "Thiago R. dos", ""], ["Almeida", "J\u00e9ssica da Assun\u00e7\u00e3o", ""]]}, {"id": "2103.02582", "submitter": "Matthew Vowels", "authors": "Matthew J. Vowels, Necati Cihan Camgoz, and Richard Bowden", "title": "D'ya like DAGs? A Survey on Structure Learning and Causal Discovery", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal reasoning is a crucial part of science and human intelligence. In\norder to discover causal relationships from data, we need structure discovery\nmethods. We provide a review of background theory and a survey of methods for\nstructure discovery. We primarily focus on modern, continuous optimization\nmethods, and provide reference to further resources such as benchmark datasets\nand software packages. Finally, we discuss the assumptive leap required to take\nus from structure to causality.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:24:26 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 17:37:46 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Vowels", "Matthew J.", ""], ["Camgoz", "Necati Cihan", ""], ["Bowden", "Richard", ""]]}, {"id": "2103.02643", "submitter": "David Benkeser", "authors": "David Benkeser, Iv\\'an D\\'iaz, Jialu Ran", "title": "Inference for natural mediation effects under case-cohort sampling with\n  applications in identifying COVID-19 vaccine correlates of protection", "comments": "26 pages, 6 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Combating the SARS-CoV2 pandemic will require the fast development of\neffective preventive vaccines. Regulatory agencies may open accelerated\napproval pathways for vaccines if an immunological marker can be established as\na mediator of a vaccine's protection. A rich source of information for\nidentifying such correlates are large-scale efficacy trials of COVID-19\nvaccines, where immune responses are measured subject to a case-cohort sampling\ndesign. We propose two approaches to estimation of mediation parameters in the\ncontext of case-cohort sampling designs. We establish the theoretical\nlarge-sample efficiency of our proposed estimators and evaluate them in a\nrealistic simulation to understand whether they can be employed in the analysis\nof COVID-19 vaccine efficacy trials.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 19:15:08 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Benkeser", "David", ""], ["D\u00edaz", "Iv\u00e1n", ""], ["Ran", "Jialu", ""]]}, {"id": "2103.02680", "submitter": "Lizhen Nie", "authors": "Lizhen Nie and Dan L. Nicolae", "title": "Weighted-Graph-Based Change Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the detection and localization of change points in the\ndistribution of an offline sequence of observations. Based on a nonparametric\nframework that uses a similarity graph among observations, we propose new test\nstatistics when at most one change point occurs and generalize them to multiple\nchange points settings. The proposed statistics leverage edge weight\ninformation in the graphs, exhibiting substantial improvements in testing power\nand localization accuracy in simulations. We derive the null limiting\ndistribution, provide accurate analytic approximations to control type I error,\nand establish theoretical guarantees on the power consistency under contiguous\nalternatives for the one change point setting, as well as the minimax\nlocalization rate. In the multiple change points setting, the asymptotic\ncorrectness of the number and location of change points are also guaranteed.\nThe methods are illustrated on the MIT proximity network data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 21:03:32 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Nie", "Lizhen", ""], ["Nicolae", "Dan L.", ""]]}, {"id": "2103.02721", "submitter": "Virgilio Gomez-Rubio", "authors": "Martin Outzen Berild and Sara Martino and Virgilio G\\'omez-Rubio and\n  H{\\aa}vard Rue", "title": "Importance Sampling with the Integrated Nested Laplace Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Integrated Nested Laplace Approximation (INLA) is a deterministic\napproach to Bayesian inference on latent Gaussian models (LGMs) and focuses on\nfast and accurate approximation of posterior marginals for the parameters in\nthe models. Recently, methods have been developed to extend this class of\nmodels to those that can be expressed as conditional LGMs by fixing some of the\nparameters in the models to descriptive values. These methods differ in the\nmanner descriptive values are chosen. This paper proposes to combine importance\nsampling with INLA (IS-INLA), and extends this approach with the more robust\nadaptive multiple importance sampling algorithm combined with INLA (AMIS-INLA).\n  This paper gives a comparison between these approaches and existing methods\non a series of applications with simulated and observed datasets and evaluates\ntheir performance based on accuracy, efficiency, and robustness. The approaches\nare validated by exact posteriors in a simple bivariate linear model; then,\nthey are applied to a Bayesian lasso model, a Bayesian imputation of missing\ncovariate values, and lastly, in parametric Bayesian quantile regression. The\napplications show that the AMIS-INLA approach, in general, outperforms the\nother methods, but the IS-INLA algorithm could be considered for faster\ninference when good proposals are available.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 22:14:01 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Berild", "Martin Outzen", ""], ["Martino", "Sara", ""], ["G\u00f3mez-Rubio", "Virgilio", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "2103.02755", "submitter": "Budhi Arta Surya", "authors": "Halina Frydman, Budhi Surya", "title": "Maximum likelihood estimation for a general mixture of Markov jump\n  processes", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We estimate a general mixture of Markov jump processes. The key novel feature\nof the proposed mixture is that the transition intensity matrices of the Markov\nprocesses comprising the mixture are entirely unconstrained. The Markov\nprocesses are mixed with distributions that depend on the initial state of the\nmixture process. The new mixture is estimated from its continuously observed\nrealizations using the EM algorithm, which provides the maximum likelihood (ML)\nestimates of the mixture's parameters. To obtain the standard errors of the\nestimates of the mixture's parameters, we employ Louis' (1982) general formula\nfor the observed Fisher information matrix. We also derive the asymptotic\nproperties of the ML estimators. Simulation study verifies the estimates'\naccuracy and confirms the consistency and asymptotic normality of the\nestimators. The developed methods are applied to a medical dataset, for which\nthe likelihood ratio test rejects the constrained mixture in favor of the\nproposed unconstrained one. This application exemplifies the usefulness of a\nnew unconstrained mixture for identification and characterization of\nhomogeneous subpopulations in a heterogeneous population.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 23:33:04 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Frydman", "Halina", ""], ["Surya", "Budhi", ""]]}, {"id": "2103.02774", "submitter": "Behtash Babadi", "authors": "Proloy Das and Behtash Babadi", "title": "Non-Asymptotic Guarantees for Robust Identification of Granger Causality\n  via the LASSO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Granger causality is among the widely used data-driven approaches for causal\nanalysis of time series data with applications in various areas including\neconomics, molecular biology, and neuroscience. Two of the main challenges of\nthis methodology are: 1) over-fitting as a result of limited data duration, and\n2) correlated process noise as a confounding factor, both leading to errors in\nidentifying the causal influences. Sparse estimation via the LASSO has\nsuccessfully addressed these challenges for parameter estimation. However, the\nclassical statistical tests for Granger causality resort to asymptotic analysis\nof ordinary least squares, which require long data durations to be useful and\nare not immune to confounding effects. In this work, we close this gap by\nintroducing a LASSO-based statistic and studying its non-asymptotic properties\nunder the assumption that the true models admit sparse autoregressive\nrepresentations. We establish that the sufficient conditions of LASSO also\nsuffice for robust identification of Granger causal influences. We also\ncharacterize the false positive error probability of a simple thresholding rule\nfor identifying Granger causal effects. We present simulation studies and\napplication to real data to compare the performance of the ordinary least\nsquares and LASSO in detecting Granger causal influences, which corroborate our\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 00:47:24 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Das", "Proloy", ""], ["Babadi", "Behtash", ""]]}, {"id": "2103.02860", "submitter": "Jiyuan Tu", "authors": "Jiyuan Tu, Weidong Liu, Xiaojun Mao, and Xi Chen", "title": "Variance Reduced Median-of-Means Estimator for Byzantine-Robust\n  Distributed Inference", "comments": "64 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops an efficient distributed inference algorithm, which is\nrobust against a moderate fraction of Byzantine nodes, namely arbitrary and\npossibly adversarial machines in a distributed learning system. In robust\nstatistics, the median-of-means (MOM) has been a popular approach to hedge\nagainst Byzantine failures due to its ease of implementation and computational\nefficiency. However, the MOM estimator has the shortcoming in terms of\nstatistical efficiency. The first main contribution of the paper is to propose\na variance reduced median-of-means (VRMOM) estimator, which improves the\nstatistical efficiency over the vanilla MOM estimator and is computationally as\nefficient as the MOM. Based on the proposed VRMOM estimator, we develop a\ngeneral distributed inference algorithm that is robust against Byzantine\nfailures. Theoretically, our distributed algorithm achieves a fast convergence\nrate with only a constant number of rounds of communications. We also provide\nthe asymptotic normality result for the purpose of statistical inference. To\nthe best of our knowledge, this is the first normality result in the setting of\nByzantine-robust distributed learning. The simulation results are also\npresented to illustrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 06:50:52 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Tu", "Jiyuan", ""], ["Liu", "Weidong", ""], ["Mao", "Xiaojun", ""], ["Chen", "Xi", ""]]}, {"id": "2103.02877", "submitter": "Anqi Wang", "authors": "Anqi Wang, Zhonghua Liu", "title": "A Two-Sample Robust Bayesian Mendelian Randomization Method Accounting\n  for Linkage Disequilibrium and Idiosyncratic Pleiotropy with Applications to\n  the COVID-19 Outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian randomization (MR) is a statistical method exploiting genetic\nvariants as instrumental variables to estimate the causal effect of modifiable\nrisk factors on an outcome of interest. Despite wide uses of various popular\ntwo-sample MR methods based on genome-wide association study summary level\ndata, however, those methods could suffer from potential power loss or/and\nbiased inference when the chosen genetic variants are in linkage disequilibrium\n(LD), and also have relatively large direct effects on the outcome whose\ndistribution might be heavy-tailed which is commonly referred to as the\nidiosyncratic pleiotropy phenomenon. To resolve those two issues, we propose a\nnovel Robust Bayesian Mendelian Randomization (RBMR) model that uses the more\nrobust multivariate generalized t-distribution to model such direct effects in\na probabilistic model framework which can also incorporate the LD structure\nexplicitly. The generalized t-distribution can be represented as a Gaussian\nscaled mixture so that our model parameters can be estimated by the EM-type\nalgorithms. We compute the standard errors by calibrating the evidence lower\nbound using the likelihood ratio test. Through extensive simulation studies, we\nshow that our RBMR has robust performance compared to other competing methods.\nWe also apply our RBMR method to two benchmark data sets and find that RBMR has\nsmaller bias and standard errors. Using our proposed RBMR method, we find that\ncoronary artery disease is associated with increased risk of critically ill\ncoronavirus disease 2019 (COVID-19). We also develop a user-friendly R package\nRBMR for public use.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 07:57:29 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 18:08:55 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Wang", "Anqi", ""], ["Liu", "Zhonghua", ""]]}, {"id": "2103.02974", "submitter": "Luciana Dalla Valle PhD", "authors": "Clara Grazian, Luciana Dalla Valle, Brunero Liseo", "title": "Approximate Bayesian Conditional Copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copula models are flexible tools to represent complex structures of\ndependence for multivariate random variables. According to Sklar's theorem\n(Sklar, 1959), any d-dimensional absolutely continuous density can be uniquely\nrepresented as the product of the marginal distributions and a copula function\nwhich captures the dependence structure among the vector components. In real\ndata applications, the interest of the analyses often lies on specific\nfunctionals of the dependence, which quantify aspects of it in a few numerical\nvalues. A broad literature exists on such functionals, however extensions to\ninclude covariates are still limited. This is mainly due to the lack of\nunbiased estimators of the copula function, especially when one does not have\nenough information to select the copula model. Recent advances in computational\nmethodologies and algorithms have allowed inference in the presence of\ncomplicated likelihood functions, especially in the Bayesian approach, whose\nmethods, despite being computationally intensive, allow us to better evaluate\nthe uncertainty of the estimates. In this work, we present several Bayesian\nmethods to approximate the posterior distribution of functionals of the\ndependence, using nonparametric models which avoid the selection of the copula\nfunction. These methods are compared in simulation studies and in two realistic\napplications, from civil engineering and astrophysics.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:49:12 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Grazian", "Clara", ""], ["Valle", "Luciana Dalla", ""], ["Liseo", "Brunero", ""]]}, {"id": "2103.02989", "submitter": "Werner M\\\"uller", "authors": "Andrej P\\'azman, Markus Hainy, Werner G. M\\\"uller", "title": "A convex approach to optimum design of experiments with correlated\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal design of experiments for correlated processes is an increasingly\nrelevant and active research topic. Until now only heuristic methods were\navailable without a possibility to judge their quality. In this work we\ncomplement the virtual noise approach by a convex formulation and an\nequivalence theorem comparable to the uncorrelated case. Hence, it is now\npossible to provide an upper performance bound against which alternative design\nmethods can be judged. We provide a comparison on some classical examples from\nthe literature.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 12:29:46 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["P\u00e1zman", "Andrej", ""], ["Hainy", "Markus", ""], ["M\u00fcller", "Werner G.", ""]]}, {"id": "2103.03184", "submitter": "Morgane Pierre-Jean", "authors": "Morgane Pierre-Jean (CNRGH), Florence Mauger (CNRGH),\n  Jean-Fran\\c{c}ois Deleuze (CNRGH), Edith Le Floch", "title": "PIntMF: Penalized Integrative Matrix Factorization Method for\n  Multi-Omics Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is more and more common to explore the genome at diverse levels and not\nonly at a single omic level. Through integrative statistical methods, omics\ndata have the power to reveal new biological processes, potential biomarkers,\nand subgroups of a cohort. The matrix factorization (MF) is a unsupervised\nstatistical method that allows giving a clustering of individuals, but also\nrevealing relevant omic variables from the various blocks. Here, we present\nPIntMF (Penalized Integrative Matrix Factorization), a model of MF with\nsparsity, positivity and equality constraints.To induce sparsity in the model,\nwe use a classical Lasso penalization on variable and individual matrices. For\nthe matrix of samples, sparsity helps for the clustering, and normalization\n(matching an equality constraint) of inferred coefficients is added for a\nbetter interpretation. Besides, we add an automatic tuning of the sparsity\nparameters using the famous glmnet package. We also proposed three criteria to\nhelp the user to choose the number of latent variables. PIntMF was compared to\nother state-of-the-art integrative methods including feature selection\ntechniques in both synthetic and real data. PIntMF succeeds in finding relevant\nclusters as well as variables in two types of simulated data (correlated and\nuncorrelated). Then, PIntMF was applied to two real datasets (Diet and cancer),\nand it reveals interpretable clusters linked to available clinical data. Our\nmethod outperforms the existing ones on two criteria (clustering and variable\nselection). We show that PIntMF is an easy, fast, and powerful tool to extract\npatterns and cluster samples from multi-omics data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 09:35:35 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Pierre-Jean", "Morgane", "", "CNRGH"], ["Mauger", "Florence", "", "CNRGH"], ["Deleuze", "Jean-Fran\u00e7ois", "", "CNRGH"], ["Floch", "Edith Le", ""]]}, {"id": "2103.03237", "submitter": "Mikkel Slot Nielsen", "authors": "Kim Christensen, Mikkel Slot Nielsen, Mark Podolskij", "title": "High-dimensional estimation of quadratic variation based on penalized\n  realized variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a penalized realized variance (PRV) estimator of\nthe quadratic variation (QV) of a high-dimensional continuous It\\^{o}\nsemimartingale. We adapt the principle idea of regularization from linear\nregression to covariance estimation in a continuous-time high-frequency\nsetting. We show that under a nuclear norm penalization, the PRV is computed by\nsoft-thresholding the eigenvalues of realized variance (RV). It therefore\nencourages sparsity of singular values or, equivalently, low rank of the\nsolution. We prove our estimator is minimax optimal up to a logarithmic factor.\nWe derive a concentration inequality, which reveals that the rank of PRV is --\nwith a high probability -- the number of non-negligible eigenvalues of the QV.\nMoreover, we also provide the associated non-asymptotic analysis for the spot\nvariance. We suggest an intuitive data-driven bootstrap procedure to select the\nshrinkage parameter. Our theory is supplemented by a simulation study and an\nempirical application. The PRV detects about three-five factors in the equity\nmarket, with a notable rank decrease during times of distress in financial\nmarkets. This is consistent with most standard asset pricing models, where a\nlimited amount of systematic factors driving the cross-section of stock returns\nare perturbed by idiosyncratic errors, rendering the QV -- and also RV -- of\nfull rank.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:57:13 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Christensen", "Kim", ""], ["Nielsen", "Mikkel Slot", ""], ["Podolskij", "Mark", ""]]}, {"id": "2103.03272", "submitter": "Ilyas Bakbergenuly", "authors": "Ilyas Bakbergenuly, David C. Hoaglin, and Elena Kulinskaya", "title": "Simulation study of Q statistic with constant weights for testing and\n  estimation of heterogeneity of standardized mean differences in meta-analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cochran's $Q$ statistic is routinely used for testing heterogeneity in\nmeta-analysis. Its expected value is also used for estimation of between-study\nvariance $\\tau^2$. Cochran's $Q$, or $Q_{IV}$, uses estimated inverse-variance\nweights which makes approximating its distribution rather complicated. As an\nalternative, we are investigating a new $Q$ statistic, $Q_F$, whose constant\nweights use only the studies' effective sample sizes. For standardized mean\ndifference as the measure of effect, we study, by simulation, approximations to\ndistributions of $Q_{IV}$ and $Q_F$, as the basis for tests of heterogeneity\nand for new point and interval estimators of the between-study variance\n$\\tau^2$. These include new DerSimonian-Kacker (2007)-type moment estimators\nbased on the first moment of $Q_F$, and novel median-unbiased estimators of\n$\\tau^2$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 19:11:16 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Bakbergenuly", "Ilyas", ""], ["Hoaglin", "David C.", ""], ["Kulinskaya", "Elena", ""]]}, {"id": "2103.03370", "submitter": "Xin Ma", "authors": "Xin Ma and Suprateek Kundu", "title": "Multi-task Learning with High-Dimensional Noisy Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent medical imaging studies have given rise to distinct but inter-related\ndatasets corresponding to multiple experimental tasks or longitudinal visits.\nStandard scalar-on-image regression models that fit each dataset separately are\nnot equipped to leverage information across inter-related images, and existing\nmulti-task learning approaches are compromised by the inability to account for\nthe noise that is often observed in images. We propose a novel joint\nscalar-on-image regression framework involving wavelet-based image\nrepresentations with grouped penalties that are designed to pool information\nacross inter-related images for joint learning, and which explicitly accounts\nfor noise in high-dimensional images via a projection-based approach. In the\npresence of non-convexity arising due to noisy images, we derive non-asymptotic\nerror bounds under non-convex as well as convex grouped penalties, even when\nthe number of voxels increases exponentially with sample size. A projected\ngradient descent algorithm is used for computation, which is shown to\napproximate the optimal solution via well-defined non-asymptotic optimization\nerror bounds under noisy images. Extensive simulations and application to a\nmotivating longitudinal Alzheimer's disease study illustrate significantly\nimproved predictive ability and greater power to detect true signals, that are\nsimply missed by existing methods without noise correction due to the\nattenuation to null phenomenon.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 22:29:04 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 19:50:47 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Ma", "Xin", ""], ["Kundu", "Suprateek", ""]]}, {"id": "2103.03437", "submitter": "Jiayi Wang", "authors": "Jiayi Wang, Raymond K. W. Wong, Shu Yang, Kwun Chuen Gary Chan", "title": "Estimation of Partially Conditional Average Treatment Effect by Hybrid\n  Kernel-covariate Balancing", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonparametric estimation for the partially conditional average\ntreatment effect, defined as the treatment effect function over an interested\nsubset of confounders. We propose a hybrid kernel weighting estimator where the\nweights aim to control the balancing error of any function of the confounders\nfrom a reproducing kernel Hilbert space after kernel smoothing over the subset\nof interested variables. In addition, we present an augmented version of our\nestimator which can incorporate estimations of outcome mean functions. Based on\nthe representer theorem, gradient-based algorithms can be applied for solving\nthe corresponding infinite-dimensional optimization problem. Asymptotic\nproperties are studied without any smoothness assumptions for propensity score\nfunction or the need of data splitting, relaxing certain existing stringent\nassumptions. The numerical performance of the proposed estimator is\ndemonstrated by a simulation study and an application to the effect of a\nmother's smoking on a baby's birth weight conditioned on the mother's age.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 02:24:00 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wang", "Jiayi", ""], ["Wong", "Raymond K. W.", ""], ["Yang", "Shu", ""], ["Chan", "Kwun Chuen Gary", ""]]}, {"id": "2103.03445", "submitter": "Archer Zhang", "authors": "Archer Gong Zhang and Jiahua Chen", "title": "Density ratio model with data-adaptive basis function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, we collect independent samples from interconnected\npopulations. These population distributions share some latent structure, so it\nis advantageous to jointly analyze the samples. One effective way to connect\nthe distributions is the semiparametric density ratio model (DRM). A key\ningredient in the DRM is that the log density ratios are linear combinations of\nprespecified functions; the vector formed by these functions is called the\nbasis function. A sensible basis function can often be chosen based on\nknowledge of the context, and DRM-based inference is effective even if the\nbasis function is imperfect. However, a data-adaptive approach to the choice of\nbasis function remains an interesting and important research problem. We\npropose an approach based on the classical functional principal component\nanalysis (FPCA). Under some conditions, we show that this approach leads to\nconsistent basis function estimation. Our simulation results show that the\nproposed adaptive choice leads to an efficiency gain. We use a real-data\nexample to demonstrate the efficiency gain and the ease of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 02:48:48 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhang", "Archer Gong", ""], ["Chen", "Jiahua", ""]]}, {"id": "2103.03462", "submitter": "Nicholas Kissel", "authors": "Nicholas Kissel, Lucas Mentch", "title": "Forward Stability and Model Path Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most scientific publications follow the familiar recipe of (i) obtain data,\n(ii) fit a model, and (iii) comment on the scientific relevance of the effects\nof particular covariates in that model. This approach, however, ignores the\nfact that there may exist a multitude of similarly-accurate models in which the\nimplied effects of individual covariates may be vastly different. This problem\nof finding an entire collection of plausible models has also received\nrelatively little attention in the statistics community, with nearly all of the\nproposed methodologies being narrowly tailored to a particular model class\nand/or requiring an exhaustive search over all possible models, making them\nlargely infeasible in the current big data era. This work develops the idea of\nforward stability and proposes a novel, computationally-efficient approach to\nfinding collections of accurate models we refer to as model path selection\n(MPS). MPS builds up a plausible model collection via a forward selection\napproach and is entirely agnostic to the model class and loss function\nemployed. The resulting model collection can be displayed in a simple and\nintuitive graphical fashion, easily allowing practitioners to visualize whether\nsome covariates can be swapped for others with minimal loss.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 04:01:45 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Kissel", "Nicholas", ""], ["Mentch", "Lucas", ""]]}, {"id": "2103.03475", "submitter": "Jingyi Kenneth Tay", "authors": "J. Kenneth Tay, Balasubramanian Narasimhan, Trevor Hastie", "title": "Elastic Net Regularization Paths for All Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lasso and elastic net are popular regularized regression models for\nsupervised learning. Friedman, Hastie, and Tibshirani (2010) introduced a\ncomputationally efficient algorithm for computing the elastic net\nregularization path for ordinary least squares regression, logistic regression\nand multinomial logistic regression, while Simon, Friedman, Hastie, and\nTibshirani (2011) extended this work to Cox models for right-censored data. We\nfurther extend the reach of the elastic net-regularized regression to all\ngeneralized linear model families, Cox models with (start, stop] data and\nstrata, and a simplified version of the relaxed lasso. We also discuss\nconvenient utility functions for measuring the performance of these fitted\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 05:18:01 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Tay", "J. Kenneth", ""], ["Narasimhan", "Balasubramanian", ""], ["Hastie", "Trevor", ""]]}, {"id": "2103.03478", "submitter": "Meng Li", "authors": "Rongjie Liu, Meng Li, David B. Dunson", "title": "PPA: Principal Parcellation Analysis for Brain Connectomes and Multiple\n  Traits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our understanding of the structure of the brain and its relationships with\nhuman traits is largely determined by how we represent the structural\nconnectome. Standard practice divides the brain into regions of interest (ROIs)\nand represents the connectome as an adjacency matrix having cells measuring\nconnectivity between pairs of ROIs. Statistical analyses are then heavily\ndriven by the (largely arbitrary) choice of ROIs. In this article, we propose a\nnovel tractography-based representation of brain connectomes, which clusters\nfiber endpoints to define a data adaptive parcellation targeted to explain\nvariation among individuals and predict human traits. This representation leads\nto Principal Parcellation Analysis (PPA), representing individual brain\nconnectomes by compositional vectors building on a basis system of fiber\nbundles that captures the connectivity at the population level. PPA reduces\nsubjectivity and facilitates statistical analyses. We illustrate the proposed\napproach through applications to data from the Human Connectome Project (HCP)\nand show that PPA connectomes improve power in predicting human traits over\nstate-of-the-art methods based on classical connectomes, while dramatically\nimproving parsimony and maintaining interpretability. Our PPA package is\npublicly available on GitHub, and can be implemented routinely for diffusion\ntensor image data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 05:26:25 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Liu", "Rongjie", ""], ["Li", "Meng", ""], ["Dunson", "David B.", ""]]}, {"id": "2103.03538", "submitter": "Edgar Santos-Fernandez", "authors": "Edgar Santos-Fernandez, Jay M. Ver Hoef, Erin E. Peterson, James\n  McGree, Daniel Isaak, Kerrie Mengersen", "title": "Bayesian spatio-temporal models for stream networks", "comments": "26 pages, 10 figs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal models are widely used in many research areas including\necology. The recent proliferation of the use of in-situ sensors in streams and\nrivers supports space-time water quality modelling and monitoring in near\nreal-time. In this paper, we introduce a new family of dynamic spatio-temporal\nmodels, in which spatial dependence is established based on stream distance and\ntemporal autocorrelation is incorporated using vector autoregression\napproaches. We propose several variations of these novel models using a\nBayesian framework. Our results show that our proposed models perform well\nusing spatio-temporal data collected from real stream networks, particularly in\nterms of out-of-sample RMSPE. This is illustrated considering a case study of\nwater temperature data in the northwestern United States.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 08:33:58 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Santos-Fernandez", "Edgar", ""], ["Hoef", "Jay M. Ver", ""], ["Peterson", "Erin E.", ""], ["McGree", "James", ""], ["Isaak", "Daniel", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2103.03688", "submitter": "John Hughes", "authors": "John Hughes", "title": "On the Occasional Exactness of the Distributional Transform\n  Approximation for Direct Gaussian Copula Models with Discrete Margins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The direct Gaussian copula model with discrete marginal distributions is an\nappealing data-analytic tool but poses difficult computational challenges due\nto its intractable likelihood. A number of approximations/surrogates for the\nlikelihood have been proposed, including the continuous extension-based\napproximation (CE) and the distributional transform-based approximation (DT).\nThe continuous extension approach is exact up to Monte Carlo error but does not\nscale well computationally. The distributional transform approach permits\nefficient computation but offers no theoretical guarantee that it is exact. In\npractice, though, the distributional transform-based approximate likelihood is\nso very nearly exact for some variants of the model as to permit genuine\nmaximum likelihood or Bayesian inference. We demonstrate the exactness of the\ndistributional transform-based objective function for two interesting variants\nof the model, and propose a quantity that can be used to assess exactness for\nexperimentally observed datasets. Said diagnostic will permit practitioners to\ndetermine whether genuine Bayesian inference or ordinary maximum likelihood\ninference using the DT-based likelihood is possible for a given dataset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:00:04 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Hughes", "John", ""]]}, {"id": "2103.03818", "submitter": "Zhaoxia Yu", "authors": "Tong Shen, Kevin Johnston, Gyorgy Lur, Michele Guindani, Hernando\n  Ombao, Zhaoxia Yu", "title": "Time-varying $\\ell_0$ optimization for Spike Inference from Multi-Trial\n  Calcium Recordings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Optical imaging of genetically encoded calcium indicators is a powerful tool\nto record the activity of a large number of neurons simultaneously over a long\nperiod of time from freely behaving animals. However, determining the exact\ntime at which a neuron spikes and estimating the underlying firing rate from\ncalcium fluorescence data remains challenging, especially for calcium imaging\ndata obtained from a longitudinal study. We propose a multi-trial time-varying\n$\\ell_0$ penalized method to jointly detect spikes and estimate firing rates by\nrobustly integrating evolving neural dynamics across trials. Our simulation\nstudy shows that the proposed method performs well in both spike detection and\nfiring rate estimation. We demonstrate the usefulness of our method on calcium\nfluorescence trace data from two studies, with the first study showing\ndifferential firing rate functions between two behaviors and the second study\nshowing evolving firing rate function across trials due to learning.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 02:05:59 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Shen", "Tong", ""], ["Johnston", "Kevin", ""], ["Lur", "Gyorgy", ""], ["Guindani", "Michele", ""], ["Ombao", "Hernando", ""], ["Yu", "Zhaoxia", ""]]}, {"id": "2103.03833", "submitter": "Harrison Quick", "authors": "Harrison Quick", "title": "Improving the Utility of Poisson-Distributed, Differentially Private\n  Synthetic Data via Prior Predictive Truncation with an Application to CDC\n  WONDER", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CDC WONDER is a web-based tool for the dissemination of epidemiologic data\ncollected by the National Vital Statistics System. While CDC WONDER has\nbuilt-in privacy protections, they do not satisfy formal privacy protections\nsuch as differential privacy and thus are susceptible to targeted attacks.\nGiven the importance of making high-quality public health data publicly\navailable while preserving the privacy of the underlying data subjects, we aim\nto improve the utility of a recently developed approach for generating\nPoisson-distributed, differentially private synthetic data by using publicly\navailable information to truncate the range of the synthetic data.\nSpecifically, we utilize county-level population information from the U.S.\nCensus Bureau and national death reports produced by the CDC to inform prior\ndistributions on county-level death rates and infer reasonable ranges for\nPoisson-distributed, county-level death counts. In doing so, the requirements\nfor satisfying differential privacy for a given privacy budget can be reduced\nby several orders of magnitude, thereby leading to substantial improvements in\nutility. To illustrate our proposed approach, we consider a dataset comprised\nof over 26,000 cancer-related deaths from the Commonwealth of Pennsylvania\nbelonging to over 47,000 combinations of cause-of-death and demographic\nvariables such as age, race, sex, and county-of-residence and demonstrate the\nproposed framework's ability to preserve features such as geographic,\nurban/rural, and racial disparities present in the true data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 06:09:42 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Quick", "Harrison", ""]]}, {"id": "2103.03834", "submitter": "Till Koebe", "authors": "Till Koebe, Alejandra Arias-Salazar, Natalia Rojas-Perilla, Timo\n  Schmid", "title": "Intercensal updating using structure-preserving methods and satellite\n  imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Censuses are fundamental building blocks of most modern-day societies, yet\ncollected every ten years at best. We propose an extension of the widely\npopular census updating technique Structure Preserving Estimation by\nincorporating auxiliary information in order to take ongoing subnational\npopulation shifts into account. We apply our method by incorporating satellite\nimagery as additional source to derive annual small-area updates of\nmultidimensional poverty indicators from 2013 to 2020 for a population at risk:\nfemale-headed households in Senegal. We evaluate the performance of our\nproposal using data from two different census periods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 09:45:31 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Koebe", "Till", ""], ["Arias-Salazar", "Alejandra", ""], ["Rojas-Perilla", "Natalia", ""], ["Schmid", "Timo", ""]]}, {"id": "2103.03857", "submitter": "Sean McGrath", "authors": "Sean McGrath, Jessica G. Young, Miguel A. Hern\\'an", "title": "Revisiting the g-null paradox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parametric g-formula is an approach to estimating causal effects of\nsustained treatment strategies from observational data. An often cited\nlimitation of the parametric g-formula is the g-null paradox: a phenomenon in\nwhich model misspecification in the parametric g-formula is guaranteed under\nthe conditions that motivate its use (i.e., when identifiability conditions\nhold and measured time-varying confounders are affected by past treatment).\nMany users of the parametric g-formula know they must acknowledge the g-null\nparadox as a limitation when reporting results but still require clarity on its\nmeaning and implications. Here we revisit the g-null paradox to clarify its\nrole in causal inference studies. In doing so, we present analytic examples and\na simulation-based illustration of the bias of parametric g-formula estimates\nunder the conditions associated with this paradox. Our results highlight the\nimportance of avoiding overly parsimonious models for the components of the\ng-formula when using this method.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 18:15:51 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["McGrath", "Sean", ""], ["Young", "Jessica G.", ""], ["Hern\u00e1n", "Miguel A.", ""]]}, {"id": "2103.04025", "submitter": "Sepideh Mosaferi", "authors": "Sepideh Mosaferi, Malay Ghosh, Rebecca C. Steorts", "title": "Transformed Fay-Herriot Model with Measurement Error in Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Statistical agencies are often asked to produce small area estimates (SAEs)\nfor positively skewed variables. When domain sample sizes are too small to\nsupport direct estimators, effects of skewness of the response variable can be\nlarge. As such, it is important to appropriately account for the distribution\nof the response variable given available auxiliary information. Motivated by\nthis issue and in order to stabilize the skewness and achieve normality in the\nresponse variable, we propose an area-level log-measurement error model on the\nresponse variable. Then, under our proposed modeling framework, we derive an\nempirical Bayes (EB) predictor of positive small area quantities subject to the\ncovariates containing measurement error. We propose a corresponding mean\nsquared prediction error (MSPE) of EB predictor using both a jackknife and a\nbootstrap method. We show that the order of the bias is $O(m^{-1})$, where $m$\nis the number of small areas. Finally, we investigate the performance of our\nmethodology using both design-based and model-based simulation studies.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 04:35:59 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Mosaferi", "Sepideh", ""], ["Ghosh", "Malay", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "2103.04050", "submitter": "Yuehan Yang", "authors": "Hanzhong Liu, Jiyang Ren and Yuehan Yang", "title": "Randomization-based joint central limit theorem and efficient covariate\n  adjustment in stratified $2^K$ factorial experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stratified factorial experiments are widely used in industrial engineering,\nclinical trials, and social science to measure the joint effects of several\nfactors on an outcome. Researchers often use a linear model and analysis of\ncovariance to analyze experimental results; however, few studies have addressed\nthe validity and robustness of the resulting inferences because assumptions for\na linear model might not be justified by randomization. In this paper, we\nestablish the finite-population joint central limit theorem for usual\n(unadjusted) factorial effect estimators in stratified $2^K$ factorial\nexperiments. Our theorem is obtained under randomization-based inference\nframework, making use of an extension of the vector form of the\nWald--Wolfowitz--Hoeffding theorem for a linear rank statistic. It is robust to\nmodel misspecification, arbitrary numbers of strata, stratum sizes, and\npropensity scores across strata. To improve the estimation and inference\nefficiency, we propose three covariate adjustment methods and show that under\nmild conditions, the resulting covariate-adjusted factorial effect estimators\nare consistent, jointly asymptotically normal, and generally more efficient\nthan the unadjusted estimator. In addition, we propose Neyman-type conservative\nestimators for the asymptotic variances to facilitate valid inferences.\nSimulation studies demonstrate the benefits of covariate adjustment methods.\nFinally, we apply the proposed methods to analyze a real dataset from a\nclinical trial to evaluate the effects of the addition of bevacizumab and/or\ncarboplatin on pathologic complete response rates in patients with stage II to\nIII triple-negative breast cancer.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 07:12:48 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Liu", "Hanzhong", ""], ["Ren", "Jiyang", ""], ["Yang", "Yuehan", ""]]}, {"id": "2103.04086", "submitter": "Yu Luo", "authors": "Yu Luo, David A. Stephens, Daniel J. Graham, Emma J. McCoy", "title": "Bayesian Doubly Robust Causal Inference via Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequentist inference has a well-established supporting theory for doubly\nrobust causal inference based on the potential outcomes framework, which is\nrealized via outcome regression (OR) and propensity score (PS) models. The\nBayesian counterpart, however, is not obvious as the PS model loses its\nbalancing property in joint modeling. In this paper, we propose a natural and\nformal Bayesian solution by bridging loss-type Bayesian inference with a\nutility function derived from the notion of a pseudo-population via the change\nof measure. Consistency of the posterior distribution is shown with correctly\nspecified and misspecified OR models. Simulation studies suggest that our\nproposed method can estimate the true causal effect more efficiently and\nachieve the frequentist coverage if either the OR model is correctly specified\nor fit with a flexible function of the confounders, compared to the previous\nBayesian approach via the Bayesian bootstrap. Finally, we apply this novel\nBayesian method to assess the impact of speed cameras on the reduction of car\ncollisions in England.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 10:03:00 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Luo", "Yu", ""], ["Stephens", "David A.", ""], ["Graham", "Daniel J.", ""], ["McCoy", "Emma J.", ""]]}, {"id": "2103.04175", "submitter": "Xiaoqing Tan", "authors": "Xiaoqing Tan, Judah Abberbock, Priya Rastogi, Gong Tang", "title": "Identifying Principal Stratum Causal Effects Conditional on a\n  Post-treatment Intermediate Response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neoadjuvant trials on early-stage breast cancer, patients are usually\nrandomized into a control group and a treatment group with an additional target\ntherapy. Early efficacy of the new regimen is assessed via the binary\npathological complete response (pCR) and the eventual efficacy is assessed via\na long-term clinical outcome such as survival. Although pCR is strongly\nassociated with survival, it has not been confirmed as a surrogate endpoint. To\nfully understand its clinical implication, it is important to establish causal\nestimands such as the causal effect in survival for patients who would achieve\npCR under the new regimen. Under the principal stratification framework,\nprevious works focused on sensitivity analyses by varying model parameters in\nan imposed model on counterfactual outcomes. Under the same assumptions, we\npropose an approach to estimate those model parameters using empirical data and\nsubsequently the causal estimand of interest. We also extend our approach to\naddress censored outcome data. The proposed method is applied to a recent\nclinical trial and its performance is evaluated via simulation studies.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 19:03:11 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tan", "Xiaoqing", ""], ["Abberbock", "Judah", ""], ["Rastogi", "Priya", ""], ["Tang", "Gong", ""]]}, {"id": "2103.04277", "submitter": "Zijun Gao", "authors": "Zijun Gao, Trevor Hastie", "title": "DINA: Estimating Heterogenous Treatment Effects in Exponential Family\n  and Cox Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use the difference in natural parameters (DINA) to quantify the\nheterogeneous treatment effect for exponential family models, in contrast to\nthe difference in means. Similarly we model the hazard ratios for the Cox\nmodel. For binary outcomes and survival times, DINA is both convenient and\nperhaps more practical for modeling the covariates' influences on the treatment\neffect. We introduce a DINA estimator that is insensitive to confounding and\nnon-collapsibility issues, and allows practitioners to use powerful\noff-the-shelf machine learning tools for nuisance estimation. We use extensive\nsimulations to demonstrate the efficacy of the proposed method with various\nresponse distributions and censoring mechanisms. We also apply the proposed\nmethod to the SPRINT dataset to estimate the heterogeneous treatment effect,\ndemonstrate the method's robustness to nuisance estimation, and conduct a\nplacebo evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 05:40:53 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 19:55:06 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 22:32:12 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Gao", "Zijun", ""], ["Hastie", "Trevor", ""]]}, {"id": "2103.04409", "submitter": "Jue Hou", "authors": "Stephanie F. Chan, Jue Hou, Xuan Wang, and Tianxi Cai", "title": "Risk Prediction with Imperfect Survival Outcome Information from\n  Electronic Health Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Readily available proxies for time of disease onset such as time of the first\ndiagnostic code can lead to substantial risk prediction error if performing\nanalyses based on poor proxies. Due to the lack of detailed documentation and\nlabor intensiveness of manual annotation, it is often only feasible to\nascertain for a small subset the current status of the disease by a follow up\ntime rather than the exact time. In this paper, we aim to develop risk\nprediction models for the onset time efficiently leveraging both a small number\nof labels on current status and a large number of unlabeled observations on\nimperfect proxies. Under a semiparametric transformation model for onset and a\nhighly flexible measurement error models for proxy onset time, we propose the\nsemisupervised risk prediction method by combining information from proxies and\nlimited labels efficiently. From an initial estimator solely based on the\nlabelled subset, we perform a one-step correction with the full data augmenting\nagainst a mean zero rank correlation score derived from the proxies. We\nestablish the consistency and asymptotic normality of the proposed\nsemi-supervised estimator and provide a resampling procedure for interval\nestimation. Simulation studies demonstrate that the proposed estimator performs\nwell in finite sample. We illustrate the proposed estimator by developing a\ngenetic risk prediction model for obesity using data from Partners Biobank\nElectronic Health Records (EHR).\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 17:39:28 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chan", "Stephanie F.", ""], ["Hou", "Jue", ""], ["Wang", "Xuan", ""], ["Cai", "Tianxi", ""]]}, {"id": "2103.04417", "submitter": "Andrew Giffin", "authors": "Andrew Giffin, Wenlong Gong, Suman Majumder, Ana G. Rappold, Brian J.\n  Reich, Shu Yang", "title": "Estimating intervention effects on infectious disease control: the\n  effect of community mobility reduction on Coronavirus spread", "comments": "25 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the effects of interventions, such as restrictions on community\nand large group gatherings, is critical to controlling the spread of COVID-19.\nSusceptible-Infectious-Recovered (SIR) models are traditionally used to\nforecast the infection rates but do not provide insights into the causal\neffects of interventions. We propose a spatiotemporal model that estimates the\ncausal effect of changes in community mobility (intervention) on infection\nrates. Using an approximation to the SIR model and incorporating spatiotemporal\ndependence, the proposed model estimates a direct and indirect (spillover)\neffect of intervention. Under an interference and treatment ignorability\nassumption, this model is able to estimate causal intervention effects, and\nadditionally allows for spatial interference between locations. Reductions in\ncommunity mobility were measured by cell phone movement data. The results\nsuggest that the reductions in mobility decrease Coronavirus cases 4 to 7 weeks\nafter the intervention.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 18:21:29 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 03:06:01 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Giffin", "Andrew", ""], ["Gong", "Wenlong", ""], ["Majumder", "Suman", ""], ["Rappold", "Ana G.", ""], ["Reich", "Brian J.", ""], ["Yang", "Shu", ""]]}, {"id": "2103.04449", "submitter": "Helton Saulo", "authors": "Dan\\'ubia R. Cunha, Jose A. Divino and Helton Saulo", "title": "On a log-symmetric quantile tobit model applied to female labor supply\n  data", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic censored regression model (tobit model) has been widely used in\nthe economic literature. This model assumes normality for the error\ndistribution and is not recommended for cases where positive skewness is\npresent. Moreover, in regression analysis, it is well-known that a quantile\nregression approach allows us to study the influences of the explanatory\nvariables on the dependent variable considering different quantiles. Therefore,\nwe propose in this paper a quantile tobit regression model based on\nquantile-based log-symmetric distributions. The proposed methodology allows us\nto model data with positive skewness (which is not suitable for the classic\ntobit model), and to study the influence of the quantiles of interest, in\naddition to accommodating heteroscedasticity. The model parameters are\nestimated using the maximum likelihood method and an elaborate Monte Carlo\nstudy is performed to evaluate the performance of the estimates. Finally, the\nproposed methodology is illustrated using two female labor supply data sets.\nThe results show that the proposed log-symmetric quantile tobit model has a\nbetter fit than the classic tobit model.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 20:41:20 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Cunha", "Dan\u00fabia R.", ""], ["Divino", "Jose A.", ""], ["Saulo", "Helton", ""]]}, {"id": "2103.04462", "submitter": "Mauro Gasparini", "authors": "Mauro Gasparini", "title": "Improving Bayesian estimation of Vaccine Efficacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A full Bayesian approach to the estimation of Vaccine Efficacy is presented,\nwhich is an improvement over the currently used exact method conditional on the\ntotal number of cases. As an example, we reconsider the statistical sections of\nthe BioNTech/Pfizer protocol, which in 2020 has led to the first approved\nanti-Covid-19 vaccine.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 21:35:16 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gasparini", "Mauro", ""]]}, {"id": "2103.04472", "submitter": "Larry Wasserman", "authors": "Matteo Bonvini and Edward Kennedy and Valerie Ventura and Larry\n  Wasserman", "title": "Causal Inference in the Time of Covid-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we develop statistical methods for causal inference in\nepidemics. Our focus is in estimating the effect of social mobility on deaths\nin the Covid-19 pandemic. We propose a marginal structural model motivated by a\nmodified version of a basic epidemic model. We estimate the counterfactual time\nseries of deaths under interventions on mobility. We conduct several types of\nsensitivity analyses. We find that the data support the idea that reduced\nmobility causes reduced deaths, but the conclusion comes with caveats. There is\nevidence of sensitivity to model misspecification and unmeasured confounding\nwhich implies that the size of the causal effect needs to be interpreted with\ncaution. While there is little doubt the the effect is real, our work\nhighlights the challenges in drawing causal inferences from pandemic data.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 22:35:08 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Bonvini", "Matteo", ""], ["Kennedy", "Edward", ""], ["Ventura", "Valerie", ""], ["Wasserman", "Larry", ""]]}, {"id": "2103.04504", "submitter": "Peijun Sang", "authors": "Peijun Sang, Adam B Kashlak and Linglong Kong", "title": "A reproducing kernel Hilbert space framework for functional data\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We encounter a bottleneck when we try to borrow the strength of classical\nclassifiers to classify functional data. The major issue is that functional\ndata are intrinsically infinite dimensional, thus classical classifiers cannot\nbe applied directly or have poor performance due to the curse of\ndimensionality. To address this concern, we propose to project functional data\nonto one specific direction, and then a distance-weighted discrimination DWD\nclassifier is built upon the projection score. The projection direction is\nidentified through minimizing an empirical risk function that contains the\nparticular loss function in a DWD classifier, over a reproducing kernel Hilbert\nspace. Hence our proposed classifier can avoid overfitting and enjoy appealing\nproperties of DWD classifiers. This framework is further extended to\naccommodate functional data classification problems where scalar covariates are\ninvolved. In contrast to previous work, we establish a non-asymptotic\nestimation error bound on the relative misclassification rate. In finite sample\ncase, we demonstrate that the proposed classifiers compare favorably with some\ncommonly used functional classifiers in terms of prediction accuracy through\nsimulation studies and a real-world application.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 01:32:14 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Sang", "Peijun", ""], ["Kashlak", "Adam B", ""], ["Kong", "Linglong", ""]]}, {"id": "2103.04613", "submitter": "Clement Benesse", "authors": "Cl\\'ement B\\'enesse (IMT), Fabrice Gamboa (IMT), Jean-Michel Loubes\n  (IMT), Thibaut Boissin", "title": "Fairness seen as Global Sensitivity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring that a predictor is not biased against a sensible feature is the key\nof Fairness learning. Conversely, Global Sensitivity Analysis is used in\nnumerous contexts to monitor the influence of any feature on an output\nvariable. We reconcile these two domains by showing how Fairness can be seen as\na special framework of Global Sensitivity Analysis and how various usual\nindicators are common between these two fields. We also present new Global\nSensitivity Analysis indices, as well as rates of convergence, that are useful\nas fairness proxies.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 09:04:45 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["B\u00e9nesse", "Cl\u00e9ment", "", "IMT"], ["Gamboa", "Fabrice", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"], ["Boissin", "Thibaut", ""]]}, {"id": "2103.04615", "submitter": "Xiaodong Wang", "authors": "Xiaodong Wang and Fushing Hsieh", "title": "Discovering Multiple Phases of Dynamics by Dissecting Multivariate Time\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We proposed a data-driven approach to dissect multivariate time series in\norder to discover multiple phases underlying dynamics of complex systems. This\ncomputing approach is developed as a multiple-dimension version of Hierarchical\nFactor Segmentation(HFS) technique. This expanded approach proposes a\nsystematic protocol of choosing various extreme events in multi-dimensional\nspace. Upon each chosen event, an empirical distribution of event-recurrence,\nor waiting time between the excursions, is fitted by a geometric distribution\nwith time-varying parameters. Iterative fittings are performed across all\nchosen events. We then collect and summarize the local recurrent patterns into\na global dynamic mechanism. Clustering is applied for partitioning the whole\ntime period into alternating segments, in which variables are identically\ndistributed. Feature weighting techniques are also considered to compensate for\nsome drawbacks of clustering. Our simulation results show that this expanded\napproach can even detect systematic differences when the joint distribution\nvaries. In real data experiments, we analyze the relationship from returns,\ntrading volume, and transaction number of a single, as well as of multiple\nstocks in S&P500. We can successfully not only map out volatile periods but\nalso provide potential associative links between stocks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 09:08:08 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wang", "Xiaodong", ""], ["Hsieh", "Fushing", ""]]}, {"id": "2103.04647", "submitter": "Santhosh Narayanan Dr.", "authors": "Santhosh Narayanan, Ioannis Kosmidis, Petros Dellaportas", "title": "Flexible marked spatio-temporal point processes with applications to\n  event sequences from association football", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a new family of marked point processes by focusing the\ncharacteristic properties of marked Hawkes processes exclusively to the space\nof marks, providing the freedom to specify a different model for the occurrence\ntimes. This is possible through a decomposition of the joint distribution of\nmarks and times that allows to separately specify the conditional distribution\nof marks given the filtration of the process and the current time. We develop a\nBayesian framework for the inference and prediction from this family of marked\npoint processes that can naturally accommodate process and point-specific\ncovariate information to drive cross-excitations, offering wide flexibility and\napplicability in the modelling of real-world processes. The framework is used\nhere for the modelling of in-game event sequences from association football,\nresulting not only in inferences about previously unquantified characteristics\nof the game dynamics and extraction of event-specific team abilities, but also\nin predictions for the occurrence of events of interest, such as goals, corners\nor fouls, in a specified interval of time.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 10:15:29 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Narayanan", "Santhosh", ""], ["Kosmidis", "Ioannis", ""], ["Dellaportas", "Petros", ""]]}, {"id": "2103.04678", "submitter": "Una Radoji\\v{c}i\\'c", "authors": "Una Radojicic, Klaus Nordhausen and Joni Virta", "title": "Large-Sample Properties of Blind Estimation of the Linear Discriminant\n  Using Projection Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of the linear discriminant with projection pursuit, a\nmethod that is blind in the sense that it does not use the class labels in the\nestimation. Our viewpoint is asymptotic and, as our main contribution, we\nderive central limit theorems for estimators based on three different\nprojection indices, skewness, kurtosis and their convex combination. The\nresults show that in each case the limiting covariance matrix is proportional\nto that of linear discriminant analysis (LDA), an unblind estimator of the\ndiscriminant. An extensive comparative study between the asymptotic variances\nreveals that projection pursuit is able to achieve efficiency equal to LDA when\nthe groups are arbitrarily well-separated and their sizes are reasonably\nbalanced. We conclude with a real data example and a simulation study\ninvestigating the validity of the obtained asymptotic formulas for finite\nsamples.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 11:36:56 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Radojicic", "Una", ""], ["Nordhausen", "Klaus", ""], ["Virta", "Joni", ""]]}, {"id": "2103.04715", "submitter": "Remi Laumont", "authors": "R\\'emi Laumont, Valentin de Bortoli, Andr\\'es Almansa, Julie Delon,\n  Alain Durmus and Marcelo Pereyra", "title": "Bayesian imaging using Plug & Play priors: when Langevin meets Tweedie", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV eess.IV math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the seminal work of Venkatakrishnan et al. (2013), Plug & Play (PnP)\nmethods have become ubiquitous in Bayesian imaging. These methods derive\nMinimum Mean Square Error (MMSE) or Maximum A Posteriori (MAP) estimators for\ninverse problems in imaging by combining an explicit likelihood function with a\nprior that is implicitly defined by an image denoising algorithm. The PnP\nalgorithms proposed in the literature mainly differ in the iterative schemes\nthey use for optimisation or for sampling. In the case of optimisation schemes,\nsome recent works guarantee the convergence to a fixed point, albeit not\nnecessarily a MAP estimate. In the case of sampling schemes, to the best of our\nknowledge, there is no known proof of convergence. There also remain important\nopen questions regarding whether the underlying Bayesian models and estimators\nare well defined, well-posed, and have the basic regularity properties required\nto support these numerical schemes. To address these limitations, this paper\ndevelops theory, methods, and provably convergent algorithms for performing\nBayesian inference with PnP priors. We introduce two algorithms: 1) PnP-ULA\n(Unadjusted Langevin Algorithm) for Monte Carlo sampling and MMSE inference;\nand 2) PnP-SGD (Stochastic Gradient Descent) for MAP inference. Using recent\nresults on the quantitative convergence of Markov chains, we establish detailed\nconvergence guarantees for these two algorithms under realistic assumptions on\nthe denoising operators used, with special attention to denoisers based on deep\nneural networks. We also show that these algorithms approximately target a\ndecision-theoretically optimal Bayesian model that is well-posed. The proposed\nalgorithms are demonstrated on several canonical problems such as image\ndeblurring, inpainting, and denoising, where they are used for point estimation\nas well as for uncertainty visualisation and quantification.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:46:53 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 15:01:21 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 16:24:03 GMT"}, {"version": "v4", "created": "Fri, 19 Mar 2021 12:11:36 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Laumont", "R\u00e9mi", ""], ["de Bortoli", "Valentin", ""], ["Almansa", "Andr\u00e9s", ""], ["Delon", "Julie", ""], ["Durmus", "Alain", ""], ["Pereyra", "Marcelo", ""]]}, {"id": "2103.04721", "submitter": "Matthias Troffaes", "authors": "Ullrika Sahlin and Matthias C. M. Troffaes and Lennart Edsman", "title": "Robust decision analysis under severe uncertainty and ambiguous\n  tradeoffs: an invasive species case study", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian decision analysis is a useful method for risk management decisions,\nbut is limited in its ability to consider severe uncertainty in knowledge, and\nvalue ambiguity in management objectives. We study the use of robust Bayesian\ndecision analysis to handle problems where one or both of these issues arise.\nThe robust Bayesian approach models severe uncertainty through bounds on\nprobability distributions, and value ambiguity through bounds on utility\nfunctions. To incorporate data, standard Bayesian updating is applied on the\nentire set of distributions. To elicit our expert's utility representing the\nvalue of different management objectives, we use a modified version of the\nswing weighting procedure that can cope with severe value ambiguity. We\ndemonstrate these methods on an environmental management problem to eradicate\nan alien invasive marmorkrebs recently discovered in Sweden, which needed a\nrapid response despite substantial knowledge gaps if the species was still\npresent (i.e. severe uncertainty) and the need for difficult tradeoffs and\ncompeting interests (i.e. value ambiguity). We identify that the decision\nalternatives to drain the system and remove individuals in combination with\ndredging and sieving with or without a degradable biocide, or increasing pH,\nare consistently bad under the entire range of probability and utility bounds.\nThis case study shows how robust Bayesian decision analysis provides a\ntransparent methodology for integrating information in risk management problems\nwhere little data are available and/or where the tradeoffs ambiguous.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:55:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Sahlin", "Ullrika", ""], ["Troffaes", "Matthias C. M.", ""], ["Edsman", "Lennart", ""]]}, {"id": "2103.04786", "submitter": "Maximilian Ilse", "authors": "Maximilian Ilse, Patrick Forr\\'e, Max Welling, Joris M. Mooij", "title": "Efficient Causal Inference from Combined Observational and\n  Interventional Data through Causal Reductions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unobserved confounding is one of the main challenges when estimating causal\neffects. We propose a novel causal reduction method that replaces an arbitrary\nnumber of possibly high-dimensional latent confounders with a single latent\nconfounder that lives in the same space as the treatment variable without\nchanging the observational and interventional distributions entailed by the\ncausal model. After the reduction, we parameterize the reduced causal model\nusing a flexible class of transformations, so-called normalizing flows. We\npropose a learning algorithm to estimate the parameterized reduced model\njointly from observational and interventional data. This allows us to estimate\nthe causal effect in a principled way from combined data. We perform a series\nof experiments on data simulated using nonlinear causal mechanisms and find\nthat we can often substantially reduce the number of interventional samples\nwhen adding observational training samples without sacrificing accuracy. Thus,\nadding observational data may help to more accurately estimate causal effects\neven in the presence of unobserved confounders.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 14:29:07 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ilse", "Maximilian", ""], ["Forr\u00e9", "Patrick", ""], ["Welling", "Max", ""], ["Mooij", "Joris M.", ""]]}, {"id": "2103.04907", "submitter": "Fan Li", "authors": "Fan Li, Ashley L. Buchanan, Stephen R. Cole", "title": "Generalizing trial evidence to target populations in non-nested designs:\n  Applications to AIDS clinical trials", "comments": "43 pages, 3 tables and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Comparative effectiveness evidence from randomized trials may not be directly\ngeneralizable to a target population of substantive interest when, as in most\ncases, trial participants are not randomly sampled from the target population.\nMotivated by the need to generalize evidence from two trials conducted in the\nAIDS Clinical Trials Group (ACTG), we consider weighting, regression and doubly\nrobust estimators to estimate the causal effects of HIV interventions in a\nspecified population of people living with HIV in the USA. We focus on a\nnon-nested trial design and discuss strategies for both point and variance\nestimation of the target population average treatment effect. Specifically in\nthe generalizability context, we demonstrate both analytically and empirically\nthat estimating the known propensity score in trials does not increase the\nvariance for each of the weighting, regression and doubly robust estimators. We\napply these methods to generalize the average treatment effects from two ACTG\ntrials to specified target populations and operationalize key practical\nconsiderations. Finally, we report on a simulation study that investigates the\nfinite-sample operating characteristics of the generalizability estimators and\ntheir sandwich variance estimators.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:10:22 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Li", "Fan", ""], ["Buchanan", "Ashley L.", ""], ["Cole", "Stephen R.", ""]]}, {"id": "2103.04985", "submitter": "Ben Dai", "authors": "Ben Dai, Xiaotong Shen, Wei Pan", "title": "Significance tests of feature relevance for a blackbox learner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An exciting recent development is the uptake of deep learning in many\nscientific fields, where the objective is seeking novel scientific insights and\ndiscoveries. To interpret a learning outcome, researchers perform hypothesis\ntesting for explainable features to advance scientific domain knowledge. In\nsuch a situation, testing for a blackbox learner poses a severe challenge\nbecause of intractable models, unknown limiting distributions of parameter\nestimates, and high computational constraints. In this article, we derive two\nconsistent tests for the feature relevance of a blackbox learner. The first one\nevaluates a loss difference with perturbation on an inference sample, which is\nindependent of an estimation sample used for parameter estimation in model\nfitting. The second further splits the inference sample into two but does not\nrequire data perturbation. Also, we develop their combined versions by\naggregating the order statistics of the $p$-values based on repeated sample\nsplitting. To estimate the splitting ratio and the perturbation size, we\ndevelop adaptive splitting schemes for suitably controlling the Type \\rom{1}\nerror subject to computational constraints. By deflating the\n\\textit{bias-sd-ratio}, we establish asymptotic null distributions of the test\nstatistics and their consistency in terms of statistical power. Our theoretical\npower analysis and simulations indicate that the one-split test is more\npowerful than the two-split test, though the latter is easier to apply for\nlarge datasets. Moreover, the combined tests are more stable while compensating\nfor a power loss by repeated sample splitting. Numerically, we demonstrate the\nutility of the proposed tests on two benchmark examples. Accompanying this\npaper is our Python library {\\tt dnn-inference}\nhttps://dnn-inference.readthedocs.io/en/latest/ that implements the proposed\ntests.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 00:59:19 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 03:28:58 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Dai", "Ben", ""], ["Shen", "Xiaotong", ""], ["Pan", "Wei", ""]]}, {"id": "2103.05059", "submitter": "Dylan Troop", "authors": "Dylan Troop, Fr\\'ed\\'eric Godin, Jia Yuan Yu", "title": "Bias-Corrected Peaks-Over-Threshold Estimation of the CVaR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conditional value-at-risk (CVaR) is a useful risk measure in fields such\nas machine learning, finance, insurance, energy, etc. When measuring very\nextreme risk, the commonly used CVaR estimation method of sample averaging does\nnot work well due to limited data above the value-at-risk (VaR), the quantile\ncorresponding to the CVaR level. To mitigate this problem, the CVaR can be\nestimated by extrapolating above a lower threshold than the VaR using a\ngeneralized Pareto distribution (GPD), which is often referred to as the\npeaks-over-threshold (POT) approach. This method often requires a very high\nthreshold to fit well, leading to high variance in estimation, and can induce\nsignificant bias if the threshold is chosen too low. In this paper, we derive a\nnew expression for the GPD approximation error of the CVaR, a bias term induced\nby the choice of threshold, as well as a bias correction method for the\nestimated GPD parameters. This leads to the derivation of a new estimator for\nthe CVaR that we prove to be asymptotically unbiased. In a practical setting,\nwe show through experiments that our estimator provides a significant\nperformance improvement compared with competing CVaR estimators in finite\nsamples. As a consequence of our bias correction method, it is also shown that\na much lower threshold can be selected without introducing significant bias.\nThis allows a larger portion of data to be be used in CVaR estimation compared\nwith the typical POT approach, leading to more stable estimates. As secondary\nresults, a new estimator for a second-order parameter of heavy-tailed\ndistributions is derived, as well as a confidence interval for the CVaR which\nenables quantifying the level of variability in our estimator.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 20:29:06 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Troop", "Dylan", ""], ["Godin", "Fr\u00e9d\u00e9ric", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "2103.05092", "submitter": "Larry Wasserman", "authors": "Isabella Verdinelli and Larry Wasserman", "title": "Forest Guided Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We use the output of a random forest to define a family of local smoothers\nwith spatially adaptive bandwidth matrices. The smoother inherits the\nflexibility of the original forest but, since it is a simple, linear smoother,\nit is very interpretable and it can be used for tasks that would be intractable\nfor the original forest. This includes bias correction, confidence intervals,\nassessing variable importance and methods for exploring the structure of the\nforest. We illustrate the method on some synthetic examples and on data related\nto Covid-19.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 21:49:52 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Verdinelli", "Isabella", ""], ["Wasserman", "Larry", ""]]}, {"id": "2103.05161", "submitter": "Robert L Obenchain PhD", "authors": "Robert L. Obenchain", "title": "The Efficient Shrinkage Path: Maximum Likelihood of Minimum MSE Risk", "comments": "21 pages, 9 figures. arXiv admin note: substantial text overlap with\n  withdrawn arXiv:2005.14291", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new generalized ridge regression shrinkage path is proposed that is as\nshort as possible under the restriction that it must pass through the vector of\nregression coefficient estimators that make the overall Optimal Variance-Bias\nTrade-Off under Normal distribution-theory. Five distinct types of ridge TRACE\ndisplays plus other graphics for this efficient path are motivated and\nillustrated here. These visualizations provide invaluable data-analytic\ninsights and improved self-confidence to researchers and data scientists\nfitting linear models to ill-conditioned (confounded) data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 01:04:55 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 15:56:19 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 19:16:58 GMT"}, {"version": "v4", "created": "Tue, 15 Jun 2021 21:19:09 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Obenchain", "Robert L.", ""]]}, {"id": "2103.05262", "submitter": "Rafael Wei{\\ss}bach", "authors": "Rafael Wei{\\ss}bachm, Achim D\\\"orre, Dominik Wied, Gabriele Doblhammer\n  and Anne Fink", "title": "The Effect of Stroke on Dementia Onset: Left-Truncation and\n  Right-Censoring", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We observe a quarter million people over a period of nine years and are\ninterested in the effect of a stroke on the probability of dementia onset.\nRandomly left-truncated has a person been that was already deceased before the\nperiod. The ages at a stroke event or dementia onset are conditionally fixed\nright-censored, when either event may still occur, but after the period. We\nincorporate death and model the history of the three events by a homogeneous\nMarkov process. The compensator for the respective counting processes is\nderived and Jacod's formula yields the likelihood contribution, conditional on\nobservation. An Appendix is devoted to the role of filtrations in deriving the\nlikelihood, for the simplification of merged non-dead states. Asymptotic\nnormality of the estimated intensities is derived by martingale theory,\nrelative to the size of the sample including the truncated persons. The data of\na German health insurance reveals that after a stroke, the intensity of\ndementia onset is increased from 0.02 to 0.07, for Germans born in the first\nhalf on the 20th century. The intensity difference has a 95%-confidence\ninterval of [0.048,0.051] and the difference halves when we extent to an\nage-inhomogeneous model due to Simpson's paradox.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 07:17:12 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Wei\u00dfbachm", "Rafael", ""], ["D\u00f6rre", "Achim", ""], ["Wied", "Dominik", ""], ["Doblhammer", "Gabriele", ""], ["Fink", "Anne", ""]]}, {"id": "2103.05499", "submitter": "Yuan Shijie", "authors": "Yuan Ji, Shijie Yuan", "title": "Lessons Learned from the Bayesian Design and Analysis for the BNT162b2\n  COVID-19 Vaccine Phase 3 Trial", "comments": "COVID-19, Bayesian credible interval, Confidence interval", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phase III BNT162b2 mRNA COVID-19 vaccine trial is based on a Bayesian\ndesign and analysis, and the main evidence of vaccine efficacy is presented in\nBayesian statistics. Confusion and mistakes are produced in the presentation of\nthe Bayesian results. Some key statistics, such as Bayesian credible intervals,\nare mislabeled and stated as confidence intervals. Posterior probabilities of\nthe vaccine efficacy are not reported as the main results. We illustrate the\nmain differences in the reporting of Bayesian analysis results for a clinical\ntrial and provide four recommendations. We argue that statistical evidence from\na Bayesian trial, when presented properly, is easier to interpret and directly\naddresses the main clinical questions, thereby better supporting regulatory\ndecision making. We also recommend using abbreviation \"BI\" to represent\nBayesian credible intervals as a differentiation to \"CI\" which stands for\nconfidence interval.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 10:33:23 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Ji", "Yuan", ""], ["Yuan", "Shijie", ""]]}, {"id": "2103.05557", "submitter": "Georgia Papadogeorgou", "authors": "Georgia Papadogeorgou, Carolina Bello, Otso Ovaskainen, David B.\n  Dunson", "title": "Covariate-informed latent interaction models: Addressing geographic &\n  taxonomic bias in predicting bird-plant interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Climate change and reductions in natural habitats necessitate that we better\nunderstand species' interactivity and how biological communities respond to\nenvironmental changes. However, ecological studies of species' interactions are\nlimited by geographic and taxonomic bias which can lead to severe\nunder-representation of certain species and distort our understanding of\ninter-species interactions. We illustrate that ignoring these biases can result\nin poor performance. We develop a model for predicting species' interactions\nthat (a) accounts for errors in the recorded interaction networks, (b)\naddresses the geographic and taxonomic bias of existing studies, (c) is based\non latent factors to increase flexibility and borrow information across\nspecies, (d) incorporates covariates in a flexible manner to inform the latent\nfactors, and (e) uses a meta-analysis data set from 166 individual studies. We\nfocus on interactions among 242 birds and 511 plants in the Brazilian Atlantic\nForest, and identify 5% of pairs of species with an unrecorded interaction, but\nposterior probability of existing that is over 80%. Finally, we develop a\npermutation-based variable importance procedure and identify that a bird's body\nmass and a plant's fruit diameter are most important in driving the presence\nand detection of species interactions, with a multiplicative relationship.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:08:39 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Papadogeorgou", "Georgia", ""], ["Bello", "Carolina", ""], ["Ovaskainen", "Otso", ""], ["Dunson", "David B.", ""]]}, {"id": "2103.05689", "submitter": "Lucy D'Agostino McGowan", "authors": "Lucy D'Agostino McGowan, Roger D. Peng, Stephanie C. Hicks", "title": "Design Principles for Data Analysis", "comments": "arXiv admin note: text overlap with arXiv:1903.07639", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data science revolution has led to an increased interest in the practice\nof data analysis. While much has been written about statistical thinking, a\ncomplementary form of thinking that appears in the practice of data analysis is\ndesign thinking -- the problem-solving process to understand the people for\nwhom a product is being designed. For a given problem, there can be significant\nor subtle differences in how a data analyst (or producer of a data analysis)\nconstructs, creates, or designs a data analysis, including differences in the\nchoice of methods, tooling, and workflow. These choices can affect the data\nanalysis products themselves and the experience of the consumer of the data\nanalysis. Therefore, the role of a producer can be thought of as designing the\ndata analysis with a set of design principles. Here, we introduce design\nprinciples for data analysis and describe how they can be mapped to data\nanalyses in a quantitative, objective and informative manner. We also provide\nempirical evidence of variation of principles within and between both producers\nand consumers of data analyses. Our work leads to two insights: it suggests a\nformal mechanism to describe data analyses based on the design principles for\ndata analysis, and it provides a framework to teach students how to build data\nanalyses using formal design principles.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:48:25 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["McGowan", "Lucy D'Agostino", ""], ["Peng", "Roger D.", ""], ["Hicks", "Stephanie C.", ""]]}, {"id": "2103.05692", "submitter": "Brian Knaeble", "authors": "Brian Knaeble, Braxton Osting, and Placede Tshiaba", "title": "Quantifying Sufficient Randomness for Causal Inference", "comments": "22 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spurious association arises from covariance between propensity for the\ntreatment and individual risk for the outcome. For sensitivity analysis with\nstochastic counterfactuals we introduce a methodology to characterize\nuncertainty in causal inference from natural experiments and quasi-experiments.\nOur sensitivity parameters are standardized measures of variation in propensity\nand individual risk, and one minus their geometric mean is an intuitive measure\nof randomness in the data generating process. Within our latent propensity-risk\nmodel, we show how to compute from contingency table data a threshold, $T$, of\nsufficient randomness for causal inference. If the actual randomness of the\ndata generating process exceeds this threshold then causal inference is\nwarranted.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:54:29 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Knaeble", "Brian", ""], ["Osting", "Braxton", ""], ["Tshiaba", "Placede", ""]]}, {"id": "2103.05747", "submitter": "Likun Zhang", "authors": "Likun Zhang and Benjamin A. Shaby", "title": "Asymptotic posterior normality of the generalized extreme value\n  distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The univariate generalized extreme value (GEV) distribution is the most\ncommonly used tool for analysing the properties of rare events. The ever\ngreater utilization of Bayesian methods for extreme value analysis warrants\ndetailed theoretical investigation, which has thus far been underdeveloped.\nEven the most basic asymptotic results are difficult to obtain because the GEV\nfails to satisfy standard regularity conditions. Here, we prove that the\nposterior distribution of the GEV parameter vector, given an independent and\nidentically distributed sequence of observations, converges to a normal\ndistribution centred at the true parameter. The proof necessitates analysing\nintegrals of the GEV likelihood function over the entire parameter space, which\nrequires considerable care because the support of the GEV density depends on\nthe parameters in complicated ways.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 22:40:55 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Zhang", "Likun", ""], ["Shaby", "Benjamin A.", ""]]}, {"id": "2103.05765", "submitter": "Tyler VanderWeele", "authors": "Adam J. Sullivan, Douglas D. Gunzler, Nathan Morris, Tyler J.\n  VanderWeele", "title": "Longitudinal Mediation Analysis with Latent Growth Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper considers mediation analysis with longitudinal data under latent\ngrowth curve models within a counterfactual framework. Estimators and their\nstandard errors are derived for natural direct and indirect effects when the\nmediator, the outcome, and possibly also the exposure can be modeled by an\nunderlying latent variable giving rise to a growth curve. Settings are also\nconsidered in which the exposure is instead fixed at a single point in time.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 23:05:24 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Sullivan", "Adam J.", ""], ["Gunzler", "Douglas D.", ""], ["Morris", "Nathan", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "2103.05775", "submitter": "Tyler VanderWeele", "authors": "Adam J. Sullivan and Tyler J. VanderWeele", "title": "Bias and sensitivity analysis for unmeasured confounders in linear\n  structural equation models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the extent of the biases that may arise when an\nunmeasured confounder is omitted from a structural equation model (SEM) and we\npropose sensitivity analysis techniques to correct for such biases. We give an\nanalysis of which effects in an SEM are, and are not, biased by an unmeasured\nconfounder. It is shown that a single unmeasured confounder will bias not just\none, but numerous, effects in an SEM. We present sensitivity analysis\ntechniques to correct for biases in total, direct, and indirect effects when\nusing SEM analyses, and illustrate these techniques with a study of aging and\ncognitive function.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 23:09:45 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Sullivan", "Adam J.", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "2103.05856", "submitter": "Rui Gong", "authors": "Rui Gong, Xiaoqian Sun, Leping Liu, Yu-Bo Wang", "title": "Bayesian Poisson Mortality Projections with Incomplete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The missing data problem pervasively exists in statistical applications. Even\nas simple as the count data in mortality projections, it may not be available\nfor certain age-and-year groups due to the budget limitations or difficulties\nin tracing research units, resulting in the follow-up estimation and prediction\ninaccuracies. To circumvent this data-driven challenge, we extend the Poisson\nlog-normal Lee-Carter model to accommodate a more flexible time structure, and\ndevelop the new sampling algorithm that improves the MCMC convergence when\ndealing with incomplete mortality data. Via the overdispersion term and Gibbs\nsampler, the extended model can be re-written as the dynamic linear model so\nthat both Kalman and sequential Kalman filters can be incorporated into the\nsampling scheme. Additionally, our meticulous prior settings can avoid the\nre-scaling step in each MCMC iteration, and allow model selection\nsimultaneously conducted with estimation and prediction. The proposed method is\napplied to the mortality data of Chinese males during the period 1995-2016 to\nyield mortality rate forecasts for 2017-2039. The results are comparable to\nthose based on the imputed data set, suggesting that our approach could handle\nincomplete data well.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:41:34 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Gong", "Rui", ""], ["Sun", "Xiaoqian", ""], ["Liu", "Leping", ""], ["Wang", "Yu-Bo", ""]]}, {"id": "2103.05909", "submitter": "Luca Maestrini", "authors": "Luca Maestrini, Robert G. Aykroyd and Matt P. Wand", "title": "A Variational Inference Framework for Inverse Problems", "comments": "40 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for fitting inverse problem models via variational\nBayes approximations. This methodology guarantees flexibility to statistical\nmodel specification for a broad range of applications, good accuracy\nperformances and reduced model fitting times, when compared with standard\nMarkov chain Monte Carlo methods. The message passing and factor graph fragment\napproach to variational Bayes we describe facilitates streamlined\nimplementation of approximate inference algorithms and forms the basis to\nsoftware development. Such approach allows for supple inclusion of numerous\nresponse distributions and penalizations into the inverse problem model. Albeit\nour analysis is circumscribed to one- and two-dimensional response variables,\nwe lay down an infrastructure where streamlining algorithmic steps based on\nnullifying weak interactions between variables are extendible to inverse\nproblems in higher dimensions. Image processing applications motivated by\nbiomedical and archaeological problems are included as illustrations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 07:37:20 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Maestrini", "Luca", ""], ["Aykroyd", "Robert G.", ""], ["Wand", "Matt P.", ""]]}, {"id": "2103.05915", "submitter": "Guillaume Chauvet", "authors": "Guillaume Chauvet (IRMAR)", "title": "A cautionary note on the Hanurav-Vijayan sampling algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Hanurav-Vijayan sampling design, which is the default method\nprogrammed in the SURVEYSELECT procedure of the SAS software. We prove that it\nis equivalent to the Sunter procedure, but is capable of handling any set of\ninclusion probabilities. We prove that the Horvitz-Thompson estimator is not\ngenerally consistent under this sampling design. We propose a conditional\nHorvitz-Thompson estimator, and prove its consistency under a non-standard\nassumption on the first-order inclusion probabilities. Since this assumption\nseems difficult to control in practice, we recommend not to use the\nHanurav-Vijayan sampling design.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:09:51 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Chauvet", "Guillaume", "", "IRMAR"]]}, {"id": "2103.06071", "submitter": "Laura Jula Vanegas", "authors": "Laura Jula Vanegas, Benjamin Eltzner, Daniel Rudolf, Miroslav Dura,\n  Stephan E. Lehnart, and Axel Munk", "title": "Analyzing cross-talk between superimposed signals: Vector norm dependent\n  hidden Markov models and applications", "comments": "An R package can be found at: https://github.com/ljvanegas/VND", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and investigate a hidden Markov model (HMM) for the analysis of\naggregated, super-imposed two-state signal recordings. A major motivation for\nthis work is that often these recordings cannot be observed individually but\nonly their superposition. Among others, such models are in high demand for the\nunderstanding of cross-talk between ion channels, where each single channel\nmight take two different states which cannot be measured separately. As an\nessential building block we introduce a parametrized vector norm dependent\nMarkov chain model and characterize it in terms of permutation invariance as\nwell as conditional independence. This leads to a hidden Markov chain \"sum\"\nprocess which can be used for analyzing aggregated two-state signal\nobservations within a HMM. Additionally, we show that the model parameters of\nthe vector norm dependent Markov chain are uniquely determined by the\nparameters of the \"sum\" process and are therefore identifiable. Finally, we\nprovide algorithms to estimate the parameters and apply our methodology to\nreal-world ion channel data measurements, where we show competitive gating.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 14:17:34 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Vanegas", "Laura Jula", ""], ["Eltzner", "Benjamin", ""], ["Rudolf", "Daniel", ""], ["Dura", "Miroslav", ""], ["Lehnart", "Stephan E.", ""], ["Munk", "Axel", ""]]}, {"id": "2103.06075", "submitter": "Zhaoyuan Li", "authors": "Zhaoyuan Li and Jianfeng Yao", "title": "Extension of the Lagrange multiplier test for error cross-section\n  independence to large panels with non normal errors", "comments": "39 papers, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reexamines the seminal Lagrange multiplier test for cross-section\nindependence in a large panel model where both the number of cross-sectional\nunits n and the number of time series observations T can be large. The first\ncontribution of the paper is an enlargement of the test with two extensions:\nfirstly the new asymptotic normality is derived in a simultaneous limiting\nscheme where the two dimensions (n, T) tend to infinity with comparable\nmagnitudes; second, the result is valid for general error distribution (not\nnecessarily normal). The second contribution of the paper is a new test\nstatistic based on the sum of the fourth powers of cross-section correlations\nfrom OLS residuals, instead of their squares used in the Lagrange multiplier\nstatistic. This new test is generally more powerful, and the improvement is\nparticularly visible against alternatives with weak or sparse cross-section\ndependence. Both simulation study and real data analysis are proposed to\ndemonstrate the advantages of the enlarged Lagrange multiplier test and the\npower enhanced test in comparison with the existing procedures.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 14:25:54 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Li", "Zhaoyuan", ""], ["Yao", "Jianfeng", ""]]}, {"id": "2103.06148", "submitter": "Klaus Nordhausen", "authors": "Lea Flumian, Markus Matilainen, Klaus Nordhausen, Sara Taskinen", "title": "Stationary subspace analysis based on second-order statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In stationary subspace analysis (SSA) one assumes that the observable\np-variate time series is a linear mixture of a k-variate nonstationary time\nseries and a (p-k)-variate stationary time series. The aim is then to estimate\nthe unmixing matrix which transforms the observed multivariate time series onto\nstationary and nonstationary components. In the classical approach multivariate\ndata are projected onto stationary and nonstationary subspaces by minimizing a\nKullback-Leibler divergence between Gaussian distributions, and the method only\ndetects nonstationarities in the first two moments. In this paper we consider\nSSA in a more general multivariate time series setting and propose SSA methods\nwhich are able to detect nonstationarities in mean, variance and\nautocorrelation, or in all of them. Simulation studies illustrate the\nperformances of proposed methods, and it is shown that especially the method\nthat detects all three types of nonstationarities performs well in various time\nseries settings. The paper is concluded with an illustrative example.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:57:21 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Flumian", "Lea", ""], ["Matilainen", "Markus", ""], ["Nordhausen", "Klaus", ""], ["Taskinen", "Sara", ""]]}, {"id": "2103.06261", "submitter": "Xiaoqing Tan", "authors": "Xiaoqing Tan, Chung-Chou H. Chang, Lu Tang", "title": "A Tree-based Federated Learning Approach for Personalized Treatment\n  Effect Estimation from Heterogeneous Data Sources", "comments": "An earlier version won JSM 2021 Student Paper Competition (SLDS\n  section) Honorable Mention", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is an appealing framework for analyzing sensitive data\nfrom distributed health data networks. Under this framework, data partners at\nlocal sites collaboratively build an analytical model under the orchestration\nof a coordinating site, while keeping the data decentralized. While integrating\ninformation from multiple sources may boost statistical efficiency, existing\nfederated learning methods mainly assume data across sites are homogeneous\nsamples of the global population, failing to properly account for the extra\nvariability across sites in estimation and inference. Drawing on a\nmulti-hospital electronic health records network, we develop an efficient and\ninterpretable tree-based ensemble of personalized treatment effect estimators\nto join results across hospital sites, while actively modeling for the\nheterogeneity in data sources through site partitioning. The efficiency of this\napproach is demonstrated by a study of causal effects of oxygen saturation on\nhospital mortality and backed up by comprehensive numerical results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 18:51:30 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 00:35:12 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Tan", "Xiaoqing", ""], ["Chang", "Chung-Chou H.", ""], ["Tang", "Lu", ""]]}, {"id": "2103.06328", "submitter": "Moritz Marbach", "authors": "Dominik Hangartner, Moritz Marbach, Leonard Henckel, Marloes H.\n  Maathuis, Rachel R. Kelz, and Luke Keele", "title": "Profiling Compliers in Instrumental Variables Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable (IV) analyses are becoming common in health services\nresearch and epidemiology. IV analyses can be used both to analyze randomized\ntrials with noncompliance and as a form of natural experiment. In these\nanalyses, investigators often adopt a monotonicity assumption, which implies\nthat the relevant effect only applies to a subset of the study population known\nas compliers. Since the estimated effect is not the average treatment effect of\nthe study population, it is important to compare the characteristics of\ncompliers and non-compliers. Profiling compliers and non-compliers is necessary\nto understand what subpopulation the researcher is making inferences about, and\nan important first step in evaluating the external validity (or lack thereof)\nof the IV estimate for compliers. Here, we discuss the assumptions necessary\nfor profiling, which are weaker than the assumptions necessary for identifying\nthe local average treatment effect if the instrument is randomly assigned. We\nthen outline a simple and general method to characterize compliers and\nnoncompliers using baseline covariates. Next, we extend current methods by\nderiving standard errors for these estimates. We demonstrate these methods\nusing an IV known as tendency to operate (TTO) from health services research.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 20:16:13 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Hangartner", "Dominik", ""], ["Marbach", "Moritz", ""], ["Henckel", "Leonard", ""], ["Maathuis", "Marloes H.", ""], ["Kelz", "Rachel R.", ""], ["Keele", "Luke", ""]]}, {"id": "2103.06347", "submitter": "Ivor Cribben", "authors": "Martin Ondrus, Emily Olds, Ivor Cribben", "title": "Factorized Binary Search: change point detection in the network\n  structure of multivariate high-dimensional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional magnetic resonance imaging (fMRI) time series data presents a\nunique opportunity to understand temporal brain connectivity, and models that\nuncover the complex dynamic workings of this organ are of keen interest in\nneuroscience. Change point models can capture and reflect the dynamic nature of\nbrain connectivity, however methods that translate well into a high-dimensional\ncontext (where $p>>n$) are scarce. To this end, we introduce\n$\\textit{factorized binary search}$ (FaBiSearch), a novel change point\ndetection method in the network structure of multivariate high-dimensional time\nseries. FaBiSearch uses non-negative matrix factorization, an unsupervised\ndimension reduction technique, and a new binary search algorithm to identify\nmultiple change points. In addition, we propose a new method for network\nestimation for data between change points. We show that FaBiSearch outperforms\nanother state-of-the-art method on simulated data sets and we apply FaBiSearch\nto a resting-state and to a task-based fMRI data set. In particular, for the\ntask-based data set, we explore network dynamics during the reading of Chapter\n9 in $\\textit{Harry Potter and the Sorcerer's Stone}$ and find that change\npoints across subjects coincide with key plot twists. Further, we find that the\ndensity of networks was positively related to the frequency of speech between\ncharacters in the story. Finally, we make all the methods discussed available\nin the R package $\\textbf{fabisearch}$ on CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 21:25:20 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ondrus", "Martin", ""], ["Olds", "Emily", ""], ["Cribben", "Ivor", ""]]}, {"id": "2103.06363", "submitter": "Mingming Liu", "authors": "Mingming Liu, Jing Yang, Yushi Liu, Bochao Jia, Yun-Fei Chen, Luna\n  Sun, Shujie Ma", "title": "A fusion learning method to subgroup analysis of Alzheimer's disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncovering the heterogeneity in the disease progression of Alzheimer's is a\nkey factor to disease understanding and treatment development, so that\ninterventions can be tailored to target the subgroups that will benefit most\nfrom the treatment, which is an important goal of precision medicine. However,\nin practice, one top methodological challenge hindering the heterogeneity\ninvestigation is that the true subgroup membership of each individual is often\nunknown. In this article, we aim to identify latent subgroups of individuals\nwho share a common disorder progress over time, to predict latent subgroup\nmemberships, and to estimate and infer the heterogeneous trajectories among the\nsubgroups. To achieve these goals, we apply a concave fusion learning method\nproposed in Ma and Huang (2017) and Ma et al. (2019) to conduct subgroup\nanalysis for longitudinal trajectories of the Alzheimer's disease data. The\nheterogeneous trajectories are represented by subject-specific unknown\nfunctions which are approximated by B-splines. The concave fusion method can\nsimultaneously estimate the spline coefficients and merge them together for the\nsubjects belonging to the same subgroup to automatically identify subgroups and\nrecover the heterogeneous trajectories. The resulting estimator of the disease\ntrajectory of each subgroup is supported by an asymptotic distribution. It\nprovides a sound theoretical basis for further conducting statistical inference\nin subgroup analysis..\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:09:42 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 13:24:12 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Liu", "Mingming", ""], ["Yang", "Jing", ""], ["Liu", "Yushi", ""], ["Jia", "Bochao", ""], ["Chen", "Yun-Fei", ""], ["Sun", "Luna", ""], ["Ma", "Shujie", ""]]}, {"id": "2103.06368", "submitter": "Meizi Liu", "authors": "Meizi Liu, Yuan Ji, Ji Lin", "title": "PoD-BIN: A Probability of Decision Bayesian Interval Design for\n  Time-to-Event Dose-Finding Trials with Multiple Toxicity Grades", "comments": "31 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Bayesian framework based on \"probability of decision\" for\ndose-finding trial designs. The proposed PoD-BIN design evaluates the posterior\npredictive probabilities of up-and-down decisions. In PoD-BIN, multiple grades\nof toxicity, categorized as the mild toxicity (MT) and dose-limiting toxicity\n(DLT), are modeled simultaneously, and the primary outcome of interests is\ntime-to-toxicity for both MT and DLT. This allows the possibility of enrolling\nnew patients when previously enrolled patients are still being followed for\ntoxicity, thus potentially shortening trial length. The Bayesian decision rules\nin PoD-BIN utilize the probability of decisions to balance the need to speed up\nthe trial and the risk of exposing patients to overly toxic doses. We\ndemonstrate via numerical examples the resulting balance of speed and safety of\nPoD-BIN and compare to existing designs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:23:06 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Liu", "Meizi", ""], ["Ji", "Yuan", ""], ["Lin", "Ji", ""]]}, {"id": "2103.06392", "submitter": "Christopher Harshaw", "authors": "Christopher Harshaw, David Eisenstat, Vahab Mirrokni, Jean\n  Pouget-Abadie", "title": "Design and Analysis of Bipartite Experiments under a Linear\n  Exposure-Response Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The bipartite experimental framework is a recently proposed causal setting,\nwhere a bipartite graph links two distinct types of units: units that receive\ntreatment and units whose outcomes are of interest to the experimenter. Often\nmotivated by market experiments, the bipartite experimental framework has been\nused for example to investigate the causal effects of supply-side changes on\ndemand-side behavior. Similar to settings with interference and other\nviolations of the stable unit treatment value assumption (SUTVA), additional\nassumptions on potential outcomes must be made for valid inference. In this\npaper, we consider the problem of estimating the average treatment effect in\nthe bipartite experimental framework under a linear exposure-response model. We\npropose the Exposure Reweighted Linear (ERL) Estimator, an unbiased linear\nestimator of the average treatment effect in this setting. Furthermore, we\npresent Exposure-Design, a cluster-based design which aims to increase the\nprecision of the ERL estimator by realizing desirable exposure distributions.\nFinally, we demonstrate the effectiveness of the proposed estimator and design\non a publicly available Amazon user-item review graph.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 00:12:57 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Harshaw", "Christopher", ""], ["Eisenstat", "David", ""], ["Mirrokni", "Vahab", ""], ["Pouget-Abadie", "Jean", ""]]}, {"id": "2103.06421", "submitter": "Yuan Shijie", "authors": "Xiaolei Lin, Jiaying Lyu, Shijie Yuan, Sue-Jane Wang, Yuan Ji", "title": "BaySize: Bayesian Sample Size Planning for Phase I Dose-Finding Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose BaySize, a sample size calculator for phase I clinical trials\nusing Bayesian models. BaySize applies the concept of effect size in dose\nfinding, assuming the MTD is defined based on an equivalence interval.\nLeveraging a decision framework that involves composite hypotheses, BaySize\nutilizes two prior distributions, the fitting prior (for model fitting) and\nsampling prior (for data generation), to conduct sample size calculation under\ndesirable statistical power. Look-up tables are generated to facilitate\npractical applications. To our knowledge, BaySize is the first sample size tool\nthat can be applied to a broad range of phase I trial designs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 02:51:42 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Lin", "Xiaolei", ""], ["Lyu", "Jiaying", ""], ["Yuan", "Shijie", ""], ["Wang", "Sue-Jane", ""], ["Ji", "Yuan", ""]]}, {"id": "2103.06428", "submitter": "Will Wei Sun", "authors": "Hilda S Ibriga and Will Wei Sun", "title": "Covariate-assisted Sparse Tensor Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to provably complete a sparse and highly-missing tensor in the\npresence of covariate information along tensor modes. Our motivation comes from\nonline advertising where users click-through-rates (CTR) on ads over various\ndevices form a CTR tensor that has about 96% missing entries and has many zeros\non non-missing entries, which makes the standalone tensor completion method\nunsatisfactory. Beside the CTR tensor, additional ad features or user\ncharacteristics are often available. In this paper, we propose\nCovariate-assisted Sparse Tensor Completion (COSTCO) to incorporate covariate\ninformation for the recovery of the sparse tensor. The key idea is to jointly\nextract latent components from both the tensor and the covariate matrix to\nlearn a synthetic representation. Theoretically, we derive the error bound for\nthe recovered tensor components and explicitly quantify the improvements on\nboth the reveal probability condition and the tensor recovery accuracy due to\ncovariates. Finally, we apply COSTCO to an advertisement dataset consisting of\na CTR tensor and ad covariate matrix, leading to 23% accuracy improvement over\nthe baseline. An important by-product is that ad latent components from COSTCO\nreveal interesting ad clusters, which are useful for better ad targeting.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 03:13:04 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ibriga", "Hilda S", ""], ["Sun", "Will Wei", ""]]}, {"id": "2103.06471", "submitter": "Fredrik S\\\"avje", "authors": "Fredrik S\\\"avje", "title": "Causal inference with misspecified exposure mappings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposure mappings facilitate investigations of complex causal effects when\nunits interact in experiments. Current methods assume that the exposures are\ncorrectly specified, but such an assumption cannot be verified, and its\nvalidity is often questionable. This paper describes conditions under which one\ncan draw inferences about exposure effects when the exposures are misspecified.\nThe main result is a proof of consistency under mild conditions on the errors\nintroduced by the misspecification. The rate of convergence is determined by\nthe dependence between units' specification errors, and consistency is achieved\neven if the errors are large as long as they are sufficiently weakly dependent.\nIn other words, exposure effects can be precisely estimated also under\nmisspecification as long as the units' exposures are not misspecified in the\nsame way. The limiting distribution of the estimator is discussed. Asymptotic\nnormality is achieved under stronger conditions than those needed for\nconsistency. Similar conditions also facilitate conservative variance\nestimation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:35:41 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["S\u00e4vje", "Fredrik", ""]]}, {"id": "2103.06476", "submitter": "Ian Waudby-Smith", "authors": "Ian Waudby-Smith, David Arbour, Ritwik Sinha, Edward H. Kennedy, and\n  Aaditya Ramdas", "title": "Doubly robust confidence sequences for sequential causal inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives time-uniform confidence sequences (CS) for causal effects\nin experimental and observational settings. A confidence sequence for a target\nparameter $\\psi$ is a sequence of confidence intervals $(C_t)_{t=1}^\\infty$\nsuch that every one of these intervals simultaneously captures $\\psi$ with high\nprobability. Such CSs provide valid statistical inference for $\\psi$ at\narbitrary stopping times, unlike classical fixed-time confidence intervals\nwhich require the sample size to be fixed in advance. Existing methods for\nconstructing CSs focus on the nonasymptotic regime where certain assumptions\n(such as known bounds on the random variables) are imposed, while doubly robust\nestimators of causal effects rely on (asymptotic) semiparametric theory. We use\nsequential versions of central limit theorem arguments to construct\nlarge-sample CSs for causal estimands, with a particular focus on the average\ntreatment effect (ATE) under nonparametric conditions. These CSs allow analysts\nto update inferences about the ATE in lieu of new data, and experiments can be\ncontinuously monitored, stopped, or continued for any data-dependent reason,\nall while controlling the type-I error. Finally, we describe how these CSs\nreadily extend to other causal estimands and estimators, providing a new\nframework for sequential causal inference in a wide array of problems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:45:35 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 19:50:19 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Waudby-Smith", "Ian", ""], ["Arbour", "David", ""], ["Sinha", "Ritwik", ""], ["Kennedy", "Edward H.", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2103.06567", "submitter": "Savita Pareek", "authors": "Savita Pareek, Kalyan Das, and Siuli Mukhopadhyay", "title": "Likelihood-based missing data analysis in multivariate crossover trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For gene expression data measured in a crossover trial, a multivariate\nmixed-effects model seems to be most appropriate. Standard statistical\ninference fails to provide reliable results when some responses are missing.\nParticularly for crossover studies, missingness is a serious concern as the\ntrial requires a small number of participants. A Monte Carlo EM (MCEM) based\ntechnique has been adopted to deal with this situation. Along with estimation,\na MCEM likelihood ratio test (LRTs) is developed for testing the fixed effects\nin such a multivariate crossover model with missing data. Intensive simulation\nstudies have been carried out prior to the analysis of the gene expression\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:50:25 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Pareek", "Savita", ""], ["Das", "Kalyan", ""], ["Mukhopadhyay", "Siuli", ""]]}, {"id": "2103.06585", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn", "title": "Simultaneous comparisons of treatments versus control (Dunnett-type\n  tests) for location-scale alternatives", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Commonly, the comparisons of treatment groups versus a control is performed\nfor location effects only where possible scale effects are considered as\ndisturbing. Sometimes scale effects are also relevant, as a kind of early\nindicator for changes. Here several approaches for Dunnett-type tests for\nlocation or scale effects are proposed and compared by a simulation study. Two\nreal data examples are analysed accordingly and the related R-code is available\nin the Appendix.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 10:29:53 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Hothorn", "Ludwig A.", ""]]}, {"id": "2103.06606", "submitter": "Alexander Volkmann", "authors": "Alexander Volkmann, Almond St\\\"ocker, Fabian Scheipl, Sonja Greven", "title": "Multivariate Functional Additive Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multivariate functional data can be intrinsically multivariate like movement\ntrajectories in 2D or complementary like precipitation, temperature, and wind\nspeeds over time at a given weather station. We propose a multivariate\nfunctional additive mixed model (multiFAMM) and show its application to both\ndata situations using examples from sports science (movement trajectories of\nsnooker players) and phonetic science (acoustic signals and articulation of\nconsonants). The approach includes linear and nonlinear covariate effects and\nmodels the dependency structure between the dimensions of the responses using\nmultivariate functional principal component analysis. Multivariate functional\nrandom intercepts capture both the auto-correlation within a given function and\ncross-correlations between the multivariate functional dimensions. They also\nallow us to model between-function correlations as induced by e.g.\\ repeated\nmeasurements or crossed study designs. Modeling the dependency structure\nbetween the dimensions can generate additional insight into the properties of\nthe multivariate functional process, improves the estimation of random effects,\nand yields corrected confidence bands for covariate effects. Extensive\nsimulation studies indicate that a multivariate modeling approach is more\nparsimonious than fitting independent univariate models to the data while\nmaintaining or improving model fit.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 11:10:23 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Volkmann", "Alexander", ""], ["St\u00f6cker", "Almond", ""], ["Scheipl", "Fabian", ""], ["Greven", "Sonja", ""]]}, {"id": "2103.06939", "submitter": "Preston Biro", "authors": "Preston Biro and Stephen G. Walker", "title": "A Reinforcement Learning Based Approach to Play Calling in Football", "comments": "62 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the vast amount of data collected on football and the growth of\ncomputing abilities, many games involving decision choices can be optimized.\nThe underlying rule is the maximization of an expected utility of outcomes and\nthe law of large numbers. The data available allows us to compute with high\naccuracy the probabilities of outcomes of decisions and the well defined points\nsystem in the game allows us to have the necessary terminal utilities. With\nsome well established theory we can then optimize choices at a single play\nlevel.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 20:23:07 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Biro", "Preston", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2103.06979", "submitter": "Dmitry Ivanenko Alexandrovich", "authors": "D. O. Ivanenko, R. V. Pogorielov", "title": "Parameter estimation in models generated by SDE's with symmetric alpha\n  stable noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The article considers vector parameter estimators in statistical models\ngenerated by Levy processes. An improved one step estimator is presented that\ncan be used for improving any other estimator. Combined numerical methods for\noptimization problems are proposed. A software has been developed and a\ncorrespondent testing and comparison have been presented.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 18:35:33 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ivanenko", "D. O.", ""], ["Pogorielov", "R. V.", ""]]}, {"id": "2103.07005", "submitter": "Luis Nieto-Barajas Dr.", "authors": "Luis E. Nieto-Barajas", "title": "Bayesian nonparametric dynamic hazard rates in evolutionary life tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the study of life tables the random variable of interest is usually\nassumed discrete since mortality rates are studied for integer ages. In dynamic\nlife tables a time domain is included to account for the evolution effect of\nthe hazard rates in time. In this article we follow a survival analysis\napproach and use a nonparametric description of the hazard rates. We construct\na discrete time stochastic processes that reflects dependence across age as\nwell as in time. This process is used as a bayesian nonparametric prior\ndistribution for the hazard rates for the study of evolutionary life tables.\nPrior properties of the process are studied and posterior distributions are\nderived. We present a simulation study, with the inclusion of right censored\nobservations, as well as a real data analysis to show the performance of our\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 23:34:58 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Nieto-Barajas", "Luis E.", ""]]}, {"id": "2103.07039", "submitter": "Christian Caama\\~no", "authors": "Diego I. Gallardo, Marcelo Bourguignon, Yolanda M. G\\'omez, Christian\n  Caama\\~no-Carrillo", "title": "Parametric quantile regression models for fitting double bounded\n  response with application to COVID-19 mortality rate data", "comments": "17 pag", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop two fully parametric quantile regression models,\nbased on power Johnson SB distribution Cancho et al. (2020), for modeling unit\ninterval response at different quantiles. In particular, the conditional\ndistribution is modelled by the power Johnson SB distribution. The maximum\nlikelihood method is employed to estimate the model parameters. Simulation\nstudies are conducted to evaluate the performance of the maximum likelihood\nestimators in finite samples. Furthermore, we discuss residuals and influence\ndiagnostic tools. The effectiveness of our proposals is illustrated with two\ndata set given by the mortality rate of COVID-19 in different countries.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 02:00:42 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Gallardo", "Diego I.", ""], ["Bourguignon", "Marcelo", ""], ["G\u00f3mez", "Yolanda M.", ""], ["Caama\u00f1o-Carrillo", "Christian", ""]]}, {"id": "2103.07060", "submitter": "Wei Chen", "authors": "Wei Chen", "title": "A Novel Probability Weighting Method To Fit Gaussian Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gaussian functions are commonly used in different fields, many real signals\ncan be modeled into such form. Research aiming to obtain a precise fitting\nresult for these functions is very meaningful. This manuscript intends to\nintroduce a new algorithm used to estimate the full parameters of the\nGaussian-shaped function. It is basically a weighting method, starting from\nCaruana's method, while the selection of weighting factors is from the\nstatistics view and based on the estimation of the confidence level for the\nsamples. Tests designed for comparison with current similar methods have been\nconducted. The simulation results indicate a good performance for this new\nmethod, mainly in precision and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 03:16:10 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Chen", "Wei", ""]]}, {"id": "2103.07066", "submitter": "Jann Spiess", "authors": "Jann Spiess and Vasilis Syrgkanis", "title": "Evidence-Based Policy Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past years have seen seen the development and deployment of\nmachine-learning algorithms to estimate personalized treatment-assignment\npolicies from randomized controlled trials. Yet such algorithms for the\nassignment of treatment typically optimize expected outcomes without taking\ninto account that treatment assignments are frequently subject to hypothesis\ntesting. In this article, we explicitly take significance testing of the effect\nof treatment-assignment policies into account, and consider assignments that\noptimize the probability of finding a subset of individuals with a\nstatistically significant positive treatment effect. We provide an efficient\nimplementation using decision trees, and demonstrate its gain over selecting\nsubsets based on positive (estimated) treatment effects. Compared to standard\ntree-based regression and classification tools, this approach tends to yield\nsubstantially higher power in detecting subgroups with positive treatment\neffects.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 03:36:03 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Spiess", "Jann", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "2103.07088", "submitter": "Xiaowu Dai", "authors": "Xiaowu Dai and Lexin Li", "title": "Orthogonal Statistical Inference for Multimodal Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal imaging has transformed neuroscience research. While it presents\nunprecedented opportunities, it also imposes serious challenges. Particularly,\nit is difficult to combine the merits of interpretability attributed to a\nsimple association model and flexibility achieved by a highly adaptive\nnonlinear model. In this article, we propose an orthogonal statistical\ninferential framework, built upon the Neyman orthogonality and a form of\ndecomposition orthogonality, for multimodal data analysis. We target the\nsetting that naturally arises in almost all multimodal studies, where there is\na primary modality of interest, plus additional auxiliary modalities. We\nsuccessfully establish the root-$N$-consistency and asymptotic normality of the\nestimated primary parameter, the semi-parametric estimation efficiency, and the\nasymptotic honesty of the confidence interval of the predicted primary modality\neffect. Our proposal enjoys, to a good extent, both model interpretability and\nmodel flexibility. It is also considerably different from the existing\nstatistical methods for multimodal data integration, as well as the\northogonality-based methods for high-dimensional inferences. We demonstrate the\nefficacy of our method through both simulations and an application to a\nmultimodal neuroimaging study of Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 05:04:31 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Dai", "Xiaowu", ""], ["Li", "Lexin", ""]]}, {"id": "2103.07200", "submitter": "Tsz Chai Fung", "authors": "Tsz Chai Fung, George Tzougas, Mario Wuthrich", "title": "Mixture composite regression models with multi-type feature selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this paper is to present a mixture composite regression model for\nclaim severity modelling. Claim severity modelling poses several challenges\nsuch as multimodality, heavy-tailedness and systematic effects in data. We\ntackle this modelling problem by studying a mixture composite regression model\nfor simultaneous modeling of attritional and large claims, and for considering\nsystematic effects in both the mixture components as well as the mixing\nprobabilities. For model fitting, we present a group-fused regularization\napproach that allows us for selecting the explanatory variables which\nsignificantly impact the mixing probabilities and the different mixture\ncomponents, respectively. We develop an asymptotic theory for this regularized\nestimation approach, and fitting is performed using a novel Generalized\nExpectation-Maximization algorithm. We exemplify our approach on real motor\ninsurance data set.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 10:48:38 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Fung", "Tsz Chai", ""], ["Tzougas", "George", ""], ["Wuthrich", "Mario", ""]]}, {"id": "2103.07272", "submitter": "Jacopo Diquigiovanni", "authors": "Marco Petretta, Lorenzo Schiavon, Jacopo Diquigiovanni", "title": "Mar-Co: a new dependence structure to model match outcomes in football", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The approaches commonly used to model the number of goals in a football match\nare characterised by strong assumptions about the dependence between the number\nof goals scored by the two competing teams and about their marginal\ndistribution. In this work, we argue that the assumptions traditionally made\nare not always based on solid arguments and sometimes they can be hardly\njustified. In light of this, we propose a modification of the Dixon and Coles\n(1997) model by relaxing the assumption of Poisson-distributed marginal\nvariables and by introducing an innovative dependence structure. Specifically,\nwe define the joint distribution of the number of goals scored during a match\nby means of thoroughly chosen marginal (Mar-) and conditional distributions\n(-Co). The resulting Mar-Co model is able to balance flexibility and conceptual\nsimplicity. A real data application involving five European leagues suggests\nthat the introduction of the novel dependence structure allows to capture and\ninterpret fundamental league-specific dynamics. In terms of betting\nperformance, the newly introduced Mar-Co model does not perform worse than the\nDixon and Coles one in a traditional framework (i.e. 1-X-2 bet) and it\noutperforms the competing model when a more comprehensive dependence structure\nis needed (i.e. Under/Over 2.5 bet).\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 13:47:42 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Petretta", "Marco", ""], ["Schiavon", "Lorenzo", ""], ["Diquigiovanni", "Jacopo", ""]]}, {"id": "2103.07405", "submitter": "Christian Agrell", "authors": "Christian Agrell, Kristina Rognlien Dahl, Andreas Hafver", "title": "Optimal sequential decision making with probabilistic digital twins", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital twins are emerging in many industries, typically consisting of\nsimulation models and data associated with a specific physical system. One of\nthe main reasons for developing a digital twin, is to enable the simulation of\npossible consequences of a given action, without the need to interfere with the\nphysical system itself. Physical systems of interest, and the environments they\noperate in, do not always behave deterministically. Moreover, information about\nthe system and its environment is typically incomplete or imperfect.\nProbabilistic representations of systems and environments may therefore be\ncalled for, especially to support decisions in application areas where actions\nmay have severe consequences.\n  In this paper we introduce the probabilistic digital twin (PDT). We will\nstart by discussing how epistemic uncertainty can be treated using measure\ntheory, by modelling epistemic information via $\\sigma$-algebras. Based on\nthis, we give a formal definition of how epistemic uncertainty can be updated\nin a PDT. We then study the problem of optimal sequential decision making. That\nis, we consider the case where the outcome of each decision may inform the\nnext. Within the PDT framework, we formulate this optimization problem. We\ndiscuss how this problem may be solved (at least in theory) via the maximum\nprinciple method or the dynamic programming principle. However, due to the\ncurse of dimensionality, these methods are often not tractable in practice. To\nmend this, we propose a generic approximate solution using deep reinforcement\nlearning together with neural networks defined on sets. We illustrate the\nmethod on a practical problem, considering optimal information gathering for\nthe estimation of a failure probability.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:06:08 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Agrell", "Christian", ""], ["Dahl", "Kristina Rognlien", ""], ["Hafver", "Andreas", ""]]}, {"id": "2103.07410", "submitter": "Xiaowu Dai", "authors": "Xiaowu Dai, Saad Mouti, Marjorie Lima do Vale, Sumantra Ray, Jeffrey\n  Bohn and Lisa Goldberg", "title": "A Resampling Approach For causal Inference On Novel Two-Point\n  Time-Series With Application To Identify Risk Factors For Type-2 Diabetes And\n  Cardiovascular Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-point time-series data, characterized by baseline and follow-up\nobservations, are frequently encountered in health research. We study a novel\ntwo-point time series structure without a control group, which is driven by an\nobservational routine clinical dataset collected to monitor key risk markers of\ntype-$2$ diabetes (T2D) and cardiovascular disease (CVD). We propose a\nresampling approach called 'I-Rand' for independently sampling one of the two\ntime points for each individual and making inference on the estimated causal\neffects based on matching methods. The proposed method is illustrated with data\nfrom a service-based dietary intervention to promote a low-carbohydrate diet\n(LCD), designed to impact risk of T2D and CVD. Baseline data contain a\npre-intervention health record of study participants, and health data after LCD\nintervention are recorded at the follow-up visit, providing a two-point\ntime-series pattern without a parallel control group. Using this approach we\nfind that obesity is a significant risk factor of T2D and CVD, and an LCD\napproach can significantly mitigate the risks of T2D and CVD. We provide code\nthat implements our method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:10:54 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Dai", "Xiaowu", ""], ["Mouti", "Saad", ""], ["Vale", "Marjorie Lima do", ""], ["Ray", "Sumantra", ""], ["Bohn", "Jeffrey", ""], ["Goldberg", "Lisa", ""]]}, {"id": "2103.07425", "submitter": "Alex Stringer", "authors": "Alex Stringer, Patrick Brown, Jamie Stafford", "title": "Fast, Scalable Approximations to Posterior Distributions in Extended\n  Latent Gaussian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We define a novel class of additive models called Extended Latent Gaussian\nModels and develop a fast, scalable approximate Bayesian inference methodology\nfor this class. The new class covers a wide range of interesting models, and\nthe new methodology is better suited to large samples than existing approaches.\nWe discuss convergence theory for our posterior approximations. We then\nillustrate the computational aspects of our approach through a comparison to\nexisting methods, and demonstrate its application in three challenging\nexamples: the analysis of aggregated spatial point process data, the fitting of\na Cox proportional hazards model with partial likelihood and a latent spatial\npoint process, and an astrophysical model for estimating the mass of the Milky\nWay in the presence of multivariate measurement uncertainties. Computations\nmake use of the publicly available aghq package in the R language and code for\nthe examples in the paper is available from\nhttps://github.com/awstringer1/elgm-paper-code\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:43:36 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Stringer", "Alex", ""], ["Brown", "Patrick", ""], ["Stafford", "Jamie", ""]]}, {"id": "2103.07680", "submitter": "Martin Scharpenberg", "authors": "Werner Brannath, Martin Scharpenberg, Sylvia Schmidt", "title": "Single-stage, three-arm, adaptive test strategies for non-inferiority\n  trials with an unstable reference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For indications where only unstable reference treatments are available and\nuse of placebo is ethically justified, three-arm `gold standard' designs with\nan experimental, reference and placebo arm are recommended for non-inferiority\ntrials. In such designs, the demonstration of efficacy of the reference or\nexperimental treatment is a requirement. They have the disadvantage that only\nlittle can be concluded from the trial if the reference fails to be\nefficacious. To overcome this, we investigate novel single-stage, adaptive test\nstrategies where non-inferiority is tested only if the reference shows\nsufficient efficacy and otherwise $\\delta$-superiority of the experimental\ntreatment over placebo is tested. With a properly chosen superiority margin,\n$\\delta$-superiority indirectly shows non-inferiority. We optimize the sample\nsize for several decision rules and find that the natural, data driven test\nstrategy, which tests non-inferiority if the reference's efficacy test is\nsignificant, leads to the smallest overall and placebo sample sizes. We proof\nthat under specific constraints on the sample sizes, this procedure controls\nthe family-wise error rate. All optimal sample sizes are found to meet this\nconstraint. We finally show how to account for a relevant placebo drop-out rate\nin an efficient way and apply the new test strategy to a real life data set.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 10:41:30 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Brannath", "Werner", ""], ["Scharpenberg", "Martin", ""], ["Schmidt", "Sylvia", ""]]}, {"id": "2103.07818", "submitter": "Yiqun Chen", "authors": "Yiqun T. Chen, Sean W. Jewell, Daniela M. Witten", "title": "Quantifying uncertainty in spikes estimated from calcium imaging data", "comments": "51 pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, a number of methods have been proposed to estimate the times\nat which neurons spike on the basis of calcium imaging data. However,\nquantifying the uncertainty associated with these estimated spikes remains an\nopen problem. We consider a simple and well-studied model for calcium imaging\ndata, which states that calcium decays exponentially in the absence of a spike,\nand instantaneously increases when a spike occurs. We wish to test the null\nhypothesis that the neuron did not spike -- i.e., that there was no increase in\ncalcium -- at a particular timepoint at which a spike was estimated. In this\nsetting, classical hypothesis tests lead to inflated Type I error, because the\nspike was estimated on the same data. To address this problem, we propose a\nselective inference approach to test the null hypothesis. We describe an\nefficient algorithm to compute finite-sample p-values that control selective\nType I error, and confidence intervals with correct selective coverage, for\nspikes estimated using a recent proposal from the literature. We apply our\nproposal in simulation and on calcium imaging data from the spikefinder\nchallenge.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 00:03:56 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chen", "Yiqun T.", ""], ["Jewell", "Sean W.", ""], ["Witten", "Daniela M.", ""]]}, {"id": "2103.07868", "submitter": "Zhuo Qu", "authors": "Zhuo Qu and Marc G. Genton", "title": "Sparse Functional Boxplots for Multivariate Curves", "comments": "34 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces the sparse functional boxplot and the intensity sparse\nfunctional boxplot as practical exploratory tools that make visualization\npossible for both complete and sparse functional data. These visualization\ntools can be used either in the univariate or multivariate functional setting.\nThe sparse functional boxplot, which is based on the functional boxplot,\ndepicts sparseness characteristics in the envelope of the 50\\% central region,\nthe median curve, and the outliers. The proportion of missingness at each time\nindex within the central region is colored in gray. The intensity sparse\nfunctional boxplot displays the relative intensity of sparse points in the\ncentral region, revealing where data are more or less sparse. The two-stage\nfunctional boxplot, a derivation from the functional boxplot to better detect\noutliers, is also extended to its sparse form. Several depth proposals for\nsparse multivariate functional data are evaluated and outlier detection is\ntested in simulations under various data settings and sparseness scenarios. The\npractical applications of the sparse functional boxplot and intensity sparse\nfunctional boxplot are illustrated with two public health datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 08:28:40 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Qu", "Zhuo", ""], ["Genton", "Marc G.", ""]]}, {"id": "2103.07920", "submitter": "Gao Zhigen", "authors": "Gao Zhigen, Yuan Chaofeng, Jing Bingyi, Huang Wei, Guo Jianhua", "title": "A two-way factor model for high-dimensional matrix data", "comments": "35 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this article, we introduce a two-way factor model for a high-dimensional\ndata matrix and study the properties of the maximum likelihood estimation\n(MLE). The proposed model assumes separable effects of row and column\nattributes and captures the correlation across rows and columns with\nlow-dimensional hidden factors. The model inherits the dimension-reduction\nfeature of classical factor models but introduces a new framework with\nseparable row and column factors, representing the covariance or correlation\nstructure in the data matrix. We propose a block alternating, maximizing\nstrategy to compute the MLE of factor loadings as well as other model\nparameters. We discuss model identifiability, obtain consistency and the\nasymptotic distribution for the MLE as the numbers of rows and columns in the\ndata matrix increase. One interesting phenomenon that we learned from our\nanalysis is that the variance of the estimates in the two-way factor model\ndepends on the distance of variances of row factors and column factors in a way\nthat is not expected in classical factor analysis. We further demonstrate the\nperformance of the proposed method through simulation and real data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 13:07:19 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 02:57:30 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhigen", "Gao", ""], ["Chaofeng", "Yuan", ""], ["Bingyi", "Jing", ""], ["Wei", "Huang", ""], ["Jianhua", "Guo", ""]]}, {"id": "2103.08028", "submitter": "Konstantinos Fokianos", "authors": "Konstantinos Fokianos", "title": "Multivariate Count Time Series Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review autoregressive models for the analysis of multivariate count time\nseries. In doing so, we discuss the choice of a suitable distribution for a\nvectors of count random variables. This review focus on three main approaches\ntaken for multivariate count time series analysis: (a) integer autoregressive\nprocesses, (b) parameter-driven models and (c) observation-driven models. The\naim of this work is to highlight some recent methodological developments and\npropose some potentially useful research topics.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 20:40:52 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Fokianos", "Konstantinos", ""]]}, {"id": "2103.08035", "submitter": "Subhadeep Paul", "authors": "Kartik Lovekar, Srijan Sengupta, Subhadeep Paul", "title": "Testing for the Network Small-World Property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have long observed that the \"small-world\" property, which\ncombines the concepts of high transitivity or clustering with a low average\npath length, is ubiquitous for networks obtained from a variety of disciplines\nincluding social sciences, biology, neuroscience, and ecology. However, we find\nthree shortcomings of the currently popular definition and detection methods\nrendering the concept less powerful. First, the classical definition combines\nhigh transitivity with a low average path length in a rather ad-hoc fashion\nwhich confounds the two separate aspects. We find that in several cases,\nnetworks get flagged as \"small world\" by the current methodology solely because\nof their high transitivity. Second, the detection methods lack a formal\nstatistical inference, and third, the comparison is typically performed against\nsimplistic random graph models as the baseline which ignores well-known network\ncharacteristics. We propose three innovations to address these issues. First,\nwe decouple the properties of high transitivity and low average path length as\nseparate events to test for. Second, we define the property as a statistical\ntest between a suitable null model and a superimposed alternative model. Third,\nthe test is performed using parametric bootstrap with several null models to\nallow a wide range of background structures in the network. In addition to the\nbootstrap tests, we also propose an asymptotic test under the\nErd\\\"{o}s-Ren\\'{y}i null model for which we provide theoretical guarantees on\nthe asymptotic level and power. Applying the proposed methods on a large number\nof network datasets, we uncover new insights about their small-world property.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 21:00:12 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Lovekar", "Kartik", ""], ["Sengupta", "Srijan", ""], ["Paul", "Subhadeep", ""]]}, {"id": "2103.08195", "submitter": "Shunsuke Horii", "authors": "Shunsuke Horii", "title": "Bayesian Model Averaging for Causality Estimation and its Approximation\n  based on Gaussian Scale Mixture Distributions", "comments": "Accepted to International Conference on Artificial Intelligence and\n  Statistics (AISTATS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the estimation of the causal effect under linear Structural Causal Models\n(SCMs), it is common practice to first identify the causal structure, estimate\nthe probability distributions, and then calculate the causal effect. However,\nif the goal is to estimate the causal effect, it is not necessary to fix a\nsingle causal structure or probability distributions. In this paper, we first\nshow from a Bayesian perspective that it is Bayes optimal to weight (average)\nthe causal effects estimated under each model rather than estimating the causal\neffect under a fixed single model. This idea is also known as Bayesian model\naveraging. Although the Bayesian model averaging is optimal, as the number of\ncandidate models increases, the weighting calculations become computationally\nhard. We develop an approximation to the Bayes optimal estimator by using\nGaussian scale mixture distributions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 08:07:58 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Horii", "Shunsuke", ""]]}, {"id": "2103.08341", "submitter": "Timothy Wolock", "authors": "Timothy M Wolock, Seth R Flaxman, Kathryn A Risher, Tawanda Dadirai,\n  Simon Gregson, Jeffrey W Eaton", "title": "Evaluating distributional regression strategies for modelling\n  self-reported sexual age-mixing", "comments": "Main text: 25 pages, 7 figures, 5 tables; Appendix: 24 pages, 11\n  figures, 10 tables; Submitted to eLife", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The age dynamics of sexual partnership formation determine patterns of\nsexually transmitted disease transmission and have long been a focus of\nresearchers studying human immunodeficiency virus. Data on self-reported sexual\npartner age distributions are available from a variety of sources. We sought to\nexplore statistical models that accurately predict the distribution of sexual\npartner ages over age and sex. We identified which probability distributions\nand outcome specifications best captured variation in partner age and\nquantified the benefits of modelling these data using distributional\nregression. We found that distributional regression with a sinh-arcsinh\ndistribution replicated observed partner age distributions most accurately\nacross three geographically diverse data sets. This framework can be extended\nwith well-known hierarchical modelling tools and can help improve estimates of\nsexual age-mixing dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 12:31:52 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wolock", "Timothy M", ""], ["Flaxman", "Seth R", ""], ["Risher", "Kathryn A", ""], ["Dadirai", "Tawanda", ""], ["Gregson", "Simon", ""], ["Eaton", "Jeffrey W", ""]]}, {"id": "2103.08390", "submitter": "Vasilis Syrgkanis", "authors": "Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Miruna\n  Oprescu, Vasilis Syrgkanis", "title": "Estimating the Long-Term Effects of Novel Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy makers typically face the problem of wanting to estimate the long-term\neffects of novel treatments, while only having historical data of older\ntreatment options. We assume access to a long-term dataset where only past\ntreatments were administered and a short-term dataset where novel treatments\nhave been administered. We propose a surrogate based approach where we assume\nthat the long-term effect is channeled through a multitude of available\nshort-term proxies. Our work combines three major recent techniques in the\ncausal machine learning literature: surrogate indices, dynamic treatment effect\nestimation and double machine learning, in a unified pipeline. We show that our\nmethod is consistent and provides root-n asymptotically normal estimates under\na Markovian assumption on the data and the observational policy. We use a\ndata-set from a major corporation that includes customer investments over a\nthree year period to create a semi-synthetic data distribution where the major\nqualitative properties of the real dataset are preserved. We evaluate the\nperformance of our method and discuss practical challenges of deploying our\nformal methodology and how to address them.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 13:56:48 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Battocchi", "Keith", ""], ["Dillon", "Eleanor", ""], ["Hei", "Maggie", ""], ["Lewis", "Greg", ""], ["Oprescu", "Miruna", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "2103.08402", "submitter": "Alexander Henzi", "authors": "Alexander Henzi and Johanna F. Ziegel", "title": "Valid sequential inference on probability forecast performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probability forecasts for binary events play a central role in many\napplications. Their quality is commonly assessed with proper scoring rules,\nwhich assign forecasts a numerical score such that a correct forecast achieves\na minimal expected score. In this paper, we construct e-values for testing the\nstatistical significance of score differences of competing forecasts in\nsequential settings. E-values have been proposed as an alternative to p-values\nfor hypothesis testing, and they can easily be transformed into conservative\np-values by taking the multiplicative inverse. The e-values proposed in this\narticle are valid in finite samples without any assumptions on the data\ngenerating processes. They also allow optional stopping, so a forecast user may\ndecide to interrupt evaluation taking into account the available data at any\ntime and still draw statistically valid inference, which is generally not true\nfor classical p-value based tests. In a case study on postprocessing of\nprecipitation forecasts, state-of-the-art forecasts dominance tests and\ne-values lead to the same conclusions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 14:18:03 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 09:22:24 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Henzi", "Alexander", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "2103.08512", "submitter": "Holger Drees", "authors": "Holger Drees, Anja Jan{\\ss}en, Sebastian Neblung", "title": "Cluster based inference for extremes of time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new type of estimator for the spectral tail process of a\nregularly varying time series. The approach is based on a characterizing\ninvariance property of the spectral tail process, which is incorporated into\nthe new estimator via a projection technique. We show uniform asymptotic\nnormality of this estimator, both in the case of known and of unknown index of\nregular variation. In a simulation study the new procedure shows a more stable\nperformance than previously proposed estimators.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:36:25 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Drees", "Holger", ""], ["Jan\u00dfen", "Anja", ""], ["Neblung", "Sebastian", ""]]}, {"id": "2103.08594", "submitter": "Jiaxin Zhang", "authors": "Jiaxin Zhang, Sirui Bi, Guannan Zhang", "title": "A Hybrid Gradient Method to Designing Bayesian Experiments for Implicit\n  Models", "comments": "This short paper has been accepted by NeurIPS 2020 Workshop on\n  Machine Learning and the Physical Sciences. arXiv admin note: substantial\n  text overlap with arXiv:2103.08026", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian experimental design (BED) aims at designing an experiment to\nmaximize the information gathering from the collected data. The optimal design\nis usually achieved by maximizing the mutual information (MI) between the data\nand the model parameters. When the analytical expression of the MI is\nunavailable, e.g., having implicit models with intractable data distributions,\na neural network-based lower bound of the MI was recently proposed and a\ngradient ascent method was used to maximize the lower bound. However, the\napproach in Kleinegesse et al., 2020 requires a pathwise sampling path to\ncompute the gradient of the MI lower bound with respect to the design\nvariables, and such a pathwise sampling path is usually inaccessible for\nimplicit models. In this work, we propose a hybrid gradient approach that\nleverages recent advances in variational MI estimator and evolution strategies\n(ES) combined with black-box stochastic gradient ascent (SGA) to maximize the\nMI lower bound. This allows the design process to be achieved through a unified\nscalable procedure for implicit models without sampling path gradients. Several\nexperiments demonstrate that our approach significantly improves the\nscalability of BED for implicit models in high-dimensional design space.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 21:10:03 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhang", "Jiaxin", ""], ["Bi", "Sirui", ""], ["Zhang", "Guannan", ""]]}, {"id": "2103.08641", "submitter": "Subhankar Dutta", "authors": "Subhankar Dutta and Suchandan Kayal", "title": "Estimation of parameters of the Gumbel type-II distribution under AT-II\n  PHCS with an application of Covid-19 data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate the classical and Bayesian estimation of\nunknown parameters of the Gumbel type-II distribution based on adaptive type-II\nprogressive hybrid censored sample (AT-II PHCS). The maximum likelihood\nestimates (MLEs) and maximum product spacing estimates (MPSEs) are developed\nand computed numerically using Newton-Raphson method. Bayesian approaches are\nemployed to estimate parameters under symmetric and asymmetric loss functions.\nBayesian estimates are not in explicit forms. Thus, Bayesian estimates are\nobtained by using Markov chain Monte Carlo (MCMC) method along with the\nMetropolis-Hastings (MH) algorithm. Based on the normality property of MLEs the\nasymptotic confidence intervals are constructed. Also, bootstrap intervals and\nhighest posterior density (HPD) credible intervals are constructed. Further a\nMonte Carlo simulation study is carried out. Finally, the data set based on the\ndeath rate due to Covid-19 in India is analyzed for illustration of the\npurpose.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 18:28:05 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Dutta", "Subhankar", ""], ["Kayal", "Suchandan", ""]]}, {"id": "2103.08691", "submitter": "Wagner Barreto-Souza", "authors": "Gabriela Oliveira, Wagner Barreto-Souza and Roger W.C. Silva", "title": "Fractional Poisson random sum and its associated normal variance mixture", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the partial sums of independent and identically\ndistributed random variables with the number of terms following a fractional\nPoisson (FP) distribution. The FP sum contains the Poisson and geometric\nsummations as particular cases. We show that the weak limit of the FP\nsummation, when properly normalized, is a mixture between the normal and\nMittag-Leffler distributions, which we call by Normal-Mittag-Leffler (NML) law.\nA parameter estimation procedure for the NML distribution is developed and the\nassociated asymptotic distribution is derived. Simulations are performed to\ncheck the performance of the proposed estimators under finite samples. An\nempirical illustration on the daily log-returns of the Brazilian stock exchange\nindex (IBOVESPA) shows that the NML distribution captures better the tails than\nsome of its competitors. Related problems such as a mixed Poisson\nrepresentation for the FP law and the weak convergence for the\nConway-Maxwell-Poisson random sum are also addressed.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 11:24:38 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 12:52:33 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Oliveira", "Gabriela", ""], ["Barreto-Souza", "Wagner", ""], ["Silva", "Roger W. C.", ""]]}, {"id": "2103.08705", "submitter": "Pamphile Roy", "authors": "Pamphile T. Roy", "title": "Newcomb-Benford's law as a fast ersatz of discrepancy measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Thanks to the increasing availability in computing power, high-dimensional\nengineering problems seem to be at reach. But the curse of dimensionality will\nalways prevent us to try out extensively all the hypotheses. There is a vast\nliterature on efficient methods to construct a Design of Experiments (DoE) such\nas low discrepancy sequences and optimized designs. Classically, the\nperformance of these methods is assessed using a discrepancy metric. Having a\nfast discrepancy measure is of prime importance if ones want to optimize a\ndesign. This work proposes a new methodology to assess the quality of a random\nsampling by using a flavor of Newcomb-Benford's law. The performance of the new\nmetric is compared to classical discrepancy measures and showed to offer\nsimilar information at a fraction of the computational cost of traditional\ndiscrepancy measures.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 20:32:52 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 08:58:43 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Roy", "Pamphile T.", ""]]}, {"id": "2103.08744", "submitter": "Daniel J. Schad", "authors": "Daniel J. Schad, Bruno Nicenboim, Paul-Christian B\\\"urkner, Michael\n  Betancourt, Shravan Vasishth", "title": "Workflow Techniques for the Robust Use of Bayes Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inferences about hypotheses are ubiquitous in the cognitive sciences. Bayes\nfactors provide one general way to compare different hypotheses by their\ncompatibility with the observed data. Those quantifications can then also be\nused to choose between hypotheses. While Bayes factors provide an immediate\napproach to hypothesis testing, they are highly sensitive to details of the\ndata/model assumptions. Moreover it's not clear how straightforwardly this\napproach can be implemented in practice, and in particular how sensitive it is\nto the details of the computational implementation. Here, we investigate these\nquestions for Bayes factor analyses in the cognitive sciences. We explain the\nstatistics underlying Bayes factors as a tool for Bayesian inferences and\ndiscuss that utility functions are needed for principled decisions on\nhypotheses. Next, we study how Bayes factors misbehave under different\nconditions. This includes a study of errors in the estimation of Bayes factors.\nImportantly, it is unknown whether Bayes factor estimates based on bridge\nsampling are unbiased for complex analyses. We are the first to use\nsimulation-based calibration as a tool to test the accuracy of Bayes factor\nestimates. Moreover, we study how stable Bayes factors are against different\nMCMC draws. We moreover study how Bayes factors depend on variation in the\ndata. We also look at variability of decisions based on Bayes factors and how\nto optimize decisions using a utility function. We outline a Bayes factor\nworkflow that researchers can use to study whether Bayes factors are robust for\ntheir individual analysis, and we illustrate this workflow using an example\nfrom the cognitive sciences. We hope that this study will provide a workflow to\ntest the strengths and limitations of Bayes factors as a way to quantify\nevidence in support of scientific hypotheses. Reproducible code is available\nfrom https://osf.io/y354c/.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 22:13:59 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 20:53:01 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Schad", "Daniel J.", ""], ["Nicenboim", "Bruno", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Betancourt", "Michael", ""], ["Vasishth", "Shravan", ""]]}, {"id": "2103.08754", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou, Yuan Ji", "title": "Incorporating External Data into the Analysis of Clinical Trials via\n  Bayesian Additive Regression Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most clinical trials involve the comparison of a new treatment to a control\narm (e.g., the standard of care) and the estimation of a treatment effect.\nExternal data, including historical clinical trial data and real-world\nobservational data, are commonly available for the control arm. Borrowing\ninformation from external data holds the promise of improving the estimation of\nrelevant parameters and increasing the power of detecting a treatment effect if\nit exists. In this paper, we propose to use Bayesian additive regression trees\n(BART) for incorporating external data into the analysis of clinical trials,\nwith a specific goal of estimating the conditional or population average\ntreatment effect. BART naturally adjusts for patient-level covariates and\ncaptures potentially heterogeneous treatment effects across different data\nsources, achieving flexible borrowing. Simulation studies demonstrate that BART\ncompares favorably to a hierarchical linear model and a normal-normal\nhierarchical model. We illustrate the proposed method with an acupuncture\ntrial.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 22:54:17 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhou", "Tianjian", ""], ["Ji", "Yuan", ""]]}, {"id": "2103.08874", "submitter": "Antonio Elias Fernandez", "authors": "Yasser Aleman-Gomez (1), Ana Arribas-Gil (2), Manuel Desco (3 and 4),\n  Antonio Elias-Fernandez (5), Juan Romo (5) ((1) Medical Image Analysis\n  Laboratory, University of Lausanne, Lausanne, Switzerland, (2) Instituto\n  UC3M-Santander de Big Data, Universidad Carlos III de Madrid, Getafe, Spain,\n  (3) Biomedical Imaging and lnstrumentation Group, Hospital General\n  Universitario Gregorio Mara\\~non, Madrid, Spain, (4) Departamento de\n  Bioingenieria e Ingenieria Aeroespacial, Universidad Carlos III de Madrid,\n  Getafe, Spain (5) Departamento de Estadistica, Universidad Carlos III de\n  Madrid, Getafe, Spain)", "title": "Visualizing Outliers in High Dimensional Functional Data for Task fMRI\n  data exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Task-based functional magnetic resonance imaging (task fMRI) is a\nnon-invasive technique that allows identifying brain regions whose activity\nchanges when individuals are asked to perform a given task. This contributes to\nthe understanding of how the human brain is organized in functionally distinct\nsubdivisions. Task fMRI experiments from high-resolution scans provide hundred\nof thousands of longitudinal signals for each individual, corresponding to\nmeasurements of brain activity over each voxel of the brain along the duration\nof the experiment. In this context, we propose some visualization techniques\nfor high dimensional functional data relying on depth-based notions that allow\nfor computationally efficient 2-dim representations of tfMRI data and that shed\nlight on sample composition, outlier presence and individual variability. We\nbelieve that this step is crucial previously to any inferential approach\nwilling to identify neuroscientific patterns across individuals, tasks and\nbrain regions. We illustrate the proposed technique through a simulation study\nand demonstrate its application on a motor and language task fMRI experiment.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 06:49:45 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Aleman-Gomez", "Yasser", "", "3 and 4"], ["Arribas-Gil", "Ana", "", "3 and 4"], ["Desco", "Manuel", "", "3 and 4"], ["Elias-Fernandez", "Antonio", ""], ["Romo", "Juan", ""]]}, {"id": "2103.08895", "submitter": "Dong Xia", "authors": "Jian-Feng Cai and Jingyang Li and Dong Xia", "title": "Generalized Low-rank plus Sparse Tensor Estimation by Fast Riemannian\n  Optimization", "comments": "corrected typos - Mar. 23", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate a generalized framework to estimate a latent low-rank plus\nsparse tensor, where the low-rank tensor often captures the multi-way principal\ncomponents and the sparse tensor accounts for potential model\nmis-specifications or heterogeneous signals that are unexplainable by the\nlow-rank part. The framework is flexible covering both linear and non-linear\nmodels, and can easily handle continuous or categorical variables. We propose a\nfast algorithm by integrating the Riemannian gradient descent and a novel\ngradient pruning procedure. Under suitable conditions, the algorithm converges\nlinearly and can simultaneously estimate both the low-rank and sparse tensors.\nThe statistical error bounds of final estimates are established in terms of the\ngradient of loss function. The error bounds are generally sharp under specific\nstatistical models, e.g., the robust tensor PCA and the community detection in\nhypergraph networks with outlier vertices. Moreover, our method achieves\nnon-trivial error bounds for heavy-tailed tensor PCA whenever the noise has a\nfinite $2+\\varepsilon$ moment. We apply our method to analyze the international\ntrade flow dataset and the statistician hypergraph co-authorship network, both\nyielding new and interesting findings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:38:54 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 06:33:59 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Cai", "Jian-Feng", ""], ["Li", "Jingyang", ""], ["Xia", "Dong", ""]]}, {"id": "2103.08916", "submitter": "Sahana Bhattacharjee Dr.", "authors": "Subrata Chakraborty and Sahana Bhattacharjee", "title": "Modeling proportion of success in high school leaving examination- A\n  comparative study of Inflated Unit Lindley and Inflated Beta distribution", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we first introduced the inflated unit Lindley distribution\nconsidering zero or/and one inflation scenario and studied its basic\ndistributional and structural properties. Both the distributions are shown to\nbe members of exponential family with full rank. Different parameter estimation\nmethods are discussed and supporting simulation studies to check their efficacy\nare also presented. Proportion of students passing the high school leaving\nexamination for the schools across the state of Manipur in India for the year\n2020 are then modeled using the proposed distributions and compared with the\ninflated beta distribution to justify its benefits.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 08:52:59 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Chakraborty", "Subrata", ""], ["Bhattacharjee", "Sahana", ""]]}, {"id": "2103.09017", "submitter": "Torben Sell", "authors": "Jacob Vorstrup Goldman, Torben Sell, and Sumeetpal Sidhu Singh", "title": "Gradient-Based Markov Chain Monte Carlo for Bayesian Inference With\n  Non-Differentiable Priors", "comments": "Accepted for publication by the Journal of the American Statistical\n  Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of non-differentiable priors in Bayesian statistics has become\nincreasingly popular, in particular in Bayesian imaging analysis. Current state\nof the art methods are approximate in the sense that they replace the posterior\nwith a smooth approximation via Moreau-Yosida envelopes, and apply\ngradient-based discretized diffusions to sample from the resulting\ndistribution. We characterize the error of the Moreau-Yosida approximation and\npropose a novel implementation using underdamped Langevin dynamics. In\nmisson-critical cases, however, replacing the posterior with an approximation\nmay not be a viable option. Instead, we show that Piecewise-Deterministic\nMarkov Processes (PDMP) can be utilized for exact posterior inference from\ndistributions satisfying almost everywhere differentiability. Furthermore, in\ncontrast with diffusion-based methods, the suggested PDMP-based samplers place\nno assumptions on the prior shape, nor require access to a computationally\ncheap proximal operator, and consequently have a much broader scope of\napplication. Through detailed numerical examples, including a\nnon-differentiable circular distribution and a non-convex genomics model, we\nelucidate the relative strengths of these sampling methods on problems of\nmoderate to high dimensions, underlining the benefits of PDMP-based methods\nwhen accurate sampling is decisive.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 12:28:26 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Goldman", "Jacob Vorstrup", ""], ["Sell", "Torben", ""], ["Singh", "Sumeetpal Sidhu", ""]]}, {"id": "2103.09175", "submitter": "Ali Eshragh", "authors": "Ali Eshragh, Glen Livingston, Thomas McCarthy McCann, Luke Yerbury", "title": "Rollage: Efficient Rolling Average Algorithm to Estimate ARMA Models for\n  Big Time Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a new method to estimate an ARMA model in the presence of big time\nseries data. Using the concept of a rolling average, we develop a new efficient\nalgorithm, called Rollage, to estimate the order of an AR model and\nsubsequently fit the model. When used in conjunction with an existing\nmethodology, specifically Durbin's algorithm, we show that our proposed method\ncan be used as a criterion to optimally fit ARMA models. Empirical results on\nlarge-scale synthetic time series data support the theoretical results and\nreveal the efficacy of this new approach, especially when compared to existing\nmethodology.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:23:41 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Eshragh", "Ali", ""], ["Livingston", "Glen", ""], ["McCann", "Thomas McCarthy", ""], ["Yerbury", "Luke", ""]]}, {"id": "2103.09303", "submitter": "Trent Lemkus", "authors": "Trent Lemkus and Philip Ramsey and Christopher Gotwalt and Maria Weese", "title": "Self-Validated Ensemble Models for Design of Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the last twenty years, the prediction accuracy of machine learning models\nfit to observational data has improved dramatically. Many machine learning\ntechniques require that the data be partitioned into at least two subsets; a\ntraining set for fitting models and a validation set for tuning models. Machine\nlearning techniques requiring data partitioning have generally not been applied\nto designed experiments (DOEs), as the design structure and small run size\nlimit the ability to withhold observations from the fitting algorithm. We\nintroduce a newmodel-building algorithm, called self-validated ensemble models\n(SVEM), that emulates data partitioning by using the complete data\nsimultaneously as both a training and a validation set. SVEM weights the two\ncopies of the data differently under a weighting scheme based on the\nfractional-random-weight bootstrap (Xu et al., 2020). Similar to bagging\n(Breiman, 1994), this fractional-random-weight bootstrapping scheme is repeated\nmany times and the final SVEM model is the sample average of the bootstrapped\nmodels. In this work, we investigate the performance of the SVEM algorithm with\nregression, Lasso, and the Dantzig Selector. However, the method is very\ngeneral and can be applied in combination with most model selection and fitting\nalgorithms. Through extensive simulations and a case study, we demonstrate that\nSVEM generates models with lower prediction error as compared to more\ntraditional statistical approaches that are based on fitting a single model.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 20:07:34 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 15:07:19 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 17:34:45 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lemkus", "Trent", ""], ["Ramsey", "Philip", ""], ["Gotwalt", "Christopher", ""], ["Weese", "Maria", ""]]}, {"id": "2103.09305", "submitter": "Bernardo Nipoti", "authors": "Riccardo Corradin, Luis Enrique Nieto-Barajas, Bernardo Nipoti", "title": "Optimal stratification of survival data via Bayesian nonparametric\n  mixtures", "comments": "33 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stratified proportional hazards model represents a simple solution to\naccount for heterogeneity within the data while keeping the multiplicative\neffect on the hazard function. Strata are typically defined a priori by\nresorting to the values taken by a categorical covariate. A general framework\nis proposed, which allows for the stratification of a generic accelerated life\ntime model, including as a special case the Weibull proportional hazard model.\nThe stratification is determined a posteriori by taking into account that\nstrata might be characterized by different baseline survivals as well as\ndifferent effects of the predictors. This is achieved by considering a Bayesian\nnonparametric mixture model and the posterior distribution it induces on the\nspace of data partitions. The optimal stratification is then identified by\nmeans of the variation of information criterion and, in turn, stratum-specific\ninference is carried out. The performance of the proposed method and its\nrobustness to the presence of right-censored observations are investigated by\nmeans of an extensive simulation study. A further illustration is provided by\nthe analysis of a data set extracted from the University of Massachusetts AIDS\nResearch Unit IMPACT Study.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 20:26:34 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Corradin", "Riccardo", ""], ["Nieto-Barajas", "Luis Enrique", ""], ["Nipoti", "Bernardo", ""]]}, {"id": "2103.09329", "submitter": "Bingling Wang", "authors": "Bingling Wang, Yinxing Li, Wolfgang Karl H\\\"ardle", "title": "K-expectiles clustering", "comments": "All calculation can be redone via https://github.com/QuantLet/KEC/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  $K$-means clustering is one of the most widely-used partitioning algorithm in\ncluster analysis due to its simplicity and computational efficiency. However,\n$K$-means does not provide an appropriate clustering result when applying to\ndata with non-spherically shaped clusters. We propose a novel partitioning\nclustering algorithm based on expectiles. The cluster centers are defined as\nmultivariate expectiles and clusters are searched via a greedy algorithm by\nminimizing the within cluster '$\\tau$ -variance'. We suggest two schemes: fixed\n$\\tau$ clustering, and adaptive $\\tau$ clustering. Validated by simulation\nresults, this method beats both $K$-means and spectral clustering on data with\nasymmetric shaped clusters, or clusters with a complicated structure, including\nasymmetric normal, beta, skewed $t$ and $F$ distributed clusters. Applications\nof adaptive $\\tau$ clustering on crypto-currency (CC) market data are provided.\nOne finds that the expectiles clusters of CC markets show the phenomena of an\ninstitutional investors dominated market. The second application is on image\nsegmentation. compared to other center based clustering methods, the adaptive\n$\\tau$ cluster centers of pixel data can better capture and describe the\nfeatures of an image. The fixed $\\tau$ clustering brings more flexibility on\nsegmentation with a decent accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 21:14:56 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Wang", "Bingling", ""], ["Li", "Yinxing", ""], ["H\u00e4rdle", "Wolfgang Karl", ""]]}, {"id": "2103.09411", "submitter": "Yuefeng Han", "authors": "Yuefeng Han, Rong Chen, Cun-Hui Zhang and Qiwei Yao", "title": "Simultaneous Decorrelation of Matrix Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a contemporaneous bilinear transformation for matrix time series\nto alleviate the difficulties in modelling and forecasting large number of time\nseries together. More precisely the transformed matrix splits into several\nsmall matrices, and those small matrix series are uncorrelated across all\ntimes. Hence an effective dimension-reduction is achieved by modelling each of\nthose small matrix series separately without the loss of information on the\noverall linear dynamics. We adopt the bilinear transformation such that the\nrows and the columns of the matrix do not mix together, as they typically\nrepresent radically different features. As the targeted transformation is not\nunique, we identify an ideal version through a new normalization, which\nfacilitates the no-cancellation accumulation of the information from different\ntime lags. The non-asymptotic error bounds of the estimated transformation are\nderived, leading to the uniform convergence rates of the estimation. The\nproposed method is illustrated numerically via both simulated and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 02:54:51 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Han", "Yuefeng", ""], ["Chen", "Rong", ""], ["Zhang", "Cun-Hui", ""], ["Yao", "Qiwei", ""]]}, {"id": "2103.09563", "submitter": "Werner M\\\"uller", "authors": "Elham Yousefi and Werner G. M\\\"uller", "title": "Impact of the error structure on the design and analysis of enzyme\n  kinetic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The statistical analysis of enzyme kinetic reactions usually involves models\nof the response functions which are well defined on the basis of\nMichaelis-Menten type equations. The error structure however is often without\ngood reason assumed as additive Gaussian noise. This simple assumption may lead\nto undesired properties of the analysis, particularly when simulations are\ninvolved and consequently negative simulated reaction rates may occur. In this\nstudy we investigate the effect of assuming multiplicative lognormal errors\ninstead. While there is typically little impact on the estimates, the\nexperimental designs and their efficiencies are decisively affected,\nparticularly when it comes to model discrimination problems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 10:59:23 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Yousefi", "Elham", ""], ["M\u00fcller", "Werner G.", ""]]}, {"id": "2103.09619", "submitter": "Keisuke Teramoto", "authors": "Keisuke Teramoto and Kei Hirose", "title": "Sparse multivariate regression with missing values and its application\n  to the prediction of material properties", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of materials science and engineering, statistical analysis and\nmachine learning techniques have recently been used to predict multiple\nmaterial properties from an experimental design. These material properties\ncorrespond to response variables in the multivariate regression model. This\nstudy conducts a penalized maximum likelihood procedure to estimate model\nparameters, including the regression coefficients and covariance matrix of\nresponse variables. In particular, we employ $l_1$-regularization to achieve a\nsparse estimation of regression coefficients and the inverse covariance matrix\nof response variables. In some cases, there may be a relatively large number of\nmissing values in response variables, owing to the difficulty in collecting\ndata on material properties. A method to improve prediction accuracy under the\nsituation with missing values incorporates a correlation structure among the\nresponse variables into the statistical model. The expectation and maximization\nalgorithm is constructed, which enables application to a data set with missing\nvalues in the responses. We apply our proposed procedure to real data\nconsisting of 22 material properties.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 13:09:06 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Teramoto", "Keisuke", ""], ["Hirose", "Kei", ""]]}, {"id": "2103.09647", "submitter": "Suzanne Thornton", "authors": "Suzanne Thornton, Dooti Roy, Stephen Parry, Donna LaLonde, Wendy\n  Martinez, Renee Ellis, David Corliss", "title": "Best Practices for Collecting Gender and Sex Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The measurement and analysis of human sex and gender is a nuanced problem\nwith many overlapping considerations including statistical bias, data privacy,\nand the ethical treatment of study subjects. Traditionally, human gender and\nsex have been categorized and measured with respect to an artificial binary\nsystem. The continuation of this tradition persists mainly because it is easy\nto replication and not, as we argue, because it produces the most valuable\nscientific information. Sex and gender identity data is crucial for many\napplications of statistical analysis and many modern scientists acknowledge the\nlimitations of the current system. However, discrimination against sex and\ngender minorities poses very real privacy concerns when collecting and\ndistributing gender and sex data. As such, extra thoughtfulness and care is\nessential to design safe and informative scientific studies. In this paper, we\npresent statistically informed recommendations for the data collection and\nanalysis of human subjects that not only respect each individual's identity and\nprotect their privacy, but also establish standards for collecting higher\nquality data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 13:42:28 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Thornton", "Suzanne", ""], ["Roy", "Dooti", ""], ["Parry", "Stephen", ""], ["LaLonde", "Donna", ""], ["Martinez", "Wendy", ""], ["Ellis", "Renee", ""], ["Corliss", "David", ""]]}, {"id": "2103.09718", "submitter": "Zhaoxia Yu", "authors": "Dustin Pluta, Xiangmin Xu, Daniel L. Gillen, Zhaoxia Yu", "title": "A Measurement of In-Betweenness and Inference Based on Shape Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a statistical framework to investigate whether a given\nsubpopulation lies between two other subpopulations in a multivariate feature\nspace. This methodology is motivated by a biological question from a\ncollaborator: Is a newly discovered cell type between two known types in\nseveral given features? We propose two in-betweenness indices (IBI) to quantify\nthe in-betweenness exhibited by a random triangle formed by the summary\nstatistics of the three subpopulations. Statistical inference methods are\nprovided for triangle shape and IBI metrics. The application of our methods is\ndemonstrated in three examples: the classic Iris data set, a study of risk of\nrelapse across three breast cancer subtypes, and the motivating neuronal cell\ndata with measured electrophysiological features.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:22:09 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Pluta", "Dustin", ""], ["Xu", "Xiangmin", ""], ["Gillen", "Daniel L.", ""], ["Yu", "Zhaoxia", ""]]}, {"id": "2103.09763", "submitter": "Zhimei Ren", "authors": "Emmanuel J. Cand\\`es, Lihua Lei and Zhimei Ren", "title": "Conformalized Survival Analysis", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing survival analysis techniques heavily rely on strong modelling\nassumptions and are, therefore, prone to model misspecification errors. In this\npaper, we develop an inferential method based on ideas from conformal\nprediction, which can wrap around any survival prediction algorithm to produce\ncalibrated, covariate-dependent lower predictive bounds on survival times. In\nthe Type I right-censoring setting, when the censoring times are completely\nexogenous, the lower predictive bounds have guaranteed coverage in finite\nsamples without any assumptions other than that of operating on independent and\nidentically distributed data points. Under a more general conditionally\nindependent censoring assumption, the bounds satisfy a doubly robust property\nwhich states the following: marginal coverage is approximately guaranteed if\neither the censoring mechanism or the conditional survival function is\nestimated well. Further, we demonstrate that the lower predictive bounds remain\nvalid and informative for other types of censoring. The validity and efficiency\nof our procedure are demonstrated on synthetic data and real COVID-19 data from\nthe UK Biobank.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 16:32:26 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Cand\u00e8s", "Emmanuel J.", ""], ["Lei", "Lihua", ""], ["Ren", "Zhimei", ""]]}, {"id": "2103.09792", "submitter": "Michael Gallaugher Ph.D.", "authors": "Michael P.B. Gallaugher, Salvatore D. Tomarchio, Paul D. McNicholas,\n  and Antonio Punzo", "title": "Multivariate Cluster Weighted Models Using Skewed Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much work has been done in the area of the cluster weighted model (CWM),\nwhich extends the finite mixture of regression model to include modelling of\nthe covariates. Although many types of distributions have been considered for\nboth the response and covariates, to our knowledge skewed distributions have\nnot yet been considered in this paradigm. Herein, a family of 24 novel CWMs are\nconsidered which allows both the covariates and response variables to be\nmodelled using one of four skewed distributions, or the normal distribution.\nParameter estimation is performed using the expectation-maximization algorithm\nand both simulated and real data are used for illustration.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 17:26:03 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["Tomarchio", "Salvatore D.", ""], ["McNicholas", "Paul D.", ""], ["Punzo", "Antonio", ""]]}, {"id": "2103.09805", "submitter": "Ryan Hornby", "authors": "Ryan Hornby and Jingchen Hu", "title": "Bayesian Estimation of Attribute Disclosure Risks in Synthetic Data with\n  the $\\texttt{AttributeRiskCalculation}$ R Package", "comments": "34 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic data is a promising approach to privacy protection in many\ncontexts. A Bayesian synthesis model, also known as a synthesizer, simulates\nsynthetic values of sensitive variables from their posterior predictive\ndistributions. The resulting synthetic data can then be released in place of\nthe confidential data. An important evaluation prior to synthetic data release\nis its level of privacy protection, which is often in the form of disclosure\nrisks evaluation. Attribute disclosure, referring to an intruder correctly\ninferring the confidential values of synthetic records, is one type of\ndisclosure that is challenging to be computationally evaluated. In this paper,\nwe review and discuss in detail some Bayesian estimation approaches to\nattribute disclosure risks evaluation, with examples of commonly-used Bayesian\nsynthesizers. We create the $\\texttt{AttributeRiskCalculation}$ R package to\nfacilitate its implementation, and demonstrate its functionality with examples\nof evaluating attribute disclosure risks in synthetic samples of the Consumer\nExpenditure Surveys.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 17:43:38 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Hornby", "Ryan", ""], ["Hu", "Jingchen", ""]]}, {"id": "2103.09872", "submitter": "Yves Rozenholc", "authors": "Dorota Desaulle, C\\'eline Hoffmann, Bernard Hainque and Yves Rozenholc", "title": "Differential analysis in Transcriptomic: The strength of randomly\n  picking 'reference' genes", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcriptomic analysis are characterized by being not directly quantitative\nand only providing relative measurements of expression levels up to an unknown\nindividual scaling factor. This difficulty is enhanced for differential\nexpression analysis. Several methods have been proposed to circumvent this lack\nof knowledge by estimating the unknown individual scaling factors however, even\nthe most used one, are suffering from being built on hardly justifiable\nbiological hypotheses or from having weak statistical background. Only two\nmethods withstand this analysis: one based on largest connected graph component\nhardly usable for large amount of expressions like in NGS, the second based on\n$\\log$-linear fits which unfortunately require a first step which uses one of\nthe methods described before.\n  We introduce a new procedure for differential analysis in the context of\ntranscriptomic data. It is the result of pooling together several differential\nanalyses each based on randomly picked genes used as reference genes. It\nprovides a differential analysis free from the estimation of the individual\nscaling factors or any other knowledge. Theoretical properties are investigated\nboth in term of FWER and power. Moreover in the context of Poisson or negative\nbinomial modelization of the transcriptomic expressions, we derived a test with\nnon asymptotic control of its bounds. We complete our study by some empirical\nsimulations and apply our procedure to a real data set of hepatic miRNA\nexpressions from a mouse model of non-alcoholic steatohepatitis (NASH), the\nCDAHFD model. This study on real data provides new hits with good biological\nexplanations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 19:18:44 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 14:14:19 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 01:57:15 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Desaulle", "Dorota", ""], ["Hoffmann", "C\u00e9line", ""], ["Hainque", "Bernard", ""], ["Rozenholc", "Yves", ""]]}, {"id": "2103.09929", "submitter": "Aaron Osgood-Zimmerman", "authors": "Aaron Osgood-Zimmerman, Jon Wakefield", "title": "A Statistical Introduction to Template Model Builder: A Flexible Tool\n  for Spatial Modeling", "comments": "84 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The integrated nested Laplace approximation (INLA) is a well-known and\npopular technique for spatial modeling with a user-friendly interface in the\nR-INLA package. Unfortunately, only a certain class of latent Gaussian models\nare amenable to fitting with INLA. In this paper we describe Template Model\nBuilder (TMB), an existing technique which is well-suited to fitting complex\nspatio-temporal models. TMB is relatively unknown to the spatial statistics\ncommunity, but is a highly flexible random effects modeling tool which allows\nusers to define complex random effects models through simple C++ templates.\nAfter contrasting the methodology behind TMB with INLA, we provide a\nlarge-scale simulation study assessing and comparing R-INLA and TMB for\ncontinuous spatial models, fitted via the Stochastic Partial Differential\nEquations (SPDE) approximation. The results show that the predictive fields\nfrom both methods are comparable in most situations even though TMB estimates\nfor fixed or random effects may have slightly larger bias than R-INLA. We also\npresent a smaller discrete spatial simulation study, in which both approaches\nperform well. We conclude with an analysis of breast cancer incidence and\nmortality data using a joint model which cannot be fit with INLA.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 21:55:37 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Osgood-Zimmerman", "Aaron", ""], ["Wakefield", "Jon", ""]]}, {"id": "2103.10058", "submitter": "Rolf Larsson", "authors": "Rolf Larsson", "title": "Bartlett correction of an independence test in a multivariate Poisson\n  model", "comments": "66 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a system of dependent Poisson variables, where each variable is\nthe sum of an independent variate and a common variate. It is the common\nvariate that creates the dependence. Within this system, a test of independence\nmay be constructed where the null hypothesis is that the common variate is\nidentically zero. In the present paper, we consider the maximum log likelihood\nratio test. For this test, it is well-known that the asymptotic distribution of\nthe test statistic is an equal mixture of zero and a chi square distribution\nwith one degree of freedom. We examine a Bartlett correction of this test, in\nthe hope that we will get better approximation of the nominal size for\nmoderately large sample sizes. This correction is explicitly derived, and its\nusefulness is explored in a simulation study. For practical purposes, the\ncorrection is found to be useful in dimension two, but not in higher\ndimensions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 07:39:26 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Larsson", "Rolf", ""]]}, {"id": "2103.10077", "submitter": "Tomas Masak", "authors": "Tomas Masak, Tomas Rubin, Victor Panaretos", "title": "Inference and Computation for Sparsely Sampled Random Surfaces", "comments": "34 pages, 9 figures, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-parametric inference for functional data over two-dimensional domains\nentails additional computational and statistical challenges, compared to the\none-dimensional case. Separability of the covariance is commonly assumed to\naddress these issues in the densely observed regime. Instead, we consider the\nsparse regime, where the latent surfaces are observed only at few irregular\nlocations with additive measurement error, and propose an estimator of\ncovariance based on local linear smoothers. Consequently, the assumption of\nseparability reduces the intrinsically four-dimensional smoothing problem into\nseveral two-dimensional smoothers and allows the proposed estimator to retain\nthe classical minimax-optimal convergence rate for two-dimensional smoothers.\nEven when separability fails to hold, imposing it can be still advantageous as\na form of regularization. A simulation study reveals a favorable bias-variance\ntrade-off and massive speed-ups achieved by our approach. Finally, the proposed\nmethodology is used for qualitative analysis of implied volatility surfaces\ncorresponding to call options, and for prediction of the latent surfaces based\non information from the entire data set, allowing for uncertainty\nquantification. Our cross-validated out-of-sample quantitative results show\nthat the proposed methodology outperforms the common approach of pre-smoothing\nevery implied volatility surface separately.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 08:20:47 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Masak", "Tomas", ""], ["Rubin", "Tomas", ""], ["Panaretos", "Victor", ""]]}, {"id": "2103.10153", "submitter": "Jonathan Schmidt", "authors": "Jonathan Schmidt, Nicholas Kr\\\"amer, Philipp Hennig", "title": "A Probabilistic State Space Model for Joint Inference from Differential\n  Equations and Data", "comments": "12 pages (+ 5 pages appendix), 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanistic models with differential equations are a key component of\nscientific applications of machine learning. Inference in such models is\nusually computationally demanding, because it involves repeatedly solving the\ndifferential equation. The main problem here is that the numerical solver is\nhard to combine with standard inference techniques. Recent work in\nprobabilistic numerics has developed a new class of solvers for ordinary\ndifferential equations (ODEs) that phrase the solution process directly in\nterms of Bayesian filtering. We here show that this allows such methods to be\ncombined very directly, with conceptual and numerical ease, with latent force\nmodels in the ODE itself. It then becomes possible to perform approximate\nBayesian inference on the latent force as well as the ODE solution in a single,\nlinear complexity pass of an extended Kalman filter / smoother - that is, at\nthe cost of computing a single ODE solution. We demonstrate the expressiveness\nand performance of the algorithm by training, among others, a non-parametric\nSIRD model on data from the COVID-19 outbreak.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:36:09 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:08:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Schmidt", "Jonathan", ""], ["Kr\u00e4mer", "Nicholas", ""], ["Hennig", "Philipp", ""]]}, {"id": "2103.10182", "submitter": "Matthew Holden", "authors": "Matthew Holden, Marcelo Pereyra, Konstantinos C. Zygalakis", "title": "Bayesian Imaging With Data-Driven Priors Encoded by Neural Networks:\n  Theory, Methods, and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new methodology for performing Bayesian inference in\nimaging inverse problems where the prior knowledge is available in the form of\ntraining data. Following the manifold hypothesis and adopting a generative\nmodelling approach, we construct a data-driven prior that is supported on a\nsub-manifold of the ambient space, which we can learn from the training data by\nusing a variational autoencoder or a generative adversarial network. We\nestablish the existence and well-posedness of the associated posterior\ndistribution and posterior moments under easily verifiable conditions,\nproviding a rigorous underpinning for Bayesian estimators and uncertainty\nquantification analyses. Bayesian computation is performed by using a parallel\ntempered version of the preconditioned Crank-Nicolson algorithm on the\nmanifold, which is shown to be ergodic and robust to the non-convex nature of\nthese data-driven models. In addition to point estimators and uncertainty\nquantification analyses, we derive a model misspecification test to\nautomatically detect situations where the data-driven prior is unreliable, and\nexplain how to identify the dimension of the latent space directly from the\ntraining data. The proposed approach is illustrated with a range of experiments\nwith the MNIST dataset, where it outperforms alternative image reconstruction\napproaches from the state of the art. A model accuracy analysis suggests that\nthe Bayesian probabilities reported by the data-driven models are also\nremarkably accurate under a frequentist definition of probability.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 11:34:08 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Holden", "Matthew", ""], ["Pereyra", "Marcelo", ""], ["Zygalakis", "Konstantinos C.", ""]]}, {"id": "2103.10231", "submitter": "Yujie Zhao", "authors": "Yujie Zhao, Xiaoming Huo, Yajun Mei", "title": "Identification of Underlying Dynamic System from Noisy Data with Splines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a two-stage method called Spline Assisted Partial\nDifferential Equation involved Model Identification (SAPDEMI) to efficiently\nidentify the underlying partial differential equation (PDE) models from the\nnoisy data. In the first stage -- functional estimation stage -- we employ the\ncubic spline to estimate the unobservable derivatives, which serve as\ncandidates included the underlying PDE models. The contribution of this stage\nis that, it is computational efficient because it only requires the\ncomputational complexity of the linear polynomial of the sample size, which\nachieves the lowest possible order of complexity. In the second stage -- model\nidentification stage -- we apply Least Absolute Shrinkage and Selection\nOperator (Lasso) to identify the underlying PDE models. The contribution of\nthis stage is that, we focus on the model selections, while the existing\nliterature mostly focuses on parameter estimations. Moreover, we develop\nstatistical properties of our method for correct identification, where the main\ntool we use is the primal-dual witness (PDW) method. Finally, we validate our\ntheory through various numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 13:08:04 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zhao", "Yujie", ""], ["Huo", "Xiaoming", ""], ["Mei", "Yajun", ""]]}, {"id": "2103.10333", "submitter": "Lorenzo Schiavon", "authors": "Lorenzo Schiavon, Antonio Canale, David B. Dunson", "title": "Generalized infinite factorization models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Factorization models express a statistical object of interest in terms of a\ncollection of simpler objects. For example, a matrix or tensor can be expressed\nas a sum of rank-one components. However, in practice, it can be challenging to\ninfer the relative impact of the different components as well as the number of\ncomponents. A popular idea is to include infinitely many components having\nimpact decreasing with the component index. This article is motivated by two\nlimitations of existing methods: (1) lack of careful consideration of the\nwithin component sparsity structure; and (2) no accommodation for grouped\nvariables and other non-exchangeable structures. We propose a general class of\ninfinite factorization models that address these limitations. Theoretical\nsupport is provided, practical gains are shown in simulation studies, and an\necology application focusing on modelling bird species occurrence is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 15:49:06 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Schiavon", "Lorenzo", ""], ["Canale", "Antonio", ""], ["Dunson", "David B.", ""]]}, {"id": "2103.10365", "submitter": "Andr\\'e Gillibert", "authors": "Andr\\'e Gillibert (1 and 2), Jacques B\\'enichou (2 and 3), Bruno\n  Falissard (1) ((1) INSERM UMR 1178, Universit\\'e Paris Sud, Maison de Solenn,\n  Paris, France (2) Department of Biostatistics and Clinical Research, CHU\n  Rouen, Rouen, France (3) Inserm U 1181, Normandie University, Rouen, France)", "title": "Best estimator for bivariate Poisson regression", "comments": "25 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  INTRODUCTION: Wald's, the likelihood ratio (LR) and Rao's score tests and\ntheir corresponding confidence intervals (CIs), are the three most common\nestimators of parameters of Generalized Linear Models. On finite samples, these\nestimators are biased. The objective of this work is to analyze the coverage\nerrors of the CI estimators in small samples for the log-Poisson model (i.e.\nestimation of incidence rate ratio) with innovative evaluation criteria, taking\nin account the overestimation/underestimation unbalance of coverage errors and\nthe variable inclusion rate and follow-up in epidemiological studies.\n  METHODS: Exact calculations equivalent to Monte Carlo simulations with an\ninfinite number of simulations have been used. Underestimation errors (due to\nthe upper bound of the CI) and overestimation coverage errors (due to the lower\nbound of the CI) have been split. The level of confidence has been analyzed\nfrom $0.95$ to $1-10^{-6}$, allowing the interpretation of P-values below\n$10^{-6}$ for hypothesis tests.\n  RESULTS: The LR bias was small (actual coverage errors less than 1.5 times\nthe nominal errors) when the expected number of events in both groups was above\n1, even when unbalanced (e.g. 10 events in one group vs 1 in the other). For\n95% CI, Wald's and the Score estimators showed high bias even when the number\nof events was large ($\\geq 20$ in both groups) when groups were unbalanced. For\nsmall P-values ($<10^{-6}$), the LR kept acceptable bias while Wald's and the\nscore P-values had severely inflated errors ($\\times 100$).\n  CONCLUSION: The LR test and LR CI should be used.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:36:53 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Gillibert", "Andr\u00e9", "", "1 and 2"], ["B\u00e9nichou", "Jacques", "", "2 and 3"], ["Falissard", "Bruno", ""]]}, {"id": "2103.10433", "submitter": "Catherine Matias", "authors": "Vincent Miele (LBBE), Catherine Matias (LPSM (UMR\\_8001)), Marc\n  Ohlmann (LECA, LAMA), Giovanni Poggiato (LECA, LJK), St\\'ephane Dray,\n  Wilfried Thuiller (LECA)", "title": "Quantifying the overall effect of biotic interactions on species\n  communities along environmental gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separating environmental effects from those of biotic interactions on species\ndistributions has always been a central objective of ecology. Despite years of\neffort in analysing patterns of species co-occurrences and communities and the\ndevelopments of sophisticated tools, we are still unable to address this major\nobjective. A key reason is that the wealth of ecological knowledge is not\nsufficiently harnessed in current statistical models, notably the knowledge on\nbiotic interactions. Here, we develop ELGRIN, the first model that combines\nsimultaneously knowledge on species interactions (i.e. metanetwork),\nenvironmental data and species occurrences to tease apart the relative effects\nof abiotic factors and overall biotic interactions on species distributions.\nInstead of focusing on single effects of pair-wise interactions, which have\nlittle sense in complex communities, ELGRIN contrasts the overall effects of\nbiotic interactions to those of the environment. Using simulated and empirical\ndata, we demonstrate the suitability of ELGRIN to address the objectives for\nvarious types of interactions like mutualism, competition and trophic\ninteractions. Data on ecological networks are everyday increasing and we\nbelieve the time is ripe to mobilize these data to better understand\nbiodiversity patterns. We believe that ELGRIN will provide the unique\nopportunity to unravel how biotic interactions truly influence species\ndistributions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:41:26 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Miele", "Vincent", "", "LBBE"], ["Matias", "Catherine", "", "LPSM"], ["Ohlmann", "Marc", "", "LECA, LAMA"], ["Poggiato", "Giovanni", "", "LECA, LJK"], ["Dray", "St\u00e9phane", "", "LECA"], ["Thuiller", "Wilfried", "", "LECA"]]}, {"id": "2103.10475", "submitter": "Mohammad Ramadan", "authors": "Mohammad S. Ramadan, Robert R. Bitmead", "title": "Maximum Likelihood Recursive State Estimation using the Expectation\n  Maximization Algorithm", "comments": "12 pages, 5 figures, 5 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Maximum Likelihood recursive state estimator is derived for non-linear and\nnon-Gaussian state-space models. The estimator combines a particle filter to\ngenerate the conditional density and the Expectation Maximization algorithm to\ncompute the maximum likelihood state estimate iteratively. Algorithms for\nmaximum likelihood state filtering, prediction and smoothing are presented. The\nconvergence properties of these algorithms, which are inherited from the\nExpectation Maximization algorithm, are proven and examined in two examples. It\nis shown that, with randomized reinitialization, which is feasible because of\nthe algorithm simplicity, these methods are able to converge to the Maximum\nLikelihood Estimate (MLE) of multimodal, truncated and skewed densities, as\nwell as those of disjoint support.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:48:07 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Ramadan", "Mohammad S.", ""], ["Bitmead", "Robert R.", ""]]}, {"id": "2103.10522", "submitter": "Aki Vehtari", "authors": "Teemu S\\\"ailynoja, Paul-Christian B\\\"urkner, Aki Vehtari", "title": "Graphical Test for Discrete Uniformity and its Applications in Goodness\n  of Fit Evaluation and Multiple Sample Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing goodness of fit to a given distribution plays an important role in\ncomputational statistics. The Probability integral transformation (PIT) can be\nused to convert the question of whether a given sample originates from a\nreference distribution into a problem of testing for uniformity. We present new\nsimulation and optimization based methods to obtain simultaneous confidence\nbands for the whole empirical cumulative distribution function (ECDF) of the\nPIT values under the assumption of uniformity. Simultaneous confidence bands\ncorrespond to such confidence intervals at each point that jointly satisfy a\ndesired coverage. These methods can also be applied in cases where the\nreference distribution is represented only by a finite sample. The confidence\nbands provide an intuitive ECDF-based graphical test for uniformity, which also\nprovides useful information on the quality of the discrepancy. We further\nextend the simulation and optimization methods to determine simultaneous\nconfidence bands for testing whether multiple samples come from the same\nunderlying distribution. This multiple sample comparison test is especially\nuseful in Markov chain Monte Carlo convergence diagnostics. We provide\nnumerical experiments to assess the properties of the tests using both\nsimulated and real world data and give recommendations on their practical\napplication in computational statistics workflows.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 21:00:56 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["S\u00e4ilynoja", "Teemu", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Vehtari", "Aki", ""]]}, {"id": "2103.10537", "submitter": "Keaven Anderson PhD", "authors": "Keaven M. Anderson, Zifang Guo, Jing Zhao, Linda Z. Sun", "title": "A unified framework for weighted parametric group sequential design\n  (WPGSD)", "comments": "24 pages (37 with appendices); 2 figures (+ 2 in appendices);\n  submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Group sequential design (GSD) is widely used in clinical trials in which\ncorrelated tests of multiple hypotheses are used. Multiple primary objectives\nresulting in tests with known correlations include evaluating 1) multiple\nexperimental treatment arms, 2) multiple populations, 3) the combination of\nmultiple arms and multiple populations, or 4) any asymptotically multivariate\nnormal tests. In this paper, we focus on the first 3 of these and extend the\nframework of the weighted parametric multiple test procedure from fixed designs\nwith a single analysis per objective to a GSD setting where different\nobjectives may be assessed at the same or different times, each in a group\nsequential fashion. Pragmatic methods for design and analysis of weighted\nparametric group sequential design(WPGSD) under closed testing procedures are\nproposed to maintain the strong control of familywise Type I error rate (FWER)\nwhen correlations between tests are incorporated. This results in the ability\nto relax testing bounds compared to designs not fully adjusting for known\ncorrelations, increasing power or allowing decreased sample size. We illustrate\nthe proposed methods using clinical trial examples and conduct a simulation\nstudy to evaluate the operating characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 21:46:23 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Anderson", "Keaven M.", ""], ["Guo", "Zifang", ""], ["Zhao", "Jing", ""], ["Sun", "Linda Z.", ""]]}, {"id": "2103.10540", "submitter": "Daniel Bonn\\'ery", "authors": "Daniel Bonnery, Francesco Pantalone and M. Giovanna Ranalli", "title": "The effect of Informative Selection on the estimation of parameters\n  related to Spatial Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the concept of informative selection, population\ndistribution and sample distribution to a spatial process context. These\nnotions were first defined in a context where the output of the random process\nof interest consists of independent and identically distributed realisations\nfor each individual of a population. It has been showed that informative\nselection was inducing a stochastic dependence among realisations on the\nselected units. In the context of spatial processes, the \"population\" is a\ncontinuous space and realisations for two different elements of the population\nare not independent. We show how informative selection may induce a different\ndependence among selected units and how the sample distribution differs from\nthe population distribution.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 21:49:44 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Bonnery", "Daniel", ""], ["Pantalone", "Francesco", ""], ["Ranalli", "M. Giovanna", ""]]}, {"id": "2103.10563", "submitter": "Srijata Samanta", "authors": "Srijata Samanta and Joseph Antonelli", "title": "Estimation and false discovery control for the analysis of environmental\n  mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of environmental mixtures is of growing importance in\nenvironmental epidemiology, and one of the key goals in such analyses is to\nidentify exposures and their interactions that are associated with adverse\nhealth outcomes. Typical approaches utilize flexible regression models combined\nwith variable selection to identify important exposures and estimate a\npotentially nonlinear relationship with the outcome of interest. Despite this\nsurge in interest, no approaches to date can identify exposures and\ninteractions while controlling any form of error rates with respect to exposure\nselection. We propose two novel approaches to estimating the health effects of\nenvironmental mixtures that simultaneously 1) Estimate and provide valid\ninference for the overall mixture effect, and 2) identify important exposures\nand interactions while controlling the false discovery rate. We show that this\ncan lead to substantial power gains to detect weak effects of environmental\nexposures. We apply our approaches to a study of persistent organic pollutants\nand find that our approach is able to identify more interactions than existing\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 23:31:20 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Samanta", "Srijata", ""], ["Antonelli", "Joseph", ""]]}, {"id": "2103.10568", "submitter": "Holger Dette", "authors": "Eftychia Solea and Holger Dette", "title": "Nonparametric and high-dimensional functional graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing nonparametric undirected graphical\nmodels for high-dimensional functional data. Most existing statistical methods\nin this context assume either a Gaussian distribution on the vertices or linear\nconditional means. In this article we provide a more flexible model which\nrelaxes the linearity assumption by replacing it by an arbitrary additive form.\nThe use of functional principal components offers an estimation strategy that\nuses a group lasso penalty to estimate the relevant edges of the graph. We\nestablish statistical guarantees for the resulting estimators, which can be\nused to prove consistency if the dimension and the number of functional\nprincipal components diverge to infinity with the sample size. We also\ninvestigate the empirical performance of our method through simulation studies\nand a real data application.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 23:41:01 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Solea", "Eftychia", ""], ["Dette", "Holger", ""]]}, {"id": "2103.10606", "submitter": "Zejian Liu", "authors": "Meng Li, Zejian Liu, Cheng-Han Yu, Marina Vannucci", "title": "Semiparametric Bayesian Inference for Local Extrema of Functions in the\n  Presence of Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a wide range of applications where the local extrema of a function\nare the key quantity of interest. However, there is surprisingly little work on\nmethods to infer local extrema with uncertainty quantification in the presence\nof noise. By viewing the function as an infinite-dimensional nuisance\nparameter, a semiparametric formulation of this problem poses daunting\nchallenges, both methodologically and theoretically, as (i) the number of local\nextrema may be unknown, and (ii) the induced shape constraints associated with\nlocal extrema are highly irregular. In this article, we address these\nchallenges by suggesting an encompassing strategy that eliminates the need to\nspecify the number of local extrema, which leads to a remarkably simple, fast\nsemiparametric Bayesian approach for inference on local extrema. We provide\nclosed-form characterization of the posterior distribution and study its large\nsample behaviors under this encompassing regime. We show a multi-modal\nBernstein-von Mises phenomenon in which the posterior measure converges to a\nmixture of Gaussians with the number of components matching the underlying\ntruth, leading to posterior exploration that accounts for multi-modality. We\nillustrate the method through simulations and a real data application to\nevent-related potential analysis.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 02:58:07 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Li", "Meng", ""], ["Liu", "Zejian", ""], ["Yu", "Cheng-Han", ""], ["Vannucci", "Marina", ""]]}, {"id": "2103.10613", "submitter": "Jiaqi Li", "authors": "Jiaqi Li and Liya Fu", "title": "Robust penalized empirical likelihood in high dimensional longitudinal\n  data analysis", "comments": "25 pages, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an effective nonparametric method, empirical likelihood (EL) is appealing\nin combining estimating equations flexibly and adaptively for incorporating\ndata information. To select important variables and estimating equations in the\nsparse high-dimensional model, we consider a penalized EL method based on\nrobust estimating functions by applying two penalty functions for regularizing\nthe regression parameters and the associated Lagrange multipliers\nsimultaneously, which allows the dimensionalities of both regression parameters\nand estimating equations to grow exponentially with the sample size. A first\ninspection on the robustness of estimating equations contributing to the\nestimating equations selection and variable selection is discussed from both\ntheoretical perspective and intuitive simulation results in this paper. The\nproposed method can improve the robustness and effectiveness when the data have\nunderlying outliers or heavy tails in the response variables and/or covariates.\nThe robustness of the estimator is measured via the bounded influence function,\nand the oracle properties are also established under some regularity\nconditions. Extensive simulation studies and a yeast cell data are used to\nevaluate the performance of the proposed method. The numerical results reveal\nthat the robustness of sparse estimating equations selection fundamentally\nenhances variable selection accuracy when the data have heavy tails and/or\ninclude underlying outliers.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 03:26:33 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 02:56:08 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Li", "Jiaqi", ""], ["Fu", "Liya", ""]]}, {"id": "2103.10640", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen and Daniel Fryer and Geoffrey McLachlan", "title": "Order selection with confidence for finite mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determination of the number of mixture components (the order) of a finite\nmixture model has been an enduring problem in statistical inference. We prove\nthat the closed testing principle leads to a sequential testing procedure (STP)\nthat allows for confidence statements to be made regarding the order of a\nfinite mixture model. We construct finite sample tests, via data splitting and\ndata swapping, for use in the STP, and we prove that such tests are consistent\nagainst fixed alternatives. Simulation studies are conducted to demonstrate the\nperformance of the finite sample tests-based STP, yielding practical\nrecommendations, and extensions to the STP are considered. In particular, we\ndemonstrate that a modification of the STP yields a method that consistently\nselects the order of a finite mixture model, in the asymptotic sense. Our STP\nnot only applicable for order selection of finite mixture models, but is also\nuseful for making confidence statements regarding any sequence of nested\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 05:32:52 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Nguyen", "Hien D", ""], ["Fryer", "Daniel", ""], ["McLachlan", "Geoffrey", ""]]}, {"id": "2103.10739", "submitter": "Veronique Maume-Deschamps", "authors": "Manaf Ahmed, V\\'eronique Maume-Deschamps (ICJ, PSPM), Pierre Ribereau\n  (ICJ, PSPM)", "title": "Recognizing a Spatial Extreme dependence structure: A Deep Learning\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the behaviour of environmental extreme events is crucial for\nevaluating economic losses, assessing risks, health care and many other\naspects. In the spatial context, relevant for environmental events, the\ndependence structure plays a central rule, as it influence joined extreme\nevents and extrapolation on them. So that, recognising or at least having\npreliminary informations on patterns of these dependence structures is a\nvaluable knowledge for understanding extreme events. In this study, we address\nthe question of automatic recognition of spatial Asymptotic Dependence (AD)\nversus Asymptotic independence (AI), using Convolutional Neural Network (CNN).\nWe have designed an architecture of Convolutional Neural Network to be an\nefficient classifier of the dependence structure. Upper and lower tail\ndependence measures are used to train the CNN. We have tested our methodology\non simulated and real data sets: air temperature data at two meter over Iraq\nland and Rainfall data in the east cost of Australia.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 11:22:55 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Ahmed", "Manaf", "", "ICJ, PSPM"], ["Maume-Deschamps", "V\u00e9ronique", "", "ICJ, PSPM"], ["Ribereau", "Pierre", "", "ICJ, PSPM"]]}, {"id": "2103.10875", "submitter": "Timoth\\'ee Stumpf-F\\'etizon", "authors": "Omiros Papaspiliopoulos, Timoth\\'ee Stumpf-F\\'etizon, Giacomo Zanella", "title": "Scalable computation for Bayesian hierarchical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article is about algorithms for learning Bayesian hierarchical models,\nthe computational complexity of which scales linearly with the number of\nobservations and the number of parameters in the model. It focuses on crossed\nrandom effect and nested multilevel models, which are used ubiquitously in\napplied sciences, and illustrates the methodology on two challenging real data\nanalyses on predicting electoral results and real estate prices respectively.\nThe posterior dependence in both classes is sparse: in crossed random effects\nmodels it resembles a random graph, whereas in nested multilevel models it is\ntree-structured. For each class we develop a framework for scalable computation\nbased on collapsed Gibbs sampling and belief propagation respectively. We\nprovide a number of negative (for crossed) and positive (for nested) results\nfor the scalability (or lack thereof) of methods based on sparse linear\nalgebra, which are relevant also to Laplace approximation methods for such\nmodels. Our numerical experiments compare with off-the-shelf variational\napproximations and Hamiltonian Monte Carlo. Our theoretical results, although\npartial, are useful in suggesting interesting methodologies and lead to\nconclusions that our numerics suggest to hold well beyond the scope of the\nunderlying assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 16:01:36 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Papaspiliopoulos", "Omiros", ""], ["Stumpf-F\u00e9tizon", "Timoth\u00e9e", ""], ["Zanella", "Giacomo", ""]]}, {"id": "2103.10943", "submitter": "Sylvain Le Corff", "authors": "Achille Thin (CMAP), Yazid Janati (IP Paris, TIPIC-SAMOVAR, CITI),\n  Sylvain Le Corff (IP Paris, TIPIC-SAMOVAR, CITI), Charles Ollion (CMAP),\n  Arnaud Doucet, Alain Durmus (CMLA), Eric Moulines (CMAP), Christian Robert\n  (CEREMADE)", "title": "Invertible Flow Non Equilibrium sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneously sampling from a complex distribution with intractable\nnormalizing constant and approximating expectations under this distribution is\na notoriously challenging problem. We introduce a novel scheme, Invertible Flow\nNon Equilibrium Sampling (InFine), which departs from classical Sequential\nMonte Carlo (SMC) and Markov chain Monte Carlo (MCMC) approaches. InFine\nconstructs unbiased estimators of expectations and in particular of normalizing\nconstants by combining the orbits of a deterministic transform started from\nrandom initializations.When this transform is chosen as an appropriate\nintegrator of a conformal Hamiltonian system, these orbits are optimization\npaths. InFine is also naturally suited to design new MCMC sampling schemes by\nselecting samples on the optimization paths.Additionally, InFine can be used to\nconstruct an Evidence Lower Bound (ELBO) leading to a new class of Variational\nAutoEncoders (VAE).\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:09:06 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Thin", "Achille", "", "CMAP"], ["Janati", "Yazid", "", "IP Paris, TIPIC-SAMOVAR, CITI"], ["Corff", "Sylvain Le", "", "IP Paris, TIPIC-SAMOVAR, CITI"], ["Ollion", "Charles", "", "CMAP"], ["Doucet", "Arnaud", "", "CMLA"], ["Durmus", "Alain", "", "CMLA"], ["Moulines", "Eric", "", "CMAP"], ["Robert", "Christian", "", "CEREMADE"]]}, {"id": "2103.11066", "submitter": "Stefan Wager", "authors": "Hao Sun, Shuyang Du, Stefan Wager", "title": "Treatment Allocation under Uncertain Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning how to optimally allocate treatments\nwhose cost is uncertain and can vary with pre-treatment covariates. This\nsetting may arise in medicine if we need to prioritize access to a scarce\nresource that different patients would use for different amounts of time, or in\nmarketing if we want to target discounts whose cost to the company depends on\nhow much the discounts are used. Here, we derive the form of the optimal\ntreatment allocation rule under budget constraints, and propose a practical\nrandom forest based method for learning a treatment rule using data from a\nrandomized trial or, more broadly, unconfounded data. Our approach leverages a\nstatistical connection between our problem and that of learning heterogeneous\ntreatment effects under endogeneity using an instrumental variable. We find our\nmethod to exhibit promising empirical performance both in simulations and in a\nmarketing application.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 00:36:28 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sun", "Hao", ""], ["Du", "Shuyang", ""], ["Wager", "Stefan", ""]]}, {"id": "2103.11085", "submitter": "Jichun Xie", "authors": "Xuechan Li and Anthony Sung and Jichun Xie", "title": "Distance Assisted Recursive Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In many applications, a large number of features are collected with the goal\nto identify a few important ones. Sometimes, these features lie in a metric\nspace with a known distance matrix, which partially reflects their\nco-importance pattern. Proper use of the distance matrix will boost the power\nof identifying important features. Hence, we develop a new multiple testing\nframework named the Distance Assisted Recursive Testing (DART). DART has two\nstages. In stage 1, we transform the distance matrix into an aggregation tree,\nwhere each node represents a set of features. In stage 2, based on the\naggregation tree, we set up dynamic node hypotheses and perform multiple\ntesting on the tree. All rejections are mapped back to the features. Under mild\nassumptions, the false discovery proportion of DART converges to the desired\nlevel in high probability converging to one. We illustrate by theory and\nsimulations that DART has superior performance under various models compared to\nthe existing methods. We applied DART to a clinical trial in the allogeneic\nstem cell transplantation study to identify the gut microbiota whose abundance\nwill be impacted by the after-transplant care.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 03:25:18 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Li", "Xuechan", ""], ["Sung", "Anthony", ""], ["Xie", "Jichun", ""]]}, {"id": "2103.11128", "submitter": "Shanika Wickramasuriya", "authors": "Shanika L Wickramasuriya", "title": "Probabilistic forecast reconciliation under the Gaussian framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecast reconciliation of multivariate time series is the process of mapping\na set of incoherent forecasts into coherent forecasts to satisfy a given set of\nlinear constraints. Commonly used projection matrix based approaches for point\nforecast reconciliation are OLS (ordinary least squares), WLS (weighted least\nsquares), and MinT (minimum trace). Even though point forecast reconciliation\nis a well-established field of research, the literature on generating\nprobabilistic forecasts subject to linear constraints is somewhat limited.\nAvailable methods follow a two-step procedure. Firstly, it draws future sample\npaths from the univariate models fitted to each series in the collection (which\nare incoherent). Secondly, it uses a projection matrix based approach or\nempirical copula based reordering approach to account for contemporaneous\ncorrelations and linear constraints. The projection matrices are estimated\neither by optimizing a scoring rule such as energy or variogram score, or\nsimply using a projection matrix derived for point forecast reconciliation.\n  This paper proves that (a) if the incoherent predictive distribution is\nGaussian then MinT minimizes the logarithmic scoring rule; and (b) the\nlogarithmic score of MinT for each marginal predictive density is smaller than\nthat of OLS. We show these theoretical results using a set of simulation\nstudies. We also evaluate them using the Australian domestic tourism data set.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 07:57:25 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wickramasuriya", "Shanika L", ""]]}, {"id": "2103.11129", "submitter": "Shanika Wickramasuriya", "authors": "Shanika L Wickramasuriya", "title": "Properties of point forecast reconciliation approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point forecast reconciliation of collection of time series with linear\naggregation constraints has evolved substantially over the last decade. A few\ncommonly used methods are GLS (generalized least squares), OLS (ordinary least\nsquares), WLS (weighted least squares), and MinT (minimum trace). GLS and MinT\nhave similar mathematical expressions, but they differ by the covariance matrix\nused. OLS and WLS can be considered as special cases of MinT where they differ\nby the assumptions made about the structure of the covariance matrix. All these\nmethods ensure that the reconciled forecasts are unbiased, provided that the\nbase forecasts are unbiased. The ERM (empirical risk minimizer) approach was\nproposed to relax the assumption of unbiasedness.\n  This paper proves that (a) GLS and MinT reduce to the same solution; (b) on\naverage, a method similar to ERM (which we refer to as MinT-U) can produce\nbetter forecasts than MinT (lowest total mean squared error) which is then\nfollowed by OLS and then by base; and (c) the mean squared error of each series\nin the structure for MinT-U is smaller than that for MinT which is then\nfollowed by that for either OLS or base forecasts. We show these theoretical\nresults using a set of simulation studies. We also evaluate them using the\nAustralian domestic tourism data set.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 07:58:18 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wickramasuriya", "Shanika L", ""]]}, {"id": "2103.11170", "submitter": "Rebecca Anthopolos", "authors": "Rebecca Anthopolos and Ying Wei and Qixuan Chen", "title": "Modeling Heterogeneity and Missing Data of Multiple Longitudinal\n  Outcomes in Electronic Health Records", "comments": "Main text: 15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In electronic health records (EHRs), latent subgroups of patients may exhibit\ndistinctive patterning in their longitudinal health trajectories. For such\ndata, growth mixture models (GMMs) enable classifying patients into different\nlatent classes based on individual trajectories and hypothesized risk factors.\nHowever, the application of GMMs is hindered by the special missing data\nproblem in EHRs, which manifests two patient-led missing data processes: the\nvisit process and the response process for an EHR variable conditional on a\npatient visiting the clinic. If either process is associated with the process\ngenerating the longitudinal outcomes, then valid inferences require accounting\nfor a nonignorable missing data mechanism. We propose a Bayesian shared\nparameter model that links GMMs of multiple longitudinal health outcomes, the\nvisit process, and the response process of each outcome given a visit using a\ndiscrete latent class variable. Our focus is on multiple longitudinal health\noutcomes for which there can be a clinically prescribed visit schedule. We\ndemonstrate our model in EHR measurements on early childhood weight and height\nz-scores. Using data simulations, we illustrate the statistical properties of\nour method with respect to subgroup-specific or marginal inferences. We built\nthe R package EHRMiss for model fitting, selection, and checking.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 12:54:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Anthopolos", "Rebecca", ""], ["Wei", "Ying", ""], ["Chen", "Qixuan", ""]]}, {"id": "2103.11175", "submitter": "Patrick Schwab", "authors": "Sonali Parbhoo, Stefan Bauer, Patrick Schwab", "title": "NCoRE: Neural Counterfactual Representation Learning for Combinations of\n  Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating an individual's potential response to interventions from\nobservational data is of high practical relevance for many domains, such as\nhealthcare, public policy or economics. In this setting, it is often the case\nthat combinations of interventions may be applied simultaneously, for example,\nmultiple prescriptions in healthcare or different fiscal and monetary measures\nin economics. However, existing methods for counterfactual inference are\nlimited to settings in which actions are not used simultaneously. Here, we\npresent Neural Counterfactual Relation Estimation (NCoRE), a new method for\nlearning counterfactual representations in the combination treatment setting\nthat explicitly models cross-treatment interactions. NCoRE is based on a novel\nbranched conditional neural representation that includes learnt treatment\ninteraction modulators to infer the potential causal generative process\nunderlying the combination of multiple treatments. Our experiments show that\nNCoRE significantly outperforms existing state-of-the-art methods for\ncounterfactual treatment effect estimation that do not account for the effects\nof combining multiple treatments across several synthetic, semi-synthetic and\nreal-world benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 13:25:00 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Parbhoo", "Sonali", ""], ["Bauer", "Stefan", ""], ["Schwab", "Patrick", ""]]}, {"id": "2103.11309", "submitter": "Jason Whyte PhD", "authors": "Jason M. Whyte", "title": "Branching out into Structural Identifiability Analysis with Maple:\n  Interactive Exploration of Uncontrolled Linear Time-Invariant Structures", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": "10.1007/978-3-030-81698-8_27", "report-no": null, "categories": "eess.SY cs.SY stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we wish to predict the behaviour of a physical system. We may choose\nto represent the system by model structure $S$ (a set of related mathematical\nmodels defined by parametric relationships between system variables), and a\nparameter set $\\Theta$. Each parameter vector in $\\Theta$ is associated with a\ncompletely specified model in $S$. We use $S$ with system observations in\nestimating the \"true\" (unknown) parameter vector. Inconveniently, multiple\nparameter vectors may cause $S$ to approximate the data equally well. If we\ncannot distinguish between such alternatives, and these lead to dissimilar\npredictions, we cannot confidently use $S$ in decision making. This result may\nrender efforts in data collection and modelling fruitless. This outcome occurs\nwhen $S$ lacks the property of structural global identifiability (SGI).\nFortunately, we can test various classes of structures for SGI prior to data\ncollection. A non-SGI result may guide changes to our structure or experimental\ndesign towards obtaining a better outcome. We aim to assist the testing of\nstructures for SGI through bespoke Maple 2020 procedures. We consider\ncontinuous-time, uncontrolled, linear time-invariant state-space structures.\nHere, the time evolution of the state-variable vector ${\\bf x}$ is modelled by\na system of constant-coefficient, ordinary differential equations. We utilise\nthe \"transfer function\" approach, which is also applicable to the\n\"compartmental\" subclass (mass is conserved). Our use of Maple's \"Explore\"\nenables an interactive consideration of a parent structure and its variants,\nobtained as the user changes which components of ${\\bf x}$ are observed, or\nhave non-zero initial conditions. Such changes may influence the information\ncontent of the idealised output available for the SGI test, and hence, its\nresult. Our approach may inform the interactive analysis of structures from\nother classes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 05:30:02 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 01:25:20 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Whyte", "Jason M.", ""]]}, {"id": "2103.11327", "submitter": "Jelena Bradic", "authors": "Jelena Bradic and Yinchu Zhu", "title": "Comments on Leo Breiman's paper 'Statistical Modeling: The Two Cultures'\n  (Statistical Science, 2001, 16(3), 199-231)", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breiman challenged statisticians to think more broadly, to step into the\nunknown, model-free learning world, with him paving the way forward. Statistics\ncommunity responded with slight optimism, some skepticism, and plenty of\ndisbelief. Today, we are at the same crossroad anew. Faced with the enormous\npractical success of model-free, deep, and machine learning, we are naturally\ninclined to think that everything is resolved. A new frontier has emerged; the\none where the role, impact, or stability of the {\\it learning} algorithms is no\nlonger measured by prediction quality, but an inferential one -- asking the\nquestions of {\\it why} and {\\it if} can no longer be safely ignored.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 07:40:37 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Bradic", "Jelena", ""], ["Zhu", "Yinchu", ""]]}, {"id": "2103.11357", "submitter": "Andreas Holzinger", "authors": "Andr\\'e M. Carrington, Douglas G. Manuel, Paul W. Fieguth, Tim Ramsay,\n  Venet Osmani, Bernhard Wernly, Carol Bennett, Steven Hawken, Matthew McInnes,\n  Olivia Magwood, Yusuf Sheikh, Andreas Holzinger", "title": "Deep ROC Analysis and AUC as Balanced Average Accuracy to Improve Model\n  Selection, Understanding and Interpretation", "comments": "14 pages, 6 Figures, submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence (TPAMI), currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Optimal performance is critical for decision-making tasks from medicine to\nautonomous driving, however common performance measures may be too general or\ntoo specific. For binary classifiers, diagnostic tests or prognosis at a\ntimepoint, measures such as the area under the receiver operating\ncharacteristic curve, or the area under the precision recall curve, are too\ngeneral because they include unrealistic decision thresholds. On the other\nhand, measures such as accuracy, sensitivity or the F1 score are measures at a\nsingle threshold that reflect an individual single probability or predicted\nrisk, rather than a range of individuals or risk. We propose a method in\nbetween, deep ROC analysis, that examines groups of probabilities or predicted\nrisks for more insightful analysis. We translate esoteric measures into\nfamiliar terms: AUC and the normalized concordant partial AUC are balanced\naverage accuracy (a new finding); the normalized partial AUC is average\nsensitivity; and the normalized horizontal partial AUC is average specificity.\nAlong with post-test measures, we provide a method that can improve model\nselection in some cases and provide interpretation and assurance for patients\nin each risk group. We demonstrate deep ROC analysis in two case studies and\nprovide a toolkit in Python.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 10:27:35 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Carrington", "Andr\u00e9 M.", ""], ["Manuel", "Douglas G.", ""], ["Fieguth", "Paul W.", ""], ["Ramsay", "Tim", ""], ["Osmani", "Venet", ""], ["Wernly", "Bernhard", ""], ["Bennett", "Carol", ""], ["Hawken", "Steven", ""], ["McInnes", "Matthew", ""], ["Magwood", "Olivia", ""], ["Sheikh", "Yusuf", ""], ["Holzinger", "Andreas", ""]]}, {"id": "2103.11493", "submitter": "Yiou Li", "authors": "Yiou Li, Xinwei Deng", "title": "On Efficient Design of Pilot Experiment for Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The experimental design for a generalized linear model (GLM) is important but\nchallenging since the design criterion often depends on model specification\nincluding the link function, the linear predictor, and the unknown regression\ncoefficients. Prior to constructing locally or globally optimal designs, a\npilot experiment is usually conducted to provide some insights on the model\nspecifications. In pilot experiments, little information on the model\nspecification of GLM is available. Surprisingly, there is very limited research\non the design of pilot experiments for GLMs. In this work, we obtain some\ntheoretical understanding of the design efficiency in pilot experiments for\nGLMs. Guided by the theory, we propose to adopt a low-discrepancy design with\nrespect to some target distribution for pilot experiments. The performance of\nthe proposed design is assessed through several numerical examples.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 21:42:29 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Li", "Yiou", ""], ["Deng", "Xinwei", ""]]}, {"id": "2103.11539", "submitter": "ShengLi Tzeng", "authors": "Heng-Hui Lue, ShengLi Tzeng", "title": "Interpretable, predictive spatio-temporal models via enhanced Pairwise\n  Directions Estimation", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article concerns the predictive modeling for spatio-temporal data as\nwell as model interpretation using data information in space and time.\nIntrinsically, we develop a novel approach based on dimension reduction for\nsuch data in order to capture nonlinear mean structures without requiring a\nprespecified parametric model. In addition to prediction as a common interest,\nthis approach focuses more on the exploration of geometric information in the\ndata. The method of Pairwise Directions Estimation (PDE) is incorporated in our\napproach to implement the data-driven function searching of spatial structures\nand temporal patterns, useful in exploring data trends. The benefit of using\ngeometrical information from the method of PDE is highlighted. We further\nenhance PDE, referring to it as PDE+, by using resolution adaptive fixed rank\nkriging to estimate the random effects not explained in the mean structures.\nOur proposal can not only produce more accurate and explainable prediction, but\nalso increase the computation efficiency for model building. Several simulation\nexamples are conducted and comparisons are made with four existing methods. The\nresults demonstrate that the proposed PDE+ method is very useful for exploring\nand interpreting the patterns of trend for spatio-temporal data. Illustrative\napplications to two real datasets are also presented.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 02:00:10 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lue", "Heng-Hui", ""], ["Tzeng", "ShengLi", ""]]}, {"id": "2103.11567", "submitter": "Xinyi Zhang", "authors": "Xinyi Zhang, Qiang Sun and Dehan Kong", "title": "Supervised Principal Component Regression for Functional Response with\n  High Dimensional Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a supervised principal component regression method for relating\nfunctional responses with high dimensional covariates. Unlike the conventional\nprincipal component analysis, the proposed method builds on a newly defined\nexpected integrated residual sum of squares, which directly makes use of the\nassociation between functional response and predictors. Minimizing the\nintegrated residual sum of squares gives the supervised principal components,\nwhich is equivalent to solving a sequence of nonconvex generalized Rayleigh\nquotient optimization problems and thus is computationally intractable. To\novercome this computational challenge, we reformulate the nonconvex\noptimization problems into a simultaneous linear regression, with a sparse\npenalty added to deal with high dimensional predictors. Theoretically, we show\nthat the reformulated regression problem recovers the same supervised principal\nsubspace under suitable conditions. Statistically, we establish non-asymptotic\nerror bounds for the proposed estimators. Numerical studies and an application\nto the Human Connectome Project lend further support.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 03:40:51 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Xinyi", ""], ["Sun", "Qiang", ""], ["Kong", "Dehan", ""]]}, {"id": "2103.11666", "submitter": "Alessandro Colombi", "authors": "Laura Codazzi, Alessandro Colombi, Matteo Gianella, Raffaele Argiento,\n  Lucia Paci, Alessia Pini", "title": "Gaussian graphical modeling for spectrometric data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the analysis of spectrometric data, we introduce a Gaussian\ngraphical model for learning the dependence structure among frequency bands of\nthe infrared absorbance spectrum. The spectra are modeled as continuous\nfunctional data through a B-spline basis expansion and a Gaussian graphical\nmodel is assumed as a prior specification for the smoothing coefficients to\ninduce sparsity in their precision matrix. Bayesian inference is carried out to\nsimultaneously smooth the curves and to estimate the conditional independence\nstructure between portions of the functional domain. The proposed model is\napplied to the analysis of infrared absorbance spectra of strawberry purees.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 08:41:58 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 09:58:03 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Codazzi", "Laura", ""], ["Colombi", "Alessandro", ""], ["Gianella", "Matteo", ""], ["Argiento", "Raffaele", ""], ["Paci", "Lucia", ""], ["Pini", "Alessia", ""]]}, {"id": "2103.11805", "submitter": "Martin Wendler", "authors": "Lea Wegner, Martin Wendler", "title": "Block Length Choice for the Bootstrap of Dependent Panel Data -- a\n  Comment on Choi and Shin (2020)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choi and Shin (2020) have constructed a bootstrap-based test for\nchange-points in panels with temporal and and/or cross-sectional dependence.\nThey have compared their test to several other proposed tests. We demonstrate\nthat by an appropriate, data-adaptive choice of the block length, the\nchange-point test by Sharipov, Tewes, Wendler (2016) can at least cope with\nmild temporal dependence, the size distortion of this test is not as severe as\nclaimed by Choi and Shin (2020).\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:07:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wegner", "Lea", ""], ["Wendler", "Martin", ""]]}, {"id": "2103.12094", "submitter": "Harry Spearing", "authors": "Harry Spearing, Jonathan Tawn, David Irons, Tim Paulden", "title": "Modelling intransitivity in pairwise comparisons with application to\n  baseball data", "comments": "26 pages, 7 figures, 2 tables in the main text. 17 pages in the\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In most commonly used ranking systems, some level of underlying transitivity\nis assumed. If transitivity exists in a system then information about pairwise\ncomparisons can be translated to other linked pairs. For example, if typically\nA beats B and B beats C, this could inform us about the expected outcome\nbetween A and C. We show that in the seminal Bradley-Terry model knowing the\nprobabilities of A beating B and B beating C completely defines the probability\nof A beating C, with these probabilities determined by individual skill levels\nof A, B and C. Users of this model tend not to investigate the validity of this\ntransitive assumption, nor that some skill levels may not be statistically\nsignificantly different from each other; the latter leading to false\nconclusions about rankings. We provide a novel extension to the Bradley-Terry\nmodel, which accounts for both of these features: the intransitive\nrelationships between pairs of objects are dealt with through interaction terms\nthat are specific to each pair; and by partitioning the $n$ skills into\n$A+1\\leq n$ distinct clusters, any differences in the objects' skills become\nsignificant, given appropriate $A$. With $n$ competitors there are $n(n-1)/2$\ninteractions, so even in multiple round robin competitions this gives too many\nparameters to efficiently estimate. Therefore we separately cluster the\n$n(n-1)/2$ values of intransitivity into $K$ clusters, giving $(A,K)$\nestimatable values respectively, typically with $A+K<n$. Using a Bayesian\nhierarchical model, $(A,K)$ are treated as unknown, and inference is conducted\nvia a reversible jump Markov chain Monte Carlo (RJMCMC) algorithm. The model is\nshown to have an improved fit out of sample in both simulated data and when\napplied to American League baseball data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 18:00:19 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Spearing", "Harry", ""], ["Tawn", "Jonathan", ""], ["Irons", "David", ""], ["Paulden", "Tim", ""]]}, {"id": "2103.12170", "submitter": "John Hughes", "authors": "John Hughes", "title": "krippendorffsalpha: An R Package for Measuring Agreement Using\n  Krippendorff's Alpha Coefficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  R package krippendorffsalpha provides tools for measuring agreement using\nKrippendorff's Alpha coefficient, a well-known nonparametric measure of\nagreement (also called inter-rater reliability and various other names). This\narticle first develops Krippendorff's Alpha in a natural way, and situates\nAlpha among statistical procedures. Then the usage of package\nkrippendorffsalpha is illustrated via analyses of two datasets, the latter of\nwhich was collected during an imaging study of hip cartilage. The package\npermits users to apply the Alpha methodology using built-in distance functions\nfor the nominal, ordinal, interval, or ratio levels of measurement.\nUser-defined distance functions are also supported. The fitting function can\naccommodate any number of units, any number of coders, and missingness.\nBootstrap inference is supported, and the bootstrap computation can be carried\nout in parallel.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 20:36:25 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Hughes", "John", ""]]}, {"id": "2103.12176", "submitter": "Jack Prothero", "authors": "Jack B. Prothero, Jan Hannig, J.S. Marron", "title": "New Perspectives on Centering", "comments": "28 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data matrix centering is an ever-present yet under-examined aspect of data\nanalysis. Functional data analysis (FDA) often operates with a default of\ncentering such that the vectors in one dimension have mean zero. We find that\ncentering along the other dimension identifies a novel useful mode of variation\nbeyond those familiar in FDA. We explore ambiguities in both matrix orientation\nand nomenclature. Differences between centerings and their potential\ninteraction can be easily misunderstood. We propose a unified framework and new\nterminology for centering operations. We clearly demonstrate the intuition\nbehind and consequences of each centering choice with informative graphics. We\nalso propose a new direction energy hypothesis test as part of a series of\ndiagnostics for determining which choice of centering is best for a data set.\nWe explore the application of these diagnostics in several FDA settings.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 20:53:13 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Prothero", "Jack B.", ""], ["Hannig", "Jan", ""], ["Marron", "J. S.", ""]]}, {"id": "2103.12206", "submitter": "Andrew Ying", "authors": "Andrew Ying, Eric J. Tchetgen Tchetgen", "title": "A New Causal Approach to Account for Treatment Switching in Randomized\n  Experiments under a Structural Cumulative Survival Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treatment switching in a randomized controlled trial is said to occur when a\npatient randomized to one treatment arm switches to another treatment arm\nduring follow-up. This can occur at the point of disease progression, whereby\npatients in the control arm may be offered the experimental treatment. It is\nwidely known that failure to account for treatment switching can seriously\ndilute the estimated effect of treatment on overall survival. In this paper, we\naim to account for the potential impact of treatment switching in a re-analysis\nevaluating the treatment effect of NucleosideReverse Transcriptase Inhibitors\n(NRTIs) on a safety outcome (time to first severe or worse sign or symptom) in\nparticipants receiving a new antiretroviral regimen that either included or\nomitted NRTIs in the Optimized Treatment That Includes or OmitsNRTIs (OPTIONS)\ntrial. We propose an estimator of a treatment causal effect under a structural\ncumulative survival model (SCSM) that leverages randomization as an\ninstrumental variable to account for selective treatment switching. Unlike\nRobins' accelerated failure time model often used to address treatment\nswitching, the proposed approach avoids the need for artificial censoring for\nestimation. We establish that the proposed estimator is uniformly consistent\nand asymptotically Gaussian under standard regularity conditions. A consistent\nvariance estimator is also given and a simple resampling approach provides\nuniform confidence bands for the causal difference comparing treatment groups\novertime on the cumulative intensity scale. We develop an R package named\n\"ivsacim\" implementing all proposed methods, freely available to download from\nR CRAN. We examine the finite performance of the estimator via extensive\nsimulations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:26:12 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ying", "Andrew", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "2103.12214", "submitter": "Rayleigh Lei", "authors": "Rayleigh Lei, Long Nguyen", "title": "Modeling Random Directions in 2D Simplex Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose models and algorithms for learning about random directions in\ntwo-dimensional simplex data, and apply our methods to the study of income\nlevel proportions and their changes over time in a geostatistical area. There\nare several notable challenges in the analysis of simplex-valued data: the\nmeasurements must respect the simplex constraint and the changes exhibit\nspatiotemporal smoothness while allowing for possible heterogeneous behaviors.\nTo that end, we propose Bayesian models that rely on and expand upon building\nblocks in circular and spatial statistics by exploiting suitable transformation\nbased on the polar coordinates for circular data. Our models also account for\nspatial correlation across locations in the simplex and the heterogeneous\npatterns via mixture modeling. We describe some properties of the models and\nmodel fitting via MCMC techniques. Our models and methods are illustrated via a\nthorough simulation study, and applied to an analysis of movements and trends\nof income categories using the Home Mortgage Disclosure Act data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:39:44 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Lei", "Rayleigh", ""], ["Nguyen", "Long", ""]]}, {"id": "2103.12409", "submitter": "Michiel Paus", "authors": "Michiel H.J. Paus, Edwin R. van den Heuvel, Marc J.M. Meddens", "title": "Binary disease prediction using tail quantiles of the distribution of\n  continuous biomarkers", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the analysis of binary disease classification, single biomarkers might not\nhave significant discriminating power and multiple biomarkers from a large set\nof biomarkers should be selected. Numerous approaches exist, but they merely\nwork well for mean differences in biomarkers between cases and controls.\nBiological processes are however much more heterogeneous, and differences could\nalso occur in other distributional characteristics (e.g. variances, skewness).\nMany machine learning techniques are better capable of utilizing these higher\norder distributional differences, sometimes at cost of explainability.\n  In this study we propose quantile based prediction (QBP), a binary\nclassification method that is based on the selection of multiple continuous\nbiomarkers. QBP generates a single score using the tails of the biomarker\ndistributions for cases and controls. This single score can then be evaluated\nby ROC analysis to investigate its predictive power.\n  The performance of QBP is compared to supervised learning methods using\nextensive simulation studies, and two case studies: major depression disorder\nand trisomy. Simultaneously, the classification performance of the existing\ntechniques in relation to each other is assessed. The key strengths of QBP are\nthe opportunity to select relevant biomarkers and the outstanding\nclassification performance in the case biomarkers predominantly show variance\ndifferences between cases and controls. When only shifts in means were present\nin the biomarkers, QBP obtained an inferior performance. Lastly, QBP proved to\nbe unbiased in case of absence of disease relevant biomarkers and outperformed\nthe other methods on the MDD case study.\n  More research is needed to further optimize QBP, since it has several\nopportunities to improve its performance. Here we wanted to introduce the\nprinciple of QBP and show its potential.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 09:20:10 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Paus", "Michiel H. J.", ""], ["Heuvel", "Edwin R. van den", ""], ["Meddens", "Marc J. M.", ""]]}, {"id": "2103.12802", "submitter": "Ankit Kumar", "authors": "Ankit Kumar, Sharmodeep Bhattacharyya, Kristofer Bouchard", "title": "Numerical Characterization of Support Recovery in Sparse Regression with\n  Correlated Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse regression is frequently employed in diverse scientific settings as a\nfeature selection method. A pervasive aspect of scientific data that hampers\nboth feature selection and estimation is the presence of strong correlations\nbetween predictive features. These fundamental issues are often not appreciated\nby practitioners, and jeapordize conclusions drawn from estimated models. On\nthe other hand, theoretical results on sparsity-inducing regularized regression\nsuch as the Lasso have largely addressed conditions for selection consistency\nvia asymptotics, and disregard the problem of model selection, whereby\nregularization parameters are chosen. In this numerical study, we address these\nissues through exhaustive characterization of the performance of several\nregression estimators, coupled with a range of model selection strategies.\nThese estimators and selection criteria were examined across correlated\nregression problems with varying degrees of signal to noise, distribution of\nthe non-zero model coefficients, and model sparsity. Our results reveal a\nfundamental tradeoff between false positive and false negative control in all\nregression estimators and model selection criteria examined. Additionally, we\nare able to numerically explore a transition point modulated by the\nsignal-to-noise ratio and spectral properties of the design covariance matrix\nat which the selection accuracy of all considered algorithms degrades. Overall,\nwe find that SCAD coupled with BIC or empirical Bayes model selection performs\nthe best feature selection across the regression problems considered.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 19:13:26 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Kumar", "Ankit", ""], ["Bhattacharyya", "Sharmodeep", ""], ["Bouchard", "Kristofer", ""]]}, {"id": "2103.12831", "submitter": "Joshua Loyal", "authors": "Joshua Daniel Loyal, Yuguo Chen", "title": "An Eigenmodel for Dynamic Multilayer Networks", "comments": "55 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic multilayer networks frequently represent the structure of multiple\nco-evolving relations; however, statistical models are not well-developed for\nthis prevalent network type. Here, we propose a new latent space model for\ndynamic multilayer networks. The key feature of our model is its ability to\nidentify common time-varying structures shared by all layers while also\naccounting for layer-wise variation and degree heterogeneity. We establish the\nidentifiability of the model's parameters and develop a structured mean-field\nvariational inference approach to estimate the model's posterior, which scales\nto networks previously intractable to dynamic latent space models. We\ndemonstrate the estimation procedure's accuracy and scalability on simulated\nnetworks. We apply the model to two real-world problems: discerning regional\nconflicts in a data set of international relations and quantifying infectious\ndisease spread throughout a school based on the student's daily contact\npatterns.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 20:50:16 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Loyal", "Joshua Daniel", ""], ["Chen", "Yuguo", ""]]}, {"id": "2103.12900", "submitter": "Sotiris Prevenas", "authors": "Sotiris Prevenas, Rachel McCrea, Luca Rossini, Cristiano Villa", "title": "Loss based prior for the degrees of freedom of the Wishart distribution", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel method to deal with Vector Autoregressive\nmodels, when the Normal-Wishart prior is considered. In particular, we depart\nfrom the current approach of setting $\\nu=m+1$ by setting a loss-based prior on\n$\\nu$. Doing so, we have been able to exploit any information about $\\nu$ in\nthe data and achieve better predictive performances than the method currently\nused in the literature. We show how this works both on simulated and real data\nsets where, in the latter case, we used data of macroeconometric fashion as\nwell as viral data. In addition, we show the reason why we believe we achieve a\nbetter performance by showing that the data appears to suggest a value of $\\nu$\nfar from the canonical $m+1$ value.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 00:27:05 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Prevenas", "Sotiris", ""], ["McCrea", "Rachel", ""], ["Rossini", "Luca", ""], ["Villa", "Cristiano", ""]]}, {"id": "2103.12903", "submitter": "Lili Tong", "authors": "Lili Tong, Piaomu Liu and Edsel Pena", "title": "Joint Stochastic Modeling and Statistical Analysis of Recurrent\n  Competing Risks, Longitudinal Marker, and Health Status", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consider a subject or unit being monitored over a period of random duration\nin a longitudinal time-to-event study in a biomedical, public health, or\nengineering setting. As time moves forward, this unit experiences recurrent\nevents of several types and a longitudinal marker transitions over a discrete\nstate-space. In addition, its \"health\" status also transitions over a discrete\nstate-space containing at least one absorbing state. A vector of covariates\nwill also be associated with this unit. Of major interest for this unit is the\ntime-to-absorption of its health status process, which represents this unit's\nlifetime. Aside from being affected by the covariate vector, there is a synergy\namong the recurrent competing risks processes, the longitudinal marker process,\nand the health status process in the sense that the time-evolution of each\nprocess is affected by the other processes. To exploit this synergy in order to\nobtain more realistic models and enhance inferential performance, a joint\nstochastic model for these components is proposed and the proper statistical\ninference methods for this model are developed. This joint model has the\npotential of facilitating precision interventions, thereby enhancing precision\nor personalized medicine. A stochastic process approach, using counting\nprocesses and continuous-time Markov chains, is utilized, which allows for\nmodeling the dynamicity arising from the synergy among the model components and\nthe impact of performed interventions after event occurrences and the\nincreasing number of event occurrences. Likelihood-based inferential methods\nare developed based on observing a sample of these units. Properties of the\ninferential procedures are examined through simulations and illustrated using\nsome real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 00:47:19 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Tong", "Lili", ""], ["Liu", "Piaomu", ""], ["Pena", "Edsel", ""]]}, {"id": "2103.12946", "submitter": "Linquan Ma", "authors": "Linquan Ma, Lan Liu, Wei Yang", "title": "Envelope Methods with Ignorable Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Envelope method was recently proposed as a method to reduce the dimension of\nresponses in multivariate regressions. However, when there exists missing data,\nthe envelope method using the complete case observations may lead to biased and\ninefficient results. In this paper, we generalize the envelope estimation when\nthe predictors and/or the responses are missing at random. Specifically, we\nincorporate the envelope structure in the expectation-maximization (EM)\nalgorithm. As the parameters under the envelope method are not pointwise\nidentifiable, the EM algorithm for the envelope method was not straightforward\nand requires a special decomposition. Our method is guaranteed to be more\nefficient, or at least as efficient as, the standard EM algorithm. Moreover,\nour method has the potential to outperform the full data MLE. We give\nasymptotic properties of our method under both normal and non-normal cases. The\nefficiency gain over the standard EM is confirmed in simulation studies and in\nan application to the Chronic Renal Insufficiency Cohort (CRIC) study.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 02:48:01 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ma", "Linquan", ""], ["Liu", "Lan", ""], ["Yang", "Wei", ""]]}, {"id": "2103.12967", "submitter": "Yusi Fang", "authors": "Yusi Fang, George C. Tseng, Chung Chang", "title": "Heavy-tailed distribution for combining dependent $p$-values with\n  asymptotic robustness", "comments": "34 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of combining individual $p$-values to aggregate multiple small\neffects is prevalent in many scientific investigations and is a long-standing\nstatistical topic. Many classical methods are designed for combining\nindependent and frequent signals in a traditional meta-analysis sense using the\nsum of transformed $p$-values with the transformation of light-tailed\ndistributions, in which Fisher's method and Stouffer's method are the most\nwell-known. Since the early 2000, advances in big data promoted methods to\naggregate independent, sparse and weak signals, such as the renowned higher\ncriticism and Berk-Jones tests. Recently, Liu and Xie(2020) and Wilson(2019)\nindependently proposed Cauchy and harmonic mean combination tests to robustly\ncombine $p$-values under \"arbitrary\" dependency structure, where a notable\napplication is to combine $p$-values from a set of often correlated SNPs in\ngenome-wide association studies. The proposed tests are the transformation of\nheavy-tailed distributions for improved power with the sparse signal. It calls\nfor a natural question to investigate heavy-tailed distribution transformation,\nto understand the connection among existing methods, and to explore the\nconditions for a method to possess robustness to dependency. In this paper, we\ninvestigate the regularly varying distribution, which is a rich family of\nheavy-tailed distribution and includes Pareto distribution as a special case.\nWe show that only an equivalent class of Cauchy and harmonic mean tests have\nsufficient robustness to dependency in a practical sense. We also show an issue\ncaused by large negative penalty in the Cauchy method and propose a simple, yet\npractical modification. Finally, we present simulations and apply to a\nneuroticism GWAS application to verify the discovered theoretical insights and\nprovide practical guidance.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 03:38:03 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Fang", "Yusi", ""], ["Tseng", "George C.", ""], ["Chang", "Chung", ""]]}, {"id": "2103.13051", "submitter": "Ke Zhu", "authors": "Ke Zhu and Hanzhong Liu", "title": "Pair-switching rerandomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rerandomization discards assignments with covariates unbalanced in the\ntreatment and control groups to improve the estimation and inference\nefficiency. However, the acceptance-rejection sampling method used by\nrerandomization is computationally inefficient. As a result, it is\ntime-consuming for classical rerandomization to draw numerous independent\nassignments, which are necessary for constructing Fisher randomization tests.\nTo address this problem, we propose a pair-switching rerandomization method to\ndraw balanced assignments much efficiently. We show that the\ndifference-in-means estimator is unbiased for the average treatment effect and\nthe Fisher randomization tests are valid under pair-switching rerandomization.\nIn addition, our method is applicable in both non-sequentially and sequentially\nrandomized experiments. We conduct comprehensive simulation studies to compare\nthe finite-sample performances of the proposed method and classical\nrerandomization. Simulation results indicate that pair-switching\nrerandomization leads to comparable power of Fisher randomization tests and is\n4-18 times faster than classical rerandomization. Finally, we apply the\npair-switching rerandomization method to analyze two clinical trial data sets,\nboth demonstrating the advantages of our method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 09:25:33 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zhu", "Ke", ""], ["Liu", "Hanzhong", ""]]}, {"id": "2103.13131", "submitter": "Andrew Whiteman", "authors": "Andrew S. Whiteman, Andreas J. Bartsch, Jian Kang, and Timothy D.\n  Johnson", "title": "Bayesian Inference for Brain Activity from Functional Magnetic Resonance\n  Imaging Collected at Two Spatial Resolutions", "comments": "37 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroradiologists and neurosurgeons increasingly opt to use functional\nmagnetic resonance imaging (fMRI) to map functionally relevant brain regions\nfor noninvasive presurgical planning and intraoperative neuronavigation. This\napplication requires a high degree of spatial accuracy, but the fMRI\nsignal-to-noise ratio (SNR) decreases as spatial resolution increases. In\npractice, fMRI scans can be collected at multiple spatial resolutions, and it\nis of interest to make more accurate inference on brain activity by combining\ndata with different resolutions. To this end, we develop a new Bayesian model\nto leverage both better anatomical precision in high resolution fMRI and higher\nSNR in standard resolution fMRI. We assign a Gaussian process prior to the mean\nintensity function and develop an efficient, scalable posterior computation\nalgorithm to integrate both sources of data. We draw posterior samples using an\nalgorithm analogous to Riemann manifold Hamiltonian Monte Carlo in an expanded\nparameter space. We illustrate our method in analysis of presurgical fMRI data,\nand show in simulation that it infers the mean intensity more accurately than\nalternatives that use either the high or standard resolution fMRI data alone.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:13:46 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Whiteman", "Andrew S.", ""], ["Bartsch", "Andreas J.", ""], ["Kang", "Jian", ""], ["Johnson", "Timothy D.", ""]]}, {"id": "2103.13221", "submitter": "Linquan Ma", "authors": "Yuyang Shi, Linquan Ma, Lan Liu", "title": "Mixed Effects Envelope Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When multiple measures are collected repeatedly over time, redundancy\ntypically exists among responses. The envelope method was recently proposed to\nreduce the dimension of responses without loss of information in regression\nwith multivariate responses. It can gain substantial efficiency over the\nstandard least squares estimator. In this paper, we generalize the envelope\nmethod to mixed effects models for longitudinal data with possibly unbalanced\ndesign and time-varying predictors. We show that our model provides more\nefficient estimators than the standard estimators in mixed effects models.\nImproved accuracy and efficiency of the proposed method over the standard mixed\neffects model estimator are observed in both the simulations and the Action to\nControl Cardiovascular Risk in Diabetes (ACCORD) study.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 14:30:00 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Shi", "Yuyang", ""], ["Ma", "Linquan", ""], ["Liu", "Lan", ""]]}, {"id": "2103.13233", "submitter": "Joshua Loyal", "authors": "Joshua Daniel Loyal, Ruoqing Zhu, Yifan Cui, Xin Zhang", "title": "Dimension Reduction Forests: Local Variable Importance using Structured\n  Random Forests", "comments": "36 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests are one of the most popular machine learning methods due to\ntheir accuracy and variable importance assessment. However, random forests only\nprovide variable importance in a global sense. There is an increasing need for\nsuch assessments at a local level, motivated by applications in personalized\nmedicine, policy-making, and bioinformatics. We propose a new nonparametric\nestimator that pairs the flexible random forest kernel with local sufficient\ndimension reduction to adapt to a regression function's local structure. This\nallows us to estimate a meaningful directional local variable importance\nmeasure at each prediction point. We develop a computationally efficient\nfitting procedure and provide sufficient conditions for the recovery of the\nsplitting directions. We demonstrate significant accuracy gains of our proposed\nestimator over competing methods on simulated and real regression problems.\nFinally, we apply the proposed method to seasonal particulate matter\nconcentration data collected in Beijing, China, which yields meaningful local\nimportance measures. The methods presented here are available in the drforest\nPython package.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 14:50:43 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Loyal", "Joshua Daniel", ""], ["Zhu", "Ruoqing", ""], ["Cui", "Yifan", ""], ["Zhang", "Xin", ""]]}, {"id": "2103.13236", "submitter": "Stavros Nikolakopoulos", "authors": "Stavros Nikolakopoulos and Ioannis Ntzoufras", "title": "Meta Analysis of Bayes Factors", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Bayes Factors, the Bayesian tool for hypothesis testing, are receiving\nincreasing attention in the literature. Compared to their frequentist rivals\n($p$-values or test statistics), Bayes Factors have the conceptual advantage of\nproviding evidence both for and against a null hypothesis and they can be\ncalibrated so that they do not depend so heavily on the sample size. However,\nresearch on the synthesis of Bayes Factors arising from individual studies has\nreceived very limited attention. In this work we review and propose methods for\ncombining Bayes Factors from multiple studies, depending on the level of\ninformation available. In the process, we provide insights with respect to the\ninterplay between frequentist and Bayesian evidence. We also clarify why some\nintuitive suggestions in the literature can be misleading. We assess the\nperformance of the methods discussed via a simulation study and apply the\nmethods in an example from the field of psychology.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 14:56:34 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Nikolakopoulos", "Stavros", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "2103.13238", "submitter": "Mathyn Vervaart", "authors": "Mathyn Vervaart, Mark Strong, Karl P. Claxton, Nicky J. Welton,\n  Torbj{\\o}rn Wisl{\\o}ff, Eline Aas", "title": "An Efficient Method for Computing Expected Value of Sample Information\n  for Survival Data from an Ongoing Trial", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The European Medicines Agency has in recent years allowed licensing of new\npharmaceuticals at an earlier stage in the clinical trial process. When trial\nevidence is obtained at an early stage, the events of interest, such as disease\nprogression or death, may have only been observed in a small proportion of\npatients. Health care authorities therefore must decide on the adoption of new\ntechnologies based on less mature evidence than previously, resulting in\ngreater uncertainty about clinical- and cost-effectiveness. When a trial is\nongoing at the point of decision making, there may be value in continuing the\ntrial in order to collect additional data before making an adoption decision.\nThis can be quantified by the Expected Value of Sample Information (EVSI).\nHowever, no guidance exists on how to compute the EVSI for survival data from\nan ongoing trial, nor on how to account for uncertainty about the choice of\nsurvival model in the EVSI calculations. In this article, we describe\nalgorithms for computing the EVSI of extending a trial's follow-up, both where\na single known survival model is assumed, and where we are uncertain about the\ntrue survival model. We compare a nested Markov Chain Monte Carlo procedure\nwith a non-parametric regression-based method in two synthetic case studies,\nand find close agreement between the two methods. The regression-based method\nis fast and straightforward to implement, and scales easily to include any\nnumber of candidate survival models in the model uncertainty case. EVSI for\nongoing trials can help decision makers determine whether early patient access\nto a new technology can be justified on the basis of the current evidence or\nwhether more mature evidence is needed.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 14:56:55 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 08:25:21 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Vervaart", "Mathyn", ""], ["Strong", "Mark", ""], ["Claxton", "Karl P.", ""], ["Welton", "Nicky J.", ""], ["Wisl\u00f8ff", "Torbj\u00f8rn", ""], ["Aas", "Eline", ""]]}, {"id": "2103.13248", "submitter": "Stavros Nikolakopoulos", "authors": "Stavros Nikolakopoulos, Eric Cator and Mart P. Janssen", "title": "Extending the Mann-Kendall test to allow for measurement uncertainty", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Mann-Kendall test for trend has gained a lot of attention in a range of\ndisciplines, especially in the environmental sciences. One of the drawbacks of\nthe Mann-Kendall test when applied to real data is that no distinction can be\nmade between meaningful and non-meaningful differences in subsequent\nobservations. We introduce the concept of partial ties, which allows inferences\nwhile accounting for (non)meaningful difference. We introduce the modified\nstatistic that accounts for such a concept and derive its variance estimator.\nWe also present analytical results for the behavior of the test in a class of\ncontiguous alternatives. Simulation results which illustrate the added value of\nthe test are presented. We apply our extended version of the test to some real\ndata concerning blood donation in Europe.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:04:01 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Nikolakopoulos", "Stavros", ""], ["Cator", "Eric", ""], ["Janssen", "Mart P.", ""]]}, {"id": "2103.13296", "submitter": "Carly Lupton-Smith", "authors": "Carly Lupton-Smith, Elena Badillo Goicoechea, Megan Collins, Justin\n  Lessler, M. Kate Grabowski, and Elizabeth A. Stuart", "title": "Consistency between household and county measures of K-12 onsite\n  schooling during the COVID-19 pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The academic, socioemotional, and health impacts of school policies\nthroughout the COVID-19 pandemic have been a source of many important questions\nthat require accurate information about the extent of onsite schooling that has\nbeen occurring throughout the pandemic. This paper investigates school\noperational status data sources during the COVID-19 pandemic, comparing\nself-report data collected nationally on the household level through a\nFacebook-based survey with data collected at district and county levels\nthroughout the country. The percentage of households reporting in-person\ninstruction within each county is compared to the district and county data at\nthe state and county levels. The results show high levels of consistency\nbetween the sources at the state level and for large counties. The consistency\nlevels across sources support the usage of the Facebook-based COVID-19 Symptom\nSurvey as a source to answer questions about the educational experiences,\nfactors, and impacts related to K-12 education across the nation during the\npandemic.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 16:15:56 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Lupton-Smith", "Carly", ""], ["Goicoechea", "Elena Badillo", ""], ["Collins", "Megan", ""], ["Lessler", "Justin", ""], ["Grabowski", "M. Kate", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "2103.13324", "submitter": "Gerhard Tutz", "authors": "Gerhard Tutz", "title": "Flexible Predictive Distributions from Varying-Thresholds Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A general class of models is proposed that is able to estimate the whole\npredictive distribution of a dependent variable $Y$ given a vector of\nexplanatory variables $\\xb$. The models exploit that the strength of\nexplanatory variables to distinguish between low and high values of the\ndependent variable may vary across the thresholds that are used to define low\nand high. Simple linear versions of the models are generalizations of classical\nlinear regression models but also of widely used ordinal regression models.\nThey allow to visualize the effect of explanatory variables in the form of\nparameter functions. More general models are based on efficient nonparametric\napproaches like random forests, which are more flexible and are strong\nprediction tools. A general estimation method is given that can use all the\nestimation tools that have been proposed for binary regression, including\nselection methods like the lasso or elastic net. For linearly structured models\nmaximum likelihood estimates are derived. The usefulness of the models is\nillustrated by simulations and several real data set.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 16:27:29 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Tutz", "Gerhard", ""]]}, {"id": "2103.13357", "submitter": "Zhiyuan Li", "authors": "Zhiyuan Li", "title": "A Two-Stage Variable Selection Approach for Correlated High Dimensional\n  Predictors", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When fitting statistical models, some predictors are often found to be\ncorrelated with each other, and functioning together. Many group variable\nselection methods are developed to select the groups of predictors that are\nclosely related to the continuous or categorical response. These existing\nmethods usually assume the group structures are well known. For example,\nvariables with similar practical meaning, or dummy variables created by\ncategorical data. However, in practice, it is impractical to know the exact\ngroup structure, especially when the variable dimensional is large. As a\nresult, the group variable selection results may be selected. To solve the\nchallenge, we propose a two-stage approach that combines a variable clustering\nstage and a group variable stage for the group variable selection problem. The\nvariable clustering stage uses information from the data to find a group\nstructure, which improves the performance of the existing group variable\nselection methods. For ultrahigh dimensional data, where the predictors are\nmuch larger than observations, we incorporated a variable screening method in\nthe first stage and shows the advantages of such an approach. In this article,\nwe compared and discussed the performance of four existing group variable\nselection methods under different simulation models, with and without the\nvariable clustering stage. The two-stage method shows a better performance, in\nterms of the prediction accuracy, as well as in the accuracy to select active\npredictors. An athlete's data is also used to show the advantages of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:28:34 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Li", "Zhiyuan", ""]]}, {"id": "2103.13435", "submitter": "Pengfei Li", "authors": "Tao Yu, Pengfei Li, Baojiang Chen, Ao Yuan, Jing Qin", "title": "Maximum pairwise-rank-likelihood-based inference for the semiparametric\n  transformation model", "comments": "6 tables and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the linear transformation model in the most general\nsetup. This model includes many important and popular models in statistics and\neconometrics as special cases. Although it has been studied for many years, the\nmethods in the literature are based on kernel-smoothing techniques or make use\nof only the ranks of the responses in the estimation of the parametric\ncomponents. The former approach needs a tuning parameter, which is not easily\noptimally specified in practice; and the latter is computationally expensive\nand may not make full use of the information in the data. In this paper, we\npropose two methods: a pairwise rank likelihood method and a\nscore-function-based method based on this pairwise rank likelihood. We also\nexplore the theoretical properties of the proposed estimators. Via extensive\nnumerical studies, we demonstrate that our methods are appealing in that the\nestimators are not only robust to the distribution of the random errors but\nalso lead to mean square errors that are in many cases comparable to or smaller\nthan those of existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:27:31 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yu", "Tao", ""], ["Li", "Pengfei", ""], ["Chen", "Baojiang", ""], ["Yuan", "Ao", ""], ["Qin", "Jing", ""]]}, {"id": "2103.13490", "submitter": "Said el Bouhaddani", "authors": "Said el Bouhaddani, Hae-Won Uh, Geurt Jongbloed, Jeanine\n  Houwing-Duistermaat", "title": "Statistical Integration of Heterogeneous Data with PO2PLS", "comments": "36 pages, 4 figures, Submitted to Journal of the American Statistical\n  Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The availability of multi-omics data has revolutionized the life sciences by\ncreating avenues for integrated system-level approaches. Data integration links\nthe information across datasets to better understand the underlying biological\nprocesses. However, high-dimensionality, correlations and heterogeneity pose\nstatistical and computational challenges. We propose a general framework,\nprobabilistic two-way partial least squares (PO2PLS), which addresses these\nchallenges. PO2PLS models the relationship between two datasets using joint and\ndata-specific latent variables. For maximum likelihood estimation of the\nparameters, we implement a fast EM algorithm and show that the estimator is\nasymptotically normally distributed. A global test for testing the relationship\nbetween two datasets is proposed, and its asymptotic distribution is derived.\nNotably, several existing omics integration methods are special cases of\nPO2PLS. Via extensive simulations, we show that PO2PLS performs better than\nalternatives in feature selection and prediction performance. In addition, the\nasymptotic distribution appears to hold when the sample size is sufficiently\nlarge. We illustrate PO2PLS with two examples from commonly used study designs:\na large population cohort and a small case-control study. Besides recovering\nknown relationships, PO2PLS also identified novel findings. The methods are\nimplemented in our R-package PO2PLS. Supplementary materials for this article\nare available online.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 21:15:04 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Bouhaddani", "Said el", ""], ["Uh", "Hae-Won", ""], ["Jongbloed", "Geurt", ""], ["Houwing-Duistermaat", "Jeanine", ""]]}, {"id": "2103.13510", "submitter": "Natalia Zemlianskaia", "authors": "Natalia Zemlianskaia, W. James Gauderman, Juan Pablo Lewinger", "title": "A scalable hierarchical lasso for gene-environment interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a regularized regression model for the selection of\ngene-environment (GxE) interactions. The model focuses on a single\nenvironmental exposure and induces a main-effect-before-interaction\nhierarchical structure. We propose an efficient fitting algorithm and screening\nrules that can discard large numbers of irrelevant predictors with high\naccuracy. We present simulation results showing that the model outperforms\nexisting joint selection methods for (GxE) interactions in terms of selection\nperformance, scalability and speed, and provide a real data application. Our\nimplementation is available in the gesso R package.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 22:33:14 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 01:04:50 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Zemlianskaia", "Natalia", ""], ["Gauderman", "W. James", ""], ["Lewinger", "Juan Pablo", ""]]}, {"id": "2103.13693", "submitter": "Yuan Shijie", "authors": "Shijie Yuan, Tianjian Zhou, Yawen Lin, and Yuan Ji", "title": "The Ci3+3 Design for Dual-Agent Combination Dose-Finding Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a rule-based statistical design for combination dose-finding\ntrials with two agents. The Ci3+3 design is an extension of the i3+3 design\nwith simple decision rules comparing the observed toxicity rates and\nequivalence intervals that define the maximum tolerated dose combination. Ci3+3\nconsists of two stages to allow fast and efficient exploration of the\ndose-combination space. Statistical inference is restricted to a beta-binomial\nmodel for dose evaluation, and the entire design is built upon a set of fixed\nrules. We show via simulation studies that the Ci3+3 design exhibits similar\nand comparable operating characteristics to more complex designs utilizing\nmodel-based inferences. We believe that the Ci3+3 design may provide an\nalternative choice to help simplify the design and conduct of combination\ndose-finding trials in practice.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 09:17:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yuan", "Shijie", ""], ["Zhou", "Tianjian", ""], ["Lin", "Yawen", ""], ["Ji", "Yuan", ""]]}, {"id": "2103.13977", "submitter": "Simone Giannerini", "authors": "Greta Goracci, Simone Giannerini, Kung-Sik Chan, Howell Tong", "title": "Testing for threshold effects in the TARMA framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present supremum Lagrange Multiplier tests to compare a linear ARMA\nspecification against its threshold ARMA extension. We derive the asymptotic\ndistribution of the test statistics both under the null hypothesis and\ncontiguous local alternatives. Moreover, we prove the consistency of the tests.\nThe Monte Carlo study shows that the tests enjoy good finite-sample properties,\nare robust against model mis-specification and their performance is not\naffected if the order of the model is unknown. The tests present a low\ncomputational burden and do not suffer from some of the drawbacks that affect\nthe quasi-likelihood ratio setting. Lastly, we apply our tests to a time series\nof standardized tree-ring growth indexes and this can lead to new research in\nclimate studies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:11:08 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Goracci", "Greta", ""], ["Giannerini", "Simone", ""], ["Chan", "Kung-Sik", ""], ["Tong", "Howell", ""]]}, {"id": "2103.14029", "submitter": "Xiaojie Mao", "authors": "Nathan Kallus, Xiaojie Mao, Masatoshi Uehara", "title": "Causal Inference Under Unmeasured Confounding With Negative Controls: A\n  Minimax Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of causal parameters when not all confounders are\nobserved and instead negative controls are available. Recent work has shown how\nthese can enable identification and efficient estimation via two so-called\nbridge functions. In this paper, we tackle the primary challenge to causal\ninference using negative controls: the identification and estimation of these\nbridge functions. Previous work has relied on uniqueness and completeness\nassumptions on these functions that may be implausible in practice and also\nfocused on their parametric estimation. Instead, we provide a new\nidentification strategy that avoids both uniqueness and completeness. And, we\nprovide a new estimators for these functions based on minimax learning\nformulations. These estimators accommodate general function classes such as\nreproducing Hilbert spaces and neural networks. We study finite-sample\nconvergence results both for estimating bridge function themselves and for the\nfinal estimation of the causal parameter. We do this under a variety of\ncombinations of assumptions that include realizability and closedness\nconditions on the hypothesis and critic classes employed in the minimax\nestimator. Depending on how much we are willing to assume, we obtain different\nconvergence rates. In some cases, we show the estimate for the causal parameter\nmay converge even when our bridge function estimators do not converge to any\nvalid bridge function. And, in other cases, we show we can obtain\nsemiparametric efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:19 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 00:23:44 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kallus", "Nathan", ""], ["Mao", "Xiaojie", ""], ["Uehara", "Masatoshi", ""]]}, {"id": "2103.14153", "submitter": "Carla Moreira", "authors": "Carla Moreira and Jacobo de U\\~na-\\'Alvarez and Ana Cristina Santos\n  and Henrique Barros", "title": "Smoothing methods to estimate the hazard rate under double truncation", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Survival Analysis, the observed lifetimes often correspond to individuals\nfor which the event occurs within a specific calendar time interval. With such\ninterval sampling, the lifetimes are doubly truncated at values determined by\nthe birth dates and the sampling interval. This double truncation may induce a\nsystematic bias in estimation, so specific corrections are needed. A relevant\ntarget in Survival Analysis is the hazard rate function, which represents the\ninstantaneous probability for the event of interest. In this work we introduce\na flexible estimation approach for the hazard rate under double truncation.\nSpecifically, a kernel smoother is considered, in both a fully nonparametric\nsetting and a semiparametric setting in which the incidence process fits a\ngiven parametric model. Properties of the kernel smoothers are investigated\nboth theoretically and through simulations. In particular, an asymptotic\nexpression of the mean integrated squared error is derived, leading to a\ndata-driven bandwidth for the estimators. The relevance of the semiparametric\napproach is emphasized, in that it is generally more accurate and, importantly,\nit avoids the potential issues of nonexistence or nonuniqueness of the fully\nnonparametric estimator. Applications to the age of diagnosis of Acute Coronary\nSyndrome (ACS) and AIDS incubation times are included.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 22:11:30 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Moreira", "Carla", ""], ["de U\u00f1a-\u00c1lvarez", "Jacobo", ""], ["Santos", "Ana Cristina", ""], ["Barros", "Henrique", ""]]}, {"id": "2103.14200", "submitter": "Samya Tajmouati", "authors": "Samya Tajmouati, Bouazza El Wahbi, Adel Bedoui, Abdallah Abarda,\n  Mohamed Dakkoun", "title": "Applying k-nearest neighbors to time series forecasting : two new\n  approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-nearest neighbors algorithm is one of the prominent techniques used in\nclassification and regression. Despite its simplicity, the k-nearest neighbors\nhas been successfully applied in time series forecasting. However, the\nselection of the number of neighbors and feature selection is a daunting task.\nIn this paper, we introduce two methodologies to forecasting time series that\nwe refer to as Classical Parameters Tuning in Weighted Nearest Neighbors and\nFast Parameters Tuning in Weighted Nearest Neighbors. The first approach uses\nclassical parameters tuning that compares the most recent subsequence with\nevery possible subsequence from the past of the same length. The second\napproach reduces the neighbors' search set, which leads to significantly\nreduced grid size and hence a lower computational time. To tune the models'\nparameters, both methods implement an approach inspired by cross-validation for\nweighted nearest neighbors. We evaluate the forecasting performance and\naccuracy of our models. Then, we compare them to some classical approaches,\nespecially, Seasonal Autoregressive Integrated Moving Average, Holt-Winters and\nExponential Smoothing State Space Model. Real data examples on retail and food\nservices sales in the USA and milk production in the UK are analyzed to\ndemonstrate the application and the efficiency of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 01:24:00 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Tajmouati", "Samya", ""], ["Wahbi", "Bouazza El", ""], ["Bedoui", "Adel", ""], ["Abarda", "Abdallah", ""], ["Dakkoun", "Mohamed", ""]]}, {"id": "2103.14368", "submitter": "Marie B\\\"ohnstedt", "authors": "Marie B\\\"ohnstedt, Jutta Gampe, Monique A. A. Caljouw, Hein Putter", "title": "Incorporating delayed entry into the joint frailty model for recurrent\n  events and a terminal event", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In studies of recurrent events, joint modeling approaches are often needed to\nallow for potential dependent censoring by a terminal event such as death.\nJoint frailty models for recurrent events and death with an additional\ndependence parameter have been studied for cases in which individuals are\nobserved from the start of the event processes. However, the samples are often\nselected at a later time, which results in delayed entry. Thus, only\nindividuals who have not yet experienced the terminal event will be included in\nthe study. We propose a method for estimating the joint frailty model from such\nleft-truncated data. The frailty distribution among the selected survivors\ndiffers from the frailty distribution in the underlying population if the\nrecurrence process and the terminal event are associated. The correctly\nadjusted marginal likelihood can be expressed as a ratio of two integrals over\nthe frailty distribution, which may be approximated using Gaussian quadrature.\nThe baseline rates are specified as piecewise constant functions, and the\ncovariates are assumed to have multiplicative effects on the event rates. We\nassess the performance of the estimation procedure in a simulation study, and\napply the method to estimate age-specific rates of recurrent urinary tract\ninfections and mortality in an older population.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 10:18:43 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["B\u00f6hnstedt", "Marie", ""], ["Gampe", "Jutta", ""], ["Caljouw", "Monique A. A.", ""], ["Putter", "Hein", ""]]}, {"id": "2103.14401", "submitter": "Camille Fr\\'event", "authors": "Camille Fr\\'event, Mohamed-Salem Ahmed, Sophie Dabo-Niang and\n  Micha\\\"el Genin", "title": "Investigating spatial scan statistics for multivariate functional data", "comments": "arXiv admin note: text overlap with arXiv:2011.03482", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces new scan statistics for multivariate functional data\nindexed in space. The new methods are derivated from a MANOVA test statistic\nfor functional data, an adaptation of the Hotelling T2-test statistic, and a\nmultivariate extension of the Wilcoxon rank-sum test statistic. In a simulation\nstudy, the latter two methods present very good performances and the adaptation\nof the functional MANOVA also shows good performances for a normal\ndistribution. Our methods detect more accurate spatial clusters than an\nexisting nonparametric functional scan statistic. Lastly we applied the methods\non multivariate functional data to search for spatial clusters of abnormal\ndaily concentrations of air pollutants in the north of France in May and June\n2020.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 11:12:28 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Fr\u00e9vent", "Camille", ""], ["Ahmed", "Mohamed-Salem", ""], ["Dabo-Niang", "Sophie", ""], ["Genin", "Micha\u00ebl", ""]]}, {"id": "2103.14421", "submitter": "Samuel Branders", "authors": "Samuel Branders, Alvaro Pereira, Guillaume Bernard, Marie Ernst,\n  Adelin Albert", "title": "Leveraging Historical Data for High-Dimensional Regression Adjustment, a\n  Composite Covariate Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of data collected from patients involved in clinical trials is\ncontinuously growing. All patient characteristics are potential covariates that\ncould be used to improve clinical trial analysis and power. However, the\nrestricted number of patients in phases I and II studies limits the possible\nnumber of covariates included in the analyses. In this paper, we investigate\nthe cost/benefit ratio of including covariates in the analysis of clinical\ntrials. Within this context, we address the long-running question \"What is the\noptimum number of covariates to include in a clinical trial?\" To further\nimprove the cost/benefit ratio of covariates, historical data can be leveraged\nto pre-specify the covariate weights, which can be viewed as the definition of\na new composite covariate. We analyze the use of a composite covariate while\nestimating the treatment effect in small clinical trials. A composite covariate\nlimits the loss of degrees of freedom and the risk of overfitting.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:01:17 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Branders", "Samuel", ""], ["Pereira", "Alvaro", ""], ["Bernard", "Guillaume", ""], ["Ernst", "Marie", ""], ["Albert", "Adelin", ""]]}, {"id": "2103.14445", "submitter": "Emilia Pompe", "authors": "Emilia Pompe", "title": "Introducing prior information in Weighted Likelihood Bootstrap with\n  applications to model misspecification", "comments": "43 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Posterior Bootstrap, a set of algorithms extending Weighted\nLikelihood Bootstrap, to properly incorporate prior information and address the\nproblem of model misspecification in Bayesian inference. We consider two\napproaches to incorporating prior knowledge: the first is based on penalization\nof the Weighted Likelihood Bootstrap objective function, and the second uses\npseudo-samples from the prior predictive distribution. We also propose\nmethodology for hierarchical models, which was not previously known for methods\nbased on Weighted Likelihood Bootstrap. Edgeworth expansions guide the\ndevelopment of our methodology and allow us to provide greater insight on\nproperties of Weighted Likelihood Bootstrap than were previously known. Our\nexperiments confirm the theoretical results and show a reduction in the impact\nof model misspecification against Bayesian inference in the misspecified\nsetting.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:55:35 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 17:24:20 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Pompe", "Emilia", ""]]}, {"id": "2103.14513", "submitter": "Nicholas Young", "authors": "Nicholas T. Young, Marcos D. Caballero", "title": "Predictive and explanatory models might miss informative features in\n  educational data", "comments": "45 pages, 15 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.ed-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We encounter variables with little variation often in educational data mining\n(EDM) and discipline-based education research (DBER) due to the demographics of\nhigher education and the questions we ask. Yet, little work has examined how to\nanalyze such data. Therefore, we conducted a simulation study using logistic\nregression, penalized regression, and random forest. We systematically varied\nthe fraction of positive outcomes, feature imbalances, and odds ratios. We find\nthe algorithms treat features with the same odds ratios differently based on\nthe features' imbalance and the outcome imbalance. While none of the algorithms\nfully solved the problem, penalized approaches such as Firth and Log-F reduced\nthe scale of the problem. Our results suggest that EDM and DBER studies might\ncontain false negatives when determining which variables are related to an\noutcome. We then apply our findings to a graduate admissions data set and we\npropose recommendations for researchers working with the kind of imbalanced\ndata common to EDM and DBER studies.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 15:14:46 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Young", "Nicholas T.", ""], ["Caballero", "Marcos D.", ""]]}, {"id": "2103.14559", "submitter": "Julien Bect", "authors": "Julien Bect (L2S, GdR MASCOT-NUM), Souleymane Zio (L2S, GdR\n  MASCOT-NUM), Guillaume Perrin (LDG, DAM/DIF, GdR MASCOT-NUM), Claire\n  Cannamela (DAM/DIF, GdR MASCOT-NUM), Emmanuel Vazquez (L2S, GdR MASCOT-NUM)", "title": "On the quantification of discretization uncertainty: comparison of two\n  paradigms", "comments": null, "journal-ref": "14th World Congress in Computational Mechanics and ECCOMAS\n  Congress 2020 (WCCM-ECCOMAS), Jan 2021, Virtual conference, originally\n  scheduled in Paris, France", "doi": "10.23967/wccm-eccomas.2020.260", "report-no": null, "categories": "physics.comp-ph physics.med-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical models based on partial differential equations (PDE), or\nintegro-differential equations, are ubiquitous in engineering and science,\nmaking it possible to understand or design systems for which physical\nexperiments would be expensive-sometimes impossible-to carry out. Such models\nusually construct an approximate solution of the underlying continuous\nequations, using discretization methods such as finite differences or the\nfinite elements method. The resulting discretization error introduces a form of\nuncertainty on the exact but unknown value of any quantity of interest (QoI),\nwhich affects the predictions of the numerical model alongside other sources of\nuncertainty such as parametric uncertainty or model inadequacy. The present\narticle deals with the quantification of this discretization uncertainty.A\nfirst approach to this problem, now standard in the V\\&V (Verification and\nValidation) literature, uses the grid convergence index (GCI) originally\nproposed by P. Roache in the field of computational fluid dynamics (CFD), which\nis based on the Richardson extrapolation technique. Another approach, based on\nBayesian inference with Gaussian process models, was more recently introduced\nin the statistical literature. In this work we present and compare these two\nparadigms for the quantification of discretization uncertainty, which have been\ndevelopped in different scientific communities, and assess the potential of\nthe-younger-Bayesian approach to provide a replacement for the well-established\nGCI-based approach, with better probabilistic foundations. The methods are\nillustrated and evaluated on two standard test cases from the literature\n(lid-driven cavity and Timoshenko beam).\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 07:51:43 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bect", "Julien", "", "L2S, GdR MASCOT-NUM"], ["Zio", "Souleymane", "", "L2S, GdR\n  MASCOT-NUM"], ["Perrin", "Guillaume", "", "LDG, DAM/DIF, GdR MASCOT-NUM"], ["Cannamela", "Claire", "", "DAM/DIF, GdR MASCOT-NUM"], ["Vazquez", "Emmanuel", "", "L2S, GdR MASCOT-NUM"]]}, {"id": "2103.14565", "submitter": "Margrethe Kvale Loe", "authors": "Margrethe Kvale Loe and H{\\aa}kon Tjelmeland", "title": "A generalised and fully Bayesian framework for ensemble updating", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a generalised framework for the updating of a prior ensemble to a\nposterior ensemble, an essential yet challenging part in ensemble-based\nfiltering methods. The proposed framework is based on a generalised and fully\nBayesian view on the traditional ensemble Kalman filter (EnKF). In the EnKF,\nthe updating of the ensemble is based on Gaussian assumptions, whereas in our\ngeneral setup the updating may be based on another parametric family. In\naddition, we propose to formulate an optimality criterion and to find the\noptimal update with respect to this criterion. The framework is fully Bayesian\nin the sense that the parameters of the assumed forecast model are treated as\nrandom variables. As a consequence, a parameter vector is simulated, for each\nensemble member, prior to the updating. In contrast to existing fully Bayesian\napproaches, where the parameters are simulated conditionally on all the\nforecast samples, the parameters are in our framework simulated conditionally\non both the data and all the forecast samples, except the forecast sample which\nis to be updated. The proposed framework is studied in detail for two\nparametric families: the linear-Gaussian model and the finite state-space\nhidden Markov model. For both cases, we present simulation examples and compare\nthe results with existing ensemble-based filtering methods. The results of the\nproposed approach indicate a promising performance. In particular, the filter\nbased on the linear-Gaussian model gives a more realistic representation of the\nuncertainty than the traditional EnKF, and the effect of not conditioning on\nthe forecast sample which is to be updated when simulating the parameters is\nremarkable.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 16:23:47 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Loe", "Margrethe Kvale", ""], ["Tjelmeland", "H\u00e5kon", ""]]}, {"id": "2103.14593", "submitter": "Valeriy Kalyagin", "authors": "V.A. Kalyagin, A.P. Koldanov, P.A. Koldanov", "title": "Reliability of MST identification in correlation-based market networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Maximum spanning tree (MST) is a popular tool in market network analysis.\nLarge number of publications are devoted to the MST calculation and it's\ninterpretation for particular stock markets. However, much less attention is\npayed in the literature to the analysis of uncertainty of obtained results. In\nthe present paper we suggest a general framework to measure uncertainty of MST\nidentification. We study uncertainty in the framework of the concept of random\nvariable network (RVN). We consider different correlation based networks in the\nlarge class of elliptical distributions. We show that true MST is the same in\nthree networks: Pearson correlation network, Fechner correlation network, and\nKendall correlation network. We argue that among different measures of\nuncertainty the FDR (False Discovery Rate) is the most appropriated for MST\nidentification. We investigate FDR of Kruskal algorithm for MST identification\nand show that reliability of MST identification is different in these three\nnetworks. In particular, for Pearson correlation network the FDR essentially\ndepends on distribution of stock returns. We prove that for market network with\nFechner correlation the FDR is non sensitive to the assumption on stock's\nreturn distribution. Some interesting phenomena are discovered for Kendall\ncorrelation network. Our experiments show that FDR of Kruskal algorithm for MST\nidentification in Kendall correlation network weakly depend on distribution and\nat the same time the value of FDR is almost the best in comparison with MST\nidentification in other networks. These facts are important in practical\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 20:17:53 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kalyagin", "V. A.", ""], ["Koldanov", "A. P.", ""], ["Koldanov", "P. A.", ""]]}, {"id": "2103.14626", "submitter": "Zhaoxing Gao", "authors": "Zhaoxing Gao and Ruey S. Tsay", "title": "Divide-and-Conquer: A Distributed Hierarchical Factor Approach to\n  Modeling Large-Scale Time Series Data", "comments": "48 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a hierarchical approximate-factor approach to analyzing\nhigh-dimensional, large-scale heterogeneous time series data using distributed\ncomputing. The new method employs a multiple-fold dimension reduction procedure\nusing Principal Component Analysis (PCA) and shows great promises for modeling\nlarge-scale data that cannot be stored nor analyzed by a single machine. Each\ncomputer at the basic level performs a PCA to extract common factors among the\ntime series assigned to it and transfers those factors to one and only one node\nof the second level. Each 2nd-level computer collects the common factors from\nits subordinates and performs another PCA to select the 2nd-level common\nfactors. This process is repeated until the central server is reached, which\ncollects common factors from its direct subordinates and performs a final PCA\nto select the global common factors. The noise terms of the 2nd-level\napproximate factor model are the unique common factors of the 1st-level\nclusters. We focus on the case of 2 levels in our theoretical derivations, but\nthe idea can easily be generalized to any finite number of hierarchies. We\ndiscuss some clustering methods when the group memberships are unknown and\nintroduce a new diffusion index approach to forecasting. We further extend the\nanalysis to unit-root nonstationary time series. Asymptotic properties of the\nproposed method are derived for the diverging dimension of the data in each\ncomputing unit and the sample size $T$. We use both simulated data and real\nexamples to assess the performance of the proposed method in finite samples,\nand compare our method with the commonly used ones in the literature concerning\nthe forecastability of extracted factors.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:40:48 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Gao", "Zhaoxing", ""], ["Tsay", "Ruey S.", ""]]}, {"id": "2103.14668", "submitter": "Alexander Kreiss", "authors": "Alexander Kreiss and Enno Mammen and Wolfgang Polonik", "title": "Testing For a Parametric Baseline-Intensity in Dynamic Interaction\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical network analysis it is common to observe so called interaction\ndata. Such data is characterized by the actors who form the vertices of a\nnetwork. These are able to interact with each other along the edges of the\nnetwork. One usually assumes that the edges in the network are randomly formed\nand dissolved over the observation horizon. In addition covariates are observed\nand the interest is to model the impact of the covariates on the interactions.\nIn this paper we develop a framework to test if a non-parametric form of the\nbaseline intensity allows for more flexibility than a baseline which is\nparametrically dependent on system-wide covariates (i.e. covariates which take\nthe same value for all individuals, e.g. time). This allows to test if certain\nseasonality effects can be explained by simple covariates like the time. The\nprocedure is applied to modeling the baseline intensity in a bike-sharing\nnetwork by using weather and time information.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 18:18:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kreiss", "Alexander", ""], ["Mammen", "Enno", ""], ["Polonik", "Wolfgang", ""]]}, {"id": "2103.14693", "submitter": "Melkior Ornik", "authors": "Melkior Ornik", "title": "Inapplicability of the TVOR Method to USHMM Data Outlier Identification", "comments": "Updated to the final accepted version. The paper has been published\n  in IEEE Access, vol. 9, pp. 78586-78593, 2021, under the title \"Comment on\n  'TVOR: Finding Discrete Total Variation Outliers Among Histograms' \". The\n  difference in titles is due to the journal policy on naming of comment papers", "journal-ref": "IEEE Access 9 (2021) 78586-78593", "doi": "10.1109/ACCESS.2021.3082900", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent paper \"TVOR: Finding Discrete Total Variation Outliers Among\nHistograms\" [arXiv:2012.11574] introduces the Total Variation Outlier\nRecognizer (TVOR) method for identification of outliers among a given set of\nhistograms. After providing a theoretical discussion of the method and\nverifying its success on synthetic and population census data, it applies the\nTVOR model to histograms of ages of Holocaust victims produced using United\nStates Holocaust Memorial Museum data. It purports to identify the list of\nvictims of the Jasenovac concentration camp as potentially suspicious. In this\ncomment paper, we show that the TVOR model and its assumptions are grossly\ninapplicable to the considered dataset. When applied to the considered data,\nthe model is biased in assigning a higher outlier score to histograms of larger\nsizes, the set of data points is extremely sparse around the point of interest,\nthe dataset has not been reviewed to remove obvious data processing errors,\nand, contrary to the model requirements, the distributions of the victims' ages\nnaturally vary significantly across victim lists.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 04:05:20 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 20:18:04 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ornik", "Melkior", ""]]}, {"id": "2103.14726", "submitter": "Avanti Athreya", "authors": "Zachary Lubberts, Avanti Athreya, Youngser Park, Carey E. Priebe", "title": "Beyond the adjacency matrix: random line graphs and inference for\n  networks with edge attributes", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any modern network inference paradigm must incorporate multiple aspects of\nnetwork structure, including information that is often encoded both in vertices\nand in edges. Methodology for handling vertex attributes has been developed for\na number of network models, but comparable techniques for edge-related\nattributes remain largely unavailable. We address this gap in the literature by\nextending the latent position random graph model to the line graph of a random\ngraph, which is formed by creating a vertex for each edge in the original\nrandom graph, and connecting each pair of edges incident to a common vertex in\nthe original graph. We prove concentration inequalities for the spectrum of a\nline graph, and then establish that although naive spectral decompositions can\nfail to extract necessary signal for edge clustering, there exist\nsignal-preserving singular subspaces of the line graph that can be recovered\nthrough a carefully-chosen projection. Moreover, we can consistently estimate\nedge latent positions in a random line graph, even though such graphs are of a\nrandom size, typically have high rank, and possess no spectral gap. Our results\nalso demonstrate that the line graph of a stochastic block model exhibits\nunderlying block structure, and we synthesize and test our methods in\nsimulations for cluster recovery and edge covariate inference in stochastic\nblock model graphs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 20:44:33 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lubberts", "Zachary", ""], ["Athreya", "Avanti", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2103.14765", "submitter": "Benjamin Lu", "authors": "Benjamin Lu, Eli Ben-Michael, Avi Feller, Luke Miratrix", "title": "Is it who you are or where you are? Accounting for compositional\n  differences in cross-site treatment variation", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multisite trials, in which treatment is randomized separately in multiple\nsites, offer a unique opportunity to disentangle treatment effect variation due\nto \"compositional\" differences in the distributions of unit-level features from\nvariation due to \"contextual\" differences in site-level features. In\nparticular, if we can re-weight (or \"transport\") each site to have a common\ndistribution of unit-level covariates, the remaining effect variation captures\ncontextual differences across sites. In this paper, we develop a framework for\ntransporting effects in multisite trials using approximate balancing weights,\nwhere the weights are chosen to directly optimize unit-level covariate balance\nbetween each site and the target distribution. We first develop our approach\nfor the general setting of transporting the effect of a single-site trial. We\nthen extend our method to multisite trials, assess its performance via\nsimulation, and use it to analyze a series of multisite trials of\nwelfare-to-work programs. Our method is available in the balancer R package.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 23:23:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lu", "Benjamin", ""], ["Ben-Michael", "Eli", ""], ["Feller", "Avi", ""], ["Miratrix", "Luke", ""]]}, {"id": "2103.14885", "submitter": "Jing Ouyang", "authors": "Jing Ouyang, Gongjun Xu", "title": "Identifiability of Latent Class Models with Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent class models with covariates are widely used for psychological,\nsocial, and educational researches. Yet the fundamental identifiability issue\nof these models has not been fully addressed. Among the previous researches on\nthe identifiability of latent class models containing covariates, Huang and\nBandeen-Roche (2004, Psychometrika, 69:5-32) studied the local identifiability\nconditions. However, motivated by recent advances in the identifiability of\nrestricted latent class models, particularly the Cognitive Diagnosis Models\n(CDMs), we show in this work that the conditions in Huang and Bandeen-Roche\n(2004) are only necessary but not sufficient to determine the local\nidentifiability of the model parameters. To address the open identifiability\nissue for latent class models with covariates, this work establishes conditions\nto ensure the global identifiability of the model parameters in both strict and\ngeneric sense. Moreover, our results extend to polytomous-response CDMs with\ncovariates, which generalizes the existing identifiability results for CDMs.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 12:04:05 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 22:38:03 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Ouyang", "Jing", ""], ["Xu", "Gongjun", ""]]}, {"id": "2103.15018", "submitter": "David Ritzwoller", "authors": "Thomas J. DiCiccio, David M. Ritzwoller, Joseph P. Romano, Azeem M.\n  Shaikh", "title": "Confidence Intervals for Seroprevalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper concerns the construction of confidence intervals in standard\nseroprevalence surveys. In particular, we discuss methods for constructing\nconfidence intervals for the proportion of individuals in a population infected\nwith a disease using a sample of antibody test results and measurements of the\ntest's false positive and false negative rates. We begin by documenting erratic\nbehavior in the coverage probabilities of standard Wald and percentile\nbootstrap intervals when applied to this problem. We then consider two\nalternative sets of intervals constructed with test inversion. The first set of\nintervals are approximate, using either asymptotic or bootstrap approximation\nto the finite-sample distribution of a chosen test statistic. We consider\nseveral choices of test statistic, including maximum likelihood estimators and\ngeneralized likelihood ratio statistics. We show with simulation that, at\nempirically relevant parameter values and sample sizes, the coverage\nprobabilities for these intervals are close to their nominal level and are\napproximately equi-tailed. The second set of intervals are shown to contain the\ntrue parameter value with probability at least equal to the nominal level, but\ncan be conservative in finite samples. To conclude, we outline the application\nof the methods that we consider to several related problems, and we provide a\nset of practical recommendations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 23:58:24 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["DiCiccio", "Thomas J.", ""], ["Ritzwoller", "David M.", ""], ["Romano", "Joseph P.", ""], ["Shaikh", "Azeem M.", ""]]}, {"id": "2103.15023", "submitter": "Maozhu Dai", "authors": "Maozhu Dai and Weining Shen and Hal S. Stern", "title": "Nonparametric tests for treatment effect heterogeneity in observational\n  studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing for treatment effect heterogeneity in\nobservational studies, and propose a nonparametric test based on multisample\nU-statistics. To account for potential confounders, we use reweighted data\nwhere the weights are determined by estimated propensity scores. The proposed\nmethod does not require any parametric assumptions on the outcomes and bypasses\nthe need for modeling the treatment effect for each study subgroup. We\nestablish the asymptotic normality for the test statistic, and demonstrate its\nsuperior numerical performance over several competing approaches via simulation\nstudies. Two real data applications including an employment program evaluation\nstudy and a mental health study of China's one-child policy are also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 00:26:34 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Dai", "Maozhu", ""], ["Shen", "Weining", ""], ["Stern", "Hal S.", ""]]}, {"id": "2103.15157", "submitter": "Gabriele Tornetta", "authors": "Gabriele N. Tornetta", "title": "Entropy methods for the confidence assessment of probabilistic\n  classification models", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many classification models produce a probability distribution as the outcome\nof a prediction. This information is generally compressed down to the single\nclass with the highest associated probability. In this paper, we argue that\npart of the information that is discarded in this process can be in fact used\nto further evaluate the goodness of models, and in particular the confidence\nwith which each prediction is made. As an application of the ideas presented in\nthis paper, we provide a theoretical explanation of a confidence degradation\nphenomenon observed in the complement approach to the (Bernoulli) Naive Bayes\ngenerative model.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 15:39:13 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tornetta", "Gabriele N.", ""]]}, {"id": "2103.15218", "submitter": "Asma Bahamyirou", "authors": "Asma Bahamyirou, Mireille E. Schnitzer", "title": "Data Integration through outcome adaptive LASSO and a collaborative\n  propensity score approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Administrative data, or non-probability sample data, are increasingly being\nused to obtain official statistics due to their many benefits over survey\nmethods. In particular, they are less costly, provide a larger sample size, and\nare not reliant on the response rate. However, it is difficult to obtain an\nunbiased estimate of the population mean from such data due to the absence of\ndesign weights. Several estimation approaches have been proposed recently using\nan auxiliary probability sample which provides representative covariate\ninformation of the target population. However, when this covariate information\nis high-dimensional, variable selection is not a straight-forward task even for\na subject matter expert. In the context of efficient and doubly robust\nestimation approaches for estimating a population mean, we develop two data\nadaptive methods for variable selection using the outcome adaptive LASSO and a\ncollaborative propensity score, respectively. Simulation studies are performed\nin order to verify the performance of the proposed methods versus competing\nmethods. Finally, we presented an anayisis of the impact of Covid-19 on\nCanadians.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 21:02:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bahamyirou", "Asma", ""], ["Schnitzer", "Mireille E.", ""]]}, {"id": "2103.15224", "submitter": "Antonio Lepore Prof.", "authors": "Fabio Centofanti, Antonio Lepore, Biagio Palumbo", "title": "Sparse and Smooth Functional Data Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new model-based procedure is developed for sparse clustering of functional\ndata that aims to classify a sample of curves into homogeneous groups while\njointly detecting the most informative portions of domain. The proposed method\nis referred to as sparse and smooth functional clustering (SaS-Funclust) and\nrelies on a general functional Gaussian mixture model whose parameters are\nestimated by maximizing a log-likelihood function penalized with a functional\nadaptive pairwise penalty and a roughness penalty. The former allows\nidentifying the noninformative portion of domain by shrinking the means of\nseparated clusters to some common values, whereas the latter improves the\ninterpretability by imposing some degree of smoothing to the estimated cluster\nmeans. The model is estimated via an expectation-conditional maximization\nalgorithm paired with a cross-validation procedure. Through a Monte Carlo\nsimulation study, the SaS-Funclust method is shown to outperform other methods\nalready appeared in the literature, both in terms of clustering performance and\ninterpretability. Finally, three real-data examples are presented to\ndemonstrate the favourable performance of the proposed method. The SaS-Funclust\nmethod is implemented in the $\\textsf{R}$ package $\\textsf{sasfunclust}$,\navailable online at https://github.com/unina-sfere/sasfunclust.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 21:26:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Centofanti", "Fabio", ""], ["Lepore", "Antonio", ""], ["Palumbo", "Biagio", ""]]}, {"id": "2103.15229", "submitter": "Michele Zemplenyi", "authors": "Michele Zemplenyi (Harvard University), Jeffrey W. Miller (Harvard\n  University)", "title": "Bayesian Optimal Experimental Design for Inferring Causal Structure", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inferring the causal structure of a system typically requires interventional\ndata, rather than just observational data. Since interventional experiments can\nbe costly, it is preferable to select interventions that yield the maximum\namount of information about a system. We propose a novel Bayesian method for\noptimal experimental design by sequentially selecting interventions that\nminimize the expected posterior entropy as rapidly as possible. A key feature\nis that the method can be implemented by computing simple summaries of the\ncurrent posterior, avoiding the computationally burdensome task of repeatedly\nperforming posterior inference on hypothetical future datasets drawn from the\nposterior predictive. After deriving the method in a general setting, we apply\nit to the problem of inferring causal networks. We present a series of\nsimulation studies in which we find that the proposed method performs favorably\ncompared to existing alternative methods. Finally, we apply the method to real\nand simulated data from a protein-signaling network.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 21:47:02 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zemplenyi", "Michele", "", "Harvard University"], ["Miller", "Jeffrey W.", "", "Harvard\n  University"]]}, {"id": "2103.15252", "submitter": "Zhou Lan", "authors": "Zhou Lan", "title": "Statistical Inference of Auto-correlated Eigenvalues with Applications\n  to Diffusion Tensor Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diffusion tensor imaging (DTI) is a prevalent neuroimaging tool in analyzing\nthe anatomical structure. The distinguishing feature of DTI is that the\nvoxel-wise variable is a 3x3 positive definite matrix other than a scalar,\ndescribing the diffusion process at the voxel. Recently, several statistical\nmethods have been proposed to analyze the DTI data. This paper focuses on the\nstatistical inference of eigenvalues of DTI because it provides more\ntransparent clinical interpretations. However, the statistical inference of\neigenvalues is statistically challenging because few treat these responses as\nrandom eigenvalues. In our paper, we rely on the distribution of the Wishart\nmatrix's eigenvalues to model the random eigenvalues. A hierarchical model\nwhich captures the eigenvalues' randomness and spatial auto-correlation is\nproposed to infer the local covariate effects. The Monte-Carlo\nExpectation-Maximization algorithm is implemented for parameter estimation.\nBoth simulation studies and application to IXI data-set are used to demonstrate\nour proposal. The results show that our proposal is more proper in analyzing\nauto-correlated random eigenvalues compared to alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 00:29:30 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lan", "Zhou", ""]]}, {"id": "2103.15281", "submitter": "Edward Kennedy", "authors": "Matteo Bonvini, Alan Mishler, Edward H. Kennedy", "title": "Comment on \"Statistical Modeling: The Two Cultures\" by Leo Breiman", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by Breiman's rousing 2001 paper on the \"two cultures\" in\nstatistics, we consider the role that different modeling approaches play in\ncausal inference. We discuss the relationship between model complexity and\ncausal (mis)interpretation, the relative merits of plug-in versus targeted\nestimation, issues that arise in tuning flexible estimators of causal effects,\nand some outstanding cultural divisions in causal inference.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 02:27:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bonvini", "Matteo", ""], ["Mishler", "Alan", ""], ["Kennedy", "Edward H.", ""]]}, {"id": "2103.15310", "submitter": "Jorge Ignacio Gonz\\'alez C\\'azares", "authors": "Jorge Ignacio Gonz\\'alez C\\'azares and Aleksandar Mijatovi\\'c", "title": "Monte Carlo algorithm for the extrema of tempered stable processes", "comments": "28 pages, 9 figures, video available on https://youtu.be/FJG6A3zk2lI", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel Monte Carlo algorithm for the vector consisting of the\nsupremum, the time at which the supremum is attained and the position of an\nexponentially tempered L\\'{e}vy process. The algorithm, based on the increments\nof the process without tempering, converges geometrically fast (as a function\nof the computational cost) for discontinuous and locally Lipschitz functions of\nthe vector. We prove that the corresponding multilevel Monte Carlo estimator\nhas optimal computational complexity (i.e. of order $\\epsilon^{-2}$ if the mean\nsquared error is at most $\\epsilon^{2}$) and provide its central limit theorem\n(CLT). Using the CLT we construct confidence intervals for barrier option\nprices and various risk measures based on drawdown under the tempered stable\n(CGMY) model calibrated/estimated on real-world data. We provide non-asymptotic\nand asymptotic comparisons of our algorithm with existing approximations,\nleading to rule-of-thumb guidelines for users to the best method for a given\nset of parameters, and illustrate its performance with numerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 03:34:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["C\u00e1zares", "Jorge Ignacio Gonz\u00e1lez", ""], ["Mijatovi\u0107", "Aleksandar", ""]]}, {"id": "2103.15311", "submitter": "Hongyuan Cao Prof", "authors": "Hongyuan Cao, Jun Chen and Xianyang Zhang", "title": "Optimal False Discovery Rate Control for Large Scale Multiple Testing\n  with Auxiliary Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Large-scale multiple testing is a fundamental problem in high dimensional\nstatistical inference. It is increasingly common that various types of\nauxiliary information, reflecting the structural relationship among the\nhypotheses, are available. Exploiting such auxiliary information can boost\nstatistical power. To this end, we propose a framework based on a two-group\nmixture model with varying probabilities of being null for different hypotheses\na priori, where a shape-constrained relationship is imposed between the\nauxiliary information and the prior probabilities of being null. An optimal\nrejection rule is designed to maximize the expected number of true positives\nwhen average false discovery rate is controlled. Focusing on the ordered\nstructure, we develop a robust EM algorithm to estimate the prior probabilities\nof being null and the distribution of $p$-values under the alternative\nhypothesis simultaneously. We show that the proposed method has better power\nthan state-of-the-art competitors while controlling the false discovery rate,\nboth empirically and theoretically. Extensive simulations demonstrate the\nadvantage of the proposed method. Datasets from genome-wide association studies\nare used to illustrate the new methodology.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 03:41:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Cao", "Hongyuan", ""], ["Chen", "Jun", ""], ["Zhang", "Xianyang", ""]]}, {"id": "2103.15394", "submitter": "Claudia Di Caterina", "authors": "Claudia Di Caterina, Nancy Reid, Nicola Sartori", "title": "Accurate directional inference in Gaussian graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directional tests to compare nested parametric models are developed in the\ngeneral context of covariance selection for Gaussian graphical models. The\nexactness of the underlying saddlepoint approximation leads to exceptional\naccuracy of the proposed approach. This is verified by simulation experiments\nwith high-dimensional parameters of interest, where the accuracy of standard\nasymptotic approximations to the likelihood ratio test and some of its\nhigher-order modifications fails. The directional p-value isused to illustrate\nthe assessment of Markovian dependencies in a dataset from a veterinary trial\non cattle. A second example with microarray data shows how to select the graph\nstructure related to genetic anomalies due to acute lymphocytic leukemia.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 07:42:53 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Di Caterina", "Claudia", ""], ["Reid", "Nancy", ""], ["Sartori", "Nicola", ""]]}, {"id": "2103.15426", "submitter": "Shayan Hundrieser", "authors": "Shayan Hundrieser, Marcel Klatt, Axel Munk", "title": "The Statistics of Circular Optimal Transport", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical optimal transport (OT) plans and distances provide effective tools\nto compare and statistically match probability measures defined on a given\nground space. Fundamental to this are distributional limit laws and we derive a\ncentral limit theorem for the empirical OT distance of circular data. Our limit\nresults require only mild assumptions in general and include prominent examples\nsuch as the von Mises or wrapped Cauchy family. Most notably, no assumptions\nare required when data are sampled from the probability measure to be compared\nwith, which is in strict contrast to the real line. A bootstrap principle\nfollows immediately as our proof relies on Hadamard differentiability of the OT\nfunctional. This paves the way for a variety of statistical inference tasks and\nis exemplified for asymptotic OT based goodness of fit testing for circular\ndistributions. We discuss numerical implementation, consistency and investigate\nits statistical power. For testing uniformity, it turns out that this approach\nperforms particularly well for unimodal alternatives and is almost as powerful\nas Rayleigh's test, the most powerful invariant test for von Mises\nalternatives. For regimes with many modes the circular OT test is less powerful\nwhich is explained by the shape of the corresponding transport plan.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 08:48:35 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Hundrieser", "Shayan", ""], ["Klatt", "Marcel", ""], ["Munk", "Axel", ""]]}, {"id": "2103.15540", "submitter": "Johan Pensar", "authors": "Johan Pensar and Henrik Nyman and Jukka Corander", "title": "Structure Learning of Contextual Markov Networks using Marginal\n  Pseudo-likelihood", "comments": null, "journal-ref": "Scandinavian Journal of Statistics, Vol. 44: 455-479, 2017", "doi": "10.1111/sjos.12260", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov networks are popular models for discrete multivariate systems where\nthe dependence structure of the variables is specified by an undirected graph.\nTo allow for more expressive dependence structures, several generalizations of\nMarkov networks have been proposed. Here we consider the class of contextual\nMarkov networks which takes into account possible context-specific\nindependences among pairs of variables. Structure learning of contextual Markov\nnetworks is very challenging due to the extremely large number of possible\nstructures. One of the main challenges has been to design a score, by which a\nstructure can be assessed in terms of model fit related to complexity, without\nassuming chordality. Here we introduce the marginal pseudo-likelihood as an\nanalytically tractable criterion for general contextual Markov networks. Our\ncriterion is shown to yield a consistent structure estimator. Experiments\ndemonstrate the favorable properties of our method in terms of predictive\naccuracy of the inferred models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:13:15 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Pensar", "Johan", ""], ["Nyman", "Henrik", ""], ["Corander", "Jukka", ""]]}, {"id": "2103.15671", "submitter": "Edwin Fong", "authors": "Edwin Fong, Chris Holmes, Stephen G. Walker", "title": "Martingale Posterior Distributions", "comments": "60 pages, 22 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prior distribution on parameters of a likelihood is the usual starting\npoint for Bayesian uncertainty quantification. In this paper, we present a\ndifferent perspective. Given a finite data sample $Y_{1:n}$ of size $n$ from an\ninfinite population, we focus on the missing $Y_{n+1:\\infty}$ as the source of\nstatistical uncertainty, with the parameter of interest being known precisely\ngiven $Y_{1:\\infty}$. We argue that the foundation of Bayesian inference is to\nassign a predictive distribution on $Y_{n+1:\\infty}$ conditional on $Y_{1:n}$,\nwhich then induces a distribution on the parameter of interest. Demonstrating\nan application of martingales, Doob shows that choosing the Bayesian predictive\ndistribution returns the conventional posterior as the distribution of the\nparameter. Taking this as our cue, we relax the predictive machine, avoiding\nthe need for the predictive to be derived solely from the usual prior to\nposterior to predictive density formula. We introduce the martingale posterior\ndistribution, which returns Bayesian uncertainty directly on any statistic of\ninterest without the need for the likelihood and prior, and this distribution\ncan be sampled through a computational scheme we name predictive resampling. To\nthat end, we introduce new predictive methodologies for multivariate density\nestimation, regression and classification that build upon recent work on\nbivariate copulas.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:49:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Fong", "Edwin", ""], ["Holmes", "Chris", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2103.15678", "submitter": "Abdenbi El Azri Azri", "authors": "Nafidi Ahmed and El Azri Abdenbi", "title": "Inference in the stochastic Cox-Ingersol-Ross diffusion process with\n  continuous sampling: Computational aspects and simulation", "comments": "6 pages with 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider a stochastic model based on the Cox- Ingersoll-\nRoss model (CIR). The stochastic model is parameterized analytically by\napplying It\\^o's calculus and the trend functions of the proposed process is\ncalculated. The parameter estimators are then derived by means of two\nprocedures: the first is used to estimate the parameters in the drift\ncoefficient by the maximum likelihood (ML) method, based on continuous\nsampling, and the second procedure approximates the diffusion coefficient by\ntwo methods. Finally, a simulation of the process is presented. Thus, a typical\nsimulated trajectory of the process and its estimators is obtained.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:02:59 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ahmed", "Nafidi", ""], ["Abdenbi", "El Azri", ""]]}, {"id": "2103.15687", "submitter": "Yi Zhao", "authors": "Yi Zhao and Lexin Li", "title": "Multimodal Data Integration via Mediation Analysis with High-Dimensional\n  Exposures and Mediators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by an imaging proteomics study for Alzheimer's disease (AD), in\nthis article, we propose a mediation analysis approach with high-dimensional\nexposures and high-dimensional mediators to integrate data collected from\nmultiple platforms. The proposed method combines principal component analysis\nwith penalized least squares estimation for a set of linear structural equation\nmodels. The former reduces the dimensionality and produces uncorrelated linear\ncombinations of the exposure variables, whereas the latter achieves\nsimultaneous path selection and effect estimation while allowing the mediators\nto be correlated. Applying the method to the AD data identifies numerous\ninteresting protein peptides, brain regions, and protein-structure-memory\npaths, which are in accordance with and also supplement existing findings of AD\nresearch. Additional simulations further demonstrate the effective empirical\nperformance of the method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:20:35 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhao", "Yi", ""], ["Li", "Lexin", ""]]}, {"id": "2103.15704", "submitter": "Marcos Matabuena", "authors": "Marcos Matabuena, Sherveen Riazati, Nick Caplan and Phil Hayes", "title": "Are Multilevel functional models the next step in sports biomechanics\n  and wearable technology? A case study of Knee Biomechanics patterns in\n  typical training sessions of recreational runners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper illustrates how multilevel functional models can detect and\ncharacterize biomechanical changes along different sport training sessions. Our\nanalysis focuses on the relevant cases to identify differences in knee\nbiomechanics in recreational runners during low and high-intensity exercise\nsessions with the same energy expenditure by recording $20$ steps. To do so, we\nreview the existing literature of multilevel models, and then, we propose a new\nhypothesis test to look at the changes between different levels of the\nmultilevel model as low and high-intensity training sessions. We also evaluate\nthe reliability of measures recorded in three-dimension knee angles from the\nfunctional intra-class correlation coefficient (ICC) obtained from the\ndecomposition performed with the multilevel funcional model taking into account\n$20$ measures recorded in each test. The results show that there are no\nstatistically significant differences between the two modes of exercise.\nHowever, we have to be careful with the conclusions since, as we have shown,\nhuman gait-patterns are very individual and heterogeneous between groups of\nathletes, and other alternatives to the p-value may be more appropriate to\ndetect statistical differences in biomechanical changes in this context.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:43:09 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 22:19:27 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Matabuena", "Marcos", ""], ["Riazati", "Sherveen", ""], ["Caplan", "Nick", ""], ["Hayes", "Phil", ""]]}, {"id": "2103.15919", "submitter": "Max Goplerud", "authors": "Max Goplerud", "title": "Modelling Heterogeneity Using Bayesian Structured Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to estimate heterogeneity, e.g. the effect of some variable differing\nacross observations, is a key question in political science. Methods for doing\nso make simplifying assumptions about the underlying nature of the\nheterogeneity to draw reliable inferences. This paper allows a common way of\nsimplifying complex phenomenon (placing observations with similar effects into\ndiscrete groups) to be integrated into regression analysis. The framework\nallows researchers to (i) use their prior knowledge to guide which groups are\npermissible and (ii) appropriately quantify uncertainty. The paper does this by\nextending work on \"structured sparsity\" from a traditional penalized likelihood\napproach to a Bayesian one by deriving new theoretical results and inferential\ntechniques. It shows that this method outperforms state-of-the-art methods for\nestimating heterogeneous effects when the underlying heterogeneity is grouped\nand more effectively identifies groups of observations with different effects\nin observational data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:54:25 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Goplerud", "Max", ""]]}, {"id": "2103.16036", "submitter": "Zhenghao Zeng", "authors": "Zhenghao Zeng, Yuqi Gu, Gongjun Xu", "title": "A Tensor-EM Method for Large-Scale Latent Class Analysis with Clustering\n  Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent class models are powerful statistical modeling tools widely used in\npsychological, behavioral, and social sciences. In the modern era of data\nscience, researchers often have access to response data collected from\nlarge-scale surveys or assessments, featuring many items (large J) and many\nsubjects (large N). This is in contrary to the traditional regime with fixed J\nand large N. To analyze such large-scale data, it is important to develop\nmethods that are both computationally efficient and theoretically valid. In\nterms of computation, the conventional EM algorithm for latent class models\ntends to have a slow algorithmic convergence rate for large-scale data and may\nconverge to some local optima instead of the maximum likelihood estimator\n(MLE). Motivated by this, we introduce the tensor decomposition perspective\ninto latent class analysis. Methodologically, we propose to use a moment-based\ntensor power method in the first step, and then use the obtained estimators as\ninitialization for the EM algorithm in the second step. Theoretically, we\nestablish the clustering consistency of the MLE in assigning subjects into\nlatent classes when N and J both go to infinity. Simulation studies suggest\nthat the proposed tensor-EM pipeline enjoys both good accuracy and\ncomputational efficiency for large-scale data. We also apply the proposed\nmethod to a personality dataset as an illustration.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 02:43:32 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zeng", "Zhenghao", ""], ["Gu", "Yuqi", ""], ["Xu", "Gongjun", ""]]}, {"id": "2103.16041", "submitter": "Arindam Fadikar", "authors": "Arindam Fadikar, Stefan M. Wild, Jonas Chaves-Montero", "title": "Scalable Statistical Inference of Photometric Redshift via Data\n  Subsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Handling big data has largely been a major bottleneck in traditional\nstatistical models. Consequently, when accurate point prediction is the primary\ntarget, machine learning models are often preferred over their statistical\ncounterparts for bigger problems. But full probabilistic statistical models\noften outperform other models in quantifying uncertainties associated with\nmodel predictions. We develop a data-driven statistical modeling framework that\ncombines the uncertainties from an ensemble of statistical models learned on\nsmaller subsets of data carefully chosen to account for imbalances in the input\nspace. We demonstrate this method on a photometric redshift estimation problem\nin cosmology, which seeks to infer a distribution of the redshift -- the\nstretching effect in observing the light of far-away galaxies -- given\nmultivariate color information observed for an object in the sky. Our proposed\nmethod performs balanced partitioning, graph-based data subsampling across the\npartitions, and training of an ensemble of Gaussian process models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 02:49:50 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 14:52:37 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Fadikar", "Arindam", ""], ["Wild", "Stefan M.", ""], ["Chaves-Montero", "Jonas", ""]]}, {"id": "2103.16048", "submitter": "Leah F. South", "authors": "Leah F. South, Marina Riabiz, Onur Teymur, Chris. J. Oates", "title": "Post-Processing of MCMC", "comments": "Version 2 has an updated description of burn-in removal and some\n  additional content on control variates. When citing this paper, please use\n  the following: South, LF, Riabiz, M, Teymur, O & Oates, CJ. 2022.\n  Post-Processing of MCMC. Annual Review of Statistics and Its Application. 9:\n  Submitted. DOI: 10.1146/annurev-statistics-040220-091727", "journal-ref": null, "doi": "10.1146/annurev-statistics-040220-091727", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is the engine of modern Bayesian statistics,\nbeing used to approximate the posterior and derived quantities of interest.\nDespite this, the issue of how the output from a Markov chain is post-processed\nand reported is often overlooked. Convergence diagnostics can be used to\ncontrol bias via burn-in removal, but these do not account for (common)\nsituations where a limited computational budget engenders a bias-variance\ntrade-off. The aim of this article is to review state-of-the-art techniques for\npost-processing Markov chain output. Our review covers methods based on\ndiscrepancy minimisation, which directly address the bias-variance trade-off,\nas well as general-purpose control variate methods for approximating expected\nquantities of interest.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 03:29:58 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 00:41:45 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["South", "Leah F.", ""], ["Riabiz", "Marina", ""], ["Teymur", "Onur", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2103.16125", "submitter": "Giovanna Jona Lasinio Prof.", "authors": "Sara Martino, Daniela Silvia Pace, Stefano Moro, Edoardo Casoli,\n  Daniele Ventura, Alessandro Frachea, Margherita Silvestri, Antonella\n  Arcangeli, Giancarlo Giacomini, Giandomenico Ardizzone, Giovanna Jona Lasinio", "title": "Integration of presence-only data from several sources. A case study on\n  dolphins' spatial distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Presence-only data are a typical occurrence in species distribution modeling.\nThey include the presence locations and no information on the absence. Their\nmodeling usually does not account for detection biases. In this work, we aim to\nmerge three different sources of information to model the presence of marine\nmammals. The approach is fully general and it is applied to two species of\ndolphins in the Central Tyrrhenian Sea (Italy) as a case study. Data come from\nthe Italian Environmental Protection Agency (ISPRA) and Sapienza University of\nRome research campaigns, and from a careful selection of social media (SM)\nimages and videos. We build a Log Gaussian Cox process where different\ndetection functions describe each data source. For the SM data, we analyze\nseveral choices that allow accounting for detection biases. Our findings allow\nfor a correct understanding of Stenella coeruleoalba and Tursiops truncatus\ndistribution in the study area. The results prove that the proposed approach is\nbroadly applicable, it can be widely used, and it is easily implemented in the\nR software using INLA and inlabru. We provide examples' code with simulated\ndata in the supplementary materials.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 07:22:03 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Martino", "Sara", ""], ["Pace", "Daniela Silvia", ""], ["Moro", "Stefano", ""], ["Casoli", "Edoardo", ""], ["Ventura", "Daniele", ""], ["Frachea", "Alessandro", ""], ["Silvestri", "Margherita", ""], ["Arcangeli", "Antonella", ""], ["Giacomini", "Giancarlo", ""], ["Ardizzone", "Giandomenico", ""], ["Lasinio", "Giovanna Jona", ""]]}, {"id": "2103.16128", "submitter": "Subhankar Dutta", "authors": "Subhankar Dutta, Suchandan Kayal", "title": "Analysis of the improved adaptive type-II progressive censoring based on\n  competing risk data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a competing risk model is analyzed based on the improved\nadaptive type-II progressive censored sample (IAT-II PCS). Only two competing\ncauses of failures from independent exponential distributions are considered.\nMaximum likelihood estimates (MLEs) for the unknown model parameters are\nobtained. By using the asymptotic normality properties of the MLEs, the\napproximate confidence intervals are constructed. The existence and uniqueness\nof the MLEs are studied. The Bayes estimates are obtained under the symmetric\nand asymmetric loss functions with non-informative and independent gamma prior\ndistributions. Further, the highest posterior density (HPD) credible intervals\nare obtained by using the MCMC technique. Coverage probabilities for each\nconfidence interval are computed. A Monte Carlo simulation study is carried out\nto compare the performance of the proposed estimates. Finally, a real data set\nis considered for illustrative purposes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 07:32:28 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Dutta", "Subhankar", ""], ["Kayal", "Suchandan", ""]]}, {"id": "2103.16157", "submitter": "Fotios Petropoulos", "authors": "Fotios Petropoulos, Evangelos Spiliotis and Anastasios Panagiotelis", "title": "Model combinations through revised base-rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard selection criteria for forecasting models focus on information that\nis calculated for each series independently, disregarding the general\ntendencies and performances of the candidate models. In this paper, we propose\na new way to statistical model selection and model combination that\nincorporates the base-rates of the candidate forecasting models, which are then\nrevised so that the per-series information is taken into account. We examine\ntwo schemes that are based on the precision and sensitivity information from\nthe contingency table of the base rates. We apply our approach on pools of\nexponential smoothing models and a large number of real time series and we show\nthat our schemes work better than standard statistical benchmarks. We discuss\nthe connection of our approach to other cross-learning approaches and offer\ninsights regarding implications for theory and practice.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:30:59 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 18:54:07 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Petropoulos", "Fotios", ""], ["Spiliotis", "Evangelos", ""], ["Panagiotelis", "Anastasios", ""]]}, {"id": "2103.16159", "submitter": "Yang Cao", "authors": "Yang Cao, Xinwei Sun, Yuan Yao", "title": "Controlling the False Discovery Rate in Structural Sparsity: Split\n  Knockoffs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling the False Discovery Rate (FDR) in a variable selection procedure\nis critical for reproducible discoveries, which receives an extensive study in\nsparse linear models. However, in many scenarios, the sparsity constraint is\nnot directly imposed on the parameters, but on a linear transformation of the\nparameters to be estimated. Examples can be found in total variations, wavelet\ntransforms, fused LASSO, and trend filtering, etc. In this paper, we proposed a\ndata adaptive FDR control in this structural sparsity setting, the Split\nKnockoff method. The proposed scheme relaxes the linear subspace constraint to\nits neighborhood, often known as variable splitting in optimization, that\nenjoys new statistical benefits. It yields orthogonal design and split knockoff\nmatrices, that exhibit desired FDR control empirically in structural sparsity\ndiscovery, and improve the power of strong feature selection by enhancing the\nincoherence condition for model selection consistency. Yet, the split knockoff\nstatistics fail to satisfy the exchangeability, a crucial property in the\nclassical knockoff method for provable FDR control. To address this challenge,\nwe introduce an almost supermartingale construction under a perturbation of\nexchangeability, that enables us to establish FDR control up to an arbitrarily\nsmall inflation that vanishes as the relaxed neighborhood enlarges. Simulation\nexperiments show the effectiveness of split knockoffs with possible\nimprovements over knockoffs in both FDR control and Power. An application to\nAlzheimer's Disease study with MRI data demonstrates that the split knockoff\nmethod can disclose important lesion regions in brains associated with the\ndisease and connections between neighboring regions of high contrast variations\nduring disease progression.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:39:42 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 12:04:43 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Cao", "Yang", ""], ["Sun", "Xinwei", ""], ["Yao", "Yuan", ""]]}, {"id": "2103.16244", "submitter": "Sean Pinkney", "authors": "Sean Pinkney", "title": "An Improved and Extended Bayesian Synthetic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An improved and extended Bayesian synthetic control model is presented,\nexpanding upon the latent factor model in Tuomaala 2019. The changes we make\ninclude 1) standardization of the data prior to model fit - which improves\nefficiency and generalization across different data sets; 2) adding time\nvarying covariates; 3) adding the ability to have multiple treated units; 4)\nfitting the latent factors within the Bayesian model; and, 5) a sparsity\ninducing prior to automatically tune the number of latent factors. We\ndemonstrate the similarity of estimates to two traditional synthetic control\nstudies in Abadie, Diamond, and Hainmueller 2010 and Abadie, Diamond, and\nHainmueller 2015 and extend to multiple target series with a new example of\nestimating digital website visitation from changes in data collection due to\ndigital privacy laws.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 10:48:14 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Pinkney", "Sean", ""]]}, {"id": "2103.16252", "submitter": "Hein Putter", "authors": "Hein Putter, Hans C. van Houwelingen", "title": "Landmarking 2.0: Bridging the gap between joint models and landmarking", "comments": "24 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of dynamic prediction with time-dependent covariates, given by\nbiomarkers, repeatedly measured over time, has received much attention over the\nlast decades. Two contrasting approaches have become in widespread use. The\nfirst is joint modelling, which attempts to jointly model the longitudinal\nmarkers and the event time. The second is landmarking, a more pragmatic\napproach that avoids modelling the marker process. Landmarking has been shown\nto be less efficient than correctly specified joint models in simulation\nstudies, when data are generated from the joint model. When the mean model is\nmisspecified, however, simulation has shown that joint models may be inferior\nto landmarking.\n  The objective of this paper is to develop methods that improve the predictive\naccuracy of landmarking, while retaining its relative simplicity and\nrobustness. We start by fitting a working longitudinal model for the biomarker,\nincluding a temporal correlation structure. Based on that model, we derive a\npredictable time-dependent process representing the expected value of the\nbiomarker after the landmark time, and we fit a time-dependent Cox model based\non the predictable time-dependent covariate. Dynamic predictions based on this\napproach for new patients can be obtained by first deriving the expected values\nof the biomarker, given the measured values before the landmark time point, and\nthen calculating the predicted probabilities based on the time-dependent Cox\nmodel.\n  We illustrate the approach in predicting overall survival in liver cirrhosis\npatients based on prothrombin index.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 11:06:39 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Putter", "Hein", ""], ["van Houwelingen", "Hans C.", ""]]}, {"id": "2103.16336", "submitter": "Ranjan Maitra", "authors": "Emily M. Goren and Ranjan Maitra", "title": "Model-based clustering of partial records", "comments": "18 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.HE cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially recorded data are frequently encountered in many applications and\nusually clustered by first removing incomplete cases or features with missing\nvalues, or by imputing missing values, followed by application of a clustering\nalgorithm to the resulting altered dataset. Here, we develop clustering\nmethodology through a model-based approach using the marginal density for the\nobserved values, assuming a finite mixture model of multivariate $t$\ndistributions. We compare our approximate algorithm to the corresponding full\nexpectation-maximization (EM) approach that considers the missing values in the\nincomplete data set and makes a missing at random (MAR) assumption, as well as\ncase deletion and imputation methods. Since only the observed values are\nutilized, our approach is computationally more efficient than imputation or\nfull EM. Simulation studies demonstrate that our approach has favorable\nrecovery of the true cluster partition compared to case deletion and imputation\nunder various missingness mechanisms, and is at least competitive with the full\nEM approach, even when MAR assumptions are violated. Our methodology is\ndemonstrated on a problem of clustering gamma-ray bursts and is implemented at\nhttps://github.com/emilygoren/MixtClust.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:30:59 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 21:18:14 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 12:54:29 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Goren", "Emily M.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2103.16387", "submitter": "Carlo Romano Marcello Alessandro Santagiustina", "authors": "Carlo Santagiustina and Massimo Warglien", "title": "The Unfolding Structure of Arguments in Online Debates: The case of a\n  No-Deal Brexit", "comments": "Main article (18 pages, 7 figures) & Supplementary material (25\n  pages, 7 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the last decade, political debates have progressively shifted to social\nmedia. Rhetorical devices employed by online actors and factions that operate\nin these debating arenas can be captured and analysed to conduct a statistical\nreading of societal controversies and their argumentation dynamics. In this\npaper, we propose a five-step methodology, to extract, categorize and explore\nthe latent argumentation structures of online debates. Using Twitter data about\na \"no-deal\" Brexit, we focus on the expected effects in case of materialisation\nof this event. First, we extract cause-effect claims contained in tweets using\nRegEx that exploit verbs related to Creation, Destruction and Causation.\nSecond, we categorise extracted \"no-deal\" effects using a Structural Topic\nModel estimated on unigrams and bigrams. Third, we select controversial effect\ntopics and explore within-topic argumentation differences between self-declared\npartisan user factions. We hence type topics using estimated covariate effects\non topic propensities, then, using the topics correlation network, we study the\ntopological structure of the debate to identify coherent topical\nconstellations. Finally, we analyse the debate time dynamics and infer\nlead/follow relations among factions. Results show that the proposed\nmethodology can be employed to perform a statistical rhetorics analysis of\ndebates, and map the architecture of controversies across time. In particular,\nthe \"no-deal\" Brexit debate is shown to have an assortative argumentation\nstructure heavily characterized by factional constellations of arguments, as\nwell as by polarized narrative frames invoked through verbs related to Creation\nand Destruction. Our findings highlight the benefits of implementing a systemic\napproach to the analysis of debates, which allows the unveiling of topical and\nfactional dependencies between arguments employed in online debates.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 12:29:43 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Santagiustina", "Carlo", ""], ["Warglien", "Massimo", ""]]}, {"id": "2103.16438", "submitter": "Hang Yu", "authors": "Hang Yu, Yuanjia Wang, Donglin Zeng", "title": "A General Framework of Nonparametric Feature Selection in\n  High-Dimensional Data", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nonparametric feature selection in high-dimensional data is an important and\nchallenging problem in statistics and machine learning fields. Most of the\nexisting methods for feature selection focus on parametric or additive models\nwhich may suffer from model misspecification. In this paper, we propose a new\nframework to perform nonparametric feature selection for both regression and\nclassification problems. In this framework, we learn prediction functions\nthrough empirical risk minimization over a reproducing kernel Hilbert space.\nThe space is generated by a novel tensor product kernel which depends on a set\nof parameters that determine the importance of the features. Computationally,\nwe minimize the empirical risk with a penalty to estimate the prediction and\nkernel parameters at the same time. The solution can be obtained by iteratively\nsolving convex optimization problems. We study the theoretical property of the\nkernel feature space and prove both the oracle selection property and the\nFisher consistency of our proposed method. Finally, we demonstrate the superior\nperformance of our approach compared to existing methods via extensive\nsimulation studies and application to a microarray study of eye disease in\nanimals.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:36:21 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Yu", "Hang", ""], ["Wang", "Yuanjia", ""], ["Zeng", "Donglin", ""]]}, {"id": "2103.16484", "submitter": "Federico Musciotto", "authors": "Federico Musciotto, Federico Battiston and Rosario N. Mantegna", "title": "Detecting informative higher-order interactions in statistically\n  validated hypergraphs", "comments": "pre-submission version, 10 pages and 3 figures + SI", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent empirical evidence has shown that in many real-world systems,\nsuccessfully represented as networks, interactions are not limited to dyads,\nbut often involve three or more agents at a time. These data are better\ndescribed by hypergraphs, where hyperlinks encode higher-order interactions\namong a group of nodes. In spite of the large number of works on networks,\nhighlighting informative hyperlinks in hypergraphs obtained from real world\ndata is still an open problem. Here we propose an analytic approach to filter\nhypergraphs by identifying those hyperlinks that are over-expressed with\nrespect to a random null hypothesis, and represent the most relevant\nhigher-order connections. We apply our method to a class of synthetic\nbenchmarks and to several datasets. For all cases, the method highlights\nhyperlinks that are more informative than those extracted with pairwise\napproaches. Our method provides a first way to obtain statistically validated\nhypergraphs, separating informative connections from redundant and noisy ones.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 16:42:01 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 10:45:58 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Musciotto", "Federico", ""], ["Battiston", "Federico", ""], ["Mantegna", "Rosario N.", ""]]}, {"id": "2103.16687", "submitter": "Olga Kaiser", "authors": "Olga Kaiser, Dimitri Igdalov, Olivia Martius, Illia Horenko", "title": "On Computationally-Scalable Spatio-Temporal Regression Clustering of\n  Precipitation Threshold Excesses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Focusing on regression based analysis of extremes in a presence of\nsystematically missing covariates, this work presents a data-driven\nspatio-temporal regression based clustering of threshold excesses. It is shown\nthat in a presence of systematically missing covariates the behavior of\nthreshold excesses becomes nonstationary and nonhomogenous. The presented\napproach describes this complex behavior by a set of local stationary\nGeneralized Pareto Distribution (GPD) models, where the parameters are\nexpressed as regression models, and a latent spatio-temporal switching process.\nThe spatio-temporal switching process is resolved by the nonparametric Finite\nElement Methodology for time series analysis with Bounded Variation of the\nmodel parameters (FEM-BV). The presented FEM-BV-GPD approach goes beyond strong\na priori assumptions made in standard latent class models like Mixture Models\nand Hidden Markov Models. In addition, it provides a pragmatic description of\nthe underlying dependency structure. The performance of the framework is\ndemonstrated on historical precipitation data for Switzerland and compared with\nthe results obtained by the standard methods on the same data.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 21:19:08 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 16:06:14 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kaiser", "Olga", ""], ["Igdalov", "Dimitri", ""], ["Martius", "Olivia", ""], ["Horenko", "Illia", ""]]}, {"id": "2103.16689", "submitter": "Wenshuo Guo", "authors": "Wenshuo Guo, Serena Wang, Peng Ding, Yixin Wang, Michael I. Jordan", "title": "Multi-Source Causal Inference Using Control Variates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many areas of machine learning have benefited from the increasing\navailability of large and varied datasets, the benefit to causal inference has\nbeen limited given the strong assumptions needed to ensure identifiability of\ncausal effects; these are often not satisfied in real-world datasets. For\nexample, many large observational datasets (e.g., case-control studies in\nepidemiology, click-through data in recommender systems) suffer from selection\nbias on the outcome, which makes the average treatment effect (ATE)\nunidentifiable. We propose a general algorithm to estimate causal effects from\n\\emph{multiple} data sources, where the ATE may be identifiable only in some\ndatasets but not others. The key idea is to construct control variates using\nthe datasets in which the ATE is not identifiable. We show theoretically that\nthis reduces the variance of the ATE estimate. We apply this framework to\ninference from observational data under outcome selection bias, assuming access\nto an auxiliary small dataset from which we can obtain a consistent estimate of\nthe ATE. We construct a control variate by taking the difference of the odds\nratio estimates from the two datasets. Across simulations and two case studies\nwith real data, we show that this control variate can significantly reduce the\nvariance of the ATE estimate.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 21:20:51 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 19:10:18 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Guo", "Wenshuo", ""], ["Wang", "Serena", ""], ["Ding", "Peng", ""], ["Wang", "Yixin", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2103.16810", "submitter": "Qingcan Wang", "authors": "Qingcan Wang and Weinan E", "title": "An Expectation-Maximization Algorithm for Continuous-time Hidden Markov\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified framework that extends the inference methods for\nclassical hidden Markov models to continuous settings, where both the hidden\nstates and observations occur in continuous time. Two different settings are\nanalyzed: hidden jump process with a finite state space, and hidden diffusion\nprocess with a continuous state space. For each setting, we first estimate the\nhidden states given the observations and model parameters, showing that the\nposterior distribution of the hidden states can be described by differential\nequations in continuous time. We then consider the estimation of unknown model\nparameters, deriving the continuous-time formulas for the\nexpectation-maximization algorithm. We also propose a Monte Carlo method based\non the continuous formulation, sampling the posterior distribution of the\nhidden states and updating the parameter estimation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 05:00:10 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 04:22:00 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wang", "Qingcan", ""], ["E", "Weinan", ""]]}, {"id": "2103.16948", "submitter": "Leonardo Egidi PhD", "authors": "Leonardo Egidi, Roberta Pappad\\`a, Francesco Pauli, Nicola Torelli", "title": "pivmet: Pivotal Methods for Bayesian Relabelling and k-Means Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The identification of groups' prototypes, i.e. elements of a dataset that\nrepresent different groups of data points, may be relevant to the tasks of\nclustering, classification and mixture modeling. The R package pivmet presented\nin this paper includes different methods for extracting pivotal units from a\ndataset. One of the main applications of pivotal methods is a Markov Chain\nMonte Carlo (MCMC) relabelling procedure to solve the label switching in\nBayesian estimation of mixture models. Each method returns posterior estimates,\nand a set of graphical tools for visualizing the output. The package offers\nJAGS and Stan sampling procedures for Gaussian mixtures, and allows for\nuser-defined priors' parameters. The package also provides functions to perform\nconsensus clustering based on pivotal units, which may allow to improve\nclassical techniques (e.g. k-means) by means of a careful seeding. The paper\nprovides examples of applications to both real and simulated datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 10:01:55 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Egidi", "Leonardo", ""], ["Pappad\u00e0", "Roberta", ""], ["Pauli", "Francesco", ""], ["Torelli", "Nicola", ""]]}, {"id": "2103.17146", "submitter": "Riccardo Rastelli", "authors": "Riccardo Rastelli and Marco Corneli", "title": "Continuous Latent Position Models for Instantaneous Interactions", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We create a framework to analyse the timing and frequency of instantaneous\ninteractions between pairs of entities. This type of interaction data is\nespecially common nowadays, and easily available. Examples of instantaneous\ninteractions include email networks, phone call networks and some common types\nof technological and transportation networks. Our framework relies on a novel\nextension of the latent position network model: we assume that the entities are\nembedded in a latent Euclidean space, and that they move along individual\ntrajectories which are continuous over time. These trajectories are used to\ncharacterize the timing and frequency of the pairwise interactions. We discuss\nan inferential framework where we estimate the individual trajectories from the\nobserved interaction data, and propose applications on artificial and real\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:10:58 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Rastelli", "Riccardo", ""], ["Corneli", "Marco", ""]]}, {"id": "2103.17240", "submitter": "Hernando Ombao", "authors": "Hernando Ombao and Marco Pinto", "title": "Spectral Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a general framework for modeling dependence in\nmultivariate time series. Its fundamental approach relies on decomposing each\nsignal in a system into various frequency components and then studying the\ndependence properties through these oscillatory activities.The unifying theme\nacross the paper is to explore the strength of dependence and possible lead-lag\ndynamics through filtering. The proposed framework is capable of representing\nboth linear and non-linear dependencies that could occur instantaneously or\nafter some delay(lagged dependence). Examples for studying dependence between\noscillations are illustrated through multichannel electroencephalograms. These\nexamples emphasized that some of the most prominent frequency domain measures\nsuch as coherence, partial coherence,and dual-frequency coherence can be\nderived as special cases under this general framework.This paper also\nintroduces related approaches for modeling dependence through phase-amplitude\ncoupling and causality of (one-sided) filtered signals.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:39:13 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ombao", "Hernando", ""], ["Pinto", "Marco", ""]]}]