[{"id": "1904.00136", "submitter": "Tyler McCormick", "authors": "Morgan Hardy, Rachel M. Heath, Wesley Lee, Tyler H. McCormick", "title": "Estimating spillovers using imprecisely measured networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many experimental contexts, whether and how network interactions impact\nthe outcome of interest for both treated and untreated individuals are key\nconcerns. Networks data is often assumed to perfectly represent these possible\ninteractions. This paper considers the problem of estimating treatment effects\nwhen measured connections are, instead, a noisy representation of the true\nspillover pathways. We show that existing methods, using the potential outcomes\nframework, yield biased estimators in the presence of this mismeasurement. We\ndevelop a new method, using a class of mixture models, that can account for\nmissing connections and discuss its estimation via the Expectation-Maximization\nalgorithm. We check our method's performance by simulating experiments on real\nnetwork data from 43 villages in India. Finally, we use data from a previously\npublished study to show that estimates using our method are more robust to the\nchoice of network measure.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 02:30:21 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 22:17:55 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 16:27:54 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Hardy", "Morgan", ""], ["Heath", "Rachel M.", ""], ["Lee", "Wesley", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "1904.00148", "submitter": "Daniel Spencer", "authors": "Daniel Spencer, Rajarshi Guhaniyogi, Raquel Prado", "title": "Bayesian Mixed Effect Sparse Tensor Response Regression Model with Joint\n  Estimation of Activation and Connectivity", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain activation and connectivity analyses in task-based functional magnetic\nresonance imaging (fMRI) experiments with multiple subjects are currently at\nthe forefront of data-driven neuroscience. In such experiments, interest often\nlies in understanding activation of brain voxels due to external stimuli and\nstrong association or connectivity between the measurements on a set of\npre-specified group of brain voxels, also known as regions of interest (ROI).\nThis article proposes a joint Bayesian additive mixed modeling framework that\nsimultaneously assesses brain activation and connectivity patterns from\nmultiple subjects. In particular, fMRI measurements from each individual\nobtained in the form of a multi-dimensional array/tensor at each time are\nregressed on functions of the stimuli. We impose a low-rank PARAFAC\ndecomposition on the tensor regression coefficients corresponding to the\nstimuli to achieve parsimony. Multiway stick breaking shrinkage priors are\nemployed to infer activation patterns and associated uncertainties in each\nvoxel. Further, the model introduces region specific random effects which are\njointly modeled with a Bayesian Gaussian graphical prior to account for the\nconnectivity among pairs of ROIs. Empirical investigations under various\nsimulation studies demonstrate the effectiveness of the method as a tool to\nsimultaneously assess brain activation and connectivity. The method is then\napplied to a multi-subject fMRI dataset from a balloon-analog risk-taking\nexperiment in order to make inference about how the brain processes risk.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 04:53:10 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Spencer", "Daniel", ""], ["Guhaniyogi", "Rajarshi", ""], ["Prado", "Raquel", ""]]}, {"id": "1904.00204", "submitter": "Anna Liu", "authors": "Runfei Luo, Anna Liu and Yuedong Wang", "title": "Combining Smoothing Spline with Conditional Gaussian Graphical Model for\n  Density and Graph Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate density estimation and graphical models play important roles in\nstatistical learning. The estimated density can be used to construct a\ngraphical model that reveals conditional relationships whereas a graphical\nstructure can be used to build models for density estimation. Our goal is to\nconstruct a consolidated framework that can perform both density and graph\nestimation. Denote $\\bm{Z}$ as the random vector of interest with density\nfunction $f(\\bz)$. Splitting $\\bm{Z}$ into two parts,\n$\\bm{Z}=(\\bm{X}^T,\\bm{Y}^T)^T$ and writing $f(\\bz)=f(\\bx)f(\\by|\\bx)$ where\n$f(\\bx)$ is the density function of $\\bm{X}$ and $f(\\by|\\bx)$ is the\nconditional density of $\\bm{Y}|\\bm{X}=\\bx$. We propose a semiparametric\nframework that models $f(\\bx)$ nonparametrically using a smoothing spline ANOVA\n(SS ANOVA) model and $f(\\by|\\bx)$ parametrically using a conditional Gaussian\ngraphical model (cGGM). Combining flexibility of the SS ANOVA model with\nsuccinctness of the cGGM, this framework allows us to deal with\nhigh-dimensional data without assuming a joint Gaussian distribution. We\npropose a backfitting estimation procedure for the cGGM with a computationally\nefficient approach for selection of tuning parameters. We also develop a\ngeometric inference approach for edge selection. We establish asymptotic\nconvergence properties for both the parameter and density estimation. The\nperformance of the proposed method is evaluated through extensive simulation\nstudies and two real data applications.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 12:02:13 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Luo", "Runfei", ""], ["Liu", "Anna", ""], ["Wang", "Yuedong", ""]]}, {"id": "1904.00340", "submitter": "Fred Lombard Prof", "authors": "F. Lombard and D.M. Hawkins", "title": "CUSUM ARL - Conditional or Unconditional?", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The behavior of CUSUM charts depends strongly on how they are initialized.\nRecent work has suggested that self-starting CUSUM methods retain some\ndependence on their very first readings, and introduced the concept of\n\"conditional average run length\" (CARL) -- the average run length conditioned\non the first few process readings -- as a result of which is it claimed that\ndifferent practitioners using the same methodology could experience different\nARLs because of the random differences in their earliest readings. We cast\ndoubt on whether CARL is relevant to practitioners who use self-starting\nmethods and argue that the unconditional ARL is the relevant measure there.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 04:52:59 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Lombard", "F.", ""], ["Hawkins", "D. M.", ""]]}, {"id": "1904.00364", "submitter": "Nicola Salvati", "authors": "Ray Chambers, Enrico Fabrizi, Nicola Salvati", "title": "Small Area Estimation with Linked Data", "comments": "29 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Small Area Estimation data linkage can be used to combine values of the\nvariableof interest from a national survey with values of auxiliary variables\nobtained from another source like a population register. Linkage errors can\ninduce bias when fitting regression models; moreover, they can create\nnon-representative outliers in the linked data in addition to the presence of\npotential representative outliers. In this paper we adopt a secondary analyst's\npoint view, assuming limited information is available on the linkage process,\nand we develop small area estimators based on linear mixed and linear\nM-quantile models to accommodate linked data containing a mix of both types of\noutliers. We illustrate the properties of these small area estimators, as well\nas estimators of their mean squared error, by means of model-based and\ndesign-based simulation experiments. These experiments show that the proposed\npredictors can lead to more efficient estimators when there is linkage error.\nFurthermore, the proposed mean-squared error estimation methods appear to\nperform well.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 09:19:10 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chambers", "Ray", ""], ["Fabrizi", "Enrico", ""], ["Salvati", "Nicola", ""]]}, {"id": "1904.00412", "submitter": "Wei Ling Katherine Tan", "authors": "W. Katherine Tan and Patrick J. Heagerty", "title": "Surrogate-guided sampling designs for classification of rare outcomes\n  from electronic medical records data", "comments": null, "journal-ref": null, "doi": "10.1093/biostatistics/kxaa028", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable and accurate identification of specific clinical outcomes has been\nenabled by machine-learning applied to electronic medical record (EMR) systems.\nThe development of classification models requires the collection of a complete\nlabeled data set, where true clinical outcomes are obtained by human expert\nmanual review. For example, the development of natural language processing\nalgorithms requires the abstraction of clinical text data to obtain outcome\ninformation necessary for training models. However, if the outcome is rare then\nsimple random sampling results in very few cases and insufficient information\nto develop accurate classifiers. Since large scale detailed abstraction is\noften expensive, time-consuming, and not feasible, more efficient strategies\nare needed. Under such resource constrained settings, we propose a class of\nenrichment sampling designs, where selection for abstraction is stratified by\nauxiliary variables related to the true outcome of interest. Stratified\nsampling on highly specific variables results in targeted samples that are more\nenriched with cases, which we show translates to increased model discrimination\nand better statistical learning performance. We provide mathematical details,\nand simulation evidence that links sampling designs to their resulting\nprediction model performance. We discuss the impact of our proposed sampling on\nboth model training and validation. Finally, we illustrate the proposed designs\nfor outcome label collection and subsequent machine-learning, using radiology\nreport text data from the Lumbar Imaging with Reporting of Epidemiology (LIRE)\nstudy.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 13:59:55 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 01:16:59 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Tan", "W. Katherine", ""], ["Heagerty", "Patrick J.", ""]]}, {"id": "1904.00495", "submitter": "Weining Shen", "authors": "Wei Hu, Tianyu Pan, Dehan Kong, Weining Shen", "title": "Nonparametric Matrix Response Regression with Application to Brain\n  Imaging Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of neuroimaging technologies, a great effort has been\ndedicated recently to investigate the dynamic changes in brain activity.\nExamples include time course calcium imaging and dynamic brain functional\nconnectivity. In this paper, we propose a novel nonparametric matrix response\nregression model to characterize the nonlinear association between 2D image\noutcomes and predictors such as time and patient information. Our estimation\nprocedure can be formulated as a nuclear norm regularization problem, which can\ncapture the underlying low-rank structure of the dynamic 2D images. We present\na computationally efficient algorithm, derive the asymptotic theory and show\nthat the method outperforms other existing approaches in simulations. We then\napply the proposed method to a calcium imaging study for estimating the change\nof fluorescent intensities of neurons, and an electroencephalography study for\na comparison in the dynamic connectivity covariance matrices between alcoholic\nand control individuals. For both studies, the method leads to a substantial\nimprovement in prediction error.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 22:12:57 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 13:42:07 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2020 06:11:44 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Hu", "Wei", ""], ["Pan", "Tianyu", ""], ["Kong", "Dehan", ""], ["Shen", "Weining", ""]]}, {"id": "1904.00521", "submitter": "Jeremiah Zhe Liu", "authors": "Jeremiah Zhe Liu, John Paisley, Marianthi-Anna Kioumourtzoglou, Brent\n  A. Coull", "title": "Adaptive Ensemble Learning of Spatiotemporal Processes with Calibrated\n  Predictive Uncertainty: A Bayesian Nonparametric Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ensemble learning is a mainstay in modern data science practice. Conventional\nensemble algorithms assign to base models a set of deterministic, constant\nmodel weights that (1) do not fully account for individual models' varying\naccuracy across data subgroups, nor (2) provide uncertainty estimates for the\nensemble prediction. These shortcomings can yield predictions that are precise\nbut biased, which can negatively impact the performance of the algorithm in\nreal-word applications. In this work, we present an adaptive, probabilistic\napproach to ensemble learning using a transformed Gaussian process as a prior\nfor the ensemble weights. Given input features, our method optimally combines\nbase models based on their predictive accuracy in the feature space, and\nprovides interpretable estimates of the uncertainty associated with both model\nselection, as reflected by the ensemble weights, and the overall ensemble\npredictions. Furthermore, to ensure that this quantification of the model\nuncertainty is accurate, we propose additional machinery to non-parametrically\nmodel the ensemble's predictive cumulative density function (CDF) so that it is\nconsistent with the empirical distribution of the data. We apply the proposed\nmethod to data simulated from a nonlinear regression model, and to generate a\nspatial prediction model and associated prediction uncertainties for fine\nparticle levels in eastern Massachusetts, USA.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 01:03:57 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Liu", "Jeremiah Zhe", ""], ["Paisley", "John", ""], ["Kioumourtzoglou", "Marianthi-Anna", ""], ["Coull", "Brent A.", ""]]}, {"id": "1904.00555", "submitter": "Abhijit Mandal", "authors": "Anirban Mondal, Abhijit Mandal", "title": "Stratified Random Sampling for Dependent Inputs", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.jspi.2019.08.001", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach of obtaining stratified random samples from statistically\ndependent random variables is described. The proposed method can be used to\nobtain samples from the input space of a computer forward model in estimating\nexpectations of functions of the corresponding output variables. The advantage\nof the proposed method over the existing methods is that it preserves the exact\nform of the joint distribution on the input variables. The asymptotic\ndistribution of the new estimator is derived. Asymptotically, the variance of\nthe estimator using the proposed method is less than that obtained using the\nsimple random sampling, with the degree of variance reduction depending on the\ndegree of additivity in the function being integrated. This technique is\napplied to a practical example related to the performance of the river flood\ninundation model.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 04:17:12 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Mondal", "Anirban", ""], ["Mandal", "Abhijit", ""]]}, {"id": "1904.00670", "submitter": "Borislav Ikonomov", "authors": "Borislav Ikonomov, Michael U. Gutmann", "title": "Robust Optimisation Monte Carlo", "comments": "8 pages + 6 page appendix; v2: made clarifications, added a second\n  possible algorithm implementation and its results; v3: small clarifications,\n  to be published in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is on Bayesian inference for parametric statistical models that\nare defined by a stochastic simulator which specifies how data is generated.\nExact sampling is then possible but evaluating the likelihood function is\ntypically prohibitively expensive. Approximate Bayesian Computation (ABC) is a\nframework to perform approximate inference in such situations. While basic ABC\nalgorithms are widely applicable, they are notoriously slow and much research\nhas focused on increasing their efficiency. Optimisation Monte Carlo (OMC) has\nrecently been proposed as an efficient and embarrassingly parallel method that\nleverages optimisation to accelerate the inference. In this paper, we\ndemonstrate an important previously unrecognised failure mode of OMC: It\ngenerates strongly overconfident approximations by collapsing regions of\nsimilar or near-constant likelihood into a single point. We propose an\nefficient, robust generalisation of OMC that corrects this. It makes fewer\nassumptions, retains the main benefits of OMC, and can be performed either as\npost-processing to OMC or as a stand-alone computation. We demonstrate the\neffectiveness of the proposed Robust OMC on toy examples and tasks in\ninverse-graphics where we perform Bayesian inference with a complex image\nrenderer.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:50:41 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 15:54:56 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 13:45:56 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Ikonomov", "Borislav", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "1904.00679", "submitter": "Joris Mulder", "authors": "J. Mulder, H. Hoijtink, and X. Gu", "title": "Default Bayesian Model Selection of Constrained Multivariate Normal\n  Linear Models", "comments": "31 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate normal linear model is one of the most widely employed\nmodels for statistical inference in applied research. Special cases include\n(multivariate) t testing, (M)AN(C)OVA, (multivariate) multiple regression, and\nrepeated measures analysis. Statistical procedures for model selection where\nthe models may have equality and order constraints on the model parameters of\ninterest are limited however. This paper presents a default Bayes factor for\nthis model selection problem. The default Bayes factor is based on generalized\nfractional Bayes methodology where different fractions are used for different\nobservations and where the default prior is centered on the boundary of the\nconstrained space under investigation. First, the method is fully automatic and\ntherefore can be applied when prior information is weak or completely\nunavailable. Second, using group specific fractions, the same amount of\ninformation is used from each group resulting in a minimally informative\ndefault prior having a matrix Cauchy distribution, resulting in a consistent\ndefault Bayes factor. Third, numerical computation can be done using\nparallelization which makes it computationally cheap. Fourth, the evidence can\nbe updated in a relatively simple manner when observing new data. Fifth, the\nselection criterion can be applied relatively straightforwardly in the presence\nof missing data that are missing at random. Applications for the social and\nbehavioral sciences are used for illustration.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 10:07:01 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 08:46:53 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Mulder", "J.", ""], ["Hoijtink", "H.", ""], ["Gu", "X.", ""]]}, {"id": "1904.00745", "submitter": "Luyang Chen", "authors": "Luyang Chen, Markus Pelger and Jason Zhu", "title": "Deep Learning in Asset Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use deep neural networks to estimate an asset pricing model for individual\nstock returns that takes advantage of the vast amount of conditioning\ninformation, while keeping a fully flexible form and accounting for\ntime-variation. The key innovations are to use the fundamental no-arbitrage\ncondition as criterion function, to construct the most informative test assets\nwith an adversarial approach and to extract the states of the economy from many\nmacroeconomic time series. Our asset pricing model outperforms out-of-sample\nall benchmark approaches in terms of Sharpe ratio, explained variation and\npricing errors and identifies the key factors that drive asset prices.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 01:37:47 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 18:50:27 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 21:40:25 GMT"}, {"version": "v4", "created": "Sat, 16 May 2020 14:20:53 GMT"}, {"version": "v5", "created": "Fri, 23 Jul 2021 22:26:00 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chen", "Luyang", ""], ["Pelger", "Markus", ""], ["Zhu", "Jason", ""]]}, {"id": "1904.00840", "submitter": "Marija Cupari\\'c", "authors": "Marija Cupari\\'c, Bojana Milo\\v{s}evi\\'c, Marko Obradovi\\'c", "title": "New consistent exponentiality tests based on $V$-empirical Laplace\n  transforms with comparison of efficiencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new consistent goodness-of-fit tests for exponential distribution,\nbased on the Desu characterization. The test statistics represent the weighted\n$L^2$ and $L^{\\infty}$ distances between appropriate V-empirical Laplace\ntransforms of random variables that appear in the characterization. In\naddition, we perform an extensive comparison of Bahadur efficiencies of\ndifferent recent and classical exponentiality tests. We also present the\nempirical powers of new tests.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 14:36:51 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 07:30:51 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Cupari\u0107", "Marija", ""], ["Milo\u0161evi\u0107", "Bojana", ""], ["Obradovi\u0107", "Marko", ""]]}, {"id": "1904.01058", "submitter": "Yichen Zhou", "authors": "Yichen Zhou, Giles Hooker", "title": "Tree Boosted Varying Coefficient Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the integration of gradient boosted decision trees\nand varying coefficient models. We introduce the tree boosted varying\ncoefficient framework which justifies the implementation of decision tree\nboosting as the nonparametric effect modifiers in varying coefficient models.\nThis framework requires no structural assumptions in the space containing the\nvarying coefficient covariates, is easy to implement, and keeps a balance\nbetween model complexity and interpretability. To provide statistical\nguarantees, we prove the asymptotic consistency of the proposed method under\nthe regression settings with $L^2$ loss. We further conduct a thorough\nempirical study to show that the proposed method is capable of providing\naccurate predictions as well as intelligible visual explanations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 18:29:32 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Zhou", "Yichen", ""], ["Hooker", "Giles", ""]]}, {"id": "1904.01101", "submitter": "Fan Li", "authors": "Fan Li, Andrea Mercatanti, Taneli Makinen, Andrea Silvestrini", "title": "A Regression Discontinuity Design for Ordinal Running Variables:\n  Evaluating Central Bank Purchases of Corporate Bonds", "comments": "Also available as Temi di discussione (Economic working papers) 1213,\n  Bank of Italy, Economic Research and International Relations Area", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression discontinuity (RD) is a widely used quasi-experimental design for\ncausal inference. In the standard RD, the assignment to treatment is determined\nby a continuous pretreatment variable (i.e., running variable) falling above or\nbelow a pre-fixed threshold. In the case of the corporate sector purchase\nprogramme (CSPP) of the European Central Bank, which involves large-scale\npurchases of securities issued by corporations in the euro area, such a\nthreshold can be defined in terms of an ordinal running variable. This feature\nposes challenges to RD estimation due to the lack of a meaningful measure of\ndistance. To evaluate such program, this paper proposes an RD approach for\nordinal running variables under the local randomization framework. The proposal\nfirst estimates an ordered probit model for the ordinal running variable. The\nestimated probability of being assigned to treatment is then adopted as a\nlatent continuous running variable and used to identify a covariate-balanced\nsubsample around the threshold. Assuming local unconfoundedness of the\ntreatment in the subsample, an estimate of the effect of the program is\nobtained by employing a weighted estimator of the average treatment effect. Two\nweighting estimators---overlap weights and ATT weights---as well as their\naugmented versions are considered. We apply the method to evaluate the causal\neffect of the CSPP and find a statistically significant and negative effect on\ncorporate bond spreads at issuance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 20:55:15 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 14:43:10 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Li", "Fan", ""], ["Mercatanti", "Andrea", ""], ["Makinen", "Taneli", ""], ["Silvestrini", "Andrea", ""]]}, {"id": "1904.01202", "submitter": "Munir Hiabu", "authors": "Munir Hiabu, Jens P. Nielsen, Thomas H. Scheike", "title": "Non-Smooth Backfitting for Excess Risk Additive Regression Model with\n  Two Survival Time-Scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new backfitting algorithm estimating the complex structured\nnon-parametric survival model of Scheike (2001) without having to use\nsmoothing. The considered model is a non-parametric survival model with two\ntime-scales that are equivalent up to a constant that varies over the subjects.\nCovariate effects are modelled linearly on each time scale by additive Aalen\nmodels. Estimators of the cumulative intensities on the two time-scales are\nsuggested by solving local estimating equations jointly on the two time-scales.\nWe are able to estimate the cumulative intensities solving backfitting\nestimating equations without using smoothing methods and we provide large\nsample properties and simultaneous confidence bands. The model is applied to\ndata on myocardial infarction providing a separation of the two effects\nstemming from time since diagnosis and age.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 03:57:00 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Hiabu", "Munir", ""], ["Nielsen", "Jens P.", ""], ["Scheike", "Thomas H.", ""]]}, {"id": "1904.01228", "submitter": "Holger Dette", "authors": "Kira Alhorn, Holger Dette, Kirsten Schorning", "title": "Optimal designs for model averaging in non-nested models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we construct optimal designs for frequentist model averaging\nestimation. We derive the asymptotic distribution of the model averaging\nestimate with fixed weights in the case where the competing models are\nnon-nested and none of these models is correctly specified. A Bayesian optimal\ndesign minimizes an expectation of the asymptotic mean squared error of the\nmodel averaging estimate calculated with respect to a suitable prior\ndistribution. We demonstrate that Bayesian optimal designs can improve the\naccuracy of model averaging substantially. Moreover, the derived designs also\nimprove the accuracy of estimation in a model selected by model selection and\nmodel averaging estimates with random weights.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 06:03:19 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 14:44:18 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Alhorn", "Kira", ""], ["Dette", "Holger", ""], ["Schorning", "Kirsten", ""]]}, {"id": "1904.01320", "submitter": "Michael Messer", "authors": "Michael Messer", "title": "Bivariate change point detection: joint detection of changes in\n  expectation and variance", "comments": "32 pages, 14 figures, corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for change point detection is proposed. We consider a univariate\nsequence of independent random variables with piecewise constant expectation\nand variance, apart from which the distribution may vary periodically. We aim\nto detect change points in both expectation and variance. For that, we propose\na statistical test for the null hypothesis of no change points and an algorithm\nfor change point detection. Both are based on a bivariate moving sum approach\nthat jointly evaluates the mean and the empirical variance. The joint\nconsideration helps improve inference as compared to separate univariate\napproaches. We infer on the strength and the type of changes with confidence.\nNonparametric methodology supports the analysis of diverse data. Additionally,\na multi-scale approach addresses complex patterns in change points and effects.\nWe demonstrate the performance through theoretical results and simulation\nstudies. A companion R-package jcp (available on CRAN) is discussed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 10:30:33 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 17:19:46 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 18:35:50 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Messer", "Michael", ""]]}, {"id": "1904.01385", "submitter": "James Bagrow", "authors": "Andrew J. Becker and James P. Bagrow", "title": "UAFS: Uncertainty-Aware Feature Selection for Problems with Missing Data", "comments": "Withdrawn due to errors in theoretical derivations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data are a concern in many real world data sets and imputation\nmethods are often needed to estimate the values of missing data, but data sets\nwith excessive missingness and high dimensionality challenge most approaches to\nimputation. Here we show that appropriate feature selection can be an effective\npreprocessing step for imputation, allowing for more accurate imputation and\nsubsequent model predictions. The key feature of this preprocessing is that it\nincorporates uncertainty: by accounting for uncertainty due to missingness when\nselecting features we can reduce the degree of missingness while also limiting\nthe number of uninformative features being used to make predictive models. We\nintroduce a method to perform uncertainty-aware feature selection (UAFS),\nprovide a theoretical motivation, and test UAFS on both real and synthetic\nproblems, demonstrating that across a variety of data sets and levels of\nmissingness we can improve the accuracy of imputations. Improved imputation due\nto UAFS also results in improved prediction accuracy when performing supervised\nlearning using these imputed data sets. Our UAFS method is general and can be\nfruitfully coupled with a variety of imputation methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 13:02:18 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 21:22:07 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 18:33:16 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Becker", "Andrew J.", ""], ["Bagrow", "James P.", ""]]}, {"id": "1904.01490", "submitter": "Jelena Bradic", "authors": "Davide Viviano and Jelena Bradic", "title": "Synthetic learner: model-free inference on treatments over time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding of the effect of a particular treatment or a policy pertains to\nmany areas of interest -- ranging from political economics, marketing to\nhealth-care and personalized treatment studies. In this paper, we develop a\nnon-parametric, model-free test for detecting the effects of treatment over\ntime that extends widely used Synthetic Control tests. The test is built on\ncounterfactual predictions arising from many learning algorithms. In the\nNeyman-Rubin potential outcome framework with possible carry-over effects, we\nshow that the proposed test is asymptotically consistent for stationary, beta\nmixing processes. We do not assume that class of learners captures the correct\nmodel necessarily. We also discuss estimates of the average treatment effect,\nand we provide regret bounds on the predictive performance. To the best of our\nknowledge, this is the first set of results that allow for example any Random\nForest to be useful for provably valid statistical inference in the Synthetic\nControl setting. In experiments, we show that our Synthetic Learner is\nsubstantially more powerful than classical methods based on Synthetic Control\nor Difference-in-Differences, especially in the presence of non-linear outcome\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:28:21 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Viviano", "Davide", ""], ["Bradic", "Jelena", ""]]}, {"id": "1904.01622", "submitter": "Reid Landes", "authors": "Jillian Tang and Reid D. Landes", "title": "Some t-tests for N-of-1 trials with serial correlation", "comments": "23 pages, 6 figures, 6 tables", "journal-ref": null, "doi": "10.1371/journal.pone.0228077", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  N-of-1 trials allow inference between two treatments given to a single\nindividual. Most often, clinical investigators analyze an individual's N-of-1\ntrial data with usual t-tests or simple nonparametric methods. These simple\nmethods do not account for serial correlation in repeated observations coming\nfrom the individual. Existing methods accounting for serial correlation require\nsimulation, multiple N-of-1 trials, or both. Here, we develop t-tests that\naccount for serial correlation in a single individual. The development includes\neffect size and precision calculations, both of which are useful for study\nplanning. We then evaluate and compare their Type I and II errors and interval\nestimators to those of usual t-tests analogues via Monte Carlo simulation. The\nserial t-tests clearly outperform the usual t-tests commonly used in reporting\nN-of-1 results. Examples from N-of-1 clinical trials in fibromyalgia patients\nand from a behavioral health setting exhibit how accounting for serial\ncorrelation can change inferences. These t-tests are easily implemented and\nmore appropriate than simple methods commonly used; however, caution is needed\nwhen analyzing only a few observations. Keywords: Autocorrelation; Cross-over\nstudies; Repeated measures analysis; Single-case experimental design;\nTime-series\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 18:56:55 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 15:54:28 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Tang", "Jillian", ""], ["Landes", "Reid D.", ""]]}, {"id": "1904.01668", "submitter": "Liangyuan Hu", "authors": "Liangyuan Hu, Joseph W. Hogan", "title": "Causal comparative effectiveness analysis of dynamic continuous-time\n  treatment initiation rules with sparsely measured outcomes and death", "comments": "Accepted by Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence supporting the current World Health Organization recommendations of\nearly antiretroviral therapy (ART) initiation for adolescents is inconclusive.\nWe leverage a large observational data and compare, in terms of mortality and\nCD4 cell count, the dynamic treatment initiation rules for HIV-infected\nadolescents. Our approaches extend the marginal structural model for estimating\noutcome distributions under dynamic treatment regimes (DTR), developed in\nRobins et al. (2008), to allow the causal comparisons of both specific regimes\nand regimes along a continuum. Furthermore, we propose strategies to address\nthree challenges posed by the complex data set: continuous-time measurement of\nthe treatment initiation process; sparse measurement of longitudinal outcomes\nof interest, leading to incomplete data; and censoring due to dropout and\ndeath. We derive a weighting strategy for continuous time treatment initiation;\nuse imputation to deal with missingness caused by sparse measurements and\ndropout; and define a composite outcome that incorporates both death and CD4\ncount as a basis for comparing treatment regimes. Our analysis suggests that\nimmediate ART initiation leads to lower mortality and higher median values of\nthe composite outcome, relative to other initiation rules.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 21:07:49 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Hu", "Liangyuan", ""], ["Hogan", "Joseph W.", ""]]}, {"id": "1904.01849", "submitter": "Paolo Berta", "authors": "Giuseppe Arbia, Paolo Berta, Carrie B. Dolan", "title": "Measurement error induced by locational uncertainty when estimating\n  discrete choice models with a distance as a regressor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial microeconometric studies typically suffer from various forms of\ninaccuracies that are not present when dealing with the classical regional\nspatial econometrics models. Among those, missing data, locational errors,\nsampling without a formal sample design, measurement errors and misalignment\nare the typical sources of inaccuracy that can affects the results in a spatial\nmicroeconometric analysis. In this paper, we have examined the effects of\nmeasurement error introduced in a logistic model by random geo-masking, when\ndistances are used as predictors. Extending the classical results on the\nmeasurement error in a linear regression model, our MC experiment on hospital\nchoices showed that the higher the distortion produced by the geo-masking, the\nhigher is the downward bias in absolute value towards zero of the coefficient\nassociated to the distance in a regression model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 08:50:55 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Arbia", "Giuseppe", ""], ["Berta", "Paolo", ""], ["Dolan", "Carrie B.", ""]]}, {"id": "1904.01932", "submitter": "Liangyuan Hu", "authors": "Liangyuan Hu, Joseph W. Hogan, Ann W. Mwangi, Abraham Siika", "title": "Modeling the Causal Effect of Treatment Initiation Time on Survival:\n  Application to HIV/TB Co-infection", "comments": "Published in Biometrics", "journal-ref": "Biometrics 2018; 74(2): 703-713", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The timing of antiretroviral therapy (ART) initiation for HIV and\ntuberculosis (TB) co-infected patients needs to be considered carefully. CD4\ncell count can be used to guide decision making about when to initiate ART.\nEvidence from recent randomized trials and observational studies generally\nsupports early initiation but does not provide information about effects of\ninitiation time on a continuous scale. In this paper, we develop and apply a\nhighly flexible structural proportional hazards model for characterizing the\neffect of treatment initiation time on a survival distribution. The model can\nbe fitted using a weighted partial likelihood score function. Construction of\nboth the score function and the weights must accommodate censoring of the\ntreatment initiation time, the outcome, or both. The methods are applied to\ndata on 4903 individuals with HIV/TB co-infection, derived from electronic\nhealth records in a large HIV care program in Kenya. We use a model formulation\nthat flexibly captures the joint effects of ART initiation time and ART\nduration using natural cubic splines. The model is used to generate survival\ncurves corresponding to specific treatment initiation times; and to identify\noptimal times for ART initiation for subgroups defined by CD4 count at time of\nTB diagnosis. Our findings potentially provide \"higher resolution\" information\nabout the relationship between ART timing and mortality, and about the\ndifferential effect of ART timing within CD4 subgroups.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 16:57:31 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Hu", "Liangyuan", ""], ["Hogan", "Joseph W.", ""], ["Mwangi", "Ann W.", ""], ["Siika", "Abraham", ""]]}, {"id": "1904.01948", "submitter": "Elena Kulinskaya", "authors": "Ilyas Bakbergenuly, David C. Hoaglin and Elena Kulinskaya", "title": "Simulation study of estimating between-study variance and overall effect\n  in meta-analyses of mean difference", "comments": "20 pages and 108 A4 format 4 by 3 display figures on simulation\n  results. arXiv admin note: substantial text overlap with arXiv:1903.01362", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for random-effects meta-analysis require an estimate of the\nbetween-study variance, $\\tau^2$. The performance of estimators of $\\tau^2$\n(measured by bias and coverage) affects their usefulness in assessing\nheterogeneity of study-level effects, and also the performance of related\nestimators of the overall effect. For the effect measure mean difference (MD),\nwe review five point estimators of $\\tau^2$ (the popular methods of\nDerSimonian-Laird, restricted maximum likelihood, and Mandel and Paule (MP);\nthe less-familiar method of Jackson; and a new method (WT) based on the\nimproved approximation to the distribution of the $Q$ statistic by\n\\cite{kulinskaya2004welch}), five interval estimators for $\\tau^2$ (profile\nlikelihood, Q-profile, Biggerstaff and Jackson, Jackson, and the new WT\nmethod), six point estimators of the overall effect (the five related to the\npoint estimators of $\\tau^2$ and an estimator whose weights use only\nstudy-level sample sizes), and eight interval estimators for the overall effect\n(five based on the point estimators for $\\tau^2$, the\nHartung-Knapp-Sidik-Jonkman (HKSJ) interval, a modification of HKSJ, and an\ninterval based on the sample-size-weighted estimator). We obtain empirical\nevidence from extensive simulations and an example.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 17:26:16 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Bakbergenuly", "Ilyas", ""], ["Hoaglin", "David C.", ""], ["Kulinskaya", "Elena", ""]]}, {"id": "1904.02190", "submitter": "Md. Noor-E-Alam", "authors": "Md Saiful Islam, Md Sarowar Morshed, Gary J. Young, Md. Noor-E-Alam", "title": "Robust policy evaluation from large-scale observational studies", "comments": null, "journal-ref": "October 2019, Plos One", "doi": "10.1371/journal.pone.0223360", "report-no": null, "categories": "stat.ME math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under current policy decision making paradigm, we make or evaluate a policy\ndecision by intervening different socio-economic parameters and analyzing the\nimpact of those interventions. This process involves identifying the causal\nrelation between interventions and outcomes. Matching method is one of the\npopular techniques to identify such causal relations. However, in one-to-one\nmatching, when a treatment or control unit has multiple pair assignment options\nwith similar match quality, different matching algorithms often assign\ndifferent pairs. Since, all the matching algorithms assign pair without\nconsidering the outcomes, it is possible that with same data and same\nhypothesis, different experimenters can make different conclusions. This\nproblem becomes more prominent in case of large-scale observational studies.\nRecently, a robust approach is proposed to tackle the uncertainty which uses\ndiscrete optimization techniques to explore all possible assignments. Though\noptimization techniques are very efficient in its own way, they are not\nscalable to big data. In this work, we consider causal inference testing with\nbinary outcomes and propose computationally efficient algorithms that are\nscalable to large-scale observational studies. By leveraging the structure of\nthe optimization model, we propose a robustness condition which further reduces\nthe computational burden. We validate the efficiency of the proposed algorithms\nby testing the causal relation between Hospital Readmission Reduction Program\n(HRRP) and readmission to different hospital (non-index readmission) on the\nState of California Patient Discharge Database from 2010 to 2014. Our result\nshows that HRRP has a causal relation with the increase in non-index\nreadmission and the proposed algorithms proved to be highly scalable in testing\ncausal relations from large-scale observational studies.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 18:28:43 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Islam", "Md Saiful", ""], ["Morshed", "Md Sarowar", ""], ["Young", "Gary J.", ""], ["Noor-E-Alam", "Md.", ""]]}, {"id": "1904.02219", "submitter": "Abhik Ghosh PhD", "authors": "Elena Castilla, Abhik Ghosh, Nirian Martin, Leandro Pardo", "title": "Robust semiparametric inference for polytomous logistic regression with\n  complex survey design", "comments": "Preprint; Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing polytomous response from a complex survey scheme, like stratified\nor cluster sampling is very crucial in several socio-economics applications. We\npresent a class of minimum quasi weighted density power divergence estimators\nfor the polytomous logistic regression model with such a complex survey. This\nfamily of semiparametric estimators is a robust generalization of the maximum\nquasi weighted likelihood estimator exploiting the advantages of the popular\ndensity power divergence measure. Accordingly robust estimators for the design\neffects are also derived. Robust testing of general linear hypotheses on the\nregression coefficients are proposed using the new estimators. Their asymptotic\ndistributions and robustness properties are theoretically studied and also\nempirically validated through a numerical example and an extensive Monte Carlo\nstudy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 19:47:17 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Castilla", "Elena", ""], ["Ghosh", "Abhik", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1904.02308", "submitter": "Guillaume Basse", "authors": "Guillaume Basse, Peng Ding, Avi Feller, Panos Toulis", "title": "Randomization tests for peer effects in group formation experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the effect of peers on individual outcomes is a challenging\nproblem, in part because individuals often select peers who are similar in both\nobservable and unobservable ways. Group formation experiments avoid this\nproblem by randomly assigning individuals to groups and observing their\nresponses; for example, do first-year students have better grades when they are\nrandomly assigned roommates who have stronger academic backgrounds? Standard\napproaches for analyzing these experiments, however, are heavily\nmodel-dependent and generally fail to exploit the randomized design. In this\npaper, we extend methods from randomization-based testing under interference to\ngroup formation experiments. The proposed tests are justified by the\nrandomization itself, require relatively few assumptions, and are exact in\nfinite samples. First, we develop procedures that yield valid tests for\narbitrary group formation designs. Second, we derive sufficient conditions on\nthe design such that the randomization test can be implemented via simple\nrandom permutations. We apply this approach to two recent group formation\nexperiments and implement the proposed method in the new RGroupFormation R\npackage.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 02:05:04 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 18:25:35 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Basse", "Guillaume", ""], ["Ding", "Peng", ""], ["Feller", "Avi", ""], ["Toulis", "Panos", ""]]}, {"id": "1904.02438", "submitter": "Assaf Rabinowicz", "authors": "Assaf Rabinowicz, Saharon Rosset", "title": "Cross-Validation for Correlated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  K-fold cross-validation (CV) with squared error loss is widely used for\nevaluating predictive models, especially when strong distributional assumptions\ncannot be taken. However, CV with squared error loss is not free from\ndistributional assumptions, in particular in cases involving non-i.i.d. data.\nThis paper analyzes CV for correlated data. We present a criterion for\nsuitability of standard CV in presence of correlations. When this criterion\ndoes not hold, we introduce a bias corrected cross-validation estimator which\nwe term $CV_c,$ that yields an unbiased estimate of prediction error in many\nsettings where standard CV is invalid. We also demonstrate our results\nnumerically, and find that introducing our correction substantially improves\nboth, model evaluation and model selection in simulations and real data\nstudies.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 09:58:35 GMT"}, {"version": "v2", "created": "Sun, 12 May 2019 10:26:05 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 15:38:35 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Rabinowicz", "Assaf", ""], ["Rosset", "Saharon", ""]]}, {"id": "1904.02596", "submitter": "Elisa Cabana Garceran Del Vall", "authors": "Elisa Cabana, Rosa E. Lillo and Henry Laniado", "title": "Multivariate outlier detection based on a robust Mahalanobis distance\n  with shrinkage estimators", "comments": null, "journal-ref": "Stat Papers (2019)", "doi": "10.1007/s00362-019-01148-1", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A collection of robust Mahalanobis distances for multivariate outlier\ndetection is proposed, based on the notion of shrinkage. Robust intensity and\nscaling factors are optimally estimated to define the shrinkage. Some\nproperties are investigated, such as affine equivariance and breakdown value.\nThe performance of the proposal is illustrated through the comparison to other\ntechniques from the literature, in a simulation study and with a real dataset.\nThe behavior when the underlying distribution is heavy-tailed or skewed, shows\nthe appropriateness of the method when we deviate from the common assumption of\nnormality. The resulting high correct detection rates and low false detection\nrates in the vast majority of cases, as well as the significantly smaller\ncomputation time shows the advantages of our proposal.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 15:06:02 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Cabana", "Elisa", ""], ["Lillo", "Rosa E.", ""], ["Laniado", "Henry", ""]]}, {"id": "1904.02855", "submitter": "Carlo Graziani", "authors": "Carlo Graziani, Robert Rosner, Jennifer M. Adams, and Reason L.\n  Machete", "title": "Probabilistic Recalibration of Forecasts", "comments": "Accepted for publication in International Journal of Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scheme by which a probabilistic forecasting system whose\npredictions have poor probabilistic calibration may be recalibrated by\nincorporating past performance information to produce a new forecasting system\nthat is demonstrably superior to the original, in that one may use it to\nconsistently win wagers against someone using the original system. The scheme\nutilizes Gaussian process (GP) modeling to estimate a probability distribution\nover the Probability Integral Transform (PIT) of a scalar predictand. The GP\ndensity estimate gives closed-form access to information entropy measures\nassociated with the estimated distribution, which allows prediction of winnings\nin wagers against the base forecasting system. A separate consequence of the\nprocedure is that the recalibrated forecast has a uniform expected PIT\ndistribution. A distinguishing feature of the procedure is that it is\nappropriate even if the PIT values are not i.i.d. The recalibration scheme is\nformulated in a framework that exploits the deep connections between\ninformation theory, forecasting, and betting. We demonstrate the effectiveness\nof the scheme in two case studies: a laboratory experiment with a nonlinear\ncircuit and seasonal forecasts of the intensity of the El Ni\\~no-Southern\nOscillation phenomenon.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 03:04:42 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Graziani", "Carlo", ""], ["Rosner", "Robert", ""], ["Adams", "Jennifer M.", ""], ["Machete", "Reason L.", ""]]}, {"id": "1904.02883", "submitter": "Daniel Ahfock", "authors": "Daniel Ahfock and Geoffrey J. McLachlan", "title": "On missing label patterns in semi-supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate model based classification with partially labelled training\ndata. In many biostatistical applications, labels are manually assigned by\nexperts, who may leave some observations unlabelled due to class uncertainty.\nWe analyse semi-supervised learning as a missing data problem and identify\nsituations where the missing label pattern is non-ignorable for the purposes of\nmaximum likelihood estimation. In particular, we find that a relationship\nbetween classification difficulty and the missing label pattern implies a\nnon-ignorable missingness mechanism. We examine a number of real datasets and\nconclude the pattern of missing labels is related to the difficulty of\nclassification. We propose a joint modelling strategy involving the observed\ndata and the missing label mechanism to account for the systematic missing\nlabels. Full likelihood inference including the missing label mechanism can\nimprove the efficiency of parameter estimation, and increase classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 06:07:14 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Ahfock", "Daniel", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1904.02921", "submitter": "Igor Koval", "authors": "Igor Koval (ARAMIS, CMAP, ICM), St\\'ephanie Allassonni\\`ere (CRC -\n  UMR-S 1138), Stanley Durrleman (ICM, ARAMIS)", "title": "Simulation of virtual cohorts increases predictive accuracy of cognitive\n  decline in MCI subjects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict the progression of biomarkers, notably in NDD, is\nlimited by the size of the longitudinal data sets, in terms of number of\npatients, number of visits per patients and total follow-up time. To this end,\nwe introduce a data augmentation technique that is able to reproduce the\nvariability seen in a longitudinal training data set and simulate continuous\nbiomarkers trajectories for any number of virtual patients. Thanks to this\nsimulation framework, we propose to transform the training set into a simulated\ndata set with more patients, more time-points per patient and longer follow-up\nduration. We illustrate this approach on the prediction of the MMSE of MCI\nsubjects of the ADNI data set. We show that it allows to reach predictions with\nerrors comparable to the noise in the data, estimated in test/retest studies,\nachieving a improvement of 37% of the mean absolute error compared to the same\nnon-augmented model.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 08:01:39 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Koval", "Igor", "", "ARAMIS, CMAP, ICM"], ["Allassonni\u00e8re", "St\u00e9phanie", "", "CRC -\n  UMR-S 1138"], ["Durrleman", "Stanley", "", "ICM, ARAMIS"]]}, {"id": "1904.02970", "submitter": "Anja Jan{\\ss}en", "authors": "Anja Jan{\\ss}en, Phyllis Wan", "title": "$k$-means clustering of extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-means clustering algorithm and its variant, the spherical $k$-means\nclustering, are among the most important and popular methods in unsupervised\nlearning and pattern detection. In this paper, we explore how the spherical\n$k$-means algorithm can be applied in the analysis of only the extremal\nobservations from a data set. By making use of multivariate extreme value\nanalysis we show how it can be adopted to find \"prototypes\" of extremal\ndependence and we derive a consistency result for our suggested estimator. In\nthe special case of max-linear models we show furthermore that our procedure\nprovides an alternative way of statistical inference for this class of models.\nFinally, we provide data examples which show that our method is able to find\nrelevant patterns in extremal observations and allows us to classify extremal\nevents.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 10:06:16 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 13:22:02 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Jan\u00dfen", "Anja", ""], ["Wan", "Phyllis", ""]]}, {"id": "1904.03012", "submitter": "Simon Wilson", "authors": "Cristina De Persis and Jose Luis Bosque and Irene Huertas and Simon\n  Paul Wilson", "title": "Quantitative system risk assessment from incomplete data with belief\n  networks and pairwise comparison elicitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for conducting Bayesian elicitation and learning in risk assessment\nis presented. It assumes that the risk process can be described as a fault\ntree. This is viewed as a belief network, for which prior distributions on\nprimary event probabilities are elicited by means of a pairwise comparison\napproach. A Bayesian updating procedure, following observation of some or all\nof the events in the fault tree, is described. The application is illustrated\nthrough the motivating example of risk assessment of spacecraft explosion\nduring controlled re-entry.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 12:09:57 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["De Persis", "Cristina", ""], ["Bosque", "Jose Luis", ""], ["Huertas", "Irene", ""], ["Wilson", "Simon Paul", ""]]}, {"id": "1904.03099", "submitter": "Qinghua Liu", "authors": "Qinghua Liu, Andrew Henry Reiner, Arnoldo Frigessi, Ida Scheel", "title": "Diverse personalized recommendations with uncertainty from implicit\n  preference data with the Bayesian Mallows Model", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clicking data, which exists in abundance and contains objective user\npreference information, is widely used to produce personalized recommendations\nin web-based applications. Current popular recommendation algorithms, typically\nbased on matrix factorizations, often have high accuracy and achieve good\nclickthrough rates. However, diversity of the recommended items, which can\ngreatly enhance user experiences, is often overlooked. Moreover, most\nalgorithms do not produce interpretable uncertainty quantifications of the\nrecommendations. In this work, we propose the Bayesian Mallows for Clicking\nData (BMCD) method, which augments clicking data into compatible full ranking\nvectors by enforcing all the clicked items to be top-ranked. User preferences\nare learned using a Mallows ranking model. Bayesian inference leads to\ninterpretable uncertainties of each individual recommendation, and we also\npropose a method to make personalized recommendations based on such\nuncertainties. With a simulation study and a real life data example, we\ndemonstrate that compared to state-of-the-art matrix factorization, BMCD makes\npersonalized recommendations with similar accuracy, while achieving much higher\nlevel of diversity, and producing interpretable and actionable uncertainty\nestimation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:51:18 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Liu", "Qinghua", ""], ["Reiner", "Andrew Henry", ""], ["Frigessi", "Arnoldo", ""], ["Scheel", "Ida", ""]]}, {"id": "1904.03246", "submitter": "Xin Zhang", "authors": "Xin Zhang and Zhengyuan Zhu", "title": "Spatial CUSUM for Signal Region Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting weak clustered signal in spatial data is important but challenging\nin applications such as medical image and epidemiology. A more efficient\ndetection algorithm can provide more precise early warning, and effectively\nreduce the decision risk and cost. To date, many methods have been developed to\ndetect signals with spatial structures. However, most of the existing methods\nare either too conservative for weak signals or computationally too intensive.\nIn this paper, we consider a novel method named Spatial CUSUM (SCUSUM), which\nemploys the idea of the CUSUM procedure and false discovery rate controlling.\nWe develop theoretical properties of the method which indicates that\nasymptotically SCUSUM can reach high classification accuracy. In the simulation\nstudy, we demonstrate that SCUSUM is sensitive to weak spatial signals. This\nnew method is applied to a real fMRI dataset as illustration, and more\nirregular weak spatial signals are detected in the images compared to some\nexisting methods, including the conventional FDR, FDR$_L$ and scan statistics.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 19:25:11 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhang", "Xin", ""], ["Zhu", "Zhengyuan", ""]]}, {"id": "1904.03372", "submitter": "Mengjia Yu", "authors": "Mengjia Yu and Xiaohui Chen", "title": "A robust bootstrap change point test for high-dimensional location\n  parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of change point detection for high-dimensional\ndistributions in a location family when the dimension can be much larger than\nthe sample size. In change point analysis, the widely used cumulative sum\n(CUSUM) statistics are sensitive to outliers and heavy-tailed distributions. In\nthis paper, we propose a robust, tuning-free (i.e., fully data-dependent), and\neasy-to-implement change point test that enjoys strong theoretical guarantees.\nTo achieve the robust purpose in a nonparametric setting, we formulate the\nchange point detection in the multivariate $U$-statistics framework with\nanti-symmetric and nonlinear kernels. Specifically, the within-sample noise is\ncancelled out by anti-symmetry of the kernel, while the signal distortion under\ncertain nonlinear kernels can be controlled such that the between-sample change\npoint signal is magnitude preserving. A (half) jackknife multiplier bootstrap\n(JMB) tailored to the change point detection setting is proposed to calibrate\nthe distribution of our $\\ell^{\\infty}$-norm aggregated test statistic. Subject\nto mild moment conditions on kernels, we derive the uniform rates of\nconvergence for the JMB to approximate the sampling distribution of the test\nstatistic, and analyze its size and power properties. Extensions to multiple\nchange point testing and estimation are discussed with illustration from\nnumeric studies.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 06:16:01 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 04:46:00 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Yu", "Mengjia", ""], ["Chen", "Xiaohui", ""]]}, {"id": "1904.03397", "submitter": "Oliver Stoner", "authors": "Oliver Stoner, Theo Economou", "title": "Multivariate Hierarchical Frameworks for Modelling Delayed Reporting in\n  Count Data", "comments": "Biometrics (2019)", "journal-ref": null, "doi": "10.1111/biom.13188", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields and applications count data can be subject to delayed\nreporting. This is where the total count, such as the number of disease cases\ncontracted in a given week, may not be immediately available, instead arriving\nin parts over time. For short term decision making, the statistical challenge\nlies in predicting the total count based on any observed partial counts, along\nwith a robust quantification of uncertainty. In this article we discuss\nprevious approaches to modelling delayed reporting and present a multivariate\nhierarchical framework where the count generating process and delay mechanism\nare modelled simultaneously. Unlike other approaches, the framework can also be\neasily adapted to allow for the presence of under-reporting in the final\nobserved count. To compare our approach with existing frameworks, one of which\nwe extend to potentially improve predictive performance, we present a case\nstudy of reported dengue fever cases in Rio de Janeiro. Based on both\nwithin-sample and out-of-sample posterior predictive model checking and\narguments of interpretability, adaptability, and computational efficiency, we\ndiscuss the advantages and disadvantages of each modelling framework.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 09:29:34 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Stoner", "Oliver", ""], ["Economou", "Theo", ""]]}, {"id": "1904.03517", "submitter": "Giorgos Bakoyannis", "authors": "Giorgos Bakoyannis", "title": "Nonparametric tests for transition probabilities in nonhomogeneous\n  Markov processes", "comments": null, "journal-ref": null, "doi": "10.1080/10485252.2019.1705298", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes nonparametric two-sample tests for the direct comparison\nof the probabilities of a particular transition between states of a continuous\ntime nonhomogeneous Markov process with a finite state space. The proposed\ntests are a linear nonparametric test, an L2-norm-based test and a\nKolmogorov-Smirnov-type test. Significance level assessment is based on\nrigorous procedures, which are justified through the use of modern empirical\nprocess theory. Moreover, the L2-norm and the Kolmogorov-Smirnov-type tests are\nshown to be consistent for every fixed alternative hypothesis. The proposed\ntests are also extended to more complex situations such as cases with\nincompletely observed absorbing states and non-Markov processes. Simulation\nstudies show that the test statistics perform well even with small sample\nsizes. Finally, the proposed tests are applied to data on the treatment of\nearly breast cancer from the European Organization for Research and Treatment\nof Cancer (EORTC) trial 10854, under an illness-death model.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 19:48:54 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Bakoyannis", "Giorgos", ""]]}, {"id": "1904.03548", "submitter": "Roger  Fan", "authors": "Roger Fan, Byoungwook Jang, Yuekai Sun, Shuheng Zhou", "title": "Precision Matrix Estimation with Noisy and Missing Data", "comments": "27 pages, 14 figures, to appear in The 22nd International Conference\n  on Artificial Intelligence and Statistics (AISTATS 2019)", "journal-ref": null, "doi": null, "report-no": "Technical Report 545, Department of Statistics, University of\n  Michigan", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating conditional dependence graphs and precision matrices are some of\nthe most common problems in modern statistics and machine learning. When data\nare fully observed, penalized maximum likelihood-type estimators have become\nstandard tools for estimating graphical models under sparsity conditions.\nExtensions of these methods to more complex settings where data are\ncontaminated with additive or multiplicative noise have been developed in\nrecent years. In these settings, however, the relative performance of different\nmethods is not well understood and algorithmic gaps still exist. In particular,\nin high-dimensional settings these methods require using non-positive\nsemidefinite matrices as inputs, presenting novel optimization challenges. We\ndevelop an alternating direction method of multipliers (ADMM) algorithm for\nthese problems, providing a feasible algorithm to estimate precision matrices\nwith indefinite input and potentially nonconvex penalties. We compare this\nmethod with existing alternative solutions and empirically characterize the\ntradeoffs between them. Finally, we use this method to explore the networks\namong US senators estimated from voting records data.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 00:05:57 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Fan", "Roger", ""], ["Jang", "Byoungwook", ""], ["Sun", "Yuekai", ""], ["Zhou", "Shuheng", ""]]}, {"id": "1904.03643", "submitter": "Donghoh Kim", "authors": "Donghoh Kim, Guebin Choi and Hee-Seok Oh", "title": "Ensemble Patch Transformation: A New Tool for Signal Decomposition", "comments": "32 pages with 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of signal decomposition and data\nvisualization. For this purpose, we introduce a new multiscale transform,\ntermed `ensemble patch transformation' that enhances identification of local\ncharacteristics embedded in a signal and provides multiscale visualization\naccording to different levels; hence, it is useful for data analysis and signal\ndecomposition. In literature, there are data-adaptive decomposition methods\nsuch as empirical mode decomposition (EMD) by Huang et al. (1998). Along the\nsame line of EMD, we propose a new decomposition algorithm that extracts\nmeaningful components from a signal that belongs to a large class of signals,\ncompared to the previous methods. Some theoretical properties of the proposed\nalgorithm are investigated. To evaluate the proposed method, we analyze several\nsynthetic examples and a real-world signal.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 12:53:42 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Kim", "Donghoh", ""], ["Choi", "Guebin", ""], ["Oh", "Hee-Seok", ""]]}, {"id": "1904.03647", "submitter": "Rico Krueger", "authors": "Prateek Bansal, Rico Krueger, Michel Bierlaire, Ricardo A. Daziano,\n  Taha H. Rashidi", "title": "Bayesian Estimation of Mixed Multinomial Logit Models: Advances and\n  Simulation-Based Evaluations", "comments": null, "journal-ref": "Transportation Research Part B: Methodological, Volume 131,\n  January 2020, Pages 124-142", "doi": "10.1016/j.trb.2019.12.001", "report-no": null, "categories": "stat.ML cs.LG econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) methods have emerged as a fast and\ncomputationally-efficient alternative to Markov chain Monte Carlo (MCMC)\nmethods for scalable Bayesian estimation of mixed multinomial logit (MMNL)\nmodels. It has been established that VB is substantially faster than MCMC at\npractically no compromises in predictive accuracy. In this paper, we address\ntwo critical gaps concerning the usage and understanding of VB for MMNL. First,\nextant VB methods are limited to utility specifications involving only\nindividual-specific taste parameters. Second, the finite-sample properties of\nVB estimators and the relative performance of VB, MCMC and maximum simulated\nlikelihood estimation (MSLE) are not known. To address the former, this study\nextends several VB methods for MMNL to admit utility specifications including\nboth fixed and random utility parameters. To address the latter, we conduct an\nextensive simulation-based evaluation to benchmark the extended VB methods\nagainst MCMC and MSLE in terms of estimation times, parameter recovery and\npredictive accuracy. The results suggest that all VB variants with the\nexception of the ones relying on an alternative variational lower bound\nconstructed with the help of the modified Jensen's inequality perform as well\nas MCMC and MSLE at prediction and parameter recovery. In particular, VB with\nnonconjugate variational message passing and the delta-method (VB-NCVMP-Delta)\nis up to 16 times faster than MCMC and MSLE. Thus, VB-NCVMP-Delta can be an\nattractive alternative to MCMC and MSLE for fast, scalable and accurate\nestimation of MMNL models.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 13:03:56 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 15:48:39 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2019 07:12:09 GMT"}, {"version": "v4", "created": "Thu, 12 Dec 2019 14:38:22 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Bansal", "Prateek", ""], ["Krueger", "Rico", ""], ["Bierlaire", "Michel", ""], ["Daziano", "Ricardo A.", ""], ["Rashidi", "Taha H.", ""]]}, {"id": "1904.03737", "submitter": "Ezequiel Smucler", "authors": "Ezequiel Smucler, Andrea Rotnitzky, James M. Robins", "title": "A unifying approach for doubly-robust $\\ell_1$ regularized estimation of\n  causal contrasts", "comments": "fixed example 11, added example 12", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference about a scalar parameter under a non-parametric model\nbased on a one-step estimator computed as a plug in estimator plus the\nempirical mean of an estimator of the parameter's influence function. We focus\non a class of parameters that have influence function which depends on two\ninfinite dimensional nuisance functions and such that the bias of the one-step\nestimator of the parameter of interest is the expectation of the product of the\nestimation errors of the two nuisance functions. Our class includes many\nimportant treatment effect contrasts of interest in causal inference and\neconometrics, such as ATE, ATT, an integrated causal contrast with a continuous\ntreatment, and the mean of an outcome missing not at random. We propose\nestimators of the target parameter that entertain approximately sparse\nregression models for the nuisance functions allowing for the number of\npotential confounders to be even larger than the sample size. By employing\nsample splitting, cross-fitting and $\\ell_1$-regularized regression estimators\nof the nuisance functions based on objective functions whose directional\nderivatives agree with those of the parameter's influence function, we obtain\nestimators of the target parameter with two desirable robustness properties:\n(1) they are rate doubly-robust in that they are root-n consistent and\nasymptotically normal when both nuisance functions follow approximately sparse\nmodels, even if one function has a very non-sparse regression coefficient, so\nlong as the other has a sufficiently sparse regression coefficient, and (2)\nthey are model doubly-robust in that they are root-n consistent and\nasymptotically normal even if one of the nuisance functions does not follow an\napproximately sparse model so long as the other nuisance function follows an\napproximately sparse model with a sufficiently sparse regression coefficient.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 20:34:26 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 19:07:12 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 21:59:14 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Smucler", "Ezequiel", ""], ["Rotnitzky", "Andrea", ""], ["Robins", "James M.", ""]]}, {"id": "1904.03767", "submitter": "Jinxin Guo", "authors": "Jinxin Guo", "title": "An IRT-based Model for Omitted and Not-reached Items", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missingness is a common occurrence in educational assessment and\npsychological measurement. It could not be casually ignored as it may threaten\nthe validity of the test if not handled properly. Considering the difference\nbetween omitted and not-reached items, we developed an IRT-based model to\nhandle these missingness. In the proposed method, not-reached responses are\ncaptured by the cumulative missingness. Moreover, the nonignorability is\nattributed to the correlation between ability and person missing trait. We\nproved that its item parameters estimate under maximum marginal likelihood\n(MML) estimation is consistent. We further proposed a Bayesian estimation\nprocedure using MCMC methods to estimate all the parameters. The simulation\nresults indicate that the model parameters under the proposed method are better\nrecovered than that under listwise deletion, and the nonignorable model fits\nthe simulated nonignorable nonresponses better than ignorable model in terms of\nBayesian model selection. Furthermore, the Program for International Student\nAssessment (PISA) data set was analyzed to further illustrate the usage of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 23:10:38 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Guo", "Jinxin", ""]]}, {"id": "1904.04091", "submitter": "Zhou Lan", "authors": "Zhou Lan, Brian J. Reich, Joseph Guinness, Dipankar Bandyopadhyay,\n  Liangsuo Ma, F. Gerard Moeller", "title": "Geostatistical Modeling of Positive Definite Matrices: An Application to\n  Diffusion Tensor Imaging", "comments": null, "journal-ref": null, "doi": "10.1111/biom.13445", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geostatistical modeling for continuous point-referenced data has been\nextensively applied to neuroimaging because it produces efficient and valid\nstatistical inference. However, diffusion tensor imaging (DTI), a neuroimaging\ncharacterizing the brain structure produces a positive definite (p.d.) matrix\nfor each voxel. Current geostatistical modeling has not been extended to p.d.\nmatrices because introducing spatial dependence among positive definite\nmatrices properly is challenging. In this paper, we use the spatial Wishart\nprocess, a spatial stochastic process (random field) where each p.d.\nmatrix-variate marginally follows a Wishart distribution, and spatial\ndependence between random matrices is induced by latent Gaussian processes.\nThis process is valid on an uncountable collection of spatial locations and is\nalmost surely continuous, leading to a reasonable means of modeling spatial\ndependence. Motivated by a DTI dataset of cocaine users, we propose a spatial\nmatrix-variate regression model based on the spatial Wishart process. A\nproblematic issue is that the spatial Wishart process has no closed-form\ndensity function. Hence, we propose approximation methods to obtain a feasible\nworking model. A local likelihood approximation method is also applied to\nachieve fast computation. The simulation studies and real data analysis\ndemonstrate that the working model produces reliable inference and improved\nperformance compared to other methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 14:28:11 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 04:48:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lan", "Zhou", ""], ["Reich", "Brian J.", ""], ["Guinness", "Joseph", ""], ["Bandyopadhyay", "Dipankar", ""], ["Ma", "Liangsuo", ""], ["Moeller", "F. Gerard", ""]]}, {"id": "1904.04185", "submitter": "Xynthia Kavelaars", "authors": "X.M. Kavelaars, S. van Buuren, J.R. van Ginkel", "title": "Multiple imputation in data that grow over time: A comparison of three\n  strategies", "comments": "15 pages, 5 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation is a highly recommended technique to deal with missing\ndata, but the application to longitudinal datasets can be done in multiple\nways. When a new wave of longitudinal data arrives, we can treat the combined\ndata of multiple waves as a new missing data problem and overwrite existing\nimputations with new values (re-imputation). Alternatively, we may keep the\nexisting imputations, and impute only the new data. We may do either a full\nmultiple imputation (nested) or a single imputation (appended) on the new data\nper imputed set. This study compares these three strategies by means of\nsimulation. All techniques resulted in valid inference under a monotone\nmissingness pattern. A non-monotone missingness pattern led to biased and\nnon-confidence valid regression coefficients after nested and appended\nimputation, depending on the correlation structure of the data. Correlations\nwithin timepoints must be stronger than correlations between timepoints to\nobtain valid inference. In an empirical example, the three strategies performed\nsimilarly.We conclude that appended imputation is especially beneficial in\nlongitudinal datasets that suffer from dropout.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 16:49:57 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Kavelaars", "X. M.", ""], ["van Buuren", "S.", ""], ["van Ginkel", "J. R.", ""]]}, {"id": "1904.04276", "submitter": "Lin Liu L", "authors": "Lin Liu, Rajarshi Mukherjee, James M. Robins", "title": "On nearly assumption-free tests of nominal confidence interval coverage\n  for causal parameters estimated by machine learning", "comments": "Significant updates from the previous version. In press in\n  Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many causal effect parameters of interest, doubly robust machine learning\n(DRML) estimators $\\hat{\\psi}_{1}$ are the state-of-the-art, incorporating the\ngood prediction performance of machine learning; the decreased bias of doubly\nrobust estimators; and the analytic tractability and bias reduction of sample\nsplitting with cross fitting. Nonetheless, even in the absence of confounding\nby unmeasured factors, the nominal $(1 - \\alpha)$ Wald confidence interval\n$\\hat{\\psi}_{1} \\pm z_{\\alpha / 2} \\widehat{\\mathsf{se}} [\\hat{\\psi}_{1}]$ may\nstill undercover even in large samples, because the bias of $\\hat{\\psi}_{1}$\nmay be of the same or even larger order than its standard error of order\n$n^{-1/2}$.\n  In this paper, we introduce essentially assumption-free tests that (i) can\nfalsify the null hypothesis that the bias of $\\hat{\\psi}_{1}$ is of smaller\norder than its standard error, (ii) can provide an upper confidence bound on\nthe true coverage of the Wald interval, and (iii) are valid under the null\nunder no smoothness/sparsity assumptions on the nuisance parameters. The tests,\nwhich we refer to as \\underline{A}ssumption \\underline{F}ree\n\\underline{E}mpirical \\underline{C}overage \\underline{T}ests (AFECTs), are\nbased on a U-statistic that estimates part of the bias of $\\hat{\\psi}_{1}$.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:09:45 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 12:47:27 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Lin", ""], ["Mukherjee", "Rajarshi", ""], ["Robins", "James M.", ""]]}, {"id": "1904.04378", "submitter": "Yuqi Gu", "authors": "Yuqi Gu and Gongjun Xu", "title": "Learning Attribute Patterns in High-Dimensional Structured Latent\n  Attribute Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured latent attribute models (SLAMs) are a special family of discrete\nlatent variable models widely used in social and biological sciences. This\npaper considers the problem of learning significant attribute patterns from a\nSLAM with potentially high-dimensional configurations of the latent attributes.\nWe address the theoretical identifiability issue, propose a penalized\nlikelihood method for the selection of the attribute patterns, and further\nestablish the selection consistency in such an overfitted SLAM with diverging\nnumber of latent patterns. The good performance of the proposed methodology is\nillustrated by simulation studies and two real datasets in educational\nassessment.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 22:04:50 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 03:50:16 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Gu", "Yuqi", ""], ["Xu", "Gongjun", ""]]}, {"id": "1904.04484", "submitter": "Diego Mesquita", "authors": "Paul Blomstedt, Diego Mesquita, Jarno Lintusaari, Tuomas Sivula, Jukka\n  Corander and Samuel Kaski", "title": "Meta-analysis of Bayesian analyses", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis aims to combine results from multiple related statistical\nanalyses. While the natural outcome of a Bayesian analysis is a posterior\ndistribution, Bayesian meta-analyses traditionally combine analyses summarized\nas point estimates, often limiting distributional assumptions. In this paper,\nwe develop a framework for combining posterior distributions, which builds on\nstandard Bayesian inference, but using distributions instead of data points as\nobservations. We show that the resulting framework preserves basic theoretical\nproperties, such as order-invariance in successive updates and posterior\nconcentration. In addition to providing a consensus analysis for multiple\nBayesian analyses, we highlight the benefit of being able to reuse posteriors\nfrom computationally costly analyses and update them post-hoc without having to\nrerun the analyses themselves. The wide applicability of the framework is\nillustrated with examples of combining results from likelihood-free Bayesian\nanalyses, which would be difficult to carry out using standard methodology.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 06:36:49 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Blomstedt", "Paul", ""], ["Mesquita", "Diego", ""], ["Lintusaari", "Jarno", ""], ["Sivula", "Tuomas", ""], ["Corander", "Jukka", ""], ["Kaski", "Samuel", ""]]}, {"id": "1904.04551", "submitter": "David Frazier", "authors": "David T. Frazier and Christopher Drovandi", "title": "Robust Approximate Bayesian Inference with Synthetic Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Bayesian synthetic likelihood (BSL) is now an established method for\nconducting approximate Bayesian inference in models where, due to the\nintractability of the likelihood function, exact Bayesian approaches are either\ninfeasible or computationally too demanding. Implicit in the application of BSL\nis the assumption that the data generating process (DGP) can produce simulated\nsummary statistics that capture the behaviour of the observed summary\nstatistics. We demonstrate that if this compatibility between the actual and\nassumed DGP is not satisfied, i.e., if the model is misspecified, BSL can yield\nunreliable parameter inference. To circumvent this issue, we propose a new BSL\napproach that can detect the presence of model misspecification, and\nsimultaneously deliver useful inferences even under significant model\nmisspecification. Two simulated and two real data examples demonstrate the\nperformance of this new approach to BSL, and document its superior accuracy\nover standard BSL when the assumed model is misspecified.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 09:16:22 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 05:53:40 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 07:23:53 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Frazier", "David T.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1904.04664", "submitter": "Yuehan Yang", "authors": "Yuehan Yang, Siwei Xia and Hu Yang", "title": "Sparse Laplacian Shrinkage with the Graphical Lasso Estimator for\n  Regression Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a high-dimensional linear regression problem where there\nare complex correlation structures among predictors. We propose a\ngraph-constrained regularization procedure, named Sparse Laplacian Shrinkage\nwith the Graphical Lasso Estimator (SLS-GLE). The procedure uses the estimated\nprecision matrix to describe the specific information on the conditional\ndependence pattern among predictors, and encourages both sparsity on the\nregression model and the graphical model. We introduce the Laplacian quadratic\npenalty adopting the graph information, and give detailed discussions on the\nadvantages of using the precision matrix to construct the Laplacian matrix.\nTheoretical properties and numerical comparisons are presented to show that the\nproposed method improves both model interpretability and accuracy of\nestimation. We also apply this method to a financial problem and prove that the\nproposed procedure is successful in assets selection.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 13:41:20 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Yang", "Yuehan", ""], ["Xia", "Siwei", ""], ["Yang", "Hu", ""]]}, {"id": "1904.04793", "submitter": "Vojtech Kejzlar", "authors": "Vojtech Kejzlar, L\\'eo Neufcourt, Taps Maiti and Frederi Viens", "title": "Bayesian averaging of computer models with domain discrepancies: a\n  nuclear physics perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME nucl-th physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies Bayesian model averaging (BMA) in the context of\ncompeting expensive computer models in a typical nuclear physics setup. While\nit is well known that BMA accounts for the additional uncertainty of the model\nitself, we show that it also decreases the posterior variance of the prediction\nerrors via an explicit decomposition. We extend BMA to the situation where the\ncompeting models are defined on non-identical study regions. Any model's local\nforecasting difficulty is offset by predictions obtained from the average\nmodel, thus extending individual models to the full domain. We illustrate our\nmethodology via pedagogical simulations and applications to forecasting nuclear\nobservables, which exhibit convincing improvements in both the BMA prediction\nerror and empirical coverage probabilities.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 17:15:35 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 18:07:48 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Kejzlar", "Vojtech", ""], ["Neufcourt", "L\u00e9o", ""], ["Maiti", "Taps", ""], ["Viens", "Frederi", ""]]}, {"id": "1904.04876", "submitter": "Kelly Van Lancker", "authors": "Kelly Van Lancker, An Vandebosch and Stijn Vansteelandt", "title": "Improving interim decisions in randomized trials by exploiting\n  information on short-term outcomes and prognostic baseline covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional power calculations are frequently used to guide the decision\nwhether or not to stop a trial for futility or to modify planned sample size.\nThese ignore the information in short-term endpoints and baseline covariates,\nand thereby do not make fully efficient use of the information in the data. We\ntherefore propose an interim decision procedure based on the conditional power\napproach which exploits the information contained in baseline covariates and\nshort-term outcomes. We will realise this by considering the estimation of the\ntreatment effect at the interim analysis as a missing data problem. This\nproblem is addressed by employing specific prediction models for the long-term\nendpoint which enable the incorporation of baseline covariates and multiple\nshort-term endpoints. We show that the proposed procedure leads to an\nefficiency gain and a reduced sample size, without compromising the Type I\nerror rate of the procedure, even when the adopted prediction models are\nmisspecified. In particular, implementing our proposal in the conditional power\napproach allows earlier decisions relative to standard approaches, whilst\ncontrolling the probability of an incorrect decision. This time gain results in\na lower expected number of recruited patients in case of stopping for futility,\nsuch that fewer patients receive the futile regimen. We explain how these\nmethods can be used in adaptive designs with unblinded sample size reassessment\nbased on the inverse normal $p$-value combination method to control type I\nerror. We support the proposal by Monte Carlo simulations based on data from a\nreal clinical trial.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 19:20:55 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Van Lancker", "Kelly", ""], ["Vandebosch", "An", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "1904.04901", "submitter": "Andrea Cremaschi", "authors": "Andrea Cremaschi, Arnoldo Frigessi, Kjetil Task\\'en, Manuela Zucknick", "title": "A Bayesian approach to study synergistic interaction effects in in-vitro\n  drug combination experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cancer translational research, increasing effort is devoted to the study\nof the combined effect of two drugs when they are administered simultaneously.\nIn this paper, we introduce a new approach to estimate the part of the effect\nof the two drugs due to the interaction of the compounds, i.e. which is due to\nsynergistic or antagonistic effects of the two drugs, compared to a reference\nvalue representing the condition when the combined compounds do not interact,\ncalled zero-interaction. We describe an in-vitro cell viability experiment as a\nrandom experiment, by interpreting cell viability as the probability of a cell\nin the experiment to be viable after treatment, and including information\nrelated to different exposure conditions. We propose a flexible Bayesian spline\nregression framework for modelling the viability surface of two drugs combined\nas a function of the concentrations. Since the proposed approach is based on a\nstatistical model, it allows to include replicates of the experiments, to\nevaluate the uncertainty of the estimates, and to perform prediction. We test\nthe model fit and prediction performance on a simulation study, and on an\novarian cancer cell dataset. Posterior estimates of the zero-interaction level\nand of the synergy term, obtained via adaptive MCMC algorithms, are used to\ncompute interpretable measures of efficacy of the combined experiment,\nincluding relative volume under the surface (rVUS) measures to summarise the\nzero-interaction and synergy terms and a bi-variate alternative to the\nwell-known EC50 measure.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:22:48 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 01:40:39 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Cremaschi", "Andrea", ""], ["Frigessi", "Arnoldo", ""], ["Task\u00e9n", "Kjetil", ""], ["Zucknick", "Manuela", ""]]}, {"id": "1904.05060", "submitter": "Giulia Bertagnolli", "authors": "Giulia Bertagnolli, Claudio Agostinelli, Manlio De Domenico", "title": "Network depth: identifying median and contours in complex networks", "comments": null, "journal-ref": null, "doi": "10.1093/comnet/cnz041", "report-no": null, "categories": "stat.ME cond-mat.dis-nn physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Centrality descriptors are widely used to rank nodes according to specific\nconcept(s) of importance. Despite the large number of centrality measures\navailable nowadays, it is still poorly understood how to identify the node\nwhich can be considered as the `centre' of a complex network. In fact, this\nproblem corresponds to finding the median of a complex network. The median is a\nnon-parametric and robust estimator of the location parameter of a probability\ndistribution. In this work, we present the most natural generalisation of the\nconcept of median to the realm of complex networks, discussing its advantages\nfor defining the centre of the system and percentiles around that centre. To\nthis aim, we introduce a new statistical data depth and we apply it to networks\nembedded in a geometric space induced by different metrics. The application of\nour framework to empirical networks allows us to identify median nodes which\nare socially or biologically relevant.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 08:34:18 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 15:21:56 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Bertagnolli", "Giulia", ""], ["Agostinelli", "Claudio", ""], ["De Domenico", "Manlio", ""]]}, {"id": "1904.05107", "submitter": "Margrethe Kvale Loe", "authors": "Margrethe Kvale Loe and H{\\aa}kon Tjelmeland", "title": "Ensemble updating of binary state vectors by maximising the expected\n  number of unchanged components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several ensemble-based filtering methods have been proposed\nand studied. The main challenge in such procedures is the updating of a prior\nensemble to a posterior ensemble at every step of the filtering recursions. In\nthe famous ensemble Kalman filter, the assumption of a linear-Gaussian state\nspace model is introduced in order to overcome this issue, and the prior\nensemble is updated with a linear shift closely related to the traditional\nKalman filter equations. In the current article, we consider how the ideas\nunderlying the ensemble Kalman filter can be applied when the components of the\nstate vectors are binary variables. While the ensemble Kalman filter relies on\nGaussian approximations of the forecast and filtering distributions, we instead\nuse first order Markov chains. To update the prior ensemble, we simulate\nsamples from a distribution constructed such that the expected number of equal\ncomponents in a prior and posterior state vector is maximised. We demonstrate\nthe performance of our approach in a simulation example inspired by the\nmovement of oil and water in a petroleum reservoir, where also a more na\\\"{i}ve\nupdating approach is applied for comparison. Here, we observe that the\nFrobenius norm of the difference between the estimated and the true marginal\nfiltering probabilities is reduced to the half with our method compared to the\nna\\\"{i}ve approach, indicating that our method is superior. Finally, we discuss\nhow our methodology can be generalised from the binary setting to more\ncomplicated situations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 11:05:42 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Loe", "Margrethe Kvale", ""], ["Tjelmeland", "H\u00e5kon", ""]]}, {"id": "1904.05187", "submitter": "Tamara Fernandez", "authors": "Tamara Fernandez and Nicolas Rivera", "title": "A Reproducing Kernel Hilbert Space log-rank test for the two-sample\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted log-rank tests are arguably the most widely used tests by\npractitioners for the two-sample problem in the context of right-censored data.\nMany approaches have been considered to make weighted log-rank tests more\nrobust against a broader family of alternatives, among them, considering linear\ncombinations of weighted log-rank tests, and taking the maximum among a finite\ncollection of them. In this paper, we propose as test statistic the supremum of\na collection of (potentially infinite) weight-indexed log-rank tests where the\nindex space is the unit ball in a reproducing kernel Hilbert space (RKHS). By\nusing some desirable properties of RKHSs we provide an exact and simple\nevaluation of the test statistic and establish connections with previous tests\nin the literature. Additionally, we show that for a special family of RKHSs,\nthe proposed test is omnibus. We finalise by performing an empirical evaluation\nof the proposed methodology and show an application to a real data scenario.\nOur theoretical results are proved using techniques for double integrals with\nrespect to martingales that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 13:44:36 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 18:28:43 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Fernandez", "Tamara", ""], ["Rivera", "Nicolas", ""]]}, {"id": "1904.05289", "submitter": "Hao Chen", "authors": "Hao Chen and Yin Xia", "title": "A Normality Test for High-dimensional Data based on a Nearest Neighbor\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical methodologies for high-dimensional data assume the\npopulation is normal. Although a few multivariate normality tests have been\nproposed, to the best of our knowledge, none of them can properly control the\ntype I error when the dimension is larger than the number of observations. In\nthis work, we propose a novel nonparametric test that utilizes the nearest\nneighbor information. The proposed method guarantees the asymptotic type I\nerror control under the high-dimensional setting. Simulation studies verify the\nempirical size performance of the proposed test when the dimension grows with\nthe sample size and at the same time exhibit a superior power performance of\nthe new test compared with alternative methods. We also illustrate our approach\nthrough two popularly used data sets in high-dimensional classification and\nclustering literatures where deviation from the normality assumption may lead\nto invalid conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:47:12 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 19:59:53 GMT"}, {"version": "v3", "created": "Sun, 2 May 2021 04:15:39 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "Hao", ""], ["Xia", "Yin", ""]]}, {"id": "1904.05330", "submitter": "Arash Ali Amini", "authors": "Marina S. Paez, Arash A. Amini and Lizhen Lin", "title": "Hierarchical Stochastic Block Model for Community Detection in Multiplex\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplex networks have become increasingly more prevalent in many fields,\nand have emerged as a powerful tool for modeling the complexity of real\nnetworks. There is a critical need for developing inference models for\nmultiplex networks that can take into account potential dependencies across\ndifferent layers, particularly when the aim is community detection. We add to a\nlimited literature by proposing a novel and efficient Bayesian model for\ncommunity detection in multiplex networks. A key feature of our approach is the\nability to model varying communities at different network layers. In contrast,\nmany existing models assume the same communities for all layers. Moreover, our\nmodel automatically picks up the necessary number of communities at each layer\n(as validated by real data examples). This is appealing, since deciding the\nnumber of communities is a challenging aspect of community detection, and\nespecially so in the multiplex setting, if one allows the communities to change\nacross layers. Borrowing ideas from hierarchical Bayesian modeling, we use a\nhierarchical Dirichlet prior to model community labels across layers, allowing\ndependency in their structure. Given the community labels, a stochastic block\nmodel (SBM) is assumed for each layer. We develop an efficient slice sampler\nfor sampling the posterior distribution of the community labels as well as the\nlink probabilities between communities. In doing so, we address some unique\nchallenges posed by coupling the complex likelihood of SBM with the\nhierarchical nature of the prior on the labels. An extensive empirical\nvalidation is performed on simulated and real data, demonstrating the superior\nperformance of the model over single-layer alternatives, as well as the ability\nto uncover interesting structures in real networks.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 02:01:09 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Paez", "Marina S.", ""], ["Amini", "Arash A.", ""], ["Lin", "Lizhen", ""]]}, {"id": "1904.05334", "submitter": "Catherine Matias", "authors": "Stephane Robin (MIA-Paris), Vincent Miele (LBBE), Catherine Matias\n  (LPSM UMR 8001), St\\'ephane Dray (LBBE)", "title": "Nine Quick Tips for Analyzing Network Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These tips provide a quick and concentrated guide for beginners in the\nanalysis of network data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 06:37:17 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 08:14:01 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Robin", "Stephane", "", "MIA-Paris"], ["Miele", "Vincent", "", "LBBE"], ["Matias", "Catherine", "", "LPSM UMR 8001"], ["Dray", "St\u00e9phane", "", "LBBE"]]}, {"id": "1904.05416", "submitter": "Michael Fay", "authors": "Michael P. Fay, Sally A. Hunsberger", "title": "Practical Valid Inferences for the Two-Sample Binomial Problem", "comments": "41 pages, 8 figures. To appear in Statistics Surveys. v2 has changes\n  based on reviewer comments. Main differences are the old v1 Sections 8\n  (Noninferiority and Equivalence Hypotheses) and 12 (Connection to Causal\n  Inferences) were deleted for length. There was no issue with the correctness\n  of those sections. There are other minor changes and additions in v2, with\n  the main changes in Section 7", "journal-ref": "Statistics Surveys (2021) 15: 72-110", "doi": "10.1214/21-SS131", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our interest is whether two binomial parameters differ, which parameter is\nlarger, and by how much. This apparently simple problem was addressed by Fisher\nin the 1930's, and has been the subject of many review papers since then. Yet\nthere continues to be new work on this issue and no consensus solution.\nPrevious reviews have focused primarily on testing and the properties of\nvalidity and power, or primarily on confidence intervals, their coverage, and\nexpected length. Here we evaluate both. For example, we consider whether a\np-value and its matching confidence interval are compatible, meaning that the\np-value rejects at level $\\alpha$ if and only if the $1-\\alpha$ confidence\ninterval excludes all null parameter values. For focus, we only examine\nnon-asymptotic inferences, so that most of the p-values and confidence\nintervals are valid (i.e., exact) by construction. Within this focus, we review\ndifferent methods emphasizing many of the properties and interpretational\naspects we desire from applied frequentist inference: validity, accuracy, good\npower, equivariance, compatibility, coherence, and parameterization and\ndirection of effect. We show that no one method can meet all the desirable\nproperties and give recommendations based on which properties are given more\nimportance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 19:58:30 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 21:20:34 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Fay", "Michael P.", ""], ["Hunsberger", "Sally A.", ""]]}, {"id": "1904.05526", "submitter": "Cong Ma", "authors": "Jianqing Fan, Cong Ma, Yiqiao Zhong", "title": "A Selective Overview of Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has arguably achieved tremendous success in recent years. In\nsimple words, deep learning uses the composition of many nonlinear functions to\nmodel the complex dependency between input features and labels. While neural\nnetworks have a long history, recent advances have greatly improved their\nperformance in computer vision, natural language processing, etc. From the\nstatistical and scientific perspective, it is natural to ask: What is deep\nlearning? What are the new characteristics of deep learning, compared with\nclassical methods? What are the theoretical foundations of deep learning? To\nanswer these questions, we introduce common neural network models (e.g.,\nconvolutional neural nets, recurrent neural nets, generative adversarial nets)\nand training techniques (e.g., stochastic gradient descent, dropout, batch\nnormalization) from a statistical point of view. Along the way, we highlight\nnew characteristics of deep learning (including depth and over-parametrization)\nand explain their practical and theoretical benefits. We also sample recent\nresults on theories of deep learning, many of which are only suggestive. While\na complete understanding of deep learning remains elusive, we hope that our\nperspectives and discussions serve as a stimulus for new statistical research.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:53:15 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 13:59:45 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Fan", "Jianqing", ""], ["Ma", "Cong", ""], ["Zhong", "Yiqiao", ""]]}, {"id": "1904.05636", "submitter": "Julie Rendlov\\'a", "authors": "Julie Rendlov\\'a, Karel Hron, Kamila Fa\\v{c}evicov\\'a and Peter\n  Filzmoser", "title": "Robust Principal Component Analysis for Compositional Tables", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data table which is arranged according to two factors can often be\nconsidered as a compositional table. An example is the number of unemployed\npeople, split according to gender and age classes. Analyzed as compositions,\nthe relevant information would consist of ratios between different cells of\nsuch a table. This is particularly useful when analyzing several compositional\ntables jointly, where the absolute numbers are in very different ranges, e.g.\nif unemployment data are considered from different countries. Within the\nframework of the logratio methodology, compositional tables can be decomposed\ninto independent and interactive parts, and orthonormal coordinates can be\nassigned to these parts. However, these coordinates usually require some prior\nknowledge about the data, and they are not easy to handle for exploring the\nrelationships between the given factors.\n  Here we propose a special choice of coordinates with a direct relation to\ncentered logratio (clr) coefficients, which are particularly useful for an\ninterpretation in terms of the original cells of the tables. With these\ncoordinates, robust principal component analysis (PCA) is performed for\ndimension reduction, allowing to investigate the relationships between the\nfactors. The link between orthonormal coordinates and clr coefficients enables\nto apply robust PCA, which would otherwise suffer from the singularity of clr\ncoefficients.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 11:30:06 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Rendlov\u00e1", "Julie", ""], ["Hron", "Karel", ""], ["Fa\u010devicov\u00e1", "Kamila", ""], ["Filzmoser", "Peter", ""]]}, {"id": "1904.05741", "submitter": "Ilmun Kim", "authors": "Ilmun Kim", "title": "Comparing a Large Number of Multivariate Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a test for the equality of multiple distributions\nbased on kernel mean embeddings. Our framework provides a flexible way to\nhandle multivariate or even high-dimensional data by virtue of kernel methods\nand allows the number of distributions to increase with the sample size. This\nis in contrast to previous studies that have been mostly restricted to\nclassical low-dimensional settings with a fixed number of distributions. By\nbuilding on Cramer-type moderate deviation for degenerate two-sample\nV-statistics, we derive the limiting null distribution of the test statistic\nand show that it converges to a Gumbel distribution. The limiting distribution,\nhowever, depends on an infinite number of nuisance parameters, which makes it\ninfeasible for use in practice. To address this issue, the proposed test is\nimplemented via the permutation procedure and is shown to be minimax rate\noptimal against sparse alternatives. During our analysis, an exponential\nconcentration inequality for the permuted test statistic is developed which may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 14:54:13 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 16:12:59 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Kim", "Ilmun", ""]]}, {"id": "1904.05828", "submitter": "Nicol\\'as Kuschinski", "authors": "Nicol\\'as E. Kuschinski and J. Andr\\'es Christen", "title": "FATSO: A family of operators for variable selection in linear models", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In linear models it is common to have situations where several regression\ncoefficients are zero. In these situations a common tool to perform regression\nis a variable selection operator. One of the most common such operators is the\nLASSO operator, which promotes point estimates which are zero. The LASSO\noperator and similar approaches, however, give little in terms of easily\ninterpretable parameters to determine the degree of variable selectivity. In\nthis paper we propose a new family of selection operators which builds on the\ngeometry of LASSO but which yield an easily interpretable way to tune\nselectivity. These operators correspond to Bayesian prior densities and hence\nare suitable for Bayesian inference. We present some examples using simulated\nand real data, with promising results.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:33:50 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Kuschinski", "Nicol\u00e1s E.", ""], ["Christen", "J. Andr\u00e9s", ""]]}, {"id": "1904.06019", "submitter": "Rina Barber", "authors": "Ryan J. Tibshirani, Rina Foygel Barber, Emmanuel J. Candes, Aaditya\n  Ramdas", "title": "Conformal Prediction Under Covariate Shift", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend conformal prediction methodology beyond the case of exchangeable\ndata. In particular, we show that a weighted version of conformal prediction\ncan be used to compute distribution-free prediction intervals for problems in\nwhich the test and training covariate distributions differ, but the likelihood\nratio between these two distributions is known---or, in practice, can be\nestimated accurately with access to a large set of unlabeled data (test\ncovariate points). Our weighted extension of conformal prediction also applies\nmore generally, to settings in which the data satisfies a certain weighted\nnotion of exchangeability. We discuss other potential applications of our new\nconformal methodology, including latent variable and missing data problems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 03:06:00 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 10:29:29 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 19:47:23 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Tibshirani", "Ryan J.", ""], ["Barber", "Rina Foygel", ""], ["Candes", "Emmanuel J.", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1904.06151", "submitter": "Maxim Panov", "authors": "Marina Gomtsyan, Nikita Mokrov, Maxim Panov and Yury Yanovich", "title": "Geometry-Aware Maximum Likelihood Estimation of Intrinsic Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing approaches to intrinsic dimension estimation usually are not\nreliable when the data are nonlinearly embedded in the high dimensional space.\nIn this work, we show that the explicit accounting to geometric properties of\nunknown support leads to the polynomial correction to the standard maximum\nlikelihood estimate of intrinsic dimension for flat manifolds. The proposed\nalgorithm (GeoMLE) realizes the correction by regression of standard MLEs based\non distances to nearest neighbors for different sizes of neighborhoods.\nMoreover, the proposed approach also efficiently handles the case of nonuniform\nsampling of the manifold. We perform numerous experiments on different\nsynthetic and real-world datasets. The results show that our algorithm achieves\nstate-of-the-art performance, while also being computationally efficient and\nrobust to noise in the data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 10:44:22 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Gomtsyan", "Marina", ""], ["Mokrov", "Nikita", ""], ["Panov", "Maxim", ""], ["Yanovich", "Yury", ""]]}, {"id": "1904.06181", "submitter": "Aniket Biswas", "authors": "Aniket Biswas and Subrata Chakraborty", "title": "R = P(Y < X) for unit-Lindley distribution: inference with an\n  application in public health", "comments": "16 pages, 5 tables, version 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unit-Lindley distribution was recently introduced in the literature as a\nviable alternative to the Beta and the Kumaraswamy distributions with support\nin (0; 1). This distribution enjoys many virtuous properties over the named\ndistributions. In this article, we address the issue of parameter estimation\nfrom a Bayesian perspective and study relative performance of different\nestimators through extensive simulation studies. Significant emphasis is given\nto the estimation of stress-strength reliability employing classical as well as\nBayesian approach. A non-trivial useful application in the public health domain\nis presented proposing a simple metric of discrepancy.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 12:19:54 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 05:47:34 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Biswas", "Aniket", ""], ["Chakraborty", "Subrata", ""]]}, {"id": "1904.06340", "submitter": "Zifeng Zhao", "authors": "Zifeng Zhao and Ting Fung Ma and Wai Leong Ng and Chun Yip Yau", "title": "A Composite Likelihood-based Approach for Change-point Detection in\n  Spatio-temporal Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a unified, accurate and computationally efficient method\nfor change-point inference in non-stationary spatio-temporal processes. By\nmodeling a non-stationary spatio-temporal process as a piecewise stationary\nspatio-temporal process, we consider simultaneous estimation of the number and\nlocations of change-points, and model parameters in each segment. A composite\nlikelihood-based criterion is developed for change-point and parameters\nestimation. Asymptotic theories including consistency and distribution of the\nestimators are derived under mild conditions. In contrast to classical results\nin fixed dimensional time series that the asymptotic error of change-point\nestimator is $O_{p}(1)$, exact recovery of true change-points is guaranteed in\nthe spatio-temporal setting. More surprisingly, the consistency of change-point\nestimation can be achieved without any penalty term in the criterion function.\nA computational efficient pruned dynamic programming algorithm is developed for\nthe challenging criterion optimization problem. Simulation studies and an\napplication to U.S. precipitation data are provided to demonstrate the\neffectiveness and practicality of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 17:43:06 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Zhao", "Zifeng", ""], ["Ma", "Ting Fung", ""], ["Ng", "Wai Leong", ""], ["Yau", "Chun Yip", ""]]}, {"id": "1904.06384", "submitter": "Michael Levine", "authors": "Jiexin Duan, Michael Levine, Junxiang Luo, Yongming Qu", "title": "Estimation of group means in generalized linear mixed models", "comments": "26 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we investigate the concept of the mean response for a\ntreatment group mean as well as its estimation and prediction for generalized\nlinear models with a subject-wise random effect. Generalized linear models are\ncommonly used to analyze categorical data. The model-based mean for a treatment\ngroup usually estimates the response at the mean covariate. However, the mean\nresponse for the treatment group for studied population is at least equally\nimportant in the context of clinical trials. New methods were proposed to\nestimate such a mean response in generalized linear models; however, this has\nonly been done when there are no random effects in the model. We suggest that,\nin a generalized linear mixed model (GLMM), there are at least two possible\ndefinitions of a treatment group mean response that can serve as\nestimation/prediction targets. The estimation of these treatment group means is\nimportant for healthcare professionals to be able to understand the absolute\nbenefit versus risk. For both of these treatment group means, we propose a new\nset of methods that suggests how to estimate/predict both of them in a GLMM\nmodels with a univariate subject-wise random effect. Our methods also suggest\nan easy way of constructing corresponding confidence and prediction intervals\nfor both possible treatment group means. Simulations show that proposed\nconfidence and prediction intervals provide correct empirical coverage\nprobability under most circumstances. Proposed methods have also been applied\nto analyze hypoglycemia data from diabetes clinical trials.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 19:21:30 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 20:27:36 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Duan", "Jiexin", ""], ["Levine", "Michael", ""], ["Luo", "Junxiang", ""], ["Qu", "Yongming", ""]]}, {"id": "1904.06519", "submitter": "Teresa Ledwina", "authors": "\\'Cmiel Bogdan and Ledwina Teresa", "title": "Validation of Association", "comments": "40 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing, quantifying and visualizing associations between two variables\nis increasingly important. This paper investigates how a new function-valued\nmeasure of dependence, the quantile dependence function, can be used to\nconstruct tests for independence and to provide an easily interpretable\ndiagnostic plot of existing departures from the null model. The dependence\nfunction is designed to detect general dependence structure between variables\nin quantiles of the joint distribution. It gives an insight into how the\ndependence structures changes in different parts of the joint distribution. We\ndefine new estimators of the dependence function, discuss some of their\nproperties, and apply them to construct new tests of independence. Numerical\nevidence is given on the test's benefits against three recognized independence\ntests introduced in the previous years. In real-data analysis, we illustrate\nthe use of our tests and the graphical presentation of the underlying\ndependence structure.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 10:16:25 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Bogdan", "\u0106miel", ""], ["Teresa", "Ledwina", ""]]}, {"id": "1904.06569", "submitter": "Kristin Kirchner", "authors": "Sonja G. Cox and Kristin Kirchner", "title": "Regularity and convergence analysis in Sobolev and H\\\"older spaces for\n  generalized Whittle-Mat\\'ern fields", "comments": "41 pages, 2 figures", "journal-ref": "Numer. Math. 146 (2020) 819-873", "doi": "10.1007/s00211-020-01151-x", "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze several Galerkin approximations of a Gaussian random field\n$\\mathcal{Z}\\colon\\mathcal{D}\\times\\Omega\\to\\mathbb{R}$ indexed by a Euclidean\ndomain $\\mathcal{D}\\subset\\mathbb{R}^d$ whose covariance structure is\ndetermined by a negative fractional power $L^{-2\\beta}$ of a second-order\nelliptic differential operator $L:= -\\nabla\\cdot(A\\nabla) + \\kappa^2$. Under\nminimal assumptions on the domain $\\mathcal{D}$, the coefficients\n$A\\colon\\mathcal{D}\\to\\mathbb{R}^{d\\times d}$,\n$\\kappa\\colon\\mathcal{D}\\to\\mathbb{R}$, and the fractional exponent $\\beta>0$,\nwe prove convergence in $L_q(\\Omega; H^\\sigma(\\mathcal{D}))$ and in\n$L_q(\\Omega; C^\\delta(\\overline{\\mathcal{D}}))$ at (essentially) optimal rates\nfor (i) spectral Galerkin methods and (ii) finite element approximations.\nSpecifically, our analysis is solely based on\n$H^{1+\\alpha}(\\mathcal{D})$-regularity of the differential operator $L$, where\n$0<\\alpha\\leq 1$. For this setting, we furthermore provide rigorous estimates\nfor the error in the covariance function of these approximations in\n$L_{\\infty}(\\mathcal{D}\\times\\mathcal{D})$ and in the mixed Sobolev space\n$H^{\\sigma,\\sigma}(\\mathcal{D}\\times\\mathcal{D})$, showing convergence which is\nmore than twice as fast compared to the corresponding $L_q(\\Omega;\nH^\\sigma(\\mathcal{D}))$-rate. For the well-known example of such Gaussian\nrandom fields, the original Whittle-Mat\\'ern class, where $L=-\\Delta +\n\\kappa^2$ and $\\kappa \\equiv \\operatorname{const.}$, we perform several\nnumerical experiments which validate our theoretical results.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 17:07:04 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Cox", "Sonja G.", ""], ["Kirchner", "Kristin", ""]]}, {"id": "1904.06605", "submitter": "Rafael Stern", "authors": "Victor Coscrato and Lu\\'is Gustavo Esteves and Rafael Izbicki and\n  Rafael Bassi Stern", "title": "Interpretable hypothesis tests", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although hypothesis tests play a prominent role in Science, their\ninterpretation can be challenging. Three issues are (i) the difficulty in\nmaking an assertive decision based on the output of an hypothesis test, (ii)\nthe logical contradictions that occur in multiple hypothesis testing, and (iii)\nthe possible lack of practical importance when rejecting a precise hypothesis.\nThese issues can be addressed through the use of agnostic tests and pragmatic\nhypotheses.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 23:47:26 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Coscrato", "Victor", ""], ["Esteves", "Lu\u00eds Gustavo", ""], ["Izbicki", "Rafael", ""], ["Stern", "Rafael Bassi", ""]]}, {"id": "1904.06632", "submitter": "Mansoor Sheikh", "authors": "M Sheikh, A.C.C. Coolen", "title": "Analysis of overfitting in the regularized Cox model", "comments": null, "journal-ref": null, "doi": "10.1088/1751-8121/ab375c", "report-no": null, "categories": "stat.ME cond-mat.dis-nn cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cox proportional hazards model is ubiquitous in the analysis of\ntime-to-event data. However, when the data dimension p is comparable to the\nsample size $N$, maximum likelihood estimates for its regression parameters are\nknown to be biased or break down entirely due to overfitting. This prompted the\nintroduction of the so-called regularized Cox model. In this paper we use the\nreplica method from statistical physics to investigate the relationship between\nthe true and inferred regression parameters in regularized multivariate Cox\nregression with L2 regularization, in the regime where both p and N are large\nbut with p/N ~ O(1). We thereby generalize a recent study from maximum\nlikelihood to maximum a posteriori inference. We also establish a relationship\nbetween the optimal regularization parameter and p/N, allowing for\nstraightforward overfitting corrections in time-to-event analysis.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 05:48:02 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 12:15:10 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Sheikh", "M", ""], ["Coolen", "A. C. C.", ""]]}, {"id": "1904.06789", "submitter": "Jun Ma", "authors": "Jun Ma and Dominique-Laurent Couturier and Stephane Heritier and Ian\n  Marschner", "title": "Proportional hazards model with partly interval censoring and its\n  penalized likelihood estimation", "comments": "23 pages, 3 figures and 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of semi-parametric proportional hazards\nmodel fitting for interval, left and right censored survival times. We adopt a\nmore versatile penalized likelihood method to estimate the baseline hazard and\nthe regression coefficients simultaneously, where the penalty is introduced in\norder to regularize the baseline hazard estimate. We present asymptotic\nproperties of our estimate, allowing for the possibility that it may lie on the\nboundary of the parameter space. We also provide a computational method based\non marginal likelihood, which allows the regularization parameter to be\ndetermined automatically. Comparisons of our method with other approaches are\ngiven in simulations which demonstrate that our method has favourable\nperformance. A real data application involving a model for melanoma recurrence\nis presented and an R package implementing the methods is available.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 00:41:10 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Ma", "Jun", ""], ["Couturier", "Dominique-Laurent", ""], ["Heritier", "Stephane", ""], ["Marschner", "Ian", ""]]}, {"id": "1904.06843", "submitter": "Bo Zhang", "authors": "Jiti Gao, Guangming Pan, Yanrong Yang and Bo Zhang", "title": "Estimation of Cross-Sectional Dependence in Large Panels", "comments": "47 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate estimation for extent of cross{sectional dependence in large panel\ndata analysis is paramount to further statistical analysis on the data under\nstudy. Grouping more data with weak relations (cross{sectional dependence)\ntogether often results in less efficient dimension reduction and worse\nforecasting. This paper describes cross-sectional dependence among a large\nnumber of objects (time series) via a factor model and parameterizes its extent\nin terms of strength of factor loadings. A new joint estimation method,\nbenefiting from unique feature of dimension reduction for high dimensional time\nseries, is proposed for the parameter representing the extent and some other\nparameters involved in the estimation procedure. Moreover, a joint asymptotic\ndistribution for a pair of estimators is established. Simulations illustrate\nthe effectiveness of the proposed estimation method in the finite sample\nperformance. Applications in cross-country macro-variables and stock returns\nfrom S&P 500 are studied.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:15:05 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Gao", "Jiti", ""], ["Pan", "Guangming", ""], ["Yang", "Yanrong", ""], ["Zhang", "Bo", ""]]}, {"id": "1904.06979", "submitter": "C\\'edric Colas", "authors": "C\\'edric Colas, Olivier Sigaud, Pierre-Yves Oudeyer", "title": "A Hitchhiker's Guide to Statistical Comparisons of Reinforcement\n  Learning Algorithms", "comments": "8 pages + supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistently checking the statistical significance of experimental results is\nthe first mandatory step towards reproducible science. This paper presents a\nhitchhiker's guide to rigorous comparisons of reinforcement learning\nalgorithms. After introducing the concepts of statistical testing, we review\nthe relevant statistical tests and compare them empirically in terms of false\npositive rate and statistical power as a function of the sample size (number of\nseeds) and effect size. We further investigate the robustness of these tests to\nviolations of the most common hypotheses (normal distributions, same\ndistributions, equal variances). Beside simulations, we compare empirical\ndistributions obtained by running Soft-Actor Critic and Twin-Delayed Deep\nDeterministic Policy Gradient on Half-Cheetah. We conclude by providing\nguidelines and code to perform rigorous comparisons of RL algorithm\nperformances.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 12:00:29 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Colas", "C\u00e9dric", ""], ["Sigaud", "Olivier", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "1904.07025", "submitter": "Amirreza Nickkar", "authors": "Amirreza Nickkar and Young-Jae Lee", "title": "Evaluation of Dedicated Lanes for Automated vehicles at Roundabouts with\n  Various Flow Patterns", "comments": "11 pages, 4 figures, 3 tables, This paper has been presented at\n  Transportation Research Board 98th TRB Annual Meeting 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph math.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles (AVs) are about to be used in transportation systems in\nthe near future. To increase the level of safety and throughput of these\nvehicles, dedicated lanes for AVs have been suggested in past studies as\nexclusive mobility infrastructure for these types of vehicles. Although these\nlanes can bring obvious advantages in a transportation network, overall\nperformance of these lanes, especially in urban areas and in micro level sight,\nis not clear. This study aims to examine the efficiency of dedicated lanes for\nAVs in roundabout with an unbalanced traffic flow pattern. Four factors of\ntravel time, delay time, speed of vehicles, and queue of vehicles have been\nselected as variables of traffic performance at the roundabout. Two microscopic\ntraffic simulation software, AIMSUN and SIDRA Intersection, were used to\nexamine the impact of the AV dedicated lanes at the roundabout. This study\nshows that the effects of an imbalanced traffic pattern in a roundabout are\nhigher when the penetration rate of AVs is lower and also dedicated lanes in\nroundabout's leg may improve traffic performance indicators when the\npenetration rate of AVs is higher, but this improvement is not significant.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:37:55 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Nickkar", "Amirreza", ""], ["Lee", "Young-Jae", ""]]}, {"id": "1904.07111", "submitter": "Yannick Guyonvarch", "authors": "Alexis Derumigny, Lucas Girard and Yannick Guyonvarch", "title": "On the construction of confidence intervals for ratios of expectations", "comments": "59 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In econometrics, many parameters of interest can be written as ratios of\nexpectations. The main approach to construct confidence intervals for such\nparameters is the delta method. However, this asymptotic procedure yields\nintervals that may not be relevant for small sample sizes or, more generally,\nin a sequence-of-model framework that allows the expectation in the denominator\nto decrease to $0$ with the sample size. In this setting, we prove a\ngeneralization of the delta method for ratios of expectations and the\nconsistency of the nonparametric percentile bootstrap. We also investigate\nfinite-sample inference and show a partial impossibility result: nonasymptotic\nuniform confidence intervals can be built for ratios of expectations but not at\nevery level. Based on this, we propose an easy-to-compute index to appraise the\nreliability of the intervals based on the delta method. Simulations and an\napplication illustrate our results and the practical usefulness of our rule of\nthumb.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:26:23 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Derumigny", "Alexis", ""], ["Girard", "Lucas", ""], ["Guyonvarch", "Yannick", ""]]}, {"id": "1904.07150", "submitter": "Kolyan Ray", "authors": "Kolyan Ray and Botond Szabo", "title": "Variational Bayes for high-dimensional linear regression with sparse\n  priors", "comments": "42 pages. To appear in Journal of the American Statistical\n  Association", "journal-ref": null, "doi": "10.1080/01621459.2020.1847121", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a mean-field spike and slab variational Bayes (VB) approximation to\nBayesian model selection priors in sparse high-dimensional linear regression.\nUnder compatibility conditions on the design matrix, oracle inequalities are\nderived for the mean-field VB approximation, implying that it converges to the\nsparse truth at the optimal rate and gives optimal prediction of the response\nvector. The empirical performance of our algorithm is studied, showing that it\nworks comparably well as other state-of-the-art Bayesian variable selection\nmethods. We also numerically demonstrate that the widely used coordinate-ascent\nvariational inference (CAVI) algorithm can be highly sensitive to the parameter\nupdating order, leading to potentially poor performance. To mitigate this, we\npropose a novel prioritized updating scheme that uses a data-driven updating\norder and performs better in simulations. The variational algorithm is\nimplemented in the R package 'sparsevb'.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 15:58:44 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 21:33:39 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 15:06:11 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Ray", "Kolyan", ""], ["Szabo", "Botond", ""]]}, {"id": "1904.07270", "submitter": "Joshua Hewitt", "authors": "Joshua Hewitt, Jennifer A. Hoeting", "title": "Approximate Bayesian Inference via Sparse grid Quadrature Evaluation for\n  Hierarchical Models", "comments": "35 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine conditioning techniques with sparse grid quadrature rules to\ndevelop a computationally efficient method to approximate marginal, but not\nnecessarily univariate, posterior quantities, yielding approximate Bayesian\ninference via Sparse grid Quadrature Evaluation (BISQuE) for hierarchical\nmodels. BISQuE reformulates posterior quantities as weighted integrals of\nconditional quantities, such as densities and expectations. Sparse grid\nquadrature rules allow computationally efficient approximation of high\ndimensional integrals, which appear in hierarchical models with many\nhyperparameters. BISQuE reduces computational effort relative to standard,\nMarkov chain Monte Carlo methods by at least two orders of magnitude on several\napplied and illustrative models. We also briefly discuss using BISQuE to apply\nIntegrated Nested Laplace Approximations (INLA) to models with more\nhyperparameters than is currently practical.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 18:07:31 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Hewitt", "Joshua", ""], ["Hoeting", "Jennifer A.", ""]]}, {"id": "1904.07295", "submitter": "Maja von Cube", "authors": "Maja von Cube, Martin Schumacher, Hein Putter, Jean-Francois Timsit,\n  Cornelis van der Velde, Martin Wolkewitz", "title": "The population-attributable fraction for time-dependent exposures using\n  dynamic prediction and landmarking", "comments": "A revised version has been submitted", "journal-ref": "Biometrical Journal, 2019", "doi": "10.1002/bimj.201800252", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The public health impact of a harmful exposure can be quantified by the\npopulation-attributable fraction (PAF). The PAF describes the attributable risk\ndue to an exposure and is often interpreted as the proportion of preventable\ncases if the exposure could be extinct. Difficulties in the definition and\ninterpretation of the PAF arise when the exposure of interest depends on time.\nThen, the definition of exposed and unexposed individuals is not\nstraightforward. We propose dynamic prediction and landmarking to define and\nestimate a PAF in this data situation. Two estimands are discussed which are\nbased on two hypothetical interventions that could prevent the exposure in\ndifferent ways. Considering the first estimand, at each landmark the estimation\nproblem is reduced to a time-independent setting. Then, estimation is simply\nperformed by using a generalized-linear model accounting for the current\nexposure state and further (time-varying) covariates. The second estimand is\nbased on counterfactual outcomes, estimation can be performed using\npseudo-values or inverse-probability weights. The approach is explored in a\nsimulation study and applied on two data examples. First, we study a large\nFrench database of intensive care unit patients to estimate the\npopulation-benefit of a pathogen-specific intervention that could prevent\nventilator-associated pneumonia caused by the pathogen Pseudomonas aeruginosa.\nMoreover, we quantify the population-attributable burden of locoregional and\ndistant recurrence in breast cancer patients.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:10:54 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["von Cube", "Maja", ""], ["Schumacher", "Martin", ""], ["Putter", "Hein", ""], ["Timsit", "Jean-Francois", ""], ["van der Velde", "Cornelis", ""], ["Wolkewitz", "Martin", ""]]}, {"id": "1904.07367", "submitter": "Simon Lunagomez", "authors": "Sim\\'on Lunag\\'omez, Sofia C. Olhede and Patrick J. Wolfe", "title": "Modeling Network Populations via Graph Distances", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a new class of models for multiple networks. The core\nidea is to parametrize a distribution on labelled graphs in terms of a\nFr\\'{e}chet mean graph (which depends on a user-specified choice of metric or\ngraph distance) and a parameter that controls the concentration of this\ndistribution about its mean. Entropy is the natural parameter for such control,\nvarying from a point mass concentrated on the Fr\\'{e}chet mean itself to a\nuniform distribution over all graphs on a given vertex set. We provide a\nhierarchical Bayesian approach for exploiting this construction, along with\nstraightforward strategies for sampling from the resultant posterior\ndistribution. We conclude by demonstrating the efficacy of our approach via\nsimulation studies and two multiple-network data analysis examples: one drawn\nfrom systems biology and the other from neuroscience.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 23:47:36 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 08:52:49 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Lunag\u00f3mez", "Sim\u00f3n", ""], ["Olhede", "Sofia C.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "1904.07383", "submitter": "Xialu Liu", "authors": "Xialu Liu and Elynn Chen", "title": "Helping Effects Against Curse of Dimensionality in Threshold Factor\n  Models for Matrix Time Series", "comments": "70 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As is known, factor analysis is a popular method to reduce dimension for\nhigh-dimensional data. For matrix data, the dimension reduction can be more\neffectively achieved through both row and column directions. In this paper, we\nintroduce a threshold factor models to analyze matrix-valued high-dimensional\ntime series data. The factor loadings are allowed to switch between regimes,\ncontrolling by a threshold variable. The estimation methods for loading spaces,\nthreshold value, and the number of factors are proposed. The asymptotic\nproperties of these estimators are investigated. Not only the strengths of\nthresholding and factors, but also their interactions from different directions\nand different regimes play an important role on the estimation performance.\nWhen the thresholding and factors are all strong across regimes, the estimation\nis immune to the impact that the increase of dimension brings, which breaks the\ncurse of dimensionality. When the thresholding in two directions and factors\nacross regimes have different levels of strength, we show that estimators for\nloadings and threshold value experience 'helping' effects against the curse of\ndimensionality. We also discover that even when the numbers of factors are\noverestimated, the estimators are still consistent. The proposed methods are\nillustrated with both simulated and real examples.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 00:41:09 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Liu", "Xialu", ""], ["Chen", "Elynn", ""]]}, {"id": "1904.07416", "submitter": "Kaijie Xue", "authors": "Kaijie Xue, Fang Yao", "title": "Distribution and correlation free two-sample test of high-dimensional\n  means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-sample test for high-dimensional means that requires neither\ndistributional nor correlational assumptions, besides some weak conditions on\nthe moments and tail properties of the elements in the random vectors. This\ntwo-sample test based on a nontrivial extension of the one-sample central limit\ntheorem (Chernozhukov et al., 2017) provides a practically useful procedure\nwith rigorous theoretical guarantees on its size and power assessment. In\nparticular, the proposed test is easy to compute and does not require the\nindependently and identically distributed assumption, which is allowed to have\ndifferent distributions and arbitrary correlation structures. Further desired\nfeatures include weaker moments and tail conditions than existing methods,\nallowance for highly unequal sample sizes, consistent power behavior under\nfairly general alternative, data dimension allowed to be exponentially high\nunder the umbrella of such general conditions. Simulated and real data examples\nare used to demonstrate the favorable numerical performance over existing\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 02:44:43 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Xue", "Kaijie", ""], ["Yao", "Fang", ""]]}, {"id": "1904.07477", "submitter": "Feng Li", "authors": "Lu Lin and Feng Li", "title": "A Global Bias-Correction DC Method for Biased Estimation under Memory\n  Constraint", "comments": "42 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes a global bias-correction divide-and-conquer (GBC-DC)\nrule for biased estimation under the case of memory constraint. In order to\nintroduce the new estimation, a closed representation of the local estimators\nobtained by the data in each batch is adopted, aiming to formulate a pro forma\nlinear regression between the local estimators and the true parameter of\ninterest. Least square method is then used within this framework to composite a\nglobal estimator of the parameter. Thus, the main advantage over the classical\nDC method is that the new GBC-DC method can absorb the information hidden in\nthe statistical structure and the variables in each batch of data.\nConsequently, the resulting global estimator is strictly unbiased even if the\nlocal estimator has a non-negligible bias. Moreover, the global estimator is\nconsistent, and even can achieve root-$n$ consistency, without the constraint\non the number of batches. Another attractive feature of the new method is\ncomputationally simple and efficient, without use of any iterative algorithm\nand local bias-correction. Specifically, the proposed GBC-DC method applies to\nvarious biased estimations such as shrinkage-type estimation and nonparametric\nregression estimation. Detailed simulation studies demonstrate that the\nproposed GBC-DC approach is significantly bias-corrected, and the behavior is\ncomparable with the full data estimation and is much better than the\ncompetitors.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 05:57:36 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 06:13:10 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Lin", "Lu", ""], ["Li", "Feng", ""]]}, {"id": "1904.07672", "submitter": "James Hodges", "authors": "Liying Luo, James S. Hodges", "title": "Constraints in Random Effects Age-Period-Cohort Models", "comments": "Submitted to \"Sociological Methodology\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random effects (RE) models have been widely used to study the contextual\neffects of structures such as neighborhood or school. The RE approach has\nrecently been applied to age-period-cohort (APC) models that are unidentified\nbecause the predictors are exactly linearly dependent. However, it has not been\nfully understood how the RE specification identifies these otherwise\nunidentified APC models. We address this challenge by first making explicit\nthat RE-APC models have greater -- not less -- rank deficiency than the\ntraditional fixed-effects model, followed by two empirical examples. We then\nprovide intuition and a mathematical proof to explain that for APC models with\none RE, treating one effect as an RE is equivalent to constraining the\nestimates of that effect's linear component and the random intercept to be\nzero. For APC models with two RE's, the effective constraints implied by the\nmodel depend on the true (i.e., in the data-generating mechanism) non-linear\ncomponents of the effects that are modeled as RE's, so that the estimated\nlinear components of the RE's are determined by the true non-linear components\nof those effects. In conclusion, RE-APC models impose arbitrary though highly\nobscure constraints and thus do not differ qualitatively from other constrained\nAPC estimators.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 13:55:25 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Luo", "Liying", ""], ["Hodges", "James S.", ""]]}, {"id": "1904.07680", "submitter": "Matthew Williams", "authors": "Terrance D. Savitsky, Matthew R. Williams", "title": "Pseudo Bayesian Mixed Models under Informative Sampling", "comments": "27 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When random effects are correlated with sample design variables, the usual\napproach of employing individual survey weights (constructed to be inversely\nproportional to the unit survey inclusion probabilities) to form a\npseudo-likelihood no longer produces asymptotically unbiased inference. We\nconstruct a weight-exponentiated formulation for the random effects\ndistribution that achieves unbiased inference for generating hyperparameters of\nthe random effects. We contrast our approach with frequentist methods that rely\non numerical integration to reveal that only the Bayesian method achieves both\nunbiased estimation with respect to the sampling design distribution and\nconsistency with respect to the population generating distribution. Our\nsimulations and real data example for a survey of business establishments\ndemonstrate the utility of our approach across different modeling formulations\nand sampling designs. This work serves as a capstone for recent developmental\nefforts that combine traditional survey estimation approaches with the Bayesian\nmodeling paradigm and provides a bridge across the two rich but disparate\nsub-fields.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 14:02:43 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 18:19:17 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 16:30:46 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 15:46:55 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Savitsky", "Terrance D.", ""], ["Williams", "Matthew R.", ""]]}, {"id": "1904.07701", "submitter": "Alessandra Cabassi", "authors": "Alessandra Cabassi, Paul D. W. Kirk", "title": "Multiple kernel learning for integrative consensus clustering of 'omic\n  datasets", "comments": "Manuscript: 18 pages, 6 figures. Supplement: 29 pages, 19 figures.\n  This version contains additional simulation studies and comparisons to other\n  methods. For associated R code, see https://CRAN.R-project.org/package=klic\n  and https://github.com/acabassi/klic-pancancer-analysis", "journal-ref": null, "doi": "10.1093/bioinformatics/btaa593", "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diverse applications - particularly in tumour subtyping - have demonstrated\nthe importance of integrative clustering techniques for combining information\nfrom multiple data sources. Cluster-Of-Clusters Analysis (COCA) is one such\napproach that has been widely applied in the context of tumour subtyping.\nHowever, the properties of COCA have never been systematically explored, and\nits robustness to the inclusion of noisy datasets, or datasets that define\nconflicting clustering structures, is unclear. We rigorously benchmark COCA,\nand present Kernel Learning Integrative Clustering (KLIC) as an alternative\nstrategy. KLIC frames the challenge of combining clustering structures as a\nmultiple kernel learning problem, in which different datasets each provide a\nweighted contribution to the final clustering. This allows the contribution of\nnoisy datasets to be down-weighted relative to more informative datasets. We\ncompare the performances of KLIC and COCA in a variety of situations through\nsimulation studies. We also present the output of KLIC and COCA in real data\napplications to cancer subtyping and transcriptional module discovery. R\npackages \"klic\" and \"coca\" are available on the Comprehensive R Archive\nNetwork.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 12:33:32 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 16:15:23 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 09:35:40 GMT"}, {"version": "v4", "created": "Mon, 18 May 2020 18:12:07 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Cabassi", "Alessandra", ""], ["Kirk", "Paul D. W.", ""]]}, {"id": "1904.07830", "submitter": "Timothy Coleman", "authors": "Tim Coleman, Wei Peng, and Lucas Mentch", "title": "Scalable and Efficient Hypothesis Testing with Random Forests", "comments": "52 pages, 10 figures [fixed some critical typo's with Algorithm 1]", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the last decade, random forests have established themselves as\namong the most accurate and popular supervised learning methods. While their\nblack-box nature has made their mathematical analysis difficult, recent work\nhas established important statistical properties like consistency and\nasymptotic normality by considering subsampling in lieu of bootstrapping.\nThough such results open the door to traditional inference procedures, all\nformal methods suggested thus far place severe restrictions on the testing\nframework and their computational overhead precludes their practical scientific\nuse. Here we propose a permutation-style testing approach to formally assess\nfeature significance. We establish asymptotic validity of the test via\nexchangeability arguments and show that the test maintains high power with\norders of magnitude fewer computations. As importantly, the procedure scales\neasily to big data settings where large training and testing sets may be\nemployed without the need to construct additional models. Simulations and\napplications to ecological data where random forests have recently shown\npromise are provided.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:15:15 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 21:51:07 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 20:31:43 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Coleman", "Tim", ""], ["Peng", "Wei", ""], ["Mentch", "Lucas", ""]]}, {"id": "1904.07920", "submitter": "Christopher Clack", "authors": "Leo Carlos-Sandberg and Christopher D. Clack", "title": "The Sensitivity of Trivariate Granger Causality to Test Criteria and\n  Data Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Trivariate Granger causality analysis seeks to distinguish between \"true\"\ncausality and \"spurious\" causality results from the topology of the system.\nHowever, this analysis is sensitive both to the choice of test criteria and the\npresence of noise, and this can lead to incorrect inference of causality:\neither to infer causality that does not exist (spurious causality), or to fail\nto infer causality that does exist (unidentified causality). Here we analyse\nthe effects of the choice of test criteria and the presence of noise and give\ngeneral conditions under which incorrect inference is likely to occur. By\nstudying the test criteria (likelihood ratio, Lagrange multiplier, Rao\nefficient scoring and Wald), we demonstrate that Rao efficient scoring and Wald\ntests are statistically indistinguishable and that for small sample sizes they\noffer a the lowest likelihood of spurious causality, with the likelihood ratio\ntest offering the lowest likelihood of unidentified causality. We also show the\nsample size at which convergence between these tests occurs. We also give\nempirical results for intrinsic noise (in a variable) and extrinsic noise\n(between an variable and a observer), with a varying signal-to-noise ratio for\neach variable, showing that for intrinsic noise a strong dependence on the\nsignal-to-noise ratio of the last variable exists, and for extrinsic noise no\ndependence the true topology exists.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 18:54:10 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Carlos-Sandberg", "Leo", ""], ["Clack", "Christopher D.", ""]]}, {"id": "1904.08018", "submitter": "Qing Zhou", "authors": "Seunghyun Min and Qing Zhou", "title": "Constructing confidence sets after lasso selection by randomized\n  estimator augmentation", "comments": "31 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a few methods have been developed recently for building confidence\nintervals after model selection, how to construct confidence sets for joint\npost-selection inference is still an open question. In this paper, we develop a\nnew method to construct confidence sets after lasso variable selection, with\nstrong numerical support for its accuracy and effectiveness. A key component of\nour method is to sample from the conditional distribution of the response $y$\ngiven the lasso active set, which, in general, is very challenging due to the\ntiny probability of the conditioning event. We overcome this technical\ndifficulty by using estimator augmentation to simulate from this conditional\ndistribution via Markov chain Monte Carlo given any estimate $\\tilde{\\mu}$ of\nthe mean $\\mu_0$ of $y$. We then incorporate a randomization step for the\nestimate $\\tilde{\\mu}$ in our sampling procedure, which may be interpreted as\nsimulating from a posterior predictive distribution by averaging over the\nuncertainty in $\\mu_0$. Our Monte Carlo samples offer great flexibility in the\nconstruction of confidence sets for multiple parameters. Extensive numerical\nresults show that our method is able to construct confidence sets with the\ndesired coverage rate and, moreover, that the diameter and volume of our\nconfidence sets are substantially smaller in comparison with a state-of-the-art\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 23:35:01 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 21:49:23 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Min", "Seunghyun", ""], ["Zhou", "Qing", ""]]}, {"id": "1904.08063", "submitter": "Alex Stivala", "authors": "Alex Stivala, Garry Robins, Alessandro Lomi", "title": "Exponential random graph model parameter estimation for very large\n  directed networks", "comments": "Minor correction of discussion of coverage and Type II error rate", "journal-ref": null, "doi": "10.1371/journal.pone.0227804", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential random graph models (ERGMs) are widely used for modeling social\nnetworks observed at one point in time. However the computational difficulty of\nERGM parameter estimation has limited the practical application of this class\nof models to relatively small networks, up to a few thousand nodes at most,\nwith usually only a few hundred nodes or fewer. In the case of undirected\nnetworks, snowball sampling can be used to find ERGM parameter estimates of\nlarger networks via network samples, and recently published improvements in\nERGM network distribution sampling and ERGM estimation algorithms have allowed\nERGM parameter estimates of undirected networks with over one hundred thousand\nnodes to be made. However the implementations of these algorithms to date have\nbeen limited in their scalability, and also restricted to undirected networks.\nHere we describe an implementation of the recently published Equilibrium\nExpectation (EE) algorithm for ERGM parameter estimation of large directed\nnetworks. We test it on some simulated networks, and demonstrate its\napplication to an online social network with over 1.6 million nodes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:16:53 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 00:46:47 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 21:32:52 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Stivala", "Alex", ""], ["Robins", "Garry", ""], ["Lomi", "Alessandro", ""]]}, {"id": "1904.08069", "submitter": "Alexandre Tartakovsky", "authors": "Ramakrishna Tipireddy and David A Barajas-Solano and Alexandre M.\n  Tartakovsky", "title": "Conditional Karhunen-Lo\\`eve expansion for uncertainty quantification\n  and active learning in partial differential equation models", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2020.109604", "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a conditional Karhunen-Lo\\`eve (KL) model to quantify and reduce\nuncertainty in a stochastic partial differential equation (SPDE) problem with\npartially-known space-dependent coefficient, $Y(x)$.\n  We assume that a small number of $Y(x)$ measurements are available and model\n$Y(x)$ with a KL expansion.\n  We achieve reduction in uncertainty by conditioning the KL expansion\ncoefficients on measurements.\n  We consider two approaches for conditioning the KL expansion: In Approach 1,\nwe condition the KL model first and then truncate it.\n  In Approach 2, we first truncate the KL expansion and then condition it.\n  We employ the conditional KL expansion together with Monte Carlo and sparse\ngrid collocation methods to compute the moments of the solution of the SPDE\nproblem.\n  Uncertainty of the problem is further reduced by adaptively selecting\nadditional observation locations using two active learning methods.\n  Method 1 minimizes the variance of the PDE coefficient, while Method 2\nminimizes the variance of the solution of the PDE.\n  We demonstrate that conditioning leads to dimension reduction of the KL\nrepresentation of $Y(x)$.\n  For a linear diffusion SPDE with uncertain log-normal coefficient, we show\nthat Approach 1 provides a more accurate approximation of the conditional\nlog-normal coefficient and solution of the SPDE than Approach 2 for the same\nnumber of random dimensions in a conditional KL expansion.\n  Furthermore, Approach 2 provides a good estimate for the number of terms of\nthe truncated KL expansion of the conditional field of Approach 1.\n  Finally, we demonstrate that active learning based on Method 2 is more\nefficient for uncertainty reduction in the SPDE's states (i.e., it leads to a\nlarger reduction of the variance) than active learning using Method 2.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:45:43 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Tipireddy", "Ramakrishna", ""], ["Barajas-Solano", "David A", ""], ["Tartakovsky", "Alexandre M.", ""]]}, {"id": "1904.08251", "submitter": "Boris Beranger", "authors": "Boris Beranger, Simone A. Padoan, Scott A. Sisson", "title": "Estimation and uncertainty quantification for extreme quantile regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of extreme quantile regions, spaces in which future extreme events\ncan occur with a given low probability, even beyond the range of the observed\ndata, is an important task in the analysis of extremes. Existing methods to\nestimate such regions are available, but do not provide any measures of\nestimation uncertainty. We develop univariate and bivariate schemes for\nestimating extreme quantile regions under the Bayesian paradigm that\noutperforms existing approaches and provides natural measures of quantile\nregion estimate uncertainty. We examine the method's performance in controlled\nsimulation studies. We illustrate the applicability of the proposed method by\nanalysing high bivariate quantiles for pairs of pollutants, conditionally on\ndifferent temperature gradations, recorded in Milan, Italy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 13:01:53 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 23:47:23 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 01:44:09 GMT"}, {"version": "v4", "created": "Tue, 27 Oct 2020 10:50:08 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Beranger", "Boris", ""], ["Padoan", "Simone A.", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1904.08356", "submitter": "Iker Perez", "authors": "Iker Perez and Theodore Kypraios", "title": "Scalable Bayesian Inference for Population Markov Jump Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for Markov jump processes (MJPs) where available\nobservations relate to either system states or jumps typically relies on\ndata-augmentation Markov Chain Monte Carlo. State-of-the-art developments\ninvolve representing MJP paths with auxiliary candidate jump times that are\nlater thinned. However, these algorithms are i) unfeasible in situations\ninvolving large or infinite capacity systems and ii) not amenable for all\nobservation types. In this paper we establish and present a general\ndata-augmentation framework for population MJPs based on uniformized\nrepresentations of the underlying non-stationary jump processes. This leads to\nmultiple novel MCMC samplers which enable exact (in the Monte Carlo sense)\ninference tasks for model parameters. We show that proposed samplers outperform\nexisting popular approaches, and offer substantial efficiency gains in\napplications to partially observed stochastic epidemics, immigration processes\nand predator-prey dynamical systems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 16:45:05 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Perez", "Iker", ""], ["Kypraios", "Theodore", ""]]}, {"id": "1904.08420", "submitter": "Yongcheng Qi", "authors": "Yizeng Li and Yongcheng Qi", "title": "Adjusted Empirical Likelihood Method for the Tail Index of A\n  Heavy-Tailed Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical likelihood is a well-known nonparametric method in statistics and\nhas been widely applied in statistical inference. The method has been employed\nby Lu and Peng (2002) to constructing confidence intervals for the tail index\nof a heavy-tailed distribution. It is demonstrated in Lu and Peng (2002) that\nthe empirical likelihood-based confidence intervals performs better than\nconfidence intervals based on normal approximation in terms of the coverage\nprobability. In general, the empirical likelihood method can be hindered by its\nimprecision in the coverage probability when the sample size is small. This may\ncause a serious undercoverage issue when we apply the empirical likelihood to\nthe tail index as only a very small portion of observations can be used in the\nestimation of the tail index. In this paper, we employ an adjusted empirical\nlikelihood method, developed by Chen et al. (2008) and Liu and Chen (2010), to\nconstructing confidence intervals of the tail index so as to achieve a better\naccuracy. We conduct a simulation study to compare the performance of the\nadjusted empirical likelihood method and the normal approximation method. Our\nsimulation results indicate that the adjusted empirical likelihood method\noutperforms other methods in terms of the coverage probability and length of\nconfidence intervals. We also apply the adjusted empirical likelihood method to\na real data set.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 15:55:07 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Li", "Yizeng", ""], ["Qi", "Yongcheng", ""]]}, {"id": "1904.08507", "submitter": "Rahul Ghosal", "authors": "Rahul Ghosal, Arnab Maity, Timothy Clark, Stefano B Longo", "title": "Variable Selection in Functional Linear Concurrent Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for variable selection in functional linear\nconcurrent regression. Our research is motivated by a fisheries footprint study\nwhere the goal is to identify important time-varying socio-structural drivers\ninfluencing patterns of seafood consumption, and hence fisheries footprint,\nover time, as well as estimating their dynamic effects. We develop a variable\nselection method in functional linear concurrent regression extending the\nclassically used scalar on scalar variable selection methods like LASSO, SCAD,\nand MCP. We show in functional linear concurrent regression the variable\nselection problem can be addressed as a group LASSO, and their natural\nextension; group SCAD or a group MCP problem. Through simulations, we\nillustrate our method, particularly with group SCAD or group MCP penalty, can\npick out the relevant variables with high accuracy and has minuscule false\npositive and false negative rate even when data is observed sparsely, is\ncontaminated with noise and the error process is highly non-stationary. We also\ndemonstrate two real data applications of our method in studies of dietary\ncalcium absorption and fisheries footprint in the selection of influential\ntime-varying covariates.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 21:14:24 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 16:45:57 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Ghosal", "Rahul", ""], ["Maity", "Arnab", ""], ["Clark", "Timothy", ""], ["Longo", "Stefano B", ""]]}, {"id": "1904.08515", "submitter": "Trang Nguyen", "authors": "Trang Quynh Nguyen, Ian Schmid, Elizabeth A. Stuart", "title": "Clarifying causal mediation analysis for the applied researcher:\n  Defining effects based on what we want to learn", "comments": null, "journal-ref": null, "doi": "10.1037/met0000299", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incorporation of causal inference in mediation analysis has led to\ntheoretical and methodological advancements -- effect definitions with causal\ninterpretation, clarification of assumptions required for effect\nidentification, and an expanding array of options for effect estimation.\nHowever, the literature on these results is fast-growing and complex, which may\nbe confusing to researchers unfamiliar with causal inference or unfamiliar with\nmediation. The goal of this paper is to help ease the understanding and\nadoption of causal mediation analysis. It starts by highlighting a key\ndifference between the causal inference and traditional approaches to mediation\nanalysis and making a case for the need for explicit causal thinking and the\ncausal inference approach in mediation analysis. It then explains in\nas-plain-as-possible language existing effect types, paying special attention\nto motivating these effects with different types of research questions, and\nusing concrete examples for illustration. This presentation differentiates two\nperspectives (or purposes of analysis): the explanatory perspective (aiming to\nexplain the total effect) and the interventional perspective (asking questions\nabout hypothetical interventions on the exposure and mediator, or\nhypothetically modified exposures). For the latter perspective, the paper\nproposes tapping into a general class of interventional effects that contains\nas special cases most of the usual effect types -- interventional direct and\nindirect effects, controlled direct effects and also a generalized\ninterventional direct effect type, as well as the total effect and overall\neffect. This general class allows flexible effect definitions which better\nmatch many research questions than the standard interventional direct and\nindirect effects.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 21:50:49 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 22:13:28 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Nguyen", "Trang Quynh", ""], ["Schmid", "Ian", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "1904.08538", "submitter": "Kyungchul Song", "authors": "Kyungchul Song", "title": "A Decomposition Analysis of Diffusion over a Large Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion over a network refers to the phenomenon of a change of state of a\ncross-sectional unit in one period leading to a change of state of its\nneighbors in the network in the next period. One may estimate or test for\ndiffusion by estimating a cross-sectionally aggregated correlation between\nneighbors over time from data. However, the estimated diffusion can be\nmisleading if the diffusion is confounded by omitted covariates. This paper\nfocuses on the measure of diffusion proposed by He and Song (2021), provides a\nmethod of decomposition analysis to measure the role of the covariates on the\nestimated diffusion, and develops an asymptotic inference procedure for the\ndecomposition analysis in such a situation. This paper also presents results\nfrom a Monte Carlo study on the small sample performance of the inference\nprocedure.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 23:54:55 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 11:55:04 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Song", "Kyungchul", ""]]}, {"id": "1904.08672", "submitter": "Francisco Javier Rubio", "authors": "Francisco J. Rubio, Bernard Rachet, Roch Giorgi, Camille Maringe,\n  Aurelien Belot", "title": "On models for the estimation of the excess mortality hazard in case of\n  insufficiently stratified life tables", "comments": "to appear in Biostatistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cancer epidemiology using population-based data, regression models for the\nexcess mortality hazard is a useful method to estimate cancer survival and to\ndescribe the association between prognosis factors and excess mortality. This\nmethod requires expected mortality rates from general population life tables:\neach cancer patient is assigned an expected (background) mortality rate\nobtained from the life tables, typically at least according to their age and\nsex, from the population they belong to. However, those life tables may be\ninsufficiently stratified, as some characteristics such as deprivation,\nethnicity, and comorbidities, are not available in the life tables for a number\nof countries. This may affect the background mortality rate allocated to each\npatient, and it has been shown that not including relevant information for\nassigning an expected mortality rate to each patient induces a bias in the\nestimation of the regression parameters of the excess hazard model. We propose\ntwo parametric corrections in excess hazard regression models, including a\nsingle-parameter or a random effect (frailty), to account for possible\nmismatches in the life table and thus misspecification of the background\nmortality rate. In an extensive simulation study, the good statistical\nperformance of the proposed approach is demonstrated, and we illustrate their\nuse on real population-based data of lung cancer patients. We present\nconditions and limitations of these methods, and provide some recommendations\nfor their use in practice.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 10:29:23 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Rubio", "Francisco J.", ""], ["Rachet", "Bernard", ""], ["Giorgi", "Roch", ""], ["Maringe", "Camille", ""], ["Belot", "Aurelien", ""]]}, {"id": "1904.08692", "submitter": "Maja von Cube", "authors": "Maja von Cube and Martin Schumacher and Sebastien Bailly and\n  Jean-Francois Timsit and Alain Lepape and Anne Savey and Anais Machut and\n  Martin Wolkewitz", "title": "The population-attributable fraction for time-dependent exposures and\n  competing risks - A discussion on estimands", "comments": "A revision has been submitted", "journal-ref": "Stat Med, 2019; 38: 3880-3895", "doi": "10.1002/sim.8208", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The population-attributable fraction (PAF) quantifies the public health\nimpact of a harmful exposure. Despite being a measure of significant importance\nan estimand accommodating complicated time-to-event data is not clearly\ndefined. We discuss current estimands of the PAF used to quantify the public\nhealth impact of an internal time-dependent exposure for data subject to\ncompeting outcomes. To overcome some limitations, we proposed a novel estimand\nwhich is based on dynamic prediction by landmarking. In a profound simulation\nstudy, we discuss interpretation and performance of the various estimands and\ntheir estimators. The methods are applied to a large French database to\nestimate the health impact of ventilator-associated pneumonia for patients in\nintensive care.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 11:13:27 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["von Cube", "Maja", ""], ["Schumacher", "Martin", ""], ["Bailly", "Sebastien", ""], ["Timsit", "Jean-Francois", ""], ["Lepape", "Alain", ""], ["Savey", "Anne", ""], ["Machut", "Anais", ""], ["Wolkewitz", "Martin", ""]]}, {"id": "1904.08895", "submitter": "Steven Howard", "authors": "Steven R. Howard, Samuel D. Pimentel", "title": "The uniform general signed rank test and its design sensitivity", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sensitivity analysis in an observational study tests whether the\nqualitative conclusions of an analysis would change if we were to allow for the\npossibility of limited bias due to confounding. The design sensitivity of a\nhypothesis test quantifies the asymptotic performance of the test in a\nsensitivity analysis against a particular alternative. We propose a new,\nnon-asymptotic, distribution-free test, the uniform general signed rank test,\nfor observational studies with paired data, and examine its performance under\nRosenbaum's sensitivity analysis model. Our test can be viewed as adaptively\nchoosing from among a large underlying family of signed rank tests, and we show\nthat the uniform test achieves design sensitivity equal to the maximum design\nsensitivity over the underlying family of signed rank tests. Our test thus\nachieves superior, and sometimes infinite, design sensitivity, indicating it\nwill perform well in sensitivity analyses on large samples. We support this\nconclusion with simulations and a data example, showing that the advantages of\nour test extend to moderate sample sizes as well.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:13:07 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Howard", "Steven R.", ""], ["Pimentel", "Samuel D.", ""]]}, {"id": "1904.09002", "submitter": "Chung-Chou Chang", "authors": "Qing Liu, Gong Tang, Joseph P. Costantino, Chung-Chou H. Chang", "title": "Landmark Proportional Subdistribution Hazards Models for Dynamic\n  Prediction of Cumulative Incidence Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An individualized risk prediction model that dynamically updates the\nprobability of a clinical event from a specific cause is valuable for\nphysicians to be able to optimize personalized treatment strategies in\nreal-time by incorporating all available information collected over the\nfollow-up. However, this is more complex and challenging when competing risks\nare present, because it requires simultaneously updating the overall survival\nand the cumulative incidence functions (CIFs) while adjusting for the\ntime-dependent covariates and time-varying covariate effects. In this study, we\ndeveloped a landmark proportional subdistribution hazards (PSH) model and a\nmore comprehensive supermodel by extending the landmark method to the Fine-Gray\nmodel. The performance of our models was assessed via simulations and through\nanalysis of data from a multicenter clinical trial for breast cancer patients.\nOur proposed models have appealing advantages over other dynamic prediction\nmodels for data with competing risks. First, our models are robust against\nviolations of the PSH assumption and can directly predict the conditional CIFs\nbypassing the estimation of overall survival and greatly simplify the\nprediction procedure. Second, our landmark PSH supermodel enables researchers\nto make predictions at a set of landmark points in one step. Third, the\nproposed models can easily incorporate various types of time-dependent\ninformation using existing standard software without computational burden.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 20:31:17 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Liu", "Qing", ""], ["Tang", "Gong", ""], ["Costantino", "Joseph P.", ""], ["Chang", "Chung-Chou H.", ""]]}, {"id": "1904.09027", "submitter": "Bai Jiang", "authors": "Jianqing Fan and Yongyi Guo and Bai Jiang", "title": "Adaptive Huber Regression on Markov-dependent Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional linear regression has been intensively studied in the\ncommunity of statistics in the last two decades. For the convenience of\ntheoretical analyses, classical methods usually assume independent observations\nand sub-Gaussian-tailed errors. However, neither of them hold in many real\nhigh-dimensional time-series data. Recently [Sun, Zhou, Fan, 2019, J. Amer.\nStat. Assoc., in press] proposed Adaptive Huber Regression (AHR) to address the\nissue of heavy-tailed errors. They discover that the robustification parameter\nof the Huber loss should adapt to the sample size, the dimensionality, and the\nmoments of the heavy-tailed errors. We progress in a vertical direction and\njustify AHR on dependent observations. Specifically, we consider an important\ndependence structure -- Markov dependence. Our results show that the Markov\ndependence impacts on the adaption of the robustification parameter and the\nestimation of regression coefficients in the way that the sample size should be\ndiscounted by a factor depending on the spectral gap of the underlying Markov\nchain.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 22:31:22 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 22:31:47 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Fan", "Jianqing", ""], ["Guo", "Yongyi", ""], ["Jiang", "Bai", ""]]}, {"id": "1904.09088", "submitter": "Tony Sit", "authors": "Junyao Chen, Tony Sit and Hoi Ying Wong", "title": "Simulation-based Value-at-Risk for Nonlinear Portfolios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Value-at-risk (VaR) has been playing the role of a standard risk measure\nsince its introduction. In practice, the delta-normal approach is usually\nadopted to approximate the VaR of portfolios with option positions. Its\neffectiveness, however, substantially diminishes when the portfolios concerned\ninvolve a high dimension of derivative positions with nonlinear payoffs; lack\nof closed form pricing solution for these potentially highly correlated,\nAmerican-style derivatives further complicates the problem. This paper proposes\na generic simulation-based algorithm for VaR estimation that can be easily\napplied to any existing procedures. Our proposal leverages cross-sectional\ninformation and applies variable selection techniques to simplify the existing\nsimulation framework. Asymptotic properties of the new approach demonstrate\nfaster convergence due to the additional model selection component introduced.\nWe have also performed sets of numerical results that verify the effectiveness\nof our approach in comparison with some existing strategies.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 06:24:22 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Chen", "Junyao", ""], ["Sit", "Tony", ""], ["Wong", "Hoi Ying", ""]]}, {"id": "1904.09204", "submitter": "Hau-tieng Wu", "authors": "Matan Gavish, Ronen Talmon, Pei-Chun Su, Hau-Tieng Wu", "title": "Optimal Recovery of Mahalanobis Distance in High Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of Mahalanobis distance (MD) estimation\nfrom high-dimensional noisy data. By relying on recent transformative results\nin covariance matrix estimation, we demonstrate the sensitivity of MD to\nmeasurement noise, determining the exact asymptotic signal-to-noise ratio at\nwhich MD fails, and quantifying its performance otherwise. In addition, for an\nappropriate loss function, we propose an asymptotically optimal shrinker, which\nis shown to be beneficial over the classical implementation of the MD, both\nanalytically and in simulations.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 14:20:42 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Gavish", "Matan", ""], ["Talmon", "Ronen", ""], ["Su", "Pei-Chun", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "1904.09232", "submitter": "Osama Idais", "authors": "Osama Idais and Rainer Schwabe", "title": "Analytic solutions for locally optimal designs for gamma models having\n  linear predictor without intercept", "comments": "18 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gamma model is a generalized linear model for gamma-distributed outcomes.\nThe model is widely applied in psychology, ecology or medicine. In this paper\nwe focus on gamma models having a linear predictor without intercept. For a\nspecific scenario sets of locally D- and A-optimal designs are to be developed.\nRecently, Gaffke et al. (2018) established a complete class and an essentially\ncomplete class of designs for gamma models to obtain locally D-optimal designs.\nHowever to extend this approach to gamma model without an intercept term is\ncomplicated. To solve that further techniques have to be developed in the\ncurrent work. Further, by a suitable transformation between gamma models with\nand without intercept optimality results may be transferred from one model to\nthe other. Additionally by means of The General Equivalence Theorem optimality\ncan be characterized for multiple regression by a system of polynomial\ninequalities which can be solved analytically or by computer algebra. By this\nnecessary and sufficient conditions on the parameter values can be obtained for\nthe local D-optimality of particular designs. The robustness of the derived\ndesigns with respect to misspecifications of the initial parameter values is\nexamined by means of their local D-efficiencies.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 22:06:46 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Idais", "Osama", ""], ["Schwabe", "Rainer", ""]]}, {"id": "1904.09306", "submitter": "Mansur Arief", "authors": "Zhiyuan Huang, Mansur Arief, Henry Lam, Ding Zhao", "title": "Evaluation Uncertainty in Data-Driven Self-Driving Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety evaluation of self-driving technologies has been extensively studied.\nOne recent approach uses Monte Carlo based evaluation to estimate the\noccurrence probabilities of safety-critical events as safety measures. These\nMonte Carlo samples are generated from stochastic input models constructed\nbased on real-world data. In this paper, we propose an approach to assess the\nimpact on the probability estimates from the evaluation procedures due to the\nestimation error caused by data variability. Our proposed method merges the\nclassical bootstrap method for estimating input uncertainty with a likelihood\nratio based scheme to reuse experiment outputs. This approach is economical and\nefficient in terms of implementation costs in assessing input uncertainty for\nthe evaluation of self-driving technology. We use an example in autonomous\nvehicle (AV) safety evaluation to demonstrate the proposed approach as a\ndiagnostic tool for the quality of the fitted input model.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 18:21:48 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 02:08:25 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Huang", "Zhiyuan", ""], ["Arief", "Mansur", ""], ["Lam", "Henry", ""], ["Zhao", "Ding", ""]]}, {"id": "1904.09339", "submitter": "Reza Mohammadi", "authors": "Reza Mohammadi, Matthew Pratola, Maurits Kaptein", "title": "Continuous-Time Birth-Death MCMC for Bayesian Regression Tree Models", "comments": "Published at http://jmlr.org/papers/v21/19-307 in the Journal of\n  Machine Learning Research (https://www.jmlr.org)", "journal-ref": "Journal of Machine Learning Research 2020, Vol. 21, No. 201, 1-26", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision trees are flexible models that are well suited for many statistical\nregression problems. In a Bayesian framework for regression trees, Markov Chain\nMonte Carlo (MCMC) search algorithms are required to generate samples of tree\nmodels according to their posterior probabilities. The critical component of\nsuch an MCMC algorithm is to construct good Metropolis-Hastings steps for\nupdating the tree topology. However, such algorithms frequently suffering from\nlocal mode stickiness and poor mixing. As a result, the algorithms are slow to\nconverge. Hitherto, authors have primarily used discrete-time birth/death\nmechanisms for Bayesian (sums of) regression tree models to explore the model\nspace. These algorithms are efficient only if the acceptance rate is high which\nis not always the case. Here we overcome this issue by developing a new search\nalgorithm which is based on a continuous-time birth-death Markov process. This\nsearch algorithm explores the model space by jumping between parameter spaces\ncorresponding to different tree structures. In the proposed algorithm, the\nmoves between models are always accepted which can dramatically improve the\nconvergence and mixing properties of the MCMC algorithm. We provide theoretical\nsupport of the algorithm for Bayesian regression tree models and demonstrate\nits performance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 20:37:05 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 11:55:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mohammadi", "Reza", ""], ["Pratola", "Matthew", ""], ["Kaptein", "Maurits", ""]]}, {"id": "1904.09347", "submitter": "Richard Samworth", "authors": "Thomas B. Berrett and Richard J. Samworth", "title": "Efficient two-sample functional estimation and the super-oracle\n  phenomenon", "comments": "82 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of two-sample integral functionals, of the type\nthat occur naturally, for example, when the object of interest is a divergence\nbetween unknown probability densities. Our first main result is that, in wide\ngenerality, a weighted nearest neighbour estimator is efficient, in the sense\nof achieving the local asymptotic minimax lower bound. Moreover, we also prove\na corresponding central limit theorem, which facilitates the construction of\nasymptotically valid confidence intervals for the functional, having\nasymptotically minimal width. One interesting consequence of our results is the\ndiscovery that, for certain functionals, the worst-case performance of our\nestimator may improve on that of the natural `oracle' estimator, which is given\naccess to the values of the unknown densities at the observations.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:22:51 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1904.09420", "submitter": "Raanju Ragavendar Sundararajan", "authors": "Raanju Ragavendar Sundararajan, Vladas Pipiras and Mohsen Pourahmadi", "title": "Stationary subspace analysis of nonstationary covariance processes:\n  eigenstructure description and testing", "comments": "43 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stationary subspace analysis (SSA) searches for linear combinations of the\ncomponents of nonstationary vector time series that are stationary. These\nlinear combinations and their number defne an associated stationary subspace\nand its dimension. SSA is studied here for zero mean nonstationary covariance\nprocesses. We characterize stationary subspaces and their dimensions in terms\nof eigenvalues and eigenvectors of certain symmetric matrices. This\ncharacterization is then used to derive formal statistical tests for estimating\ndimensions of stationary subspaces. Eigenstructure-based techniques are also\nproposed to estimate stationary subspaces, without relying on previously used\ncomputationally intensive optimization-based methods. Finally, the introduced\nmethodologies are examined on simulated and real data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 08:22:10 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Sundararajan", "Raanju Ragavendar", ""], ["Pipiras", "Vladas", ""], ["Pourahmadi", "Mohsen", ""]]}, {"id": "1904.09480", "submitter": "Ionas Erb", "authors": "Ionas Erb", "title": "Partial Correlations in Compositional Data Analysis", "comments": "11 pages, 1 figure, submitted to CoDaWork 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial correlations quantify linear association between two variables\nadjusting for the influence of the remaining variables. They form the backbone\nfor graphical models and are readily obtained from the inverse of the\ncovariance matrix. For compositional data, the covariance structure is\nspecified from log ratios of variables, so unless we try to \"open\" the data via\na normalization, this implies changes in the definition and interpretation of\npartial correlations. In the present work, we elucidate how results derived by\nAitchison (1986) lead to a natural definition of partial correlation that has a\nnumber of advantages over current measures of association. For this, we show\nthat the residuals of log-ratios between a variable with a reference, when\nadjusting for all remaining variables including the reference, are\nreference-independent. Since the reference itself can be controlled for,\ncorrelations between residuals are defined for the variables directly without\nthe necessity to recur to ratios except when specifying which variables are\npartialled out. Thus, perhaps surprisingly, partial correlations do not have\nthe problems commonly found with measures of pairwise association on\ncompositional data. They are well-defined between two variables, are properly\nscaled, and allow for negative association. By design, they are\nsubcompositionally incoherent, but they share this property with conventional\npartial correlations (where results change when adjusting for the influence of\nfewer variables). We discuss the equivalence with normalization-based\napproaches whenever the normalizing variables are controlled for. We also\ndiscuss the partial variances and correlations we obtain from a previously\nstudied data set of Roman glass cups.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 18:59:56 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Erb", "Ionas", ""]]}, {"id": "1904.09514", "submitter": "Mohammad Nabhan", "authors": "Mohammad Nabhan, Yajun Mei and Jianjun Shi", "title": "High Dimensional Process Monitoring Using Robust Sparse Probabilistic\n  Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional data has introduced challenges that are difficult to address\nwhen attempting to implement classical approaches of statistical process\ncontrol. This has made it a topic of interest for research due in recent years.\nHowever, in many cases, data sets have underlying structures, such as in\nadvanced manufacturing systems. If extracted correctly, efficient methods for\nprocess control can be developed. This paper proposes a robust sparse\ndimensionality reduction approach for correlated high-dimensional process\nmonitoring to address the aforementioned issues. The developed monitoring\ntechnique uses robust sparse probabilistic PCA to reduce the dimensionality of\nthe data stream while retaining interpretability. The proposed methodology\nutilizes Bayesian variational inference to obtain the estimates of a\nprobabilistic representation of PCA. Simulation studies were conducted to\nverify the efficacy of the proposed methodology. Furthermore, we conducted a\ncase study for change detection for in-line Raman spectroscopy to validate the\nefficiency of our proposed method in a practical scenario.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 23:28:41 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Nabhan", "Mohammad", ""], ["Mei", "Yajun", ""], ["Shi", "Jianjun", ""]]}, {"id": "1904.09532", "submitter": "Shengming Luo", "authors": "Jiashun Jin, Zheng Tracy Ke, Shengming Luo", "title": "Optimal Adaptivity of Signed-Polygon Statistics for Network Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a symmetric social network, we are interested in testing whether it has\nonly one community or multiple communities. The desired tests should (a)\naccommodate severe degree heterogeneity, (b) accommodate mixed-memberships, (c)\nhave a tractable null distribution, and (d) adapt automatically to different\nlevels of sparsity, and achieve the optimal phase diagram. How to find such a\ntest is a challenging problem.\n  We propose the Signed Polygon as a class of new tests. Fixing $m \\geq 3$, for\neach $m$-gon in the network, define a score using the centered adjacency\nmatrix. The sum of such scores is then the $m$-th order Signed Polygon\nstatistic. The Signed Triangle (SgnT) and the Signed Quadrilateral (SgnQ) are\nspecial examples of the Signed Polygon.\n  We show that both the SgnT and SgnQ tests satisfy (a)-(d), and especially,\nthey work well for both very sparse and less sparse networks. Our proposed\ntests compare favorably with the existing tests. For example, the EZ and GC\ntests behave unsatisfactorily in the less sparse case and do not achieve the\noptimal phase diagram. Also, many existing tests do not allow for severe\nheterogeneity or mixed-memberships, and they behave unsatisfactorily in our\nsettings.\n  The analysis of the SgnT and SgnQ tests is delicate and extremely tedious,\nand the main reason is that we need a unified proof that covers a wide range of\nsparsity levels and a wide range of degree heterogeneity. For lower bound\ntheory, we use a phase transition framework, which includes the standard\nminimax argument, but is more informative. The proof uses classical theorems on\nmatrix scaling.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 02:41:07 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 20:04:31 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Jin", "Jiashun", ""], ["Ke", "Zheng Tracy", ""], ["Luo", "Shengming", ""]]}, {"id": "1904.09609", "submitter": "Ranjan Maitra", "authors": "Nicholas S. Berry and Ranjan Maitra", "title": "TiK-means: $K$-means clustering for skewed groups", "comments": "15 pages, 6 figures, to appear in Statistical Analysis and Data\n  Mining - The ASA Data Science Journal", "journal-ref": "Statistical Analysis and Data Mining -- The ASA Data Science\n  Journal, 2019, volume 12, number 3, pages 223-233", "doi": "10.1002/sam11416", "report-no": null, "categories": "stat.ML astro-ph.HE cs.CV cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $K$-means algorithm is extended to allow for partitioning of skewed\ngroups. Our algorithm is called TiK-Means and contributes a $K$-means type\nalgorithm that assigns observations to groups while estimating their\nskewness-transformation parameters. The resulting groups and transformation\nreveal general-structured clusters that can be explained by inverting the\nestimated transformation. Further, a modification of the jump statistic chooses\nthe number of groups. Our algorithm is evaluated on simulated and real-life\ndatasets and then applied to a long-standing astronomical dispute regarding the\ndistinct kinds of gamma ray bursts.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 14:32:42 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Berry", "Nicholas S.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1904.09632", "submitter": "Jeremiah Zhe Liu", "authors": "Jeremiah Zhe Liu", "title": "Gaussian Process Regression and Classification under Mathematical\n  Constraints with Learning Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce constrained Gaussian process (CGP), a Gaussian process model for\nrandom functions that allows easy placement of mathematical constrains (e.g.,\nnon-negativity, monotonicity, etc) on its sample functions. CGP comes with\nclosed-form probability density function (PDF), and has the attractive feature\nthat its posterior distributions for regression and classification are again\nCGPs with closed-form expressions. Furthermore, we show that CGP inherents the\noptimal theoretical properties of the Gaussian process, e.g. rates of posterior\ncontraction, due to the fact that CGP is an Gaussian process with a more\nefficient model space.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 17:19:47 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Liu", "Jeremiah Zhe", ""]]}, {"id": "1904.09647", "submitter": "Zhenhua Lin", "authors": "Zhenhua Lin and Hans-Georg M\\\"uller", "title": "Total Variation Regularized Fr\\'echet Regression for Metric-Space Valued\n  Data", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Euclidean data that are indexed with a scalar predictor such as time are\nincreasingly encountered in data applications, while statistical methodology\nand theory for such random objects are not well developed yet. To address the\nneed for new methodology in this area, we develop a total variation\nregularization technique for nonparametric Fr\\'echet regression, which refers\nto a regression setting where a response residing in a metric space is paired\nwith a scalar predictor and the target is a conditional Fr\\'echet mean.\nSpecifically, we seek to approximate an unknown metric-space valued function by\nan estimator that minimizes the Fr\\'echet version of least squares and at the\nsame time has small total variation, appropriately defined for metric-space\nvalued objects. We show that the resulting estimator is representable by a\npiece-wise constant function and establish the minimax convergence rate of the\nproposed estimator for metric data objects that reside in Hadamard spaces. We\nillustrate the numerical performance of the proposed method for both simulated\nand real data, including metric spaces of symmetric positive-definite matrices\nwith the affine-invariant distance, of probability distributions on the real\nline with the Wasserstein distance, and of phylogenetic trees with the\nBillera--Holmes--Vogtmann metric.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 19:18:24 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 10:14:03 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 11:18:50 GMT"}, {"version": "v4", "created": "Wed, 17 Feb 2021 09:21:10 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lin", "Zhenhua", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1904.09668", "submitter": "Alexander Litvinenko", "authors": "Sergey Dolgov, Alexander Litvinenko, Dishi Liu", "title": "Kriging in Tensor Train data format", "comments": "19 pages,4 figures, 1 table, UNCECOMP 2019 3rd International\n  Conference on Uncertainty Quantification in Computational Sciences and\n  Engineering 24-26 June 2019, Crete, Greece https://2019.uncecomp.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combination of low-tensor rank techniques and the Fast Fourier transform\n(FFT) based methods had turned out to be prominent in accelerating various\nstatistical operations such as Kriging, computing conditional covariance,\ngeostatistical optimal design, and others. However, the approximation of a full\ntensor by its low-rank format can be computationally formidable. In this work,\nwe incorporate the robust Tensor Train (TT) approximation of covariance\nmatrices and the efficient TT-Cross algorithm into the FFT-based Kriging. It is\nshown that here the computational complexity of Kriging is reduced to\n$\\mathcal{O}(d r^3 n)$, where $n$ is the mode size of the estimation grid, $d$\nis the number of variables (the dimension), and $r$ is the rank of the TT\napproximation of the covariance matrix. For many popular covariance functions\nthe TT rank $r$ remains stable for increasing $n$ and $d$. The advantages of\nthis approach against those using plain FFT are demonstrated in synthetic and\nreal data examples.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 22:01:01 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Dolgov", "Sergey", ""], ["Litvinenko", "Alexander", ""], ["Liu", "Dishi", ""]]}, {"id": "1904.09733", "submitter": "Raffaele Argiento", "authors": "Raffaele Argiento and Maria De Iorio", "title": "Is infinity that far? A Bayesian nonparametric perspective of finite\n  mixture models", "comments": "46 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Mixture models are one of the most widely used statistical tools when dealing\nwith data from heterogeneous populations. This paper considers the\nlong-standing debate over finite mixture and infinite mixtures and brings the\ntwo modelling strategies together, by showing that a finite mixture is simply a\nrealization of a point process. Following a Bayesian nonparametric perspective,\nwe introduce a new class of prior: the Normalized Independent Point Processes.\nWe investigate the probabilistic properties of this new class. Moreover, we\ndesign a conditional algorithm for finite mixture models with a random number\nof components overcoming the challenges associated with the Reversible Jump\nscheme and the recently proposed marginal algorithms. We illustrate our model\non real data and discuss an important application in population genetics.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 05:59:47 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Argiento", "Raffaele", ""], ["De Iorio", "Maria", ""]]}, {"id": "1904.10036", "submitter": "Michael Higgins", "authors": "James J. Higgins, Michael J. Higgins, Jinguang Lin", "title": "From one environment to many: The problem of replicability of\n  statistical inferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among plausible causes for replicability failure, one that has not received\nsufficient attention is the environment in which the research is conducted.\nConsisting of the population, equipment, personnel, and various conditions such\nas location, time, and weather, the research environment can affect treatments\nand outcomes, and changes in the research environment that occur when an\nexperiment is redone can affect replicability. We examine the extent to which\nsuch changes contribute to replicability failure. Our framework is that of an\ninitial experiment that generates the data and a follow-up experiment that is\ndone the same way except for a change in the research environment. We assume\nthat the initial experiment satisfies the assumptions of the two-sample\nt-statistic and that the follow-up experiment is described by a mixed model\nwhich includes environmental parameters. We derive expressions for the effect\nthat the research environment has on power, sample size selection, p-values,\nand confidence levels. We measure the size of the environmental effect with the\nenvironmental effect ratio EER which is the ratio of the standard deviations of\nenvironment by treatment interaction and error. By varying EER, it is possible\nto determine conditions that favor replicability and those that do not.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 19:11:42 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 17:45:32 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 23:01:35 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Higgins", "James J.", ""], ["Higgins", "Michael J.", ""], ["Lin", "Jinguang", ""]]}, {"id": "1904.10046", "submitter": "Priyam Das", "authors": "Raju Maiti, Jialiang Li, Priyam Das, Lei Feng, Derek Hausenloy, Bibhas\n  Chakraborty", "title": "A distribution-free smoothed combination method of biomarkers to improve\n  diagnostic accuracy in multi-category classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Results from multiple diagnostic tests are usually combined to improve the\noverall diagnostic accuracy. For binary classification, maximization of the\nempirical estimate of the area under the receiver operating characteristic\n(ROC) curve is widely adopted to produce the optimal linear combination of\nmultiple biomarkers. In the presence of large number of biomarkers, this method\nproves to be computationally expensive and difficult to implement since it\ninvolves maximization of a discontinuous, non-smooth function for which\ngradient-based methods cannot be used directly. Complexity of this problem\nincreases when the classification problem becomes multi-category. In this\narticle, we develop a linear combination method that maximizes a smooth\napproximation of the empirical Hypervolume Under Manifolds (HUM) for\nmulti-category outcome. We approximate HUM by replacing the indicator function\nwith the sigmoid function or normal cumulative distribution function (CDF).\nWith the above smooth approximations, efficient gradient-based algorithms can\nbe employed to obtain better solution with less computing time. We show that\nunder some regularity conditions, the proposed method yields consistent\nestimates of the coefficient parameters. We also derive the asymptotic\nnormality of the coefficient estimates. We conduct extensive simulations to\nexamine our methods. Under different simulation scenarios, the proposed methods\nare compared with other existing methods and are shown to outperform them in\nterms of diagnostic accuracy. The proposed method is illustrated using two real\nmedical data sets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 19:48:19 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Maiti", "Raju", ""], ["Li", "Jialiang", ""], ["Das", "Priyam", ""], ["Feng", "Lei", ""], ["Hausenloy", "Derek", ""], ["Chakraborty", "Bibhas", ""]]}, {"id": "1904.10265", "submitter": "Per Arnqvist", "authors": "Per Arnqvist and Sara Sj\\\"ostedt de Luna", "title": "Model based functional clustering of varved lake sediments", "comments": "34 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a model-based method for clustering subjects for\nwhich functional data together with covariates are observed. The model allows\nthe covariance structures within the different clusters to be different. The\nmodel thus extends a model proposed by James and Sugar (2003). We derive an EM\nalgorithm to estimate the parameters. The method is applied to annually\nlaminated (varved) sediment from lake Kassj\\\"on in northern Sweden, to infer on\npast climate changes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 12:07:19 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Arnqvist", "Per", ""], ["de Luna", "Sara Sj\u00f6stedt", ""]]}, {"id": "1904.10279", "submitter": "Age Smilde", "authors": "Age K. Smilde, Yipeng Song, Johan A. Westerhuis, Henk A.L. Kiers,\n  Nanne Aben and Lodewyk F.A. Wessels", "title": "Heterofusion: Fusing genomics data of different measurement scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In systems biology, it is becoming increasingly common to measure biochemical\nentities at different levels of the same biological system. Hence, data fusion\nproblems are abundant in the life sciences. With the availability of a\nmultitude of measuring techniques, one of the central problems is the\nheterogeneity of the data. In this paper, we discuss a specific form of\nheterogeneity, namely that of measurements obtained at different measurement\nscales, such as binary, ordinal, interval and ratio-scaled variables. Three\ngeneric fusion approaches are presented of which two are new to the systems\nbiology community. The methods are presented, put in context and illustrated\nwith a real-life genomics example.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 12:35:29 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Smilde", "Age K.", ""], ["Song", "Yipeng", ""], ["Westerhuis", "Johan A.", ""], ["Kiers", "Henk A. L.", ""], ["Aben", "Nanne", ""], ["Wessels", "Lodewyk F. A.", ""]]}, {"id": "1904.10345", "submitter": "Jon Steingrimsson", "authors": "Jon Arni Steingrimsson", "title": "Deep Learning for Survival Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscripts develops a new class of deep learning algorithms for\noutcomes that are potentially censored. To account for censoring, the\nunobservable loss function used in the absence of censoring is replaced by a\ncensoring unbiased transformation. The resulting class of algorithms can be\nused to estimate both survival probabilities and restricted mean survival. We\nshow how the deep learning algorithms can be implemented using software for\nuncensored data using a form of response transformation. Simulations and\nanalysis of the Netherlands 70 Gene Signature Data show strong performance of\nthe proposed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 14:08:38 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Steingrimsson", "Jon Arni", ""]]}, {"id": "1904.10406", "submitter": "George Vega Yon", "authors": "George G. Vega Yon, Andrew Slaughter, Kayla de la Haye", "title": "Exponential Random Graph models for Little Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.socnet.2020.07.005", "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Statistical models for social networks have enabled researchers to study\ncomplex social phenomena that give rise to observed patterns of relationships\namong social actors and to gain a rich understanding of the interdependent\nnature of social ties and actors. Much of this research has focused on social\nnetworks within medium to large social groups. To date, these advances in\nstatistical models for social networks, and in particular, of\nExponential-Family Random Graph Models (ERGMS), have rarely been applied to the\nstudy of small networks, despite small network data in teams, families, and\npersonal networks being common in many fields. In this paper, we revisit the\nestimation of ERGMs for small networks and propose using exhaustive enumeration\nwhen possible. We developed an R package that implements the estimation of\npooled ERGMs for small networks using Maximum Likelihood Estimation (MLE),\ncalled \"ergmito\". Based on the results of an extensive simulation study to\nassess the properties of the MLE estimator, we conclude that there are several\nbenefits of direct MLE estimation compared to approximate methods and that this\ncreates opportunities for valuable methodological innovations that can be\napplied to modeling social networks with ERGMs.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 16:16:15 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 17:17:12 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Yon", "George G. Vega", ""], ["Slaughter", "Andrew", ""], ["de la Haye", "Kayla", ""]]}, {"id": "1904.10416", "submitter": "Haozhe Zhang", "authors": "Haozhe Zhang, Dan Nettleton, Zhengyuan Zhu", "title": "Regression-Enhanced Random Forests", "comments": "12 pages, 5 figures", "journal-ref": "In JSM Proceedings (2017), Section on Statistical Learning and\n  Data Science, Alexandria, VA: American Statistical Association. 636 -- 647", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forest (RF) methodology is one of the most popular machine learning\ntechniques for prediction problems. In this article, we discuss some cases\nwhere random forests may suffer and propose a novel generalized RF method,\nnamely regression-enhanced random forests (RERFs), that can improve on RFs by\nborrowing the strength of penalized parametric regression. The algorithm for\nconstructing RERFs and selecting its tuning parameters is described. Both\nsimulation study and real data examples show that RERFs have better predictive\nperformance than RFs in important situations often encountered in practice.\nMoreover, RERFs may incorporate known relationships between the response and\nthe predictors, and may give reliable predictions in extrapolation problems\nwhere predictions are required at points out of the domain of the training\ndataset. Strategies analogous to those described here can be used to improve\nother machine learning methods via combination with penalized parametric\nregression techniques.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 16:45:07 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Zhang", "Haozhe", ""], ["Nettleton", "Dan", ""], ["Zhu", "Zhengyuan", ""]]}, {"id": "1904.10499", "submitter": "Mar\\'ia Juliana Gambini", "authors": "Alejandro C. Frery, Juliana Gambini", "title": "Comparing Samples from the $\\mathcal{G}^0$ Distribution using a Geodesic\n  Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\mathcal{G}^0$ distribution is widely used for monopolarized SAR image\nmodeling because it can characterize regions with different degree of texture\naccurately. It is indexed by three parameters: the number of looks (which can\nbe estimated for the whole image), a scale parameter and a texture parameter.\nThis paper presents a new proposal for comparing samples from the\n$\\mathcal{G}^0$ distribution using a Geodesic Distance (GD) as a measure of\ndissimilarity between models. The objective is quantifying the difference\nbetween pairs of samples from SAR data using both local parameters (scale and\ntexture) of the $\\mathcal{G}^0$ distribution. We propose three tests based on\nthe GD which combine the tests presented in~\\cite{GeodesicDistanceGI0JSTARS},\nand we estimate their probability distributions using permutation methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 19:13:29 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Frery", "Alejandro C.", ""], ["Gambini", "Juliana", ""]]}, {"id": "1904.10575", "submitter": "Yan Liu", "authors": "Yan Liu, Minggen Lu and Christopher S. McMahan", "title": "A penalized likelihood approach for efficiently estimating a partially\n  linear additive transformation model with current status data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current status data are commonly encountered in medical and epidemiological\nstudies in which the failure time for study units is the outcome variable of\ninterest. Data of this form are characterized by the fact that the failure time\nis not directly observed but rather is known relative to an observation time;\ni.e., the failure times are either left- or right-censored. Due to its\nstructure, the analysis of such data can be challenging. To circumvent these\nchallenges and to provide for a flexible modeling construct which can be used\nto analyze current status data, herein, a partially linear additive\ntransformation model is proposed. In the formulation of this model, constrained\n$B$-splines are employed to model the monotone transformation function and\nnonlinear covariate effects. To provide for more efficient estimates, a\npenalization technique is used to regularize the estimation of all unknown\nfunctions. An easy to implement hybrid algorithm is developed for model fitting\nand a simple estimator of the large-sample variance-covariance matrix is\nproposed. It is shown theoretically that the proposed estimators of the\nfinite-dimensional regression coefficients are root-$n$ consistent,\nasymptotically normal, and achieve the semi-parametric information bound while\nthe estimators of the nonparametric components attain the optimal rate of\nconvergence. The finite-sample performance of the proposed methodology is\nevaluated through extensive numerical studies and is further demonstrated\nthrough the analysis of uterine leiomyomata data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 23:59:26 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Liu", "Yan", ""], ["Lu", "Minggen", ""], ["McMahan", "Christopher S.", ""]]}, {"id": "1904.10582", "submitter": "Eric Chi", "authors": "Halley L. Brantley and Joseph Guinness and Eric C. Chi", "title": "Baseline Drift Estimation for Air Quality Data Using Quantile Trend\n  Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating smoothly varying baseline trends in time\nseries data. This problem arises in a wide range of fields, including\nchemistry, macroeconomics, and medicine; however, our study is motivated by the\nanalysis of data from low cost air quality sensors. Our methods extend the\nquantile trend filtering framework to enable the estimation of multiple\nquantile trends simultaneously while ensuring that the quantiles do not cross.\nTo handle the computational challenge posed by very long time series, we\npropose a parallelizable alternating direction method of moments (ADMM)\nalgorithm. The ADMM algorthim enables the estimation of trends in a piecewise\nmanner, both reducing the computation time and extending the limits of the\nmethod to larger data sizes. We also address smoothing parameter selection and\npropose a modified criterion based on the extended Bayesian Information\nCriterion. Through simulation studies and our motivating application to low\ncost air quality sensor data, we demonstrate that our model provides better\nquantile trend estimates than existing methods and improves signal\nclassification of low-cost air quality sensor output.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 00:23:00 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Brantley", "Halley L.", ""], ["Guinness", "Joseph", ""], ["Chi", "Eric C.", ""]]}, {"id": "1904.10639", "submitter": "Siyang Gao", "authors": "Fei Gao, Zhongshun Shi, Siyang Gao, Hui Xiao", "title": "Efficient Simulation Budget Allocation for Subset Selection Using\n  Regression Metamodels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research considers the ranking and selection (R&S) problem of selecting\nthe optimal subset from a finite set of alternative designs. Given the total\nsimulation budget constraint, we aim to maximize the probability of correctly\nselecting the top-m designs. In order to improve the selection efficiency, we\nincorporate the information from across the domain into regression metamodels.\nIn this research, we assume that the mean performance of each design is\napproximately quadratic. To achieve a better fit of this model, we divide the\nsolution space into adjacent partitions such that the quadratic assumption can\nbe satisfied within each partition. Using the large deviation theory, we\npropose an approximately optimal simulation budget allocation rule in the\npresence of partitioned domains. Numerical experiments demonstrate that our\napproach can enhance the simulation efficiency significantly.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 04:41:40 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Gao", "Fei", ""], ["Shi", "Zhongshun", ""], ["Gao", "Siyang", ""], ["Xiao", "Hui", ""]]}, {"id": "1904.10792", "submitter": "Wenlin Dai", "authors": "Zonghui Yao, Wenlin Dai, and Marc G. Genton", "title": "Trajectory Functional Boxplots", "comments": "15 pages, 9 figures, 1 movie", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of data-monitoring techniques in various fields of\nscience, multivariate functional data are often observed. Consequently, an\nincreasing number of methods have appeared to extend the general summary\nstatistics of multivariate functional data. However, trajectory functional\ndata, as an important sub-type, have not been studied very well. This article\nproposes two informative exploratory tools, the trajectory functional boxplot,\nand the modified simplicial band depth (MSBD) versus Wiggliness of Directional\nOutlyingness (WO) plot, to visualize the centrality of trajectory functional\ndata. The newly defined WO index effectively measures the shape variation of\ncurves and hence serves as a detector for shape outliers; additionally, MSBD\nprovides a center-outward ranking result and works as a detector for magnitude\noutliers. Using the two measures, the functional boxplot of the trajectory\nreveals center-outward patterns and potential outliers using the raw curves,\nwhereas the MSBD-WO plot illustrates such patterns and outliers in a space\nspanned by MSBD and WO. The proposed methods are validated on hurricane path\ndata and migration trace data recorded from two types of birds.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 13:18:40 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 09:44:41 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Yao", "Zonghui", ""], ["Dai", "Wenlin", ""], ["Genton", "Marc G.", ""]]}, {"id": "1904.10939", "submitter": "Anindya Bhadra", "authors": "Anindya Bhadra, Jyotishka Datta, Yunfan Li, Nicholas G. Polson", "title": "Horseshoe Regularization for Machine Learning in Complex and Deep Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the advent of the horseshoe priors for regularization, global-local\nshrinkage methods have proved to be a fertile ground for the development of\nBayesian methodology in machine learning, specifically for high-dimensional\nregression and classification problems. They have achieved remarkable success\nin computation, and enjoy strong theoretical support. Most of the existing\nliterature has focused on the linear Gaussian case; see Bhadra et al. (2019b)\nfor a systematic survey. The purpose of the current article is to demonstrate\nthat the horseshoe regularization is useful far more broadly, by reviewing both\nmethodological and computational developments in complex models that are more\nrelevant to machine learning applications. Specifically, we focus on\nmethodological challenges in horseshoe regularization in nonlinear and\nnon-Gaussian models; multivariate models; and deep neural networks. We also\noutline the recent computational developments in horseshoe shrinkage for\ncomplex models along with a list of available software implementations that\nallows one to venture out beyond the comfort zone of the canonical linear\nregression problems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 17:28:34 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 13:50:34 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Bhadra", "Anindya", ""], ["Datta", "Jyotishka", ""], ["Li", "Yunfan", ""], ["Polson", "Nicholas G.", ""]]}, {"id": "1904.11075", "submitter": "Jun Yang", "authors": "Yan Cui, Jun Yang, Zhou Zhou", "title": "State-domain Change Point Detection for Nonlinear Time Series Regression", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change point detection in time series has attracted substantial interest, but\nmost of the existing results have been focused on detecting change points in\nthe time domain. This paper considers the situation where nonlinear time series\nhave potential change points in the state domain. We apply a density-weighted\nanti-symmetric kernel function to the state domain and therefore propose a\nnonparametric procedure to test the existence of change points. When the\nexistence of change points is affirmative, we further introduce an algorithm to\nestimate their number together with locations and show the convergence result\non the estimation procedure. A real dataset of German daily confirmed cases of\nCOVID-19 is used to illustrate our results.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 21:31:12 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 00:49:41 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 19:52:07 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Cui", "Yan", ""], ["Yang", "Jun", ""], ["Zhou", "Zhou", ""]]}, {"id": "1904.11085", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Mauricio Sadinle", "title": "Nonparametric Pattern-Mixture Models for Inference with Missing Data", "comments": "65 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern-mixture models provide a transparent approach for handling missing\ndata, where the full-data distribution is factorized in a way that explicitly\nshows the parts that can be estimated from observed data alone, and the parts\nthat require identifying restrictions. We introduce a nonparametric estimator\nof the full-data distribution based on the pattern-mixture model factorization.\nOur approach uses the empirical observed-data distribution and augments it with\na nonparametric estimator of the missing-data distributions under a given\nidentifying restriction. Our results apply to a large class of donor-based\nidentifying restrictions that encompasses commonly used ones and can handle\nboth monotone and nonmonotone missingness. We propose a Monte Carlo procedure\nto derive point estimates of functionals of interest, and the bootstrap to\nconstruct confidence intervals.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 21:54:14 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Sadinle", "Mauricio", ""]]}, {"id": "1904.11087", "submitter": "Thomas Loughin", "authors": "Thomas M. Loughin, Yan Zhang", "title": "A Comparison of Methods for Identifying Location Effects in Unreplicated\n  Fractional Factorials in the Presence of Dispersion Effects", "comments": "The version of record of this manuscript has been published online\n  and is available in Journal of Quality Technology (2019),\n  https://www.tandfonline.com/doi/full/10.1080/00224065.2019.1569960", "journal-ref": null, "doi": "10.1080/00224065.2019.1569960", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most methods for identifying location effects in unreplicated fractional\nfactorial designs assume homoscedasticity of the response values. However,\ndispersion effects in the underlying process may create heteroscedasticity in\nthe response values. This heteroscedasticity may go undetected when\nidentification of location effects is pursued. Indeed, methods for identifying\ndispersion effects typically require first modeling location effects.\nTherefore, it is imperative to understand how methods for identifying location\neffects function in the presence of undetected dispersion effects. We used\nsimulation studies to examine the robustness of four different methods for\nidentifying location effects---Box and Meyer (1986), Lenth (1989), Berk and\nPicard (1991), and Loughin and Noble (1997)---under models with one, two, or\nthree dispersion effects of varying sizes. We found that the first three\nmethods usually performed acceptably with respect to error rates and power, but\nthe Loughin-Noble method lost control of the individual error rate when\nmoderate-to-large dispersion effects were present.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 22:20:19 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Loughin", "Thomas M.", ""], ["Zhang", "Yan", ""]]}, {"id": "1904.11101", "submitter": "Monika Bhattacharjee", "authors": "Monika Bhattacharjee, Moulinath Banerjee and George Michailidis", "title": "Change Point Estimation in Panel Data with Temporal and Cross-sectional\n  Dependence", "comments": "57 pages, 1 figure, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of detecting a common change point in large panel data\nbased on a mean shift model, wherein the errors exhibit both temporal and\ncross-sectional dependence. A least squares based procedure is used to estimate\nthe location of the change point. Further, we establish the convergence rate\nand obtain the asymptotic distribution of the least squares estimator. The form\nof the distribution is determined by the behavior of the norm difference of the\nmeans before and after the change point. Since the behavior of this norm\ndifference is, a priori, unknown to the practitioner, we also develop a novel\ndata driven adaptive procedure that provides valid confidence intervals for the\ncommon change point, without requiring any such knowledge. Numerical work based\non synthetic data illustrates the performance of the estimator in finite\nsamples under different settings of temporal and cross-sectional dependence,\nsample size and number of panels. Finally, we examine an application to\nfinancial stock data and discuss the identified change points.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 00:03:07 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Bhattacharjee", "Monika", ""], ["Banerjee", "Moulinath", ""], ["Michailidis", "George", ""]]}, {"id": "1904.11109", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa, Genya Kobayashi, Yuki Kawakubo", "title": "Estimation and inference for area-wise spatial income distributions from\n  grouped data", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating income distributions plays an important role in the measurement of\ninequality and poverty over space. The existing literature on income\ndistributions predominantly focuses on estimating an income distribution for a\ncountry or a region separately and the simultaneous estimation of multiple\nincome distributions has not been discussed in spite of its practical\nimportance. In this work, we develop an effective method for the simultaneous\nestimation and inference for area-wise spatial income distributions taking\naccount of geographical information from grouped data. Based on the multinomial\nlikelihood function for grouped data, we propose a spatial state-space model\nfor area-wise parameters of parametric income distributions. We provide an\nefficient Bayesian approach to estimation and inference for area-wise latent\nparameters, which enables us to compute area-wise summary measures of income\ndistributions such as mean incomes and Gini indices, not only for sampled areas\nbut also for areas without any samples thanks to the latent spatial state-space\nstructure. The proposed method is demonstrated using the Japanese\nmunicipality-wise grouped income data. The simulation studies show the\nsuperiority of the proposed method to a crude conventional approach which\nestimates the income distributions separately.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 00:59:05 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 04:26:21 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kobayashi", "Genya", ""], ["Kawakubo", "Yuki", ""]]}, {"id": "1904.11144", "submitter": "Jairo Fuquene", "authors": "Jairo F\\'uquene, Andryu Mendoza, Cesar Cristancho, Mariana Ospina", "title": "A Bayesian approach for small area population estimates using multiple\n  administrative records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small area population estimates are useful for decision making in the private\nand public sectors. However, in small areas (i.e., those that are difficult to\nreach and with small population sizes) computing demographic quantities is\ncomplicated. Bayesian methods are an alternative for demographic population\nestimates which uses data from multiple administrative records. In this paper\nwe explore a Bayesian approach which is simple and flexible and represents an\nalternative procedure for base population estimates particularly powerful for\nintercensal periods. The applicability of the methodological procedure is\nillustrated using population pyramids in the municipality of Jamund\\'i in\nColombia.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 03:42:05 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 01:12:16 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 22:57:03 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["F\u00faquene", "Jairo", ""], ["Mendoza", "Andryu", ""], ["Cristancho", "Cesar", ""], ["Ospina", "Mariana", ""]]}, {"id": "1904.11430", "submitter": "Raiden Hasegawa", "authors": "Raiden B. Hasegawa, Dylan S. Small, and Daniel W Webster", "title": "Bracketing in the Comparative Interrupted Time-Series Design to Address\n  Concerns about History Interacting with Group: Evaluating Missouri Handgun\n  Purchaser Law", "comments": null, "journal-ref": "Epidemiology, Volume 30, Issue 3, p.371-379, 2019", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the comparative interrupted time series design (also called the method of\ndifference-in-differences), the change in outcome in a group exposed to\ntreatment in the periods before and after the exposure is compared to the\nchange in outcome in a control group not exposed to treatment in either period.\nThe standard difference-in-difference estimator for a comparative interrupted\ntime series design will be biased for estimating the causal effect of the\ntreatment if there is an interaction between history in the after period and\nthe groups; for example, there is a historical event besides the start of the\ntreatment in the after period that benefits the treated group more than the\ncontrol group. We present a bracketing method for bounding the effect of an\ninteraction between history and the groups that arises from a time-invariant\nunmeasured confounder having a different effect in the after period than the\nbefore period. The method is applied to a study of the effect of the repeal of\nMissouri's permit-to-purchase handgun law on its firearm homicide rate. We\nestimate that the effect of the permit-to-purchase repeal on Missouri's firearm\nhomicide rate is bracketed between 0.9 and 1.3 homicides per 100,000 people,\ncorresponding to a percentage increase of 17% to 27% (95% confidence interval:\n[0.6,1.7] or [11%,35%]). A placebo study provides additional support for the\nhypothesis that the repeal has a causal effect of increasing the rate of\nstate-wide firearm homicides.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 16:09:02 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Hasegawa", "Raiden B.", ""], ["Small", "Dylan S.", ""], ["Webster", "Daniel W", ""]]}, {"id": "1904.11518", "submitter": "Philip White", "authors": "Philip A. White and Alan E. Gelfand", "title": "Multivariate Functional Data Modeling with Time-varying Clustering", "comments": null, "journal-ref": "TEST (2020+)", "doi": "10.1007/s11749-020-00733-z", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the situation where multivariate functional data has been\ncollected over time at each of a set of sites. Our illustrative setting is\nbivariate, monitoring ozone and PM$_{10}$ levels as a function of time over the\ncourse of a year at a set of monitoring sites. The data we work with is from 24\nmonitoring sites in Mexico City which record hourly ozone and PM$_{10}$ levels.\nWe use the data for the year 2017. Hence, we have 48 functions to work with.\nOur objective is to implement model-based clustering of the functions across\nthe sites. Using our example, such clustering can be considered for ozone and\nPM$_{10}$ individually or jointly. It may occur differentially for the two\npollutants. More importantly for us, we allow that such clustering can vary\nwith time.\n  We model the multivariate functions across sites using a multivariate\nGaussian process. With many sites and several functions at each site, we use\ndimension reduction to provide a stochastic process specification for the\ndistribution of the collection of multivariate functions over the say $n$\nsites. Furthermore, to cluster the functions, either individually by component\nor jointly with all components, we use the Dirichlet process which enables\nshared labeling of the functions across the sites. Specifically, we cluster\nfunctions based on their response to exogenous variables. Though the functions\narise in continuous time, clustering in continuous time is extremely\ncomputationally demanding and not of practical interest. Therefore, we employ a\npartitioning of the time scale to capture time-varying clustering.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 18:06:26 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 18:52:02 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["White", "Philip A.", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "1904.11594", "submitter": "Debamita Kundu", "authors": "Debamita Kundu, Riten Mitra, Jeremy T. Gaskins", "title": "Bayesian Variable Selection for Multi-Outcome Models Through Shared\n  Shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection over a potentially large set of covariates in a linear\nmodel is quite popular. In the Bayesian context, common prior choices can lead\nto a posterior expectation of the regression coefficients that is a sparse (or\nnearly sparse) vector with a few non-zero components, those covariates that are\nmost important. This article extends the global-local shrinkage idea to a\nscenario where one wishes to model multiple response variables simultaneously.\nHere, we have developed a variable selection method for a K-outcome model\n(multivariate regression) that identifies the most important covariates across\nall outcomes. The prior for all regression coefficients is a mean zero normal\nwith coefficient-specific variance term that consists of a predictor-specific\nfactor (shared local shrinkage parameter) and a model-specific factor (global\nshrinkage term) that differs in each model. The performance of our modeling\napproach is evaluated through simulation studies and a data example.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 21:19:28 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Kundu", "Debamita", ""], ["Mitra", "Riten", ""], ["Gaskins", "Jeremy T.", ""]]}, {"id": "1904.11603", "submitter": "Federico Ferrari", "authors": "Federico Ferrari and David B Dunson", "title": "Bayesian Factor Analysis for Inference on Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is motivated by the problem of inference on interactions among\nchemical exposures impacting human health outcomes. Chemicals often co-occur in\nthe environment or in synthetic mixtures and as a result exposure levels can be\nhighly correlated. We propose a latent factor joint model, which includes\nshared factors in both the predictor and response components while assuming\nconditional independence. By including a quadratic regression in the latent\nvariables in the response component, we induce flexible dimension reduction in\ncharacterizing main effects and interactions. We propose a Bayesian approach to\ninference under this Factor analysis for INteractions (FIN) framework. Through\nappropriate modifications of the factor modeling structure, FIN can accommodate\nhigher order interactions and multivariate outcomes. We provide theory on\nposterior consistency and the impact of misspecifying the number of factors. We\nevaluate the performance using a simulation study and data from the National\nHealth and Nutrition Examination Survey (NHANES). Code is available on GitHub.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 21:50:54 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 09:02:13 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Ferrari", "Federico", ""], ["Dunson", "David B", ""]]}, {"id": "1904.11648", "submitter": "Chong Ma", "authors": "Chong Ma, Wenxuan Deng, Shuangge Ma, Ray Liu, Kevin Galinsky", "title": "Structural modeling using overlapped group penalties for discovering\n  predictive biomarkers for subgroup analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of predictive biomarkers from a large scale of covariates\nfor subgroup analysis has attracted fundamental attention in medical research.\nIn this article, we propose a generalized penalized regression method with a\nnovel penalty function, for enforcing the hierarchy structure between the\nprognostic and predictive effects, such that a nonzero predictive effect must\ninduce its ancestor prognostic effects being nonzero in the model. Our method\nis able to select useful predictive biomarkers by yielding a sparse,\ninterpretable, and predictable model for subgroup analysis, and can deal with\ndifferent types of response variable such as continuous, categorical, and\ntime-to-event data. We show that our method is asymptotically consistent under\nsome regularized conditions. To minimize the generalized penalized regression\nmodel, we propose a novel integrative optimization algorithm by integrating the\nmajorization-minimization and the alternating direction method of multipliers,\nwhich is named after \\texttt{smog}. The enriched simulation study and real case\nstudy demonstrate that our method is very powerful for discovering the true\npredictive biomarkers and identifying subgroups of patients.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 02:06:26 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Ma", "Chong", ""], ["Deng", "Wenxuan", ""], ["Ma", "Shuangge", ""], ["Liu", "Ray", ""], ["Galinsky", "Kevin", ""]]}, {"id": "1904.11718", "submitter": "Pietro Vischia", "authors": "Pietro Vischia", "title": "Reporting Results in High Energy Physics Publications: a Manifesto", "comments": "26 pages, 3 tables, 7 figures. Accepted by Reviews in Physics on July\n  3rd, 2020. Preproof at:\n  https://www.sciencedirect.com/science/article/pii/S2405428320300095", "journal-ref": null, "doi": "10.1016/j.revip.2020.100046", "report-no": null, "categories": "hep-ph hep-ex stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of collider data analyses has dramatically increased from\nearly colliders to the CERN LHC. Reconstruction of the collision products in\nthe particle detectors has reached a point that requires dedicated publications\ndocumenting the techniques, and periodic retuning of the algorithms themselves.\nAnalysis methods evolved to account for the increased complexity of the\ncombination of particles required in each collision event (final states) and\nfor the need of squeezing every last bit of sensitivity from the data;\nphysicists often seek to fully reconstruct the final state, a process that is\nmostly relatively easy at lepton colliders but sometimes exceedingly difficult\nat hadron colliders to the point of requiring sometimes using advanced\nstatistical techniques such as machine learning.\n  The need for keeping the publications documenting results to a reasonable\nsize implies a greater level of compression or even omission of information\nwith respect to publications from twenty years ago. The need for compression\nshould however not prevent sharing a reasonable amount of information that is\nessential to understanding a given analysis. Infrastructures like Rivet or\nHepData have been developed to host additional material, but physicists in the\nexperimental Collaborations often still send an insufficient amount of material\nto these databases.\n  In this manuscript I advocate for an increase in the information shared by\nthe Collaborations, and try to define a minimum standard for acceptable level\nof information when reporting the results of statistical procedures in High\nEnergy Physics publications.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 08:44:38 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 07:22:37 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Vischia", "Pietro", ""]]}, {"id": "1904.11745", "submitter": "Toby Kenney", "authors": "Toby Kenney, Tianshu Huang and Hong Gu", "title": "Poisson PCA: Poisson Measurement Error corrected PCA, with Application\n  to Microbiome Data", "comments": "32 pages, 11 figures", "journal-ref": null, "doi": "10.1111/biom.13384", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of computing a Principal Component\nAnalysis of data affected by Poisson noise. We assume samples are drawn from\nindependent Poisson distributions. We want to estimate principle components of\na fixed transformation of the latent Poisson means. Our motivating example is\nmicrobiome data, though the methods apply to many other situations. We develop\na semiparametric approach to correct the bias of variance estimators, both for\nuntransformed and transformed (with particular attention to log-transformation)\nPoisson means. Furthermore, we incorporate methods for correcting different\nexposure or sequencing depth in the data. In addition to identifying the\nprincipal components, we also address the non-trivial problem of computing the\nprincipal scores in this semiparametric framework. Most previous approaches\ntend to take a more parametric line. For example the Poisson-log-normal (PLN)\nmodel, approach. We compare our method with the PLN approach and find that our\nmethod is better at identifying the main principal components of the latent\nlog-transformed Poisson means, and as a further major advantage, takes far less\ntime to compute. Comparing methods on real data, we see that our method also\nappears to be more robust to outliers than the parametric method.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 10:01:57 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kenney", "Toby", ""], ["Huang", "Tianshu", ""], ["Gu", "Hong", ""]]}, {"id": "1904.11758", "submitter": "Nicolo' Margaritella Dr", "authors": "Nicolo' Margaritella, Vanda Inacio, Ruth King", "title": "Parameter clustering in Bayesian functional PCA of fMRI data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraordinary advancements in neuroscientific technology for brain\nrecordings over the last decades have led to increasingly complex\nspatio-temporal datasets. To reduce oversimplifications, new models have been\ndeveloped to be able to identify meaningful patterns and new insights within a\nhighly demanding data environment. To this extent, we propose a new model\ncalled parameter clustering functional Principal Component Analysis (PCl-fPCA)\nthat merges ideas from Functional Data Analysis and Bayesian nonparametrics to\nobtain a flexible and computationally feasible signal reconstruction and\nexploration of spatio-temporal neuroscientific data. In particular, we use a\nDirichlet process Gaussian mixture model to cluster functional principal\ncomponent scores within the standard Bayesian functional PCA framework. This\napproach captures the spatial dependence structure among smoothed time series\n(curves) and its interaction with the time domain without imposing a prior\nspatial structure on the data. Moreover, by moving the mixture from data to\nfunctional principal component scores, we obtain a more general clustering\nprocedure, thus allowing a higher level of intricate insight and understanding\nof the data. We present results from a simulation study showing improvements in\ncurve and correlation reconstruction compared with different Bayesian and\nfrequentist fPCA models and we apply our method to functional Magnetic\nResonance Imaging and Electroencephalogram data analyses providing a rich\nexploration of the spatio-temporal dependence in brain time series.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 10:54:53 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 17:13:04 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 14:26:03 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Margaritella", "Nicolo'", ""], ["Inacio", "Vanda", ""], ["King", "Ruth", ""]]}, {"id": "1904.11912", "submitter": "James M. Flegal", "authors": "Nathan Robertson, James M. Flegal, Dootika Vats, and Galin L. Jones", "title": "Assessing and Visualizing Simultaneous Simulation Error", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo experiments produce samples in order to estimate features of a\ngiven distribution. However, simultaneous estimation of means and quantiles has\nreceived little attention, despite being common practice. In this setting we\nestablish a multivariate central limit theorem for any finite combination of\nsample means and quantiles under the assumption of a strongly mixing process,\nwhich includes the standard Monte Carlo and Markov chain Monte Carlo settings.\nWe build on this to provide a fast algorithm for constructing hyperrectangular\nconfidence regions having the desired simultaneous coverage probability and a\nconvenient marginal interpretation. The methods are incorporated into standard\nways of visualizing the results of Monte Carlo experiments enabling the\npractitioner to more easily assess the reliability of the results. We\ndemonstrate the utility of this approach in various Monte Carlo settings\nincluding simulation studies based on independent and identically distributed\nsamples and Bayesian analyses using Markov chain Monte Carlo sampling.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 16:00:35 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 19:30:23 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Robertson", "Nathan", ""], ["Flegal", "James M.", ""], ["Vats", "Dootika", ""], ["Jones", "Galin L.", ""]]}, {"id": "1904.12057", "submitter": "Geoffrey McLachlan", "authors": "Geoffrey J. McLachlan, Sharon X. Lee", "title": "Comment on \"Hidden truncation hyperbolic distributions, finite mixtures\n  thereof and their application for clustering\" Murray, Browne, and \\McNicholas", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We comment on the paper of Murray, Browne, and McNicholas (2017), who\nproposed mixtures of skew distributions, which they termed hidden truncation\nhyperbolic (HTH). They recently made a clarification (Murray, Browne,\nMcNicholas, 2019) concerning their claim that the so-called CFUST distribution\nis a special case of the HTH distribution. There are also some other matters in\nthe original version of the paper that were in need of clarification as\ndiscussed here.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 21:52:17 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["McLachlan", "Geoffrey J.", ""], ["Lee", "Sharon X.", ""]]}, {"id": "1904.12064", "submitter": "Adam Sykulski Dr", "authors": "Jeffrey J. Early and Adam M. Sykulski", "title": "Smoothing and Interpolating Noisy GPS Data with Smoothing Splines", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": "10.1175/JTECH-D-19-0087.1", "report-no": null, "categories": "stat.ME physics.data-an stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive methodology is provided for smoothing noisy, irregularly\nsampled data with non-Gaussian noise using smoothing splines. We demonstrate\nhow the spline order and tension parameter can be chosen a priori from physical\nreasoning. We also show how to allow for non-Gaussian noise and outliers which\nare typical in GPS signals. We demonstrate the effectiveness of our methods on\nGPS trajectory data obtained from oceanographic floating instruments known as\ndrifters.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 22:28:58 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 23:14:02 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Early", "Jeffrey J.", ""], ["Sykulski", "Adam M.", ""]]}, {"id": "1904.12070", "submitter": "Fangzheng Xie", "authors": "Fangzheng Xie, Yanxun Xu", "title": "Optimal Bayesian Estimation for Random Dot Product Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian approach, called the posterior spectral embedding, for\nestimating the latent positions in random dot product graphs, and prove its\noptimality. Unlike the classical spectral-based adjacency/Laplacian spectral\nembedding, the posterior spectral embedding is a fully-likelihood based graph\nestimation method taking advantage of the Bernoulli likelihood information of\nthe observed adjacency matrix. We develop a minimax-lower bound for estimating\nthe latent positions, and show that the posterior spectral embedding achieves\nthis lower bound since it both results in a minimax-optimal posterior\ncontraction rate, and yields a point estimator achieving the minimax risk\nasymptotically. The convergence results are subsequently applied to clustering\nin stochastic block models, the result of which strengthens an existing result\nconcerning the number of mis-clustered vertices. We also study a spectral-based\nGaussian spectral embedding as a natural Bayesian analogy of the adjacency\nspectral embedding, but the resulting posterior contraction rate is sub-optimal\nwith an extra logarithmic factor. The practical performance of the proposed\nmethodology is illustrated through extensive synthetic examples and the\nanalysis of a Wikipedia graph data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 23:10:12 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Xie", "Fangzheng", ""], ["Xu", "Yanxun", ""]]}, {"id": "1904.12103", "submitter": "Arkaprava Roy", "authors": "Arkaprava Roy, Jana Schaich-Borg, David B Dunson", "title": "Bayesian time-aligned factor analysis of paired multivariate time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern data sets require inference methods that can estimate the shared\nand individual-specific components of variability in collections of matrices\nthat change over time. Promising methods have been developed to analyze these\ntypes of data in static cases, but very few approaches are available for\ndynamic settings. To address this gap, we consider novel models and inference\nmethods for pairs of matrices in which the columns correspond to multivariate\nobservations at different time points. In order to characterize common and\nindividual features, we propose a Bayesian dynamic factor modeling framework\ncalled Time Aligned Common and Individual Factor Analysis (TACIFA) that\nincludes uncertainty in time alignment through an unknown warping function. We\nprovide theoretical support for the proposed model, showing identifiability and\nposterior concentration. The structure enables efficient computation through a\nHamiltonian Monte Carlo (HMC) algorithm. We show excellent performance in\nsimulations, and illustrate the method through application to a social\nsynchrony experiment.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 03:55:14 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Roy", "Arkaprava", ""], ["Schaich-Borg", "Jana", ""], ["Dunson", "David B", ""]]}, {"id": "1904.12113", "submitter": "Ingo Hoffmann", "authors": "Ingo Hoffmann and Christoph J. B\\\"orner", "title": "Tail models and the statistical limit of accuracy in risk assessment", "comments": "3 figures", "journal-ref": "Journal of Risk Finance 2020", "doi": "10.1108/JRF-11-2019-0217", "report-no": null, "categories": "q-fin.RM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In risk management, tail risks are of crucial importance. The assessment of\nrisks should be carried out in accordance with the regulatory authority's\nrequirement at high quantiles. In general, the underlying distribution function\nis unknown, the database is sparse, and therefore special tail models are used.\nVery often, the generalized Pareto distribution is employed as a basic model,\nand its parameters are determined with data from the tail area. With the\ndetermined tail model, statisticians then calculate the required high\nquantiles. In this context, we consider the possible accuracy of the\ncalculation of the quantiles and determine the finite sample distribution\nfunction of the quantile estimator, depending on the confidence level and the\nparameters of the tail model, and then calculate the finite sample bias and the\nfinite sample variance of the quantile estimator. Finally, we present an impact\nanalysis on the quantiles of an unknown distribution function.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 05:58:45 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Hoffmann", "Ingo", ""], ["B\u00f6rner", "Christoph J.", ""]]}, {"id": "1904.12159", "submitter": "Pier Luigi Conti", "authors": "Pier Luigi Conti and Livia De Giovanni", "title": "Estimation of distributional effects of treatment and control under\n  selection on observables: consistency, weak convergence, and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the estimation of the distribution function for potential\noutcomes to receiving or not receiving a treatment is studied. The approach is\nbased on weighting observed data on the basis on estimated propensity score. A\nweighted version of empirical process is constructed and its weak convergence\nto bivariate Gaussian process is established. Results for the estimation of the\nAverage Treatment Effect (ATE) and Quantile Treatment Effect (QTE) are obtained\nas by-products. Applications to the construction of nonparametric tests for the\ntreatment effect and for the stochastic dominance of the treatment over control\nare considered, and their finite sample properties and merits are studied via\nsimulation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 13:40:10 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Conti", "Pier Luigi", ""], ["De Giovanni", "Livia", ""]]}, {"id": "1904.12204", "submitter": "Ionut-Gabriel Farcas", "authors": "Ionut-Gabriel Farcas, Jonas Latz, Elisabeth Ullmann, Tobias Neckel and\n  Hans-Joachim Bungartz", "title": "Multilevel adaptive sparse Leja approximations for Bayesian inverse\n  problems", "comments": "24 pages, 9 figures", "journal-ref": "SIAM J. Sci. Comput. 42(1), pp. A424-A451, 2020", "doi": "10.1137/19M1260293", "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deterministic interpolation and quadrature methods are often unsuitable to\naddress Bayesian inverse problems depending on computationally expensive\nforward mathematical models. While interpolation may give precise posterior\napproximations, deterministic quadrature is usually unable to efficiently\ninvestigate an informative and thus concentrated likelihood. This leads to a\nlarge number of required expensive evaluations of the mathematical model. To\novercome these challenges, we formulate and test a multilevel adaptive sparse\nLeja algorithm. At each level, adaptive sparse grid interpolation and\nquadrature are used to approximate the posterior and perform all quadrature\noperations, respectively. Specifically, our algorithm uses coarse\ndiscretizations of the underlying mathematical model to investigate the\nparameter space and to identify areas of high posterior probability. Adaptive\nsparse grid algorithms are then used to place points in these areas, and ignore\nother areas of small posterior probability. The points are weighted Leja\npoints. As the model discretization is coarse, the construction of the sparse\ngrid is computationally efficient. On this sparse grid, the posterior measure\ncan be approximated accurately with few expensive, fine model discretizations.\nThe efficiency of the algorithm can be enhanced further by exploiting more than\ntwo discretization levels. We apply the proposed multilevel adaptive sparse\nLeja algorithm in numerical experiments involving elliptic inverse problems in\n2D and 3D space, in which we compare it with Markov chain Monte Carlo sampling\nand a standard multilevel approximation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 20:23:50 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 14:04:54 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Farcas", "Ionut-Gabriel", ""], ["Latz", "Jonas", ""], ["Ullmann", "Elisabeth", ""], ["Neckel", "Tobias", ""], ["Bungartz", "Hans-Joachim", ""]]}, {"id": "1904.12321", "submitter": "Ted Westling", "authors": "Ted Westling, Kevin J. Downes, Dylan S. Small", "title": "Nonparametric maximum likelihood estimation under a likelihood ratio\n  order", "comments": "Revised paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparison of two univariate distributions based on independent samples from\nthem is a fundamental problem in statistics, with applications in a wide\nvariety of scientific disciplines. In many situations, we might hypothesize\nthat the two distributions are stochastically ordered, meaning intuitively that\nsamples from one distribution tend to be larger than those from the other. One\ntype of stochastic order that arises in economics, biomedicine, and elsewhere\nis the likelihood ratio order, also known as the density ratio order, in which\nthe ratio of the density functions of the two distributions is monotone\nnon-decreasing. In this article, we derive and study the nonparametric maximum\nlikelihood estimator of the individual distributions and the ratio of their\ndensities under the likelihood ratio order. Our work applies to discrete\ndistributions, continuous distributions, and mixed continuous-discrete\ndistributions. We demonstrate convergence in distribution of the estimator in\ncertain cases, and we illustrate our results using numerical experiments and an\nanalysis of a biomarker for predicting bacterial infection in children with\nsystemic inflammatory response syndrome.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 13:34:31 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 12:41:40 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 18:46:02 GMT"}, {"version": "v4", "created": "Wed, 7 Jul 2021 14:42:44 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Westling", "Ted", ""], ["Downes", "Kevin J.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1904.12369", "submitter": "Elynn Chen", "authors": "Krishna Balasubramanian, Elynn Y. Chen, Jianqing Fan, Xiang Wu", "title": "Low-Rank Principal Eigenmatrix Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse PCA is a widely used technique for high-dimensional data analysis. In\nthis paper, we propose a new method called low-rank principal eigenmatrix\nanalysis. Different from sparse PCA, the dominant eigenvectors are allowed to\nbe dense but are assumed to have a low-rank structure when matricized\nappropriately. Such a structure arises naturally in several practical cases:\nIndeed the top eigenvector of a circulant matrix, when matricized appropriately\nis a rank-1 matrix. We propose a matricized rank-truncated power method that\ncould be efficiently implemented and establish its computational and\nstatistical properties. Extensive experiments on several synthetic data sets\ndemonstrate the competitive empirical performance of our method.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 18:51:39 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Balasubramanian", "Krishna", ""], ["Chen", "Elynn Y.", ""], ["Fan", "Jianqing", ""], ["Wu", "Xiang", ""]]}, {"id": "1904.12417", "submitter": "Ander Wilson", "authors": "Ander Wilson, Hsiao-Hsien Leon Hsu, Yueh-Hsiu Mathilda Chiu, Robert O.\n  Wright, Rosalind J. Wright, Brent A. Coull", "title": "Kernel Machine and Distributed Lag Models for Assessing Windows of\n  Susceptibility to Environmental Mixtures in Children's Health Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposures to environmental chemicals during gestation can alter health status\nlater in life. Most studies of maternal exposure to chemicals during pregnancy\nfocus on a single chemical exposure observed at high temporal resolution.\nRecent research has turned to focus on exposure to mixtures of multiple\nchemicals, generally observed at a single time point. We consider statistical\nmethods for analyzing data on chemical mixtures that are observed at a high\ntemporal resolution. As motivation, we analyze the association between exposure\nto four ambient air pollutants observed weekly throughout gestation and birth\nweight in a Boston-area prospective birth cohort. To explore patterns in the\ndata, we first apply methods for analyzing data on (1) a single chemical\nobserved at high temporal resolution, and (2) a mixture measured at a single\npoint in time. We highlight the shortcomings of these approaches for\ntemporally-resolved data on exposure to chemical mixtures. Second, we propose a\nnovel method, a Bayesian kernel machine regression distributed lag model\n(BKMR-DLM), that simultaneously accounts for nonlinear associations and\ninteractions among time-varying measures of exposure to mixtures. BKMR-DLM uses\na functional weight for each exposure that parameterizes the window of\nsusceptibility corresponding to that exposure within a kernel machine framework\nthat captures non-linear and interaction effects of the multivariate exposure\non the outcome. In a simulation study, we show that the proposed method can\nbetter estimate the exposure-response function and, in high signal settings,\ncan identify critical windows in time during which exposure has an increased\nassociation with the outcome. Applying the proposed method to the Boston birth\ncohort data, we found evidence of a negative association between organic carbon\nand birth weight and that nitrate modifies the organic carbon, elemental\ncarbon...\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 01:33:08 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 14:21:04 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 22:34:26 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2020 04:48:19 GMT"}, {"version": "v5", "created": "Wed, 30 Jun 2021 03:03:00 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wilson", "Ander", ""], ["Hsu", "Hsiao-Hsien Leon", ""], ["Chiu", "Yueh-Hsiu Mathilda", ""], ["Wright", "Robert O.", ""], ["Wright", "Rosalind J.", ""], ["Coull", "Brent A.", ""]]}, {"id": "1904.12459", "submitter": "Sudip Roy", "authors": "Sudip Roy, Ram C. Tripathi, N. Balakrishnan", "title": "A Closed Form Approximation of Moments of New Generalization of Negative\n  Binomial Distribution", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a closed form approximation to the mean and\nvariance of a new generalization of negative binomial (NGNB) distribution\narising from the Extended COM-Poisson (ECOMP) distribution developed by\nChakraborty and Imoto (2016)(see [4]). The NGNB is a special case of the ECOMP\ndistribution and was named so by these authors. This distribution is more\nflexible in terms of the dispersion index as compared to its ordinary\ncounterparts. It approaches the COM-Poisson distribution (Shmueli et al. 2005)\n[11] under suitable limiting conditions. The NGNB can also be obtained from the\nCOM-Negative Hypergeometric distribution (Roy et al. 2019)[10] as a limiting\ndistribution. In this paper, we present closed-form approximations for the mean\nand variance of the NGNB distribution. These approximations can be viewed as\nthe mean and variance of convolution of independent and identically distributed\nnegative binomial populations. The proposed closed-form approximations of the\nmean and variance will be helpful in building the link function for the\ngeneralized negative binomial regression model based on the NGNB distribution\nand other extended applications, hence resulting in enhanced applicability of\nthis model.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 06:03:27 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Roy", "Sudip", ""], ["Tripathi", "Ram C.", ""], ["Balakrishnan", "N.", ""]]}, {"id": "1904.12513", "submitter": "Monica Musio", "authors": "Silvia Columbu, Valentina Mameli, Monica Musio and A.Philip Dawid", "title": "The Hyv\\\"arinen scoring rule in Gaussian linear time series models", "comments": "23 pages, 1 figure, 3 tables. arXiv admin note: text overlap with\n  arXiv:1409.3690", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-based estimation methods involve the normalising constant of the\nmodel distributions, expressed as a function of the parameter. However in many\nproblems this function is not easily available, and then less efficient but\nmore easily computed estimators may be attractive. In this work we study\nstationary time-series models, and construct and analyse \"score-matching''\nestimators, that do not involve the normalising constant. We consider two\nscenarios: a single series of increasing length, and an increasing number of\nindependent series of fixed length. In the latter case there are two variants,\none based on the full data, and another based on a sufficient statistic. We\nstudy the empirical performance of these estimators in three special cases,\nautoregressive (\\AR), moving average (MA) and fractionally differenced white\nnoise (\\ARFIMA) models, and make comparisons with full and pairwise likelihood\nestimators. The results are somewhat model-dependent, with the new estimators\ndoing well for $\\MA$ and \\ARFIMA\\ models, but less so for $\\AR$ models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 09:06:00 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Columbu", "Silvia", ""], ["Mameli", "Valentina", ""], ["Musio", "Monica", ""], ["Dawid", "A. Philip", ""]]}, {"id": "1904.12608", "submitter": "J\\'an Dolinsk\\'y", "authors": "J\\'an Dolinsk\\'y, M\\'aria Starovsk\\'a, Robert T\\'oth", "title": "Automatic Model Building in GEFCom 2017 Qualifying Match", "comments": "10 pages, 3 figures, competition report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tangent Works team participated in GEFCom 2017 to test its automatic\nmodel building strategy for time series known as Tangent Information Modeller\n(TIM). Model building using TIM combined with historical temperature shuffling\nresulted in winning the competition. This strategy involved one remaining\ndegree of freedom, a decision on using a trend variable. This paper describes\nour modelling efforts in the competition, and furthermore outlines a fully\nautomated scenario where the decision on using the trend variable is handled by\nTIM. The results show that such a setup would also win the competition.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 15:02:15 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Dolinsk\u00fd", "J\u00e1n", ""], ["Starovsk\u00e1", "M\u00e1ria", ""], ["T\u00f3th", "Robert", ""]]}, {"id": "1904.12652", "submitter": "Azam Yazdani", "authors": "Azam Yazdani, Akram Yazdani, Sarah H. Elsea, Daniel J. Schaid, Michael\n  R. Kosorok, Gita Dangol, Ahmad Samiei", "title": "Genome analysis and pleiotropy assessment using causal networks with\n  loss of function mutation and metabolomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Many genome-wide association studies have detected genomic\nregions associated with traits, yet understanding the functional causes of\nassociation often remains elusive. Utilizing systems approaches and focusing on\nintermediate molecular phenotypes might facilitate biologic understanding.\nResults: The availability of exome sequencing of two populations of\nAfrican-Americans and European-Americans from the Atherosclerosis Risk in\nCommunities study allowed us to investigate the effects of annotated\nloss-of-function (LoF) mutations on 122 serum metabolites. To assess the\nfindings, we built metabolomic causal networks for each population separately\nand utilized structural equation modeling. We then validated our findings with\na set of independent samples. By use of methods based on concepts of Mendelian\nrandomization of genetic variants, we showed that some of the affected\nmetabolites are risk predictors in the causal pathway of disease. For example,\nLoF mutations in the gene KIAA1755 were identified to elevate the levels of\neicosapentaenoate (p-value=5E-14), an essential fatty acid clinically\nidentified to increase essential hypertension. We showed that this gene is in\nthe pathway to triglycerides, where both triglycerides and essential\nhypertension are risk factors of metabolomic disorder and heart attack. We also\nidentified that the gene CLDN17, harboring loss-of-function mutations, had\npleiotropic actions on metabolites from amino acid and lipid pathways.\nConclusion: Using systems biology approaches for the analysis of metabolomics\nand genetic data, we integrated several biological processes, which lead to\nfindings that may functionally connect genetic variants with complex diseases.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 12:45:06 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yazdani", "Azam", ""], ["Yazdani", "Akram", ""], ["Elsea", "Sarah H.", ""], ["Schaid", "Daniel J.", ""], ["Kosorok", "Michael R.", ""], ["Dangol", "Gita", ""], ["Samiei", "Ahmad", ""]]}, {"id": "1904.12765", "submitter": "Daniel J. Schad", "authors": "Daniel J. Schad, Michael Betancourt, Shravan Vasishth", "title": "Toward a principled Bayesian workflow in cognitive science", "comments": "75 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Experiments in research on memory, language, and in other areas of cognitive\nscience are increasingly being analyzed using Bayesian methods. This has been\nfacilitated by the development of probabilistic programming languages such as\nStan, and easily accessible front-end packages such as brms. The utility of\nBayesian methods, however, ultimately depends on the relevance of the Bayesian\nmodel, in particular whether or not it accurately captures the structure of the\ndata and the data analyst's domain expertise. Even with powerful software, the\nanalyst is responsible for verifying the utility of their model. To demonstrate\nthis point, we introduce a principled Bayesian workflow (Betancourt, 2018) to\ncognitive science. Using a concrete working example, we describe basic\nquestions one should ask about the model: prior predictive checks,\ncomputational faithfulness, model sensitivity, and posterior predictive checks.\nThe running example for demonstrating the workflow is data on reading times\nwith a linguistic manipulation of object versus subject relative clause\nsentences. This principled Bayesian workflow also demonstrates how to use\ndomain knowledge to inform prior distributions. It provides guidelines and\nchecks for valid data analysis, avoiding overfitting complex models to noise,\nand capturing relevant data structure in a probabilistic model. Given the\nincreasing use of Bayesian methods, we aim to discuss how these methods can be\nproperly employed to obtain robust answers to scientific questions. All data\nand code accompanying this paper are available from https://osf.io/b2vx9/.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 15:16:04 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 09:25:22 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 17:41:26 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Schad", "Daniel J.", ""], ["Betancourt", "Michael", ""], ["Vasishth", "Shravan", ""]]}, {"id": "1904.12891", "submitter": "Zijian Guo", "authors": "Tianxi Cai, Tony Cai, Zijian Guo", "title": "Optimal Statistical Inference for Individualized Treatment Effects in\n  High-dimensional Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict individualized treatment effects (ITEs) based on a\ngiven patient's profile is essential for personalized medicine. We propose a\nhypothesis testing approach to choosing between two potential treatments for a\ngiven individual in the framework of high-dimensional linear models. The\nmethodological novelty lies in the construction of a debiased estimator of the\nITE and establishment of its asymptotic normality uniformly for an arbitrary\nfuture high-dimensional observation, while the existing methods can only handle\ncertain specific forms of observations. We introduce a testing procedure with\nthe type-I error controlled and establish its asymptotic power. The proposed\nmethod can be extended to making inference for general linear contrasts,\nincluding both the average treatment effect and outcome prediction. We\nintroduce the optimality framework for hypothesis testing from both the\nminimaxity and adaptivity perspectives and establish the optimality of the\nproposed procedure. An extension to high-dimensional approximate linear models\nis also considered. The finite sample performance of the procedure is\ndemonstrated in simulation studies and further illustrated through an analysis\nof electronic health records data from patients with rheumatoid arthritis.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 18:20:15 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 22:49:25 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Cai", "Tianxi", ""], ["Cai", "Tony", ""], ["Guo", "Zijian", ""]]}, {"id": "1904.12918", "submitter": "Drew Dimmery", "authors": "Drew Dimmery, Eytan Bakshy and Jasjeet Sekhon", "title": "Shrinkage Estimators in Online Experiments", "comments": null, "journal-ref": null, "doi": "10.1145/3292500.3330771", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze empirical Bayes Stein-type estimators for use in the\nestimation of causal effects in large-scale online experiments. While online\nexperiments are generally thought to be distinguished by their large sample\nsize, we focus on the multiplicity of treatment groups. The typical analysis\npractice is to use simple differences-in-means (perhaps with covariate\nadjustment) as if all treatment arms were independent. In this work we develop\nconsistent, small bias, shrinkage estimators for this setting. In addition to\nachieving lower mean squared error these estimators retain important\nfrequentist properties such as coverage under most reasonable scenarios. Modern\nsequential methods of experimentation and optimization such as multi-armed\nbandit optimization (where treatment allocations adapt over time to prior\nresponses) benefit from the use of our shrinkage estimators. Exploration under\nempirical Bayes focuses more efficiently on near-optimal arms, improving the\nresulting decisions made under uncertainty. We demonstrate these properties by\nexamining seventeen large-scale experiments conducted on Facebook from April to\nJune 2017.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 19:28:57 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Dimmery", "Drew", ""], ["Bakshy", "Eytan", ""], ["Sekhon", "Jasjeet", ""]]}, {"id": "1904.12981", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou, Wentian Guo and Yuan Ji", "title": "PoD-TPI: Probability-of-Decision Toxicity Probability Interval Design to\n  Accelerate Phase I Trials", "comments": null, "journal-ref": null, "doi": "10.1007/s12561-019-09264-0", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cohort-based enrollment can slow down dose-finding trials since the outcomes\nof the previous cohort must be fully evaluated before the next cohort can be\nenrolled. This results in frequent suspension of patient enrollment. The issue\nis exacerbated in recent immune-oncology trials where toxicity outcomes can\ntake a long time to observe. We propose a novel phase I design, the\nprobability-of-decision toxicity probability interval (PoD-TPI) design, to\naccelerate phase I trials. PoD-TPI enables dose assignment in real-time in the\npresence of pending toxicity outcomes. With uncertain outcomes, the dose\nassignment decisions are treated as a random variable, and we calculate the\nposterior distribution of the decisions. The posterior distribution reflects\nthe variability in the pending outcomes and allows a direct and intuitive\nevaluation of the confidence of all possible decisions. Optimal decisions are\ncalculated based on 0-1 loss, and extra safety rules are constructed to enforce\nsufficient protection from exposing patients to risky doses. A new and useful\nfeature of PoD-TPI is that it allows investigators and regulators to balance\nthe trade-off between enrollment speed and making risky decisions by tuning a\npair of intuitive design parameters. Through numerical studies, we evaluate the\noperating characteristics of PoD-TPI and demonstrate that PoD-TPI shortens\ntrial duration and maintains trial safety and efficiency compared to existing\ntime-to-event designs.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 22:49:46 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 19:13:07 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Zhou", "Tianjian", ""], ["Guo", "Wentian", ""], ["Ji", "Yuan", ""]]}, {"id": "1904.13236", "submitter": "Soheil Esmaeilzadeh", "authors": "Soheil Esmaeilzadeh, Amir Salehi, Gill Hetz, Feyisayo Olalotiti-lawal,\n  Hamed Darabi, David Castineira", "title": "A General Spatio-Temporal Clustering-Based Non-local Formulation for\n  Multiscale Modeling of Compartmentalized Reservoirs", "comments": null, "journal-ref": "Machine Learning Session, WRM 2019 Conference", "doi": "10.2118/195329-MS", "report-no": "SPE-195329-MS", "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing the reservoir as a network of discrete compartments with\nneighbor and non-neighbor connections is a fast, yet accurate method for\nanalyzing oil and gas reservoirs. Automatic and rapid detection of coarse-scale\ncompartments with distinct static and dynamic properties is an integral part of\nsuch high-level reservoir analysis. In this work, we present a hybrid framework\nspecific to reservoir analysis for an automatic detection of clusters in space\nusing spatial and temporal field data, coupled with a physics-based multiscale\nmodeling approach. In this work a novel hybrid approach is presented in which\nwe couple a physics-based non-local modeling framework with data-driven\nclustering techniques to provide a fast and accurate multiscale modeling of\ncompartmentalized reservoirs. This research also adds to the literature by\npresenting a comprehensive work on spatio-temporal clustering for reservoir\nstudies applications that well considers the clustering complexities, the\nintrinsic sparse and noisy nature of the data, and the interpretability of the\noutcome.\n  Keywords: Artificial Intelligence; Machine Learning; Spatio-Temporal\nClustering; Physics-Based Data-Driven Formulation; Multiscale Modeling\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 23:01:05 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Esmaeilzadeh", "Soheil", ""], ["Salehi", "Amir", ""], ["Hetz", "Gill", ""], ["Olalotiti-lawal", "Feyisayo", ""], ["Darabi", "Hamed", ""], ["Castineira", "David", ""]]}]