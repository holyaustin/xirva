[{"id": "2003.00043", "submitter": "Irene Epifanio", "authors": "Ismael Cabero, Irene Epifanio", "title": "Finding archetypal patterns for binary questionnaires", "comments": null, "journal-ref": "SORT, 44(1): 39-66. 2020", "doi": "10.2436/20.8080.02.94", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Archetypal analysis is an exploratory tool that explains a set of\nobservations as mixtures of pure (extreme) patterns. If the patterns are actual\nobservations of the sample, we refer to them as archetypoids. For the first\ntime, we propose to use archetypoid analysis for binary observations. This tool\ncan contribute to the understanding of a binary data set, as in the\nmultivariate case. We illustrate the advantages of the proposed methodology in\na simulation study and two applications, one exploring objects (rows) and the\nother exploring items (columns). One is related to determining student skill\nset profiles and the other to describing item response functions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 20:01:24 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Cabero", "Ismael", ""], ["Epifanio", "Irene", ""]]}, {"id": "2003.00078", "submitter": "David Tyler", "authors": "David E. Tyler, Mengxi Yi", "title": "Breakdown points of penalized and hybrid M-estimators of covariance", "comments": "8 pages, no figures or tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of hybrid M-estimators of multivariate scatter which,\nanalogous to the popular spatial sign covariance matrix (SSCM), possess high\nbreakdown points. We also show that the SSCM can be viewed as an extreme member\nof this class. Unlike the SSCM, but like the regular M-estimators of scatter,\nthis new class of estimators takes into account the shape of the contours of\nthe data cloud for downweighting observations.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 21:37:39 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Tyler", "David E.", ""], ["Yi", "Mengxi", ""]]}, {"id": "2003.00113", "submitter": "Bowen Gang", "authors": "Bowen Gang, Wenguang Sun, and Weinan Wang", "title": "Structure-Adaptive Sequential Testing for Online False Discovery Rate\n  Control", "comments": "45 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the online testing of a stream of hypotheses where a real--time\ndecision must be made before the next data point arrives. The error rate is\nrequired to be controlled at {all} decision points. Conventional\n\\emph{simultaneous testing rules} are no longer applicable due to the more\nstringent error constraints and absence of future data. Moreover, the online\ndecision--making process may come to a halt when the total error budget, or\nalpha--wealth, is exhausted. This work develops a new class of\nstructure--adaptive sequential testing (SAST) rules for online false discover\nrate (FDR) control. A key element in our proposal is a new alpha--investment\nalgorithm that precisely characterizes the gains and losses in sequential\ndecision making. SAST captures time varying structures of the data stream,\nlearns the optimal threshold adaptively in an ongoing manner and optimizes the\nalpha-wealth allocation across different time periods. We present theory and\nnumerical results to show that the proposed method is valid for online FDR\ncontrol and achieves substantial power gain over existing online testing rules.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 23:16:44 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Gang", "Bowen", ""], ["Sun", "Wenguang", ""], ["Wang", "Weinan", ""]]}, {"id": "2003.00117", "submitter": "Li Cai", "authors": "Li Cai, Lijie Gu, Qihua Wang, Suojin Wang", "title": "Simultaneous confidence bands for nonparametric regression with\n  partially missing covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a weighted local linear estimator based on the\ninverse selection probability for nonparametric regression with missing\ncovariates at random. The asymptotic distribution of the maximal deviation\nbetween the estimator and the true regression function is derived and an\nasymptotically accurate simultaneous confidence band is constructed. The\nestimator for the regression function is shown to be oracally efficient in the\nsense that it is uniformly indistinguishable from that when the selection\nprobabilities are known. Finite sample performance is examined via simulation\nstudies which support our asymptotic theory. The proposed method is\ndemonstrated via an analysis of a data set from the Canada 2010/2011 Youth\nStudent Survey.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 23:32:07 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Cai", "Li", ""], ["Gu", "Lijie", ""], ["Wang", "Qihua", ""], ["Wang", "Suojin", ""]]}, {"id": "2003.00192", "submitter": "Elsayed Elamir A. H.", "authors": "Elsayed A. H. Elamir", "title": "Simultaneous test for Means: An Unblind Way to the F-test in One-way\n  Analysis of Variance", "comments": "22 pages, 4 Figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After rejecting the null hypothesis in the analysis of variance, the next\nstep is to make the pairwise comparisons to find out differences in means. The\npurpose of this paper is threefold. The foremost aim is to suggest expression\nfor calculating decision limit that enables us to collect the test and pairwise\ncomparisons in one step. This expression is proposed as the ratio of between\nsquare for each treatment and within sum of squares for all treatments. The\nsecond aim is to obtain the exact sampling distribution of the proposed ratio\nunder the null hypothesis. The exact sampling distribution is derived as the\nbeta distribution of the second type. The third aim is to use beta distribution\nand adjusted p values to create decision limit. Therefore, reject the null\nhypothesis of equal means if any adjusted point falls outsides the decision\nlimit. Simulation study is conducted to compute Type one error. The results\nshow that the proposed method controls the type one error near nominal values\nusing Benjamini-Hochberg adjusted p-values. Two applications are given to show\nthe benefits of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 06:46:38 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Elamir", "Elsayed A. H.", ""]]}, {"id": "2003.00280", "submitter": "Bruce Hoadley", "authors": "Bruce Hoadley", "title": "A Quadratic Programming Solution to the FICO Credit Scoring Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After decades of experience in developing credit scores, the FICO corporation\nhas formulated the FICO Credit Scoring Problem as follows: Find the Generalized\nAdditive Model (GAM), with component step functions, that maximizes divergence\nsubject to the PILE (Palatability, Interpretability, Legal, Explain-ability)\nconstraints. The PILE constraints are also called shape constraints, and\nsatisfying them is called score engineering. Before 2003, FICO used an\nalgorithm, based on Linear Programing, to approximately solve the FICO Credit\nScoring Problem. In this paper, I develop an exact solution to the FICO Credit\nScoring Problem. Finding the exact solution has eluded FICO for years.\nDivergence is a ratio of quadratic functions of the score weights. I show that\nthe max divergence problem can be transformed into a quadratic program. The\nquadratic programming formulation allows one to handle the PILE constraints\nvery easily. FICO currently uses aspects of this technology to develop the\nfamous FICO Credit Score.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 15:32:12 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Hoadley", "Bruce", ""]]}, {"id": "2003.00287", "submitter": "Xiaoyang Guo", "authors": "Xiaoyang Guo, Anuj Srivastava", "title": "Representations, Metrics and Statistics For Shape Analysis of Elastic\n  Graphs", "comments": "Visualization improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past approaches for statistical shape analysis of objects have focused mainly\non objects within the same topological classes, e.g., scalar functions,\nEuclidean curves, or surfaces, etc. For objects that differ in more complex\nways, the current literature offers only topological methods. This paper\nintroduces a far-reaching geometric approach for analyzing shapes of graphical\nobjects, such as road networks, blood vessels, brain fiber tracts, etc. It\nrepresents such objects, exhibiting differences in both geometries and\ntopologies, as graphs made of curves with arbitrary shapes (edges) and\nconnected at arbitrary junctions (nodes). To perform statistical analyses, one\nneeds mathematical representations, metrics and other geometrical tools, such\nas geodesics, means, and covariances. This paper utilizes a quotient structure\nto develop efficient algorithms for computing these quantities, leading to\nuseful statistical tools, including principal component analysis and analytical\nstatistical testing and modeling of graphical shapes. The efficacy of this\nframework is demonstrated using various simulated as well as the real data from\nneurons and brain arterial networks.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 16:07:48 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 00:34:06 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Guo", "Xiaoyang", ""], ["Srivastava", "Anuj", ""]]}, {"id": "2003.00316", "submitter": "Mohsen Sadatsafavi", "authors": "Mohsen Sadatsafavi, Paramita Saha-Chaudhuri, John Petkau", "title": "Model-based ROC (mROC) curve: examining the effect of case-mix and model\n  calibration on the ROC plot", "comments": "44 pages, 1 table, 5 figures, 6 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of risk prediction models is often characterized in terms of\ndiscrimination and calibration. The Receiver Operating Characteristic (ROC)\ncurve is widely used for evaluating model discrimination. When evaluating the\nperformance of a risk prediction model in a new sample, the shape of the ROC\ncurve is affected by both case-mix and the postulated model. Further, compared\nto discrimination, evaluating calibration has not received the same level of\nattention. Commonly used methods for model calibration involve subjective\nspecification of smoothing or grouping. Leveraging the familiar ROC framework,\nwe introduce the model-based ROC (mROC) curve to assess the calibration of a\npre-specified model in a new sample. mROC curve is the ROC curve that should be\nobserved if a pre-specified model is calibrated in the sample. We show the\nempirical ROC and mROC curves for a sample converge asymptotically if the model\nis calibrated in that sample. As a consequence, the mROC curve can be used to\nassess visually the effect of case-mix and model mis-calibration. Further, we\npropose a novel statistical test for calibration that does not require any\nsmoothing or grouping. Simulations support the adequacy of the test. A case\nstudy puts these developments in a practical context. We conclude that mROC can\neasily be constructed and used to evaluate the effect of case-mix and model\ncalibration on the ROC plot, thus adding to the utility of ROC curve analysis\nin the evaluation of risk prediction models. R code for the proposed\nmethodology is provided (https://github.com/msadatsafavi/mROC/).\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 17:34:41 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 22:24:06 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 15:21:48 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Sadatsafavi", "Mohsen", ""], ["Saha-Chaudhuri", "Paramita", ""], ["Petkau", "John", ""]]}, {"id": "2003.00354", "submitter": "Andrey A Popov", "authors": "Andrey A Popov, Adrian Sandu, Elias D. Nino-Ruiz, Geir Evensen", "title": "A Stochastic Covariance Shrinkage Approach in Ensemble Transform Kalman\n  Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": "CSL-TR-20-3", "categories": "stat.ME cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ensemble Kalman Filters (EnKF) employ a Monte-Carlo approach to represent\ncovariance information, and are affected by sampling errors in operational\nsettings where the number of model realizations is much smaller than the model\nstate dimension. To alleviate the effects of these errors EnKF relies on\nmodel-specific heuristics such as covariance localization, which takes\nadvantage of the spatial locality of correlations among the model variables.\nThis work proposes an approach to alleviate sampling errors that utilizes a\nlocally averaged-in-time dynamics of the model, described in terms of a\nclimatological covariance of the dynamical system. We use this covariance as\nthe target matrix in covariance shrinkage methods, and develop a stochastic\ncovariance shrinkage approach where synthetic ensemble members are drawn to\nenrich both the ensemble subspace and the ensemble transformation.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 22:35:13 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 21:35:47 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 16:12:23 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Popov", "Andrey A", ""], ["Sandu", "Adrian", ""], ["Nino-Ruiz", "Elias D.", ""], ["Evensen", "Geir", ""]]}, {"id": "2003.00371", "submitter": "Bradley Price", "authors": "Bradley S. Price and Aaron J. Molstad and Ben Sherwood", "title": "Estimating Multiple Precision Matrices with Cluster Fusion\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized likelihood framework for estimating multiple precision\nmatrices from different classes. Most existing methods either incorporate no\ninformation on relationships between the precision matrices, or require this\ninformation be known a priori. The framework proposed in this article allows\nfor simultaneous estimation of the precision matrices and relationships between\nthe precision matrices, jointly. Sparse and non-sparse estimators are proposed,\nboth of which require solving a non-convex optimization problem. To compute our\nproposed estimators, we use an iterative algorithm which alternates between a\nconvex optimization problem solved by blockwise coordinate descent and a\nk-means clustering problem. Blockwise updates for computing the sparse\nestimator require solving an elastic net penalized precision matrix estimation\nproblem, which we solve using a proximal gradient descent algorithm. We prove\nthat this subalgorithm has a linear rate of convergence. In simulation studies\nand two real data applications, we show that our method can outperform\ncompetitors that ignore relevant relationships between precision matrices and\nperforms similarly to methods which use prior information often uknown in\npractice.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 01:03:22 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Price", "Bradley S.", ""], ["Molstad", "Aaron J.", ""], ["Sherwood", "Ben", ""]]}, {"id": "2003.00498", "submitter": "Bruce Hoadley", "authors": "Bruce Hoadley", "title": "Roughness Penalty for liquid Scorecards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A liquid scorecard has liquid characteristics, for which the characteristic\nscore is a smooth function of the characteristic over a liquid range. The\nsmooth function is based on B-splines, typically cubic. In contrast, the\ncharacteristic scores for traditional scorecards are step functions of the\ncharacteristics. Previously, there were two ways to control the smoothness of\nthe liquid characteristic score: (1) coarse classing where the fewer the number\nof classes, the smoother the curve; (2) the penalty parameter, which penalizes\nthe norm of the score coefficient vector. However, in classical cubic spline\nfitting theory, a direct measure of curve roughness is used as a penalty term\nin the fitting objective function. In this paper, I work out the details of\nthis concept for our characteristic scores, which are linear functions of a\ncubic B-spline basis. The roughness penalty is the integral of the second\nderivative squared. As you vary the characteristic smoothness parameter from\nzero to infinity, the characteristic score goes from being rough to being very\nsmooth. As one moves from rough to smooth, the palatable characteristic score\njumps off the page. This is illustrated by a case study. This case study also\nshows that smoothness parameters, which maximize validation divergence, do not\nalways yield the most palatable model.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 14:45:15 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Hoadley", "Bruce", ""]]}, {"id": "2003.00593", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk and Ruodu Wang", "title": "True and false discoveries with independent e-values", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we use e-values (a non-Bayesian version of Bayes factors) in the\ncontext of multiple hypothesis testing assuming that the base tests produce\nindependent e-values. Our simulation studies and theoretical considerations\nsuggest that, under this assumption, our new algorithms are superior to the\nknown algorithms using independent p-values and to our recent algorithms using\ne-values that are not necessarily independent.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 21:31:01 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Vovk", "Vladimir", ""], ["Wang", "Ruodu", ""]]}, {"id": "2003.00958", "submitter": "Bruce Hoadley", "authors": "Bruce Hoadley", "title": "Score Engineered Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several FICO studies logistic regression has been shown to be a very\ncompetitive technology for developing unrestricted scoring models, especially\nfor performance metrics like ROC area. Application of logistic regression has\nbeen hampered by the lack of software to handle complex score engineering such\nas shape and pattern constraints. The purpose of this paper is to develop a\nsequential quadratic programming algorithm for score engineered logistic\nregression. This approach is based on a simple Taylor series expansion of minus\nlog likelihood, which is locally quadratic. and fits in with the method that\napplies quadratic programming to B-Splines.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 15:04:48 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Hoadley", "Bruce", ""]]}, {"id": "2003.00964", "submitter": "Marco Morucci", "authors": "M. Usaid Awan, Marco Morucci, Vittorio Orlandi, Sudeepa Roy, Cynthia\n  Rudin, Alexander Volfovsky", "title": "Almost-Matching-Exactly for Treatment Effect Estimation under Network\n  Interference", "comments": "Proceedings of the 23rd International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a matching method that recovers direct treatment effects from\nrandomized experiments where units are connected in an observed network, and\nunits that share edges can potentially influence each others' outcomes.\nTraditional treatment effect estimators for randomized experiments are biased\nand error prone in this setting. Our method matches units almost exactly on\ncounts of unique subgraphs within their neighborhood graphs. The matches that\nwe construct are interpretable and high-quality. Our method can be extended\neasily to accommodate additional unit-level covariate information. We show\nempirically that our method performs better than other existing methodologies\nfor this problem, while producing meaningful, interpretable results.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 15:21:20 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Awan", "M. Usaid", ""], ["Morucci", "Marco", ""], ["Orlandi", "Vittorio", ""], ["Roy", "Sudeepa", ""], ["Rudin", "Cynthia", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "2003.00987", "submitter": "Pascal Pernot", "authors": "Pascal Pernot and Andreas Savin", "title": "Probabilistic performance estimators for computational chemistry\n  methods: Systematic Improvement Probability and Ranking Probability Matrix.\n  I. Theory", "comments": null, "journal-ref": "J. Chem. Phys. 152, 164108 (2020)", "doi": "10.1063/5.0006202", "report-no": null, "categories": "stat.ME physics.chem-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The comparison of benchmark error sets is an essential tool for the\nevaluation of theories in computational chemistry. The standard ranking of\nmethods by their Mean Unsigned Error is unsatisfactory for several reasons\nlinked to the non-normality of the error distributions and the presence of\nunderlying trends. Complementary statistics have recently been proposed to\npalliate such deficiencies, such as quantiles of the absolute errors\ndistribution or the mean prediction uncertainty. We introduce here a new score,\nthe systematic improvement probability (SIP), based on the direct system-wise\ncomparison of absolute errors. Independently of the chosen scoring rule, the\nuncertainty of the statistics due to the incompleteness of the benchmark data\nsets is also generally overlooked. However, this uncertainty is essential to\nappreciate the robustness of rankings. In the present article, we develop two\nindicators based on robust statistics to address this problem: P_{inv}, the\ninversion probability between two values of a statistic, and \\mathbf{P}_{r},\nthe ranking probability matrix. We demonstrate also the essential contribution\nof the correlations between error sets in these scores comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 16:11:33 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 07:00:50 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 07:58:57 GMT"}, {"version": "v4", "created": "Mon, 28 Sep 2020 07:52:41 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Pernot", "Pascal", ""], ["Savin", "Andreas", ""]]}, {"id": "2003.01002", "submitter": "Bruce Hoadley", "authors": "Bruce Hoadley", "title": "Score Engineered Robust Least Squares Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In other FICO Technical Papers, I have shown how to fit Generalized Additive\nModels (GAM) with shape constraints using quadratic programming applied to\nB-Spline component functions. In this paper, I extend the method to Robust\nLeast Squares Regression.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 16:31:49 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Hoadley", "Bruce", ""]]}, {"id": "2003.01067", "submitter": "Naji Shajarisales", "authors": "Naji Shajarisales, Peter Spirtes, Kun Zhang", "title": "Learning from Positive and Unlabeled Data by Identifying the Annotation\n  Process", "comments": "Submitted to UAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In binary classification, Learning from Positive and Unlabeled data (LePU) is\nsemi-supervised learning but with labeled elements from only one class. Most of\nthe research on LePU relies on some form of independence between the selection\nprocess of annotated examples and the features of the annotated class, known as\nthe Selected Completely At Random (SCAR) assumption. Yet the annotation process\nis an important part of the data collection, and in many cases it naturally\ndepends on certain features of the data (e.g., the intensity of an image and\nthe size of the object to be detected in the image). Without any constraints on\nthe model for the annotation process, classification results in the LePU\nproblem will be highly non-unique. So proper, flexible constraints are needed.\nIn this work we incorporate more flexible and realistic models for the\nannotation process than SCAR, and more importantly, offer a solution for the\nchallenging LePU problem. On the theory side, we establish the identifiability\nof the properties of the annotation process and the classification function, in\nlight of the considered constraints on the data-generating process. We also\npropose an inference algorithm to learn the parameters of the model, with\nsuccessful experimental results on both simulated and real data. We also\npropose a novel real-world dataset forLePU, as a benchmark dataset for future\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 17:57:12 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Shajarisales", "Naji", ""], ["Spirtes", "Peter", ""], ["Zhang", "Kun", ""]]}, {"id": "2003.01169", "submitter": "Alexander Moreno", "authors": "Alexander Moreno, Zhenke Wu, Jamie Yap, David Wetter, Cho Lam, Inbal\n  Nahum-Shani, Walter Dempsey, James M. Rehg", "title": "A Robust Functional EM Algorithm for Incomplete Panel Count Data", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panel count data describes aggregated counts of recurrent events observed at\ndiscrete time points. To understand dynamics of health behaviors, the field of\nquantitative behavioral research has evolved to increasingly rely upon panel\ncount data collected via multiple self reports, for example, about frequencies\nof smoking using in-the-moment surveys on mobile devices. However, missing\nreports are common and present a major barrier to downstream statistical\nlearning. As a first step, under a missing completely at random assumption\n(MCAR), we propose a simple yet widely applicable functional EM algorithm to\nestimate the counting process mean function, which is of central interest to\nbehavioral scientists. The proposed approach wraps several popular panel count\ninference methods, seamlessly deals with incomplete counts and is robust to\nmisspecification of the Poisson process assumption. Theoretical analysis of the\nproposed algorithm provides finite-sample guarantees by expanding parametric EM\ntheory to our general non-parametric setting. We illustrate the utility of the\nproposed algorithm through numerical experiments and an analysis of smoking\ncessation data. We also discuss useful extensions to address deviations from\nthe MCAR assumption and covariate effects.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 20:04:38 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 16:56:30 GMT"}, {"version": "v3", "created": "Sat, 20 Jun 2020 02:34:52 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Moreno", "Alexander", ""], ["Wu", "Zhenke", ""], ["Yap", "Jamie", ""], ["Wetter", "David", ""], ["Lam", "Cho", ""], ["Nahum-Shani", "Inbal", ""], ["Dempsey", "Walter", ""], ["Rehg", "James M.", ""]]}, {"id": "2003.01242", "submitter": "Lin Dong", "authors": "Lin Dong, Shu Yang, Xiaofei Wang, Donglin Zeng and Jianwen Cai", "title": "Integrative analysis of randomized clinical trials with real world\n  evidence studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We leverage the complementing features of randomized clinical trials (RCTs)\nand real world evidence (RWE) to estimate the average treatment effect of the\ntarget population. First, we propose a calibration weighting estimator that\nuses only covariate information from the RWE study. Because this estimator\nenforces the covariate balance between the RCT and RWE study, the\ngeneralizability of the trial-based estimator is improved. We further propose a\ndoubly robust augmented calibration weighting estimator that achieves the\nsemiparametric efficiency bound derived under the identification assumptions\nwhen the nuisance models are correctly specified. A data-adaptive nonparametric\nsieve method is provided as an alternative to the parametric approach. The\nsieve method guarantees good approximation of the nuisance models. We establish\nasymptotic results under mild regularity conditions, and confirm the finite\nsample performances of the proposed estimators by simulation experiments. When\nthe treatment and outcome information is also available from the RWE study, we\nborrow its predictive power to improve the estimation of the nuisance functions\nunder the outcome mean function transportability assumption. We apply our\nproposed methods to estimate the effect of adjuvant chemotherapy in early-stage\nresected non-small-cell lung cancer integrating data from a RCT and a sample\nfrom the National Cancer Database.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 23:15:31 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Dong", "Lin", ""], ["Yang", "Shu", ""], ["Wang", "Xiaofei", ""], ["Zeng", "Donglin", ""], ["Cai", "Jianwen", ""]]}, {"id": "2003.01280", "submitter": "Lixing Zhu", "authors": "Wenbiao Zhao, Xuehu Zhu and Lixing Zhu", "title": "Detecting multiple change points: a PULSE criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research described herewith investigates detecting change points of means\nand of variances in a sequence of observations. The number of change points can\nbe divergent at certain rate as the sample size goes to infinity. We define a\nMOSUM-based objective function for this purpose. Unlike all existing\nMOSUM-based methods, the novel objective function exhibits an useful ``PULSE\"\npattern near change points in the sense: at the population level, the value at\nany change point plus 2 times of the segment length of the moving average\nattains a local minimum tending to zero following by a local maximum going to\ninfinity. This feature provides an efficient way to simultaneously identify all\nchange points at the sample level. In theory, the number of change points can\nbe consistently estimated and the locations can also be consistently estimated\nin a certain sense. Further, because of its visualization nature, in practice,\nthe locations can be relatively more easily identified by plots than existing\nmethods in the literature. The method can also handle the case in which the\nsignals of some change points are very weak in the sense that those changes go\nto zero. Further, the computational cost is very inexpensive. The numerical\nstudies we conduct validate its good performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 01:18:44 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Zhao", "Wenbiao", ""], ["Zhu", "Xuehu", ""], ["Zhu", "Lixing", ""]]}, {"id": "2003.01286", "submitter": "Zheyang Wu", "authors": "Hong Zhang and Zheyang Wu", "title": "Accurate $p$-Value Calculation for Generalized Fisher's Combination\n  Tests Under Dependence", "comments": "53 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining dependent tests of significance has broad applications but the\n$p$-value calculation is challenging. Current moment-matching methods (e.g.,\nBrown's approximation) for Fisher's combination test tend to significantly\ninflate the type I error rate at the level less than 0.05. It could lead to\nsignificant false discoveries in big data analyses. This paper provides several\nmore accurate and computationally efficient $p$-value calculation methods for a\ngeneral family of Fisher type statistics, referred as the GFisher. The GFisher\ncovers Fisher's combination, Good's statistic, Lancaster's statistic, weighted\nZ-score combination, etc. It allows a flexible weighting scheme, as well as an\nomnibus procedure that automatically adapts proper weights and degrees of\nfreedom to a given data. The new $p$-value calculation methods are based on\nnovel ideas of moment-ratio matching and joint-distribution surrogating.\nSystematic simulations show that they are accurate under multivariate Gaussian,\nand robust under the generalized linear model and the multivariate\n$t$-distribution, down to at least $10^{-6}$ level. We illustrate the\nusefulness of the GFisher and the new $p$-value calculation methods in\nanalyzing both simulated and real data of gene-based SNP-set association\nstudies in genetics. Relevant computation has been implemented into R package\n$GFisher$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 01:33:17 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Zhang", "Hong", ""], ["Wu", "Zheyang", ""]]}, {"id": "2003.01319", "submitter": "Joe Watson", "authors": "Joe Watson", "title": "A fast Monte Carlo test for preferential sampling", "comments": "24 pages, 4 figures, plus 10 pages of supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The preferential sampling of locations chosen to observe a spatio-temporal\nprocess has been identified as a major problem across multiple fields.\nPredictions of the process can be severely biased when standard statistical\nmethodologies are applied to preferentially sampled data without adjustment.\nCurrently, methods that can adjust for preferential sampling are rarely\nimplemented in the software packages most popular with researchers.\nFurthermore, they are technically demanding to design and fit. This paper\npresents a fast and intuitive Monte Carlo test for detecting preferential\nsampling. The test can be applied across a wide range of data types.\nImportantly, the method can also help with the discovery of a set of\ninformative covariates that can sufficiently control for the preferential\nsampling. The discovery of these covariates can justify continued use of\nstandard methodologies. A thorough simulation study is presented to demonstrate\nboth the power and validity of the test in various data settings. The test is\nshown to attain high power for non-Gaussian data with sample sizes as low as\n50. Finally, two previously-published case studies are revisited and new\ninsights into the nature of the informative sampling are gained. The test can\nbe implemented with the R package PStestR\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 03:48:48 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 02:22:14 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Watson", "Joe", ""]]}, {"id": "2003.01489", "submitter": "Dong Liu", "authors": "Dong Liu and Viktoria Fodor and Lars K. Rasmussen", "title": "Will Scale-free Popularity Develop Scale-free Geo-social Networks?", "comments": null, "journal-ref": "IEEE Transactions on Network Science and Engineering, 2019", "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical results show that spatial factors such as distance, population\ndensity and communication range affect our social activities, also reflected by\nthe development of ties in social networks. This motivates the need for social\nnetwork models that take these spatial factors into account. Therefore, in this\npaper we propose a gravity-low-based geo-social network model, where\nconnections develop according to the popularity of the individuals, but are\nconstrained through their geographic distance and the surrounding population\ndensity. Specifically, we consider a power-law distributed popularity, and\nrandom node positions governed by a Poisson point process. We evaluate the\ncharacteristics of the emerging networks, considering the degree distribution,\nthe average degree of neighbors and the local clustering coefficient. These\nlocal metrics reflect the robustness of the network, the information\ndissemination speed and the communication locality. We show that unless the\ncommunication range is strictly limited, the emerging networks are scale-free,\nwith a rank exponent affected by the spatial factors. Even the average neighbor\ndegree and the local clustering coefficient show tendencies known in\nnon-geographic scale-free networks, at least when considering individuals with\nlow popularity. At high-popularity values, however, the spatial constraints\nlead to popularity-independent average neighbor degrees and clustering\ncoefficients.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 12:57:43 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Liu", "Dong", ""], ["Fodor", "Viktoria", ""], ["Rasmussen", "Lars K.", ""]]}, {"id": "2003.01747", "submitter": "Victor Veitch", "authors": "Victor Veitch and Anisha Zaveri", "title": "Sense and Sensitivity Analysis: Simple Post-Hoc Analysis of Bias Due to\n  Unobserved Confounding", "comments": "\"Austen\" is Jane Austen, in service of the pun in the title. Paper\n  published at NeurIPS 2020. Arxiv version has identical content but nicer\n  formating. NeurIPS spotlight talk here:\n  https://nips.cc/virtual/2020/public/poster_7d265aa7147bd3913fb84c7963a209d1.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a truth universally acknowledged that an observed association without\nknown mechanism must be in want of a causal estimate. However, causal\nestimation from observational data often relies on the (untestable) assumption\nof `no unobserved confounding'. Violations of this assumption can induce bias\nin effect estimates. In principle, such bias could invalidate or reverse the\nconclusions of a study. However, in some cases, we might hope that the\ninfluence of unobserved confounders is weak relative to a `large' estimated\neffect, so the qualitative conclusions are robust to bias from unobserved\nconfounding. The purpose of this paper is to develop \\emph{Austen plots}, a\nsensitivity analysis tool to aid such judgments by making it easier to reason\nabout potential bias induced by unobserved confounding. We formalize\nconfounding strength in terms of how strongly the confounder influences\ntreatment assignment and outcome. For a target level of bias, an Austen plot\nshows the minimum values of treatment and outcome influence required to induce\nthat level of bias. Domain experts can then make subjective judgments about\nwhether such strong confounders are plausible. To aid this judgment, the Austen\nplot additionally displays the estimated influence strength of (groups of) the\nobserved covariates. Austen plots generalize the classic sensitivity analysis\napproach of Imbens [Imb03]. Critically, Austen plots allow any approach for\nmodeling the observed data and producing the initial estimate. We illustrate\nthe tool by assessing biases for several real causal inference problems, using\na variety of machine learning approaches for the initial data analysis. Code is\navailable at https://github.com/anishazaveri/austen_plots\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 19:18:24 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 01:11:49 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Veitch", "Victor", ""], ["Zaveri", "Anisha", ""]]}, {"id": "2003.01804", "submitter": "Piaomu Liu", "authors": "Piaomu Liu", "title": "Prediction of Time-to-terminal Event (TTTE) in a Class of Joint Dynamic\n  Models", "comments": "The title is edited. A real-life scenario where the proposed method\n  can be applied to is added to Introduction. Model evaluation is updated\n  (5-fold cross-validation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In different areas of research, multiple recurrent competing risks (RCR) are\noften observed on the same observational unit. For instance, different types of\ncancer relapses are observed on the same patient and several types of component\nfailures are observed in the same reliability system. When a terminal event\n(TE) such as death is also observed on the same unit, since the RCRs are\ngenerally informative about death, we develop joint dynamic models that\nsimultaneously model the RCRs and the TE. A key interest of such joint dynamic\nmodeling is to predict time-to-terminal event (TTTE) for new units that have\nnot experienced the TE by the end of monitoring period. In this paper, we\npropose a simulation approach to predict TTTE which arises from a class of\njoint dynamic models of RCRs and TE. The proposed approach can be applied to\nproblems in precision medicine and potentially many other settings. The\nsimulation method makes personalized predictions of TTTE and provides an\nempirical predictive distribution of TTTE. Predictions of the RCR occurrences\nbeyond a possibly random monitoring time and leading up to the TE occurrence\nare also produced. The approach is dynamic in that each simulated occurrence of\nRCR increases the amount of knowledge we obtain on an observational unit which\ninforms the simulation of TTTE. We demonstrate the approach on a synthetic\ndataset and evaluate predictive accuracy of the prediction method through\n5-fold cross-validation using empirical Brier score.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 21:24:41 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 18:56:50 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Liu", "Piaomu", ""]]}, {"id": "2003.01805", "submitter": "Marco Morucci", "authors": "Marco Morucci, Vittorio Orlandi, Sudeepa Roy, Cynthia Rudin, Alexander\n  Volfovsky", "title": "Adaptive Hyper-box Matching for Interpretable Individualized Treatment\n  Effect Estimation", "comments": null, "journal-ref": "Proceedings of the Thirty-sixth Conference on Uncertainty in\n  Artificial Intelligence (UAI 2020)", "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a matching method for observational data that matches units with\nothers in unit-specific, hyper-box-shaped regions of the covariate space. These\nregions are large enough that many matches are created for each unit and small\nenough that the treatment effect is roughly constant throughout. The regions\nare found as either the solution to a mixed integer program, or using a (fast)\napproximation algorithm. The result is an interpretable and tailored estimate\nof a causal effect for each unit.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 21:26:56 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 14:05:26 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Morucci", "Marco", ""], ["Orlandi", "Vittorio", ""], ["Roy", "Sudeepa", ""], ["Rudin", "Cynthia", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "2003.01850", "submitter": "Jiayin Zheng", "authors": "Zheng Jiayin, Zheng Yingye and Hsu Li", "title": "Risk Projection for Time-to-event Outcome Leveraging Summary Statistics\n  With Source Individual-level Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting risks of chronic diseases has become increasingly important in\nclinical practice. When a prediction model is developed in a given source\ncohort, there is often a great interest to apply the model to other cohorts.\nHowever, due to potential discrepancy in baseline disease incidences between\ndifferent cohorts and shifts in patient composition, the risk predicted by the\noriginal model often under- or over-estimates the risk in the new cohort. The\nremedy of such a poorly calibrated prediction is needed for proper medical\ndecision-making. In this article, we assume the relative risks of predictors\nare the same between the two cohorts, and propose a novel weighted estimating\nequation approach to re-calibrating the projected risk for the targeted\npopulation through updating the baseline risk. The recalibration leverages the\nknowledge about the overall survival probabilities for the disease of interest\nand competing events, and the summary information of risk factors from the\ntargeted population. The proposed re-calibrated risk estimators gain efficiency\nif the risk factor distributions are the same for both the source and target\ncohorts, and are robust with little bias if they differ. We establish the\nconsistency and asymptotic normality of the proposed estimators. Extensive\nsimulation studies demonstrate that the proposed estimators perform very well\nin terms of robustness and efficiency in finite samples. A real data\napplication to colorectal cancer risk prediction also illustrates that the\nproposed method can be used in practice for model recalibration.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 01:19:30 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Jiayin", "Zheng", ""], ["Yingye", "Zheng", ""], ["Li", "Hsu", ""]]}, {"id": "2003.01856", "submitter": "Hongxiang Qiu", "authors": "Hongxiang Qiu, Alex Luedtke, Marco Carone", "title": "Universal sieve-based strategies for efficient estimation using machine\n  learning tools", "comments": "46 pages, 6 figures, submitted to Bernoulli", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we wish to estimate a finite-dimensional summary of one or more\nfunction-valued features of an underlying data-generating mechanism under a\nnonparametric model. One approach to estimation is by plugging in flexible\nestimates of these features. Unfortunately, in general, such estimators may not\nbe asymptotically efficient, which often makes these estimators difficult to\nuse as a basis for inference. Though there are several existing methods to\nconstruct asymptotically efficient plug-in estimators, each such method either\ncan only be derived using knowledge of efficiency theory or is only valid under\nstringent smoothness assumptions. Among existing methods, sieve estimators\nstand out as particularly convenient because efficiency theory is not required\nin their construction, their tuning parameters can be selected data adaptively,\nand they are universal in the sense that the same fits lead to efficient\nplug-in estimators for a rich class of estimands. Inspired by these desirable\nproperties, we propose two novel universal approaches for estimating\nfunction-valued features that can be analyzed using sieve estimation theory.\nCompared to traditional sieve estimators, these approaches are valid under more\ngeneral conditions on the smoothness of the function-valued features by\nutilizing flexible estimates that can be obtained, for example, using machine\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 01:59:46 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 23:36:33 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Qiu", "Hongxiang", ""], ["Luedtke", "Alex", ""], ["Carone", "Marco", ""]]}, {"id": "2003.01873", "submitter": "Bowen Gang", "authors": "Bowen Gang, Gourab Mukherjee and Wenguang Sun", "title": "Large-Scale Shrinkage Estimation under Markovian Dependence", "comments": "26 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of simultaneous estimation of a sequence of dependent\nparameters that are generated from a hidden Markov model. Based on observing a\nnoise contaminated vector of observations from such a sequence model, we\nconsider simultaneous estimation of all the parameters irrespective of their\nhidden states under square error loss. We study the roles of statistical\nshrinkage for improved estimation of these dependent parameters. Being\ncompletely agnostic on the distributional properties of the unknown underlying\nHidden Markov model, we develop a novel non-parametric shrinkage algorithm. Our\nproposed method elegantly combines \\textit{Tweedie}-based non-parametric\nshrinkage ideas with efficient estimation of the hidden states under Markovian\ndependence. Based on extensive numerical experiments, we establish superior\nperformance our our proposed algorithm compared to non-shrinkage based\nstate-of-the-art parametric as well as non-parametric algorithms used in hidden\nMarkov models. We provide decision theoretic properties of our methodology and\nexhibit its enhanced efficacy over popular shrinkage methods built under\nindependence. We demonstrate the application of our methodology on real-world\ndatasets for analyzing of temporally dependent social and economic indicators\nsuch as search trends and unemployment rates as well as estimating spatially\ndependent Copy Number Variations.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 03:29:40 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 23:25:16 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Gang", "Bowen", ""], ["Mukherjee", "Gourab", ""], ["Sun", "Wenguang", ""]]}, {"id": "2003.01946", "submitter": "Aritz Adin", "authors": "A. Adin, T. Goicoa, J. S. Hodges, P. Schnell and M. D. Ugarte", "title": "Alleviating confounding in spatio-temporal areal models with an\n  application on crimes against women in India", "comments": null, "journal-ref": "Statistical Modelling 2021", "doi": "10.1177/1471082X211015452", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing associations between a response of interest and a set of covariates\nin spatial areal models is the leitmotiv of ecological regression. However, the\npresence of spatially correlated random effects can mask or even bias estimates\nof such associations due to confounding effects if they are not carefully\nhandled. Though potentially harmful, confounding issues have often been ignored\nin practice leading to wrong conclusions about the underlying associations\nbetween the response and the covariates. In spatio-temporal areal models, the\ntemporal dimension may emerge as a new source of confounding, and the problem\nmay be even worse. In this work, we propose two approaches to deal with\nconfounding of fixed effects by spatial and temporal random effects, while\nobtaining good model predictions. In particular, restricted regression and an\napparently -- though in fact not -- equivalent procedure using constraints are\nproposed within both fully Bayes and empirical Bayes approaches. The methods\nare compared in terms of fixed-effect estimates and model selection criteria.\nThe techniques are used to assess the association between dowry deaths and\ncertain socio-demographic covariates in the districts of Uttar Pradesh, India.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 08:36:11 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 10:03:31 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 17:09:04 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Adin", "A.", ""], ["Goicoa", "T.", ""], ["Hodges", "J. S.", ""], ["Schnell", "P.", ""], ["Ugarte", "M. D.", ""]]}, {"id": "2003.01951", "submitter": "Felix Abramovich", "authors": "Felix Abramovich, Vadim Grinshtein and Tomer Levy", "title": "Multiclass classification by sparse multinomial logistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider high-dimensional multiclass classification by\nsparse multinomial logistic regression. We propose first a feature selection\nprocedure based on penalized maximum likelihood with a complexity penalty on\nthe model size and derive the nonasymptotic bounds for misclassification excess\nrisk of the resulting classifier. We establish also their tightness by deriving\nthe corresponding minimax lower bounds. In particular, we show that there exist\ntwo regimes corresponding to small and large number of classes. The bounds can\nbe reduced under the additional low noise condition. To find a penalized\nmaximum likelihood solution with a complexity penalty requires, however, a\ncombinatorial search over all possible models. To design a feature selection\nprocedure computationally feasible for high-dimensional data, we propose\nmultinomial logistic group Lasso and Slope classifiers and show that they also\nachieve the minimax order.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 08:44:48 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 09:53:00 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 11:35:48 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Abramovich", "Felix", ""], ["Grinshtein", "Vadim", ""], ["Levy", "Tomer", ""]]}, {"id": "2003.01962", "submitter": "Morteza Raeisi", "authors": "Morteza Raeisi, Florent Bonneu, Edith Gabriel", "title": "On spatial and spatio-temporal multi-structure point process models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial and spatio-temporal single-structure point process models are widely\nused in epidemiology, biology, ecology, seismology... . However, most natural\nphenomena present multiple interaction structure or exhibit dependence at\nmultiple scales in space and/or time, leading to define new spatial and\nspatio-temporal multi-structure point process models. In this paper, we\ninvestigate and review such multi-structure point process models mainly based\non Gibbs and Cox processes.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 09:19:49 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 07:25:16 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Raeisi", "Morteza", ""], ["Bonneu", "Florent", ""], ["Gabriel", "Edith", ""]]}, {"id": "2003.02130", "submitter": "Tiejun Tong", "authors": "Jiandong Shi and Dehui Luo and Hong Weng and Xian-Tao Zeng and Lu Lin\n  and Haitao Chu and Tiejun Tong", "title": "Optimally estimating the sample standard deviation from the five-number\n  summary", "comments": "30 pages and 4 figures and 2 tables. arXiv admin note: substantial\n  text overlap with arXiv:1801.01267", "journal-ref": "Research Synthesis Methods, 2020", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When reporting the results of clinical studies, some researchers may choose\nthe five-number summary (including the sample median, the first and third\nquartiles, and the minimum and maximum values) rather than the sample mean and\nstandard deviation, particularly for skewed data. For these studies, when\nincluded in a meta-analysis, it is often desired to convert the five-number\nsummary back to the sample mean and standard deviation. For this purpose,\nseveral methods have been proposed in the recent literature and they are\nincreasingly used nowadays. In this paper, we propose to further advance the\nliterature by developing a smoothly weighted estimator for the sample standard\ndeviation that fully utilizes the sample size information. For ease of\nimplementation, we also derive an approximation formula for the optimal weight,\nas well as a shortcut formula for the sample standard deviation. Numerical\nresults show that our new estimator provides a more accurate estimate for\nnormal data and also performs favorably for non-normal data. Together with the\noptimal sample mean estimator in Luo et al., our new methods have dramatically\nimproved the existing methods for data transformation, and they are capable to\nserve as \"rules of thumb\" in meta-analysis for studies reported with the\nfive-number summary. Finally for practical use, an Excel spreadsheet and an\nonline calculator are also provided for implementing our optimal estimators.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 11:32:33 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 04:13:48 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Shi", "Jiandong", ""], ["Luo", "Dehui", ""], ["Weng", "Hong", ""], ["Zeng", "Xian-Tao", ""], ["Lin", "Lu", ""], ["Chu", "Haitao", ""], ["Tong", "Tiejun", ""]]}, {"id": "2003.02155", "submitter": "Thomas Opitz", "authors": "Patrizia Zamberletti, Julien Papa\\\"ix, Edith Gabriel, Thomas Opitz", "title": "Landscape allocation: stochastic generators and statistical inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In agricultural landscapes, the composition and spatial configuration of\ncultivated and semi-natural elements strongly impact species dynamics, their\ninteractions and habitat connectivity. To allow for landscape structural\nanalysis and scenario generation, we here develop statistical tools for real\nlandscapes composed of geometric elements including 2D patches but also 1D\nlinear elements such as hedges. We design generative stochastic models that\ncombine a multiplex network representation and Gibbs energy terms to\ncharacterize the distributional behavior of landscape descriptors for land-use\ncategories. We implement Metropolis-Hastings for this new class of models to\nsample agricultural scenarios featuring parameter-controlled spatial and\ntemporal patterns (e.g., geometry, connectivity, crop-rotation).\nPseudolikelihood-based inference allows studying the relevance of model\ncomponents in real landscapes through statistical and functional validation,\nthe latter achieved by comparing commonly used landscape metrics between\nobserved and simulated landscapes. Models fitted to subregions of the Lower\nDurance Valley (France) indicate strong deviation from random allocation, and\nthey realistically capture small-scale landscape patterns. In summary, our\napproach of statistical modeling improves the understanding of structural and\nfunctional aspects of agro-ecosystems, and it enables simulation-based\ntheoretical analysis of how landscape patterns shape biological and ecological\nprocesses.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 09:07:33 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Zamberletti", "Patrizia", ""], ["Papa\u00efx", "Julien", ""], ["Gabriel", "Edith", ""], ["Opitz", "Thomas", ""]]}, {"id": "2003.02205", "submitter": "Marco Broccardo", "authors": "Ziqi Wang, Marco Broccardo, Junho Song", "title": "Probabilistic Performance-Pattern Decomposition (PPPD): analysis\n  framework and applications to stochastic mechanical systems", "comments": "Autoencoder, clustering, diffusion map, manifold learning, Monte\n  Carlo simulation, pattern recognition, stochastic dynamics, uncertainty\n  quantification. 44 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the early 1900s, numerous research efforts have been devoted to\ndeveloping quantitative solutions to stochastic mechanical systems. In general,\nthe problem is perceived as solved when a complete or partial probabilistic\ndescription on the quantity of interest (QoI) is determined. However, in the\npresence of complex system behavior, there is a critical need to go beyond mere\nprobabilistic descriptions. In fact, to gain a full understanding of the\nsystem, it is crucial to extract physical characterizations from the\nprobabilistic structure of the QoI, especially when the QoI solution is\nobtained in a data-driven fashion. Motivated by this perspective, the paper\nproposes a framework to obtain structuralized characterizations on behaviors of\nstochastic systems. The framework is named Probabilistic Performance-Pattern\nDecomposition (PPPD). PPPD analysis aims to decompose complex response\nbehaviors, conditional to a prescribed performance state, into meaningful\npatterns in the space of system responses, and to investigate how the patterns\nare triggered in the space of basic random variables. To illustrate the\napplication of PPPD, the paper studies three numerical examples: 1) an\nillustrative example with hypothetical stochastic processes input and output;\n2) a stochastic Lorenz system with periodic as well as chaotic behaviors; and\n3) a simplified shear-building model subjected to a stochastic ground motion\nexcitation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 17:18:43 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Wang", "Ziqi", ""], ["Broccardo", "Marco", ""], ["Song", "Junho", ""]]}, {"id": "2003.02367", "submitter": "Nick James", "authors": "Nick James and Max Menzies", "title": "Optimally adaptive Bayesian spectral density estimation for stationary\n  and nonstationary processes", "comments": "Equal contribution. Expression and structure edits relative to v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article improves on existing methods to estimate the spectral density of\nstationary and nonstationary time series assuming a Gaussian process prior. By\noptimising an appropriate eigendecomposition using a smoothing spline\ncovariance structure, our method more appropriately models both smooth and\nrough data. We further justify the utility of this optimal eigendecomposition\nby investigating the performance of alternative covariance functions other than\nsmoothing splines. We show that the optimal eigendecomposition provides a\nmaterial improvement, while the other covariance functions under examination do\nnot, all performing comparatively well as the smoothing spline. During our\ncomputational investigation, we introduce new validation metrics for the\nspectral density estimate, inspired from the physical sciences. We validate our\nmodels in an extensive simulation study and demonstrate superior performance\nwith real data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 23:35:57 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 10:00:46 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["James", "Nick", ""], ["Menzies", "Max", ""]]}, {"id": "2003.02421", "submitter": "Sagar Kumar Tamang", "authors": "Sagar K. Tamang, Ardeshir Ebtehaj, Dongmian Zou and Gilad Lerman", "title": "Regularized Variational Data Assimilation for Bias Treatment using the\n  Wasserstein Metric", "comments": "7 figures", "journal-ref": "Quarterly Journal of the Royal Meteorological Society, Volume 146,\n  Issue 730, pages 2332-2346, July 2020", "doi": "10.1002/qj.3794", "report-no": null, "categories": "stat.ME math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new variational data assimilation (VDA) approach for\nthe formal treatment of bias in both model outputs and observations. This\napproach relies on the Wasserstein metric stemming from the theory of optimal\nmass transport to penalize the distance between the probability histograms of\nthe analysis state and an a priori reference dataset, which is likely to be\nmore uncertain but less biased than both model and observations. Unlike\nprevious bias-aware VDA approaches, the new Wasserstein metric VDA (WM-VDA)\ndynamically treats systematic biases of unknown magnitude and sign in both\nmodel and observations through assimilation of the reference data in the\nprobability domain and can fully recover the probability histogram of the\nanalysis state. The performance of WM-VDA is compared with the classic\nthree-dimensional VDA (3D-Var) scheme on first-order linear dynamics and the\nchaotic Lorenz attractor. Under positive systematic biases in both model and\nobservations, we consistently demonstrate a significant reduction in the\nforecast bias and unbiased root mean squared error.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 03:56:18 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Tamang", "Sagar K.", ""], ["Ebtehaj", "Ardeshir", ""], ["Zou", "Dongmian", ""], ["Lerman", "Gilad", ""]]}, {"id": "2003.02476", "submitter": "Matthias Eckardt", "authors": "Matthias Eckardt, Jonatan A. Gonz\\'alez, and Jorge Mateu", "title": "Graphical modelling and partial characteristics for multitype and\n  multivariate-marked spatio-temporal point processes", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes to the multivariate analysis of marked spatio-temporal\npoint process data by introducing different partial point characteristics and\nextending the spatial dependence graph model formalism. Our approach yields a\nunified framework for different types of spatio-temporal data including both,\npurely qualitatively (multivariate) cases and multivariate cases with\nadditional quantitative marks. The proposed graphical model is defined through\npartial spectral density characteristics, it is highly computationally\nefficient and reflects the conditional similarity among sets of spatio-temporal\nsub-processes of either points or marked points with identical discrete marks.\nThe paper considers three applications, two on crime data and a third one on\nforestry.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 08:14:48 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Eckardt", "Matthias", ""], ["Gonz\u00e1lez", "Jonatan A.", ""], ["Mateu", "Jorge", ""]]}, {"id": "2003.02561", "submitter": "Benjamin Taylor", "authors": "Benjamin M. Taylor", "title": "A Multi-Way Correlation Coefficient", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pearson's correlation is an important summary measure of the amount of\ndependence between two variables. It is natural to want to generalise the\nconcept of correlation as a single number that measures the inter-relatedness\nof three or more variables e.g. how `correlated' are a collection of variables\nin which non are specifically to be treated as an `outcome'? In this short\narticle, we introduce such a measure, and show that it reduces to the modulus\nof Pearson's $r$ in the two dimensional case.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 12:19:41 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Taylor", "Benjamin M.", ""]]}, {"id": "2003.02566", "submitter": "Matthieu Garcin", "authors": "Matthieu Garcin", "title": "A comparison of maximum likelihood and absolute moments for the\n  estimation of Hurst exponents in a stationary framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The absolute-moment method is widespread for estimating the Hurst exponent of\na fractional Brownian motion $X$. But this method is biased when applied to a\nstationary version of $X$, in particular an inverse Lamperti transform of $X$,\nwith a linear time contraction of parameter $\\theta$. We present an adaptation\nof the absolute-moment method to this framework and we compare it to the\nmaximum likelihood method, with simulations. While it appears that the\nmaximum-likelihood method is more accurate than the adapted absolute-moment\nestimation, this last method is not uninteresting for two reasons: it makes it\npossible to confirm visually that the model is well specified and it is\ncomputationally more performing.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 12:34:46 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 22:28:13 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Garcin", "Matthieu", ""]]}, {"id": "2003.02578", "submitter": "Hee-Seok Oh", "authors": "Jang-Hyun Kim, Jongmin Lee, Hee-Seok Oh", "title": "Spherical Principal Curves", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  43, 2165-2171 (2021)", "doi": "10.1109/TPAMI.2020.3025327", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for dimension reduction of data observed\nin a sphere. Several dimension reduction techniques have recently developed for\nthe analysis of non-Euclidean data. As a pioneer work, Hauberg (2016) attempted\nto implement principal curves on Riemannian manifolds. However, this approach\nuses approximations to deal with data on Riemannian manifolds, which causes\ndistorted results. In this study, we propose a new approach to construct\nprincipal curves on a sphere by a projection of the data onto a continuous\ncurve. Our approach lies in the same line of Hastie and Stuetzle (1989) that\nproposed principal curves for Euclidean space data. We further investigate the\nstationarity of the proposed principal curves that satisfy the self-consistency\non a sphere. Results from real data analysis with earthquake data and\nsimulation examples demonstrate the promising empirical properties of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 12:50:51 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 13:13:26 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 07:11:41 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Kim", "Jang-Hyun", ""], ["Lee", "Jongmin", ""], ["Oh", "Hee-Seok", ""]]}, {"id": "2003.02580", "submitter": "Shuangge Ma", "authors": "Qingzhao Zhang, Hao Chai, Shuangge Ma", "title": "Robust Identification of Gene-Environment Interactions under\n  High-Dimensional Accelerated Failure Time Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For complex diseases, beyond the main effects of genetic (G) and\nenvironmental (E) factors, gene-environment (G-E) interactions also play an\nimportant role. Many of the existing G-E interaction methods conduct marginal\nanalysis, which may not appropriately describe disease biology. Joint analysis\nmethods have been developed, with most of the existing loss functions\nconstructed based on likelihood. In practice, data contamination is not\nuncommon. Development of robust methods for interaction analysis that can\naccommodate data contamination is very limited. In this study, we consider\ncensored survival data and adopt an accelerated failure time (AFT) model. An\nexponential squared loss is adopted to achieve robustness. A sparse group\npenalization approach, which respects the \"main effects, interactions\"\nhierarchy, is adopted for estimation and identification. Consistency properties\nare rigorously established. Simulation shows that the proposed method\noutperforms direct competitors. In data analysis, the proposed method makes\nbiologically sensible findings.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 12:52:33 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Zhang", "Qingzhao", ""], ["Chai", "Hao", ""], ["Ma", "Shuangge", ""]]}, {"id": "2003.02682", "submitter": "Sven Otto", "authors": "Sven Otto and J\\\"org Breitung", "title": "Backward CUSUM for Testing and Monitoring Structural Change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the conventional CUSUM test suffers from low power and\nlarge detection delay. We therefore propose two alternative detector\nstatistics. The backward CUSUM detector sequentially cumulates the recursive\nresiduals in reverse chronological order, whereas the stacked backward CUSUM\ndetector considers a triangular array of backward cumulated residuals. While\nboth the backward CUSUM detector and the stacked backward CUSUM detector are\nsuitable for retrospective testing, only the stacked backward CUSUM detector\ncan be monitored on-line. The limiting distributions of the maximum statistics\nunder suitable sequences of alternatives are derived for retrospective testing\nand fixed endpoint monitoring. In the retrospective testing context, the local\npower of the tests is shown to be substantially higher than that for the\nconventional CUSUM test if a single break occurs after one third of the sample\nsize. When applied to monitoring schemes, the detection delay of the stacked\nbackward CUSUM is shown to be much shorter than that of the conventional\nmonitoring CUSUM procedure. Moreover, an infinite horizon monitoring procedure\nand critical values are presented.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 14:53:58 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Otto", "Sven", ""], ["Breitung", "J\u00f6rg", ""]]}, {"id": "2003.02761", "submitter": "Elena Ballante", "authors": "Elena Ballante (1), Pierpaolo Uberti (2), Silvia Figini (3) ((1)\n  Department of Mathematics, University of Pavia, (2) Department of Economics,\n  University of Genova, (3) Department of Political and Social Sciences,\n  University of Pavia)", "title": "A new approach in model selection for ordinal target variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach to assess model performance for\npredictive models characterized by an ordinal target variable in order to\nsatisfy the lack of suitable tools in this framework. Our methodological\nproposal is a new index for model assessment which satisfies mathematical\nproperties and can be easily computed. In order to show how our performance\nindicator works, empirical evidence achieved on a toy examples and simulated\ndata are provided. On the basis of results at hand, we underline that our\napproach discriminates better for model selection with respect to performance\nindexes proposed in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 16:53:45 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Ballante", "Elena", ""], ["Uberti", "Pierpaolo", ""], ["Figini", "Silvia", ""]]}, {"id": "2003.02791", "submitter": "Lok Ting Yuen", "authors": "Christine Yuen and Piotr Fryzlewicz", "title": "Exploiting disagreement between high-dimensional variable selectors for\n  uncertainty visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Combined Selection and Uncertainty Visualizer (CSUV), which\nestimates the set of true covariates in high-dimensional linear regression and\nvisualizes selection uncertainties by exploiting the (dis)agreement among\ndifferent base selectors. Our proposed method selects covariates that get\nselected the most frequently by the different variable selection methods on\nsubsampled data. The method is generic and can be used with different existing\nvariable selection methods. We demonstrate its variable selection performance\nusing real and simulated data. The variable selection method and its\nuncertainty illustration tool are publicly available as R package CSUV\n(https://github.com/christineyuen/CSUV). The graphical tool is also available\nonline via https://csuv.shinyapps.io/csuv\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 17:40:37 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Yuen", "Christine", ""], ["Fryzlewicz", "Piotr", ""]]}, {"id": "2003.02906", "submitter": "Vartan Choulakian", "authors": "Choulakian Vartan and Abou Samra Ghassan", "title": "Mean absolute deviations about the mean, the cut norm and taxicab\n  correspondence analysis", "comments": "18 pages, 4 figures", "journal-ref": "Open Journal of Statistics 2020", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization has two faces, minimization of a loss function or maximization\nof a gain function. We show that the mean absolute deviations about the mean,\nd, maximizes a gain function based on the power set of the individuals, and it\nis equal to twice the value of its cut-norm. This property is generalized to\ndouble-centered and triple-centered data sets. Furthermore, we show that among\nthe three well known dispersion measures, standard deviation, least absolute\ndeviation and d, d is the most robust based on the relative contribution\ncriterion. More importantly, we show that the computation of each principal\ndimension of taxicab correspondence analysis corresponds to balanced 2-blocks\nseriation. Examples are provided.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 20:28:22 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Vartan", "Choulakian", ""], ["Ghassan", "Abou Samra", ""]]}, {"id": "2003.02929", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin, Geir Storvik, Florian Frommlet", "title": "Flexible Bayesian Nonlinear Model Configuration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models are used in a wide range of applications providing a\npowerful scientific tool for researchers from different fields. Linear models\nare often not sufficient to describe the complex relationship between input\nvariables and a response. This relationship can be better described by\nnon-linearities and complex functional interactions. Deep learning models have\nbeen extremely successful in terms of prediction although they are often\ndifficult to specify and potentially suffer from overfitting. In this paper, we\nintroduce a class of Bayesian generalized nonlinear regression models with a\ncomprehensive non-linear feature space. Non-linear features are generated\nhierarchically, similarly to deep learning, but have additional flexibility on\nthe possible types of features to be considered. This flexibility, combined\nwith variable selection, allows us to find a small set of important features\nand thereby more interpretable models. A genetically modified Markov chain\nMonte Carlo algorithm is developed to make inference. Model averaging is also\npossible within our framework. In various applications, we illustrate how our\napproach is used to obtain meaningful non-linear models. Additionally, we\ncompare its predictive performance with a number of machine learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:20:55 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Hubin", "Aliaksandr", ""], ["Storvik", "Geir", ""], ["Frommlet", "Florian", ""]]}, {"id": "2003.02930", "submitter": "Fei Zhou", "authors": "Fei Zhou, Jie Ren, Xi Lu, Shuangge Ma, Cen Wu", "title": "Gene-Environment Interaction: A Variable Selection Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene-environment interactions have important implications to elucidate the\ngenetic basis of complex diseases beyond the joint function of multiple genetic\nfactors and their interactions (or epistasis). In the past, G$\\times$E\ninteractions have been mainly conducted within the framework of genetic\nassociation studies. The high dimensionality of G$\\times$E interactions, due to\nthe complicated form of environmental effects and presence of a large number of\ngenetic factors including gene expressions and SNPs, has motivated the recent\ndevelopment of penalized variable selection methods for dissecting G$\\times$E\ninteractions, which has been ignored in majority of published reviews on\ngenetic interaction studies. In this article, we first survey existing\noverviews on both gene-environment and gene-gene interactions. Then, after a\nbrief introduction on the variable selection methods, we review penalization\nand relevant variable selection methods in marginal and joint paradigms\nrespectively under a variety of conceptual models. Discussions on strengths and\nlimitations, as well as computational aspects of the variable selection methods\ntailored for G$\\times$E studies have also been provided.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:21:46 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Zhou", "Fei", ""], ["Ren", "Jie", ""], ["Lu", "Xi", ""], ["Ma", "Shuangge", ""], ["Wu", "Cen", ""]]}, {"id": "2003.02937", "submitter": "Trambak Banerjee", "authors": "Trambak Banerjee, Bhaswar B. Bhattacharya and Gourab Mukherjee", "title": "A Nearest-Neighbor Based Nonparametric Test for Viral Remodeling in\n  Heterogeneous Single-Cell Proteomic Data", "comments": "Final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in contemporary immunology studies based on single-cell\nprotein expression data is to determine whether cellular expressions are\nremodeled post infection by a pathogen. One natural approach for detecting such\nchanges is to use non-parametric two-sample statistical tests. However, in\nsingle-cell studies, direct application of these tests is often inadequate\nbecause single-cell level expression data from uninfected populations often\ncontains attributes of several latent sub-populations with highly heterogeneous\ncharacteristics. As a result, viruses often infect these different\nsub-populations at different rates in which case the traditional nonparametric\ntwo-sample tests for checking similarity in distributions are no longer\nconservative. We propose a new nonparametric method for Testing Remodeling\nUnder Heterogeneity (TRUH) that can accurately detect changes in the infected\nsamples compared to possibly heterogeneous uninfected samples. Our testing\nframework is based on composite nulls and is designed to allow the null model\nto encompass the possibility that the infected samples, though unaltered by the\nvirus, might be dominantly arising from under-represented sub-populations in\nthe baseline data. The TRUH statistic, which uses nearest neighbor projections\nof the infected samples into the baseline uninfected population, is calibrated\nusing a novel bootstrap algorithm. We demonstrate the non-asymptotic\nperformance of the test via simulation experiments and derive the large sample\nlimit of the test statistic, which provides theoretical support towards\nconsistent asymptotic calibration of the test. We use the TRUH statistic for\nstudying remodeling in tonsillar T cells under different types of HIV infection\nand find that unlike traditional tests, TRUH based statistical inference\nconforms to the biologically validated immunological theories on HIV infection.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:37:39 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 18:16:38 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Banerjee", "Trambak", ""], ["Bhattacharya", "Bhaswar B.", ""], ["Mukherjee", "Gourab", ""]]}, {"id": "2003.02938", "submitter": "Brian Vegetabile", "authors": "Brian G. Vegetabile, Beth Ann Griffin, Donna L. Coffman, Matthew\n  Cefalu, Daniel F. McCaffrey", "title": "Nonparametric Estimation of Population Average Dose-Response Curves\n  using Entropy Balancing Weights for Continuous Exposures", "comments": "26 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted estimators are commonly used for estimating exposure effects in\nobservational settings to establish causal relations. These estimators have a\nlong history of development when the exposure of interest is binary and where\nthe weights are typically functions of an estimated propensity score. Recent\ndevelopments in optimization-based estimators for constructing weights in\nbinary exposure settings, such as those based on entropy balancing, have shown\nmore promise in estimating treatment effects than those methods that focus on\nthe direct estimation of the propensity score using likelihood-based methods.\nThis paper explores recent developments of entropy balancing methods to\ncontinuous exposure settings and the estimation of population dose-response\ncurves using nonparametric estimation combined with entropy balancing weights,\nfocusing on factors that would be important to applied researchers in medical\nor health services research. The methods developed here are applied to data\nfrom a study assessing the effect of non-randomized components of an\nevidence-based substance use treatment program on emotional and substance use\nclinical outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:38:46 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Vegetabile", "Brian G.", ""], ["Griffin", "Beth Ann", ""], ["Coffman", "Donna L.", ""], ["Cefalu", "Matthew", ""], ["McCaffrey", "Daniel F.", ""]]}, {"id": "2003.02941", "submitter": "Mickael Albertus", "authors": "Mickael Albertus", "title": "Exponential increase of test power for Z-test and Chi-square test with\n  auxiliary information", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this article is to study how an auxiliary information can be\nused to improve the power of two famous statistical tests: the $ Z$-test and\nthe chi-square test. This information can be of any nature - probability of\nsets of partitions, expectation of a function, ... - and is not even required\nto be an exact information, it can be given by an estimate based on a larger\nsample for example. Some definitions of auxiliary information can be found in\nthe statistical literature and will be recalled. In this article, the notion of\nauxiliary information is discussed here from a very general point of view.\nThese two statistical tests are modified so that the auxiliary information is\ntaken into account. One show in particular that the power of these tests is\nincreased exponentially. Some statistical examples are treated to show the\nconcreteness of this method.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:46:21 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Albertus", "Mickael", ""]]}, {"id": "2003.03004", "submitter": "Hisashi Noma", "authors": "Katsuhiro Iba, Tomohiro Shinozaki, Kazushi Maruo and Hisashi Noma", "title": "Re-evaluation of the comparative effectiveness of bootstrap-based\n  optimism correction methods in the development of multivariable clinical\n  prediction models", "comments": null, "journal-ref": "BMC Med Res Methodol 2021 7;21(1):9", "doi": "10.1186/s12874-020-01201-w", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariable predictive models are important statistical tools for providing\nsynthetic diagnosis and prognostic algorithms based on multiple patients'\ncharacteristics. Their apparent discriminant and calibration measures usually\nhave overestimation biases (known as 'optimism') relative to the actual\nperformances for external populations. Existing statistical evidence and\nguidelines suggest that three bootstrap-based bias correction methods are\npreferable in practice, namely Harrell's bias correction and the .632 and .632+\nestimators. Although Harrell's method has been widely adopted in clinical\nstudies, simulation-based evidence indicates that the .632+ estimator may\nperform better than the other two methods. However, there is limited evidence\nand these methods' actual comparative effectiveness is still unclear. In this\narticle, we conducted extensive simulations to compare the effectiveness of\nthese methods, particularly using the following modern regression models:\nconventional logistic regression, stepwise variable selections, Firth's\npenalized likelihood method, ridge, lasso, and elastic-net. Under relatively\nlarge sample settings, the three bootstrap-based methods were comparable and\nperformed well. However, all three methods had biases under small sample\nsettings, and the directions and sizes of the biases were inconsistent. In\ngeneral, the .632+ estimator is recommended, but we provide several notes\nconcerning the operating characteristics of each method.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 02:23:34 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Iba", "Katsuhiro", ""], ["Shinozaki", "Tomohiro", ""], ["Maruo", "Kazushi", ""], ["Noma", "Hisashi", ""]]}, {"id": "2003.03006", "submitter": "Guanyu Hu", "authors": "Lijiang Geng, Guanyu Hu", "title": "Bayesian Spatial Homogeneity Pursuit for Survival Data with an\n  Application to the SEER Respiratory Cancer Data", "comments": "27 pages, 3 figures", "journal-ref": "Biometrics 2021", "doi": "10.1111/biom.13439", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work, we propose a new Bayesian spatial homogeneity pursuit method\nfor survival data under the proportional hazards model to detect spatially\nclustered patterns in baseline hazard and regression coefficients. Specially,\nregression coefficients and baseline hazard are assumed to have spatial\nhomogeneity pattern over space. To capture such homogeneity, we develop a\ngeographically weighted Chinese restaurant process prior to simultaneously\nestimate coefficients and baseline hazards and their uncertainty measures. An\nefficient Markov chain Monte Carlo (MCMC) algorithm is designed for our\nproposed methods. Performance is evaluated using simulated data, and further\napplied to a real data analysis of respiratory cancer in the state of\nLouisiana.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 02:30:15 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 02:40:09 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Geng", "Lijiang", ""], ["Hu", "Guanyu", ""]]}, {"id": "2003.03042", "submitter": "Jiabei Yang", "authors": "Jiabei Yang, Issa J. Dahabreh and Jon A. Steingrimsson", "title": "Causal Interaction Trees: Tree-Based Subgroup Identification for\n  Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Causal Interaction Trees for identifying subgroups of participants\nthat have enhanced treatment effects using observational data. We extend the\nClassification and Regression Tree algorithm by using splitting criteria that\nfocus on maximizing between-group treatment effect heterogeneity based on\nsubgroup-specific treatment effect estimators to dictate decision-making in the\nalgorithm. We derive properties of three subgroup-specific treatment effect\nestimators that account for the observational nature of the data -- inverse\nprobability weighting, g-formula and doubly robust estimators. We study the\nperformance of the proposed algorithms using simulations and implement the\nalgorithms in an observational study that evaluates the effectiveness of right\nheart catheterization on critically ill patients.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 05:51:27 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Yang", "Jiabei", ""], ["Dahabreh", "Issa J.", ""], ["Steingrimsson", "Jon A.", ""]]}, {"id": "2003.03187", "submitter": "Jouni Helske", "authors": "Jouni Helske, Santtu Tikka, Juha Karvanen", "title": "Estimation of causal effects with small data in the presence of trapdoor\n  variables", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating causal effects of interventions from\nobservational data when well-known back-door and front-door adjustments are not\napplicable. We show that when an identifiable causal effect is subject to an\nimplicit functional constraint that is not deducible from conditional\nindependence relations, the estimator of the causal effect can exhibit bias in\nsmall samples. This bias is related to variables that we call trapdoor\nvariables. We use simulated data to study different strategies to account for\ntrapdoor variables and suggest how the related trapdoor bias might be\nminimized. The importance of trapdoor variables in causal effect estimation is\nillustrated with real data from the Life Course 1971-2002 study. Using this\ndataset, we estimate the causal effect of education on income in the Finnish\ncontext. Bayesian modelling allows us to take the parameter uncertainty into\naccount and to present the estimated causal effects as posterior distributions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 13:28:44 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 17:34:20 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 15:34:04 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Helske", "Jouni", ""], ["Tikka", "Santtu", ""], ["Karvanen", "Juha", ""]]}, {"id": "2003.03299", "submitter": "Youngki Shin", "authors": "Ji Hyung Lee, Youngki Shin", "title": "Complete Subset Averaging for Quantile Regressions", "comments": "46 pages, 3 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a novel conditional quantile prediction method based on complete\nsubset averaging (CSA) for quantile regressions. All models under consideration\nare potentially misspecified and the dimension of regressors goes to infinity\nas the sample size increases. Since we average over the complete subsets, the\nnumber of models is much larger than the usual model averaging method which\nadopts sophisticated weighting schemes. We propose to use an equal weight but\nselect the proper size of the complete subset based on the leave-one-out\ncross-validation method. Building upon the theory of Lu and Su (2015), we\ninvestigate the large sample properties of CSA and show the asymptotic\noptimality in the sense of Li (1987). We check the finite sample performance\nvia Monte Carlo simulations and empirical applications.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 16:23:59 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 02:54:44 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 00:54:16 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Lee", "Ji Hyung", ""], ["Shin", "Youngki", ""]]}, {"id": "2003.03621", "submitter": "Moritz Herrmann", "authors": "Moritz Herrmann, Philipp Probst, Roman Hornung, Vindi Jurinovic,\n  Anne-Laure Boulesteix", "title": "Large-scale benchmark study of survival prediction methods using\n  multi-omics data", "comments": "23 pages, 6 tables, 3 figures", "journal-ref": "Briefings in Bioinformatics (2020) bbaa167", "doi": "10.1093/bib/bbaa167", "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-omics data, that is, datasets containing different types of\nhigh-dimensional molecular variables (often in addition to classical clinical\nvariables), are increasingly generated for the investigation of various\ndiseases. Nevertheless, questions remain regarding the usefulness of\nmulti-omics data for the prediction of disease outcomes such as survival time.\nIt is also unclear which methods are most appropriate to derive such prediction\nmodels. We aim to give some answers to these questions by means of a\nlarge-scale benchmark study using real data. Different prediction methods from\nmachine learning and statistics were applied on 18 multi-omics cancer datasets\nfrom the database \"The Cancer Genome Atlas\", containing from 35 to 1,000\nobservations and from 60,000 to 100,000 variables. The considered outcome was\nthe (censored) survival time. Twelve methods based on boosting, penalized\nregression and random forest were compared, comprising both methods that do and\nthat do not take the group structure of the omics variables into account. The\nKaplan-Meier estimate and a Cox model using only clinical variables were used\nas reference methods. The methods were compared using several repetitions of\n5-fold cross-validation. Uno's C-index and the integrated Brier-score served as\nperformance metrics. The results show that, although multi-omics data can\nimprove the prediction performance, this is not generally the case. Only the\nmethod block forest slightly outperformed the Cox model on average over all\ndatasets. Taking into account the multi-omics structure improves the predictive\nperformance and protects variables in low-dimensional groups - especially\nclinical variables - from not being included in the model. All analyses are\nreproducible using freely available R code.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 18:03:17 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Herrmann", "Moritz", ""], ["Probst", "Philipp", ""], ["Hornung", "Roman", ""], ["Jurinovic", "Vindi", ""], ["Boulesteix", "Anne-Laure", ""]]}, {"id": "2003.03636", "submitter": "Filippo Pagani Mr", "authors": "Simon Cotter and Thomas House and Filippo Pagani", "title": "The NuZZ: Numerical ZigZag Sampling for General Models", "comments": "28 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Numerical ZigZag (NuZZ) algorithm, a Piecewise Deterministic\nMCMC algorithm that is applicable to general statistical models, without the\nneed for bounds on the gradient of the log posterior. This allows us to\ninvestigate: (i) how the ZigZag process behaves on some test problems with\ncommon challenging features; (ii) the performance of NuZZ compared to other\nnumerical approaches to the ZigZag; (iii) the error between the target and\nsampled distributions as a function of computational effort for different MCMC\nalgorithms including the NuZZ. Through a series of test problems we compare the\nmixing of the ZigZag process against other common methods. We present numerical\nevidence and an analytical argument that the Wasserstein distance between the\ntarget distribution and the invariant distribution of the NuZZ process is\nexpected to exhibit asymptotically linearly dependence on the tolerances of\nboth the numerical integration and root finding schemes used. Notably we\npresent a real-life example which demonstrates that NuZZ can outperform not\nonly the super-efficient version of the ZigZag process with thinning, but also\nwell-established methods such as Hamiltonian Monte Carlo.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 18:46:13 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Cotter", "Simon", ""], ["House", "Thomas", ""], ["Pagani", "Filippo", ""]]}, {"id": "2003.03649", "submitter": "Karolos Konstantinos Korkas", "authors": "Karolos K. Korkas", "title": "Ensemble Binary Segmentation for irregularly spaced data with\n  change-points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new technique for consistent estimation of the number and\nlocations of the change-points in the structure of an irregularly spaced time\nseries. The core of the segmentation procedure is the Ensemble Binary\nSegmentation method (EBS), a technique in which a large number of multiple\nchange-point detection tasks using the Binary Segmentation (BS) method are\napplied on sub-samples of the data of differing lengths, and then the results\nare combined to create an overall answer. We do not restrict the total number\nof change-points a time series can have, therefore, our proposed method works\nwell when the spacings between change-points are short. Our main change-point\ndetection statistic is the time-varying Autoregressive Conditional Duration\nmodel on which we apply a transformation process in order to decorrelate it. To\nexamine the performance of EBS we provide a simulation study for various types\nof scenarios. A proof of consistency is also provided. Our methodology is\nimplemented in the R package eNchange, available to download from CRAN.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 19:56:32 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 17:18:45 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 19:50:37 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Korkas", "Karolos K.", ""]]}, {"id": "2003.03668", "submitter": "Richard Samworth", "authors": "Yudong Chen, Tengyao Wang and Richard J. Samworth", "title": "High-dimensional, multiscale online changepoint detection", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for high-dimensional, online changepoint detection\nin settings where a $p$-variate Gaussian data stream may undergo a change in\nmean. The procedure works by performing likelihood ratio tests against simple\nalternatives of different scales in each coordinate, and then aggregating test\nstatistics across scales and coordinates. The algorithm is online in the sense\nthat both its storage requirements and worst-case computational complexity per\nnew observation are independent of the number of previous observations; in\npractice, it may even be significantly faster than this. We prove that the\npatience, or average run length under the null, of our procedure is at least at\nthe desired nominal level, and provide guarantees on its response delay under\nthe alternative that depend on the sparsity of the vector of mean change.\nSimulations confirm the practical effectiveness of our proposal, which is\nimplemented in the R package 'ocd', and we also demonstrate its utility on a\nseismology data set.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 21:54:09 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 15:46:08 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chen", "Yudong", ""], ["Wang", "Tengyao", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2003.03685", "submitter": "Jakob Runge", "authors": "Jakob Runge", "title": "Discovering contemporaneous and lagged causal relations in\n  autocorrelated nonlinear time series datasets", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider causal discovery from time series using conditional independence\n(CI) based network learning algorithms such as the PC algorithm. The PC\nalgorithm is divided into a skeleton phase where adjacencies are determined\nbased on efficiently selected CI tests and subsequent phases where links are\noriented utilizing the Markov and Faithfulness assumptions. Here we show that\nautocorrelation makes the PC algorithm much less reliable with very low\nadjacency and orientation detection rates and inflated false positives. We\npropose a new algorithm, called PCMCI$^+$ that extends the PCMCI method from\n[Runge et al., 2019b] to also include discovery of contemporaneous links. It\nseparates the skeleton phase for lagged and contemporaneous conditioning sets\nand modifies the conditioning sets for the individual CI tests. We show that\nthis algorithm now benefits from increasing autocorrelation and yields much\nmore adjacency detection power and especially more orientation recall for\ncontemporaneous links while controlling false positives and having much shorter\nruntimes. Numerical experiments indicate that the algorithm can be of\nconsiderable use in many application scenarios for dozens of variables and\nlarge time delays.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 23:33:34 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Runge", "Jakob", ""]]}, {"id": "2003.03814", "submitter": "Simo S\\\"arkk\\\"a", "authors": "Jarkko Suuronen, Muhammad Emzir, Sari Lasanen, Simo S\\\"arkk\\\"a, Lassi\n  Roininen", "title": "Enhancing Industrial X-ray Tomography by Data-Centric Statistical\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE eess.IV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray tomography has applications in various industrial fields such as\nsawmill industry, oil and gas industry, chemical engineering, and geotechnical\nengineering. In this article, we study Bayesian methods for the X-ray\ntomography reconstruction. In Bayesian methods, the inverse problem of\ntomographic reconstruction is solved with help of a statistical prior\ndistribution which encodes the possible internal structures by assigning\nprobabilities for smoothness and edge distribution of the object. We compare\nGaussian random field priors, that favour smoothness, to non-Gaussian total\nvariation, Besov, and Cauchy priors which promote sharp edges and high-contrast\nand low-contrast areas in the object. We also present computational schemes for\nsolving the resulting high-dimensional Bayesian inverse problem with\n100,000-1,000,000 unknowns. In particular, we study the applicability of a\nno-U-turn variant of Hamiltonian Monte Carlo methods and of a more classical\nadaptive Metropolis-within-Gibbs algorithm for this purpose. These methods also\nenable full uncertainty quantification of the reconstructions. For faster\ncomputations, we use maximum a posteriori estimates with limited-memory BFGS\noptimisation algorithm. As the first industrial application, we consider\nsawmill industry X-ray log tomography. The logs have knots, rotten parts, and\neven possibly metallic pieces, making them good examples for non-Gaussian\npriors. Secondly, we study drill-core rock sample tomography, an example from\noil and gas industry. We show that Cauchy priors produce smaller number of\nartefacts than other choices, especially with sparse high-noise measurements,\nand choosing Hamiltonian Monte Carlo enables systematic uncertainty\nquantification.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 17:22:58 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Suuronen", "Jarkko", ""], ["Emzir", "Muhammad", ""], ["Lasanen", "Sari", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Roininen", "Lassi", ""]]}, {"id": "2003.03880", "submitter": "Achmad Choiruddin", "authors": "Achmad Choiruddin, Jean-Fran\\c{c}ois Coeurjolly, Rasmus Waagepetersen", "title": "Information criteria for inhomogeneous spatial point processes", "comments": "6 figures", "journal-ref": null, "doi": "10.1088/1742-6596/1752/1/012015", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical foundation for a number of model selection criteria is\nestablished in the context of inhomogeneous point processes and under various\nasymptotic settings: infill, increasing domain, and combinations of these. For\ninhomogeneous Poisson processes we consider Akaike information criterion and\nthe Bayesian information criterion, and in particular we identify the point\nprocess analogue of sample size needed for the Bayesian information criterion.\nConsidering general inhomogeneous point processes we derive new composite\nlikelihood and composite Bayesian information criteria for selecting a\nregression model for the intensity function. The proposed model selection\ncriteria are evaluated using simulations of Poisson processes and cluster point\nprocesses.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 01:45:04 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Choiruddin", "Achmad", ""], ["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Waagepetersen", "Rasmus", ""]]}, {"id": "2003.03881", "submitter": "Zijun Gao", "authors": "Zijun Gao, Trevor Hastie, Robert Tibshirani", "title": "Assessment of Heterogeneous Treatment Effect Estimation Accuracy via\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the assessment of the accuracy of heterogeneous treatment effect\n(HTE) estimation, where the HTE is not directly observable so standard\ncomputation of prediction errors is not applicable. To tackle the difficulty,\nwe propose an assessment approach by constructing pseudo-observations of the\nHTE based on matching. Our contributions are three-fold: first, we introduce a\nnovel matching distance derived from proximity scores in random forests;\nsecond, we formulate the matching problem as an average minimum-cost flow\nproblem and provide an efficient algorithm; third, we propose a\nmatch-then-split principle for the assessment with cross-validation. We\ndemonstrate the efficacy of the assessment approach on synthetic data and data\ngenerated from a real dataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 01:50:15 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Gao", "Zijun", ""], ["Hastie", "Trevor", ""], ["Tibshirani", "Robert", ""]]}, {"id": "2003.03886", "submitter": "Ryan Tibshirani", "authors": "Ryan J. Tibshirani", "title": "Divided Differences, Falling Factorials, and Discrete Splines: Another\n  Look at Trend Filtering and Related Problems", "comments": "74 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews a class of univariate piecewise polynomial functions known\nas discrete splines, which share properties analogous to the better-known class\nof spline functions, but where continuity in derivatives is replaced by (a\nsuitable notion of) continuity in divided differences. As it happens, discrete\nsplines bear connections to a wide array of developments in applied mathematics\nand statistics, from divided differences and Newton interpolation (dating back\nto over 300 years ago) to trend filtering (from the last 15 years). We survey\nthese connections, and contribute some new perspectives and new results along\nthe way.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 02:02:03 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 16:04:42 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Tibshirani", "Ryan J.", ""]]}, {"id": "2003.03887", "submitter": "Oliver Cliff", "authors": "Oliver M. Cliff, Leonardo Novelli, Ben D. Fulcher, James M. Shine and\n  Joseph T. Lizier", "title": "Assessing the Significance of Directed and Multivariate Measures of\n  Linear Dependence Between Time Series", "comments": "27 pages, 14 figures, final submission to Phys Rev. Research editors\n  (before copyediting)", "journal-ref": "Phys. Rev. Research 3, 013145 (2021)", "doi": "10.1103/PhysRevResearch.3.013145", "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST physics.data-an q-bio.NC stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring linear dependence between time series is central to our\nunderstanding of natural and artificial systems. Unfortunately, the hypothesis\ntests that are used to determine statistically significant directed or\nmultivariate relationships from time-series data often yield spurious\nassociations (Type I errors) or omit causal relationships (Type II errors).\nThis is due to the autocorrelation present in the analysed time series -- a\nproperty that is ubiquitous across diverse applications, from brain dynamics to\nclimate change. Here we show that, for limited data, this issue cannot be\nmediated by fitting a time-series model alone (e.g., in Granger causality or\nprewhitening approaches), and instead that the degrees of freedom in\nstatistical tests should be altered to account for the effective sample size\ninduced by cross-correlations in the observations. This insight enabled us to\nderive modified hypothesis tests for any multivariate correlation-based\nmeasures of linear dependence between covariance-stationary time series,\nincluding Granger causality and mutual information with Gaussian marginals. We\nuse both numerical simulations (generated by autoregressive models and digital\nfiltering) as well as recorded fMRI-neuroimaging data to show that our tests\nare unbiased for a variety of stationary time series. Our experiments\ndemonstrate that the commonly used $F$- and $\\chi^2$-tests can induce\nsignificant false-positive rates of up to $100\\%$ for both measures, with and\nwithout prewhitening of the signals. These findings suggest that many\ndependencies reported in the scientific literature may have been, and may\ncontinue to be, spuriously reported or missed if modified hypothesis tests are\nnot used when analysing time series.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 02:06:01 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 11:02:18 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 10:13:03 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Cliff", "Oliver M.", ""], ["Novelli", "Leonardo", ""], ["Fulcher", "Ben D.", ""], ["Shine", "James M.", ""], ["Lizier", "Joseph T.", ""]]}, {"id": "2003.03915", "submitter": "Takashi Goda", "authors": "Josef Dick, Takashi Goda, Hiroya Murata", "title": "Toeplitz Monte Carlo", "comments": null, "journal-ref": "Statistics and Computing, Volume 31, Article number 1, 2021", "doi": "10.1007/s11222-020-09987-x", "report-no": null, "categories": "math.NA cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated mainly by applications to partial differential equations with\nrandom coefficients, we introduce a new class of Monte Carlo estimators, called\nToeplitz Monte Carlo (TMC) estimator for approximating the integral of a\nmultivariate function with respect to the direct product of an identical\nunivariate probability measure. The TMC estimator generates a sequence\n$x_1,x_2,\\ldots$ of i.i.d. samples for one random variable, and then uses\n$(x_{n+s-1},x_{n+s-2}\\ldots,x_n)$ with $n=1,2,\\ldots$ as quadrature points,\nwhere $s$ denotes the dimension. Although consecutive points have some\ndependency, the concatenation of all quadrature nodes is represented by a\nToeplitz matrix, which allows for a fast matrix-vector multiplication. In this\npaper we study the variance of the TMC estimator and its dependence on the\ndimension $s$. Numerical experiments confirm the considerable efficiency\nimprovement over the standard Monte Carlo estimator for applications to partial\ndifferential equations with random coefficients, particularly when the\ndimension $s$ is large.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 03:59:32 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 14:35:31 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Dick", "Josef", ""], ["Goda", "Takashi", ""], ["Murata", "Hiroya", ""]]}, {"id": "2003.03948", "submitter": "You-Gan Wang", "authors": "Liya Fu, Zhuoran Yang, Yan Zhou and You-Gan Wang", "title": "An efficient Gehan-type estimation for the accelerated failure time\n  model with clustered and censored data", "comments": "ready for submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical studies, the collected covariates usually contain underlying\noutliers. For clustered /longitudinal data with censored observations, the\ntraditional Gehan-type estimator is robust to outliers existing in response but\nsensitive to outliers in the covariate domain, and it also ignores the\nwithin-cluster correlations. To take account of within-cluster correlations,\nvarying cluster sizes, and outliers in covariates, we propose weighted\nGehan-type estimating functions for parameter estimation in the accelerated\nfailure time model for clustered data. We provide the asymptotic properties of\nthe resulting estimators and carry out simulation studies to evaluate the\nperformance of the proposed method under a variety of realistic settings. The\nsimulation results demonstrate that the proposed method is robust to the\noutliers existing in the covariate domain and lead to much more efficient\nestimators when a strong within-cluster correlation exists. Finally, the\nproposed method is applied to a medical dataset and more reliable and\nconvincing results are hence obtained.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 07:04:17 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Fu", "Liya", ""], ["Yang", "Zhuoran", ""], ["Zhou", "Yan", ""], ["Wang", "You-Gan", ""]]}, {"id": "2003.04067", "submitter": "Ben Youngman", "authors": "Benjamin D. Youngman", "title": "evgam: An R package for Generalized Additive Extreme Value Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces the R package evgam. The package provides functions\nfor fitting extreme value distributions. These include the generalized extreme\nvalue and generalized Pareto distributions. The former can also be fitted\nthrough a point process representation. evgam supports quantile regression via\nthe asymmetric Laplace distribution, which can be useful for estimating high\nthresholds, sometimes used to discriminate between extreme and non-extreme\nvalues. The main addition of evgam is to let extreme value distribution\nparameters have generalized additive model forms, which can be objectively\nestimated using Laplace's method. Illustrative examples fitting various\ndistributions with various specifications are given. These include daily\nprecipitation accumulations for part of Colorado, US, used to illustrate\nspatial models, and daily maximum temperatures for Fort Collins, Colorado, US,\nused to illustrate temporal models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 12:31:55 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 17:27:33 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 16:31:26 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Youngman", "Benjamin D.", ""]]}, {"id": "2003.04125", "submitter": "Ayman Boustati", "authors": "Ayman Boustati, Sattar Vakili, James Hensman, ST John", "title": "Amortized variance reduction for doubly stochastic objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate inference in complex probabilistic models such as deep Gaussian\nprocesses requires the optimisation of doubly stochastic objective functions.\nThese objectives incorporate randomness both from mini-batch subsampling of the\ndata and from Monte Carlo estimation of expectations. If the gradient variance\nis high, the stochastic optimisation problem becomes difficult with a slow rate\nof convergence. Control variates can be used to reduce the variance, but past\napproaches do not take into account how mini-batch stochasticity affects\nsampling stochasticity, resulting in sub-optimal variance reduction. We propose\na new approach in which we use a recognition network to cheaply approximate the\noptimal control variate for each mini-batch, with no additional model gradient\ncomputations. We illustrate the properties of this proposal and test its\nperformance on logistic regression and deep Gaussian processes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 13:23:14 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Boustati", "Ayman", ""], ["Vakili", "Sattar", ""], ["Hensman", "James", ""], ["John", "ST", ""]]}, {"id": "2003.04235", "submitter": "Ali Shojaie", "authors": "Ali Shojaie", "title": "Differential Network Analysis: A Statistical Perspective", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks effectively capture interactions among components of complex\nsystems, and have thus become a mainstay in many scientific disciplines.\nGrowing evidence, especially from biology, suggest that networks undergo\nchanges over time, and in response to external stimuli. In biology and\nmedicine, these changes have been found to be predictive of complex diseases.\nThey have also been used to gain insight into mechanisms of disease initiation\nand progression. Primarily motivated by biological applications, this article\nprovides a review of recent statistical machine learning methods for inferring\nnetworks and identifying changes in their structures.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 16:19:28 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Shojaie", "Ali", ""]]}, {"id": "2003.04238", "submitter": "Thomas Stringham", "authors": "Thomas Stringham", "title": "Fast Bayesian Record Linkage With Record-Specific Disagreement\n  Parameters", "comments": null, "journal-ref": null, "doi": "10.1080/07350015.2021.1934478", "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers are often interested in linking individuals between two datasets\nthat lack a common unique identifier. Matching procedures often struggle to\nmatch records with common names, birthplaces or other field values.\nComputational feasibility is also a challenge, particularly when linking large\ndatasets. We develop a Bayesian method for automated probabilistic record\nlinkage and show it recovers more than 50% more true matches, holding accuracy\nconstant, than comparable methods in a matching of military recruitment data to\nthe 1900 US Census for which expert-labelled matches are available. Our\napproach, which builds on a recent state-of-the-art Bayesian method, refines\nthe modelling of comparison data, allowing disagreement probability parameters\nconditional on non-match status to be record-specific in the smaller of the two\ndatasets. This flexibility significantly improves matching when many records\nshare common field values. We show that our method is computationally feasible\nin practice, despite the added complexity, with an R/C++ implementation that\nachieves significant improvement in speed over comparable recent methods. We\nalso suggest a lightweight method for treatment of very common names and show\nhow to estimate true positive rate and positive predictive value when true\nmatch status is unavailable.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 16:23:54 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 13:12:53 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Stringham", "Thomas", ""]]}, {"id": "2003.04314", "submitter": "Felix Cheysson", "authors": "Felix Cheysson (1, 2 and 3), Gabriel Lang (1) ((1) Universit\\'e\n  Paris-Saclay, AgroParisTech, INRAE, UMR MIA-Paris, Paris, France, (2)\n  Epidemiology and Modeling of bacterial Evasion to Antibacterials Unit (EMEA),\n  Institut Pasteur, Paris, France, (3) Anti-infective Evasion and\n  Pharmacoepidemiology Team, Centre for Epidemiology and Public health (CESP),\n  Inserm / UVSQ, France)", "title": "Strong mixing condition for Hawkes processes and application to Whittle\n  estimation from count data", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the time series generated by the event counts of\nstationary Hawkes processes. When the exact locations of points are not\nobserved, but only counts over time intervals of fixed size, existing methods\nof estimation are not applicable. We first establish a strong mixing condition\nwith polynomial decay rate for Hawkes processes, from their Poisson cluster\nstructure. This allows us to propose a spectral approach to the estimation of\nHawkes processes, based on Whittle's method, which provides consistent and\nasymptotically normal estimates under common regularity conditions on their\nreproduction kernels. Simulated datasets and a case-study illustrate the\nperformances of the estimation, notably of the Hawkes reproduction mean and\nkernel when time intervals are relatively large.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:37:28 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Cheysson", "Felix", "", "1, 2 and 3"], ["Lang", "Gabriel", ""]]}, {"id": "2003.04433", "submitter": "Somabha Mukherjee", "authors": "Somabha Mukherjee, Rohit K. Patra, Andrew L. Johnson, Hiroshi Morita", "title": "Least Squares Estimation of a Monotone Quasiconvex Regression Function", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach for the estimation of a multivariate function based\non the economic axioms of monotonicity and quasiconvexity. We prove the\nexistence of the nonparametric least squares estimator (LSE) for a monotone and\nquasiconvex function and provide two characterizations for it. One of these\ncharacterizations is useful from the theoretical point of view, while the other\nhelps in the computation of the estimator. We show that the LSE is almost\nsurely unique and is the solution to a mixed-integer quadratic optimization\nproblem. We prove consistency and find finite sample risk bounds for the LSE\nunder both fixed lattice and random design settings for the covariates. We\nillustrate the superior performance of the LSE against existing estimators via\nsimulation. Finally, we use the LSE to estimate the production function for the\nJapanese plywood industry and the cost function for hospitals across the US.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 22:16:57 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Mukherjee", "Somabha", ""], ["Patra", "Rohit K.", ""], ["Johnson", "Andrew L.", ""], ["Morita", "Hiroshi", ""]]}, {"id": "2003.04493", "submitter": "Qinqing Zheng", "authors": "Qinqing Zheng, Jinshuo Dong, Qi Long, Weijie J. Su", "title": "Sharp Composition Bounds for Gaussian Differential Privacy via Edgeworth\n  Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CR cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets containing sensitive information are often sequentially analyzed by\nmany algorithms. This raises a fundamental question in differential privacy\nregarding how the overall privacy bound degrades under composition. To address\nthis question, we introduce a family of analytical and sharp privacy bounds\nunder composition using the Edgeworth expansion in the framework of the\nrecently proposed f-differential privacy. In contrast to the existing\ncomposition theorems using the central limit theorem, our new privacy bounds\nunder composition gain improved tightness by leveraging the refined\napproximation accuracy of the Edgeworth expansion. Our approach is easy to\nimplement and computationally efficient for any number of compositions. The\nsuperiority of these new bounds is confirmed by an asymptotic error analysis\nand an application to quantifying the overall privacy guarantees of noisy\nstochastic gradient descent used in training private deep neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 01:54:15 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 15:18:11 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Zheng", "Qinqing", ""], ["Dong", "Jinshuo", ""], ["Long", "Qi", ""], ["Su", "Weijie J.", ""]]}, {"id": "2003.04598", "submitter": "Hisashi Noma", "authors": "Hisashi Noma, Kengo Nagashima, Shogo Kato, Satoshi Teramukai and Toshi\n  A. Furukawa", "title": "Flexible random-effects distribution models for meta-analysis", "comments": null, "journal-ref": "J Epidemiol. 2021", "doi": "10.2188/jea.JE20200376", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In meta-analysis, the random-effects models are standard tools to address\nbetween-study heterogeneity in evidence synthesis analyses. For the\nrandom-effects distribution models, the normal distribution model has been\nadopted in most systematic reviews due to its computational and conceptual\nsimplicity. However, the restrictive model assumption might have serious\ninfluences on the overall conclusions in practices. In this article, we first\nprovide two examples of real-world evidence that clearly show that the normal\ndistribution assumption is unsuitable. To address the model restriction\nproblem, we propose alternative flexible random-effects models that can\nflexibly regulate skewness, kurtosis and tailweight: skew normal distribution,\nskew t-distribution, asymmetric Subbotin distribution, Jones-Faddy\ndistribution, and sinh-arcsinh distribution. We also developed a R package,\nflexmeta, that can easily perform these methods. Using the flexible\nrandom-effects distribution models, the results of the two meta-analyses were\nmarkedly altered, potentially influencing the overall conclusions of these\nsystematic reviews. The flexible methods and computational tools can provide\nmore precise evidence, and these methods would be recommended at least as\nsensitivity analysis tools to assess the influence of the normal distribution\nassumption of the random-effects model.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 09:25:55 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 18:21:14 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 04:09:37 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Noma", "Hisashi", ""], ["Nagashima", "Kengo", ""], ["Kato", "Shogo", ""], ["Teramukai", "Satoshi", ""], ["Furukawa", "Toshi A.", ""]]}, {"id": "2003.04636", "submitter": "Tiejun Tong", "authors": "Zongliang Hu and Tiejun Tong and Marc G. Genton", "title": "A Pairwise Hotelling Method for Testing High-Dimensional Mean Vectors", "comments": "66 pages and 6 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For high-dimensional small sample size data, Hotelling's T2 test is not\napplicable for testing mean vectors due to the singularity problem in the\nsample covariance matrix. To overcome the problem, there are three main\napproaches in the literature. Note, however, that each of the existing\napproaches may have serious limitations and only works well in certain\nsituations. Inspired by this, we propose a pairwise Hotelling method for\ntesting high-dimensional mean vectors, which, in essence, provides a good\nbalance between the existing approaches. To effectively utilize the correlation\ninformation, we construct the new test statistics as the summation of\nHotelling's test statistics for the covariate pairs with strong correlations\nand the squared $t$ statistics for the individual covariates that have little\ncorrelation with others. We further derive the asymptotic null distributions\nand power functions for the proposed Hotelling tests under some regularity\nconditions. Numerical results show that our new tests are able to control the\ntype I error rates, and can achieve a higher statistical power compared to\nexisting methods, especially when the covariates are highly correlated. Two\nreal data examples are also analyzed and they both demonstrate the efficacy of\nour pairwise Hotelling tests.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 11:15:36 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Hu", "Zongliang", ""], ["Tong", "Tiejun", ""], ["Genton", "Marc G.", ""]]}, {"id": "2003.04786", "submitter": "Kun Chen", "authors": "Xiaokang Liu, Shujie Ma, Kun Chen", "title": "Multivariate Functional Regression via Nested Reduced-Rank\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nested reduced-rank regression (NRRR) approach in fitting\nregression model with multivariate functional responses and predictors, to\nachieve tailored dimension reduction and facilitate\ninterpretation/visualization of the resulting functional model. Our approach is\nbased on a two-level low-rank structure imposed on the functional regression\nsurfaces. A global low-rank structure identifies a small set of latent\nprincipal functional responses and predictors that drives the underlying\nregression association. A local low-rank structure then controls the complexity\nand smoothness of the association between the principal functional responses\nand predictors. Through a basis expansion approach, the functional problem\nboils down to an interesting integrated matrix approximation task, where the\nblocks or submatrices of an integrated low-rank matrix share some common row\nspace and/or column space. An iterative algorithm with convergence guarantee is\ndeveloped. We establish the consistency of NRRR and also show through\nnon-asymptotic analysis that it can achieve at least a comparable error rate to\nthat of the reduced-rank regression. Simulation studies demonstrate the\neffectiveness of NRRR. We apply NRRR in an electricity demand problem, to\nrelate the trajectories of the daily electricity consumption with those of the\ndaily temperatures.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 14:58:54 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Liu", "Xiaokang", ""], ["Ma", "Shujie", ""], ["Chen", "Kun", ""]]}, {"id": "2003.04787", "submitter": "Kun Chen", "authors": "Yan Li, Chun Yu, Yize Zhao, Robert H. Aseltine, Weixin Yao, Kun Chen", "title": "Pursuing Sources of Heterogeneity in Modeling Clustered Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often have to deal with heterogeneous population with mixed\nregression relationships, increasingly so in the era of data explosion. In such\nproblems, when there are many candidate predictors, it is not only of interest\nto identify the predictors that are associated with the outcome, but also to\ndistinguish the true sources of heterogeneity, i.e., to identify the predictors\nthat have different effects among the clusters and thus are the true\ncontributors to the formation of the clusters. We clarify the concepts of the\nsource of heterogeneity that account for potential scale differences of the\nclusters and propose a regularized finite mixture effects regression to achieve\nheterogeneity pursuit and feature selection simultaneously. As the name\nsuggests, the problem is formulated under an effects-model parameterization, in\nwhich the cluster labels are missing and the effect of each predictor on the\noutcome is decomposed to a common effect term and a set of cluster-specific\nterms. A constrained sparse estimation of these effects leads to the\nidentification of both the variables with common effects and those with\nheterogeneous effects. We propose an efficient algorithm and show that our\napproach can achieve both estimation and selection consistency. Simulation\nstudies further demonstrate the effectiveness of our method under various\npractical scenarios. Three applications are presented, namely, an imaging\ngenetics study for linking genetic factors and brain neuroimaging traits in\nAlzheimer's disease, a public health study for exploring the association\nbetween suicide risk among adolescents and their school district\ncharacteristics, and a sport analytics study for understanding how the salary\nlevels of baseball players are associated with their performance and\ncontractual status.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 14:59:35 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 20:03:13 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Li", "Yan", ""], ["Yu", "Chun", ""], ["Zhao", "Yize", ""], ["Aseltine", "Robert H.", ""], ["Yao", "Weixin", ""], ["Chen", "Kun", ""]]}, {"id": "2003.04854", "submitter": "Ian Shrier", "authors": "Ian Shrier and Etsuji Suzuki", "title": "A different perspective of cross-world independence assumption and the\n  utility of natural effects versus controlled effects", "comments": "7 pages, 2 figures, no tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The pure effects described by Robins and Greenland, and later called natural\neffects by Pearl, have been criticized because they require a cross-world\nindependence assumption. In this paper, we use potential outcomes and\nsufficient causal sets to present a conceptual perspective of the cross-world\nindependence assumption that explains why the clinical utility of natural\neffects is sometimes greater than that of controlled effects. Our perspective\nis consistent with recent work on mediation of natural effects, path specific\neffects and separable effects.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 17:00:25 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Shrier", "Ian", ""], ["Suzuki", "Etsuji", ""]]}, {"id": "2003.04896", "submitter": "Kody Law", "authors": "Ajay Jasra, Kody J. H. Law, Deng Lu", "title": "Unbiased Estimation of the Gradient of the Log-Likelihood in Inverse\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a parameter associated to a Bayesian\ninverse problem. Treating the unknown initial condition as a nuisance\nparameter, typically one must resort to a numerical approximation of gradient\nof the log-likelihood and also adopt a discretization of the problem in space\nand/or time. We develop a new methodology to unbiasedly estimate the gradient\nof the log-likelihood with respect to the unknown parameter, i.e. the\nexpectation of the estimate has no discretization bias. Such a property is not\nonly useful for estimation in terms of the original stochastic model of\ninterest, but can be used in stochastic gradient algorithms which benefit from\nunbiased estimates. Under appropriate assumptions, we prove that our estimator\nis not only unbiased but of finite variance. In addition, when implemented on a\nsingle processor, we show that the cost to achieve a given level of error is\ncomparable to multilevel Monte Carlo methods, both practically and\ntheoretically. However, the new algorithm provides the possibility for parallel\ncomputation on arbitrarily many processors without any loss of efficiency,\nasymptotically. In practice, this means any precision can be achieved in a\nfixed, finite constant time, provided that enough processors are available.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 12:07:00 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 12:33:03 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Jasra", "Ajay", ""], ["Law", "Kody J. H.", ""], ["Lu", "Deng", ""]]}, {"id": "2003.05092", "submitter": "Xiaohuan Xue", "authors": "Xiaohuan Xue", "title": "Estimation of within-study covariances in multivariate meta-analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate meta-analysis can be adapted to a wide range of situations for\nmultiple outcomes and multiple treatment groups when combining studies\ntogether. The within-study correlation between effect sizes is often assumed\nknown in multivariate meta-analysis while it is not always known practically.\nIn this paper, we propose a generic method to approximate the within-study\ncovariance for effect sizes in multivariate meta-analysis and apply this method\nto the scenarios with multiple outcomes and one outcome with multiple treatment\ngroups respectively.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 03:13:23 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Xue", "Xiaohuan", ""]]}, {"id": "2003.05095", "submitter": "Zurab Kakushadze", "authors": "Zura Kakushadze and Willie Yu", "title": "Machine Learning Treasury Yields", "comments": "68 pages", "journal-ref": "Bulletin of Applied Economics 7(1) (2020) 1-65", "doi": null, "report-no": null, "categories": "stat.ME q-fin.CP q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give explicit algorithms and source code for extracting factors underlying\nTreasury yields using (unsupervised) machine learning (ML) techniques, such as\nnonnegative matrix factorization (NMF) and (statistically deterministic)\nclustering. NMF is a popular ML algorithm (used in computer vision,\nbioinformatics/computational biology, document classification, etc.), but is\noften misconstrued and misused. We discuss how to properly apply NMF to\nTreasury yields. We analyze the factors based on NMF and clustering and their\ninterpretation. We discuss their implications for forecasting Treasury yields\nin the context of out-of-sample ML stability issues.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 03:15:18 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Kakushadze", "Zura", ""], ["Yu", "Willie", ""]]}, {"id": "2003.05157", "submitter": "Wagner Barreto-Souza", "authors": "Wagner Barreto-Souza, Vin\\'icius D. Mayrink and Alexandre B. Simas", "title": "Bessel regression model: Robustness to analyze bounded data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beta regression has been extensively used by statisticians and practitioners\nto model bounded continuous data and there is no strong and similar competitor\nhaving its main features. A class of normalized inverse-Gaussian (N-IG) process\nwas introduced in the literature, being explored in the Bayesian context as a\npowerful alternative to the Dirichlet process. Until this moment, no attention\nhas been paid for the univariate N-IG distribution in the classical inference.\nIn this paper, we propose the bessel regression based on the univariate N-IG\ndistribution, which is a robust alternative to the beta model. This robustness\nis illustrated through simulated and real data applications. The estimation of\nthe parameters is done through an Expectation-Maximization algorithm and the\npaper discusses how to perform inference. A useful and practical discrimination\nprocedure is proposed for model selection between bessel and beta regressions.\nMonte Carlo simulation results are presented to verify the finite-sample\nbehavior of the EM-based estimators and the discrimination procedure. Further,\nthe performances of the regressions are evaluated under misspecification, which\nis a critical point showing the robustness of the proposed model. Finally,\nthree empirical illustrations are explored to confront results from bessel and\nbeta regressions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 08:30:26 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Barreto-Souza", "Wagner", ""], ["Mayrink", "Vin\u00edcius D.", ""], ["Simas", "Alexandre B.", ""]]}, {"id": "2003.05221", "submitter": "Savi Virolainen", "authors": "Savi Virolainen", "title": "A mixture autoregressive model based on Gaussian and Student's\n  $t$-distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new mixture autoregressive model which combines Gaussian and\nStudent's $t$ mixture components. The model has very attractive properties\nanalogous to the Gaussian and Student's $t$ mixture autoregressive models, but\nit is more flexible as it enables to model series which consist of both\nconditionally homoscedastic Gaussian regimes and conditionally heteroscedastic\nStudent's $t$ regimes. The usefulness of our model is demonstrated in an\nempirical application to the monthly U.S. interest rate spread between the\n3-month Treasury bill rate and the effective federal funds rate.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 11:16:36 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 12:16:52 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 15:16:44 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Virolainen", "Savi", ""]]}, {"id": "2003.05228", "submitter": "Nico M. Temme", "authors": "Swaine L. Chen and Nico M. Temme", "title": "A faster and more accurate algorithm for calculating population genetics\n  statistics requiring sums of Stirling numbers of the first kind", "comments": "17 pages, 8 figures", "journal-ref": "G3: GENES, GENOMES, GENETIC, Volume 10 Issue 9, September 2020", "doi": "10.1534/g3.120.401575", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stirling numbers of the first kind are used in the derivation of several\npopulation genetics statistics, which in turn are useful for testing\nevolutionary hypotheses directly from DNA sequences. Here, we explore the\ncumulative distribution function of these Stirling numbers, which enables a\nsingle direct estimate of the sum, using representations in terms of the\nincomplete beta function. This estimator enables an improved method for\ncalculating an asymptotic estimate for one useful statistic, Fu's $F_s$. By\nreducing the calculation from a sum of terms involving Stirling numbers to a\nsingle estimate, we simultaneously improve accuracy and dramatically increase\nspeed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 11:44:30 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 17:02:24 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 17:36:45 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Chen", "Swaine L.", ""], ["Temme", "Nico M.", ""]]}, {"id": "2003.05355", "submitter": "Igor Mikol\\'a\\v{s}ek", "authors": "Igor Mikolasek", "title": "New stochastic highway capacity estimation method and why product limit\n  method is unsuitable", "comments": "22 pages, 7 figures, 7 tables. Originally submitted to Transportation\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kaplan-Meier estimate, commonly known as product limit method (PLM), and\nmaximum likelihood estimate (MLE) methods in general are often cited as means\nof stochastic highway capacity estimation. This article discusses their\nunsuitability for such application as properties of traffic flow do not meet\nthe assumptions for use of the methods. They assume the observed subject has a\nhistory which it went through and did not fail. However, due to its nature,\neach traffic flow measurement behaves as a separate subject which did not go\nthrough all the lower levels of intensity (did not \"age\"). An alternative\nmethod is proposed. It fits the resulting cumulative frequency of breakdowns\nwith respect to the traffic flow intensity leading to the breakdown instead of\ndirectly estimating the underlying probability distribution of capacity.\nAnalyses of accuracy and sensitivity to data quantity and censoring rate of the\nnew method are provided along with comparison to the PLM. The results prove\nunsuitability of the PLM and MLE methods in general. The new method is then\nused in a case study which compares capacity of a work-zone with and without a\ntraffic flow speed harmonisation system installed. The results confirm positive\neffect of harmonisation on capacity.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 15:25:58 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Mikolasek", "Igor", ""]]}, {"id": "2003.05492", "submitter": "Philippe Gagnon", "authors": "Philippe Gagnon and Florian Maire", "title": "An asymptotic Peskun ordering and its application to lifted samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Peskun ordering between two samplers, implying a dominance of one over the\nother, is known among the Markov chain Monte Carlo community for being a\nremarkably strong result, but it is also known for being one that is notably\ndifficult to establish. Indeed, one has to prove that the probability to reach\na state, using a sampler, is greater than or equal to the probability using the\nother sampler, and this must hold for all states excepting the current state.\nWe provide in this paper a weaker version that does not require an inequality\nbetween the probabilities for all these states: the dominance holds\nasymptotically, as a varying parameter grows without bound, as long as the\nstates for which the probabilities are greater than or equal to belong to a\nmass-concentrating set. The weak ordering turns out to be useful to compare\nlifted samplers for partially-ordered discrete state-spaces with their\nMetropolis-Hastings counterparts. An analysis yields a qualitative conclusion:\nthey asymptotically perform better in certain situations (and we are able to\nidentify these situations), but not necessarily in others (and the reasons why\nare made clear). The difference in performance is evaluated quantitatively in\nimportant applications such as graphical model simulation and variable\nselection. The code to reproduce all numerical experiments is available online.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 19:15:47 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 15:45:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gagnon", "Philippe", ""], ["Maire", "Florian", ""]]}, {"id": "2003.05568", "submitter": "Yanqing Zhang", "authors": "Yanqing Zhang, Xuan Bi, Niansheng Tang and Annie Qu", "title": "Dynamic Tensor Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have been extensively used by the entertainment industry,\nbusiness marketing and the biomedical industry. In addition to its capacity of\nproviding preference-based recommendations as an unsupervised learning\nmethodology, it has been also proven useful in sales forecasting, product\nintroduction and other production related businesses. Since some consumers and\ncompanies need a recommendation or prediction for future budget, labor and\nsupply chain coordination, dynamic recommender systems for precise forecasting\nhave become extremely necessary. In this article, we propose a new\nrecommendation method, namely the dynamic tensor recommender system (DTRS),\nwhich aims particularly at forecasting future recommendation. The proposed\nmethod utilizes a tensor-valued function of time to integrate time and\ncontextual information, and creates a time-varying coefficient model for\ntemporal tensor factorization through a polynomial spline approximation. Major\nadvantages of the proposed method include competitive future recommendation\npredictions and effective prediction interval estimations. In theory, we\nestablish the convergence rate of the proposed tensor factorization and\nasymptotic normality of the spline coefficient estimator. The proposed method\nis applied to simulations and IRI marketing data. Numerical studies demonstrate\nthat the proposed method outperforms existing methods in terms of future time\nforecasting.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 01:50:42 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Zhang", "Yanqing", ""], ["Bi", "Xuan", ""], ["Tang", "Niansheng", ""], ["Qu", "Annie", ""]]}, {"id": "2003.05611", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Hisashi Noma", "title": "Efficient testing and effect size estimation for set-based genetic\n  association inference via semiparametric multilevel mixture modeling:\n  Application to a genome-wide association study of coronary artery disease", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genetic association studies, rare variants with extremely small allele\nfrequency play a crucial role in complex traits, and the set-based testing\nmethods that jointly assess the effects of groups of single nucleotide\npolymorphisms (SNPs) were developed to improve powers for the association\ntests. However, the powers of these tests are still severely limited due to the\nextremely small allele frequency, and precise estimations for the effect sizes\nof individual SNPs are substantially impossible. In this article, we provide an\nefficient set-based inference framework that addresses the two important issues\nsimultaneously based on a Bayesian semiparametric multilevel mixture model. We\npropose to use the multilevel hierarchical model that incorporate the\nvariations in set-specific effects and variant-specific effects, and to apply\nthe optimal discovery procedure (ODP) that achieves the largest overall power\nin multiple significance testing. In addition, we provide Bayesian optimal\n\"set-based\" estimator of the empirical distribution of effect sizes. Efficiency\nof the proposed methods is demonstrated through application to a genome-wide\nassociation study of coronary artery disease (CAD), and through simulation\nstudies. These results suggested there could be a lot of rare variants with\nlarge effect sizes for CAD, and the number of significant sets detected by the\nODP was much greater than those by existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 04:12:26 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Noma", "Hisashi", ""]]}, {"id": "2003.05778", "submitter": "Nobutaka Ito PhD", "authors": "Nobutaka Ito and Simon Godsill", "title": "A Multi-Target Track-Before-Detect Particle Filter Using Superpositional\n  Data in Non-Gaussian Noise", "comments": "Multi-target tracking (MTT), track-before-detect, superpositional\n  data, particle filter, birth/death process", "journal-ref": null, "doi": "10.1109/LSP.2020.3002704", "report-no": null, "categories": "eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel particle filter for tracking time-varying states\nof multiple targets jointly from superpositional data, which depend on the sum\nof contributions of all targets. Many conventional tracking methods rely on\npreprocessing for detection (e.g., thresholding), which severely limits\ntracking performance at a low signal-to-noise ratio (SNR). In contrast, the\nproposed method operates directly on raw sensor signals without requiring such\npreprocessing. Though there also exist methods applicable to raw sensor signals\ncalled track-before-detect, the proposed method has significant advantages over\nthem. First, it is general without any restrictions on observation/process\nnoise statistics (e.g., Gaussian) or the functional form of each target's\ncontribution to the sensors (e.g., linear, separable, binary). Especially, it\nincludes Salmond et al.'s track-before-detect particle filter for a single\ntarget as a particular example up to some implementation details. Second, it\ncan track an unknown, time-varying number of targets without knowing their\ninitial states owing to a target birth/death model. We present a simulation\nexample of radio-frequency tomography, where it significantly outperformed\nNannuru et al.'s state-of-the-art method based on random finite sets in terms\nof the optimal subpattern assignment (OSPA) metric.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 08:22:06 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 09:45:51 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Ito", "Nobutaka", ""], ["Godsill", "Simon", ""]]}, {"id": "2003.05807", "submitter": "Damien Challet", "authors": "Christian Bongiorno and Damien Challet", "title": "Covariance matrix filtering with bootstrapped hierarchies", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0245092", "report-no": null, "categories": "q-fin.RM q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference of the dependence between objects often relies on\ncovariance matrices. Unless the number of features (e.g. data points) is much\nlarger than the number of objects, covariance matrix cleaning is necessary to\nreduce estimation noise. We propose a method that is robust yet flexible enough\nto account for fine details of the structure covariance matrix. Robustness\ncomes from using a hierarchical ansatz and dependence averaging between\nclusters; flexibility comes from a bootstrap procedure. This method finds\nseveral possible hierarchical structures in DNA microarray gene expression\ndata, and leads to lower realized risk in global minimum variance portfolios\nthan current filtering methods when the number of data points is relatively\nsmall.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 13:58:41 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Bongiorno", "Christian", ""], ["Challet", "Damien", ""]]}, {"id": "2003.05880", "submitter": "Christy Brown", "authors": "Christy Brown and Jonathan Templin", "title": "Modification Indices for Diagnostic Classification Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic classification models (DCMs) are psychometric models for\nevaluating a student's mastery of the essential skills in a content domain\nbased upon their responses to a set of test items. Currently, diagnostic model\nand/or Q-matrix misspecification is a known problem with limited avenues for\nremediation. To address this problem, this paper defines a one-sided score\nstatistic that is a computationally efficient method for detecting\nunder-specification of both the Q-matrix and the model parameters of the\nparticular DCM chosen in the analysis. This method is analogous to the\nmodification indices widely used in structural equation modeling. The results\nof a simulation study show the Type I error rate of modification indices for\nDCMs are acceptably close to the nominal significance level when the\nappropriate mixture chi-squared reference distribution is used. The simulation\nresults indicate that modification indices are very powerful in the detection\nof an under-specified Q-matrix and have ample power to detect the omission of\nmodel parameters in large samples or when the items are highly discriminating.\nAn application of modification indices for DCMs to analysis of response data\nfrom a large-scale administration of a diagnostic test demonstrates how they\ncan be useful in diagnostic model refinement.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 16:16:15 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Brown", "Christy", ""], ["Templin", "Jonathan", ""]]}, {"id": "2003.05885", "submitter": "Johannes Bracher", "authors": "Johannes Bracher and Leonhard Held", "title": "A marginal moment matching approach for fitting endemic-epidemic models\n  to underreported disease surveillance counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count data are often subject to underreporting, especially in infectious\ndisease surveillance. We propose an approximate maximum likelihood method to\nfit count time series models from the endemic-epidemic class to underreported\ndata. The approach is based on marginal moment matching where underreported\nprocesses are approximated through completely observed processes from the same\nclass. Moreover, the form of the bias when underreporting is ignored or taken\ninto account via multiplication factors is analysed. Notably, we show that this\nleads to a downward bias in model-based estimates of the effective reproductive\nnumber. A marginal moment matching approach can also be used to account for\nreporting intervals which are longer than the mean serial interval of a\ndisease. The good performance of the proposed methodology is demonstrated in\nsimulation studies. An extension to time-varying parameters and reporting\nprobabilities is discussed and applied in a case study on weekly rotavirus\ngastroenteritis counts in Berlin, Germany.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 16:27:06 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 14:53:12 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Bracher", "Johannes", ""], ["Held", "Leonhard", ""]]}, {"id": "2003.05979", "submitter": "Bonnie Shook-Sa", "authors": "Bonnie E. Shook-Sa and Michael G. Hudgens", "title": "Power and Sample Size for Marginal Structural Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal structural models fit via inverse probability of treatment weighting\nare commonly used to control for confounding when estimating causal effects\nfrom observational data. When planning a study that will be analyzed with\nmarginal structural modeling, determining the required sample size for a given\nlevel of statistical power is challenging because of the effect of weighting on\nthe variance of the estimated causal means. This paper considers the utility of\nthe design effect to quantify the effect of weighting on the precision of\ncausal estimates. The design effect is defined as the ratio of the variance of\nthe causal mean estimator divided by the variance of a naive estimator if,\ncounter to fact, no confounding had been present and weights were not needed. A\nsimple, closed-form approximation of the design effect is derived that is\noutcome invariant and can be estimated during the study design phase. Once the\ndesign effect is approximated for each treatment group, sample size\ncalculations are conducted as for a randomized trial, but with variances\ninflated by the design effects to account for weighting. Simulations\ndemonstrate the accuracy of the design effect approximation, and practical\nconsiderations are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 19:04:57 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Shook-Sa", "Bonnie E.", ""], ["Hudgens", "Michael G.", ""]]}, {"id": "2003.05990", "submitter": "Ranjan Maitra", "authors": "Karl T. Pazdernik and Ranjan Maitra", "title": "Estimating Basis Functions in Massive Fields under the Spatial Mixed\n  Effects Model", "comments": "21 pages, 18 figures, 7 tables", "journal-ref": null, "doi": "10.1002/SAM.11537", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial prediction is commonly achieved under the assumption of a Gaussian\nrandom field (GRF) by obtaining maximum likelihood estimates of parameters, and\nthen using the kriging equations to arrive at predicted values. For massive\ndatasets, fixed rank kriging using the Expectation-Maximization (EM) algorithm\nfor estimation has been proposed as an alternative to the usual but\ncomputationally prohibitive kriging method. The method reduces computation cost\nof estimation by redefining the spatial process as a linear combination of\nbasis functions and spatial random effects. A disadvantage of this method is\nthat it imposes constraints on the relationship between the observed locations\nand the knots. We develop an alternative method that utilizes the Spatial Mixed\nEffects (SME) model, but allows for additional flexibility by estimating the\nrange of the spatial dependence between the observations and the knots via an\nAlternating Expectation Conditional Maximization (AECM) algorithm. Experiments\nshow that our methodology improves estimation without sacrificing prediction\naccuracy while also minimizing the additional computational burden of extra\nparameter estimation. The methodology is applied to a temperature data set\narchived by the United States National Climate Data Center, with improved\nresults over previous methodology.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 19:36:40 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Pazdernik", "Karl T.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2003.06024", "submitter": "Mathias Drton", "authors": "Mathias Drton, Satoshi Kuriki, Peter Hoff", "title": "Existence and Uniqueness of the Kronecker Covariance MLE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In matrix-valued datasets the sampled matrices often exhibit correlations\namong both their rows and their columns. A useful and parsimonious model of\nsuch dependence is the matrix normal model, in which the covariances among the\nelements of a random matrix are parameterized in terms of the Kronecker product\nof two covariance matrices, one representing row covariances and one\nrepresenting column covariance. An appealing feature of such a matrix normal\nmodel is that the Kronecker covariance structure allows for standard likelihood\ninference even when only a very small number of data matrices is available. For\ninstance, in some cases a likelihood ratio test of dependence may be performed\nwith a sample size of one. However, more generally the sample size required to\nensure boundedness of the matrix normal likelihood or the existence of a unique\nmaximizer depends in a complicated way on the matrix dimensions. This motivates\nthe study of how large a sample size is needed to ensure that maximum\nlikelihood estimators exist, and exist uniquely with probability one. Our main\nresult gives precise sample size thresholds in the paradigm where the number of\nrows and the number of columns of the data matrices differ by at most a factor\nof two. Our proof uses invariance properties that allow us to consider data\nmatrices in canonical form, as obtained from the Kronecker canonical form for\nmatrix pencils.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 21:19:18 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 19:51:37 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Drton", "Mathias", ""], ["Kuriki", "Satoshi", ""], ["Hoff", "Peter", ""]]}, {"id": "2003.06037", "submitter": "Alexandra Larsen", "authors": "Alexandra Larsen, Shu Yang, Brian J. Reich, Ana G. Rappold", "title": "A spatial causal analysis of wildland fire-contributed PM2.5 using\n  numerical model output", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wildland fire smoke contains hazardous levels of fine particulate matter\nPM2.5, a pollutant shown to adversely effect health. Estimating fire\nattributable PM2.5 concentrations is key to quantifying the impact on air\nquality and subsequent health burden. This is a challenging problem since only\ntotal PM2.5 is measured at monitoring stations and both fire-attributable PM2.5\nand PM2.5 from all other sources are correlated in space and time. We propose a\nframework for estimating fire-contributed PM2.5 and PM2.5 from all other\nsources using a novel causal inference framework and bias-adjusted chemical\nmodel representations of PM2.5 under counterfactual scenarios. The chemical\nmodel representation of PM2.5 for this analysis is simulated using Community\nMulti-Scale Air Quality Modeling System (CMAQ), run with and without fire\nemissions across the contiguous U.S. for the 2008-2012 wildfire seasons. The\nCMAQ output is calibrated with observations from monitoring sites for the same\nspatial domain and time period. We use a Bayesian model that accounts for\nspatial variation to estimate the effect of wildland fires on PM2.5 and state\nassumptions under which the estimate has a valid causal interpretation. Our\nresults include estimates of absolute, relative and cumulative contributions of\nwildfire smoke to PM2.5 for the contiguous U.S. Additionally, we compute the\nhealth burden associated with the PM2.5 attributable to wildfire smoke.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 22:08:33 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Larsen", "Alexandra", ""], ["Yang", "Shu", ""], ["Reich", "Brian J.", ""], ["Rappold", "Ana G.", ""]]}, {"id": "2003.06067", "submitter": "Han Lin Shang", "authors": "Ufuk Beyaztas and Han Lin Shang", "title": "A comparison of parameter estimation in function-on-function regression", "comments": "43 pages, 9 figures, 8 tables, to appear at Communications in\n  Statistics - Simulation and Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technological developments have enabled us to collect complex and\nhigh-dimensional data in many scientific fields, such as population health,\nmeteorology, econometrics, geology, and psychology. It is common to encounter\nsuch datasets collected repeatedly over a continuum. Functional data, whose\nsample elements are functions in the graphical forms of curves, images, and\nshapes, characterize these data types. Functional data analysis techniques\nreduce the complex structure of these data and focus on the dependences within\nand (possibly) between the curves. A common research question is to investigate\nthe relationships in regression models that involve at least one functional\nvariable. However, the performance of functional regression models depends on\nseveral factors, such as the smoothing technique, the number of basis\nfunctions, and the estimation method. This paper provides a selective\ncomparison for function-on-function regression models where both the response\nand predictor(s) are functions, to determine the optimal choice of basis\nfunction from a set of model evaluation criteria. We also propose a bootstrap\nmethod to construct a confidence interval for the response function. The\nnumerical comparisons are implemented through Monte Carlo simulations and two\nreal data examples.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 23:52:17 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Beyaztas", "Ufuk", ""], ["Shang", "Han Lin", ""]]}, {"id": "2003.06222", "submitter": "Gerrit van den Burg", "authors": "Gerrit J.J. van den Burg and Christopher K.I. Williams", "title": "An Evaluation of Change Point Detection Algorithms", "comments": "For code and data, see\n  https://github.com/alan-turing-institute/TCPDBench ; Changelog in pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change point detection is an important part of time series analysis, as the\npresence of a change point indicates an abrupt and significant change in the\ndata generating process. While many algorithms for change point detection\nexist, little attention has been paid to evaluating their performance on\nreal-world time series. Algorithms are typically evaluated on simulated data\nand a small number of commonly-used series with unreliable ground truth.\nClearly this does not provide sufficient insight into the comparative\nperformance of these algorithms. Therefore, instead of developing yet another\nchange point detection method, we consider it vastly more important to properly\nevaluate existing algorithms on real-world data. To achieve this, we present\nthe first data set specifically designed for the evaluation of change point\ndetection algorithms, consisting of 37 time series from various domains. Each\ntime series was annotated by five expert human annotators to provide ground\ntruth on the presence and location of change points. We analyze the consistency\nof the human annotators, and describe evaluation metrics that can be used to\nmeasure algorithm performance in the presence of multiple ground truth\nannotations. Subsequently, we present a benchmark study where 14 existing\nalgorithms are evaluated on each of the time series in the data set. This study\nshows that binary segmentation (Scott and Knott, 1974) and Bayesian online\nchange point detection (Adams and MacKay, 2007) are among the best performing\nmethods. Our aim is that this data set will serve as a proving ground in the\ndevelopment of novel change point detection algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 12:23:41 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 12:28:45 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Burg", "Gerrit J. J. van den", ""], ["Williams", "Christopher K. I.", ""]]}, {"id": "2003.06278", "submitter": "Fabian Dablander", "authors": "Fabian Dablander, Don van den Bergh, Alexander Ly, Eric-Jan\n  Wagenmakers", "title": "Default Bayes Factors for Testing the (In)equality of Several Population\n  Variances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Testing the (in)equality of variances is an important problem in many\nstatistical applications. We develop default Bayes factor tests to assess the\n(in)equality of two or more population variances, as well as a test for whether\nthe population variance equals a specific value. The resulting test can be used\nto check assumptions for commonly used procedures such as the $t$-test or\nANOVA, or test substantive hypotheses concerning variances directly. We further\nextend the Bayes factor to allow $\\mathcal{H}_0$ to have a null-region.\nResearchers may have directed hypotheses such as $\\sigma_1^2 > \\sigma_2^2$, or\nwant to combine hypotheses about equality with hypotheses about inequality, for\nexample $\\sigma_1^2 = \\sigma_2^2 > (\\sigma_3^2, \\sigma_4^2)$. We generalize our\nBayes factor to accommodate such hypotheses for $K > 2$ groups. We show that\nour Bayes factor fulfills a number of desiderata, provide practical examples\nillustrating the method, and compare it to a recently proposed fractional Bayes\nfactor procedure by B\\\"oing-Messing & Mulder (2018). Our procedure is\nimplemented in the R package $bfvartest$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 13:35:18 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Dablander", "Fabian", ""], ["Bergh", "Don van den", ""], ["Ly", "Alexander", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "2003.06299", "submitter": "Aritra Halder", "authors": "Aritra Halder, Shariq Mohammed, Kun Chen and Dipak K. Dey", "title": "Spatial Tweedie exponential dispersion models", "comments": "26 pages, 3 figures and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general modeling framework that allows for uncertainty\nquantification at the individual covariate level and spatial referencing,\noperating withing a double generalized linear model (DGLM). DGLMs provide a\ngeneral modeling framework allowing dispersion to depend in a link-linear\nfashion on chosen covariates. We focus on working with Tweedie exponential\ndispersion models while considering DGLMs, the reason being their recent\nwide-spread use for modeling mixed response types. Adopting a regularization\nbased approach, we suggest a class of flexible convex penalties derived from an\nun-directed graph that facilitates estimation of the unobserved spatial effect.\nDevelopments are concisely showcased by proposing a co-ordinate descent\nalgorithm that jointly explains variation from covariates in mean and\ndispersion through estimation of respective model coefficients while estimating\nthe unobserved spatial effect. Simulations performed show that proposed\napproach is superior to competitors like the ridge and un-penalized versions.\nFinally, a real data application is considered while modeling insurance losses\narising from automobile collisions in the state of Connecticut, USA for the\nyear 2008.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 06:16:41 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Halder", "Aritra", ""], ["Mohammed", "Shariq", ""], ["Chen", "Kun", ""], ["Dey", "Dipak K.", ""]]}, {"id": "2003.06368", "submitter": "Naveed Merchant", "authors": "Jeffery Hart and Taeryon Choi and Naveed Merchant", "title": "Use of Cross-validation Bayes Factors to Test Equality of Two Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-parametric, two-sample Bayesian test for checking whether or\nnot two data sets share a common distribution. The test makes use of data\nsplitting ideas and does not require priors for high-dimensional parameter\nvectors as do other nonparametric Bayesian procedures. We provide evidence that\nthe new procedure provides more stable Bayes factors than do methods based on\nP\\'olya trees. Somewhat surprisingly, the behavior of the proposed Bayes\nfactors when the two distributions are the same is usually superior to that of\nP\\'olya tree Bayes factors. We showcase the effectiveness of the test by\nproving its consistency, conducting a simulation study and applying the test to\nHiggs boson data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 16:26:00 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Hart", "Jeffery", ""], ["Choi", "Taeryon", ""], ["Merchant", "Naveed", ""]]}, {"id": "2003.06416", "submitter": "Sameer Deshpande", "authors": "Sameer K. Deshpande and Ray Bai and Cecilia Balocchi and Jennifer E.\n  Starling and Jordan Weiss", "title": "VCBART: Bayesian trees for varying coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have reported associations between later-life cognition and\nsocioeconomic position in childhood, young adulthood, and mid-life. However,\nthe vast majority of these studies are unable to quantify how these\nassociations vary over time and with respect to several demographic factors.\nVarying coefficient (VC) models, which treat the covariate effects in a linear\nmodel as nonparametric functions of additional effect modifiers, offer an\nappealing way to overcome these limitations. Unfortunately, state-of-the-art VC\nmodeling methods require computationally prohibitive parameter tuning or make\nrestrictive assumptions about the functional form of the covariate effects.\n  In response, we propose VCBART, which estimates the covariate effects in a VC\nmodel using Bayesian Additive Regression Trees. With simple default\nhyperparameter settings, VCBART outperforms existing methods in terms of\ncovariate effect estimation and prediction. Using VCBART, we predict the\ncognitive trajectories of 4,167 subjects from the Health and Retirement Study\nusing multiple measures of socioeconomic position and physical health. We find\nthat socioeconomic position in childhood and young adulthood have small effects\nthat do not vary with age. In contrast, the effects of measures of mid-life\nphysical health tend to vary with respect to age, race, and marital status. An\nR package implementing VC-BART is available at\nhttps://github.com/skdeshpande91/VCBART\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 17:58:28 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 16:57:14 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 19:03:17 GMT"}, {"version": "v4", "created": "Mon, 24 Aug 2020 15:41:16 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Deshpande", "Sameer K.", ""], ["Bai", "Ray", ""], ["Balocchi", "Cecilia", ""], ["Starling", "Jennifer E.", ""], ["Weiss", "Jordan", ""]]}, {"id": "2003.06455", "submitter": "Santu Ghosh", "authors": "Santu Ghosh, Deepak Nag Ayyala and Rafael Hellebuyck", "title": "Two-Sample High Dimensional Mean Test Based On Prepivots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Testing equality of mean vectors is a very commonly used criterion when\ncomparing two multivariate random variables. Traditional tests such as\nHotelling's T-squared become either unusable or output small power when the\nnumber of variables is greater than the combined sample size. In this paper,\nw}e propose a test using both prepivoting and Edgeworth expansion for testing\nthe equality of two population mean vectors in the \"large p, small n\" setting.\nThe asymptotic null distribution of the test statistic is derived and it is\nshown that the power of suggested test converges to one under certain\nalternatives when both n and p increase to infinity against sparse\nalternatives. Finite sample performance of the proposed test statistic is\ncompared with other recently developed tests designed to also handle the \"large\np, small n\" situation through simulations. The proposed test achieves\ncompetitive rates for both type I error rate and power. The usefulness of our\ntest is illustrated by applications to two microarray gene expression data sets\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 19:27:53 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ghosh", "Santu", ""], ["Ayyala", "Deepak Nag", ""], ["Hellebuyck", "Rafael", ""]]}, {"id": "2003.06462", "submitter": "Austin Lawson", "authors": "Yu-Min Chung, William Cruse, and Austin Lawson", "title": "A Persistent Homology Approach to Time Series Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological Data Analysis (TDA) is a rising field of computational topology\nin which the topological structure of a data set can be observed by persistent\nhomology. By considering a sequence of sublevel sets, one obtains a filtration\nthat tracks changes in topological information. These changes can be recorded\nin multi-sets known as {\\it persistence diagrams}. Converting information\nstored in persistence diagrams into a form compatible with modern machine\nlearning algorithms is a major vein of research in TDA. {\\it Persistence\ncurves}, a recently developed framework, provides a canonical and flexible way\nto encode the information presented in persistence diagrams into vectors. In\nthis work, we propose a new set of metrics based on persistence curves. We\nprove the stability of the proposed metrics. Finally, we apply these metrics to\nthe UCR Time Series Classification Archive. These empirical results show that\nour metrics perform better than the relevant benchmark in most cases and\nwarrant further study.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 19:45:20 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chung", "Yu-Min", ""], ["Cruse", "William", ""], ["Lawson", "Austin", ""]]}, {"id": "2003.06512", "submitter": "Cristina Mollica", "authors": "Cristina Mollica and Luca Tardella", "title": "New algorithms and goodness-of-fit diagnostics from remarkable\n  properties of ranking models", "comments": "19 pages, 3 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forward order assumption postulates that the ranking process of the items\nis carried out by sequentially assigning the positions from the top\n(most-liked) to the bottom (least-liked) alternative. This assumption has been\nrecently relaxed with the Extended Plackett-Luce model (EPL) through the\nintroduction of the discrete reference order parameter, describing the rank\nattribution path. By starting from two formal properties of the EPL, the former\nrelated to the inverse ordering of the item probabilities at the first and last\nstage of the ranking process and the latter well-known as independence of\nirrelevant alternatives (or Luce's choice axiom), we derive novel diagnostic\ntools for testing the appropriateness of the EPL assumption as the actual\nsampling distribution of the observed rankings. Besides contributing to fill\nthe gap of goodness-of-fit methods for the family of multistage models, we also\nshow how one of the two statistics can be conveniently exploited to construct a\nheuristic method, that surrogates the maximum likelihood approach for inferring\nthe underlying reference order parameter. The relative performance of the\nproposals compared with more conventional approaches is illustrated by means of\nextensive simulation studies.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 23:50:43 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Mollica", "Cristina", ""], ["Tardella", "Luca", ""]]}, {"id": "2003.06675", "submitter": "Xiaohuan Xue", "authors": "Xiaohuan Xue", "title": "Improved Approximations of Hedges' g*", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hedges' unbiased estimator g* has been broadly used in statistics. We propose\na sequence of polynomials to better approximate the multiplicative correction\nfactor of g* by incorporating analytic estimations to the ratio of gamma\nfunctions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 17:45:28 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Xue", "Xiaohuan", ""]]}, {"id": "2003.06684", "submitter": "Gilles Mordant", "authors": "Marc Hallin and Gilles Mordant and Johan Segers", "title": "Multivariate goodness-of-Fit tests based on Wasserstein distance", "comments": "42 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goodness-of-fit tests based on the empirical Wasserstein distance are\nproposed for simple and composite null hypotheses involving general\nmultivariate distributions. For group families, the procedure is to be\nimplemented after preliminary reduction of the data via invariance.This\nproperty allows for calculation of exact critical values and p-values at finite\nsample sizes. Applications include testing for location--scale families and\ntesting for families arising from affine transformations, such as elliptical\ndistributions with given standard radial density and unspecified location\nvector and scatter matrix. A novel test for multivariate normality with\nunspecified mean vector and covariance matrix arises as a special case. For\nmore general parametric families, we propose a parametric bootstrap procedure\nto calculate critical values. The lack of asymptotic distribution theory for\nthe empirical Wasserstein distance means that the validity of the parametric\nbootstrap under the null hypothesis remains a conjecture. Nevertheless, we show\nthat the test is consistent against fixed alternatives. To this end, we prove a\nuniform law of large numbers for the empirical distribution in Wasserstein\ndistance, where the uniformity is over any class of underlying distributions\nsatisfying a uniform integrability condition but no additional moment\nassumptions. The calculation of test statistics boils down to solving the\nwell-studied semi-discrete optimal transport problem. Extensive numerical\nexperiments demonstrate the practical feasibility and the excellent performance\nof the proposed tests for the Wasserstein distance of order p = 1 and p = 2 and\nfor dimensions at least up to d = 5. The simulations also lend support to the\nconjecture of the asymptotic validity of the parametric bootstrap.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 19:08:53 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 20:45:44 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 11:37:59 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Hallin", "Marc", ""], ["Mordant", "Gilles", ""], ["Segers", "Johan", ""]]}, {"id": "2003.06723", "submitter": "Hyunseung Kang", "authors": "Nan Bi, Hyunseung Kang, Jonathan Taylor", "title": "Inferring Treatment Effects After Testing Instrument Strength in Linear\n  Models", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common practice in IV studies is to check for instrument strength, i.e. its\nassociation to the treatment, with an F-test from regression. If the\nF-statistic is above some threshold, usually 10, the instrument is deemed to\nsatisfy one of the three core IV assumptions and used to test for the treatment\neffect. However, in many cases, the inference on the treatment effect does not\ntake into account the strength test done a priori. In this paper, we show that\nnot accounting for this pretest can severely distort the distribution of the\ntest statistic and propose a method to correct this distortion, producing valid\ninference. A key insight in our method is to frame the F-test as a randomized\nconvex optimization problem and to leverage recent methods in selective\ninference. We prove that our method provides conditional and marginal Type I\nerror control. We also extend our method to weak instrument settings. We\nconclude with a reanalysis of studies concerning the effect of education on\nearning where we show that not accounting for pre-testing can dramatically\nalter the original conclusion about education's effects.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 23:53:12 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Bi", "Nan", ""], ["Kang", "Hyunseung", ""], ["Taylor", "Jonathan", ""]]}, {"id": "2003.06804", "submitter": "Chris U. Carmona", "authors": "Chris U. Carmona and Geoff K. Nicholls", "title": "Semi-Modular Inference: enhanced learning in multi-modular models by\n  tempering the influence of components", "comments": "for associated R package to reproduce results, see\n  https://github.com/christianu7/aistats2020smi", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian statistical inference loses predictive optimality when generative\nmodels are misspecified.\n  Working within an existing coherent loss-based generalisation of Bayesian\ninference, we show existing Modular/Cut-model inference is coherent, and write\ndown a new family of Semi-Modular Inference (SMI) schemes, indexed by an\ninfluence parameter, with Bayesian inference and Cut-models as special cases.\nWe give a meta-learning criterion and estimation procedure to choose the\ninference scheme. This returns Bayesian inference when there is no\nmisspecification.\n  The framework applies naturally to Multi-modular models. Cut-model inference\nallows directed information flow from well-specified modules to misspecified\nmodules, but not vice versa. An existing alternative power posterior method\ngives tunable but undirected control of information flow, improving prediction\nin some settings. In contrast, SMI allows tunable and directed information flow\nbetween modules.\n  We illustrate our methods on two standard test cases from the literature and\na motivating archaeological data set.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 11:55:55 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Carmona", "Chris U.", ""], ["Nicholls", "Geoff K.", ""]]}, {"id": "2003.06938", "submitter": "Daiver V\\'elez", "authors": "D. V\\'elez, M.E. P\\'erez and L. R. Pericchi", "title": "Increasing the Replicability for Linear Models via Adaptive Significance\n  Levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We put forward an adaptive alpha (Type I Error) that decreases as the\ninformation grows, for hypothesis tests in which nested linear models are\ncompared. A less elaborate adaptation was already presented in \\citet{PP2014}\nfor comparing general i.i.d. models. In this article we present refined\nversions to compare nested linear models. This calibration may be interpreted\nas a Bayes-non-Bayes compromise, of a simple translations of a Bayes Factor on\nfrequentist terms that leads to statistical consistency, and most importantly,\nit is a step towards statistics that promotes replicable scientific findings.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 22:22:51 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 20:14:11 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 15:10:40 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["V\u00e9lez", "D.", ""], ["P\u00e9rez", "M. E.", ""], ["Pericchi", "L. R.", ""]]}, {"id": "2003.06955", "submitter": "Izabel Souza", "authors": "Izabel Nolau de Souza and Kelly Cristina Mota Gon\\c{c}alves and Jo\\~ao\n  Batista de Morais Pereira", "title": "Model-based Inference for Rare and Clustered Populations from Adaptive\n  Cluster Sampling using Auxiliary Variables", "comments": "36 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rare populations, such as endangered animals and plants, drug users and\nindividuals with rare diseases, tend to cluster in regions. Adaptive cluster\nsampling is generally applied to obtain information from clustered and sparse\npopulations since it increases survey effort in areas where the individuals of\ninterest are observed. This work aims to propose a unit-level model which\nassumes that counts are related to auxiliary variables, improving the sampling\nprocess, assigning different weights to the cells, besides referring them\nspatially. The proposed model fits rare and grouped populations, disposed over\na regular grid, in a Bayesian framework. The approach is compared to\nalternative methods using simulated data and a real experiment in which\nadaptive samples were drawn from an African Buffaloes population in a 24,108\nsquare kilometers area of East Africa. Simulation studies show that the model\nis efficient under several settings, validating the methodology proposed in\nthis paper for practical situations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 00:27:02 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["de Souza", "Izabel Nolau", ""], ["Gon\u00e7alves", "Kelly Cristina Mota", ""], ["Pereira", "Jo\u00e3o Batista de Morais", ""]]}, {"id": "2003.06961", "submitter": "Hossein Keshavarz", "authors": "Hossein Keshavarz, George Michailidis", "title": "Online detection of local abrupt changes in high-dimensional Gaussian\n  graphical models", "comments": "40 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of identifying change points in high-dimensional Gaussian\ngraphical models (GGMs) in an online fashion is of interest, due to new\napplications in biology, economics and social sciences. The offline version of\nthe problem, where all the data are a priori available, has led to a number of\nmethods and associated algorithms involving regularized loss functions.\nHowever, for the online version, there is currently only a single work in the\nliterature that develops a sequential testing procedure and also studies its\nasymptotic false alarm probability and power. The latter test is best suited\nfor the detection of change points driven by global changes in the structure of\nthe precision matrix of the GGM, in the sense that many edges are involved.\nNevertheless, in many practical settings the change point is driven by local\nchanges, in the sense that only a small number of edges exhibit changes. To\nthat end, we develop a novel test to address this problem that is based on the\n$\\ell_\\infty$ norm of the normalized covariance matrix of an appropriately\nselected portion of incoming data. The study of the asymptotic distribution of\nthe proposed test statistic under the null (no presence of a change point) and\nthe alternative (presence of a change point) hypotheses requires new technical\ntools that examine maxima of graph-dependent Gaussian random variables, and\nthat of independent interest. It is further shown that these tools lead to the\nimposition of mild regularity conditions for key model parameters, instead of\nmore stringent ones required by leveraging previously used tools in related\nproblems in the literature. Numerical work on synthetic data illustrates the\ngood performance of the proposed detection procedure both in terms of\ncomputational and statistical efficiency across numerous experimental settings.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 00:41:34 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Keshavarz", "Hossein", ""], ["Michailidis", "George", ""]]}, {"id": "2003.07201", "submitter": "Maria B\\.ankestad", "authors": "Maria B{\\aa}nkestad, Jens Sj\\\"olund, Jalil Taghia, Thomas Sch\\\"on", "title": "The Elliptical Processes: a Family of Fat-tailed Stochastic Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the elliptical processes -- a family of non-parametric\nprobabilistic models that subsumes the Gaussian process and the Student-t\nprocess. This generalization includes a range of new fat-tailed behaviors yet\nretains computational tractability. We base the elliptical processes on a\nrepresentation of elliptical distributions as a continuous mixture of Gaussian\ndistributions and derive closed-form expressions for the marginal and\nconditional distributions. We perform numerical experiments on robust\nregression using an elliptical process defined by a piecewise constant mixing\ndistribution, and show advantages compared with a Gaussian process. The\nelliptical processes may become a replacement for Gaussian processes in several\nsettings, including when the likelihood is not Gaussian or when accurate tail\nmodeling is critical.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 08:36:39 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 07:27:47 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["B\u00e5nkestad", "Maria", ""], ["Sj\u00f6lund", "Jens", ""], ["Taghia", "Jalil", ""], ["Sch\u00f6n", "Thomas", ""]]}, {"id": "2003.07214", "submitter": "Oswaldo Gressani", "authors": "Oswaldo Gressani, Philippe Lambert", "title": "Laplace approximation for fast Bayesian inference in generalized\n  additive models based on penalized regression splines", "comments": "39 pages, 7 figures, 8 tables", "journal-ref": "Computational Statistics & Data Analysis 154 (2021): 107088", "doi": "10.1016/j.csda.2020.107088", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized additive models (GAMs) are a well-established statistical tool\nfor modeling complex nonlinear relationships between covariates and a response\nassumed to have a conditional distribution in the exponential family. In this\narticle, P-splines and the Laplace approximation are coupled for flexible and\nfast approximate Bayesian inference in GAMs. The proposed Laplace-P-spline\nmodel contributes to the development of a new methodology to explore the\nposterior penalty space by considering a deterministic grid-based strategy or a\nMarkov chain sampler, depending on the number of smooth additive terms in the\npredictor. Our approach has the merit of relying on closed form analytical\nexpressions for the gradient and Hessian of the approximate posterior penalty\nvector, which enables to construct accurate posterior pointwise and credible\nset estimators for latent field variables at a relatively low computational\nbudget even for a large number of smooth additive components. Based upon simple\nGaussian approximations of the conditional latent field posterior, the\nsuggested methodology enjoys excellent statistical properties. The performance\nof the Laplace-P-spline model is confirmed through different simulation\nscenarios and the method is illustrated on two real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 13:43:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Gressani", "Oswaldo", ""], ["Lambert", "Philippe", ""]]}, {"id": "2003.07320", "submitter": "Mikkel S{\\o}lvsten", "authors": "Stanislav Anatolyev, Mikkel S{\\o}lvsten", "title": "Testing Many Restrictions Under Heteroskedasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hypothesis test that allows for many tested restrictions in a\nheteroskedastic linear regression model. The test compares the conventional F\nstatistic to a critical value that corrects for many restrictions and\nconditional heteroskedasticity. The correction utilizes leave-one-out\nestimation to correctly center the critical value and leave-three-out\nestimation to appropriately scale it. Large sample properties of the test are\nestablished in an asymptotic framework where the number of tested restrictions\nmay be fixed or may grow with the sample size and can even be proportional to\nthe number of observations. We show that the test is asymptotically valid and\nhas non-trivial asymptotic power against the same local alternatives as the\nexact F test when the latter is valid. Simulations corroborate the relevance of\nthese theoretical findings and suggest excellent size control in moderately\nsmall samples also under strong heteroskedasticity.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 16:44:44 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 23:22:41 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Anatolyev", "Stanislav", ""], ["S\u00f8lvsten", "Mikkel", ""]]}, {"id": "2003.07398", "submitter": "Jiacong Du", "authors": "Jiacong Du, Jonathan Boss, Peisong Han, Lauren J Beesley, Stephen A\n  Goutman, Stuart Batterman, Eva L Feldman, Bhramar Mukherjee", "title": "Variable selection with multiply-imputed datasets: choosing between\n  stacked and grouped methods", "comments": "23 pages, 6 figures. This paper has been submitted to Statistics in\n  Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression methods, such as lasso and elastic net, are used in many\nbiomedical applications when simultaneous regression coefficient estimation and\nvariable selection is desired. However, missing data complicates the\nimplementation of these methods, particularly when missingness is handled using\nmultiple imputation. Applying a variable selection algorithm on each imputed\ndataset will likely lead to different sets of selected predictors, making it\ndifficult to ascertain a final active set without resorting to ad hoc\ncombination rules. In this paper we consider a general class of penalized\nobjective functions which, by construction, force selection of the same\nvariables across multiply-imputed datasets. By pooling objective functions\nacross imputations, optimization is then performed jointly over all imputed\ndatasets rather than separately for each dataset. We consider two objective\nfunction formulations that exist in the literature, which we will refer to as\n\"stacked\" and \"grouped\" objective functions. Building on existing work, we (a)\nderive and implement efficient cyclic coordinate descent and\nmajorization-minimization optimization algorithms for both continuous and\nbinary outcome data, (b) incorporate adaptive shrinkage penalties, (c) compare\nthese methods through simulation, and (d) develop an R package miselect for\neasy implementation. Simulations demonstrate that the \"stacked\" objective\nfunction approaches tend to be more computationally efficient and have better\nestimation and selection properties. We apply these methods to data from the\nUniversity of Michigan ALS Patients Repository (UMAPR) which aims to identify\nthe association between persistent organic pollutants and ALS risk.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 18:39:30 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Du", "Jiacong", ""], ["Boss", "Jonathan", ""], ["Han", "Peisong", ""], ["Beesley", "Lauren J", ""], ["Goutman", "Stephen A", ""], ["Batterman", "Stuart", ""], ["Feldman", "Eva L", ""], ["Mukherjee", "Bhramar", ""]]}, {"id": "2003.07404", "submitter": "Joshua Loyal", "authors": "Joshua Daniel Loyal, Yuguo Chen", "title": "A Bayesian Nonparametric Latent Space Approach to Modeling Evolving\n  Communities in Dynamic Networks", "comments": "52 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of communities in dynamic (time-varying) network data is a\nprominent topic of interest. A popular approach to understanding these dynamic\nnetworks is to embed the dyadic relations into a latent metric space. While\nmethods for clustering with this approach exist for dynamic networks, they all\nassume a static community structure. This paper presents a Bayesian\nnonparametric model for dynamic networks that can model networks with evolving\ncommunity structures. Our model extends existing latent space approaches by\nexplicitly modeling the additions, deletions, splits, and mergers of groups\nwith a hierarchical Dirichlet process hidden Markov model. Our proposed\napproach, the hierarchical Dirichlet process latent position clustering model\n(HDP-LPCM), incorporates transitivity, models both individual and group level\naspects of the data, and avoids the computationally expensive selection of the\nnumber of groups required by most popular methods. We provide a Markov chain\nMonte Carlo estimation algorithm and apply our method to synthetic and\nreal-world networks to demonstrate its performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 18:51:33 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Loyal", "Joshua Daniel", ""], ["Chen", "Yuguo", ""]]}, {"id": "2003.07429", "submitter": "Lili Zheng", "authors": "Lili Zheng, Garvesh Raskutti, Rebecca Willett, Benjamin Mark", "title": "Context-dependent self-exciting point processes: models, methods, and\n  risk bounds in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional autoregressive point processes model how current events\ntrigger or inhibit future events, such as activity by one member of a social\nnetwork can affect the future activity of his or her neighbors. While past work\nhas focused on estimating the underlying network structure based solely on the\ntimes at which events occur on each node of the network, this paper examines\nthe more nuanced problem of estimating context-dependent networks that reflect\nhow features associated with an event (such as the content of a social media\npost) modulate the strength of influences among nodes. Specifically, we\nleverage ideas from compositional time series and regularization methods in\nmachine learning to conduct network estimation for high-dimensional marked\npoint processes. Two models and corresponding estimators are considered in\ndetail: an autoregressive multinomial model suited to categorical marks and a\nlogistic-normal model suited to marks with mixed membership in different\ncategories. Importantly, the logistic-normal model leads to a convex negative\nlog-likelihood objective and captures dependence across categories. We provide\ntheoretical guarantees for both estimators, which we validate by simulations\nand a synthetic data-generating model. We further validate our methods through\ntwo real data examples and demonstrate the advantages and disadvantages of both\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 20:22:43 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Zheng", "Lili", ""], ["Raskutti", "Garvesh", ""], ["Willett", "Rebecca", ""], ["Mark", "Benjamin", ""]]}, {"id": "2003.07494", "submitter": "Mostafa Karimi", "authors": "Kahkashan Afrin, Ashif S. Iquebal, Mostafa Karimi, Allyson Souris, Se\n  Yoon Lee, and Bani K. Mallick", "title": "Directionally Dependent Multi-View Clustering Using Copula Model", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0238996", "report-no": null, "categories": "stat.ME q-bio.GN stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent biomedical scientific problems, it is a fundamental issue to\nintegratively cluster a set of objects from multiple sources of datasets. Such\nproblems are mostly encountered in genomics, where data is collected from\nvarious sources, and typically represent distinct yet complementary\ninformation. Integrating these data sources for multi-source clustering is\nchallenging due to their complex dependence structure including directional\ndependency. Particularly in genomics studies, it is known that there is certain\ndirectional dependence between DNA expression, DNA methylation, and RNA\nexpression, widely called The Central Dogma.\n  Most of the existing multi-view clustering methods either assume an\nindependent structure or pair-wise (non-directional) dependency, thereby\nignoring the directional relationship. Motivated by this, we propose a\ncopula-based multi-view clustering model where a copula enables the model to\naccommodate the directional dependence existing in the datasets. We conduct a\nsimulation experiment where the simulated datasets exhibiting inherent\ndirectional dependence: it turns out that ignoring the directional dependence\nnegatively affects the clustering performance. As a real application, we\napplied our model to the breast cancer tumor samples collected from The Cancer\nGenome Altas (TCGA).\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 02:04:10 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 15:34:44 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Afrin", "Kahkashan", ""], ["Iquebal", "Ashif S.", ""], ["Karimi", "Mostafa", ""], ["Souris", "Allyson", ""], ["Lee", "Se Yoon", ""], ["Mallick", "Bani K.", ""]]}, {"id": "2003.07500", "submitter": "Benjamin Ackerman", "authors": "Benjamin Ackerman, Catherine R. Lesko, Juned Siddique, Ryoko Susukida\n  and Elizabeth A. Stuart", "title": "Generalizing Randomized Trial Findings to a Target Population using\n  Complex Survey Population Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized trials are considered the gold standard for estimating causal\neffects. Trial findings are often used to inform policy and programming\nefforts, yet their results may not generalize well to a relevant target\npopulation due to potential differences in effect moderators between the trial\nand population. Statistical methods have been developed to improve\ngeneralizability by combining trials and population data, and weighting the\ntrial to resemble the population on baseline covariates.Large-scale surveys in\nfields such as health and education with complex survey designs are a logical\nsource for population data; however, there is currently no best practice for\nincorporating survey weights when generalizing trial findings to a complex\nsurvey. We propose and investigate ways to incorporate survey weights in this\ncontext. We examine the performance of our proposed estimator in simulations by\ncomparing its performance to estimators that ignore the complex survey\ndesign.We then apply the methods to generalize findings from two trials - a\nlifestyle intervention for blood pressure reduction and a web-based\nintervention to treat substance use disorders - to their respective target\npopulations using population data from complex surveys. The work highlights the\nimportance in properly accounting for the complex survey design when\ngeneralizing trial findings to a population represented by a complex survey\nsample.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 02:29:58 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 18:59:00 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Ackerman", "Benjamin", ""], ["Lesko", "Catherine R.", ""], ["Siddique", "Juned", ""], ["Susukida", "Ryoko", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "2003.07662", "submitter": "Annabel Davies", "authors": "Annabel L Davies and Tobias Galla", "title": "Degree irregularity and rank probability bias in network meta-analysis", "comments": "15 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network meta-analysis (NMA) is a statistical technique for the comparison of\ntreatment options. The nodes of the network are the competing treatments and\nedges represent comparisons of treatments in trials. Outcomes of Bayesian NMA\ninclude estimates of treatment effects, and the probabilities that each\ntreatment is ranked best, second best and so on. How exactly network geometry\naffects the accuracy and precision of these outcomes is not fully understood.\nHere we carry out a simulation study and find that disparity in the number of\ntrials involving different treatments leads to a systematic bias in estimated\nrank probabilities. This bias is associated with an increased variation in the\nprecision of treatment effect estimates. Using ideas from the theory of complex\nnetworks, we define a measure of `degree irregularity' to quantify asymmetry in\nthe number of studies involving each treatment. Our simulations indicate that\nmore regular networks have more precise treatment effect estimates and smaller\nbias of rank probabilities. We also find that degree regularity is a better\nindicator of NMA quality than both the total number of studies in a network and\nthe disparity in the number of trials per comparison. These results have\nimplications for planning future trials. We demonstrate that choosing trials\nwhich reduce the network's irregularity can improve the precision and accuracy\nof NMA outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 12:26:30 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Davies", "Annabel L", ""], ["Galla", "Tobias", ""]]}, {"id": "2003.07899", "submitter": "Abhinav Prakash", "authors": "Abhinav Prakash, Rui Tuo, Yu Ding", "title": "Gaussian process aided function comparison using noisy scattered data", "comments": null, "journal-ref": null, "doi": "10.1080/00401706.2021.1905073", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a nonparametric method to compare the underlying mean\nfunctions given two noisy datasets. The motivation for the work stems from an\napplication of comparing wind turbine power curves. Comparing wind turbine data\npresents new problems, namely the need to identify the regions of difference in\nthe input space and to quantify the extent of difference that is statistically\nsignificant. Our proposed method, referred to as funGP, estimates the\nunderlying functions for different data samples using Gaussian process models.\nWe build a confidence band using the probability law of the estimated function\ndifferences under the null hypothesis. Then, the confidence band is used for\nthe hypothesis test as well as for identifying the regions of difference. This\nidentification of difference regions is a distinct feature, as existing methods\ntend to conduct an overall hypothesis test stating whether two functions are\ndifferent. Understanding the difference regions can lead to further practical\ninsights and help devise better control and maintenance strategies for wind\nturbines. The merit of funGP is demonstrated by using three simulation studies\nand four real wind turbine datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 19:15:16 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 01:47:02 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Prakash", "Abhinav", ""], ["Tuo", "Rui", ""], ["Ding", "Yu", ""]]}, {"id": "2003.07900", "submitter": "Ben Lambert", "authors": "Ben Lambert, Aki Vehtari", "title": "$R^*$: A robust MCMC convergence diagnostic with uncertainty using\n  decision tree classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) has transformed Bayesian model inference over\nthe past three decades: mainly because of this, Bayesian inference is now a\nworkhorse of applied scientists. Under general conditions, MCMC sampling\nconverges asymptotically to the posterior distribution, but this provides no\nguarantees about its performance in finite time. The predominant method for\nmonitoring convergence is to run multiple chains and monitor individual chains'\ncharacteristics and compare these to the population as a whole: if within-chain\nand between-chain summaries are comparable, then this is taken to indicate that\nthe chains have converged to a common stationary distribution. Here, we\nintroduce a new method for diagnosing convergence based on how well a machine\nlearning classifier model can successfully discriminate the individual chains.\nWe call this convergence measure $R^*$. In contrast to the predominant\n$\\widehat{R}$, $R^*$ is a single statistic across all parameters that indicates\nlack of mixing, although individual variables' importance for this metric can\nalso be determined. Additionally, $R^*$ is not based on any single\ncharacteristic of the sampling distribution; instead it uses all the\ninformation in the chain, including that given by the joint sampling\ndistribution, which is currently largely overlooked by existing approaches. We\nrecommend calculating $R^*$ using two different machine learning classifiers -\ngradient-boosted regression trees and random forests - which each work well in\nmodels of different dimensions. Because each of these methods outputs a\nclassification probability, as a byproduct, we obtain uncertainty in $R^*$. The\nmethod is straightforward to implement and could be a complementary additional\ncheck on MCMC convergence for applied analyses.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 19:17:11 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 17:14:39 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 11:36:25 GMT"}, {"version": "v4", "created": "Fri, 4 Sep 2020 12:14:57 GMT"}, {"version": "v5", "created": "Thu, 19 Nov 2020 10:00:45 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Lambert", "Ben", ""], ["Vehtari", "Aki", ""]]}, {"id": "2003.07953", "submitter": "Shounak Chattopadhyay", "authors": "Shounak Chattopadhyay, Antik Chakraborty, David B. Dunson", "title": "Nearest Neighbor Dirichlet Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There is a rich literature on Bayesian nonparametric methods for unknown\ndensities. The most popular approach relies on Dirichlet process mixture\nmodels. These models characterize the unknown density as a kernel convolution\nwith an unknown almost surely discrete mixing measure, which is given a\nDirichlet process prior. Such models are very flexible and have good\nperformance in many settings, but posterior computation typically relies on\nMarkov chain Monte Carlo algorithms that can be complex and inefficient. As a\nsimple alternative, we propose a class of nearest neighbor-Dirichlet processes.\nThe approach starts by grouping the data into neighborhoods based on standard\nalgorithms. Within each neighborhood, the density is characterized via a\nBayesian parametric model, such as a Gaussian with unknown parameters.\nAssigning a Dirichlet prior to the weights on these local kernels, we obtain a\nsimple pseudo-posterior for the weights and kernel parameters. A simple and\nembarrassingly parallel Monte Carlo algorithm is proposed to sample from the\nresulting pseudo-posterior for the unknown density. Desirable asymptotic\nproperties are shown, and the methods are evaluated in simulation studies and\napplied to a motivating data set in the context of classification.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 21:39:11 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 00:21:27 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Chattopadhyay", "Shounak", ""], ["Chakraborty", "Antik", ""], ["Dunson", "David B.", ""]]}, {"id": "2003.07991", "submitter": "Yuming Chen", "authors": "Daniele Bigoni, Yuming Chen, Nicolas Garcia Trillos, Youssef Marzouk,\n  Daniel Sanz-Alonso", "title": "Data-Driven Forward Discretizations for Bayesian Inversion", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/abb2fa", "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper suggests a framework for the learning of discretizations of\nexpensive forward models in Bayesian inverse problems. The main idea is to\nincorporate the parameters governing the discretization as part of the unknown\nto be estimated within the Bayesian machinery. We numerically show that in a\nvariety of inverse problems arising in mechanical engineering, signal\nprocessing and the geosciences, the observations contain useful information to\nguide the choice of discretization.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 00:08:12 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 20:44:33 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Bigoni", "Daniele", ""], ["Chen", "Yuming", ""], ["Trillos", "Nicolas Garcia", ""], ["Marzouk", "Youssef", ""], ["Sanz-Alonso", "Daniel", ""]]}, {"id": "2003.07998", "submitter": "Hsien-Wei Chen", "authors": "Hsien-Wei Chen", "title": "Modeling of Multisite Precipitation Occurrences Using Latent\n  Gaussian-based Multivariate Binary Response Time Series", "comments": null, "journal-ref": "Journal of Hydrology 2020, Vol 590, 125069", "doi": "10.1016/j.jhydrol.2020.125069", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new stochastic model for daily precipitation occurrence processes observed\nat multiple locations is developed. The modeling concept is to use the\nindicator function and the elliptical shape of multivariate Gaussian\ndistribution to represent the joint probabilities of daily precipitation\noccurrences. By using this concept, the number of parameters needed for\nprecipitation occurrence modeling can be largely reduced when compared to the\ncommonly used two-state Markov chain approach. With this parameter reduction,\nthe modeling of spatio-temporal dependence of daily precipitation occurrence\nprocesses observed at different locations is no longer difficult. Results of an\nillustrative application using the precipitation record available from a\nnetwork of ten raingauges in the southern Quebec region, also demonstrate the\naccuracy and the feasibility of the proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 00:51:36 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Chen", "Hsien-Wei", ""]]}, {"id": "2003.08421", "submitter": "Davide Viviano Mr.", "authors": "Davide Viviano", "title": "Experimental Design under Network Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper discusses the problem of the design of a two-wave experiment under\nnetwork interference. We consider (i) a possibly fully connected network, (ii)\nspillover effects occurring across neighbors, (iii) local dependence of\nunobservables characteristics. We allow for a class of estimands of interest\nwhich includes the average effect of treating the entire network, the average\nspillover effects, average direct effects, and interactions of the latter two.\nWe propose a design mechanism where the experimenter optimizes over\nparticipants and treatment assignments to minimize the variance of the\nestimators of interest, using the first-wave experiment for estimation of the\nvariance. We characterize conditions on the first and second wave experiments\nto guarantee unconfounded experimentation, we showcase tradeoffs in the choice\nof the pilot's size, and we formally characterize the pilot's size relative to\nthe main experiment. We derive asymptotic properties of estimators of interest\nunder the proposed design mechanism and regret guarantees of the proposed\nmethod. Finally we illustrate the advantage of the method over state-of-art\nmethodologies on simulated and real-world networks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 18:22:36 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 16:58:53 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 02:44:22 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Viviano", "Davide", ""]]}, {"id": "2003.08449", "submitter": "Tyrel Stokes", "authors": "Tyrel Stokes, Russell Steele, Ian Shrier", "title": "Causal Simulation Experiments: Lessons from Bias Amplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent theoretical work in causal inference has explored an important class\nof variables which, when conditioned on, may further amplify existing\nunmeasured confounding bias (bias amplification). Despite this theoretical\nwork, existing simulations of bias amplification in clinical settings have\nsuggested bias amplification may not be as important in many practical cases as\nsuggested in the theoretical literature.We resolve this tension by using tools\nfrom the semi-parametric regression literature leading to a general\ncharacterization in terms of the geometry of OLS estimators which allows us to\nextend current results to a larger class of DAGs, functional forms, and\ndistributional assumptions. We further use these results to understand the\nlimitations of current simulation approaches and to propose a new framework for\nperforming causal simulation experiments to compare estimators. We then\nevaluate the challenges and benefits of extending this simulation approach to\nthe context of a real clinical data set with a binary treatment, laying the\ngroundwork for a principled approach to sensitivity analysis for bias\namplification in the presence of unmeasured confounding.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 19:33:16 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Stokes", "Tyrel", ""], ["Steele", "Russell", ""], ["Shrier", "Ian", ""]]}, {"id": "2003.08670", "submitter": "Takashi Takahashi", "authors": "Takashi Takahashi, Yoshiyuki Kabashima", "title": "Semi-analytic approximate stability selection for correlated data in\n  generalized linear models", "comments": "34 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the variable selection problem of generalized linear models\n(GLMs). Stability selection (SS) is a promising method proposed for solving\nthis problem. Although SS provides practical variable selection criteria, it is\ncomputationally demanding because it needs to fit GLMs to many re-sampled\ndatasets. We propose a novel approximate inference algorithm that can conduct\nSS without the repeated fitting. The algorithm is based on the replica method\nof statistical mechanics and vector approximate message passing of information\ntheory. For datasets characterized by rotation-invariant matrix ensembles, we\nderive state evolution equations that macroscopically describe the dynamics of\nthe proposed algorithm. We also show that their fixed points are consistent\nwith the replica symmetric solution obtained by the replica method. Numerical\nexperiments indicate that the algorithm exhibits fast convergence and high\napproximation accuracy for both synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 10:43:12 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 03:10:40 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Takahashi", "Takashi", ""], ["Kabashima", "Yoshiyuki", ""]]}, {"id": "2003.08817", "submitter": "Adrian Bowman Prof.", "authors": "Stanislav Katina and Liberty Vittert and Adrian W. Bowman", "title": "Functional Data Analysis and Visualisation of Three-dimensional Surface\n  Shape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of high resolution imaging has made data on surface shape\nwidespread. Methods for the analysis of shape based on landmarks are well\nestablished but high resolution data require a functional approach. The\nstarting point is a systematic and consistent description of each surface\nshape. Three innovative forms of analysis are then introduced. The first uses\nsurface integration to address issues of registration, principal component\nanalysis and the measurement of asymmetry, all in functional form.\nComputational issues are handled through discrete approximations to integrals,\nbased in this case on appropriate surface area weighted sums. The second\ninnovation is to focus on sub-spaces where interesting behaviour such as group\ndifferences are exhibited, rather than on individual principal components. The\nthird innovation concerns the comparison of individual shapes with a relevant\ncontrol set, where the concept of a normal range is extended to the highly\nmultivariate setting of surface shape. This has particularly strong\napplications to medical contexts where the assessment of individual patients is\nvery important. All of these ideas are developed and illustrated in the\nimportant context of human facial shape, with a strong emphasis on the\neffective visual communication of effects of interest.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 13:54:24 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Katina", "Stanislav", ""], ["Vittert", "Liberty", ""], ["Bowman", "Adrian W.", ""]]}, {"id": "2003.08858", "submitter": "Frederic Schoenberg", "authors": "Frederic Paik Schoenberg", "title": "Nonparametric estimation of variable productivity Hawkes processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An extension of the Hawkes model where the productivity is variable is\nconsidered. In particular, the case is considered where each point may have its\nown productivity and a simple analytic formula is derived for the maximum\nlikelihood estimators of these productivities. This estimator is compared with\nan empirical estimator and ways are explored of stabilizing both estimators by\nlower truncating, smoothing, and rescaling the estimates. Properties of the\nestimators are explored in simulations, and the methods are applied to\nseismological and epidemic datasets to show and quantify substantial variation\nin productivity.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 15:15:05 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Schoenberg", "Frederic Paik", ""]]}, {"id": "2003.08921", "submitter": "Jack Kennedy", "authors": "Jack C. Kennedy, Daniel A. Henderson, Kevin J. Wilson", "title": "Multilevel Emulation for Stochastic Computer Models with Application to\n  Large Offshore Wind farms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large renewable energy projects, such as large offshore wind farms, are\ncritical to achieving low-emission targets set by governments. Stochastic\ncomputer models allow us to explore future scenarios to aid decision making\nwhilst considering the most relevant uncertainties. Complex stochastic computer\nmodels can be prohibitively slow and thus an emulator may be constructed and\ndeployed to allow for efficient computation. We present a novel heteroscedastic\nGaussian Process emulator which exploits cheap approximations to a stochastic\noffshore wind farm simulator. We conduct a probabilistic sensitivity analysis\nto understand the influence of key parameters in the wind farm simulator which\nwill help us to plan a probability elicitation in the future.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 17:36:22 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 08:33:20 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 14:03:36 GMT"}, {"version": "v4", "created": "Tue, 13 Oct 2020 15:08:16 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kennedy", "Jack C.", ""], ["Henderson", "Daniel A.", ""], ["Wilson", "Kevin J.", ""]]}, {"id": "2003.08965", "submitter": "Katrin Madjar", "authors": "Katrin Madjar and J\\\"org Rahnenf\\\"uhrer", "title": "Weighted Cox regression for the prediction of heterogeneous patient\n  subgroups", "comments": "under review, 15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in clinical medicine is the construction of risk prediction\nmodels for specific subgroups of patients based on high-dimensional molecular\nmeasurements such as gene expression data. Major objectives in modeling\nhigh-dimensional data are good prediction performance and feature selection to\nfind a subset of predictors that are truly associated with a clinical outcome\nsuch as a time-to-event endpoint. In clinical practice, this task is\nchallenging since patient cohorts are typically small and can be heterogeneous\nwith regard to their relationship between predictors and outcome. When data of\nseveral subgroups of patients with the same or similar disease are available,\nit is tempting to combine them to increase sample size, such as in multicenter\nstudies. However, heterogeneity between subgroups can lead to biased results\nand subgroup-specific effects may remain undetected. For this situation, we\npropose a penalized Cox regression model with a weighted version of the Cox\npartial likelihood that includes patients of all subgroups but assigns them\nindividual weights based on their subgroup affiliation. Patients who are likely\nto belong to the subgroup of interest obtain higher weights in the\nsubgroup-specific model. Our proposed approach is evaluated through simulations\nand application to real lung cancer cohorts. Simulation results demonstrate\nthat our model can achieve improved prediction and variable selection accuracy\nover standard approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 18:17:54 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Madjar", "Katrin", ""], ["Rahnenf\u00fchrer", "J\u00f6rg", ""]]}, {"id": "2003.09202", "submitter": "David Mori\\~na Prof.", "authors": "David Mori\\~na, Amanda Fern\\'andez-Fontelo, Alejandra Caba\\~na, Pedro\n  Puig", "title": "New statistical model for misreported data with application to current\n  public health challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main goal of this work is to present a new model able to deal with\npotentially misreported continuous time series. The proposed model is able to\nhandle the autocorrelation structure in continuous time series data, which\nmight be partially or totally underreported or overreported. Its performance is\nillustrated through a comprehensive simulation study considering several\nautocorrelation structures and two real data applications on human\npapillomavirus incidence in Girona (Catalunya, Spain) and COVID-19 incidence in\nthe Chinese region of Heilongjiang.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 11:32:06 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 08:27:35 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Mori\u00f1a", "David", ""], ["Fern\u00e1ndez-Fontelo", "Amanda", ""], ["Caba\u00f1a", "Alejandra", ""], ["Puig", "Pedro", ""]]}, {"id": "2003.09276", "submitter": "Richard Tol", "authors": "Richard S.J. Tol", "title": "Kernel density decomposition with an application to the social cost of\n  carbon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A kernel density is an aggregate of kernel functions, which are itself\ndensities and could be kernel densities. This is used to decompose a kernel\ninto its constituent parts. Pearson's test for equality of proportions is\napplied to quantiles to test whether the component distributions differ from\none another. The proposed methods are illustrated with a meta-analysis of the\nsocial cost of carbon. Different discount rates lead to significantly different\nPigou taxes, but not different growth rates. Estimates have not varied over\ntime. Different authors have contributed different estimates, but these\ndifferences are insignificant. Kernel decomposition can be applied in many\nother fields with discrete explanatory variables.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 13:44:53 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Tol", "Richard S. J.", ""]]}, {"id": "2003.09379", "submitter": "Steven Kleinegesse", "authors": "Steven Kleinegesse, Christopher Drovandi, Michael U. Gutmann", "title": "Sequential Bayesian Experimental Design for Implicit Models via Mutual\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian experimental design (BED) is a framework that uses statistical\nmodels and decision making under uncertainty to optimise the cost and\nperformance of a scientific experiment. Sequential BED, as opposed to static\nBED, considers the scenario where we can sequentially update our beliefs about\nthe model parameters through data gathered in the experiment. A class of models\nof particular interest for the natural and medical sciences are implicit\nmodels, where the data generating distribution is intractable, but sampling\nfrom it is possible. Even though there has been a lot of work on static BED for\nimplicit models in the past few years, the notoriously difficult problem of\nsequential BED for implicit models has barely been touched upon. We address\nthis gap in the literature by devising a novel sequential design framework for\nparameter estimation that uses the Mutual Information (MI) between model\nparameters and simulated data as a utility function to find optimal\nexperimental designs, which has not been done before for implicit models. Our\napproach uses likelihood-free inference by ratio estimation to simultaneously\nestimate posterior distributions and the MI. During the sequential BED\nprocedure we utilise Bayesian optimisation to help us optimise the MI utility.\nWe find that our framework is efficient for the various implicit models tested,\nyielding accurate parameter estimates after only a few iterations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 16:52:10 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Kleinegesse", "Steven", ""], ["Drovandi", "Christopher", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "2003.09467", "submitter": "Li-Chun Zhang", "authors": "Li-Chun Zhang and Melike Oguz-Alper", "title": "BIG sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph sampling is a statistical approach to study real graphs, which\nrepresent the structure of many technological, social or biological phenomena\nof interest. We develop bipartite incident graph sampling (BIGS) as a feasible\nrepresentation of graph sampling from arbitrary finite graphs. It provides also\na unified treatment of the existing unconventional sampling methods which were\nstudied separately in the past, including indirect, network and adaptive\ncluster sampling. The sufficient and necessary conditions of feasible BIGS\nrepresentation are established, given which one can apply a family of\nHansen-Hurwitz type design-unbiased estimators in addition to the standard\nHorvitz-Thompson estimator. The approach increases therefore the potentials of\nefficiency gains in graph sampling. A general result regarding the relative\nefficiency of the two types of estimators is obtained. Numerical examples are\ngiven to illustrate the versatility of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 19:06:05 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Zhang", "Li-Chun", ""], ["Oguz-Alper", "Melike", ""]]}, {"id": "2003.09507", "submitter": "Thomas Muehlenstaedt", "authors": "Thomas Muehlenstaedt, Maria Lanzerath", "title": "Space Filling Split Plot Design using Fast Flexible Filling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, an adaption of an algorithm for the creation of experimental\ndesigns by Lekivetz and Jones (2015) is suggested, dealing with constraints\naround randomization. Split-plot design of experiments is used, when the levels\nof some factors cannot be modified as easily as others. While most split-plot\ndesigns deal in the context of I-optimal or D-optimal designs for continuous\nresponse outputs, a space filling design strategy is suggested in here. The\nproposed designs are evaluated based on different design criteria, as well as\nan analytical example.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 21:39:48 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Muehlenstaedt", "Thomas", ""], ["Lanzerath", "Maria", ""]]}, {"id": "2003.09680", "submitter": "Jeffrey Boatman", "authors": "Jeffrey A. Boatman, David M. Vock, Joseph S. Koopmeiners", "title": "Borrowing from Supplemental Sources to Estimate Causal Effects from a\n  Primary Data Source", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The increasing multiplicity of data sources offers exciting possibilities in\nestimating the effects of a treatment, intervention, or exposure, particularly\nif observational and experimental sources could be used simultaneously.\nBorrowing between sources can potentially result in more efficient estimators,\nbut it must be done in a principled manner to mitigate increased bias and Type\nI error. Furthermore, when the effect of treatment is confounded, as in\nobservational sources or in clinical trials with noncompliance, causal effect\nestimators are needed to simultaneously adjust for confounding and to estimate\neffects across data sources. We consider the problem of estimating causal\neffects from a primary source and borrowing from any number of supplemental\nsources. We propose using regression-based estimators that borrow based on\nassuming exchangeability of the regression coefficients and parameters between\ndata sources. Borrowing is accomplished with multisource exchangeability models\nand Bayesian model averaging. We show via simulation that a Bayesian linear\nmodel and Bayesian additive regression trees both have desirable properties and\nborrow under appropriate circumstances. We apply the estimators to recently\ncompleted trials of very low nicotine content cigarettes investigating their\nimpact on smoking behavior.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 15:20:03 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Boatman", "Jeffrey A.", ""], ["Vock", "David M.", ""], ["Koopmeiners", "Joseph S.", ""]]}, {"id": "2003.09759", "submitter": "Matthew Heiner", "authors": "Matthew Heiner and Athanasios Kottas", "title": "Bayesian Nonparametric Density Autoregression with Lag Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian nonparametric autoregressive model applied to flexibly\nestimate general transition densities exhibiting nonlinear lag dependence. Our\napproach is related to Bayesian density regression using Dirichlet process\nmixtures, with the Markovian likelihood defined through the conditional\ndistribution obtained from the mixture. This results in a Bayesian\nnonparametric extension of a mixtures-of-experts model formulation. We address\ncomputational challenges to posterior sampling that arise from the Markovian\nstructure in the likelihood. The base model is illustrated with synthetic data\nfrom a classical model for population dynamics, as well as a series of waiting\ntimes between eruptions of Old Faithful Geyser. We study inferences available\nthrough the base model before extending the methodology to include automatic\nrelevance detection among a pre-specified set of lags. Inference for global and\nlocal lag selection is explored with additional simulation studies, and the\nmethods are illustrated through analysis of an annual time series of pink\nsalmon abundance in a stream in Alaska. We further explore and compare\ntransition density estimation performance for alternative configurations of the\nproposed model.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 21:59:44 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 19:18:18 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Heiner", "Matthew", ""], ["Kottas", "Athanasios", ""]]}, {"id": "2003.09795", "submitter": "Yanjun Han", "authors": "Yanjun Han, Zhengyuan Zhou, Tsachy Weissman", "title": "Optimal No-regret Learning in Repeated First-price Auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.IT math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online learning in repeated first-price auctions with censored\nfeedback, where a bidder, only observing the winning bid at the end of each\nauction, learns to adaptively bid in order to maximize her cumulative payoff.\nTo achieve this goal, the bidder faces a challenging dilemma: if she wins the\nbid--the only way to achieve positive payoffs--then she is not able to observe\nthe highest bid of the other bidders, which we assume is iid drawn from an\nunknown distribution. This dilemma, despite being reminiscent of the\nexploration-exploitation trade-off in contextual bandits, cannot directly be\naddressed by the existing UCB or Thompson sampling algorithms in that\nliterature, mainly because contrary to the standard bandits setting, when a\npositive reward is obtained here, nothing about the environment can be learned.\n  In this paper, by exploiting the structural properties of first-price\nauctions, we develop the first learning algorithm that achieves\n$O(\\sqrt{T}\\log^2 T)$ regret bound when the bidder's private values are\nstochastically generated. We do so by providing an algorithm on a general class\nof problems, which we call monotone group contextual bandits, where the same\nregret bound is established under stochastically generated contexts. Further,\nby a novel lower bound argument, we characterize an $\\Omega(T^{2/3})$ lower\nbound for the case where the contexts are adversarially generated, thus\nhighlighting the impact of the contexts generation mechanism on the fundamental\nlearning limit. Despite this, we further exploit the structure of first-price\nauctions and develop a learning algorithm that operates sample-efficiently (and\ncomputationally efficiently) in the presence of adversarially generated private\nvalues. We establish an $O(\\sqrt{T}\\log^3 T)$ regret bound for this algorithm,\nhence providing a complete characterization of optimal learning guarantees for\nthis problem.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 03:32:09 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 20:56:11 GMT"}, {"version": "v3", "created": "Tue, 14 Apr 2020 06:42:43 GMT"}, {"version": "v4", "created": "Fri, 8 May 2020 22:18:17 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Han", "Yanjun", ""], ["Zhou", "Zhengyuan", ""], ["Weissman", "Tsachy", ""]]}, {"id": "2003.09830", "submitter": "Ismaila Ba", "authors": "Isma\\\"ila Ba and Jean-Fran\\c{c}ois Coeurjolly", "title": "Inference for possibly high-dimensional inhomogeneous Gibbs point\n  processes", "comments": "55 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs point processes (GPPs) constitute a large and flexible class of spatial\npoint processes with explicit dependence between the points. They can model\nattractive as well as repulsive point patterns. Feature selection procedures\nare an important topic in high-dimensional statistical modeling. In this paper,\ncomposite likelihood approach regularized with convex and non-convex penalty\nfunctions is proposed to handle statistical inference for possibly\nhigh-dimensional inhomogeneous GPPs. The composite likelihood incorporates both\nthe pseudo-likelihood and the logistic composite likelihood. We particularly\ninvestigate the setting where the number of covariates diverges as the domain\nof observation increases. Under some conditions provided on the spatial GPP and\non the penalty functions, we show that the oracle property, the consistency and\nthe asymptotic normality hold. Our results also cover the low-dimensional case\nwhich fills a large gap in the literature. Through simulation experiments, we\nvalidate our theoretical results and finally, an application to a tropical\nforestry dataset illustrates the use of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 07:25:11 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 13:09:30 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Ba", "Isma\u00efla", ""], ["Coeurjolly", "Jean-Fran\u00e7ois", ""]]}, {"id": "2003.09915", "submitter": "Ashesh Rambachan", "authors": "Iavor Bojinov, Ashesh Rambachan, and Neil Shephard", "title": "Panel Experiments and Dynamic Causal Effects: A Finite Population\n  Perspective", "comments": "Forthcoming in Quantitative Economics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In panel experiments, we randomly assign units to different interventions,\nmeasuring their outcomes, and repeating the procedure in several periods. Using\nthe potential outcomes framework, we define finite population dynamic causal\neffects that capture the relative effectiveness of alternative treatment paths.\nFor a rich class of dynamic causal effects, we provide a nonparametric\nestimator that is unbiased over the randomization distribution and derive its\nfinite population limiting distribution as either the sample size or the\nduration of the experiment increases. We develop two methods for inference: a\nconservative test for weak null hypotheses and an exact randomization test for\nsharp null hypotheses. We further analyze the finite population probability\nlimit of linear fixed effects estimators. These commonly-used estimators do not\nrecover a causally interpretable estimand if there are dynamic causal effects\nand serial correlation in the assignments, highlighting the value of our\nproposed estimator.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 14:58:03 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 23:08:59 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 21:34:59 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 19:56:55 GMT"}, {"version": "v5", "created": "Thu, 27 May 2021 13:44:29 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Bojinov", "Iavor", ""], ["Rambachan", "Ashesh", ""], ["Shephard", "Neil", ""]]}, {"id": "2003.09960", "submitter": "Kaizheng Wang", "authors": "Kaizheng Wang, Yuling Yan, Mateo Diaz", "title": "Efficient Clustering for Stretched Mixtures: Landscape and Optimality", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a canonical clustering problem where one receives\nunlabeled samples drawn from a balanced mixture of two elliptical distributions\nand aims for a classifier to estimate the labels. Many popular methods\nincluding PCA and k-means require individual components of the mixture to be\nsomewhat spherical, and perform poorly when they are stretched. To overcome\nthis issue, we propose a non-convex program seeking for an affine transform to\nturn the data into a one-dimensional point cloud concentrating around -1 and 1,\nafter which clustering becomes easy. Our theoretical contributions are\ntwo-fold: (1) we show that the non-convex loss function exhibits desirable\nlandscape properties as long as the sample size exceeds some constant multiple\nof the dimension, and (2) we leverage this to prove that an efficient\nfirst-order algorithm achieves near-optimal statistical precision even without\ngood initialization. We also propose a general methodology for multi-class\nclustering tasks with flexible choices of feature transforms and loss\nobjectives.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 17:57:07 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 17:45:00 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Kaizheng", ""], ["Yan", "Yuling", ""], ["Diaz", "Mateo", ""]]}, {"id": "2003.10021", "submitter": "Gregorio Landi", "authors": "Gregorio Landi, Giovanni E. Landi", "title": "Proofs of non-optimality of the standard least-squares method for track\n  reconstructions", "comments": "It completes the results of INSTRUMENTS 2020 4,2-- 13 pages, 2\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST hep-ex physics.ins-det stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a standard criterium in statistics to define an optimal estimator the\none with the minimum variance. Thus, the optimality is proved with inequality\namong variances of competing estimators. The inequalities, demonstrated here,\ndisfavor the standard least squares estimators. Inequalities among estimators\nare connected to names of Cramer, Rao and Frechet. The standard demonstrations\nof these inequalities require very special analytical properties of the\nprobability functions, globally indicated as regular models. These limiting\nconditions are too restrictive to handle realistic problems in track fitting. A\nprevious extension to heteroscedastic models of the Cramer-Rao-Frechet\ninequalities was performed with Gaussian distributions. These demonstrations\nproved beyond any possible doubts the superiority of the heteroscedastic models\ncompared to the standard least squares method. However, the Gaussian\ndistributions are typical members of the required regular models. Instead, the\nrealistic probability distributions, encountered in tracker detectors, are very\ndifferent from Gaussian distributions. Therefore, to have well grounded set of\ninequalities, the limitations to regular models must be overtaken. The aim of\nthis paper is to demonstrate the inequalities for least squares estimators for\nirregular models of probabilities, explicitly excluded by the\nCramer-Rao-Frechet demonstrations. Estimators for straight and parabolic tracks\nwill be considered. The final part deals with the form of the distributions of\nsimplified heteroscedastic track models reconstructed with optimal estimators\nand the standard (non-optimal) estimators. A comparison among the distributions\nof these different estimators shows the large loss in resolution of the\nstandard least-squares estimators.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 23:03:33 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Landi", "Gregorio", ""], ["Landi", "Giovanni E.", ""]]}, {"id": "2003.10043", "submitter": "Jieying Jiao", "authors": "Jieying Jiao, Guanyu Hu, Jun Yan", "title": "Heterogeneity Pursuit for Spatial Point Pattern with Application to Tree\n  Locations: A Bayesian Semiparametric Recourse", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial point pattern data are routinely encountered. A flexible regression\nmodel for the underlying intensity is essential to characterizing the spatial\npoint pattern and understanding the impacts of potential risk factors on such\npattern. We propose a Bayesian semiparametric regression model where the\nobserved spatial points follow a spatial Poisson process with an intensity\nfunction which adjusts a nonparametric baseline intensity with multiplicative\ncovariate effects. The baseline intensity is piecewise constant, approached\nwith a powered Chinese restaurant process prior which prevents an unnecessarily\nlarge number of pieces. The parametric regression part allows for variable\nselection through the spike-slab prior on the regression coefficients. An\nefficient Markov chain Monte Carlo (MCMC) algorithm is developed for the\nproposed methods. The performance of the methods is validated in an extensive\nsimulation study. In application to the locations of Beilschmiedia pendula\ntrees in the Barro Colorado Island forest dynamics research plot in central\nPanama, the spatial heterogeneity is attributed to a subset of soil\nmeasurements in addition to geographic measurements with a spatially varying\nbaseline intensity.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 01:34:10 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 01:08:54 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Jiao", "Jieying", ""], ["Hu", "Guanyu", ""], ["Yan", "Jun", ""]]}, {"id": "2003.10051", "submitter": "Lu Zhang", "authors": "Lu Zhang, Sudipto Banerjee, Andrew O. Finley", "title": "High-dimensional Multivariate Geostatistics: A Bayesian Matrix-Normal\n  Approach", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": "10.1002/env.2675", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint modeling of spatially-oriented dependent variables is commonplace in\nthe environmental sciences, where scientists seek to estimate the relationships\namong a set of environmental outcomes accounting for dependence among these\noutcomes and the spatial dependence for each outcome. Such modeling is now\nsought for massive data sets with variables measured at a very large number of\nlocations. Bayesian inference, while attractive for accommodating uncertainties\nthrough hierarchical structures, can become computationally onerous for\nmodeling massive spatial data sets because of its reliance on iterative\nestimation algorithms. This manuscript develops a conjugate Bayesian framework\nfor analyzing multivariate spatial data using analytically tractable posterior\ndistributions that obviate iterative algorithms. We discuss differences between\nmodeling the multivariate response itself as a spatial process and that of\nmodeling a latent process in a hierarchical model. We illustrate the\ncomputational and inferential benefits of these models using simulation studies\nand analysis of a Vegetation Index data set with spatially dependent\nobservations numbering in the millions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 02:17:21 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 05:39:06 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 05:40:43 GMT"}, {"version": "v4", "created": "Thu, 10 Sep 2020 18:52:47 GMT"}, {"version": "v5", "created": "Mon, 7 Dec 2020 01:44:38 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Zhang", "Lu", ""], ["Banerjee", "Sudipto", ""], ["Finley", "Andrew O.", ""]]}, {"id": "2003.10285", "submitter": "Yong He", "authors": "Long Yu, Yong He, Xin-bing Kong, Xinsheng Zhang", "title": "Projected Estimation for Large-dimensional Matrix Factor Models", "comments": "This is a revised version of the original paper \"Two-stage Projected\n  Estimation for Large-dimensional Matrix Factor Models\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a projection estimation method for\nlarge-dimensional matrix factor models with cross-sectionally spiked\neigenvalues. By projecting the observation matrix onto the row or column factor\nspace, we simplify factor analysis for matrix series to that for a\nlower-dimensional tensor. This method also reduces the magnitudes of the\nidiosyncratic error components, thereby increasing the signal-to-noise ratio,\nbecause the projection matrix linearly filters the idiosyncratic error matrix.\nWe theoretically prove that the projected estimators of the factor loading\nmatrices achieve faster convergence rates than existing estimators under\nsimilar conditions. Asymptotic distributions of the projected estimators are\nalso presented. A novel iterative procedure is given to specify the pair of row\nand column factor numbers. Extensive numerical studies verify the empirical\nperformance of the projection method. Two real examples in finance and\nmacroeconomics reveal factor patterns across rows and columns, which coincides\nwith financial, economic, or geographical interpretations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 14:07:28 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 11:26:08 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 11:23:19 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Yu", "Long", ""], ["He", "Yong", ""], ["Kong", "Xin-bing", ""], ["Zhang", "Xinsheng", ""]]}, {"id": "2003.10341", "submitter": "Ryan Andrews", "authors": "Ryan M. Andrews, Vanessa Didelez", "title": "Insights into the \"cross-world\" independence assumption of causal\n  mediation analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis is a useful tool for epidemiological research, but\nit has been criticized for relying on a \"cross-world\" independence assumption\nthat is empirically difficult to verify and problematic to justify based on\nbackground knowledge. In the present article we aim to assist the applied\nresearcher in understanding this assumption. Synthesizing what is known about\nthe cross-world independence assumption, we discuss the relationship between\nassumptions for causal mediation analyses, causal models, and non-parametric\nidentification of natural direct and indirect effects. In particular we give a\npractical example of an applied setting where the cross-world independence\nassumption is violated even without any post-treatment confounding. Further, we\nreview possible alternatives to the cross-world independence assumption,\nincluding the use of computation of bounds that avoid the assumption\naltogether. Finally, we carry out a numerical study in which the cross-world\nindependence assumption is violated to assess the ensuing bias in estimating\nnatural direct and indirect effects. We conclude with recommendations for\ncarrying out causal mediation analyses.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 16:03:25 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 10:05:06 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Andrews", "Ryan M.", ""], ["Didelez", "Vanessa", ""]]}, {"id": "2003.10374", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth and Fredrik Lindsten and David Blei", "title": "Markovian Score Climbing: Variational Inference with KL(p||q)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern variational inference (VI) uses stochastic gradients to avoid\nintractable expectations, enabling large-scale probabilistic inference in\ncomplex models. VI posits a family of approximating distributions q and then\nfinds the member of that family that is closest to the exact posterior p.\nTraditionally, VI algorithms minimize the \"exclusive Kullback-Leibler (KL)\"\nKL(q || p), often for computational convenience. Recent research, however, has\nalso focused on the \"inclusive KL\" KL(p || q), which has good statistical\nproperties that makes it more appropriate for certain inference problems. This\npaper develops a simple algorithm for reliably minimizing the inclusive KL\nusing stochastic gradients with vanishing bias. This method, which we call\nMarkovian score climbing (MSC), converges to a local optimum of the inclusive\nKL. It does not suffer from the systematic errors inherent in existing methods,\nsuch as Reweighted Wake-Sleep and Neural Adaptive Sequential Monte Carlo, which\nlead to bias in their final estimates. We illustrate convergence on a toy model\nand demonstrate the utility of MSC on Bayesian probit regression for\nclassification as well as a stochastic volatility model for financial data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 16:38:10 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 19:46:38 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Lindsten", "Fredrik", ""], ["Blei", "David", ""]]}, {"id": "2003.10442", "submitter": "Shannon Gallagher", "authors": "Shannon Gallagher, Andersen Chang, William F. Eddy", "title": "Exploring the nuances of R0: Eight estimates and application to 2009\n  pandemic influenza", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For nearly a century, the initial reproduction number (R0) has been used as a\none number summary to compare outbreaks of infectious disease, yet there is no\n`standard' estimator for R0. Difficulties in estimating R0 arise both from how\na disease transmits through a population as well as from differences in\nstatistical estimation method. We describe eight methods used to estimate R0\nand provide a thorough simulation study of how these estimates change in the\npresence of different disease parameters. As motivation, we analyze the 2009\noutbreak of the H1N1 pandemic influenza in the USA and compare the results from\nour eight methods to a previous study. We discuss the most important aspects\nfrom our results which effect the estimation of R0, which include the\npopulation size, time period used, and the initial percent of infectious\nindividuals. Additionally, we discuss how pre-processing incidence counts may\neffect estimates of R0. Finally, we provide guidelines for estimating point\nestimates and confidence intervals to create reliable, comparable estimates of\nR0.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 16:09:08 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Gallagher", "Shannon", ""], ["Chang", "Andersen", ""], ["Eddy", "William F.", ""]]}, {"id": "2003.10490", "submitter": "Ninna Vihrs", "authors": "Ninna Vihrs, Jesper M{\\o}ller and Alan E. Gelfand", "title": "Approximate Bayesian inference for a spatial point process model\n  exhibiting regularity and random aggregation", "comments": "37 pages, 10 figures; one line was added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a doubly stochastic spatial point process model\nwith both aggregation and repulsion. This model combines the ideas behind\nStrauss processes and log Gaussian Cox processes. The likelihood for this model\nis not expressible in closed form but it is easy to simulate realisations under\nthe model. We therefore explain how to use approximate Bayesian computation\n(ABC) to carry out statistical inference for this model. We suggest a method\nfor model validation based on posterior predictions and global envelopes. We\nillustrate the ABC procedure and model validation approach using both simulated\npoint patterns and a real data example.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 18:40:57 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 09:53:03 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 07:54:00 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Vihrs", "Ninna", ""], ["M\u00f8ller", "Jesper", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "2003.10525", "submitter": "Costanza Tort\\`u", "authors": "C. Tort\\`u, I. Crimaldi, F. Mealli, L. Forastiere", "title": "Modelling Network Interference with Multi-valued Treatments: the Causal\n  Effect of Immigration Policy on Crime Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy evaluation studies, which intend to assess the effect of an\nintervention, face some statistical challenges: in real-world settings\ntreatments are not randomly assigned and the analysis might be further\ncomplicated by the presence of interference between units. Researchers have\nstarted to develop novel methods that allow to manage spillover mechanisms in\nobservational studies; recent works focus primarily on binary treatments.\nHowever, many policy evaluation studies deal with more complex interventions.\nFor instance, in political science, evaluating the impact of policies\nimplemented by administrative entities often implies a multivariate approach,\nas a policy towards a specific issue operates at many different levels and can\nbe defined along a number of dimensions. In this work, we extend the\nstatistical framework about causal inference under network interference in\nobservational studies, allowing for a multi-valued individual treatment and an\ninterference structure shaped by a weighted network. The estimation strategy is\nbased on a joint multiple generalized propensity score and allows one to\nestimate direct effects, controlling for both individual and network\ncovariates. We follow the proposed methodology to analyze the impact of the\nnational immigration policy on the crime rate. We define a multi-valued\ncharacterization of political attitudes towards migrants and we assume that the\nextent to which each country can be influenced by another country is modeled by\nan appropriate indicator, summarizing their cultural and geographical\nproximity. Results suggest that implementing a highly restrictive immigration\npolicy leads to an increase of the crime rate and the estimated effects is\nlarger if we take into account interference from other countries.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 11:17:00 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 08:42:31 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 19:00:01 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Tort\u00f9", "C.", ""], ["Crimaldi", "I.", ""], ["Mealli", "F.", ""], ["Forastiere", "L.", ""]]}, {"id": "2003.10592", "submitter": "Yuan Tian", "authors": "Yuan Tian, Brian J. Reich", "title": "A Bayesian semi-parametric hybrid model for spatial extremes with\n  unknown dependence structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The max-stable process is an asymptotically justified model for spatial\nextremes. In particular, we focus on the hierarchical extreme-value process\n(HEVP), which is a particular max-stable process that is conducive to Bayesian\ncomputing. The HEVP and all max-stable process models are parametric and impose\nstrong assumptions including that all marginal distributions belong to the\ngeneralized extreme value family and that nearby sites are asymptotically\ndependent. We generalize the HEVP by relaxing these assumptions to provide a\nwider class of marginal distributions via a Dirichlet process prior for the\nspatial random effects distribution. In addition, we present a hybrid\nmax-mixture model that combines the strengths of the parametric and\nsemi-parametric models. We show that this versatile max-mixture model\naccommodates both asymptotic independence and dependence and can be fit using\nstandard Markov chain Monte Carlo algorithms. The utility of our model is\nevaluated in Monte Carlo simulation studies and application to Netherlands wind\ngust data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 00:40:29 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Tian", "Yuan", ""], ["Reich", "Brian J.", ""]]}, {"id": "2003.10609", "submitter": "Xinlian Zhang", "authors": "Cheng Meng, Xinlian Zhang, Jingyi Zhang, Wenxuan Zhong, Ping Ma", "title": "More efficient approximation of smoothing splines via space-filling\n  basis selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating smoothing spline estimators in a\nnonparametric regression model. When applied to a sample of size $n$, the\nsmoothing spline estimator can be expressed as a linear combination of $n$\nbasis functions, requiring $O(n^3)$ computational time when the number of\npredictors $d\\geq 2$. Such a sizable computational cost hinders the broad\napplicability of smoothing splines. In practice, the full sample smoothing\nspline estimator can be approximated by an estimator based on $q$\nrandomly-selected basis functions, resulting in a computational cost of\n$O(nq^2)$. It is known that these two estimators converge at the identical rate\nwhen $q$ is of the order $O\\{n^{2/(pr+1)}\\}$, where $p\\in [1,2]$ depends on the\ntrue function $\\eta$, and $r > 1$ depends on the type of spline. Such $q$ is\ncalled the essential number of basis functions. In this article, we develop a\nmore efficient basis selection method. By selecting the ones corresponding to\nroughly equal-spaced observations, the proposed method chooses a set of basis\nfunctions with a large diversity. The asymptotic analysis shows our proposed\nsmoothing spline estimator can decrease $q$ to roughly $O\\{n^{1/(pr+1)}\\}$,\nwhen $d\\leq pr+1$. Applications on synthetic and real-world datasets show the\nproposed method leads to a smaller prediction error compared with other basis\nselection methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 01:46:24 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Meng", "Cheng", ""], ["Zhang", "Xinlian", ""], ["Zhang", "Jingyi", ""], ["Zhong", "Wenxuan", ""], ["Ma", "Ping", ""]]}, {"id": "2003.10702", "submitter": "Michael Sachs", "authors": "Michael C Sachs, Erin E Gabriel, Arvid Sj\\\"olander", "title": "Symbolic Computation of Tight Causal Bounds", "comments": "Submitted to JRSS-B in March 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference involves making a set of assumptions about the nature of\nthings, defining a causal query, and attempting to find estimators of the query\nbased on the distribution of observed variables. When causal queries are not\nidentifiable from the observed data, it still may be possible to derive bounds\nfor these quantities in terms of the distribution of observed variables. We\ndevelop and describe a general approach for computation of bounds, proving that\nif the problem can be stated as a linear program, then the true global extrema\nresult in tight bounds. Building upon previous work in this area, we\ncharacterize a class of problems that can always be stated as a linear\nprogramming problem; we describe a general algorithm for constructing the\nlinear objective and constraints based on the causal model and the causal query\nof interest. These problems therefore can be solved using a vertex enumeration\nalgorithm. We develop an R package implementing this algorithm with a user\nfriendly graphical interface using directed acyclic graphs, which only allows\nfor problems within this class to be depicted. We have implemented additional\nfeatures to help with interpreting and applying the bounds that we illustrate\nin examples.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 07:50:40 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Sachs", "Michael C", ""], ["Gabriel", "Erin E", ""], ["Sj\u00f6lander", "Arvid", ""]]}, {"id": "2003.10726", "submitter": "Yue Su", "authors": "Yue Su and Patrick Kandege Mwanakatwe", "title": "Model selection criteria of the standard censored regression model based\n  on the bootstrap sample augmentation mechanism", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical regression technique is an extraordinarily essential data\nfitting tool to explore the potential possible generation mechanism of the\nrandom phenomenon. Therefore, the model selection or the variable selection is\nbecoming extremely important so as to identify the most appropriate model with\nthe most optimal explanation effect on the interesting response. In this paper,\nwe discuss and compare the bootstrap-based model selection criteria on the\nstandard censored regression model (Tobit regression model) under the\ncircumstance of limited observation information. The Monte Carlo numerical\nevidence demonstrates that the performances of the model selection criteria\nbased on the bootstrap sample augmentation strategy will become more\ncompetitive than their alternative ones, such as the Akaike Information\nCriterion (AIC) and the Bayesian Information Criterion (BIC) etc. under the\ncircumstance of the inadequate observation information. Meanwhile, the\nnumerical simulation experiments further demonstrate that the model\nidentification risk due to the deficiency of the data information, such as the\nhigh censoring rate and rather limited number of observations, can be\nadequately compensated by increasing the scientific computation cost in terms\nof the bootstrap sample augmentation strategies. We also apply the recommended\nbootstrap-based model selection criterion on the Tobit regression model to fit\nthe real fidelity dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 09:19:27 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Su", "Yue", ""], ["Mwanakatwe", "Patrick Kandege", ""]]}, {"id": "2003.10793", "submitter": "Yuehan Yang", "authors": "Yuehan Yang, Siwei Xia and Hu Yang", "title": "Interaction Pursuit Biconvex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate regression models are widely used in various fields such as\nbiology and finance. In this paper, we focus on two key challenges: (a) When\nshould we favor a multivariate model over a series of univariate models; (b) If\nthe numbers of responses and predictors are allowed to greatly exceed the\nsample size, how to reduce the computational cost and provide precise\nestimation. The proposed method, Interaction Pursuit Biconvex Optimization\n(IPBO), explores the regression relationship allowing the predictors and\nresponses derived from different multivariate normal distributions with general\ncovariance matrices. In practice, the correlation structures within are complex\nand interact on each other based on the regression function. The proposed\nmethod solves this problem by building a structured sparsity penalty to\nencourages the shared structure between the network and the regression\ncoefficients. We prove theoretical results under interpretable conditions, and\nprovide an efficient algorithm to compute the estimator. Simulation studies and\nreal data examples compare the proposed method with several existing methods,\nindicating that IPBO works well.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 12:10:09 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Yang", "Yuehan", ""], ["Xia", "Siwei", ""], ["Yang", "Hu", ""]]}, {"id": "2003.11181", "submitter": "Rui Duan", "authors": "Rui Duan, C. Jason Liang, Pamela Shaw, Cheng Yong Tang and Yong Chen", "title": "Missing at Random or Not: A Semiparametric Testing Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical problems with missing data are common, and statistical methods have\nbeen developed concerning the validity and/or efficiency of statistical\nprocedures. On a central focus, there have been longstanding interests on the\nmechanism governing data missingness, and correctly deciding the appropriate\nmechanism is crucially relevant for conducting proper practical investigations.\nThe conventional notions include the three common potential classes -- missing\ncompletely at random, missing at random, and missing not at random. In this\npaper, we present a new hypothesis testing approach for deciding between\nmissing at random and missing not at random. Since the potential alternatives\nof missing at random are broad, we focus our investigation on a general class\nof models with instrumental variables for data missing not at random. Our\nsetting is broadly applicable, thanks to that the model concerning the missing\ndata is nonparametric, requiring no explicit model specification for the data\nmissingness. The foundational idea is to develop appropriate discrepancy\nmeasures between estimators whose properties significantly differ only when\nmissing at random does not hold. We show that our new hypothesis testing\napproach achieves an objective data oriented choice between missing at random\nor not. We demonstrate the feasibility, validity, and efficacy of the new test\nby theoretical analysis, simulation studies, and a real data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 02:09:42 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Duan", "Rui", ""], ["Liang", "C. Jason", ""], ["Shaw", "Pamela", ""], ["Tang", "Cheng Yong", ""], ["Chen", "Yong", ""]]}, {"id": "2003.11188", "submitter": "Hamdi Raissi", "authors": "Valentin Patilea and Hamdi Ra\\\"issi", "title": "Orthogonal Impulse Response Analysis in Presence of Time-Varying\n  Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the orthogonal impulse response functions (OIRF) are studied in\nthe non-standard, though quite common, case where the covariance of the error\nvector is not constant in time. The usual approach for taking into account such\nbehavior of the covariance consists in applying the standard tools to\nsub-periods of the whole sample. We underline that such a practice may lead to\nsevere upward bias. We propose a new approach intended to give what we argue to\nbe a more accurate resume of the time-varying OIRF. This consists in averaging\nthe Cholesky decomposition of nonparametric covariance estimators. In addition\nan index is developed to evaluate the heteroscedasticity effect on the OIRF\nanalysis. The asymptotic behavior of the different estimators considered in the\npaper is investigated. The theoretical results are illustrated by Monte Carlo\nexperiments. The analysis of the orthogonal response functions of the U.S.\ninflation to an oil price shock, shows the relevance of the tools proposed\nherein for an appropriate analysis of economic variables.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 02:39:46 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 17:07:22 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Patilea", "Valentin", ""], ["Ra\u00efssi", "Hamdi", ""]]}, {"id": "2003.11194", "submitter": "Steven Schiff", "authors": "Donald Ebeigbe, Tyrus Berry, Steven J. Schiff, and Timothy Sauer", "title": "A Poisson Kalman filter for disease surveillance", "comments": "19 Pages, 8 Figures", "journal-ref": "Phys. Rev. Research 2, 043028 (2020)", "doi": "10.1103/PhysRevResearch.2.043028", "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An optimal filter for Poisson observations is developed as a variant of the\ntraditional Kalman filter. Poisson distributions are characteristic of\ninfectious diseases, which model the number of patients recorded as presenting\neach day to a health care system. We develop both a linear and nonlinear\n(extended) filter. The methods are applied to a case study of neonatal sepsis\nand postinfectious hydrocephalus in Africa, using parameters estimated from\npublicly available data. Our approach is applicable to a broad range of disease\ndynamics, including both noncommunicable and the inherent nonlinearities of\ncommunicable infectious diseases and epidemics such as from COVID-19.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 02:55:10 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 21:36:56 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2020 02:42:35 GMT"}, {"version": "v4", "created": "Sun, 12 Jul 2020 20:38:23 GMT"}, {"version": "v5", "created": "Mon, 14 Sep 2020 03:16:58 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Ebeigbe", "Donald", ""], ["Berry", "Tyrus", ""], ["Schiff", "Steven J.", ""], ["Sauer", "Timothy", ""]]}, {"id": "2003.11208", "submitter": "Michele Peruzzi", "authors": "Michele Peruzzi, Sudipto Banerjee, Andrew O. Finley", "title": "Highly Scalable Bayesian Geostatistical Modeling via Meshed Gaussian\n  Processes on Partitioned Domains", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2020.1833889", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of scalable Bayesian hierarchical models for the\nanalysis of massive geostatistical datasets. The underlying idea combines ideas\non high-dimensional geostatistics by partitioning the spatial domain and\nmodeling the regions in the partition using a sparsity-inducing directed\nacyclic graph (DAG). We extend the model over the DAG to a well-defined spatial\nprocess, which we call the Meshed Gaussian Process (MGP). A major contribution\nis the development of a MGPs on tessellated domains, accompanied by a Gibbs\nsampler for the efficient recovery of spatial random effects. In particular,\nthe cubic MGP (Q-MGP) can harness high-performance computing resources by\nexecuting all large-scale operations in parallel within the Gibbs sampler,\nimproving mixing and computing time compared to sequential updating schemes.\nUnlike some existing models for large spatial data, a Q-MGP facilitates massive\ncaching of expensive matrix operations, making it particularly apt in dealing\nwith spatiotemporal remote-sensing data. We compare Q-MGPs with large synthetic\nand real world data against state-of-the-art methods. We also illustrate using\nNormalized Difference Vegetation Index (NDVI) data from the Serengeti park\nregion to recover latent multivariate spatiotemporal random effects at millions\nof locations. The source code is available at https://github.com/mkln/meshgp.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 04:15:23 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:51:18 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Peruzzi", "Michele", ""], ["Banerjee", "Sudipto", ""], ["Finley", "Andrew O.", ""]]}, {"id": "2003.11542", "submitter": "Zhiyang Zhou", "authors": "Zhiyang Zhou and Richard A. Lockhart", "title": "Partial least squares for sparsely observed curves with measurement\n  errors", "comments": "42 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional partial least squares (FPLS) is commonly used for fitting\nscalar-on-function regression models. For the sake of accuracy, FPLS demands\nthat each realization of the functional predictor is recorded as densely as\npossible over the entire time span; however, this condition is sometimes\nviolated in, e.g., longitudinal studies and missing data research. Targeting\nthis point, we adapt FPLS to scenarios in which the number of measurements per\nsubject is small and bounded from above. The resulting proposal is abbreviated\nas PLEASS. Under certain regularity conditions, we establish the consistency of\nestimators and give confidence intervals for scalar responses. Simulation\nstudies and real-data applications illustrate the competitive accuracy of\nPLEASS\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 17:59:58 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 02:57:16 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Zhou", "Zhiyang", ""], ["Lockhart", "Richard A.", ""]]}, {"id": "2003.11588", "submitter": "Denis Talbot Mr", "authors": "Denis Talbot and Claudia Beaudoin", "title": "A generalized double robust Bayesian model averaging approach to causal\n  effect estimation with application to the Study of Osteoporotic Fractures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysts often use data-driven approaches to supplement their substantive\nknowledge when selecting covariates for causal effect estimation. Multiple\nvariable selection procedures tailored for causal effect estimation have been\ndevised in recent years, but additional developments are still required to\nadequately address the needs of data analysts. In this paper, we propose a\nGeneralized Bayesian Causal Effect Estimation (GBCEE) algorithm to perform\nvariable selection and produce double robust estimates of causal effects for\nbinary or continuous exposures and outcomes. GBCEE employs a prior distribution\nthat targets the selection of true confounders and predictors of the outcome\nfor the unbiased estimation of causal effects with reduced standard errors.\nDouble robust estimators provide some robustness against model\nmisspecification, whereas the Bayesian machinery allows GBCEE to directly\nproduce inferences for its estimate. GBCEE was compared to multiple\nalternatives in various simulation scenarios and was observed to perform\nsimilarly or to outperform double robust alternatives. Its ability to directly\nproduce inferences is also an important advantage from a computational\nperspective. The method is finally illustrated for the estimation of the effect\nof meeting physical activity recommendations on the risk of hip or upper-leg\nfractures among elderly women in the Study of Osteoporotic Fractures. The 95%\nconfidence interval produced by GBCEE is 61% shorter than that of a double\nrobust estimator adjusting for all potential confounders in this illustration.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 19:13:38 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Talbot", "Denis", ""], ["Beaudoin", "Claudia", ""]]}, {"id": "2003.11635", "submitter": "Peter Aronow", "authors": "Peter M. Aronow, Fredrik S\\\"avje", "title": "Review of The Book of Why: The New Science of Cause and Effect", "comments": null, "journal-ref": "J. Amer. Statist. Assoc. (2020) 115: 482--485", "doi": "10.1080/01621459.2020.1721245", "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Book review published as: Aronow, Peter M. and Fredrik S\\\"avje (2020), \"The\nBook of Why: The New Science of Cause and Effect.\" Journal of the American\nStatistical Association, 115: 482-485.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 21:03:51 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Aronow", "Peter M.", ""], ["S\u00e4vje", "Fredrik", ""]]}, {"id": "2003.11744", "submitter": "Molei Liu", "authors": "Yichi Zhang, Molei Liu, Matey Neykov and Tianxi Cai", "title": "Prior Adaptive Semi-supervised Learning with Application to EHR\n  Phenotyping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic Health Records (EHR) data, a rich source for biomedical research,\nhave been successfully used to gain novel insight into a wide range of\ndiseases. Despite its potential, EHR is currently underutilized for discovery\nresearch due to it's major limitation in the lack of precise phenotype\ninformation. To overcome such difficulties, recent efforts have been devoted to\ndeveloping supervised algorithms to accurately predict phenotypes based on\nrelatively small training datasets with gold standard labels extracted via\nchart review. However, supervised methods typically require a sizable training\nset to yield generalizable algorithms especially when the number of candidate\nfeatures, $p$, is large. In this paper, we propose a semi-supervised (SS) EHR\nphenotyping method that borrows information from both a small labeled data\nwhere both the label $Y$ and the feature set $X$ are observed and a much larger\nunlabeled data with observations on $X$ only as well as a surrogate variable\n$S$ that is predictive of $Y$ and available for all patients, under a high\ndimensional setting. Under a working prior assumption that $S$ is related to\n$X$ only through $Y$ and allowing it to hold approximately, we propose a prior\nadaptive semi-supervised (PASS) estimator that adaptively incorporates the\nprior knowledge by shrinking the estimator towards a direction derived under\nthe prior. We derive asymptotic theory for the proposed estimator and\ndemonstrate its superiority over existing estimators via simulation studies.\nThe proposed method is applied to an EHR phenotyping study of rheumatoid\narthritis at Partner's Healthcare.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 04:50:28 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Zhang", "Yichi", ""], ["Liu", "Molei", ""], ["Neykov", "Matey", ""], ["Cai", "Tianxi", ""]]}, {"id": "2003.11862", "submitter": "Federico Ricciardi", "authors": "Federico Ricciardi, Silvia Liverani and Gianluca Baio", "title": "Dirichlet Process Mixture Models for Regression Discontinuity Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Regression Discontinuity Design (RDD) is a quasi-experimental design that\nestimates the causal effect of a treatment when its assignment is defined by a\nthreshold value for a continuous assignment variable. The RDD assumes that\nsubjects with measurements within a bandwidth around the threshold belong to a\ncommon population, so that the threshold can be seen as a randomising device\nassigning treatment to those falling just above the threshold and withholding\nit from those who fall just below.\n  Bandwidth selection represents a compelling decision for the RDD analysis as\nthe results may be highly sensitive to its choice. A number of methods to\nselect the optimal bandwidth, mainly originating from the econometric\nliterature, have been proposed. However, their use in practice is limited.\n  We propose a methodology that, tackling the problem from an applied point of\nview, consider units' exchangeability, i.e., their similarity with respect to\nmeasured covariates, as the main criteria to select subjects for the analysis,\nirrespectively of their distance from the threshold. We carry out clustering on\nthe sample using a Dirichlet process mixture model to identify balanced and\nhomogeneous clusters. Our proposal exploits the posterior similarity matrix,\nwhich contains the pairwise probabilities that two observations are allocated\nto the same cluster in the MCMC sample. Thus we include in the RDD analysis\nonly those clusters for which we have stronger evidence of exchangeability.\n  We illustrate the validity of our methodology with both a simulated\nexperiment and a motivating example on the effect of statins to lower\ncholesterol level, using UK primary care data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 12:22:45 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Ricciardi", "Federico", ""], ["Liverani", "Silvia", ""], ["Baio", "Gianluca", ""]]}, {"id": "2003.11991", "submitter": "Shantanu Gupta", "authors": "Shantanu Gupta, Zachary C. Lipton, David Childers", "title": "Estimating Treatment Effects with Observed Confounders and Mediators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a causal graph, the do-calculus can express treatment effects as\nfunctionals of the observational joint distribution that can be estimated\nempirically. Sometimes the do-calculus identifies multiple valid formulae,\nprompting us to compare the statistical properties of the corresponding\nestimators. For example, the backdoor formula applies when all confounders are\nobserved and the frontdoor formula applies when an observed mediator transmits\nthe causal effect. In this paper, we investigate the over-identified scenario\nwhere both confounders and mediators are observed, rendering both estimators\nvalid. Addressing the linear Gaussian causal model, we demonstrate that either\nestimator can dominate the other by an unbounded constant factor. Next, we\nderive an optimal estimator, which leverages all observed variables, and bound\nits finite-sample variance. We show that it strictly outperforms the backdoor\nand frontdoor estimators and that this improvement can be unbounded. We also\npresent a procedure for combining two datasets, one with observed confounders\nand another with observed mediators. Finally, we evaluate our methods on both\nsimulated data and the IHDP and JTPA datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 15:50:25 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 21:45:29 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 05:25:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gupta", "Shantanu", ""], ["Lipton", "Zachary C.", ""], ["Childers", "David", ""]]}, {"id": "2003.12008", "submitter": "Beth Ann Griffin PhD", "authors": "Beth Ann Griffin, Megan S. Schuler, Elizabeth A. Stuart, Stephen\n  Patrick, Elizabeth McNeer, Rosanna Smart, David Powell, Bradley D. Stein,\n  Terry Schell, Rosalie L. Pacula", "title": "Moving beyond the classic difference-in-differences model: A simulation\n  study comparing statistical methods for estimating effectiveness of\n  state-level policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-level policy evaluations commonly employ a difference-in-differences\n(DID) study design; yet within this framework, statistical model specification\nvaries notably across studies. Motivated by applied state-level opioid policy\nevaluations, this simulation study compares statistical performance of multiple\nvariations of two-way fixed effect models traditionally used for DID under a\nrange of simulation conditions. While most linear models resulted in minimal\nbias, non-linear models and population-weighted versions of classic linear\ntwo-way fixed effect and linear GEE models yielded considerable bias (60 to\n160%). Further, root mean square error is minimized by linear AR models when\nexamining crude mortality rates and by negative binomial models when examining\nraw death counts. In the context of frequentist hypothesis testing, many models\nyielded high Type I error rates and very low rates of correctly rejecting the\nnull hypothesis (< 10%), raising concerns of spurious conclusions about policy\neffectiveness. When considering performance across models, the linear\nautoregressive models were optimal in terms of directional bias, root mean\nsquared error, Type I error, and correct rejection rates. These findings\nhighlight notable limitations of traditional statistical models commonly used\nfor DID designs, designs widely used in opioid policy studies and in state\npolicy evaluations more broadly.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 16:20:44 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 14:40:24 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 13:26:41 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Griffin", "Beth Ann", ""], ["Schuler", "Megan S.", ""], ["Stuart", "Elizabeth A.", ""], ["Patrick", "Stephen", ""], ["McNeer", "Elizabeth", ""], ["Smart", "Rosanna", ""], ["Powell", "David", ""], ["Stein", "Bradley D.", ""], ["Schell", "Terry", ""], ["Pacula", "Rosalie L.", ""]]}, {"id": "2003.12156", "submitter": "Jae-Kwang Kim", "authors": "Jae-kwang Kim and Siu-Ming Tam", "title": "Data Integration by combining big data and survey sample data for finite\n  population inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical challenges in using big data for making valid statistical\ninference in the finite population have been well documented in literature.\nThese challenges are due primarily to statistical bias arising from\nunder-coverage in the big data source to represent the population of interest\nand measurement errors in the variables available in the data set. By\nstratifying the population into a big data stratum and a missing data stratum,\nwe can estimate the missing data stratum by using a fully responding\nprobability sample, and hence the population as a whole by using a data\nintegration estimator. By expressing the data integration estimator as a\nregression estimator, we can handle measurement errors in the variables in big\ndata and also in the probability sample. We also propose a fully nonparametric\nclassification method for identifying the overlapping units and develop a\nbias-corrected data integration estimator under misclassification errors.\nFinally, we develop a two-step regression data integration estimator to deal\nwith measurement errors in the probability sample. An advantage of the approach\nadvocated in this paper is that we do not have to make unrealistic\nmissing-at-random assumptions for the methods to work. The proposed method is\napplied to the real data example using 2015-16 Australian Agricultural Census\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 20:56:25 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 20:07:09 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 11:37:50 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Kim", "Jae-kwang", ""], ["Tam", "Siu-Ming", ""]]}, {"id": "2003.12182", "submitter": "Lorenzo Trapani", "authors": "Lorenzo Trapani and Emily Whitehouse", "title": "Sequential monitoring for cointegrating regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop monitoring procedures for cointegrating regressions, testing the\nnull of no breaks against the alternatives that there is either a change in the\nslope, or a change to non-cointegration. After observing the regression for a\ncalibration sample m, we study a CUSUM-type statistic to detect the presence of\nchange during a monitoring horizon m+1,...,T. Our procedures use a class of\nboundary functions which depend on a parameter whose value affects the delay in\ndetecting the possible break. Technically, these procedures are based on almost\nsure limiting theorems whose derivation is not straightforward. We therefore\ndefine a monitoring function which - at every point in time - diverges to\ninfinity under the null, and drifts to zero under alternatives. We cast this\nsequence in a randomised procedure to construct an i.i.d. sequence, which we\nthen employ to define the detector function. Our monitoring procedure rejects\nthe null of no break (when correct) with a small probability, whilst it rejects\nwith probability one over the monitoring horizon in the presence of breaks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 22:58:54 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Trapani", "Lorenzo", ""], ["Whitehouse", "Emily", ""]]}, {"id": "2003.12200", "submitter": "Tommaso Rigon", "authors": "Tommaso Rigon, Bruno Scarpa and Sonia Petrone", "title": "Enriched Pitman-Yor processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian nonparametrics there exists a rich variety of discrete priors,\nincluding the Dirichlet process and its generalizations, which are nowadays\nwell-established tools. Despite the remarkable advances, few proposals are\ntailored for modeling observations lying on product spaces, such as\n$\\mathbb{R}^p$. In this setting, most of the available priors lack of\nflexibility and they do not allow for separate partition structures among the\nspaces. We address these issues by introducing a discrete nonparametric prior\ntermed enriched Pitman-Yor process (EPY). Theoretical properties of this novel\nprior are extensively investigated. Specifically, we discuss its formal link\nwith the enriched Dirichlet process and normalized random measures, we describe\na square-breaking representation and we obtain closed form expressions for the\nposterior law and the involved urn schemes. In second place, we show that\nseveral existing approaches, including Dirichlet processes with a spike and\nslab base measure and mixture of mixtures models, implicitly rely on special\ncases of the EPY, which therefore constitutes a unified probabilistic framework\nfor many Bayesian nonparametric priors. Interestingly, our unifying formulation\nwill allow to naturally extend these models, while preserving their analytical\ntractability. As an illustration, we employ the EPY for a species sampling\nproblem in ecology.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 01:34:27 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Rigon", "Tommaso", ""], ["Scarpa", "Bruno", ""], ["Petrone", "Sonia", ""]]}, {"id": "2003.12247", "submitter": "Shouto Yonekura", "authors": "Shouto Yonekura and Alexandros Beskos", "title": "Online Smoothing for Diffusion Processes Observed with Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a methodology for online estimation of smoothing expectations\nfor a class of additive functionals, in the context of a rich family of\ndiffusion processes (that may include jumps) -- observed at discrete-time\ninstances. We overcome the unavailability of the transition density of the\nunderlying SDE by working on the augmented pathspace. The new method can be\napplied, for instance, to carry out online parameter inference for the\ndesignated class of models. Algorithms defined on the infinite-dimensional\npathspace have been developed in the last years mainly in the context of MCMC\ntechniques. There, the main benefit is the achievement of mesh-free mixing\ntimes for the practical time-discretised algorithm used on a PC. Our own\nmethodology sets up the framework for infinite-dimensional online filtering --\nan important positive practical consequence is the construct of estimates with\nthe variance that does not increase with decreasing mesh-size. Besides\nregularity conditions, our method is, in principle, applicable under the weak\nassumption -- relatively to restrictive conditions often required in the MCMC\nor filtering literature of methods defined on pathspace -- that the SDE\ncovariance matrix is invertible.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 06:07:51 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 01:56:50 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 13:00:03 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Yonekura", "Shouto", ""], ["Beskos", "Alexandros", ""]]}, {"id": "2003.12408", "submitter": "Xiaojie Mao", "authors": "Nathan Kallus, Xiaojie Mao", "title": "On the role of surrogates in the efficient estimation of treatment\n  effects with limited outcome data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating treatment effects when the outcome of\nprimary interest (e.g., long-term health status) is only seldom observed but\nabundant surrogate observations (e.g., short-term health outcomes) are\navailable. To investigate the role of surrogates in this setting, we derive the\nsemiparametric efficiency lower bounds of average treatment effect (ATE) both\nwith and without presence of surrogates, as well as several intermediary\nsettings. These bounds characterize the best-possible precision of ATE\nestimation in each case, and their difference quantifies the efficiency gains\nfrom optimally leveraging the surrogates in terms of key problem\ncharacteristics when only limited outcome data are available. We show these\nresults apply in two important regimes: when the number of surrogate\nobservations is comparable to primary-outcome observations and when the former\ndominates the latter. Importantly, we take a missing-data approach that\ncircumvents strong surrogate conditions which are commonly assumed in previous\nliterature but almost always fail in practice. To show how to leverage the\nefficiency gains of surrogate observations, we propose ATE estimators and\ninferential methods based on flexible machine learning methods to estimate\nnuisance parameters that appear in the influence functions. We show our\nestimators enjoy efficiency and robustness guarantees under weak conditions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 13:31:49 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Kallus", "Nathan", ""], ["Mao", "Xiaojie", ""]]}, {"id": "2003.12411", "submitter": "Moritz Berger Dr.", "authors": "Moritz Berger and Gerhard Tutz", "title": "Transition Models for Count Data: a Flexible Alternative to Fixed\n  Distribution Models", "comments": "24 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A flexible semiparametric class of models is introduced that offers an\nalternative to classical regression models for count data as the Poisson and\nnegative binomial model, as well as to more general models accounting for\nexcess zeros that are also based on fixed distributional assumptions. The model\nallows that the data itself determine the distribution of the response\nvariable, but, in its basic form, uses a parametric term that specifies the\neffect of explanatory variables. In addition, an extended version is\nconsidered, in which the effects of covariates are specified nonparametrically.\nThe proposed model and traditional models are compared by utilizing several\nreal data applications.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 13:38:29 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Berger", "Moritz", ""], ["Tutz", "Gerhard", ""]]}, {"id": "2003.12427", "submitter": "Ashkan Ertefaie", "authors": "Ashkan Ertefaie, James R. McKay, David Oslin and Robert L. Strawderman", "title": "Robust Q-learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Q-learning is a regression-based approach that is widely used to formalize\nthe development of an optimal dynamic treatment strategy. Finite dimensional\nworking models are typically used to estimate certain nuisance parameters, and\nmisspecification of these working models can result in residual confounding\nand/or efficiency loss. We propose a robust Q-learning approach which allows\nestimating such nuisance parameters using data-adaptive techniques. We study\nthe asymptotic behavior of our estimators and provide simulation studies that\nhighlight the need for and usefulness of the proposed method in practice. We\nuse the data from the \"Extending Treatment Effectiveness of Naltrexone\"\nmulti-stage randomized trial to illustrate our proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 14:10:38 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Ertefaie", "Ashkan", ""], ["McKay", "James R.", ""], ["Oslin", "David", ""], ["Strawderman", "Robert L.", ""]]}, {"id": "2003.12540", "submitter": "Ning Hao", "authors": "Ning Hao, Yue Selena Niu, Feifei Xiao, and Heping Zhang", "title": "A super scalable algorithm for short segment detection", "comments": "To be published in Statistics in Biosciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications such as copy number variant (CNV) detection, the goal is\nto identify short segments on which the observations have different means or\nmedians from the background. Those segments are usually short and hidden in a\nlong sequence, and hence are very challenging to find. We study a super\nscalable short segment (4S) detection algorithm in this paper. This\nnonparametric method clusters the locations where the observations exceed a\nthreshold for segment detection. It is computationally efficient and does not\nrely on Gaussian noise assumption. Moreover, we develop a framework to assign\nsignificance levels for detected segments. We demonstrate the advantages of our\nproposed method by theoretical, simulation, and real data studies.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 17:08:22 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Hao", "Ning", ""], ["Niu", "Yue Selena", ""], ["Xiao", "Feifei", ""], ["Zhang", "Heping", ""]]}, {"id": "2003.12542", "submitter": "Giuseppe Arbia Dr", "authors": "Giuseppe Arbia, Gloria Solano-Hermosilla, Fabio Micale, Vincenzo\n  Nardelli and Giampiero Genovese", "title": "Post-sampling crowdsourced data to allow reliable statistical inference:\n  the case of food price indices in Nigeria", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sound policy and decision making in developing countries is often limited by\nthe lack of timely and reliable data. Crowdsourced data may provide a valuable\nalternative for data collection and analysis, e. g. in remote and insecure\nareas or of poor accessibility where traditional methods are difficult or\ncostly. However, crowdsourced data are not directly usable to draw sound\nstatistical inference. Indeed, its use involves statistical problems because\ndata do not obey any formal sampling design and may also suffer from various\nnon-sampling errors. To overcome this, we propose the use of a special form of\npost-stratification with which crowdsourced data are reweighted prior their use\nin an inferential context. An example in Nigeria illustrates the applicability\nof the method.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 17:14:46 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Arbia", "Giuseppe", ""], ["Solano-Hermosilla", "Gloria", ""], ["Micale", "Fabio", ""], ["Nardelli", "Vincenzo", ""], ["Genovese", "Giampiero", ""]]}, {"id": "2003.12814", "submitter": "Hamdi Raissi", "authors": "Junichi Hirukawa and Hamdi Ra\\\"issi", "title": "Investigating linear relationships between non constant variances of\n  economic variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we aim to assess linear relationships between the non constant\nvariances of economic variables. The proposed methodology is based on a\nbootstrap cumulative sum (CUSUM) test. Simulations suggest a good behavior of\nthe test for sample sizes commonly encountered in practice. The tool we provide\nis intended to highlight relations or draw common patterns between economic\nvariables through their non constant variances. The outputs of this paper is\nillustrated considering U.S. regional data.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 15:17:29 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Hirukawa", "Junichi", ""], ["Ra\u00efssi", "Hamdi", ""]]}, {"id": "2003.12816", "submitter": "Adam Walder", "authors": "Adam Walder, Ephraim M. Hanks, Aleksandra Slavkovi\\'c", "title": "Privacy for Spatial Point Process Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop methods for privatizing spatial location data, such\nas spatial locations of individual disease cases. We propose two novel Bayesian\nmethods for generating synthetic location data based on log-Gaussian Cox\nprocesses (LGCPs). We show that conditional predictive ordinate (CPO) estimates\ncan easily be obtained for point process data. We construct a novel risk metric\nthat utilizes CPO estimates to evaluate individual disclosure risks. We adapt\nthe propensity mean square error (pMSE) data utility metric for LGCPs. We\ndemonstrate that our synthesis methods offer an improved risk vs. utility\nbalance in comparison to radial synthesis with a case study of Dr. John Snow's\ncholera outbreak data.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 15:23:21 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 20:11:21 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Walder", "Adam", ""], ["Hanks", "Ephraim M.", ""], ["Slavkovi\u0107", "Aleksandra", ""]]}, {"id": "2003.12844", "submitter": "Jonathan Boss", "authors": "Jonathan Boss, Alexander Rix, Yin-Hsiu Chen, Naveen N. Narisetty,\n  Zhenke Wu, Kelly K. Ferguson, Thomas F. McElrath, John D. Meeker, Bhramar\n  Mukherjee", "title": "A Hierarchical Integrative Group LASSO (HiGLASSO) Framework for\n  Analyzing Environmental Mixtures", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental health studies are increasingly measuring multiple pollutants\nto characterize the joint health effects attributable to exposure mixtures.\nHowever, the underlying dose-response relationship between toxicants and health\noutcomes of interest may be highly nonlinear, with possible nonlinear\ninteraction effects. Existing penalized regression methods that account for\nexposure interactions either cannot accommodate nonlinear interactions while\nmaintaining strong heredity or are computationally unstable in applications\nwith limited sample size. In this paper, we propose a general shrinkage and\nselection framework to identify noteworthy nonlinear main and interaction\neffects among a set of exposures. We design hierarchical integrative group\nLASSO (HiGLASSO) to (a) impose strong heredity constraints on two-way\ninteraction effects (hierarchical), (b) incorporate adaptive weights without\nnecessitating initial coefficient estimates (integrative), and (c) induce\nsparsity for variable selection while respecting group structure (group LASSO).\nWe prove sparsistency of the proposed method and apply HiGLASSO to an\nenvironmental toxicants dataset from the LIFECODES birth cohort, where the\ninvestigators are interested in understanding the joint effects of 21 urinary\ntoxicant biomarkers on urinary 8-isoprostane, a measure of oxidative stress. An\nimplementation of HiGLASSO is available in the higlasso R package, accessible\nthrough the Comprehensive R Archive Network.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 17:12:29 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Boss", "Jonathan", ""], ["Rix", "Alexander", ""], ["Chen", "Yin-Hsiu", ""], ["Narisetty", "Naveen N.", ""], ["Wu", "Zhenke", ""], ["Ferguson", "Kelly K.", ""], ["McElrath", "Thomas F.", ""], ["Meeker", "John D.", ""], ["Mukherjee", "Bhramar", ""]]}, {"id": "2003.12919", "submitter": "Gennady Gorin", "authors": "Gennady Gorin, Lior Pachter", "title": "Special Function Methods for Bursty Models of Transcription", "comments": "Body: 15 pages, 2 figures, 2 tables. Supplement: 10 pages, 1 figure", "journal-ref": "Phys. Rev. E 102, 022409 (2020)", "doi": "10.1103/PhysRevE.102.022409", "report-no": null, "categories": "stat.ME q-bio.MN q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore a Markov model used in the analysis of gene expression, involving\nthe bursty production of pre-mRNA, its conversion to mature mRNA, and its\nconsequent degradation. We demonstrate that the integration used to compute the\nsolution of the stochastic system can be approximated by the evaluation of\nspecial functions. Furthermore, the form of the special function solution\ngeneralizes to a broader class of burst distributions. In light of the broader\ngoal of biophysical parameter inference from transcriptomics data, we apply the\nmethod to simulated data, demonstrating effective control of precision and\nruntime. Finally, we suggest a non-Bayesian approach to reducing the\ncomputational complexity of parameter inference to linear order in state space\nsize and number of candidate parameters.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 01:24:56 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Gorin", "Gennady", ""], ["Pachter", "Lior", ""]]}, {"id": "2003.12936", "submitter": "Yevgeniy Kovchegov", "authors": "Evgenia Chunikhina, Paul Logan, Yevgeniy Kovchegov, Anatoly\n  Yambartsev, Debashis Mondal, Andrey Morgun", "title": "The covariance shift (C-SHIFT) algorithm for normalizing biological data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omics technologies are powerful tools for analyzing patterns in gene\nexpression data for thousands of genes. Due to a number of systematic\nvariations in experiments, the raw gene expression data is often obfuscated by\nundesirable technical noises. Various normalization techniques were designed in\nan attempt to remove these non-biological errors prior to any statistical\nanalysis. One of the reasons for normalizing data is the need for recovering\nthe covariance matrix used in gene network analysis. In this paper, we\nintroduce a novel normalization technique, called the covariance shift\n(C-SHIFT) method. This normalization algorithm uses optimization techniques\ntogether with the blessing of dimensionality philosophy and energy minimization\nhypothesis for covariance matrix recovery under additive noise (in biology,\nknown as the bias). Thus, it is perfectly suited for the analysis of\nlogarithmic gene expression data. Numerical experiments on synthetic data\ndemonstrate the method's advantage over the classical normalization techniques.\nNamely, the comparison is made with rank, quantile, cyclic LOESS (locally\nestimated scatterplot smoothing), and MAD (median absolute deviation)\nnormalization methods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 03:24:51 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Chunikhina", "Evgenia", ""], ["Logan", "Paul", ""], ["Kovchegov", "Yevgeniy", ""], ["Yambartsev", "Anatoly", ""], ["Mondal", "Debashis", ""], ["Morgun", "Andrey", ""]]}, {"id": "2003.13004", "submitter": "Stephan Huckemann", "authors": "Maryam K. Garba, Tom M.W. Nye, Jonas Lueg, Stephan F. Huckemann", "title": "Information geometry for phylogenetic trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.DG math.IT q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new space of phylogenetic trees which we call wald space. The\nmotivation is to develop a space suitable for statistical analysis of\nphylogenies, but with a geometry based on more biologically principled\nassumptions than existing spaces: in wald space, trees are close if they induce\nsimilar distributions on genetic sequence data. As a point set, wald space\ncontains the previously developed Billera-Holmes-Vogtmann (BHV) tree space; it\nalso contains disconnected forests, like the edge-product (EP) space but\nwithout certain singularities of the EP space. We investigate two related\ngeometries on wald space. The first is the geometry of the Fisher information\nmetric of character distributions induced by the two-state symmetric Markov\nsubstitution process on each tree. Infinitesimally, the metric is proportional\nto the Kullback-Leibler divergence, or equivalently, as we show, any to f\n-divergence. The second geometry is obtained analogously but using a related\ncontinuous-valued Gaussian process on each tree, and it can be viewed as the\ntrace metric of the affine-invariant metric for covariance matrices. We derive\na gradient descent algorithm to project from the ambient space of covariance\nmatrices to wald space. For both geometries we derive computational methods to\ncompute geodesics in polynomial time and show numerically that the two\ninformation geometries (discrete and continuous) are very similar. In\nparticular geodesics are approximated extrinsically. Comparison with the BHV\ngeometry shows that our canonical and biologically motivated space is\nsubstantially different.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 11:30:55 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 20:49:45 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Garba", "Maryam K.", ""], ["Nye", "Tom M. W.", ""], ["Lueg", "Jonas", ""], ["Huckemann", "Stephan F.", ""]]}, {"id": "2003.13080", "submitter": "Viktor Ra\\v{c}inskij", "authors": "Viktor Ra\\v{c}inskij", "title": "Naive linkage error corrected dual system estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The utility of capture-recapture methods is offset by strong underlying\nassumptions. In this paper we discuss a failure in the perfect linkage\nassumption in a special case of capture-recapture known as the dual system\nestimation. We propose a naive linkage error corrected dual system estimator.\nThe advantage of the proposed approach compared with the existing linkage error\ncorrected methods is that it permits an easy way to obtain the corresponding\nvariance estimator.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 17:16:21 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ra\u010dinskij", "Viktor", ""]]}, {"id": "2003.13111", "submitter": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez", "authors": "Maria Xose Rodriguez-Alvarez and Vanda Inacio", "title": "ROCnReg: An R Package for Receiver Operating Characteristic Curve\n  Inference with and without Covariate Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The receiver operating characteristic (ROC) curve is the most popular tool\nused to evaluate the discriminatory capability of diagnostic tests/biomarkers\nmeasured on a continuous scale when distinguishing between two alternative\ndisease states (e.g, diseased and nondiseased). In some circumstances, the\ntest's performance and its discriminatory ability may vary according to\nsubject-specific characteristics or different test settings. In such cases,\ninformation-specific accuracy measures, such as the covariate-specific and the\ncovariate-adjusted ROC curve are needed, as ignoring covariate information may\nlead to biased or erroneous results. This paper introduces the R package\nROCnReg that allows estimating the pooled (unadjusted) ROC curve, the\ncovariate-specific ROC curve, and the covariate-adjusted ROC curve by different\nmethods, both from (semi) parametric and nonparametric perspectives and within\nBayesian and frequentist paradigms. From the estimated ROC curve (pooled,\ncovariate-specific or covariate-adjusted), several summary measures of\naccuracy, such as the (partial) area under the ROC curve and the Youden index,\ncan be obtained. The package also provides functions to obtain ROC-based\noptimal threshold values using several criteria, namely, the Youden Index\ncriterion and the criterion that sets a target value for the false positive\nfraction. For the Bayesian methods, we provide tools for assessing model fit\nvia posterior predictive checks, while model choice can be carried out via\nseveral information criteria. Numerical and graphical outputs are provided for\nall methods. The package is illustrated through the analyses of data from an\nendocrine study where the aim is to assess the capability of the body mass\nindex to detect the presence or absence of cardiovascular disease risk factors.\nThe package is available from CRAN at\nhttps://CRAN.R-project.org/package=ROCnReg.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 19:04:42 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 17:55:33 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 09:53:14 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2021 18:00:15 GMT"}, {"version": "v5", "created": "Fri, 19 Mar 2021 17:04:27 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Rodriguez-Alvarez", "Maria Xose", ""], ["Inacio", "Vanda", ""]]}, {"id": "2003.13119", "submitter": "Julien Bodelet", "authors": "Julien Bodelet and Jiajun Shan", "title": "Nonparametric additive factor models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an additive factor model in which the common components are related\nto the latent factors through unknown smooth functions. The model is new to the\nliterature and generalizes nonlinear parametric factor models. Identification\nconditions are provided and a general nonparametric estimation procedure is\nspecified. Convergence of the estimator is obtained when both time and\ncross-sectional size increase at appropriate rates. The finite sample\nperformance is then illustrated in extensive numerical experiments. Finally,\nthe relevance and usefulness of the method is shown in an application to a\nnonlinear CAPM on S&P500 data.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 19:29:28 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 11:42:29 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 09:58:48 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Bodelet", "Julien", ""], ["Shan", "Jiajun", ""]]}, {"id": "2003.13161", "submitter": "Mei Dong", "authors": "Konstantin Shestopaloff, Mei Dong, Fan Gao, Wei Xu", "title": "DCMD: Distance-based Classification Using Mixture Distributions on\n  Microbiome Data", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1008799", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current advances in next generation sequencing techniques have allowed\nresearchers to conduct comprehensive research on microbiome and human diseases,\nwith recent studies identifying associations between human microbiome and\nhealth outcomes for a number of chronic conditions. However, microbiome data\nstructure, characterized by sparsity and skewness, presents challenges to\nbuilding effective classifiers. To address this, we present an innovative\napproach for distance-based classification using mixture distributions (DCMD).\nThe method aims to improve classification performance when using microbiome\ncommunity data, where the predictors are composed of sparse and heterogeneous\ncount data. This approach models the inherent uncertainty in sparse counts by\nestimating a mixture distribution for the sample data, and representing each\nobservation as a distribution, conditional on observed counts and the estimated\nmixture, which are then used as inputs for distance-based classification. The\nmethod is implemented into a k-means and k-nearest neighbours framework and we\nidentify two distance metrics that produce optimal results. The performance of\nthe model is assessed using simulations and applied to a human microbiome\nstudy, with results compared against a number of existing machine learning and\ndistance-based approaches. The proposed method is competitive when compared to\nthe machine learning approaches and showed a clear improvement over commonly\nused distance-based classifiers. The range of applicability and robustness make\nthe proposed method a viable alternative for classification using sparse\nmicrobiome count data.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 23:30:20 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Shestopaloff", "Konstantin", ""], ["Dong", "Mei", ""], ["Gao", "Fan", ""], ["Xu", "Wei", ""]]}, {"id": "2003.13277", "submitter": "Marc Ditzhaus", "authors": "Marc Ditzhaus and {\\L}ukas Smaga", "title": "Permutation test for the multivariate coefficient of variation in\n  factorial designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New inference methods for the multivariate coefficient of variation and its\nreciprocal, the standardized mean, are presented. While there are various\ntesting procedures for both parameters in the univariate case, it is less known\nhow to do inference in the multivariate setting appropriately. There are some\nexisting procedures but they rely on restrictive assumptions on the underlying\ndistributions. We tackle this problem by applying Wald-type statistics in the\ncontext of general, potentially heteroscedastic factorial designs. In addition\nto the $k$-sample case, higher-way layouts can be incorporated into this\nframework allowing the discussion of main and interaction effects. The\nresulting procedures are shown to be asymptotically valid under the null\nhypothesis and consistent under general alternatives. To improve the finite\nsample performance, we suggest permutation versions of the tests and shown that\nthe tests' asymptotic properties can be transferred to them. An exhaustive\nsimulation study compares the new tests, their permutation counterparts and\nexisting methods. To further analyse the differences between the tests, we\nconduct two illustrative real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 08:56:47 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ditzhaus", "Marc", ""], ["Smaga", "\u0141ukas", ""]]}, {"id": "2003.13299", "submitter": "Shuichi Kawano", "authors": "Shengyi Wu, Kaito Shimamura, Kohei Yoshikawa, Kazuaki Murayama,\n  Shuichi Kawano", "title": "Variable fusion for Bayesian linear regression via spike-and-slab priors", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In linear regression models, fusion of coefficients is used to identify\npredictors having similar relationships with a response. This is called\nvariable fusion. This paper presents a novel variable fusion method in terms of\nBayesian linear regression models. We focus on hierarchical Bayesian models\nbased on a spike-and-slab prior approach. A spike-and-slab prior is tailored to\nperform variable fusion. To obtain estimates of the parameters, we develop a\nGibbs sampler for the parameters. Simulation studies and a real data analysis\nshow that our proposed method achieves better performance than previous\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 09:38:00 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 10:12:59 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 09:06:39 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Wu", "Shengyi", ""], ["Shimamura", "Kaito", ""], ["Yoshikawa", "Kohei", ""], ["Murayama", "Kazuaki", ""], ["Kawano", "Shuichi", ""]]}, {"id": "2003.13301", "submitter": "Jan G\\'orecki", "authors": "Jan G\\'orecki, Marius Hofert, Ostap Okhrin", "title": "Outer power transformations of hierarchical Archimedean copulas:\n  Construction, sampling and estimation", "comments": "38 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of commonly used parametric Archimedean copula (AC) families\nare restricted to a single parameter, connected to a concordance measure such\nas Kendall's tau. This often leads to poor statistical fits, particularly in\nthe joint tails, and can sometimes even limit the ability to model concordance\nor tail dependence mathematically. This work suggests outer power (OP)\ntransformations of Archimedean generators to overcome these limitations. The\ncopulas generated by OP-transformed generators can, for example, allow one to\ncapture both a given concordance measure and a tail dependence coefficient\nsimultaneously. For exchangeable OP-transformed ACs, a formula for computing\ntail dependence coefficients is obtained, as well as two feasible OP AC\nestimators are proposed and their properties studied by simulation. For\nhierarchical extensions of OP-transformed ACs, a new construction principle,\nefficient sampling and parameter estimation are addressed. By simulation,\nconvergence rate and standard errors of the proposed estimator are studied.\nExcellent tail fitting capabilities of OP-transformed hierarchical AC models\nare demonstrated in a risk management application. The results show that the OP\ntransformation is able to improve the statistical fit of exchangeable ACs,\nparticularly of those that cannot capture upper tail dependence or strong\nconcordance, as well as the statistical fit of hierarchical ACs, especially in\nterms of tail dependence and higher dimensions. Given how comparably simple it\nis to include OP transformations into existing exchangeable and hierarchical AC\nmodels, this transformation provides an attractive trade-off between\ncomputational effort and statistical improvement.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 09:43:20 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 08:39:05 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["G\u00f3recki", "Jan", ""], ["Hofert", "Marius", ""], ["Okhrin", "Ostap", ""]]}, {"id": "2003.13373", "submitter": "Zunli Yuan", "authors": "Zunli Yuan, Matt J. Jarvis and Jiancheng Wang", "title": "A flexible method for estimating luminosity functions via Kernel Density\n  Estimation", "comments": "23 pages, accepted for publication in The Astrophysical Journal\n  Supplement Series", "journal-ref": "2020, ApJS, 248, 1", "doi": "10.3847/1538-4365/ab855b", "report-no": null, "categories": "stat.ME astro-ph.GA astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible method for estimating luminosity functions (LFs) based\non kernel density estimation (KDE), the most popular nonparametric density\nestimation approach developed in modern statistics, to overcome issues\nsurrounding binning of LFs. One challenge in applying KDE to LFs is how to\ntreat the boundary bias problem, since astronomical surveys usually obtain\ntruncated samples predominantly due to the flux-density limits of surveys. We\nuse two solutions, the transformation KDE method ($\\hat{\\phi}_{\\mathrm{t}}$),\nand the transformation-reflection KDE method ($\\hat{\\phi}_{\\mathrm{tr}}$) to\nreduce the boundary bias. We develop a new likelihood cross-validation\ncriterion for selecting optimal bandwidths, based on which, the posterior\nprobability distribution of bandwidth and transformation parameters for\n$\\hat{\\phi}_{\\mathrm{t}}$ and $\\hat{\\phi}_{\\mathrm{tr}}$ are derived within a\nMarkov chain Monte Carlo (MCMC) sampling procedure. The simulation result shows\nthat $\\hat{\\phi}_{\\mathrm{t}}$ and $\\hat{\\phi}_{\\mathrm{tr}}$ perform better\nthan the traditional binned method, especially in the sparse data regime around\nthe flux-limit of a survey or at the bright-end of the LF. To further improve\nthe performance of our KDE methods, we develop the transformation-reflection\nadaptive KDE approach ($\\hat{\\phi}_{\\mathrm{tra}}$). Monte Carlo simulations\nsuggest that it has a good stability and reliability in performance, and is\naround an order of magnitude more accurate than using the binned method. By\napplying our adaptive KDE method to a quasar sample, we find that it achieves\nestimates comparable to the rigorous determination by a previous work, while\nmaking far fewer assumptions about the LF. The KDE method we develop has the\nadvantages of both parametric and non-parametric methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 12:09:01 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 08:19:10 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Yuan", "Zunli", ""], ["Jarvis", "Matt J.", ""], ["Wang", "Jiancheng", ""]]}, {"id": "2003.13462", "submitter": "Zachary Pisano", "authors": "Zachary M. Pisano, Joshua S. Agterberg, Carey E. Priebe, and Daniel Q.\n  Naiman", "title": "Spectral graph clustering via the Expectation-Solution algorithm", "comments": "31 pages, intended for submission to J. Comp. and Graph. Stat", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic blockmodel (SBM) models the connectivity within and between\ndisjoint subsets of nodes in networks. Prior work demonstrated that the rows of\nan SBM's adjacency spectral embedding (ASE) and Laplacian spectral embedding\n(LSE) both converge in law to Gaussian mixtures where the components are curved\nexponential families. Maximum likelihood estimation via the\nExpectation-Maximization (EM) algorithm for a full Gaussian mixture model (GMM)\ncan then perform the task of clustering graph nodes, albeit without appealing\nto the components' curvature. Noting that EM is a special case of the\nExpectation-Solution (ES) algorithm, we propose two ES algorithms that allow us\nto take full advantage of these curved structures. After presenting the ES\nalgorithm for the general curved-Gaussian mixture, we develop those\ncorresponding to the ASE and LSE limiting distributions. Simulating from\nartificial SBMs and a brain connectome SBM reveals that clustering graph nodes\nvia our ES algorithms improves upon that of EM for a full GMM.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:21:46 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Pisano", "Zachary M.", ""], ["Agterberg", "Joshua S.", ""], ["Priebe", "Carey E.", ""], ["Naiman", "Daniel Q.", ""]]}, {"id": "2003.13490", "submitter": "Johannes Krebs", "authors": "Johannes Krebs and Christian Hirsch", "title": "Functional central limit theorems for persistent Betti numbers on\n  cylindrical networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study functional central limit theorems for persistent Betti numbers\nobtained from networks defined on a Poisson point process. The limit is formed\nin large volumes of cylindrical shape stretching only in one dimension. The\nresults cover a directed sublevel-filtration for stabilizing networks and the\nCech and Vietoris-Rips complex on the random geometric graph.\n  The presented functional central limit theorems open the door to a variety of\nstatistical applications in topological data analysis and we consider\ngoodness-of-fit tests in a simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:14:23 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 19:39:33 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Krebs", "Johannes", ""], ["Hirsch", "Christian", ""]]}, {"id": "2003.13498", "submitter": "Lorenzo Zaninetti", "authors": "Lorenzo Zaninetti", "title": "New probability distributions in astrophysics: II. The generalized and\n  double truncated Lindley", "comments": "15 pages, 7 figures", "journal-ref": "published on The International Journal of Astronomy and\n  Astrophysics, 2020,10, 39-55", "doi": "10.4236/ijaa.2020.101004", "report-no": null, "categories": "stat.ME astro-ph.IM astro-ph.SR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical parameters of five generalizations of the Lindley\ndistribution, such as the average, variance and moments, are reviewed. A new\ndouble truncated Lindley distribution with three parameters is derived. The new\ndistributions are applied to model the initial mass function for stars.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:22:13 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Zaninetti", "Lorenzo", ""]]}, {"id": "2003.13555", "submitter": "Georgia Papadogeorgou", "authors": "Georgia Papadogeorgou, Kosuke Imai, Jason Lyall, Fan Li", "title": "Causal Inference with Spatio-temporal Data: Estimating the Effects of\n  Airstrikes on Insurgent Violence in Iraq", "comments": "Includes new theoretical results for dependent, spatio-temporal data:\n  (1) asymptotic properties of the estimator based on the estimated propensity\n  scores, (2) proof that the asymptotic variance using the estimated propensity\n  scores is no greater than the variance of the estimator based on the true\n  propensity scores, and (3) a new sensitivity analysis method applicable to\n  the Hajek estimator", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many causal processes have spatial and temporal dimensions. Yet the classic\ncausal inference framework is not directly applicable when the treatment and\noutcome variables are generated by spatio-temporal processes with an infinite\nnumber of possible event locations. We extend the potential outcomes framework\nto these settings by formulating the treatment point process as a stochastic\nintervention. Our causal estimands include the expected number of outcome\nevents in a specified area under a particular stochastic treatment assignment\nstrategy. We develop methodology that allows for arbitrary patterns of spatial\nspillover and temporal carryover effects. Using martingale theory, we show that\nthe proposed estimator is consistent and asymptotically normal as the number of\ntime periods increases, even when the propensity score is estimated. We propose\na sensitivity analysis for the possible existence of unmeasured confounders,\nand extend it to the H\\'ajek estimator. Simulation studies are conducted to\nexamine the estimators' finite sample performance. Finally, we use the proposed\nmethods to estimate the effects of American airstrikes on insurgent violence in\nIraq from February 2007 to July 2008. We find that increasing the average\nnumber of daily airstrikes for up to one month results in more insurgent\nattacks across Iraq and within Baghdad. We also find evidence that airstrikes\ncan displace attacks from Baghdad to new locations up to 400 kilometers away.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 15:29:11 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 17:22:16 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 14:53:24 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Papadogeorgou", "Georgia", ""], ["Imai", "Kosuke", ""], ["Lyall", "Jason", ""], ["Li", "Fan", ""]]}, {"id": "2003.13771", "submitter": "Nima Hejazi", "authors": "Nima S. Hejazi, Mark J. van der Laan, Holly E. Janes, Peter B.\n  Gilbert, David C. Benkeser", "title": "Efficient nonparametric inference on the effects of stochastic\n  interventions under two-phase sampling, with applications to vaccine efficacy\n  trials", "comments": null, "journal-ref": "Biometrics, 2020", "doi": "10.1111/biom.13375", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent and subsequent widespread availability of preventive vaccines has\naltered the course of public health over the past century. Despite this\nsuccess, effective vaccines to prevent many high-burden diseases, including\nHIV, have been slow to develop. Vaccine development can be aided by the\nidentification of immune response markers that serve as effective surrogates\nfor clinically significant infection or disease endpoints. However, measuring\nimmune response marker activity is often costly, which has motivated the usage\nof two-phase sampling for immune response evaluation in clinical trials of\npreventive vaccines. In such trials, the measurement of immunological markers\nis performed on a subset of trial participants, where enrollment in this second\nphase is potentially contingent on the observed study outcome and other\nparticipant-level information. We propose nonparametric methodology for\nefficiently estimating a counterfactual parameter that quantifies the impact of\na given immune response marker on the subsequent probability of infection.\nAlong the way, we fill in theoretical gaps pertaining to the asymptotic\nbehavior of nonparametric efficient estimators in the context of two-phase\nsampling, including a multiple robustness property enjoyed by our estimators.\nTechniques for constructing confidence intervals and hypothesis tests are\npresented, and an open source software implementation of the methodology, the\ntxshift R package, is introduced. We illustrate the proposed techniques using\ndata from a recent preventive HIV vaccine efficacy trial.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 19:39:52 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 21:18:45 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Hejazi", "Nima S.", ""], ["van der Laan", "Mark J.", ""], ["Janes", "Holly E.", ""], ["Gilbert", "Peter B.", ""], ["Benkeser", "David C.", ""]]}, {"id": "2003.13803", "submitter": "Pedro H. C. Sant'Anna", "authors": "Pedro H. C. Sant'Anna and Xiaojun Song", "title": "Specification tests for generalized propensity scores using double\n  projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new class of nonparametric tests for the correct\nspecification of generalized propensity score models. The test procedure is\nbased on two different projection arguments, which lead to test statistics with\nseveral appealing properties. They accommodate high-dimensional covariates; are\nasymptotically invariant to the estimation method used to estimate the nuisance\nparameters and do not requite estimators to be root-n asymptotically linear;\nare fully data-driven and do not require tuning parameters, can be written in\nclosed-form, facilitating the implementation of an easy-to-use multiplier\nbootstrap procedure. We show that our proposed tests are able to detect a broad\nclass of local alternatives converging to the null at the parametric rate.\nMonte Carlo simulation studies indicate that our double projected tests have\nmuch higher power than other tests available in the literature, highlighting\ntheir practical appeal.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 20:38:18 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Sant'Anna", "Pedro H. C.", ""], ["Song", "Xiaojun", ""]]}, {"id": "2003.13808", "submitter": "Riccardo Fogliato", "authors": "Riccardo Fogliato, Max G'Sell, Alexandra Chouldechova", "title": "Fairness Evaluation in Presence of Biased Noisy Labels", "comments": "Accepted at International Conference on Artificial Intelligence and\n  Statistics (AISTATS), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk assessment tools are widely used around the country to inform decision\nmaking within the criminal justice system. Recently, considerable attention has\nbeen devoted to the question of whether such tools may suffer from racial bias.\nIn this type of assessment, a fundamental issue is that the training and\nevaluation of the model is based on a variable (arrest) that may represent a\nnoisy version of an unobserved outcome of more central interest (offense). We\npropose a sensitivity analysis framework for assessing how assumptions on the\nnoise across groups affect the predictive bias properties of the risk\nassessment model as a predictor of reoffense. Our experimental results on two\nreal world criminal justice data sets demonstrate how even small biases in the\nobserved labels may call into question the conclusions of an analysis based on\nthe noisy outcome.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 20:47:00 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Fogliato", "Riccardo", ""], ["G'Sell", "Max", ""], ["Chouldechova", "Alexandra", ""]]}, {"id": "2003.13819", "submitter": "Milad Bakhshizadeh", "authors": "Milad Bakhshizadeh, Arian Maleki, Victor H. de la Pena", "title": "Sharp Concentration Results for Heavy-Tailed Distributions", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain concentration and large deviation for the sums of independent and\nidentically distributed random variables with heavy-tailed distributions. Our\nconcentration results are concerned with random variables whose distributions\nsatisfy $P(X>t) \\leq {\\rm e}^{- I(t)}$, where $I: \\mathbb{R} \\rightarrow\n\\mathbb{R}$ is an increasing function and $I(t)/t \\rightarrow \\alpha \\in [0,\n\\infty)$ as $t \\rightarrow \\infty$. Our main theorem can not only recover some\nof the existing results, such as the concentration of the sum of subWeibull\nrandom variables, but it can also produce new results for the sum of random\nvariables with heavier tails. We show that the concentration inequalities we\nobtain are sharp enough to offer large deviation results for the sums of\nindependent random variables as well. Our analyses which are based on standard\ntruncation arguments simplify, unify and generalize the existing results on the\nconcentration and large deviation of heavy-tailed random variables.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 21:05:29 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 15:12:56 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Bakhshizadeh", "Milad", ""], ["Maleki", "Arian", ""], ["de la Pena", "Victor H.", ""]]}, {"id": "2003.13844", "submitter": "Xin Bing", "authors": "Xin Bing and Yang Ning and Yaosheng Xu", "title": "Adaptive Estimation in Multivariate Response Regression with Hidden\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the estimation of the coefficient matrix $\\Ttheta$ in\nmultivariate regression with hidden variables, $Y = (\\Ttheta)^TX + (B^*)^TZ +\nE$, where $Y$ is a $m$-dimensional response vector, $X$ is a $p$-dimensional\nvector of observable features, $Z$ represents a $K$-dimensional vector of\nunobserved hidden variables, possibly correlated with $X$, and $E$ is an\nindependent error. The number of hidden variables $K$ is unknown and both $m$\nand $p$ are allowed but not required to grow with the sample size $n$. Since\nonly $Y$ and $X$ are observable, we provide necessary conditions for the\nidentifiability of $\\Ttheta$. The same set of conditions are shown to be\nsufficient when the error $E$ is homoscedastic. Our identifiability proof is\nconstructive and leads to a novel and computationally efficient estimation\nalgorithm, called HIVE. The first step of the algorithm is to estimate the best\nlinear prediction of $Y$ given $X$ in which the unknown coefficient matrix\nexhibits an additive decomposition of $\\Ttheta$ and a dense matrix originated\nfrom the correlation between $X$ and the hidden variable $Z$. Under the row\nsparsity assumption on $\\Ttheta$, we propose to minimize a penalized least\nsquares loss by regularizing $\\Ttheta$ via a group-lasso penalty and\nregularizing the dense matrix via a multivariate ridge penalty. Non-asymptotic\ndeviation bounds of the in-sample prediction error are established. Our second\nstep is to estimate the row space of $B^*$ by leveraging the covariance\nstructure of the residual vector from the first step. In the last step, we\nremove the effect of hidden variable by projecting $Y$ onto the complement of\nthe estimated row space of $B^*$. Non-asymptotic error bounds of our final\nestimator are established. The model identifiability, parameter estimation and\nstatistical guarantees are further extended to the setting with heteroscedastic\nerrors.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 22:17:19 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 19:17:45 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Bing", "Xin", ""], ["Ning", "Yang", ""], ["Xu", "Yaosheng", ""]]}, {"id": "2003.13849", "submitter": "Ad Ridder", "authors": "Shaul K. Bar-Lev and Ad Ridder", "title": "New exponential dispersion models for count data -- the ABM and LM\n  classes", "comments": "27 pages, 4 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their fundamental paper on cubic variance functions, Letac and Mora (The\nAnnals of Statistics,1990) presented a systematic, rigorous and comprehensive\nstudy of natural exponential families on the real line, their characterization\nthrough their variance functions and mean value parameterization. They\npresented a section that for some reason has been left unnoticed. This section\ndeals with the construction of variance functions associated with natural\nexponential families of counting distributions on the set of nonnegative\nintegers and allows to find the corresponding generating measures. As\nexponential dispersion models are based on natural exponential families, we\nintroduce in this paper two new classes of exponential dispersion models based\non their results. For these classes, which are associated with simple variance\nfunctions, we derive their mean value parameterization and their associated\ngenerating measures. We also prove that they have some desirable properties.\nBoth classes are shown to be overdispersed and zero-inflated in ascending\norder, making them as competitive statistical models for those in use in both,\nstatistical and actuarial modeling. To our best knowledge, the classes of\ncounting distributions we present in this paper, have not been introduced or\ndiscussed before in the literature. To show that our classes can serve as\ncompetitive statistical models for those in use (e.g., Poisson, Negative\nbinomial), we include a numerical example of real data. In this example, we\ncompare the performance of our classes with relevant competitive models.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 22:38:35 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 21:45:14 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Bar-Lev", "Shaul K.", ""], ["Ridder", "Ad", ""]]}, {"id": "2003.13854", "submitter": "Ad Ridder", "authors": "Shaul K. Bar-Lev and Ad Ridder", "title": "Exponential Dispersion Models for Overdispersed Zero-Inflated Count Data", "comments": "22 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider three new classes of exponential dispersion models of discrete\nprobability distributions which are defined by specifying their variance\nfunctions in their mean value parameterization. In a previous paper (Bar-Lev\nand Ridder, 2020a), we have developed the framework of these classes and proved\nthat they have some desirable properties. Each of these classes was shown to be\noverdispersed and zero inflated in ascending order, making them as competitive\nstatistical models for those in use in statistical modeling. In this paper we\nelaborate on the computational aspects of their probability mass functions.\nFurthermore, we apply these classes for fitting real data sets having\noverdispersed and zero-inflated statistics. Classic models based on Poisson or\nnegative binomial distributions show poor fits, and therefore many alternatives\nhave already proposed in recent years. We execute an extensive comparison with\nthese other proposals, from which we may conclude that our framework is a\nflexible tool that gives excellent results in all cases. Moreover, in most\ncases our model gives the best fit.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 22:51:50 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Bar-Lev", "Shaul K.", ""], ["Ridder", "Ad", ""]]}, {"id": "2003.13936", "submitter": "Hanyu Song", "authors": "Hanyu Song and Yingjian Wang and David B. Dunson", "title": "Distributed Bayesian clustering using finite mixture of mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many modern applications, there is interest in analyzing enormous data\nsets that cannot be easily moved across computers or loaded into memory on a\nsingle computer. In such settings, it is very common to be interested in\nclustering. Existing distributed clustering algorithms are mostly distance or\ndensity based without a likelihood specification, precluding the possibility of\nformal statistical inference. Model-based clustering allows statistical\ninference, yet research on distributed inference has emphasized nonparametric\nBayesian mixture models over finite mixture models. To fill this gap, we\nintroduce a nearly embarrassingly parallel algorithm for clustering under a\nBayesian overfitted finite mixture of Gaussian mixtures, which we term\ndistributed Bayesian clustering (DIB-C). DIB-C can flexibly accommodate data\nsets with various shapes (e.g. skewed or multi-modal). With data randomly\npartitioned and distributed, we first run Markov chain Monte Carlo in an\nembarrassingly parallel manner to obtain local clustering draws and then refine\nacross workers for a final clustering estimate based on any loss function on\nthe space of partitions. DIB-C can also estimate cluster densities, quickly\nclassify new subjects and provide a posterior predictive distribution. Both\nsimulation studies and real data applications show superior performance of\nDIB-C in terms of robustness and computational efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 03:28:36 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 04:42:43 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Song", "Hanyu", ""], ["Wang", "Yingjian", ""], ["Dunson", "David B.", ""]]}, {"id": "2003.14118", "submitter": "Andreas Groll", "authors": "Maike Hohberg and Andreas Groll", "title": "A flexible adaptive lasso Cox frailty model based on the full likelihood", "comments": "Keywords: Cox Proportional Hazards Model, Lasso, Regularization,\n  Variable Selection, B-Splines", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a method to regularize Cox frailty models is proposed that\naccommodates time-varying covariates and time-varying coefficients and is based\non the full instead of the partial likelihood. A particular advantage in this\nframework is that the baseline hazard can be explicitly modeled in a smooth,\nsemi-parametric way, e.g. via P-splines. Regularization for variable selection\nis performed via a lasso penalty and via group lasso for categorical variables\nwhile a second penalty regularizes wiggliness of smooth estimates of\ntime-varying coefficients and the baseline hazard. Additionally, adaptive\nweights are included to stabilize the estimation. The method is implemented in\nR as coxlasso and will be compared to other packages for regularized Cox\nregression. Existing packages, however, do not allow for the combination of\ndifferent effects that are accommodated in coxlasso.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 11:49:30 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Hohberg", "Maike", ""], ["Groll", "Andreas", ""]]}, {"id": "2003.14337", "submitter": "Anze Slosar", "authors": "An\\v{z}e Slosar", "title": "Efficient identification of infected sub-population", "comments": "2 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When testing for infections, the standard method is to test each subject\nindividually. If testing methodology is such that samples from multiple\nsubjects can be efficiently combined and tested at once, yielding a positive\nresults if any one subject in the subgroup is positive, then one can often\nidentify the infected sub-population with a considerably lower number of tests\ncompared to the number of test subjects. We present two such methods that allow\nan increase in testing efficiency (in terms of total number of test performed)\nby a factor of $\\approx$ 10 if population infection rate is $10^{-2}$ and a\nfactor of $\\approx$50 when it is $10^{-3}$. Such methods could be useful when\ntesting large fractions of the total population, as will be perhaps required\nduring the current coronavirus pandemic.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 16:20:27 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Slosar", "An\u017ee", ""]]}]