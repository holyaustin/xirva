[{"id": "1603.00118", "submitter": "Alan Huang", "authors": "Alan Huang", "title": "On generalized estimating equations for vector regression", "comments": "20 pages, 5 tables, 1 figure. (To appear in the Australian and New\n  Zealand Journal of Statistics)", "journal-ref": null, "doi": "10.1111/anzs.12191", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized estimating equations (GEE; Liang & Zeger 1986) for general vector\nregression settings are examined. When the response vectors are of mixed type\n(e.g. continuous-binary response pairs), the GEE approach is a semiparametric\nalternative to full-likelihood copula methods, and is closely related to the\nmean-covariance estimation equations approach of Prentice & Zhao (1991). When\nthe response vectors are of the same type (e.g. measurements on left and right\neyes), the GEE approach can be viewed as a \"plug-in\" to existing methods, such\nas the vglm function from the state-of-the-art VGAM R package of Yee (2015). In\neither scenario, the GEE approach offers asymptotically correct inferences on\nmodel parameters regardless of whether the working variance-covariance model is\ncorrectly or incorrectly specified. The finite-sample performance of the method\nis assessed using simulation studies based on a burn injury dataset (Song 2007)\nand a Sorbinil eye trial dataset (Rosner et. al 2006). The method is applied to\ndata analysis examples using the same two datasets, as well as on a\npresence/absence dataset on three plant species in the Hunua ranges of\nAuckland.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 03:04:49 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 06:55:36 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 07:19:06 GMT"}, {"version": "v4", "created": "Thu, 24 Nov 2016 13:41:47 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Huang", "Alan", ""]]}, {"id": "1603.00235", "submitter": "Youngki Shin Youngki Shin", "authors": "Sokbae Lee, Yuan Liao, Myung Hwan Seo, Youngki Shin", "title": "Oracle Estimation of a Change Point in High Dimensional Quantile\n  Regression", "comments": "128 pages, 12 figures. A part of this paper was circulated under the\n  title \"Structural Change in Sparsity\" arXiv:1411.3062", "journal-ref": "JASA 113 (2018) 1184-1194", "doi": "10.1080/01621459.2017.1319840", "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a high-dimensional quantile regression model where\nthe sparsity structure may differ between two sub-populations. We develop\n$\\ell_1$-penalized estimators of both regression coefficients and the threshold\nparameter. Our penalized estimators not only select covariates but also\ndiscriminate between a model with homogeneous sparsity and a model with a\nchange point. As a result, it is not necessary to know or pretest whether the\nchange point is present, or where it occurs. Our estimator of the change point\nachieves an oracle property in the sense that its asymptotic distribution is\nthe same as if the unknown active sets of regression coefficients were known.\nImportantly, we establish this oracle property without a perfect covariate\nselection, thereby avoiding the need for the minimum level condition on the\nsignals of active covariates. Dealing with high-dimensional quantile regression\nwith an unknown change point calls for a new proof technique since the quantile\nloss function is non-smooth and furthermore the corresponding objective\nfunction is non-convex with respect to the change point. The technique\ndeveloped in this paper is applicable to a general M-estimation framework with\na change point, which may be of independent interest. The proposed methods are\nthen illustrated via Monte Carlo experiments and an application to tipping in\nthe dynamics of racial segregation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 11:21:42 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 10:47:55 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Lee", "Sokbae", ""], ["Liao", "Yuan", ""], ["Seo", "Myung Hwan", ""], ["Shin", "Youngki", ""]]}, {"id": "1603.00393", "submitter": "Hailiang Du", "authors": "Sarah Higgins and Hailiang Du and Leonard A. Smith", "title": "On the Design and use of Ensembles of Multi-model Simulations for\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probability forecasting is common in the geosciences, the finance sector, and\nelsewhere. It is sometimes the case that one has multiple probability-forecasts\nfor the same target. How is the information in these multiple forecast systems\nbest \"combined\"? Assuming stationary, then in the limit of a very large\nforecast-outcome archive, each model-based probability density function can be\nweighted to form a \"multi-model forecast\" which will, in expectation, provide\nthe most information. In the case that one of the forecast systems yields a\nprobability distribution which reflects the distribution from which the outcome\nwill be drawn, then Bayesian Model Averaging will identify this model as the\nnumber of forecast-outcome pairs goes to infinity. In many applications, like\nthose of seasonal forecasting, data are precious: the archive is often limited\nto fewer than $2^6$ entries. And no perfect model is in hand. In this case, it\nis shown that forming a single \"multi-model probability forecast\" can be\nexpected to prove misleading. These issues are investigated using probability\nforecasts of a simple mathematical system, which allows most limiting\nbehaviours to be quantified.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 18:35:04 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Higgins", "Sarah", ""], ["Du", "Hailiang", ""], ["Smith", "Leonard A.", ""]]}, {"id": "1603.00444", "submitter": "Nirian Mart\\'in", "authors": "Nirian Martin, Leandro Pardo, Konstantinos Zografos", "title": "On divergences tests for composite hypotheses under composite likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that in some situations it is not easy to compute the\nlikelihood function as the datasets might be large or the model is too complex.\nIn that contexts composite likelihood, derived by multiplying the likelihoods\nof subjects of the variables, may be useful. The extension of the classical\nlikelihood ratio test statistics to the framework of composite likelihoods is\nused as a procedure to solve the problem of testing in the context of composite\nlikelihood. In this paper we introduce and study a new family of test\nstatistics for composite likelihood: Composite {\\phi}-divergence test\nstatistics for solving the problem of testing a simple null hypothesis or a\ncomposite null hypothesis. To do that we introduce and study the asymptotic\ndistribution of the restricted maximum composite likelihood estimate.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 20:29:54 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Martin", "Nirian", ""], ["Pardo", "Leandro", ""], ["Zografos", "Konstantinos", ""]]}, {"id": "1603.00861", "submitter": "Trevor Campbell", "authors": "Trevor Campbell, Jonathan H. Huggins, Jonathan P. How, Tamara\n  Broderick", "title": "Truncated Random Measures", "comments": "To appear in Bernoulli; 58 pages, 3 figures", "journal-ref": "Bernoulli, Volume 25, Number 2 (2019), 1256-1288", "doi": "10.3150/18-BEJ1020", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Completely random measures (CRMs) and their normalizations are a rich source\nof Bayesian nonparametric priors. Examples include the beta, gamma, and\nDirichlet processes. In this paper we detail two major classes of sequential\nCRM representations---series representations and superposition\nrepresentations---within which we organize both novel and existing sequential\nrepresentations that can be used for simulation and posterior inference. These\ntwo classes and their constituent representations subsume existing ones that\nhave previously been developed in an ad hoc manner for specific processes.\nSince a complete infinite-dimensional CRM cannot be used explicitly for\ncomputation, sequential representations are often truncated for tractability.\nWe provide truncation error analyses for each type of sequential\nrepresentation, as well as their normalized versions, thereby generalizing and\nimproving upon existing truncation error bounds in the literature. We analyze\nthe computational complexity of the sequential representations, which in\nconjunction with our error bounds allows us to directly compare representations\nand discuss their relative efficiency. We include numerous applications of our\ntheoretical results to commonly-used (normalized) CRMs, demonstrating that our\nresults enable a straightforward representation and analysis of CRMs that has\nnot previously been available in a Bayesian nonparametric context.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 20:40:33 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 01:33:39 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2018 14:34:19 GMT"}, {"version": "v4", "created": "Mon, 18 Feb 2019 18:08:51 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Campbell", "Trevor", ""], ["Huggins", "Jonathan H.", ""], ["How", "Jonathan P.", ""], ["Broderick", "Tamara", ""]]}, {"id": "1603.00921", "submitter": "Alan Huang", "authors": "Alan Huang, Nanxi Zhang", "title": "Doubly-nonparametric generalized additive models", "comments": "15 pages double-spaced, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular generalized additive model framework is extended to allow both\nthe mean curves and the response distribution to be nonparametric. The approach\nis demonstrated to be a flexible yet parsimonious tool for data analysis in its\nown right, as well as being a useful tool for model selection and diagnosis in\nthe classical generalized additive model framework. Finite-sample performance\nof the method is examined via various simulation settings and the method is\nillustrated on two data analysis examples.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 22:55:43 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 09:11:53 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Huang", "Alan", ""], ["Zhang", "Nanxi", ""]]}, {"id": "1603.01041", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Wilson Y. Chen, Richard H. Gerlach", "title": "Estimating Quantile Families of Loss Distributions for Non-Life\n  Insurance Modelling via L-moments", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses different classes of loss models in non-life insurance\nsettings. It then overviews the class Tukey transform loss models that have not\nyet been widely considered in non-life insurance modelling, but offer\nopportunities to produce flexible skewness and kurtosis features often required\nin loss modelling. In addition, these loss models admit explicit quantile\nspecifications which make them directly relevant for quantile based risk\nmeasure calculations. We detail various parameterizations and sub-families of\nthe Tukey transform based models, such as the g-and-h, g-and-k and g-and-j\nmodels, including their properties of relevance to loss modelling.\n  One of the challenges with such models is to perform robust estimation for\nthe loss model parameters that will be amenable to practitioners when fitting\nsuch models. In this paper we develop a novel, efficient and robust estimation\nprocedure for estimation of model parameters in this family Tukey transform\nmodels, based on L-moments. It is shown to be more robust and efficient than\ncurrent state of the art methods of estimation for such families of loss models\nand is simple to implement for practical purposes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 10:11:29 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Peters", "Gareth W.", ""], ["Chen", "Wilson Y.", ""], ["Gerlach", "Richard H.", ""]]}, {"id": "1603.01214", "submitter": "Patrick J. Wolfe", "authors": "Beate Franke and Patrick J. Wolfe", "title": "Network modularity in the presence of covariates", "comments": "56 pages, 4 figures; submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the large-sample properties of network modularity in the\npresence of covariates, under a natural and flexible nonparametric null model.\nThis provides for the first time an objective measure of whether or not a\nparticular value of modularity is meaningful. In particular, our results\nquantify the strength of the relation between observed community structure and\nthe interactions in a network. Our technical contribution is to provide limit\ntheorems for modularity when a community assignment is given by nodal features\nor covariates. These theorems hold for a broad class of network models over a\nrange of sparsity regimes, as well as weighted, multi-edge, and power-law\nnetworks. This allows us to assign $p$-values to observed community structure,\nwhich we validate using several benchmark examples in the literature. We\nconclude by applying this methodology to investigate a multi-edge network of\ncorporate email interactions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 18:45:44 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Franke", "Beate", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "1603.01308", "submitter": "Leopoldo Catania", "authors": "Leopoldo Catania", "title": "Dynamic Adaptive Mixture Models", "comments": "47 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new class of Dynamic Mixture Models (DAMMs) being\nable to sequentially adapt the mixture components as well as the mixture\ncomposition using information coming from the data. The information driven\nnature of the proposed class of models allows to exactly compute the full\nlikelihood and to avoid computer intensive simulation schemes. An extensive\nMonte Carlo experiment reveals that the new proposed model can accurately\napproximate the more complicated Stochastic Dynamic Mixture Model previously\nintroduced in the literature as well as other kind of models. The properties of\nthe new proposed class of models are discussed through the paper and an\napplication in financial econometrics is reported.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 22:50:57 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Catania", "Leopoldo", ""]]}, {"id": "1603.01399", "submitter": "Tomoyuki Obuchi", "authors": "Tomoyuki Obuchi and Yoshiyuki Kabashima", "title": "Sampling approach to sparse approximation problem: determining degrees\n  of freedom by simulated annealing", "comments": "5 pages, 3 figures, Proceedings of Eusipco 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cond-mat.dis-nn cond-mat.stat-mech math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximation of a high-dimensional vector by a small combination of\ncolumn vectors selected from a fixed matrix has been actively debated in\nseveral different disciplines. In this paper, a sampling approach based on the\nMonte Carlo method is presented as an efficient solver for such problems.\nEspecially, the use of simulated annealing (SA), a metaheuristic optimization\nalgorithm, for determining degrees of freedom (the number of used columns) by\ncross validation is focused on and tested. Test on a synthetic model indicates\nthat our SA-based approach can find a nearly optimal solution for the\napproximation problem and, when combined with the CV framework, it can optimize\nthe generalization ability. Its utility is also confirmed by application to a\nreal-world supernova data set.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 09:49:46 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 05:06:47 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Obuchi", "Tomoyuki", ""], ["Kabashima", "Yoshiyuki", ""]]}, {"id": "1603.01424", "submitter": "Christian Schellhase Dr.", "authors": "Christian Schellhase, Fabian Spanhel", "title": "Estimating Non-Simplified Vine Copulas Using Penalized Splines", "comments": null, "journal-ref": "Statistics and Computing, 2017", "doi": "10.1007/s11222-017-9737-7", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vine copulas (or pair-copula constructions) have become an important tool for\nhigh-dimensional dependence modeling. Typically, so called simplified vine\ncopula models are estimated where bivariate conditional copulas are\napproximated by bivariate unconditional copulas. We present the first\nnon-parametric estimator of a non-simplified vine copula that allows for\nvarying conditional copulas using penalized hierarchical B-splines. Throughout\nthe vine copula, we test for the simplifying assumption in each edge,\nestablishing a data-driven non-simplified vine copula estimator. To overcome\nthe curse of dimensionality, we approximate conditional copulas with more than\none conditioning argument by a conditional copula with the first principal\ncomponent as conditioning argument. An extensive simulation study is conducted,\nshowing a substantial improvement in the out-of-sample Kullback-Leibler\ndivergence if the null hypothesis of a simplified vine copula can be rejected.\nWe apply our method to the famous uranium data and present a classification of\nan eye state data set, demonstrating the potential benefit that can be achieved\nwhen conditional copulas are modeled.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 11:22:55 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 10:12:27 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Schellhase", "Christian", ""], ["Spanhel", "Fabian", ""]]}, {"id": "1603.01476", "submitter": "Nicole Barthel", "authors": "Nicole Barthel, Candida Geerdens, Matthias Killiches, Paul Janssen and\n  Claudia Czado", "title": "Vine copula based likelihood estimation of dependence patterns in\n  multivariate event time data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many studies multivariate event time data are generated from clusters\nhaving a possibly complex association pattern. Flexible models are needed to\ncapture this dependence. Vine copulas serve this purpose. Inference methods for\nvine copulas are available for complete data. Event time data, however, are\noften subject to right-censoring. As a consequence, the existing inferential\ntools, e.g. likelihood estimation, need to be adapted. A two-stage estimation\napproach is proposed. First, the marginal distributions are modeled. Second,\nthe dependence structure modeled by a vine copula is estimated via likelihood\nmaximization. Due to the right-censoring single and double integrals show up in\nthe copula likelihood expression such that numerical integration is needed for\nits evaluation. For the dependence modeling a sequential estimation approach\nthat facilitates the computational challenges of the likelihood optimization is\nprovided. A three-dimensional simulation study provides evidence for the good\nfinite sample performance of the proposed method. Using four-dimensional\nmastitis data, it is shown how an appropriate vine copula model can be selected\nfor data at hand.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 14:29:18 GMT"}, {"version": "v2", "created": "Sat, 22 Jul 2017 08:22:19 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Barthel", "Nicole", ""], ["Geerdens", "Candida", ""], ["Killiches", "Matthias", ""], ["Janssen", "Paul", ""], ["Czado", "Claudia", ""]]}, {"id": "1603.01631", "submitter": "Wei-Yin Loh", "authors": "Wei-Yin Loh, John Eltinge, MoonJung Cho and Yuanzhi Li", "title": "Classification and regression tree methods for incomplete data from\n  sample surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of sample survey data often requires adjustments to account for\nmissing data in the outcome variables of principal interest. Standard\nadjustment methods based on item imputation or on propensity weighting factors\nrely heavily on the availability of auxiliary variables for both responding and\nnon-responding units. Application of these adjustment methods can be especially\nchallenging in cases for which the auxiliary variables are numerous and are\nthemselves subject to substantial incomplete-data problems. This paper shows\nhow classification and regression trees and forests can overcome some of the\ncomputational difficulties. An in-depth simulation study based on\nincomplete-data patterns encountered in the U.S. Consumer Expenditure Survey is\nused to compare the methods with two standard methods for estimating a\npopulation mean in terms of bias, mean squared error, computational speed and\nnumber of variables that can be analyzed.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 21:11:20 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Loh", "Wei-Yin", ""], ["Eltinge", "John", ""], ["Cho", "MoonJung", ""], ["Li", "Yuanzhi", ""]]}, {"id": "1603.01700", "submitter": "Martin Spindler", "authors": "Victor Chernozhukov and Chris Hansen and Martin Spindler", "title": "High-Dimensional Metrics in R", "comments": "34 pages; vignette for the R package hdm, available at\n  http://cran.r-project.org/web/packages/hdm/ and\n  http://r-forge.r-project.org/R/?group_id=2084 (development version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The package High-dimensional Metrics (\\Rpackage{hdm}) is an evolving\ncollection of statistical methods for estimation and quantification of\nuncertainty in high-dimensional approximately sparse models. It focuses on\nproviding confidence intervals and significance testing for (possibly many)\nlow-dimensional subcomponents of the high-dimensional parameter vector.\nEfficient estimators and uniformly valid confidence intervals for regression\ncoefficients on target variables (e.g., treatment or policy variable) in a\nhigh-dimensional approximately sparse regression model, for average treatment\neffect (ATE) and average treatment effect for the treated (ATET), as well for\nextensions of these parameters to the endogenous setting are provided. Theory\ngrounded, data-driven methods for selecting the penalization parameter in Lasso\nregressions under heteroscedastic and non-Gaussian errors are implemented.\nMoreover, joint/ simultaneous confidence intervals for regression coefficients\nof a high-dimensional sparse regression are implemented, including a joint\nsignificance test for Lasso regression. Data sets which have been used in the\nliterature and might be useful for classroom demonstration and for testing new\nestimators are included. \\R and the package \\Rpackage{hdm} are open-source\nsoftware projects and can be freely downloaded from CRAN:\n\\texttt{http://cran.r-project.org}.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 08:57:26 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 12:20:09 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Hansen", "Chris", ""], ["Spindler", "Martin", ""]]}, {"id": "1603.01775", "submitter": "Sungkyu Jung", "authors": "Sungwon Lee and Sungkyu Jung", "title": "Combined Analysis of Amplitude and Phase Variations in Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When functional data manifest amplitude and phase variations, a\ncommonly-employed framework for analyzing them is to take away the phase\nvariation through a function alignment and then to apply standard tools to the\naligned functions. A downside of this approach is that the important variations\ncontained in the phases are completely ignored. To combine both of amplitude\nand phase variations, we propose a variant of principal component analysis\n(PCA) that captures non-linear components representing the amplitude, phase and\ntheir associations simultaneously. The proposed method, which we call\nfunctional combined PCA, is aimed to provide more efficient dimension reduction\nwith interpretable components, in particular when the amplitudes and phases are\nclearly associated. We model principal components by non-linearly combining\ntime-warping functions and aligned functions. A data-adaptive weighting\nprocedure helps our dimension reduction to attain a maximal explaining power of\nobserved functions. We also discuss an application of functional canonical\ncorrelation analysis in investigation of the correlation structure between the\ntwo variations. We show that for two sets of real data the proposed method\nprovides interpretable major non-linear components, which are not typically\nfound in the usual functional PCA.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 01:47:56 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 01:55:21 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Lee", "Sungwon", ""], ["Jung", "Sungkyu", ""]]}, {"id": "1603.01795", "submitter": "Saeid Rezakhah", "authors": "N. AleMohammad, S. Rezakhah, H. Hoseinalizadeh", "title": "Markov Switching Smooth Transition GARCH Model", "comments": "20 pages. arXiv admin note: text overlap with arXiv:1303.5525", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Markov switching asymmetric GARCH model which imposes more leverage effect\nof the negative shocks is considered. The asymptotic behavior of the second\nmoment is investigated and an upper bound for it is calculated. A bayesian\nstrategy through Gibbs and griddy Gibbs sampling is used to estimate the\nparameters. Finally we study the performance of the model by two real data\nsets. We show that this model has the best in-sample fit via DIC and provides a\nbetter forecast when the negative skewness is large enough.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 06:32:36 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 21:47:13 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["AleMohammad", "N.", ""], ["Rezakhah", "S.", ""], ["Hoseinalizadeh", "H.", ""]]}, {"id": "1603.01851", "submitter": "Zhigang Li", "authors": "Zhigang Li, H. R. Frost, Tor D. Tosteson, Lihui Zhao, Lei Liu,\n  Kathleen Lyons, Huaihou Chen, Bernard Cole, David Currow and Marie Bakitas", "title": "A Semiparametric Joint Model for Terminal Trend of Quality of Life and\n  Survival in Palliative Care Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Palliative medicine is an interdisciplinary specialty focusing on improving\nquality of life (QOL) for patients with serious illness and their families.\nPalliative care programs are available or under development at over 80% of\nlarge US hospitals (300+ beds). Palliative care clinical trials present unique\nanalytic challenges relative to evaluating the palliative care treatment\nefficacy which is to improve patients diminishing QOL as disease progresses\ntowards end of life (EOL). A unique feature of palliative care clinical trials\nis that patients will experience decreasing QOL during the trial despite\npotentially beneficial treatment. Often longitudinal QOL and survival data are\nhighly correlated which, in the face of censoring, makes it challenging to\nproperly analyze and interpret longitudinal QOL trajectory. To address these\nissues, we propose a novel semiparametric statistical approach to jointly model\nlongitudinal QOL and survival data. There are two sub-models in our approach: a\nsemiparametric mixed effects model for longitudinal QOL and a Cox model for\nsurvival. We use regression splines method to estimate the nonparametric curves\nand AIC to select knots. We assess the model through simulation and application\nto establish a novel modeling approach that could be applied in future\npalliative care treatment research trials.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 18:03:01 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 16:40:41 GMT"}, {"version": "v3", "created": "Sat, 16 Apr 2016 00:35:05 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Li", "Zhigang", ""], ["Frost", "H. R.", ""], ["Tosteson", "Tor D.", ""], ["Zhao", "Lihui", ""], ["Liu", "Lei", ""], ["Lyons", "Kathleen", ""], ["Chen", "Huaihou", ""], ["Cole", "Bernard", ""], ["Currow", "David", ""], ["Bakitas", "Marie", ""]]}, {"id": "1603.01874", "submitter": "Ran Dai", "authors": "Cheng Zheng, Ran Dai, Parameswaran Hari, and Mei-Jie Zhang", "title": "Instrumental Variable with Competing Risk Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss causal inference on the efficacy of a treatment or\nmedication on a time-to-event outcome with competing risks. Although the\ntreatment group can be randomized, there can be confoundings between the\ncompliance and the outcome. Unmeasured confoundings may exist even after\nadjustment for measured co- variates. Instrumental variable (IV) methods are\ncommonly used to yield consistent estimations of causal parameters in the\npresence of unmeasured confoundings. Based on a semi-parametric additive hazard\nmodel for the subdistribution hazard, we pro- pose an instrumental variable\nestimator to yield consistent estimation of efficacy in the presence of\nunmeasured confoundings for competing risk settings. We derived the asymptotic\nproperties for the proposed estimator. The estimator is shown to be well per-\nformed under finite sample size according to simulation results. We applied our\nmethod to a real transplant data example and showed that the unmeasured\nconfoundings lead to significant bias in the estimation of the effect (about\n50% attenuated).\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 20:50:10 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 19:49:58 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Zheng", "Cheng", ""], ["Dai", "Ran", ""], ["Hari", "Parameswaran", ""], ["Zhang", "Mei-Jie", ""]]}, {"id": "1603.01882", "submitter": "Robert Zinkov", "authors": "Robert Zinkov, Chung-chieh Shan", "title": "Composing inference algorithms as program transformations", "comments": "10 pages, 5 figures. To appear in Proceedings of the 33rd Conference\n  on Uncertainty in Artificial Intelligence (UAI2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic inference procedures are usually coded painstakingly from\nscratch, for each target model and each inference algorithm. We reduce this\neffort by generating inference procedures from models automatically. We make\nthis code generation modular by decomposing inference algorithms into reusable\nprogram-to-program transformations. These transformations perform exact\ninference as well as generate probabilistic programs that compute expectations,\ndensities, and MCMC samples. The resulting inference procedures are about as\naccurate and fast as other probabilistic programming systems on real-world\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 21:30:10 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 16:01:42 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Zinkov", "Robert", ""], ["Shan", "Chung-chieh", ""]]}, {"id": "1603.01897", "submitter": "Gael Martin Prof", "authors": "Don S. Poskitt, Gael M. Martin and Simone D. Grose", "title": "Bias Correction of Semiparametric Long Memory Parameter Estimators via\n  the Pre-filtered Sieve Bootstrap", "comments": "This is an extended version (with additional numerical results) of\n  the paper with the same name forthcoming in Econometric Theory, 2016. arXiv\n  admin note: substantial text overlap with arXiv:1402.6781", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates bootstrap-based bias correction of semiparametric\nestimators of the long memory parameter, $d$, in fractionally integrated\nprocesses. The re-sampling method involves the application of the sieve\nbootstrap to data pre-filtered by a preliminary semiparametric estimate of the\nlong memory parameter. Theoretical justification for using the bootstrap\ntechnique to bias adjust log periodogram and semiparametric local Whittle\nestimators of the memory parameter is provided in the case where the true value\nof $d$ lies in the range $0\\leq d<0.5$. That the bootstrap method provides\nconfidence intervals with the correct asymptotic coverage is also proven, with\nthe intervals shown to adjust explicitly for bias, as estimated via the\nbootstrap. Simulation evidence comparing the performance of the bootstrap bias\ncorrection with analytical bias-correction techniques is presented. The\nbootstrap method is shown to produce notable bias reductions, in particular\nwhen applied to an estimator for which some degree of bias reduction has\nalready been accomplished by analytical means.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 23:29:05 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Poskitt", "Don S.", ""], ["Martin", "Gael M.", ""], ["Grose", "Simone D.", ""]]}, {"id": "1603.01985", "submitter": "Lucia Modugno mrs", "authors": "Silvia Cagnone, Simone Giannerini, Lucia Modugno", "title": "Multilevel Models with Stochastic Volatility for Repeated\n  Cross-Sections: an Application to tribal Art Prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a multilevel specification with stochastic\nvolatility for repeated cross-sectional data. Modelling the time dynamics in\nrepeated cross sections requires a suitable adaptation of the multilevel\nframework where the individuals/items are modelled at the first level whereas\nthe time component appears at the second level. We perform maximum likelihood\nestimation by means of a nonlinear state space approach combined with\nGauss-Legendre quadrature methods to approximate the likelihood function. We\napply the model to the first database of tribal art items sold in the most\nimportant auction houses worldwide. The model allows to account properly for\nthe heteroscedastic and autocorrelated volatility observed and has superior\nforecasting performance. Also, it provides valuable information on market\ntrends and on predictability of prices that can be used by art markets\nstakeholders.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 09:47:12 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Cagnone", "Silvia", ""], ["Giannerini", "Simone", ""], ["Modugno", "Lucia", ""]]}, {"id": "1603.02049", "submitter": "Johannes Klepsch", "authors": "J. Klepsch, C. Kl\\\"uppelberg and T. Wei", "title": "Prediction of functional ARMA processes with an application to traffic\n  data", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is devoted to functional ARMA$(p, q)$ processes and approximating\nvector models based on functional PCA in the context of prediction. After\nderiving sufficient conditions for the existence of a stationary solution to\nboth the functional and the vector model equations, the structure of the\napproximating vector model is investigated. The stationary vector process is\nused to predict the functional process. A bound for the difference between\nvector and functional best linear predictor is derived. The paper concludes by\napplying functional ARMA processes for the modeling and prediction of highway\ntraffic data.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 13:15:00 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 13:44:14 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Klepsch", "J.", ""], ["Kl\u00fcppelberg", "C.", ""], ["Wei", "T.", ""]]}, {"id": "1603.02245", "submitter": "Yakov Nikitin", "authors": "Ya. Yu. Nikitin and K. Yu. Volkova", "title": "Efficiency of Exponentiality Tests Based on a Special Property of\n  Exponential Distribution", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New goodness-of-fit tests for exponentiality based on a particular property\nof exponential law are constructed. Test statistics are functionals of\nU-empirical processes. The first of these statistics is of integral type, the\nsecond one is a Kolmogorov type statistic. We show that the kernels\ncorresponding to our statistics are nondegenerate. The limiting distributions\nand large deviations of new statistics under the null hypothesis are described.\nTheir local Bahadur efficiency for various parametric alternatives is\ncalculated and is compared with simulated powers of new tests. Conditions of\nlocal optimality of new statistics in Bahadur sense are discussed and examples\nof \"most favorable\" alternatives are given. New tests are applied to reject the\nhypothesis of exponentiality for the length of reigns of Roman emperors which\nwas intensively discussed in recent years.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 20:34:09 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Nikitin", "Ya. Yu.", ""], ["Volkova", "K. Yu.", ""]]}, {"id": "1603.02485", "submitter": "Robert Kohn", "authors": "M.-N. Tran, R. Kohn, M. Quiroz and M. Villani", "title": "The Block Pseudo-Marginal Sampler", "comments": "41 pages, 6 tables , 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pseudo-marginal (PM) approach is increasingly used for Bayesian inference\nin statistical models, where the likelihood is intractable but can be estimated\nunbiasedly. %Examples include random effect models, state-space models and data\nsubsampling in big-data settings. Deligiannidis et al. (2016) show how the PM\napproach can be made much more efficient by correlating the underlying Monte\nCarlo (MC) random numbers used to form the estimate of the likelihood at the\ncurrent and proposed values of the unknown parameters. Their approach greatly\nspeeds up the standard PM algorithm, as it requires a much smaller number of\nsamples or particles to form the optimal likelihood estimate. Our paper\npresents an alternative implementation of the correlated PM approach, called\nthe block PM, which divides the underlying random numbers into blocks so that\nthe likelihood estimates for the proposed and current values of the parameters\nonly differ by the random numbers in one block. We show that this\nimplementation of the correlated PM can be much more efficient for some\nspecific problems than the implementation in Deligiannidis et al. (2016); for\nexample when the likelihood is estimated by subsampling or the likelihood is a\nproduct of terms each of which is given by an integral which can be estimated\nunbiasedly by randomised quasi-Monte Carlo. Our article provides methodology\nand guidelines for efficiently implementing the block PM. A second advantage of\nthe the block PM is that it provides a direct way to control the correlation\nbetween the logarithms of the estimates of the likelihood at the current and\nproposed values of the parameters than the implementation in Deligiannidis et\nal. (2016). We obtain methods and guidelines for selecting the optimal number\nof samples based on idealized but realistic assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 11:40:24 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 07:12:09 GMT"}, {"version": "v3", "created": "Thu, 10 Nov 2016 05:29:12 GMT"}, {"version": "v4", "created": "Sat, 18 Mar 2017 12:22:38 GMT"}, {"version": "v5", "created": "Sun, 10 Sep 2017 03:33:09 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Tran", "M. -N.", ""], ["Kohn", "R.", ""], ["Quiroz", "M.", ""], ["Villani", "M.", ""]]}, {"id": "1603.02712", "submitter": "Yunjian Yin", "authors": "Yunjian Yin, Lan Liu, Zhi Geng", "title": "Assessing the Treatment Effect Heterogeneity with a Latent Variable", "comments": "21 pages, 3 tables, statistical methodology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The average treatment effect (ATE) is popularly used to assess the treatment\neffect. However, the ATE implicitly assumes a homogenous treatment effect even\namongst individuals with different characteristics. In this paper, we mainly\nfocus on assessing the treatment effect heterogeneity, which has important\nimplications in designing the optimal individual treatment regimens and in\npolicy making. The treatment benefit rate (TBR) and treatment harm rate (THR)\nhave been defined to characterize the magnitude of heterogeneity for binary\noutcomes. When the outcomes are continuous, we extend the definitions of the\nTBR and THR to compare the difference between potential outcomes with a\npre-specified level c. Unlike the ATE, these rates involve the joint\ndistribution of the potential outcomes and can not be identified without\nfurther assumptions even in randomized clinical trials. In this article, we\nassume the potential outcomes are independent conditional on the observed\ncovariates and an unmeasured latent variable. Under this assumption, we prove\nthe identification of the TBR and THR in non-separable (generalized) linear\nmodels for both continuous and binary outcomes. We then propose estimators and\nderive their asymptotic distributions. In the simulation studies, we implement\nour proposed methods to assess the performance of our estimators and carry out\na sensitive analysis for different underlying distribution for the latent\nvariable. Finally, we illustrate the proposed methods in two randomized\ncontrolled trials.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 21:33:41 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Yin", "Yunjian", ""], ["Liu", "Lan", ""], ["Geng", "Zhi", ""]]}, {"id": "1603.02745", "submitter": "Francois Bavaud", "authors": "Fran\\c{c}ois Bavaud", "title": "Non-parametric latent modeling and network clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper exposes a non-parametric approach to latent and co-latent modeling\nof bivariate data, based upon alternating minimization of the Kullback-Leibler\ndivergence (EM algorithm) for complete log-linear models. For categorical data,\nthe iterative algorithm generates a soft clustering of both rows and columns of\nthe contingency table. Well-known results are systematically revisited, and\nsome variants are presumably original. In particular, the consideration of\nsquare contingency tables induces a clustering algorithm for weighted networks,\ndiffering from spectral clustering or modularity maximization techniques. Also,\nwe present a co-clustering algorithm applicable to HMM models of general kind,\ndistinct from the Baum-Welch algorithm. Three case studies illustrate the\ntheory.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 00:05:49 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Bavaud", "Fran\u00e7ois", ""]]}, {"id": "1603.02791", "submitter": "Yanglei Song", "authors": "Yanglei Song and Georgios Fellouris", "title": "Asymptotically optimal, sequential, multiple testing procedures with\n  prior information on the number of signals", "comments": "30 pages, 3 figures, 1 table", "journal-ref": "Electron. J. Statist. 11 (2017), no. 1, 338--363", "doi": "10.1214/17-EJS1223", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assuming that data are collected sequentially from independent streams, we\nconsider the simultaneous testing of multiple binary hypotheses under two\ngeneral setups; when the number of signals (correct alternatives) is known in\nadvance, and when we only have a lower and an upper bound for it. In each of\nthese setups, we propose feasible procedures that control, without any\ndistributional assumptions, the familywise error probabilities of both type I\nand type II below given, user-specified levels. Then, in the case of i.i.d.\nobservations in each stream, we show that the proposed procedures achieve the\noptimal expected sample size, under every possible signal configuration,\nasymptotically as the two error probabilities vanish at arbitrary rates. A\nsimulation study is presented in a completely symmetric case and supports\ninsights obtained from our asymptotic results, such as the fact that knowledge\nof the exact number of signals roughly halves the expected number of\nobservations compared to the case of no prior information.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 06:41:39 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2016 23:51:21 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Song", "Yanglei", ""], ["Fellouris", "Georgios", ""]]}, {"id": "1603.02802", "submitter": "Alan Huang", "authors": "Thomas Fung, Alan Huang", "title": "Semiparametric generalized linear models for time-series data", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series data in population health and epidemiology often involve\nnon-Gaussian responses. In this note, we propose a semiparametric generalized\nlinear models framework for time-series data that does not require\nspecification of a working conditional response distribution for the data.\nInstead, the underlying response distribution is treated as an\ninfinite-dimensional parameter which is estimated simultaneously with the usual\nfinite-dimensional parameters via a maximum empirical likelihood approach. A\ngeneral consistency result for the resulting estimators is given. Simulations\nsuggest that both estimation and inferences using the proposed method can\nperform as well as correctly-specified parametric models even for moderate\nsample sizes, but can be more robust than parametric methods under model\nmisspecification. The method is used to analyse the Polio dataset from Zeger\n(1988) and a recent Kings Cross assault dataset from Menendez et al. (2015).\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 07:55:34 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Fung", "Thomas", ""], ["Huang", "Alan", ""]]}, {"id": "1603.02834", "submitter": "Jere Koskela", "authors": "Jere Koskela, Dario Spano and Paul A. Jenkins", "title": "Inference and rare event simulation for stopped Markov processes via\n  reverse-time sequential Monte Carlo", "comments": "21 pages, 6 figures", "journal-ref": "Statistics and Computing 28(1):131-144, 2018", "doi": "10.1007/s11222-017-9722-1", "report-no": null, "categories": "stat.CO math.PR q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sequential Monte Carlo algorithm for Markov chain trajectories\nwith proposals constructed in reverse time, which is advantageous when paths\nare conditioned to end in a rare set. The reverse time proposal distribution is\nconstructed by approximating the ratio of Green's functions in Nagasawa's\nformula. Conditioning arguments can be used to interpret these ratios as\nlow-dimensional conditional sampling distributions of some coordinates of the\nprocess given the others. Hence the difficulty in designing SMC proposals in\nhigh dimension is greatly reduced. We illustrate our method on estimating an\noverflow probability in a queueing model, the probability that a diffusion\nfollows a narrowing corridor, and the initial location of an infection in an\nepidemic model on a network.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 10:26:24 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 10:02:39 GMT"}, {"version": "v3", "created": "Mon, 2 Jan 2017 13:37:02 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Koskela", "Jere", ""], ["Spano", "Dario", ""], ["Jenkins", "Paul A.", ""]]}, {"id": "1603.02982", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal, David S. Matteson, and David Ruppert", "title": "Functional Autoregression for Sparsely Sampled Data", "comments": null, "journal-ref": null, "doi": "10.1080/07350015.2017.1279058", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a hierarchical Gaussian process model for forecasting and\ninference of functional time series data. Unlike existing methods, our approach\nis especially suited for sparsely or irregularly sampled curves and for curves\nsampled with non-negligible measurement error. The latent process is\ndynamically modeled as a functional autoregression (FAR) with Gaussian process\ninnovations. We propose a fully nonparametric dynamic functional factor model\nfor the dynamic innovation process, with broader applicability and improved\ncomputational efficiency over standard Gaussian process models. We prove\nfinite-sample forecasting and interpolation optimality properties of the\nproposed model, which remain valid with the Gaussian assumption relaxed. An\nefficient Gibbs sampling algorithm is developed for estimation, inference, and\nforecasting, with extensions for FAR(p) models with model averaging over the\nlag p. Extensive simulations demonstrate substantial improvements in\nforecasting performance and recovery of the autoregressive surface over\ncompeting methods, especially under sparse designs. We apply the proposed\nmethods to forecast nominal and real yield curves using daily U.S. data. Real\nyields are observed more sparsely than nominal yields, yet the proposed methods\nare highly competitive in both settings.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 18:01:49 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 02:04:59 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2016 14:55:24 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Kowal", "Daniel R.", ""], ["Matteson", "David S.", ""], ["Ruppert", "David", ""]]}, {"id": "1603.03028", "submitter": "Radu V. Craiu", "authors": "Evgeny Levi and Radu V. Craiu", "title": "Gaussian Process Single Index Models for Conditional Copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric conditional copula models allow the copula parameters to vary with\na set of covariates according to an unknown calibration function. Flexible\nBayesian inference for the calibration function of a bivariate conditional\ncopula is proposed via a sparse Gaussian process (GP) prior distribution over\nthe set of smooth calibration functions for the single index model (SIM). The\nestimation of parameters from the marginal distributions and the calibration\nfunction is done jointly via Markov Chain Monte Carlo sampling from the full\nposterior distribution. A new Conditional Cross Validated Pseudo-Marginal\n(CCVML) criterion is introduced in order to perform copula selection and is\nmodified using a permutation-based procedure to assess data support for the\nsimplifying assumption. The performance of the estimation method and model\nselection criteria is studied via a series of simulations using correct and\nmisspecified models with Clayton, Frank and Gaussian copulas and a numerical\napplication involving red wine features.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 20:59:41 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 01:52:38 GMT"}, {"version": "v3", "created": "Thu, 25 May 2017 14:05:15 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Levi", "Evgeny", ""], ["Craiu", "Radu V.", ""]]}, {"id": "1603.03174", "submitter": "Julie Josse", "authors": "Patrick J.F. Groenen and Julie Josse", "title": "Multinomial Multiple Correspondence Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relations between categorical variables can be analyzed conveniently by\nmultiple correspondence analysis (MCA). %It is well suited to discover\nrelations that may exist between categories of different variables. The\ngraphical representation of MCA results in so-called biplots makes it easy to\ninterpret the most important associations. However, a major drawback of MCA is\nthat it does not have an underlying probability model for an individual\nselecting a category on a variable. In this paper, we propose such probability\nmodel called multinomial multiple correspondence analysis (MMCA) that combines\nthe underlying low-rank representation of MCA with maximum likelihood. An\nefficient majorization algorithm that uses an elegant bound for the second\nderivative is derived to estimate the parameters. The proposed model can easily\nlead to overfitting causing some of the parameters to wander of to infinity. We\nadd the nuclear norm penalty to counter this issue and discuss ways of\nselecting regularization parameters. The proposed approach is well suited to\nstudy and vizualise the dependences for high dimensional data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 08:01:08 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Groenen", "Patrick J. F.", ""], ["Josse", "Julie", ""]]}, {"id": "1603.03221", "submitter": "Guy Nason Prof.", "authors": "M. I. Knight, M. A. Nunes and G. P. Nason", "title": "Modelling, Detrending and Decorrelation of Network Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A network time series is a multivariate time series augmented by a graph that\ndescribes how variables (or nodes) are connected. We introduce the network\nautoregressive (integrated) moving average (NARIMA) processes: a set of\nflexible models for network time series. For fixed networks the NARIMA models\nare essentially equivalent to vector autoregressive moving average-type models.\nHowever, NARIMA models are especially useful when the structure of the graph,\nassociated with the multivariate time series, changes over time. Such network\ntopology changes are invisible to standard VARMA-like models. For integrated\nNARIMA models we introduce network differencing, based on the network lifting\n(wavelet) transform, which removes trend. We exhibit our techniques on a\nnetwork time series describing the evolution of mumps throughout counties of\nEngland and Wales weekly during 2005. We further demonstrate the action of\nnetwork lifting on a simple bivariate VAR(1) model with associated two-node\ngraph. We show theoretically that decorrelation occurs only in certain\ncircumstances and maybe less than expected. This suggests that the\ntime-decorrelation properties of spatial network lifting are due more to the\ntrend removal properties of lifting rather than any kind of stochastic\ndecorrelation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 11:08:26 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Knight", "M. I.", ""], ["Nunes", "M. A.", ""], ["Nason", "G. P.", ""]]}, {"id": "1603.03336", "submitter": "Francois Belletti", "authors": "Francois W. Belletti, Evan R. Sparks, Michael J. Franklin, Alexandre\n  M. Bayen, Joseph E. Gonzalez", "title": "Scalable Linear Causal Inference for Irregularly Sampled Time Series\n  with Long Range Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear causal analysis is central to a wide range of important application\nspanning finance, the physical sciences, and engineering. Much of the existing\nliterature in linear causal analysis operates in the time domain.\nUnfortunately, the direct application of time domain linear causal analysis to\nmany real-world time series presents three critical challenges: irregular\ntemporal sampling, long range dependencies, and scale. Moreover, real-world\ndata is often collected at irregular time intervals across vast arrays of\ndecentralized sensors and with long range dependencies which make naive time\ndomain correlation estimators spurious. In this paper we present a frequency\ndomain based estimation framework which naturally handles irregularly sampled\ndata and long range dependencies while enabled memory and communication\nefficient distributed processing of time series data. By operating in the\nfrequency domain we eliminate the need to interpolate and help mitigate the\neffects of long range dependencies. We implement and evaluate our new work-flow\nin the distributed setting using Apache Spark and demonstrate on both Monte\nCarlo simulations and high-frequency financial trading that we can accurately\nrecover causal structure at scale.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 17:12:03 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Belletti", "Francois W.", ""], ["Sparks", "Evan R.", ""], ["Franklin", "Michael J.", ""], ["Bayen", "Alexandre M.", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1603.03418", "submitter": "Ruth Heller", "authors": "Ruth Heller and Yair Heller", "title": "Multivariate tests of association based on univariate tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For testing two random vectors for independence, we consider testing whether\nthe distance of one vector from a center point is independent from the distance\nof the other vector from a center point by a univariate test. In this paper we\nprovide conditions under which it is enough to have a consistent univariate\ntest of independence on the distances to guarantee that the power to detect\ndependence between the random vectors increases to one, as the sample size\nincreases. These conditions turn out to be minimal. If the univariate test is\ndistribution-free, the multivariate test will also be distribution-free. If we\nconsider multiple center points and aggregate the center-specific univariate\ntests, the power may be further improved, and the resulting multivariate test\nmay be distribution-free for specific aggregation methods (if the univariate\ntest is distribution-free). We show that several multivariate tests recently\nproposed in the literature can be viewed as instances of this general approach.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 20:47:54 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Heller", "Ruth", ""], ["Heller", "Yair", ""]]}, {"id": "1603.03449", "submitter": "Ehsan Taghavi", "authors": "Ehsan Taghavi, R. Tharmarasa, T. Kirubarajan, Yaakov Bar-Shalom and\n  Mike McDonald", "title": "A Practical Bias Estimation Algorithm for Multisensor--Multitarget\n  Tracking", "comments": null, "journal-ref": "IEEE Transactions on Aerospace and Electronics Systems, 52 (1),\n  2016", "doi": "10.1109/TAES.2015.140574", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bias estimation or sensor registration is an essential step in ensuring the\naccuracy of global tracks in multisensor-multitarget tracking. Most previously\nproposed algorithms for bias estimation rely on local measurements in\ncentralized systems or tracks in distributed systems, along with additional\ninformation like covariances, filter gains or targets of opportunity. In\naddition, it is generally assumed that such data are made available to the\nfusion center at every sampling time. In practical distributed multisensor\ntracking systems, where each platform sends local tracks to the fusion center,\nonly state estimates and, perhaps, their covariances are sent to the fusion\ncenter at non-consecutive sampling instants or scans. That is, not all the\ninformation required for exact bias estimation at the fusion center is\navailable in practical distributed tracking systems. In this paper, a new\nalgorithm that is capable of accurately estimating the biases even in the\nabsence of filter gain information from local platforms is proposed for\ndistributed tracking systems with intermittent track transmission. Through the\ncalculation of the Posterior Cram\\'er--Rao lower bound and various simulation\nresults, it is shown that the performance of the new algorithm, which uses the\ntracklet idea and does not require track transmission at every sampling time or\nexchange of filter gains, can approach the performance of the exact bias\nestimation algorithm that requires local filter gains.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 14:59:45 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Taghavi", "Ehsan", ""], ["Tharmarasa", "R.", ""], ["Kirubarajan", "T.", ""], ["Bar-Shalom", "Yaakov", ""], ["McDonald", "Mike", ""]]}, {"id": "1603.03450", "submitter": "Ehsan Taghavi", "authors": "Ehsan Taghavi, R. Tharmarasa, T. Kirubarajan and Mike McDonald", "title": "Multisensor--Multitarget Bearing--Only Sensor Registration", "comments": null, "journal-ref": "IEEE Transactions on Aerospace and Electronics Systems, 52 (4),\n  2016", "doi": null, "report-no": null, "categories": "stat.ME cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bearing--only estimation is one of the fundamental and challenging problems\nin target tracking. As in the case of radar tracking, the presence of offset or\nposition biases can exacerbate the challenges in bearing--only estimation.\nModeling various sensor biases is not a trivial task and not much has been done\nin the literature specifically for bearing--only tracking. This paper addresses\nthe modeling of offset biases in bearing--only sensors and the ensuing\nmultitarget tracking with bias compensation. Bias estimation is handled at the\nfusion node to which individual sensors report their local tracks in the form\nof associated measurement reports (AMR) or angle-only tracks. The modeling is\nbased on a multisensor approach that can effectively handle a time--varying\nnumber of targets in the surveillance region. The proposed algorithm leads to a\nmaximum likelihood bias estimator. The corresponding Cram\\'er--Rao Lower Bound\nto quantify the theoretical accuracy that can be achieved by the proposed\nmethod or any other algorithm is also derived. Finally, simulation results on\ndifferent distributed tracking scenarios are presented to demonstrate the\ncapabilities of the proposed approach. In order to show that the proposed\nmethod can work even with false alarms and missed detections, simulation\nresults on a centralized tracking scenario where the local sensors send all\ntheir measurements (not AMRs or local tracks) are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 14:52:17 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Taghavi", "Ehsan", ""], ["Tharmarasa", "R.", ""], ["Kirubarajan", "T.", ""], ["McDonald", "Mike", ""]]}, {"id": "1603.03481", "submitter": "Robert Staudte", "authors": "Luke A. Prendergast and Robert G. Staudte", "title": "A Simple and Effective Inequality Measure", "comments": "20 pages, 9 figures", "journal-ref": "The American Statistician Volume 72, 2018 - Issue 4", "doi": "10.1080/00031305.2017.1366366", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ratios of quantiles are often computed for income distributions as rough\nmeasures of inequality, and inference for such ratios have recently become\navailable. The special case when the quantiles are symmetrically chosen; that\nis, when the p/2 quantile is divided by the (1-p/2), is of special interest\nbecause the graph of such ratios, plotted as a function of p over the unit\ninterval, yields an informative inequality curve. The area above the curve and\nless than the horizontal line at one is an easily interpretable coefficient of\ninequality. The advantages of these concepts over the traditional Lorenz curve\nand Gini coefficient are numerous: they are defined for all positive income\ndistributions, they can be robustly estimated and distribution-free confidence\nintervals for the inequality coefficient are easily found. Moreover the\ninequality curves satisfy a median-based transference principle and are convex\nfor many commonly assumed income distributions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 22:54:14 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Prendergast", "Luke A.", ""], ["Staudte", "Robert G.", ""]]}, {"id": "1603.03484", "submitter": "Luca Rossini", "authors": "Luciana Dalla Valle, Fabrizio Leisen, Luca Rossini", "title": "Bayesian Nonparametric Conditional Copula Estimation of Twin Data", "comments": "Forthcoming in Journal of the Royal Statistical Society (Series C)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several studies on heritability in twins aim at understanding the different\ncontribution of environmental and genetic factors to specific traits.\nConsidering the National Merit Twin Study, our purpose is to correctly analyse\nthe influence of the socioeconomic status on the relationship between twins'\ncognitive abilities. Our methodology is based on conditional copulas, which\nallow us to model the effect of a covariate driving the strength of dependence\nbetween the main variables. We propose a flexible Bayesian nonparametric\napproach for the estimation of conditional copulas, which can model any\nconditional copula density. Our methodology extends the work of Wu et al (2015)\nby introducing dependence from a covariate in an infinite mixture model. Our\nresults suggest that environmental factors are more influential in families\nwith lower socio-economic position.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 23:11:37 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 09:28:20 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 08:10:05 GMT"}, {"version": "v4", "created": "Mon, 3 Jul 2017 06:28:35 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Valle", "Luciana Dalla", ""], ["Leisen", "Fabrizio", ""], ["Rossini", "Luca", ""]]}, {"id": "1603.03640", "submitter": "Marcela Svarc", "authors": "Ana Justel and Marcela Svarc", "title": "Sequential Clustering for Functional Data", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents SeqClusFD, a top-down sequential clustering method for\nfunctional data. The clustering algorithm extracts the splitting information\neither from trajectories, first or second derivatives. Initial partition is\nbased on gap statistic that provides local information to identify the instant\nwith more clustering evidence in trajectories or derivatives. Then functional\nboxplots allow reconsidering overall allocation and each observation is finally\nassigned to the cluster where it spends most of the time within whiskers. These\nlocal and global searches are repeated recursively until there is no evidence\nof clustering at any time on trajectories or first and second derivatives.\nSeqClusFD simultaneously estimates the number of groups and provides data\nallocation. It also provides valuable information about the most important\nfeatures that determine cluster structure. Computational aspects have been\nanalyzed and the new method is tested on synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 14:30:30 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 22:50:45 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Justel", "Ana", ""], ["Svarc", "Marcela", ""]]}, {"id": "1603.03675", "submitter": "Sokbae Lee", "authors": "Pedro Carneiro, Sokbae Lee, Daniel Wilhelm", "title": "Optimal Data Collection for Randomized Control Trials", "comments": "54 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a randomized control trial, the precision of an average treatment effect\nestimator can be improved either by collecting data on additional individuals,\nor by collecting additional covariates that predict the outcome variable. We\npropose the use of pre-experimental data such as a census, or a household\nsurvey, to inform the choice of both the sample size and the covariates to be\ncollected. Our procedure seeks to minimize the resulting average treatment\neffect estimator's mean squared error, subject to the researcher's budget\nconstraint. We rely on a modification of an orthogonal greedy algorithm that is\nconceptually simple and easy to implement in the presence of a large number of\npotential covariates, and does not require any tuning parameters. In two\nempirical applications, we show that our procedure can lead to substantial\ngains of up to 58%, measured either in terms of reductions in data collection\ncosts or in terms of improvements in the precision of the treatment effect\nestimator.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 16:06:03 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 16:16:11 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2016 14:37:33 GMT"}, {"version": "v4", "created": "Mon, 22 Aug 2016 08:11:03 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Carneiro", "Pedro", ""], ["Lee", "Sokbae", ""], ["Wilhelm", "Daniel", ""]]}, {"id": "1603.03719", "submitter": "Niharika Gauraha Niharika Gauraha", "authors": "Niharika Gauraha", "title": "Model Selection for Graphical Log-linear Models: A Forward Model\n  Selection Algorithm based on Mutual Conditional Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection and learning the structure of graphical models from the data\nsample constitutes an important field of probabilistic graphical model\nresearch, as in most of the situations the structure is unknown and has to be\nlearnt from the given dataset. In this paper, we present a new forward model\nselection algorithm for graphical log-linear models. We use mutual conditional\nindependence check to reduce the search space which also takes care of the\nevaluation of the joint effects and chances of missing important interactions\nare eliminated. We illustrate our algorithm with a real dataset example.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 18:31:28 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Gauraha", "Niharika", ""]]}, {"id": "1603.03733", "submitter": "Niharika Gauraha Niharika Gauraha", "authors": "Niharika Gauraha", "title": "Mutual Conditional Independence and its Applications to Inference in\n  Markov Networks", "comments": "Submitted to JMLR on 02 Apr 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental concepts underlying in Markov networks are the conditional\nindependence and the set of rules called Markov properties that translates\nconditional independence constraints into graphs. In this article we introduce\nthe concept of mutual conditional independence relationship among elements of\nan independent set of a Markov network. We first prove that the mutual\nconditional independence property holds within the elements of a maximal\nindependent set afterwards we prove equivalence between the set of mutual\nconditional independence relations encoded by all the maximal independent sets\nand the three Markov properties(pair-wise, local and the global) under certain\nregularity conditions. The proof employs diverse methods involving graphoid\naxioms, factorization of the joint probability density functions and the graph\ntheory. We present inference methods for decomposable and non-decomposable\ngraphical models exploiting newly revealed mutual conditional independence\nproperty.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 19:31:07 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Gauraha", "Niharika", ""]]}, {"id": "1603.03788", "submitter": "Andrey Kormilitzin", "authors": "Ilya Chevyrev, Andrey Kormilitzin", "title": "A Primer on the Signature Method in Machine Learning", "comments": "45 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In these notes, we wish to provide an introduction to the signature method,\nfocusing on its basic theoretical properties and recent numerical applications.\n  The notes are split into two parts. The first part focuses on the definition\nand fundamental properties of the signature of a path, or the path signature.\nWe have aimed for a minimalistic approach, assuming only familiarity with\nclassical real analysis and integration theory, and supplementing theory with\nstraightforward examples. We have chosen to focus in detail on the principle\nproperties of the signature which we believe are fundamental to understanding\nits role in applications. We also present an informal discussion on some of its\ndeeper properties and briefly mention the role of the signature in rough paths\ntheory, which we hope could serve as a light introduction to rough paths for\nthe interested reader.\n  The second part of these notes discusses practical applications of the path\nsignature to the area of machine learning. The signature approach represents a\nnon-parametric way for extraction of characteristic features from data. The\ndata are converted into a multi-dimensional path by means of various embedding\nalgorithms and then processed for computation of individual terms of the\nsignature which summarise certain information contained in the data. The\nsignature thus transforms raw data into a set of features which are used in\nmachine learning tasks. We will review current progress in applications of\nsignatures to machine learning problems.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 21:24:42 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Chevyrev", "Ilya", ""], ["Kormilitzin", "Andrey", ""]]}, {"id": "1603.03950", "submitter": "Pavel Krupskii", "authors": "Pavel Krupskii and Marc G. Genton", "title": "A Copula Model for Non-Gaussian Multivariate Spatial Data", "comments": "33 pages, 4 tables and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new copula model for replicated multivariate spatial data.\nUnlike classical models that assume multivariate normality of the data, the\nproposed copula is based on the assumption that some factors exist that affect\nthe joint spatial dependence of all measurements of each variable as well as\nthe joint dependence among these variables. The model is parameterized in terms\nof a cross-covariance function that may be chosen from the many models proposed\nin the literature. In addition, there are additive factors in the model that\nallow tail dependence and reflection asymmetry of each variable measured at\ndifferent locations and of different variables to be modeled. The proposed\napproach can therefore be seen as an extension of the linear model of\ncoregionalization widely used for modeling multivariate spatial data. The\nlikelihood of the model can be obtained in a simple form and therefore the\nlikelihood estimation is quite fast. The model is not restricted to the set of\ndata locations, and using the estimated copula, spatial data can be\ninterpolated at locations where values of variables are unknown. We apply the\nproposed model to temperature and pressure data and compare its performance\nwith the performance of a popular model from multivariate geostatistics.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 17:59:59 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 22:11:09 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Krupskii", "Pavel", ""], ["Genton", "Marc G.", ""]]}, {"id": "1603.04093", "submitter": "Wei Ning", "authors": "Ying-Ju Chen and Wei Ning", "title": "Adjusted Jackknife Empirical Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jackknife empirical likelihood (JEL) is an effective modified version of\nempirical likelihood method (EL). Through the construction of the jackknife\npseudo-values, JEL overcomes the computational difficulty of EL method when its\nconstraints are nonlinear while maintaining the same asymptotic results for one\nsample and two-sample U statistics. In this paper, we propose an adjusted\nversion of JEL to guarantee that the adjusted jackknife empirical likelihood\n(AJEL) statistic is well-defined for all the values of the parameter, instead\nof restricting on the convex hull of the estimation equation. The properties of\nJEL have been preserved for AJEL.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 23:21:52 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Chen", "Ying-Ju", ""], ["Ning", "Wei", ""]]}, {"id": "1603.04122", "submitter": "Niharika Gauraha", "authors": "Niharika Gauraha", "title": "Graphical Log-linear Models: Fundamental Concepts and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive study of graphical log-linear models for\ncontingency tables. High dimensional contingency tables arise in many areas\nsuch as computational biology, collection of survey and census data and others.\nAnalysis of contingency tables involving several factors or categorical\nvariables is very hard. To determine interactions among various factors,\ngraphical and decomposable log-linear models are preferred. First, we explore\nconnections between the conditional independence in probability and graphs;\nthereafter we provide a few illustrations to describe how graphical log-linear\nmodel are useful to interpret the conditional independences between factors. We\nalso discuss the problem of estimation and model selection in decomposable\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 03:22:54 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Gauraha", "Niharika", ""]]}, {"id": "1603.04140", "submitter": "Gongjun Xu", "authors": "Gongjun Xu", "title": "Identifiability of restricted latent class models with binary responses", "comments": "36 pages, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical latent class models are widely used in social and psychological\nresearches, yet it is often difficult to establish the identifiability of the\nmodel parameters. In this paper we consider the identifiability issue of a\nfamily of restricted latent class models, where the restriction structures are\nneeded to reflect pre-specified assumptions on the related assessment. We\nestablish the identifiability results in the strict sense and specify which\ntypes of restriction structure would give the identifiability of the model\nparameters. The results not only guarantee the validity of many of the\npopularly used models, but also provide a guideline for the related\nexperimental design, where in the current applications the design is usually\nexperience based and identifiability is not guaranteed. Theoretically, we\ndevelop a new technique to establish the identifiability result, which may be\nextended to other restricted latent class models.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 05:37:13 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Xu", "Gongjun", ""]]}, {"id": "1603.04160", "submitter": "Shin-Ichi Ito", "authors": "Shin-ichi Ito, Hiromichi Nagao, Akinori Yamanaka, Yuhki Tsukada,\n  Toshiyuki Koyama, Masayuki Kano, Junya Inoue", "title": "Data assimilation for massive autonomous systems based on second-order\n  adjoint method", "comments": null, "journal-ref": "Phys. Rev. E 94, 043307 (2016)", "doi": "10.1103/PhysRevE.94.043307", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation (DA) is a fundamental computational technique that\nintegrates numerical simulation models and observation data on the basis of\nBayesian statistics. Originally developed for meteorology, especially weather\nforecasting, DA is now an accepted technique in various scientific fields. One\nkey issue that remains controversial is the implementation of DA in massive\nsimulation models under limited computation time and resources. In this paper,\nwe propose an adjoint-based DA method for massive autonomous models that\nproduces optimum estimates and their uncertainties within practical computation\ntime and resource constraints. The uncertainties are given as several diagonal\ncomponents of an inverse Hessian matrix, which is the covariance matrix of a\nnormal distribution that approximates the target posterior probability density\nfunction in the neighborhood of the optimum. Conventional algorithms for\nderiving the inverse Hessian matrix require $O(CN^2+N^3)$ computations and\n$O(N^2)$ memory, where $N$ is the number of degrees of freedom of a given\nautonomous system and $C$ is the number of computations needed to simulate time\nseries of suitable length. The proposed method using a second-order adjoint\nmethod allows us to directly evaluate the diagonal components of the inverse\nHessian matrix without computing all of its components. This drastically\nreduces the number of computations to $O(C)$ and the amount of memory to $O(N)$\nfor each diagonal component. The proposed method is validated through numerical\ntests using a massive two-dimensional Kobayashi's phase-field model. We confirm\nthat the proposed method correctly reproduces the parameter and initial state\nassumed in advance, and successfully evaluates the uncertainty of the\nparameter. Such information regarding uncertainty is valuable, as it can be\nused to optimize the design of experiments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 08:08:18 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Ito", "Shin-ichi", ""], ["Nagao", "Hiromichi", ""], ["Yamanaka", "Akinori", ""], ["Tsukada", "Yuhki", ""], ["Koyama", "Toshiyuki", ""], ["Kano", "Masayuki", ""], ["Inoue", "Junya", ""]]}, {"id": "1603.04176", "submitter": "Anower Hossain", "authors": "Anower Hossain, Karla Diaz-Ordaz and Jonathan W. Bartlett", "title": "Missing continuous outcomes under covariate dependent missingness in\n  cluster randomised trials", "comments": "25 pages and 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attrition is a common occurrence in cluster randomised trials (CRTs) which\nleads to missing outcome data. Two approaches for analysing such trials are\ncluster-level analysis and individual-level analysis. This paper compares the\nperformance of unadjusted cluster-level analysis, baseline covariate adjusted\ncluster-level analysis and linear mixed model (LMM) analysis, under baseline\ncovariate dependent missingness (CDM) in continuous outcomes, in terms of bias,\naverage estimated standard error and coverage probability. The methods of\ncomplete case analysis (CCA) and multiple imputation (MI) are used to handle\nthe missing outcome data. Four possible scenarios are considered depending on\nwhether the missingness mechanisms and covariate effects on outcome are the\nsame or different in the two intervention groups. We show that both unadjusted\ncluster-level analysis and baseline covariate adjusted cluster-level analysis\ngive unbiased estimates of the intervention effect only if both intervention\ngroups have the same missingness mechanisms and the same covariate effects,\nwhich is arguably unlikely to hold in practice. LMM and MI give unbiased\nestimates under all four considered scenarios, provided that an interaction of\nintervention indicator and covariate is included in the model when the\ncovariate effects are different in the two intervention groups. MI gives\nslightly overestimation of average standard error, which leads to a decrease in\npower.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 09:36:43 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Hossain", "Anower", ""], ["Diaz-Ordaz", "Karla", ""], ["Bartlett", "Jonathan W.", ""]]}, {"id": "1603.04196", "submitter": "Jake Carson", "authors": "Jake Carson, Murray Pollock and Mark Girolami", "title": "Unbiased local solutions of partial differential equations via the\n  Feynman-Kac Identities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Feynman-Kac formulae (FKF) express local solutions of partial\ndifferential equations (PDEs) as expectations with respect to some\ncomplementary stochastic differential equation (SDE). Repeatedly sampling paths\nfrom the complementary SDE enables the construction of Monte Carlo estimates of\nlocal solutions, which are more naturally suited to statistical inference than\nthe numerical approximations obtained via finite difference and finite element\nmethods. Until recently, simulating from the complementary SDE would have\nrequired the use of a discrete-time approximation, leading to biased estimates.\nIn this paper we utilize recent developments in two areas to demonstrate that\nit is now possible to obtain unbiased solutions for a wide range of PDE models\nvia the FKF. The first is the development of algorithms that simulate diffusion\npaths exactly (without discretization error), and so make it possible to obtain\nMonte Carlo estimates of the FKF directly. The second is the development of\ndebiasing methods for SDEs, enabling the construction of unbiased estimates\nfrom a sequence of biased estimates.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 10:41:43 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Carson", "Jake", ""], ["Pollock", "Murray", ""], ["Girolami", "Mark", ""]]}, {"id": "1603.04222", "submitter": "Jens Malmros", "authors": "Jens Malmros, Luis E.C. Rocha", "title": "Multiple seed structure and disconnected networks in respondent-driven\n  sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) is a link-tracing sampling method that is\nespecially suitable for sampling hidden populations. RDS combines an efficient\nsnowball-type sampling scheme with inferential procedures that yield unbiased\npopulation estimates under some assumptions about the sampling procedure and\npopulation structure. Several seed individuals are typically used to initiate\nRDS recruitment. However, standard RDS estimation theory assume that all\nsampled individuals originate from only one seed. We present an estimator,\nbased on a random walk with teleportation, which accounts for the multiple seed\nstructure of RDS. The new estimator can also be used on populations with\ndisconnected social networks. We numerically evaluate our estimator by\nsimulations on artificial and real networks. Our estimator outperforms previous\nestimators, especially when the proportion of seeds in the sample is large. We\nrecommend our new estimator to be used in RDS studies, in particular when the\nnumber of seeds is large or the social network of the population is\ndisconnected.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 11:51:34 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Malmros", "Jens", ""], ["Rocha", "Luis E. C.", ""]]}, {"id": "1603.04360", "submitter": "Feng Liang", "authors": "Jin Wang, Feng Liang, Yuan Ji", "title": "An Ensemble EM Algorithm for Bayesian Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Bayesian approach to variable selection in the context of linear\nregression. Motivated by a recent work by Rockova and George (2014), we propose\nan EM algorithm that returns the MAP estimate of the set of relevant variables.\nDue to its particular updating scheme, our algorithm can be implemented\nefficiently without inverting a large matrix in each iteration and therefore\ncan scale up with big data. We also show that the MAP estimate returned by our\nEM algorithm achieves variable selection consistency even when $p$ diverges\nwith $n$. In practice, our algorithm could get stuck with local modes, a common\nproblem with EM algorithms. To address this issue, we propose an ensemble EM\nalgorithm, in which we repeatedly apply the EM algorithm on a subset of the\nsamples with a subset of the covariates, and then aggregate the variable\nselection results across those bootstrap replicates. Empirical studies have\ndemonstrated the superior performance of the ensemble EM algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 17:51:05 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Wang", "Jin", ""], ["Liang", "Feng", ""], ["Ji", "Yuan", ""]]}, {"id": "1603.04500", "submitter": "Florian Heinrichs", "authors": "Chrystel Feller, Kirsten Schorning, Holger Dette, Georgina Bermann,\n  Bj\\\"orn Bornkamp", "title": "Optimal designs for dose response curves with common parameters", "comments": "Keywords and Phrases: Nonlinear regression, different treatment\n  groups, $D$-optimal design, models with common parameters, admissible design,\n  Bayesian optimal design AMS Subject Classification: Primary 62K05; Secondary\n  62F03", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in Phase II clinical trials is the comparison of dose\nresponse curves corresponding to different treatment groups. If the effect of\nthe dose level is described by parametric regression models and the treatments\ndiffer in the administration frequency (but not in the sort of drug) a\nreasonable assumption is that the regression models for the different\ntreatments share common parameters. This paper develops optimal design theory\nfor the comparison of different regression models with common parameters. We\nderive upper bounds on the number of support points of admissible designs, and\nexplicit expressions for $D$-optimal designs are derived for frequently used\ndose response models with a common location parameter. If the location and\nscale parameter in the different models coincide, minimally supported designs\nare determined and sufficient conditions for their optimality in the class of\nall designs derived. The results are illustrated in a dose-finding study\ncomparing monthly and weekly administration.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 23:13:45 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Feller", "Chrystel", ""], ["Schorning", "Kirsten", ""], ["Dette", "Holger", ""], ["Bermann", "Georgina", ""], ["Bornkamp", "Bj\u00f6rn", ""]]}, {"id": "1603.04549", "submitter": "Vijay Kamble", "authors": "Ramesh Johari, Vijay Kamble and Yash Kanoria", "title": "Matching while Learning", "comments": "This paper has been accepted for publication in Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem faced by a service platform that needs to match\nlimited supply with demand but also to learn the attributes of new users in\norder to match them better in the future. We introduce a benchmark model with\nheterogeneous \"workers\" (demand) and a limited supply of \"jobs\" that arrive\nover time. Job types are known to the platform, but worker types are unknown\nand must be learned by observing match outcomes. Workers depart after\nperforming a certain number of jobs. The expected payoff from a match depends\non the pair of types and the goal is to maximize the steady-state rate of\naccumulation of payoff. Though we use terminology inspired by labor markets,\nour framework applies more broadly to platforms where a limited supply of\nheterogeneous products is matched to users over time.\n  Our main contribution is a complete characterization of the structure of the\noptimal policy in the limit that each worker performs many jobs. The platform\nfaces a trade-off for each worker between myopically maximizing payoffs\n(exploitation) and learning the type of the worker (exploration). This creates\na multitude of multi-armed bandit problems, one for each worker, coupled\ntogether by the constraint on availability of jobs of different types (capacity\nconstraints). We find that the platform should estimate a shadow price for each\njob type, and use the payoffs adjusted by these prices, first, to determine its\nlearning goals and then, for each worker, (i) to balance learning with payoffs\nduring the \"exploration phase,\" and (ii) to myopically match after it has\nachieved its learning goals during the \"exploitation phase.\"\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 04:29:31 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 00:11:06 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 00:39:01 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 21:36:16 GMT"}, {"version": "v5", "created": "Sat, 7 Dec 2019 18:16:30 GMT"}, {"version": "v6", "created": "Thu, 23 Apr 2020 19:49:49 GMT"}, {"version": "v7", "created": "Wed, 5 Aug 2020 22:17:03 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Johari", "Ramesh", ""], ["Kamble", "Vijay", ""], ["Kanoria", "Yash", ""]]}, {"id": "1603.04565", "submitter": "Yuthika Gardiyawasam Punchihewa", "authors": "Yuthika Punchihewa, Ba-Ngu Vo, Ba-Tuong Vo", "title": "A Generalized Labeled Multi-Bernoulli Filter for Maneuvering Targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multiple maneuvering target system can be viewed as a Jump Markov System\n(JMS) in the sense that the target movement can be modeled using different\nmotion models where the transition between the motion models by a particular\ntarget follows a Markov chain probability rule. This paper describes a\nGeneralized Labelled Multi-Bernoulli (GLMB) filter for tracking maneuvering\ntargets whose movement can be modeled via such a JMS. The proposed filter is\nvalidated with two linear and nonlinear maneuvering target tracking examples.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 06:15:34 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Punchihewa", "Yuthika", ""], ["Vo", "Ba-Ngu", ""], ["Vo", "Ba-Tuong", ""]]}, {"id": "1603.04803", "submitter": "Panagiotis Tsilifis", "authors": "Panagiotis Tsilifis and Roger Ghanem", "title": "Reduced Wiener Chaos representation of random fields via basis\n  adaptation and projection", "comments": "Submitted to the Journal of Computational Physics", "journal-ref": null, "doi": "10.1016/j.jcp.2017.04.009", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new characterization of random fields appearing in physical models is\npresented that is based on their well-known Homogeneous Chaos expansions. We\ntake advantage of the adaptation capabilities of these expansions where the\ncore idea is to rotate the basis of the underlying Gaussian Hilbert space, in\norder to achieve reduced functional representations that concentrate the\ninduced probability measure in a lower dimensional subspace. For a smooth\nfamily of rotations along the domain of interest, the uncorrelated Gaussian\ninputs are transformed into a Gaussian process, thus introducing a mesoscale\nthat captures intermediate characteristics of the quantity of interest.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 18:27:15 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 00:31:59 GMT"}, {"version": "v3", "created": "Tue, 22 Mar 2016 02:17:53 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Tsilifis", "Panagiotis", ""], ["Ghanem", "Roger", ""]]}, {"id": "1603.04929", "submitter": "Konstantin Zuev M", "authors": "Konstantin Zuev", "title": "Statistical Inference", "comments": "ACM Lecture Notes, 145 pages, 79 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.HO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is Statistics? Opinions vary. In fact, there is a continuous spectrum of\nattitudes toward statistics ranging from pure theoreticians, proving asymptotic\nefficiency and searching for most powerful tests, to wild practitioners,\nblindly reporting p-values and claiming statistical significance for\nscientifically insignificant results. In these notes statistics is viewed as a\nbranch of mathematical engineering, that studies ways of extracting reliable\ninformation from limited data for learning, prediction, and decision making in\nthe presence of uncertainty. These ACM lecture notes are based on the courses\nthe author taught at the University of Southern California in 2012 and 2013,\nand at the California Institute of Technology in 2016.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 01:02:07 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Zuev", "Konstantin", ""]]}, {"id": "1603.05031", "submitter": "Azzimonti Dario", "authors": "Dario Azzimonti (Idiap, IMSV), David Ginsbourger (Idiap, IMSV)", "title": "Estimating orthant probabilities of high dimensional Gaussian vectors\n  with an application to set estimation", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, Taylor \\&\n  Francis, 2018, 27 (2), pp.255-267", "doi": "10.1080/10618600.2017.1360781", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of Gaussian orthant probabilities has been extensively\nstudied for low-dimensional vectors. Here, we focus on the high-dimensional\ncase and we present a two-step procedure relying on both deterministic and\nstochastic techniques. The proposed estimator relies indeed on splitting the\nprobability into a low-dimensional term and a remainder. While the\nlow-dimensional probability can be estimated by fast and accurate quadrature,\nthe remainder requires Monte Carlo sampling. We further refine the estimation\nby using a novel asymmetric nested Monte Carlo (anMC) algorithm for the\nremainder and we highlight cases where this approximation brings substantial\nefficiency gains. The proposed methods are compared against state-of-the-art\ntechniques in a numerical study, which also calls attention to the advantages\nand drawbacks of the procedure. Finally, the proposed method is applied to\nderive conservative estimates of excursion sets of expensive to evaluate\ndeterministic functions under a Gaussian random field prior, without requiring\na Markov assumption. Supplementary material for this article is available\nonline.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 11:01:56 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 09:52:34 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2018 12:50:30 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Azzimonti", "Dario", "", "Idiap, IMSV"], ["Ginsbourger", "David", "", "Idiap, IMSV"]]}, {"id": "1603.05068", "submitter": "Caren Hasler", "authors": "Caren Hasler and Radu V. Craiu", "title": "Nonparametric imputation method for nonresponse in surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many imputation methods are based on statistical models that assume that the\nvariable of interest is a noisy observation of a function of the auxiliary\nvariables or covariates. Misspecification of this model may lead to severe\nerrors in estimates and to misleading conclusions. A new imputation method for\nitem nonresponse in surveys is proposed based on a nonparametric estimation of\nthe functional dependence between the variable of interest and the auxiliary\nvariables. We consider the use of smoothing spline estimation within an\nadditive model framework to flexibly build an imputation model in the case of\nmultiple auxiliary variables. The performance of our method is assessed via\nnumerical experiments involving simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 12:48:26 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 15:00:40 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Hasler", "Caren", ""], ["Craiu", "Radu V.", ""]]}, {"id": "1603.05224", "submitter": "Zijian Guo", "authors": "Zijian Guo, Hyunseung Kang, T. Tony Cai, Dylan S. Small", "title": "Confidence Intervals for Causal Effects with Invalid Instruments using\n  Two-Stage Hard Thresholding with Voting", "comments": "The title is revised to highlight the two important parts of the\n  proposed method, Two-Stage Hard Thresholding and Voting", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in instrumental variables (IV) analysis is to find\ninstruments that are valid, or have no direct effect on the outcome and are\nignorable. Typically one is unsure whether all of the putative IVs are in fact\nvalid. We propose a general inference procedure in the presence of invalid IVs,\ncalled Two-Stage Hard Thresholding (TSHT) with voting. TSHT uses two hard\nthresholding steps to select strong instruments and generate candidate sets of\nvalid IVs. Voting takes the candidate sets and uses majority and plurality\nrules to determine the true set of valid IVs. In low dimensions, if the\nsufficient and necessary identification condition under invalid instruments is\nmet, which is more general than the so-called 50% rule or the majority rule,\nour proposal (i) correctly selects valid IVs, (ii) consistently estimates the\ncausal effect, (iii) produces valid confidence intervals for the causal effect,\nand (iv) has oracle-optimal width. In high dimensions, we establish nearly\nidentical results without oracle-optimality. In simulations, our proposal\noutperforms traditional and recent methods in the invalid IV literature. We\nalso apply our method to re-analyze the causal effect of education on earnings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 19:21:45 GMT"}, {"version": "v2", "created": "Sun, 14 Aug 2016 23:57:30 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 20:18:19 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Guo", "Zijian", ""], ["Kang", "Hyunseung", ""], ["Cai", "T. Tony", ""], ["Small", "Dylan S.", ""]]}, {"id": "1603.05324", "submitter": "Shiwen Zhao", "authors": "Shiwen Zhao and Barbara E. Engelhardt and Sayan Mukherjee and David B.\n  Dunson", "title": "Fast moment estimation for generalized latent Dirichlet models", "comments": "corrected a typo in figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a generalized method of moments (GMM) approach for fast parameter\nestimation in a new class of Dirichlet latent variable models with mixed data\ntypes. Parameter estimation via GMM has been demonstrated to have computational\nand statistical advantages over alternative methods, such as expectation\nmaximization, variational inference, and Markov chain Monte Carlo. The key\ncomputational advan- tage of our method (MELD) is that parameter estimation\ndoes not require instantiation of the latent variables. Moreover, a\nrepresentational advantage of the GMM approach is that the behavior of the\nmodel is agnostic to distributional assumptions of the observations. We derive\npopulation moment conditions after marginalizing out the sample-specific\nDirichlet latent variables. The moment conditions only depend on component mean\nparameters. We illustrate the utility of our approach on simulated data,\ncomparing results from MELD to alternative methods, and we show the promise of\nour approach through the application of MELD to several data sets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 00:36:39 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 18:12:35 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Zhao", "Shiwen", ""], ["Engelhardt", "Barbara E.", ""], ["Mukherjee", "Sayan", ""], ["Dunson", "David B.", ""]]}, {"id": "1603.05334", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban", "title": "Weighted mining of massive collections of $p$-values by convex\n  optimization", "comments": "This is an entirely rewritten version of the paper. The title of the\n  paper, the name of the method, and the introduction have been changed, with\n  the goal of making the paper more accessible and appealing to practitioners.\n  New sections on monotone likelihood ratio families and two-sided tests have\n  been added, which expand the scope of the method", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in data-rich disciplines---think of computational genomics and\nobservational cosmology---often wish to mine large bodies of $p$-values looking\nfor significant effects, while controlling the false discovery rate or\nfamily-wise error rate. Increasingly, researchers also wish to prioritize\ncertain hypotheses, for example those thought to have larger effect sizes, by\nupweighting, and to impose constraints on the underlying mining, such as\nmonotonicity along a certain sequence.\n  We introduce Princessp, a principled method for performing weighted multiple\ntesting by constrained convex optimization. Our method elegantly allows one to\nprioritize certain hypotheses through upweighting and to discount others\nthrough downweighting, while constraining the underlying weights involved in\nthe mining process. When the $p$-values derive from monotone likelihood ratio\nfamilies like the Gaussian means model, the new method allows exact solution of\nan important optimal weighting problem previously thought to be nonconvex and\ncomputationally infeasible. Our method scales to massive dataset sizes.\n  We illustrate the applications of Princessp on a series of standard genomics\ndatasets and offer comparisons with several previous `standard' methods.\nPrincessp offers both ease of operation and the ability to scale to extremely\nlarge problem sizes. The method is available as open-source software from\nhttp://github.com/dobriban/pvalue_weighting_matlab .\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 02:06:02 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 20:27:56 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Dobriban", "Edgar", ""]]}, {"id": "1603.05343", "submitter": "Roberto D. Pascual-Marqui", "authors": "RD Pascual-Marqui, P Faber, T Kinoshita, Y Kitaura, K Kochi, P Milz, K\n  Nishida, M Yoshimura", "title": "The dual frequency RV-coupling coefficient: a novel measure for\n  quantifying cross-frequency information transactions in the brain", "comments": "technical report, pre-print, 2016-03-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Identifying dynamic transactions between brain regions has become\nincreasingly important. Measurements within and across brain structures,\ndemonstrating the occurrence of bursts of beta/gamma oscillations only during\none specific phase of each theta/alpha cycle, have motivated the need to\nadvance beyond linear and stationary time series models. Here we offer a novel\nmeasure, namely, the \"dual frequency RV-coupling coefficient\", for assessing\ndifferent types of frequency-frequency interactions that subserve information\nflow in the brain. This is a measure of coherence between two complex-valued\nvectors, consisting of the set of Fourier coefficients for two different\nfrequency bands, within or across two brain regions. RV-coupling is expressed\nin terms of instantaneous and lagged components. Furthermore, by using\nnormalized Fourier coefficients (unit modulus), phase-type couplings can also\nbe measured. The dual frequency RV-coupling coefficient is based on previous\nwork: the second order bispectrum, i.e. the dual-frequency coherence (Thomson\n1982; Haykin & Thomson 1998); the RV-coefficient (Escoufier 1973); Gorrostieta\net al (2012); and Pascual-Marqui et al (2011). This paper presents the new\nmeasure, and outlines relevant statistical tests. The novel aspects of the\n\"dual frequency RV-coupling coefficient\" are: (1) it can be applied to two\nmultivariate time series; (2) the method is not limited to single discrete\nfrequencies, and in addition, the frequency bands are treated by means of\nappropriate multivariate statistical methodology; (3) the method makes use of a\nnovel generalization of the RV-coefficient for complex-valued multivariate\ndata; (4) real and imaginary covariance contributions to the RV-coherence are\nobtained, allowing the definition of a \"lagged-coupling\" measure that is\nminimally affected by the low spatial resolution of estimated cortical electric\nneuronal activity.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 03:02:50 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 00:52:48 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Pascual-Marqui", "RD", ""], ["Faber", "P", ""], ["Kinoshita", "T", ""], ["Kitaura", "Y", ""], ["Kochi", "K", ""], ["Milz", "P", ""], ["Nishida", "K", ""], ["Yoshimura", "M", ""]]}, {"id": "1603.05587", "submitter": "Mohammad Ghasemi Hamed", "authors": "Mohammad Ghasemi Hamed and Masoud Ebadi Kivaj", "title": "Reliable Prediction Intervals for Local Linear Regression", "comments": "40 pages,11 figures, 10 tables and 1 algorithm. arXiv admin note:\n  text overlap with arXiv:1402.5874", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces two methods for estimating reliable prediction\nintervals for local linear least-squares regressions, named Bounded Oscillation\nPrediction Intervals (BOPI). It also proposes a new measure for comparing\ninterval prediction models named Equivalent Gaussian Standard Deviation (EGSD).\nThe experimental results compare BOPI to other methods using coverage\nprobability, Mean Interval Size and the introduced EGSD measure. The results\nwere generally in favor of the BOPI on considered benchmark regression\ndatasets. It also, reports simulation studies validating the BOPI method's\nreliability.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 17:39:12 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 17:54:37 GMT"}, {"version": "v3", "created": "Wed, 30 Mar 2016 21:52:48 GMT"}, {"version": "v4", "created": "Fri, 1 Apr 2016 10:23:38 GMT"}, {"version": "v5", "created": "Tue, 12 Jul 2016 17:39:50 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Hamed", "Mohammad Ghasemi", ""], ["Kivaj", "Masoud Ebadi", ""]]}, {"id": "1603.05694", "submitter": "Diaa Al Mohamad", "authors": "Diaa Al Mohamad and Assia Boumahdaf", "title": "Semiparametric two-component mixture models under linear constraints", "comments": "This Paper was combined with my other paper on semiparametric mixture\n  models under L-moments constraints and was accepted for publication in IEEE\n  Transactions on Information Theory, see published paper for final corrections", "journal-ref": null, "doi": "10.1109/TIT.2017.2786345", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a structure of a semiparametric two-component mixture model when\none component is parametric and the other is defined through linear constraints\non its distribution function. Estimation of a two-component mixture model with\nan unknown component is very difficult when no particular assumption is made on\nthe structure of the unknown component. A symmetry assumption was used in the\nliterature to simplify the estimation. Such method has the advantage of\nproducing consistent and asymptotically normal estimators, and identifiability\nof the semiparametric mixture model becomes tractable. Still, existing methods\nwhich estimate a semiparametric mixture model have their limits when the\nparametric component has unknown parameters or the proportion of the parametric\npart is either very high or very low. We propose in this paper a method to\nincorporate a prior linear information about the distribution of the unknown\ncomponent in order to better estimate the model when existing estimation\nmethods fail. The new method is based on $\\varphi-$divergences and has an\noriginal form since the minimization is carried over both arguments of the\ndivergence. The resulting estimators are proved to be consistent and\nasymptotically normal under standard assumptions. We show that using the\nPearson's $\\chi^2$ divergence our algorithm has a linear complexity when the\nconstraints are moment-type. Simulations on univariate and multivariate\nmixtures demonstrate the viability and the interest of our novel approach.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 21:19:01 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 14:01:02 GMT"}, {"version": "v3", "created": "Sun, 15 Jan 2017 20:12:42 GMT"}, {"version": "v4", "created": "Thu, 21 Dec 2017 07:46:40 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Mohamad", "Diaa Al", ""], ["Boumahdaf", "Assia", ""]]}, {"id": "1603.05758", "submitter": "Cai Li", "authors": "Luo Xiao, Cai Li, William Checkley and Ciprian M. Crainiceanu", "title": "Fast Covariance Estimation for Sparse Functional Data", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothing of noisy sample covariances is an important component in functional\ndata analysis. We propose a novel covariance smoothing method based on\npenalized splines and associated software. The proposed method is a bivariate\nspline smoother that is designed for covariance smoothing and can be used for\nsparse functional or longitudinal data. We propose a fast algorithm for\ncovariance smoothing using leave-one-subject-out cross validation. Our\nsimulations show that the proposed method compares favorably against several\ncommonly used methods. The method is applied to a study of child growth led by\none of coauthors and to a public dataset of longitudinal CD4 counts.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 03:37:57 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 00:49:38 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Xiao", "Luo", ""], ["Li", "Cai", ""], ["Checkley", "William", ""], ["Crainiceanu", "Ciprian M.", ""]]}, {"id": "1603.05938", "submitter": "Mette Langaas", "authors": "K. K. Halle and {\\O}. Bakke and S. Djurovic and A. Bye and E. Ryeng\n  and U. Wisl{\\o}ff and O. A. Andreassen and M. Langaas", "title": "Efficient and powerful familywise error control in genome-wide\n  association studies using generalized linear models", "comments": null, "journal-ref": null, "doi": "10.1111/sjos.12451", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genetic association studies, detecting phenotype-genotype association is a\nprimary goal. We assume that the relationship between the data -phenotype,\ngenetic markers and environmental covariates - can be modelled by a generalized\nlinear model (GLM). The inclusion of environmental covariates makes it possible\nto account for important confounding factors, such as sex and population\nsubstructure. A multivariate score statistic, which under the complete null\nhypothesis of no phenotype-genotype association asymptotically has a\nmultivariate normal distribution with a covariance matrix that can be estimated\nfrom the data, is used to test a large number of genetic markers for\nassociation with the phenotype. We stress the importance of controlling the\nfamilywise error rate (FWER), and use the asymptotic distribution of the\nmultivariate score test statistic to find a local significance level for the\nindividual test. Using real data (from one study on schizophrenia and bipolar\ndisorder and one on maximal oxygen uptake) and constructed correlated\nstructures, we show that our method is a powerful alternative to the popular\nBonferroni and Sidak methods. For GLMs without environmental covariates, we\nshow that our method is an efficient alternative to permutation methods for\nmultiple testing. Further, we show that if environmental covariates and genetic\nmarkers are uncorrelated, the estimated covariance matrix of the score test\nstatistic can be approximated by the estimated correlation matrix for just the\ngenetic markers. As byproducts of our method, an effective number of\nindependent tests can be defined, and FWER-adjusted $p$-values can be\ncalculated as an alternative to using a local significance level.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 17:51:37 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 12:09:23 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Halle", "K. K.", ""], ["Bakke", "\u00d8.", ""], ["Djurovic", "S.", ""], ["Bye", "A.", ""], ["Ryeng", "E.", ""], ["Wisl\u00f8ff", "U.", ""], ["Andreassen", "O. A.", ""], ["Langaas", "M.", ""]]}, {"id": "1603.06045", "submitter": "Edoardo Airoldi", "authors": "Alexander M Franks, Edoardo M Airoldi, Donald B Rubin", "title": "Non-standard conditionally specified models for non-ignorable missing\n  data", "comments": "37 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analyses typically rely upon assumptions about missingness mechanisms\nthat lead to observed versus missing data. When the data are missing not at\nrandom, direct assumptions about the missingness mechanism, and indirect\nassumptions about the distributions of observed and missing data, are typically\nuntestable. We explore an approach, where the joint distribution of observed\ndata and missing data is specified through non-standard conditional\ndistributions. In this formulation, which traces back to a factorization of the\njoint distribution, apparently proposed by J.W. Tukey, the modeling assumptions\nabout the conditional factors are either testable or are designed to allow the\nincorporation of substantive knowledge about the problem at hand, thereby\noffering a possibly realistic portrayal of the data, both missing and observed.\nWe apply Tukey's conditional representation to exponential family models, and\nwe propose a computationally tractable inferential strategy for this class of\nmodels. We illustrate the utility of this approach using high-throughput\nbiological data with missing data that are not missing at random.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 04:20:28 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Franks", "Alexander M", ""], ["Airoldi", "Edoardo M", ""], ["Rubin", "Donald B", ""]]}, {"id": "1603.06138", "submitter": "Jian Kang", "authors": "Jichun Xie and Jian Kang", "title": "High Dimensional Tests for Functional Networks of Brain Anatomic Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been increasing interests in learning resting-state brain\nfunctional connectivity of autism disorders using functional magnetic resonance\nimaging (fMRI) data. The data in a standard brain template consist of over\n200,000 voxel specific time series for each single subject. Such an ultra-high\ndimensionality of data makes the voxel-level functional connectivity analysis\n(involving four billion voxel pairs) lack of power and extremely inefficient.\nIn this work, we introduce a new framework to identify functional brain network\nat brain anatomic region-level for each individual. We propose two pairwise\ntests to detect region dependence, and one multiple testing procedure to\nidentify global structures of the network. The limiting null distributions of\nthe test statistics are derived. It is also shown that the tests are rate\noptimal when the alternative networks are sparse. The numerical studies show\nthe proposed tests are valid and powerful. We apply our method to a\nresting-state fMRI study on autism and identify patient-unique and\ncontrol-unique hub regions. These findings are consistent with autism clinical\nsymptoms.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 20:20:02 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Xie", "Jichun", ""], ["Kang", "Jian", ""]]}, {"id": "1603.06145", "submitter": "Jian Kang", "authors": "Hyokyoung Grace Hong, Jian Kang and Yi Li", "title": "Conditional Screening for Ultra-high Dimensional Covariates with\n  Survival Outcomes", "comments": "34 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying important biomarkers that are predictive for cancer patients'\nprognosis is key in gaining better insights into the biological influences on\nthe disease and has become a critical component of precision medicine. The\nemergence of large-scale biomedical survival studies, which typically involve\nexcessive number of biomarkers, has brought high demand in designing efficient\nscreening tools for selecting predictive biomarkers. The vast amount of\nbiomarkers defies any existing variable selection methods via regularization.\nThe recently developed variable screening methods, though powerful in many\npractical setting, fail to incorporate prior information on the importance of\neach biomarker and are less powerful in detecting marginally weak while jointly\nimportant signals. We propose a new conditional screening method for survival\noutcome data by computing the marginal contribution of each biomarker given\npriorly known biological information. This is based on the premise that some\nbiomarkers are known to be associated with disease outcomes a priori. Our\nmethod possesses sure screening properties and a vanishing false selection\nrate. The utility of the proposal is further confirmed with extensive\nsimulation studies and analysis of a Diffuse large B-cell lymphoma (DLBCL)\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 21:10:54 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Hong", "Hyokyoung Grace", ""], ["Kang", "Jian", ""], ["Li", "Yi", ""]]}, {"id": "1603.06284", "submitter": "Jonathan Bartlett", "authors": "Jonathan W. Bartlett and Ruth H. Keogh", "title": "Bayesian correction for covariate measurement error: a frequentist\n  evaluation and comparison with regression calibration", "comments": null, "journal-ref": null, "doi": "10.1177/0962280216667764", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian approaches for handling covariate measurement error are well\nestablished, and yet arguably are still relatively little used by researchers.\nFor some this is likely due to unfamiliarity or disagreement with the Bayesian\ninferential paradigm. For others a contributory factor is the inability of\nstandard statistical packages to perform such Bayesian analyses. In this paper\nwe first give an overview of the Bayesian approach to handling covariate\nmeasurement error, and contrast it with regression calibration (RC), arguably\nthe most commonly adopted approach. We then argue why the Bayesian approach has\na number of statistical advantages compared to RC, and demonstrate that\nimplementing the Bayesian approach is usually quite feasible for the analyst.\nNext we describe the closely related maximum likelihood and multiple imputation\napproaches, and explain why we believe the Bayesian approach to generally be\npreferable. We then empirically compare the frequentist properties of RC and\nthe Bayesian approach through simulation studies. The flexibility of the\nBayesian approach to handle both measurement error and missing data is then\nillustrated through an analysis of data from the Third National Health and\nNutrition Examination Survey.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 22:48:09 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Bartlett", "Jonathan W.", ""], ["Keogh", "Ruth H.", ""]]}, {"id": "1603.06349", "submitter": "Wei Yi", "authors": "Meng Jiang, Wei Yi, Reza Hoseinnezhad and Lingjiang Kong", "title": "Distributed Multi-Sensor Fusion Using Generalized Multi-Bernoulli\n  Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses distributed multi-target tracking in the framework of\ngeneralized Covariance Intersection (GCI) over multistatic radar system. The\nproposed method is based on the unlabeled version of generalized labeled\nmulti-Bernoulli (GLMB) family by discarding the labels, referred as generalized\nmulti-Bernoulli (GMB) family. However, it doesn't permit closed form solution\nfor GCI fusion with GMB family. To solve this challenging problem, firstly, we\npropose an efficient approximation to the GMB family which preserves both the\nprobability hypothesis density (PHD) and cardinality distribution, named as\nsecond-order approximation of GMB (SO-GMB) density. Then, we derive explicit\nexpression for the GCI fusion with SO-GMB density. Finally, we compare the\nfirst-order approximation of GMB (FO-GMB) density with SO-GMB density in two\nscenarios and make a concrete analysis of the advantages of the second-order\napproximation. Simulation results are presented to verify the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 08:14:48 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Jiang", "Meng", ""], ["Yi", "Wei", ""], ["Hoseinnezhad", "Reza", ""], ["Kong", "Lingjiang", ""]]}, {"id": "1603.06358", "submitter": "Linda S. L. Tan", "authors": "Linda S. L. Tan, Ajay Jasra, Maria De Iorio and Timothy M. D. Ebbels", "title": "Bayesian inference for multiple Gaussian graphical models with\n  application to metabolic association networks", "comments": null, "journal-ref": "Ann. Appl. Stat. 11 (2017) 2222-2251", "doi": "10.1214/17-AOAS1076", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the effect of cadmium (a toxic environmental pollutant) on the\ncorrelation structure of a number of urinary metabolites using Gaussian\ngraphical models (GGMs). The inferred metabolic associations can provide\nimportant information on the physiological state of a metabolic system and\ninsights on complex metabolic relationships. Using the fitted GGMs, we\nconstruct differential networks, which highlight significant changes in\nmetabolite interactions under different experimental conditions. The analysis\nof such metabolic association networks can reveal differences in the underlying\nbiological reactions caused by cadmium exposure. We consider Bayesian inference\nand propose using the multiplicative (or Chung-Lu random graph) model as a\nprior on the graphical space. In the multiplicative model, each edge is chosen\nindependently with probability equal to the product of the connectivities of\nthe end nodes. This class of prior is parsimonious yet highly flexible; it can\nbe used to encourage sparsity or graphs with a pre-specified degree\ndistribution when such prior knowledge is available. We extend the\nmultiplicative model to multiple GGMs linking the probability of edge inclusion\nthrough logistic regression and demonstrate how this leads to joint inference\nfor multiple GGMs. A sequential Monte Carlo (SMC) algorithm is developed for\nestimating the posterior distribution of the graphs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 09:08:02 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 04:47:39 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Tan", "Linda S. L.", ""], ["Jasra", "Ajay", ""], ["De Iorio", "Maria", ""], ["Ebbels", "Timothy M. D.", ""]]}, {"id": "1603.06372", "submitter": "Alexander Luedtke", "authors": "Alexander R. Luedtke, Mark J. van der Laan", "title": "Evaluating the Impact of Treating the Optimal Subgroup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we have a binary treatment used to influence an outcome. Given data\nfrom an observational or controlled study, we wish to determine whether or not\nthere exists some subset of observed covariates in which the treatment is more\neffective than the standard practice of no treatment. Furthermore, we wish to\nquantify the improvement in population mean outcome that will be seen if this\nsubgroup receives treatment and the rest of the population remains untreated.\nWe show that this problem is surprisingly challenging given how often it is an\n(at least implicit) study objective. Blindly applying standard techniques fails\nto yield any apparent asymptotic results, while using existing techniques to\nconfront the non-regularity does not necessarily help at distributions where\nthere is no treatment effect. Here we describe an approach to estimate the\nimpact of treating the subgroup which benefits from treatment that is valid in\na nonparametric model and is able to deal with the case where there is no\ntreatment effect. The approach is a slight modification of an approach that\nrecently appeared in the individualized medicine literature.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 10:08:20 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Luedtke", "Alexander R.", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1603.06400", "submitter": "Ikenna Odinaka", "authors": "Ikenna Odinaka, Joseph A. O'Sullivan, David G. Politte, Kenneth P.\n  MacCabe, Yan Kaganovsky, Joel A. Greenberg, Manu Lakshmanan, Kalyani\n  Krishnamurthy, Anuj Kapadia, Lawrence Carin, and David J. Brady", "title": "Joint System and Algorithm Design for Computationally Efficient Fan Beam\n  Coded Aperture X-ray Coherent Scatter Imaging", "comments": "This paper has been submitted to IEEE Transactions on Computational\n  Imaging for consideration. 18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In x-ray coherent scatter tomography, tomographic measurements of the forward\nscatter distribution are used to infer scatter densities within a volume. A\nradiopaque 2D pattern placed between the object and the detector array enables\nthe disambiguation between different scatter events. The use of a fan beam\nsource illumination to speed up data acquisition relative to a pencil beam\npresents computational challenges. To facilitate the use of iterative\nalgorithms based on a penalized Poisson log-likelihood function, efficient\ncomputational implementation of the forward and backward models are needed. Our\nproposed implementation exploits physical symmetries and structural properties\nof the system and suggests a joint system-algorithm design, where the system\ndesign choices are influenced by computational considerations, and in turn lead\nto reduced reconstruction time. Computational-time speedups of approximately\n146 and 32 are achieved in the computation of the forward and backward models,\nrespectively. Results validating the forward model and reconstruction algorithm\nare presented on simulated analytic and Monte Carlo data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 16:35:57 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Odinaka", "Ikenna", ""], ["O'Sullivan", "Joseph A.", ""], ["Politte", "David G.", ""], ["MacCabe", "Kenneth P.", ""], ["Kaganovsky", "Yan", ""], ["Greenberg", "Joel A.", ""], ["Lakshmanan", "Manu", ""], ["Krishnamurthy", "Kalyani", ""], ["Kapadia", "Anuj", ""], ["Carin", "Lawrence", ""], ["Brady", "David J.", ""]]}, {"id": "1603.06408", "submitter": "Catia  Scricciolo", "authors": "Catia Scricciolo", "title": "Sharp sup-norm Bayesian curve estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sup-norm curve estimation is a fundamental statistical problem and, in\nprinciple, a premise for the construction of confidence bands for\ninfinite-dimensional parameters. In a Bayesian framework, the issue of whether\nthe sup-norm-concentration- of-posterior-measure approach proposed by Gin\\'e\nand Nickl (2011), which involves solving a testing problem exploiting\nconcentration properties of kernel and projection-type density estimators\naround their expectations, can yield minimax-optimal rates is herein settled in\nthe affirmative beyond conjugate-prior settings obtaining sharp rates for\ncommon prior-model pairs like random histograms, Dirichlet Gaussian or Laplace\nmixtures, which can be employed for density, regression or quantile estimation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 12:30:02 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Scricciolo", "Catia", ""]]}, {"id": "1603.06415", "submitter": "Guy Nason Prof.", "authors": "Guy Nason", "title": "Simulation Study Comparing Two Tests of Second-order Stationarity and\n  Confidence Intervals for Localized Autocovariance", "comments": "University of Bristol, School of Mathematics, Statistics Group,\n  Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report compares two tests of second-order stationarity through\nsimulation. It also provides several examples of localised autocovariances and\ntheir approximate confidence intervals on different real and simulated data\nsets. An empirical verification of an asymptotic Gaussianity result is also\ndemonstrated. The commands use to produce figures in a companion paper are also\ndescribed.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 12:52:52 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Nason", "Guy", ""]]}, {"id": "1603.06619", "submitter": "Holger Rootz\\'en", "authors": "Holger Rootz\\'en, Johan Segers, Jennifer L. Wadsworth", "title": "Multivariate peaks over thresholds models", "comments": "25 pages, 3 figure", "journal-ref": null, "doi": "10.1007/s10687-017-0294-4", "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate peaks over thresholds modeling based on generalized Pareto\ndistributions has up to now only been used in few and mostly 2-dimensional\nsituations. This paper contributes theoretical understanding, physically based\nmodels, inference tools, and simulation methods to support routine use, with an\naim at higher dimensions. We derive a general point process model for extreme\nepisodes in data, and show how conditioning the distribution of extreme\nepisodes on threshold exceedance gives four basic representations of the family\nof generalized Pareto distributions. The first representation is constructed on\nthe real scale of the observations. The second one starts with a model on a\nstandard exponential scale which then is transformed to the real scale. The\nthird and fourth are reformulations of a spectral representation proposed in A.\nFerreira and L. de Haan [Bernoulli 20 (2014) 1717--1737]. Numerically tractable\nforms of densities and censored densities are found and give tools for flexible\nparametric likelihood inference. New simulation algorithms, explicit formulas\nfor probabilities and conditional probabilities, and conditions which make the\nconditional distribution of weighted component sums generalized Pareto are\nderived.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 21:19:46 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 08:14:00 GMT"}, {"version": "v3", "created": "Wed, 3 May 2017 12:55:01 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Rootz\u00e9n", "Holger", ""], ["Segers", "Johan", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "1603.06663", "submitter": "Jinyuan Chang", "authors": "Jinyuan Chang, Yumou Qiu, Qiwei Yao, Tao Zou", "title": "Confidence regions for entries of a large precision matrix", "comments": "The original title of this paper is \"On the statistical inference for\n  large precision matrices with dependent data\"", "journal-ref": "Journal of Econometrics 2018, Vol. 206, No. 1, 57-82", "doi": "10.1016/j.jeconom.2018.03.020", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision matrices play important roles in many practical applications.\nMotivated by temporally dependent multivariate data in modern social and\nscientific studies, we consider the statistical inference of precision matrices\nfor high-dimensional time dependent observations. Specifically, we propose a\ndata-driven procedure to construct a class of simultaneous confidence regions\nfor the precision coefficients within an index set of interest. The confidence\nregions can be applied to test for specific structures of a precision matrix\nand to recover its nonzero components. We first construct an estimator of the\nunderlying precision matrix via penalized node-wise regressions, and then\ndevelope the Gaussian approximation results on the maximal difference between\nthe estimated and true precision matrices. A computationally feasible\nparametric bootstrap algorithm is developed to implement the proposed\nprocedure. Theoretical results indicate that the proposed procedure works well\nwithout the second order cross-time stationary assumption on the data and\nsparse structure conditions on the long-run covariance of the estimates.\nSimulation studies and a real example on S&P 500 stock return data confirm the\nperformance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 02:45:06 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 14:21:56 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 17:09:14 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Chang", "Jinyuan", ""], ["Qiu", "Yumou", ""], ["Yao", "Qiwei", ""], ["Zou", "Tao", ""]]}, {"id": "1603.06743", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Koh Takeuchi, Tomoharu Iwata, John Shawe-Taylor, Samuel\n  Kaski", "title": "Localized Lasso for High-Dimensional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the localized Lasso, which is suited for learning models that\nare both interpretable and have a high predictive power in problems with high\ndimensionality $d$ and small sample size $n$. More specifically, we consider a\nfunction defined by local sparse models, one at each data point. We introduce\nsample-wise network regularization to borrow strength across the models, and\nsample-wise exclusive group sparsity (a.k.a., $\\ell_{1,2}$ norm) to introduce\ndiversity into the choice of feature sets in the local models. The local models\nare interpretable in terms of similarity of their sparsity patterns. The cost\nfunction is convex, and thus has a globally optimal solution. Moreover, we\npropose a simple yet efficient iterative least-squares based optimization\nprocedure for the localized Lasso, which does not need a tuning parameter, and\nis guaranteed to converge to a globally optimal solution. The solution is\nempirically shown to outperform alternatives for both simulated and genomic\npersonalized medicine data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 11:41:28 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 13:43:21 GMT"}, {"version": "v3", "created": "Fri, 14 Oct 2016 02:15:06 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Yamada", "Makoto", ""], ["Takeuchi", "Koh", ""], ["Iwata", "Tomoharu", ""], ["Shawe-Taylor", "John", ""], ["Kaski", "Samuel", ""]]}, {"id": "1603.06898", "submitter": "Diana Cai", "authors": "Tamara Broderick and Diana Cai", "title": "Edge-exchangeable graphs and sparsity", "comments": "This paper appeared in the NIPS 2015 Workshop on Networks in the\n  Social and Information Sciences,\n  http://stanford.edu/~jugander/NetworksNIPS2015/. An earlier version appeared\n  in the NIPS 2015 Workshop Bayesian Nonparametrics: The Next Generation,\n  https://sites.google.com/site/nipsbnp2015/", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A known failing of many popular random graph models is that the Aldous-Hoover\nTheorem guarantees these graphs are dense with probability one; that is, the\nnumber of edges grows quadratically with the number of nodes. This behavior is\nconsidered unrealistic in observed graphs. We define a notion of edge\nexchangeability for random graphs in contrast to the established notion of\ninfinite exchangeability for random graphs --- which has traditionally relied\non exchangeability of nodes (rather than edges) in a graph. We show that,\nunlike node exchangeability, edge exchangeability encompasses models that are\nknown to provide a projective sequence of random graphs that circumvent the\nAldous-Hoover Theorem and exhibit sparsity, i.e., sub-quadratic growth of the\nnumber of edges with the number of nodes. We show how edge-exchangeability of\ngraphs relates naturally to existing notions of exchangeability from clustering\n(a.k.a. partitions) and other familiar combinatorial structures.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 18:28:09 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Broderick", "Tamara", ""], ["Cai", "Diana", ""]]}, {"id": "1603.06915", "submitter": "Diana Cai", "authors": "Diana Cai and Tamara Broderick", "title": "Completely random measures for modeling power laws in sparse graphs", "comments": "This paper appeared in the NIPS 2015 Workshop on Networks in the\n  Social and Information Sciences,\n  http://stanford.edu/~jugander/NetworksNIPS2015/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data appear in a number of applications, such as online social\nnetworks and biological networks, and there is growing interest in both\ndeveloping models for networks as well as studying the properties of such data.\nSince individual network datasets continue to grow in size, it is necessary to\ndevelop models that accurately represent the real-life scaling properties of\nnetworks. One behavior of interest is having a power law in the degree\ndistribution. However, other types of power laws that have been observed\nempirically and considered for applications such as clustering and feature\nallocation models have not been studied as frequently in models for graph data.\nIn this paper, we enumerate desirable asymptotic behavior that may be of\ninterest for modeling graph data, including sparsity and several types of power\nlaws. We outline a general framework for graph generative models using\ncompletely random measures; by contrast to the pioneering work of Caron and Fox\n(2015), we consider instantiating more of the existing atoms of the random\nmeasure as the dataset size increases rather than adding new atoms to the\nmeasure. We see that these two models can be complementary; they respectively\nyield interpretations as (1) time passing among existing members of a network\nand (2) new individuals joining a network. We detail a particular instance of\nthis framework and show simulated results that suggest this model exhibits some\ndesirable asymptotic power-law behavior.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 19:14:55 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Cai", "Diana", ""], ["Broderick", "Tamara", ""]]}, {"id": "1603.06988", "submitter": "Cheng Zheng", "authors": "Cheng Zheng, Ying Qing Chen", "title": "On a Shape-Invariant Hazard Regression Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In survival analysis, Cox model is widely used for most clinical trial data.\nAlternatives include the additive hazard model, the accelerated failure time\n(AFT) model and a more general transformation model. All these models assume\nthat the effects for all covariates are on the same scale. However, it is\npossible that for different covariates, the effects are on different scales. In\nthis paper, we propose a shape-invariant hazard regression model that allows us\nto estimate the multiplicative treatment effect with adjustment of covariates\nthat have non-multiplicative effects. We propose moment-based inference\nprocedures for the regression parameters. We also discuss the risk prediction\nand goodness of fit test for our proposed model. Numerical studies show good\nfinite sample performance of our proposed estimator. We applied our method to\nVeteran's Administration (VA) lung cancer data and the HIVNET 012 data. For the\nlatter, we found that single-dose nevirapine treatment has a significant\nimprovement for 18-month survival with appropriate adjustment for maternal CD4\ncounts and virus load.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 21:22:16 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Zheng", "Cheng", ""], ["Chen", "Ying Qing", ""]]}, {"id": "1603.07041", "submitter": "Yuan Liao", "authors": "Jianqing Fan, Yuan Ke, Yuan Liao", "title": "Augmented Factor Models with Applications to Validating Market Risk\n  Factors and Forecasting Bond Risk Premia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study factor models augmented by observed covariates that have explanatory\npowers on the unknown factors. In financial factor models, the unknown factors\ncan be reasonably well explained by a few observable proxies, such as the\nFama-French factors. In diffusion index forecasts, identified factors are\nstrongly related to several directly measurable economic variables such as\nconsumption-wealth variable, financial ratios, and term spread. With those\ncovariates, both the factors and loadings are identifiable up to a rotation\nmatrix even only with a finite dimension. To incorporate the explanatory power\nof these covariates, we propose a smoothed principal component analysis (PCA):\n(i) regress the data onto the observed covariates, and (ii) take the principal\ncomponents of the fitted data to estimate the loadings and factors. This allows\nus to accurately estimate the percentage of both explained and unexplained\ncomponents in factors and thus to assess the explanatory power of covariates.\nWe show that both the estimated factors and loadings can be estimated with\nimproved rates of convergence compared to the benchmark method. The degree of\nimprovement depends on the strength of the signals, representing the\nexplanatory power of the covariates on the factors. The proposed estimator is\nrobust to possibly heavy-tailed distributions. We apply the model to forecast\nUS bond risk premia, and find that the observed macroeconomic characteristics\ncontain strong explanatory powers of the factors. The gain of forecast is more\nsubstantial when the characteristics are incorporated to estimate the common\nfactors than directly used for forecasts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 01:38:24 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 02:54:10 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Fan", "Jianqing", ""], ["Ke", "Yuan", ""], ["Liao", "Yuan", ""]]}, {"id": "1603.07066", "submitter": "Zhengwu Zhang", "authors": "Zhengwu Zhang and Eric Klassen and Anuj Srivastava", "title": "Phase-Amplitude Separation and Modeling of Spherical Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of separating phase-amplitude components in\nsample paths of a spherical process (longitudinal data on a unit two-sphere).\nSuch separation is essential for efficient modeling and statistical analysis of\nspherical longitudinal data in a manner that is invariant to any phase\nvariability. The key idea is to represent each path or trajectory with a pair\nof variables, a starting point and a Transported Square-Root Velocity Curve\n(TSRVC). A TSRVC is a curve in the tangent (vector) space at the starting point\nand has some important invariance properties under the L2 norm. The space of\nall such curves forms a vector bundle and the L2 norm, along with the standard\nRiemannian metric on S2, provides a natural metric on this vector bundle. This\ninvariant representation allows for separating phase and amplitude components\nin given data, using a template-based idea. Furthermore, the metric property is\nuseful in deriving computational procedures for clustering, mean computation,\nprincipal component analysis (PCA), and modeling. This comprehensive framework\nis demonstrated using two datasets: a set of bird-migration trajectories and a\nset of hurricane paths in the Atlantic ocean.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 04:19:33 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Zhang", "Zhengwu", ""], ["Klassen", "Eric", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1603.07117", "submitter": "Diaa Al Mohamad", "authors": "Diaa Al Mohamad and Michel Broniatowski", "title": "A Proximal Point Algorithm for Minimum Divergence Estimators with\n  Application to Mixture Models", "comments": "19 pages. Article submitted to Journal Entropy, special issue :\n  Diffierential Geometrical Theory of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimators derived from a divergence criterion such as $\\varphi-$divergences\nare generally more robust than the maximum likelihood ones. We are interested\nin particular in the so-called MD$\\varphi$DE, an estimator built using a dual\nrepresentation of $\\varphi$--divergences. We present in this paper an iterative\nproximal point algorithm which permits to calculate such estimator. This\nalgorithm contains by its construction the well-known EM algorithm. Our work is\nbased on the paper of \\citep{Tseng} on the likelihood function. We provide\nseveral convergence properties of the sequence generated by the algorithm, and\nimprove the existing results by relaxing the identifiability condition on the\nproximal term, a condition which is not verified for most mixture models and\nhard to be verified for non mixture ones. Since convergence analysis uses\nregularity conditions (continuity and differentiability) of the objective\nfunction, which has a supremal form, we find it useful to present some\nanalytical approaches for studying such functions. Convergence of the EM\nalgorithm is discussed here again in a Gaussian and Weibull mixtures in the\nspirit of our approach. Simulations are provided to confirm the validity of our\nwork and the robustness of the resulting estimators against outliers.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 10:19:16 GMT"}, {"version": "v2", "created": "Sun, 12 Jun 2016 16:50:39 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Mohamad", "Diaa Al", ""], ["Broniatowski", "Michel", ""]]}, {"id": "1603.07237", "submitter": "Coralie Merle", "authors": "Coralie Merle (IMAG, CBGP, IBC), Rapha\\\"el Leblois (CBGP, IBC),\n  Fran\\c{c}ois Rousset (ISEM, IBC), Pierre Pudlo (I2M, IBC)", "title": "Resampling: an improvement of Importance Sampling in varying population\n  size models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential importance sampling algorithms have been defined to estimate\nlikelihoods in models of ancestral population processes. However, these\nalgorithms are based on features of the models with constant population size,\nand become inefficient when the population size varies in time, making\nlikelihood-based inferences difficult in many demographic situations. In this\nwork, we modify a previous sequential importance sampling algorithm to improve\nthe efficiency of the likelihood estimation. Our procedure is still based on\nfeatures of the model with constant size, but uses a resampling technique with\na new resampling probability distribution depending on the pairwise composite\nlikelihood. We tested our algorithm, called sequential importance sampling with\nresampling (SISR) on simulated data sets under different demographic cases. In\nmost cases, we divided the computational cost by two for the same accuracy of\ninference, in some cases even by one hundred. This study provides the first\nassessment of the impact of such resampling techniques on parameter inference\nusing sequential importance sampling, and extends the range of situations where\nlikelihood inferences can be easily performed.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 15:31:05 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Merle", "Coralie", "", "IMAG, CBGP, IBC"], ["Leblois", "Rapha\u00ebl", "", "CBGP, IBC"], ["Rousset", "Fran\u00e7ois", "", "ISEM, IBC"], ["Pudlo", "Pierre", "", "I2M, IBC"]]}, {"id": "1603.07277", "submitter": "Xiaoli Gao", "authors": "Xiaoli Gao and S.E.Ahmed and Yang Feng", "title": "Post Selection Shrinkage Estimation for High Dimensional Data Analysis", "comments": "40 pages, 2 figures, discussion paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional data settings where $p\\gg n$, many penalized\nregularization approaches were studied for simultaneous variable selection and\nestimation. However, with the existence of covariates with weak effect, many\nexisting variable selection methods, including Lasso and its generations,\ncannot distinguish covariates with weak and no contribution. Thus, prediction\nbased on a subset model of selected covariates only can be inefficient. In this\npaper, we propose a post selection shrinkage estimation strategy to improve the\nprediction performance of a selected subset model. Such a post selection\nshrinkage estimator (PSE) is data-adaptive and constructed by shrinking a post\nselection weighted ridge estimator in the direction of a selected candidate\nsubset. Under an asymptotic distributional quadratic risk criterion, its\nprediction performance is explored analytically. We show that the proposed post\nselection PSE performs better than the post selection weighted ridge estimator.\nMore importantly, it improves the prediction performance of any candidate\nsubset model selected from most existing Lasso-type variable selection methods\nsignificantly. The relative performance of the post selection PSE is\ndemonstrated by both simulation studies and real data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 17:21:17 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Gao", "Xiaoli", ""], ["Ahmed", "S. E.", ""], ["Feng", "Yang", ""]]}, {"id": "1603.07427", "submitter": "Xiaoli Gao", "authors": "Xiaoli Gao and Yixin Fang", "title": "Penalized Weighted Least Squares for Outlier Detection and Robust\n  Regression", "comments": "27 pages; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To conduct regression analysis for data contaminated with outliers, many\napproaches have been proposed for simultaneous outlier detection and robust\nregression, so is the approach proposed in this manuscript. This new approach\nis called \"penalized weighted least squares\" (PWLS). By assigning each\nobservation an individual weight and incorporating a lasso-type penalty on the\nlog-transformation of the weight vector, the PWLS is able to perform outlier\ndetection and robust regression simultaneously. A Bayesian point-of-view of the\nPWLS is provided, and it is showed that the PWLS can be seen as an example of\nM-estimation. Two methods are developed for selecting the tuning parameter in\nthe PWLS. The performance of the PWLS is demonstrated via simulations and real\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 03:58:32 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Gao", "Xiaoli", ""], ["Fang", "Yixin", ""]]}, {"id": "1603.07493", "submitter": "Mickael De Backer", "authors": "Mickael De Backer, Anouar El Ghouch, Ingrid Van Keilegom", "title": "Semiparametric Copula Quantile Regression for Complete or Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When facing multivariate covariates, general semiparametric regression\ntechniques come at hand to propose flexible models that are unexposed to the\ncurse of dimensionality. In this work a semiparametric copula-based estimator\nfor conditional quantiles is investigated for complete or right-censored data.\nIn spirit, the methodology is extending the recent work of Noh et al. (2013)\nand Noh et al. (2015), as the main idea consists in appropriately defining the\nquantile regression in terms of a multivariate copula and marginal\ndistributions. Prior estimation of the latter and simple plug-in lead to an\neasily implementable estimator expressed, for both contexts with or without\ncensoring, as a weighted quantile of the observed response variable. In\naddition, and contrary to the initial suggestion in the literature, a\nsemiparametric estimation scheme for the multivariate copula density is\nstudied, motivated by the possible shortcomings of a purely parametric approach\nand driven by the regression context. The resulting quantile regression\nestimator has the valuable property of being automatically monotonic across\nquantile levels, and asymptotic normality for both complete and censored data\nis obtained under classical regularity conditions. Finally, numerical examples\nas well as a real data application are used to illustrate the validity and\nfinite sample performance of the proposed procedure.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 09:27:09 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["De Backer", "Mickael", ""], ["Ghouch", "Anouar El", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "1603.07749", "submitter": "Xi Luo", "authors": "Yi Zhao, Xi Luo", "title": "Pathway Lasso: Estimate and Select Sparse Mediation Pathways with High\n  Dimensional Mediators", "comments": "26 pages and 7 figures. Presented at the 2016 ENAR meeting, March 8,\n  2016, see slides at\n  https://rluo.github.io/slides/MultipleMediator_ENAR_2016.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific studies, it becomes increasingly important to delineate\nthe causal pathways through a large number of mediators, such as genetic and\nbrain mediators. Structural equation modeling (SEM) is a popular technique to\nestimate the pathway effects, commonly expressed as products of coefficients.\nHowever, it becomes unstable to fit such models with high dimensional\nmediators, especially for a general setting where all the mediators are\ncausally dependent but the exact causal relationships between them are unknown.\nThis paper proposes a sparse mediation model using a regularized SEM approach,\nwhere sparsity here means that a small number of mediators have nonzero\nmediation effects between a treatment and an outcome. To address the model\nselection challenge, we innovate by introducing a new penalty called Pathway\nLasso. This penalty function is a convex relaxation of the non-convex product\nfunction, and it enables a computationally tractable optimization criterion to\nestimate and select many pathway effects simultaneously. We develop a fast\nADMM-type algorithm to compute the model parameters, and we show that the\niterative updates can be expressed in closed form. On both simulated data and a\nreal fMRI dataset, the proposed approach yields higher pathway selection\naccuracy and lower estimation bias than other competing methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 20:46:24 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Zhao", "Yi", ""], ["Luo", "Xi", ""]]}, {"id": "1603.07816", "submitter": "Jared Murray", "authors": "Jared S. Murray", "title": "Probabilistic Record Linkage and Deduplication after Indexing, Blocking,\n  and Filtering", "comments": null, "journal-ref": "Journal of Privacy and Confidentiality: Vol. 7: Iss. 1, Article 2", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic record linkage, the task of merging two or more databases in\nthe absence of a unique identifier, is a perennial and challenging problem. It\nis closely related to the problem of deduplicating a single database, which can\nbe cast as linking a single database against itself. In both cases the number\nof possible links grows rapidly in the size of the databases under\nconsideration, and in most applications it is necessary to first reduce the\nnumber of record pairs that will be compared.\n  Spurred by practical considerations, a range of methods have been developed\nfor this task. These methods go under a variety of names, including indexing\nand blocking, and have seen significant development. However, methods for\ninferring linkage structure that account for indexing, blocking, and additional\nfiltering steps have not seen commensurate development. In this paper we review\nthe implications of indexing, blocking and filtering within the popular\nFellegi-Sunter framework, and propose a new model to account for particular\nforms of indexing and filtering.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 03:53:55 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Murray", "Jared S.", ""]]}, {"id": "1603.07822", "submitter": "Gautier Marti", "authors": "Gautier Marti, Frank Nielsen, Philippe Donnat, S\\'ebastien Andler", "title": "On clustering financial time series: a need for distances between\n  dependent random variables", "comments": "Work presented during a workshop on Information Geometry at the\n  International Centre for Mathematical Sciences, Edinburgh, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following working document summarizes our work on the clustering of\nfinancial time series. It was written for a workshop on information geometry\nand its application for image and signal processing. This workshop brought\nseveral experts in pure and applied mathematics together with applied\nresearchers from medical imaging, radar signal processing and finance. The\nauthors belong to the latter group. This document was written as a long\nintroduction to further development of geometric tools in financial\napplications such as risk or portfolio analysis. Indeed, risk and portfolio\nanalysis essentially rely on covariance matrices. Besides that the Gaussian\nassumption is known to be inaccurate, covariance matrices are difficult to\nestimate from empirical data. To filter noise from the empirical estimate,\nMantegna proposed using hierarchical clustering. In this work, we first show\nthat this procedure is statistically consistent. Then, we propose to use\nclustering with a much broader application than the filtering of empirical\ncovariance matrices from the estimate correlation coefficients. To be able to\ndo that, we need to obtain distances between the financial time series that\nincorporate all the available information in these cross-dependent random\nprocesses.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 05:15:50 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Marti", "Gautier", ""], ["Nielsen", "Frank", ""], ["Donnat", "Philippe", ""], ["Andler", "S\u00e9bastien", ""]]}, {"id": "1603.07843", "submitter": "Yuta Umezu", "authors": "Yuta Umezu, Yoshiyuki Ninomiya", "title": "On the Consistency of the Bias Correction Term of the AIC for the\n  Non-Concave Penalized Likelihood Method", "comments": "18 pages. arXiv admin note: substantial text overlap with\n  arXiv:1509.01688", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized likelihood methods with an $\\ell_{\\gamma}$-type penalty, such as\nthe Bridge, the SCAD, and the MCP, allow us to estimate a parameter and to do\nvariable selection, simultaneously, if $\\gamma\\in (0,1]$. In this method, it is\nimportant to choose a tuning parameter which controls the penalty level, since\nwe can select the model as we want when we choose it arbitrarily. Nowadays,\nseveral information criteria have been developed to choose the tuning parameter\nwithout such an arbitrariness. However the bias correction term of such\ninformation criteria depend on the true parameter value in general, then we\nusually plug-in a consistent estimator of it to compute the information\ncriteria from the data. In this paper, we derive a consistent estimator of the\nbias correction term of the AIC for the non-concave penalized likelihood method\nand propose a simple AIC-type information criterion for such models.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 08:41:24 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Umezu", "Yuta", ""], ["Ninomiya", "Yoshiyuki", ""]]}, {"id": "1603.07895", "submitter": "Rebecca Wooten Dr.", "authors": "Rebecca D. Wooten", "title": "Lattice Designs in Standard and Simple Implicit Multi-linear Regression", "comments": "15 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statisticians generally use ordinary least squares to minimize the random\nerror in a subject response with respect to independent explanatory variable.\nHowever, Wooten shows illustrates how ordinary least squares can be used to\nminimize the random error in the system without defining a subject response.\nUsing lattice design Wooten shows that non-response analysis is a superior\nalternative rotation of the pyramidal relationship between random variables and\nparameter estimates in multi-linear regression. Non-Response Analysis for\nsimple linear co-linearity and Rotational Analysis in Simple Linear Regression\nchallenge the notion of fixed effects; unity is included as a random measure\n(variable). The illustrations using lattice designs a mean operator that\ngenerates the standard mean and the self-weighing mean, among other point\nestimates with random weights; and a join that illustrates variance and\ncovariance; and develops the measures of variance referred to as internal\nco-variance and base variance. These concepts are used to illustrate how these\nmeasures are used to evaluate parameter estimates in standard simple linear\nregression and simple implicit regression (non-response and rotational). The\nresulting analysis of these lattice designs show standard simple linear\nregression limits the relationship by consider the variance in one direction as\nrelating to the two adjacent co-variances (standard and internal) whereas\nnon-response analysis defines the relationship in terms of the internal\nco-variances and the base variance.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 12:40:44 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Wooten", "Rebecca D.", ""]]}, {"id": "1603.07978", "submitter": "Guido Kuersteiner", "authors": "Guido M. Kuersteiner", "title": "Invariance Principles for Dependent Processes Indexed by Besov Classes\n  with an Application to a Hausman Test for Linearity", "comments": null, "journal-ref": "Journal of Econometrics, Volume 211, 2019", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers functional central limit theorems for stationary\nabsolutely regular mixing processes. Bounds for the entropy with bracketing are\nderived using recent results in Nickl and P\\\"otscher (2007). More specifically,\ntheir bracketing metric entropy bounds are extended to a norm defined in\nDoukhan, Massart and Rio (1995, henceforth DMR) that depends both on the\nmarginal distribution of the process and on the mixing coefficients. Using\nthese bounds, and based on a result in DMR, it is shown that for the class of\nweighted Besov spaces polynomially decaying tail behavior of the function class\nis sufficient to obtain a functional central limit theorem under minimal\nconditions. A second class of functions that allow for a functional central\nlimit theorem under minimal conditions are smooth functions defined on bounded\nsets. Similarly, a functional CLT for polynomially explosive tail behavior is\nobtained under additional moment conditions that are easy to check. An\napplication to a Hausman specification test illustrates the theory.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 18:26:30 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 15:58:26 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Kuersteiner", "Guido M.", ""]]}, {"id": "1603.07987", "submitter": "Takuya Ura", "authors": "Federico A. Bugni and Takuya Ura", "title": "Inference in Dynamic Discrete Choice Problems under Local\n  Misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-agent dynamic discrete choice models are typically estimated using\nheavily parametrized econometric frameworks, making them susceptible to model\nmisspecification. This paper investigates how misspecification affects the\nresults of inference in these models. Specifically, we consider a local\nmisspecification framework in which specification errors are assumed to vanish\nat an arbitrary and unknown rate with the sample size. Relative to global\nmisspecification, the local misspecification analysis has two important\nadvantages. First, it yields tractable and general results. Second, it allows\nus to focus on parameters with structural interpretation, instead of\n\"pseudo-true\" parameters.\n  We consider a general class of two-step estimators based on the K-stage\nsequential policy function iteration algorithm, where K denotes the number of\niterations employed in the estimation. This class includes Hotz and Miller\n(1993)'s conditional choice probability estimator, Aguirregabiria and Mira\n(2002)'s pseudo-likelihood estimator, and Pesendorfer and Schmidt-Dengler\n(2008)'s asymptotic least squares estimator.\n  We show that local misspecification can affect the asymptotic distribution\nand even the rate of convergence of these estimators. In principle, one might\nexpect that the effect of the local misspecification could change with the\nnumber of iterations K. One of our main findings is that this is not the case,\ni.e., the effect of local misspecification is invariant to K. In practice, this\nmeans that researchers cannot eliminate or even alleviate problems of model\nmisspecification by changing K.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 19:28:15 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 18:59:35 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2017 16:34:52 GMT"}, {"version": "v4", "created": "Tue, 22 Aug 2017 13:06:10 GMT"}, {"version": "v5", "created": "Wed, 7 Feb 2018 16:04:26 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Bugni", "Federico A.", ""], ["Ura", "Takuya", ""]]}, {"id": "1603.08057", "submitter": "Victor Minden", "authors": "Victor Minden, Anil Damle, Kenneth L. Ho and Lexing Ying", "title": "Fast spatial Gaussian process maximum likelihood estimation via\n  skeletonization factorizations", "comments": "36 pages, 8 figures", "journal-ref": null, "doi": "10.1137/17M1116477", "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimation for parameter-fitting given observations from a\nGaussian process in space is a computationally-demanding task that restricts\nthe use of such methods to moderately-sized datasets. We present a framework\nfor unstructured observations in two spatial dimensions that allows for\nevaluation of the log-likelihood and its gradient (i.e., the score equations)\nin $\\tilde O(n^{3/2})$ time under certain assumptions, where $n$ is the number\nof observations. Our method relies on the skeletonization procedure described\nby Martinsson & Rokhlin in the form of the recursive skeletonization\nfactorization of Ho & Ying. Combining this with an adaptation of the matrix\npeeling algorithm of Lin et al. for constructing $\\mathcal{H}$-matrix\nrepresentations of black-box operators, we obtain a framework that can be used\nin the context of any first-order optimization routine to quickly and\naccurately compute maximum-likelihood estimates.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 23:39:45 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 09:10:46 GMT"}, {"version": "v3", "created": "Thu, 21 Apr 2016 00:39:09 GMT"}, {"version": "v4", "created": "Tue, 14 Feb 2017 18:39:33 GMT"}, {"version": "v5", "created": "Mon, 17 Jul 2017 19:07:06 GMT"}, {"version": "v6", "created": "Thu, 7 Sep 2017 20:46:02 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Minden", "Victor", ""], ["Damle", "Anil", ""], ["Ho", "Kenneth L.", ""], ["Ying", "Lexing", ""]]}, {"id": "1603.08113", "submitter": "Yohann De Castro", "authors": "Yohann De Castro and Thibault Espinasse and Paul Rochet", "title": "Reconstructing undirected graphs from eigenspaces", "comments": "25 pages, some figures. Final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at recovering an undirected weighted graph of $N$\nvertices from the knowledge of a perturbed version of the eigenspaces of its\nadjacency matrix $W$. For instance, this situation arises for stationary\nsignals on graphs or for Markov chains observed at random times. Our approach\nis based on minimizing a cost function given by the Frobenius norm of the\ncommutator $\\mathsf{A} \\mathsf{B}-\\mathsf{B} \\mathsf{A}$ between symmetric\nmatrices $\\mathsf{A}$ and $\\mathsf{B}$.\n  In the Erd\\H{o}s-R\\'enyi model with no self-loops, we show that\nidentifiability (i.e., the ability to reconstruct $W$ from the knowledge of its\neigenspaces) follows a sharp phase transition on the expected number of edges\nwith threshold function $N\\log N/2$.\n  Given an estimation of the eigenspaces based on a $n$-sample, we provide\nsupport selection procedures from theoretical and practical point of views. In\nparticular, when deleting an edge from the active support, our study unveils\nthat our test statistic is the order of $\\mathcal O(1/n)$ when we overestimate\nthe true support and lower bounded by a positive constant when the estimated\nsupport is smaller than the true support. This feature leads to a powerful\npractical support estimation procedure. Simulated and real life numerical\nexperiments assert our new methodology.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 14:56:35 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 08:43:21 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 11:31:17 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["De Castro", "Yohann", ""], ["Espinasse", "Thibault", ""], ["Rochet", "Paul", ""]]}, {"id": "1603.08232", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, Minh-Ngoc Tran, Mattias Villani, Robert Kohn and\n  Khue-Dung Dang", "title": "The block-Poisson estimator for optimally tuned exact subsampling MCMC", "comments": "The main paper is 28 pages. The supplementary material is 28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speeding up Markov Chain Monte Carlo (MCMC) for datasets with many\nobservations by data subsampling has recently received considerable attention.\nA pseudo-marginal MCMC method is proposed that estimates the likelihood by data\nsubsampling using a block-Poisson estimator. The estimator is a product of\nPoisson estimators, allowing us to update a single block of subsample\nindicators in each MCMC iteration so that a desired correlation is achieved\nbetween the logs of successive likelihood estimates. This is important since\npseudo-marginal MCMC with positively correlated likelihood estimates can use\nsubstantially smaller subsamples without adversely affecting the sampling\nefficiency. The block-Poisson estimator is unbiased but not necessarily\npositive, so the algorithm runs the MCMC on the absolute value of the\nlikelihood estimator and uses an importance sampling correction to obtain\nconsistent estimates of the posterior mean of any function of the parameters.\nOur article derives guidelines to select the optimal tuning parameters for our\nmethod and shows that it compares very favourably to regular MCMC without\nsubsampling, and to two other recently proposed exact subsampling approaches in\nthe literature.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 16:25:34 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 02:38:59 GMT"}, {"version": "v3", "created": "Fri, 20 Jan 2017 03:57:16 GMT"}, {"version": "v4", "created": "Mon, 26 Mar 2018 07:05:31 GMT"}, {"version": "v5", "created": "Tue, 10 Apr 2018 07:06:36 GMT"}, {"version": "v6", "created": "Tue, 7 Apr 2020 03:42:46 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Quiroz", "Matias", ""], ["Tran", "Minh-Ngoc", ""], ["Villani", "Mattias", ""], ["Kohn", "Robert", ""], ["Dang", "Khue-Dung", ""]]}, {"id": "1603.08315", "submitter": "Ziwei Zhu", "authors": "Jianqing Fan, Weichen Wang, Ziwei Zhu", "title": "A Shrinkage Principle for Heavy-Tailed Data: High-Dimensional Robust\n  Low-Rank Matrix Recovery", "comments": "57 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a simple principle for robust high-dimensional\nstatistical inference via an appropriate shrinkage on the data. This widens the\nscope of high-dimensional techniques, reducing the moment conditions from\nsub-exponential or sub-Gaussian distributions to merely bounded second or\nfourth moment. As an illustration of this principle, we focus on robust\nestimation of the low-rank matrix $\\Theta^*$ from the trace regression model\n$Y=Tr (\\Theta^{*T}X) +\\epsilon$. It encompasses four popular problems: sparse\nlinear models, compressed sensing, matrix completion and multi-task regression.\nWe propose to apply penalized least-squares approach to appropriately truncated\nor shrunk data. Under only bounded $2+\\delta$ moment condition on the response,\nthe proposed robust methodology yields an estimator that possesses the same\nstatistical error rates as previous literature with sub-Gaussian errors. For\nsparse linear models and multi-tasking regression, we further allow the design\nto have only bounded fourth moment and obtain the same statistical rates,\nagain, by appropriate shrinkage of the design matrix. As a byproduct, we give a\nrobust covariance matrix estimator and establish its concentration inequality\nin terms of the spectral norm when the random samples have only bounded fourth\nmoment. Extensive simulations have been carried out to support our theories.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 05:36:59 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 18:19:10 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Fan", "Jianqing", ""], ["Wang", "Weichen", ""], ["Zhu", "Ziwei", ""]]}, {"id": "1603.08355", "submitter": "Wei Yi", "authors": "Meng Jiang, Wei Yi and Lingjiang Kong", "title": "Multi-Sensor Control for Multi-Target Tracking Using Cauchy-Schwarz\n  Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of multi-sensor control for multi-target\ntracking via labelled random finite sets (RFS) in the sensor network systems.\nBased on an information theoretic divergence measure, namely Cauchy-Schwarz\n(CS) divergence which admits a closed form solution for GLMB densities, we\npropose two novel multi-sensor control approaches in the framework of\ngeneralized Covariance Intersection (GCI). The first joint decision making\n(JDM) method is optimal and can achieve overall good performance, while the\nsecond independent decision making (IDM) method is suboptimal as a fast\nrealization with smaller amount of computations. Simulation in challenging\nsituation is presented to verify the effectiveness of the two proposed\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 10:48:56 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Jiang", "Meng", ""], ["Yi", "Wei", ""], ["Kong", "Lingjiang", ""]]}, {"id": "1603.08602", "submitter": "Jairo Fuquene", "authors": "Jairo Alberto Fuquene Pati\\~no and Brenda Betancourt and Jo\\~ao B. M.\n  Pereira", "title": "A weakly informative prior for Bayesian dynamic model selection with\n  applications in fMRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Bayesian statistics methods in neuroscience have been\nshowing important advances. In particular, detection of brain signals for\nstudying the complexity of the brain is an active area of research. Functional\nmagnetic resonance imagining (fMRI) is an important tool to determine which\nparts of the brain are activated by different types of physical behavior.\nAccording to recent results there is evidence that the values of the\nconnectivity brain signal parameters are close to zero and due to the nature of\ntime series fMRI data with high frequency behavior, Bayesian dynamic models for\nidentifying sparsity are indeed far-reaching. We propose a multivariate\nBayesian dynamic approach for model selection and shrinkage estimation of the\nconnectivity parameters. We describe the coupling or lead-lag between any pair\nof regions by using mixture priors for the connectivity parameters and propose\na new weakly informative default prior for the state variances. This framework\nproduces one-step-ahead proper posterior predictive results and induces\nshrinkage and robustness suitable for fMRI data in the presence of sparsity. To\nexplore the performance of the proposed methodology we present simulation\nstudies and an application to functional magnetic resonance imaging data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 00:59:13 GMT"}, {"version": "v2", "created": "Sat, 3 Jun 2017 20:04:26 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Pati\u00f1o", "Jairo Alberto Fuquene", ""], ["Betancourt", "Brenda", ""], ["Pereira", "Jo\u00e3o B. M.", ""]]}, {"id": "1603.08652", "submitter": "Kun Liu", "authors": "Kun Liu, Ruizhi Zhang, and Yajun Mei", "title": "Scalable SUM-Shrinkage Schemes for Distributed Monitoring Large-Scale\n  Data Streams", "comments": "submitted to statistica sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, motivated by biosurveillance and censoring sensor networks,\nwe investigate the problem of distributed monitoring large-scale data streams\nwhere an undesired event may occur at some unknown time and affect only a few\nunknown data streams. We propose to develop scalable global monitoring schemes\nby parallel running local detection procedures and by combining these local\nprocedures together to make a global decision based on SUM-shrinkage\ntechniques. Our approach is illustrated in two concrete examples: one is the\nnonhomogeneous case when the pre-change and post-change local distributions are\ngiven, and the other is the homogeneous case of monitoring a large number of\nindependent $N(0,1)$ data streams where the means of some data streams might\nshift to unknown positive or negative values. Numerical simulation studies\ndemonstrate the usefulness of the proposed schemes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 05:44:26 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Liu", "Kun", ""], ["Zhang", "Ruizhi", ""], ["Mei", "Yajun", ""]]}, {"id": "1603.08815", "submitter": "Dustin Tran", "authors": "Dustin Tran, Minjae Kim, Finale Doshi-Velez", "title": "Spectral M-estimation with Applications to Hidden Markov Models", "comments": "Appears in Artificial Intelligence and Statistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Method of moment estimators exhibit appealing statistical properties, such as\nasymptotic unbiasedness, for nonconvex problems. However, they typically\nrequire a large number of samples and are extremely sensitive to model\nmisspecification. In this paper, we apply the framework of M-estimation to\ndevelop both a generalized method of moments procedure and a principled method\nfor regularization. Our proposed M-estimator obtains optimal sample efficiency\nrates (in the class of moment-based estimators) and the same well-known rates\non prediction accuracy as other spectral estimators. It also makes it\nstraightforward to incorporate regularization into the sample moment\nconditions. We demonstrate empirically the gains in sample efficiency from our\napproach on hidden Markov models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 15:34:29 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Tran", "Dustin", ""], ["Kim", "Minjae", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1603.09000", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Andrea Montanari", "title": "Online Rules for Control of False Discovery Rate and False Discovery\n  Exceedance", "comments": "44 pages, 9 figures, to appear in Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypothesis testing is a core problem in statistical inference and\narises in almost every scientific field. Given a set of null hypotheses\n$\\mathcal{H}(n) = (H_1,\\dotsc, H_n)$, Benjamini and Hochberg introduced the\nfalse discovery rate (FDR), which is the expected proportion of false positives\namong rejected null hypotheses, and proposed a testing procedure that controls\nFDR below a pre-assigned significance level. Nowadays FDR is the criterion of\nchoice for large scale multiple hypothesis testing. In this paper we consider\nthe problem of controlling FDR in an \"online manner\". Concretely, we consider\nan ordered --possibly infinite-- sequence of null hypotheses $\\mathcal{H} =\n(H_1,H_2,H_3,\\dots )$ where, at each step $i$, the statistician must decide\nwhether to reject hypothesis $H_i$ having access only to the previous\ndecisions. This model was introduced by Foster and Stine. We study a class of\n\"generalized alpha-investing\" procedures and prove that any rule in this class\ncontrols online FDR, provided $p$-values corresponding to true nulls are\nindependent from the other $p$-values. (Earlier work only established mFDR\ncontrol.) Next, we obtain conditions under which generalized alpha-investing\ncontrols FDR in the presence of general $p$-values dependencies. Finally, we\ndevelop a modified set of procedures that also allow to control the false\ndiscovery exceedance (the tail of the proportion of false discoveries).\nNumerical simulations and analytical results indicate that online procedures do\nnot incur a large loss in statistical power with respect to offline approaches,\nsuch as Benjamini-Hochberg.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 23:41:51 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 23:49:37 GMT"}, {"version": "v3", "created": "Thu, 6 Jul 2017 23:37:43 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1603.09088", "submitter": "Christian P. Robert", "authors": "Christian P. Robert and Judith Rousseau (Universit\\'e Paris-Dauphine,\n  PSL)", "title": "Some comments about James Watson's and Chris Holmes' \"Approximate Models\n  and Robust Decisions\": Nonparametric Bayesian clay for robust decision bricks", "comments": "7 pages, discussion of Watson and Holmes (2016) to appear in\n  Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note discusses Watson and Holmes (2016) and their pro- posals towards\nmore robust Bayesian decisions. While we acknowledge and commend the authors\nfor setting new and all-encompassing prin- ciples of Bayesian robustness, and\nwe appreciate the strong anchoring of those within a decision-theoretic\nreferential, we remain uncertain as to which extent such principles can be\napplied outside binary de- cisions. We also wonder at the ultimate relevance of\nKullback-Leibler neighbourhoods to characterise robustness and favour\nextensions along non-parametric axes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 09:09:32 GMT"}, {"version": "v2", "created": "Sat, 9 Apr 2016 10:19:06 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine,\n  PSL"], ["Rousseau", "Judith", "", "Universit\u00e9 Paris-Dauphine,\n  PSL"]]}, {"id": "1603.09272", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta and Paul Blomstedt and Samuel Kaski", "title": "Bayesian inference in hierarchical models by combining independent\n  posteriors", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical models are versatile tools for joint modeling of data sets\narising from different, but related, sources. Fully Bayesian inference may,\nhowever, become computationally prohibitive if the source-specific data models\nare complex, or if the number of sources is very large. To facilitate\ncomputation, we propose an approach, where inference is first made\nindependently for the parameters of each data set, whereupon the obtained\nposterior samples are used as observed data in a substitute hierarchical model,\nbased on a scaled likelihood function. Compared to direct inference in a full\nhierarchical model, the approach has the advantage of being able to speed up\nconvergence by breaking down the initial large inference problem into smaller\nindividual subproblems with better convergence properties. Moreover it enables\nparallel processing of the possibly complex inferences of the source-specific\nparameters, which may otherwise create a computational bottleneck if processed\njointly as part of a hierarchical model. The approach is illustrated with both\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 16:42:35 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 09:33:22 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Blomstedt", "Paul", ""], ["Kaski", "Samuel", ""]]}, {"id": "1603.09326", "submitter": "Guido Imbens", "authors": "Susan Athey, Raj Chetty, Guido Imbens, Hyunseung Kang", "title": "Estimating Treatment Effects using Multiple Surrogates: The Role of the\n  Surrogate Score and the Surrogate Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the long-term effects of treatments is of interest in many fields.\nA common challenge in estimating such treatment effects is that long-term\noutcomes are unobserved in the time frame needed to make policy decisions. One\napproach to overcome this missing data problem is to analyze treatments effects\non an intermediate outcome, often called a statistical surrogate, if it\nsatisfies the condition that treatment and outcome are independent conditional\non the statistical surrogate. The validity of the surrogacy condition is often\ncontroversial. Here we exploit that fact that in modern datasets, researchers\noften observe a large number, possibly hundreds or thousands, of intermediate\noutcomes, thought to lie on or close to the causal chain between the treatment\nand the long-term outcome of interest. Even if none of the individual proxies\nsatisfies the statistical surrogacy criterion by itself, using multiple proxies\ncan be useful in causal inference. We focus primarily on a setting with two\nsamples, an experimental sample containing data about the treatment indicator\nand the surrogates and an observational sample containing information about the\nsurrogates and the primary outcome. We state assumptions under which the\naverage treatment effect be identified and estimated with a high-dimensional\nvector of proxies that collectively satisfy the surrogacy assumption, and\nderive the bias from violations of the surrogacy assumption, and show that even\nif the primary outcome is also observed in the experimental sample, there is\nstill information to be gained from using surrogates.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 19:45:52 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2016 19:32:10 GMT"}, {"version": "v3", "created": "Sat, 29 Feb 2020 07:18:27 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Athey", "Susan", ""], ["Chetty", "Raj", ""], ["Imbens", "Guido", ""], ["Kang", "Hyunseung", ""]]}, {"id": "1603.09706", "submitter": "Georgios Papageorgiou", "authors": "Georgios Papageorgiou", "title": "Bayesian density regression for discrete outcomes", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": "10.1111/anzs.12273", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Bayesian models for density regression with emphasis on discrete\noutcomes. The problem of density regression is approached by considering\nmethods for multivariate density estimation of mixed scale variables, and\nobtaining conditional densities from the multivariate ones. The approach to\nmultivariate mixed scale outcome density estimation that we describe represents\ndiscrete variables, either responses or covariates, as discretised versions of\ncontinuous latent variables. We present and compare several models for\nobtaining these thresholds in the challenging context of count data analysis\nwhere the response may be over- and/or under-dispersed in some of the regions\nof the covariate space. We utilise a nonparametric mixture of multivariate\nGaussians to model the directly observed and the latent continuous variables.\nThe paper presents a Markov chain Monte Carlo algorithm for posterior sampling,\nsufficient conditions for weak consistency, and illustrations on density, mean\nand quantile regression utilizing simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 18:25:27 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 18:43:47 GMT"}, {"version": "v3", "created": "Sun, 22 Apr 2018 13:51:52 GMT"}, {"version": "v4", "created": "Tue, 13 Aug 2019 12:47:50 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Papageorgiou", "Georgios", ""]]}, {"id": "1603.09730", "submitter": "Nicolette Meshkat", "authors": "Heather A. Harrington, Kenneth L. Ho, and Nicolette Meshkat", "title": "Differential Algebra for Model Comparison", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS math.AG q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for rejecting competing models from noisy time-course\ndata that does not rely on parameter inference. First we characterize ordinary\ndifferential equation models in only measurable variables using differential\nalgebra elimination. Next we extract additional information from the given data\nusing Gaussian Process Regression (GPR) and then transform the differential\ninvariants. We develop a test using linear algebra and statistics to reject\ntransformed models with the given data in a parameter-free manner. This\nalgorithm exploits the information about transients that is encoded in the\nmodel's structure. We demonstrate the power of this approach by discriminating\nbetween different models from mathematical biology.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 19:21:47 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Harrington", "Heather A.", ""], ["Ho", "Kenneth L.", ""], ["Meshkat", "Nicolette", ""]]}]