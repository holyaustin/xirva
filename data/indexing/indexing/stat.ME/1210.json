[{"id": "1210.0200", "submitter": "Paul von Hippel", "authors": "Paul T. von Hippel, Igor Holas, Samuel V. Scarpino", "title": "Estimation with Binned Data", "comments": "16 pages + 2 tables + 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variables such as household income are sometimes binned, so that we only know\nhow many households fall in each of several bins such as $0-10,000,\n$10,000-15,000, or $200,000+. We provide a SAS macro that estimates the mean\nand variance of binned data by fitting the extended generalized gamma (EGG)\ndistribution, the power normal (PN) distribution, and a new distribution that\nwe call the power logistic (PL). The macro also implements a \"best-of-breed\"\nestimator that chooses from among the EGG, PN, and PL estimates on the basis of\nlikelihood and finite variance. We test the macro by estimating the mean family\nand household incomes of approximately 13,000 US school districts between 1970\nand 2009. The estimates have negligible bias (0-2%) and a root mean squared\nerror of just 3-6%. The estimates compare favorably with estimates obtained by\nfitting the Dagum, generalized beta (GB2), or logspline distributions.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2012 13:50:20 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2012 14:36:56 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["von Hippel", "Paul T.", ""], ["Holas", "Igor", ""], ["Scarpino", "Samuel V.", ""]]}, {"id": "1210.0380", "submitter": "Linn Cecilie Bergersen", "authors": "Linn Cecilie Bergersen, Isma\\\"il Ahmed, Arnoldo Frigessi, Ingrid K.\n  Glad and Sylvia Richardson", "title": "Safe preselection in lasso-type problems by cross-validation freezing", "comments": "This paper has been withdrawn by the authors because of a mistake", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to safe variable preselection in high-dimensional\npenalized regression, such as the lasso. Preselection - to start with a\nmanageable set of covariates - has often been implemented without clear\nappreciation of its potential bias. Based on sequential implementation of the\nlasso with increasing lists of predictors, we find a new property of the set of\ncorresponding cross-validation curves, a pattern that we call freezing. It\nallows to determine a subset of covariates with which we reach the same lasso\nsolution as would be obtained using the full set of covariates. Freezing has\nnot been characterized before and is different from recently discussed safe\nrules for discarding predictors. We demonstrate by simulation that ranking\npredictors by their univariate correlation with the outcome, leads in a\nmajority of cases to early freezing, giving a safe and efficient way of\nfocusing the lasso analysis on a smaller and manageable number of predictors.\nWe illustrate the applicability of our strategy in the context of a GWAS\nanalysis and on microarray genomic data. Freezing offers great potential for\nextending the applicability of penalized regressions to ultra highdimensional\ndata sets. Its applicability is not limited to the standard lasso but is a\ngeneric property of many penalized approaches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 12:59:32 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2012 15:08:08 GMT"}], "update_date": "2012-12-10", "authors_parsed": [["Bergersen", "Linn Cecilie", ""], ["Ahmed", "Isma\u00efl", ""], ["Frigessi", "Arnoldo", ""], ["Glad", "Ingrid K.", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1210.0385", "submitter": "William DuMouchel", "authors": "William DuMouchel", "title": "Multivariate Bayesian Logistic Regression for Analysis of Clinical Study\n  Safety Issues", "comments": "Published in at http://dx.doi.org/10.1214/11-STS381 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 3, 319-339", "doi": "10.1214/11-STS381", "report-no": "IMS-STS-STS381", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for a model-based analysis of clinical safety\ndata called multivariate Bayesian logistic regression (MBLR). Parallel logistic\nregression models are fit to a set of medically related issues, or response\nvariables, and MBLR allows information from the different issues to \"borrow\nstrength\" from each other. The method is especially suited to sparse response\ndata, as often occurs when fine-grained adverse events are collected from\nsubjects in studies sized more for efficacy than for safety investigations. A\ncombined analysis of data from multiple studies can be performed and the method\nenables a search for vulnerable subgroups based on the covariates in the\nregression model. An example involving 10 medically related issues from a pool\nof 8 studies is presented, as well as simulations showing distributional\nproperties of the method.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 13:05:01 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["DuMouchel", "William", ""]]}, {"id": "1210.0460", "submitter": "Maciej Kurant", "authors": "Maciej Kurant, Carter T. Butts, Athina Markopoulou", "title": "Graph Size Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online networks are not fully known and are often studied via sampling.\nRandom Walk (RW) based techniques are the current state-of-the-art for\nestimating nodal attributes and local graph properties, but estimating global\nproperties remains a challenge. In this paper, we are interested in a\nfundamental property of this type - the graph size N, i.e., the number of its\nnodes. Existing methods for estimating N are (i) inefficient and (ii) cannot be\neasily used with RW sampling due to dependence between successive samples. In\nthis paper, we address both problems. First, we propose IE (Induced Edges), an\nefficient technique for estimating N from an independence sample of graph's\nnodes. IE exploits the edges induced on the sampled nodes. Second, we introduce\nSafetyMargin, a method that corrects estimators for dependence in RW samples.\nFinally, we combine these two stand-alone techniques to obtain a RW-based graph\nsize estimator. We evaluate our approach in simulations on a wide range of\nreal-life topologies, and on several samples of Facebook. IE with SafetyMargin\ntypically requires at least 10 times fewer samples than the state-of-the-art\ntechniques (over 100 times in the case of Facebook) for the same estimation\nerror.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 16:34:58 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Kurant", "Maciej", ""], ["Butts", "Carter T.", ""], ["Markopoulou", "Athina", ""]]}, {"id": "1210.0493", "submitter": "Pavel  Krivitsky", "authors": "Pavel N. Krivitsky (School of Mathematics and Applied Statistics and\n  National Institute for Applied Statistics Research Australia (NIASRA),\n  University of Wollongong, Wollongong), Carter T. Butts (Departments of\n  Sociology and Statistics and Institute for Mathematical Behavioral Sciences,\n  University of California, Irvine)", "title": "Exponential-Family Random Graph Models for Rank-Order Relational Data", "comments": "50 pages, 6 figures, 4 tables, 1 algorithm. The paper has been\n  expanded and clarified, and some terms were changed", "journal-ref": "Krivitsky, P. N. & Butts, C. T. (2017) Exponential-Family Random\n  Graph Models for Rank-Order Relational Data. Sociological Methodology, 47(1):\n  68-112", "doi": "10.1177/0081175017692623", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank-order relational data, in which each actor ranks the others according to\nsome criterion, often arise from sociometric measurements of judgment (e.g.,\nself-reported interpersonal interaction) or preference (e.g., relative liking).\nWe propose a class of exponential-family models for rank-order relational data\nand derive a new class of sufficient statistics for such data, which assume no\nmore than within-subject ordinal properties. Application of MCMC MLE to this\nfamily allows us to estimate effects for a variety of plausible mechanisms\ngoverning rank structure in cross-sectional context, and to model the evolution\nof such structures over time. We apply this framework to model the evolution of\nrelative liking judgments in an acquaintance process, and to model recall of\nrelative volume of interpersonal interaction among members of a technology\neducation program.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 18:21:12 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2015 13:49:12 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Krivitsky", "Pavel N.", "", "School of Mathematics and Applied Statistics and\n  National Institute for Applied Statistics Research Australia"], ["Butts", "Carter T.", "", "Departments of\n  Sociology and Statistics and Institute for Mathematical Behavioral Sciences,\n  University of California, Irvine"]]}, {"id": "1210.0586", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan, K{\\i}van\\c{c} Ertu\\u{g}ay, \\c{S}ebnem D\\\"uzg\\\"un", "title": "Exploratory and Inferential Methods for Spatio-Temporal Analysis of\n  Residential Fire Clustering in Urban Areas", "comments": "37 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": "#KU-EC-12-2", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatio-temporal analysis of residential fires could allow decision makers\nto plan effective resource allocations in fire management according to fire\nclustering levels in space and time. In this study, we provide guidelines for\nthe use of various methods in detecting the differences in clustering patterns\nof fire and non-fire (i.e., background residential) locations and how these\npatterns change over time. As a preliminary analysis step, various exploratory\ndata analysis methods, such as, intensity plots (i.e., kernel density\nestimates) are used. Moreover, the use of Diggle's D-function (a second order\nanalysis technique) is proposed for detecting the clustering of residential\nfire locations (if any) and whether there is additional clustering (or\nregularity) in the locations of the fires compared to background residential\npattern. A test for trend over time (in years, months, and weeks) of the fire\nlocation patterns are provided with a space-time interaction analysis by\nspatio-temporal K-function. Residential fire data from \\c{C}ankaya Municipality\nof Ankara, Turkey is used as an illustrative example. The presented methodology\nis also applicable to residential fire data from similar urban settings.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 21:33:06 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["Ceyhan", "Elvan", ""], ["Ertu\u011fay", "K\u0131van\u00e7", ""], ["D\u00fczg\u00fcn", "\u015eebnem", ""]]}, {"id": "1210.0655", "submitter": "Bradley W. McEvoy", "authors": "Bradley W. McEvoy, Ram C. Tiwari", "title": "Discussion of \"Multivariate Bayesian Logistic Regression for Analysis of\n  Clinical Trial Safety Issues\" by W. DuMouchel", "comments": "Published in at http://dx.doi.org/10.1214/12-STS381A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 3, 340-343", "doi": "10.1214/12-STS381A", "report-no": "IMS-STS-STS381A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Multivariate Bayesian Logistic Regression for Analysis of\nClinical Trial Safety Issues\" by W. DuMouchel [arXiv:1210.0385].\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 05:37:11 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["McEvoy", "Bradley W.", ""], ["Tiwari", "Ram C.", ""]]}, {"id": "1210.0658", "submitter": "Don Berry", "authors": "Don Berry", "title": "Discussion of \"Multivariate Bayesian Logistic Regression for Analysis of\n  Clinical Trial Safety Issues\" by W. DuMouchel", "comments": "Published in at http://dx.doi.org/10.1214/12-STS381B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 3, 344-345", "doi": "10.1214/12-STS381B", "report-no": "IMS-STS-STS381B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Multivariate Bayesian Logistic Regression for Analysis of\nClinical Trial Safety Issues\" by W. DuMouchel [arXiv:1210.0385].\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 06:06:15 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Berry", "Don", ""]]}, {"id": "1210.0663", "submitter": "Stephen Evans", "authors": "Stephen Evans", "title": "An Answer to Multiple Problems with Analysis of Data on Harms?", "comments": "Published in at http://dx.doi.org/10.1214/12-STS381C the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 3, 346-347", "doi": "10.1214/12-STS381C", "report-no": "IMS-STS-STS381C", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Multivariate Bayesian Logistic Regression for Analysis of\nClinical Trial Safety Issues\" by W. DuMouchel [arXiv:1210.0385].\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 06:24:36 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Evans", "Stephen", ""]]}, {"id": "1210.0669", "submitter": "William DuMouchel", "authors": "William DuMouchel", "title": "Rejoinder to \"Multivariate Bayesian Logistic Regression for Analysis of\n  Clinical Trial Safety Issues\"", "comments": "Published in at http://dx.doi.org/10.1214/12-STS381REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 3, 348-349", "doi": "10.1214/12-STS381REJ", "report-no": "IMS-STS-STS381REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder to \"Multivariate Bayesian Logistic Regression for Analysis of\nClinical Trial Safety Issues\" by W. DuMouchel [arXiv:1210.0385].\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 06:38:04 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["DuMouchel", "William", ""]]}, {"id": "1210.0701", "submitter": "Yoonkyung Lee", "authors": "Yoonkyung Lee, Steven N. MacEachern, Yoonsuh Jung", "title": "Regularization of Case-Specific Parameters for Robustness and Efficiency", "comments": "Published in at http://dx.doi.org/10.1214/11-STS377 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 3, 350-372", "doi": "10.1214/11-STS377", "report-no": "IMS-STS-STS377", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization methods allow one to handle a variety of inferential problems\nwhere there are more covariates than cases. This allows one to consider a\npotentially enormous number of covariates for a problem. We exploit the power\nof these techniques, supersaturating models by augmenting the \"natural\"\ncovariates in the problem with an additional indicator for each case in the\ndata set. We attach a penalty term for these case-specific indicators which is\ndesigned to produce a desired effect. For regression methods with squared error\nloss, an $\\ell_1$ penalty produces a regression which is robust to outliers and\nhigh leverage cases; for quantile regression methods, an $\\ell_2$ penalty\ndecreases the variance of the fit enough to overcome an increase in bias. The\nparadigm thus allows us to robustify procedures which lack robustness and to\nincrease the efficiency of procedures which are robust. We provide a general\nframework for the inclusion of case-specific parameters in regularization\nproblems, describing the impact on the effective loss for a variety of\nregression and classification problems. We outline a computational strategy by\nwhich existing software can be modified to solve the augmented regularization\nproblem, providing conditions under which such modification will converge to\nthe optimum solution. We illustrate the benefits of including case-specific\nparameters in the context of mean regression and quantile regression through\nanalysis of NHANES and linguistic data sets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 08:56:26 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Lee", "Yoonkyung", ""], ["MacEachern", "Steven N.", ""], ["Jung", "Yoonsuh", ""]]}, {"id": "1210.0736", "submitter": "Yazhen Wang", "authors": "Yazhen Wang", "title": "Quantum Computation and Quantum Information", "comments": "Published in at http://dx.doi.org/10.1214/11-STS378 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 3, 373-394", "doi": "10.1214/11-STS378", "report-no": "IMS-STS-STS378", "categories": "stat.ME quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computation and quantum information are of great current interest in\ncomputer science, mathematics, physical sciences and engineering. They will\nlikely lead to a new wave of technological innovations in communication,\ncomputation and cryptography. As the theory of quantum physics is fundamentally\nstochastic, randomness and uncertainty are deeply rooted in quantum\ncomputation, quantum simulation and quantum information. Consequently quantum\nalgorithms are random in nature, and quantum simulation utilizes Monte Carlo\ntechniques extensively. Thus statistics can play an important role in quantum\ncomputation and quantum simulation, which in turn offer great potential to\nrevolutionize computational statistics. While only pseudo-random numbers can be\ngenerated by classical computers, quantum computers are able to produce genuine\nrandom numbers; quantum computers can exponentially or quadratically speed up\nmedian evaluation, Monte Carlo integration and Markov chain simulation. This\npaper gives a brief review on quantum computation, quantum simulation and\nquantum information. We introduce the basic concepts of quantum computation and\nquantum simulation and present quantum algorithms that are known to be much\nfaster than the available classic algorithms. We provide a statistical\nframework for the analysis of quantum algorithms and quantum simulation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 11:47:37 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Wang", "Yazhen", ""]]}, {"id": "1210.0767", "submitter": "Arvid Sj\\\"{o}lander", "authors": "Arvid Sj\\\"olander, Anna L. V. Johansson, Cecilia Lundholm, Daniel\n  Altman, Catarina Almqvist, Yudi Pawitan", "title": "Analysis of 1:1 Matched Cohort Studies and Twin Studies, with Binary\n  Exposures and Binary Outcomes", "comments": "Published in at http://dx.doi.org/10.1214/12-STS390 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 3, 395-411", "doi": "10.1214/12-STS390", "report-no": "IMS-STS-STS390", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve confounder adjustments, observational studies are often matched on\npotential confounders. While matched case-control studies are common and well\ncovered in the literature, our focus here is on matched cohort studies, which\nare less common and sparsely discussed in the literature. Matched data also\narise naturally in twin studies, as a cohort of exposure-discordant twins can\nbe viewed as being matched on a large number of potential confounders. The\nanalysis of twin studies will be given special attention. We give an overview\nof various analysis methods for matched cohort studies with binary exposures\nand binary outcomes. In particular, our aim is to answer the following\nquestions: (1) What are the target parameters in the common analysis methods?\n(2) What are the underlying assumptions in these methods? (3) How do the\nmethods compare in terms of statistical power?\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 13:28:06 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Sj\u00f6lander", "Arvid", ""], ["Johansson", "Anna L. V.", ""], ["Lundholm", "Cecilia", ""], ["Altman", "Daniel", ""], ["Almqvist", "Catarina", ""], ["Pawitan", "Yudi", ""]]}, {"id": "1210.0773", "submitter": "Xiaogang (Steven) Wang", "authors": "Hsiao-Hsuan Wang, Yuehua Wu, Yuejiao Fu and Xiaogang Wang", "title": "Data Fusion Using Robust Empirical Likelihood Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors propose a robust semi-parametric empirical likelihood method to\nintegrate all available information from multiple samples with a common center\nof measurements.\n  Two different sets of estimating equations are used to improve the classical\nlikelihood inference on the measurement center.\n  The proposed method does not require the knowledge of the functional forms of\nthe probability density functions of related populations.\n  The advantages of the proposed method were demonstrated through the extensive\nsimulation studies by comparing mean squared error, coverage probabilities and\naverage length of confidence intervals with those from the classical likelihood\nmethod. Simulation results suggest that our approach provides more informative\nand efficient inference than the conventional maximum likelihood estimator when\ncertain structural relationships exist among the parameters for these relevant\nsamples.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 13:43:03 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Wang", "Hsiao-Hsuan", ""], ["Wu", "Yuehua", ""], ["Fu", "Yuejiao", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1210.0870", "submitter": "Paul von Hippel", "authors": "Paul T. von Hippel, Jonathan Bartlett", "title": "Maximum likelihood multiple imputation: Faster imputations and\n  consistent standard errors without posterior draws", "comments": "38 pages, 7 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation (MI) is a method for repairing and analyzing data with\nmissing values. MI replaces missing values with a sample of random values drawn\nfrom an imputation model. The most popular form of MI, which we call posterior\ndraw multiple imputation (PDMI), draws the parameters of the imputation model\nfrom a Bayesian posterior distribution. An alternative, which we call maximum\nlikelihood multiple imputation (MLMI), estimates the parameters of the\nimputation model using maximum likelihood (or equivalent). Compared to PDMI,\nMLMI is less computationally intensive, faster, and yields slightly more\nefficient point estimates.\n  A past barrier to using MLMI was the difficulty of estimating the standard\nerrors of MLMI point estimates. We derive, implement, and evaluate three\nconsistent standard error formulas: (1) one combines variances within and\nbetween the imputed datasets, (2) one uses the score function, and (3) one uses\nthe bootstrap to estimate variance components due to sampling and imputation.\nFormula (1) modifies for MLMI a formula that has long been used under PDMI,\nwhile formulas (2) and (3) can be used without modification under either PDMI\nor MLMI. We have implemented MLMI and the standard error estimators in the mlmi\nand bootImpute packages for R.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 18:31:58 GMT"}, {"version": "v10", "created": "Thu, 14 Nov 2019 23:35:47 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2012 20:25:24 GMT"}, {"version": "v3", "created": "Fri, 19 Aug 2016 18:09:44 GMT"}, {"version": "v4", "created": "Tue, 31 Jan 2017 21:28:47 GMT"}, {"version": "v5", "created": "Sat, 4 Feb 2017 12:45:45 GMT"}, {"version": "v6", "created": "Thu, 23 Mar 2017 20:26:38 GMT"}, {"version": "v7", "created": "Tue, 9 May 2017 21:45:10 GMT"}, {"version": "v8", "created": "Fri, 5 Jan 2018 19:36:46 GMT"}, {"version": "v9", "created": "Tue, 9 Jan 2018 03:08:24 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["von Hippel", "Paul T.", ""], ["Bartlett", "Jonathan", ""]]}, {"id": "1210.1016", "submitter": "Manuela Cattelan", "authors": "Manuela Cattelan", "title": "Models for Paired Comparison Data: A Review with Emphasis on Dependent\n  Data", "comments": "Published in at http://dx.doi.org/10.1214/12-STS396 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 3, 412-433", "doi": "10.1214/12-STS396", "report-no": "IMS-STS-STS396", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thurstonian and Bradley-Terry models are the most commonly applied models in\nthe analysis of paired comparison data. Since their introduction, numerous\ndevelopments have been proposed in different areas. This paper provides an\nupdated overview of these extensions, including how to account for object- and\nsubject-specific covariates and how to deal with ordinal paired comparison\ndata. Special emphasis is given to models for dependent comparisons. Although\nthese models are more realistic, their use is complicated by numerical\ndifficulties. We therefore concentrate on implementation issues. In particular,\na pairwise likelihood approach is explored for models for dependent paired\ncomparison data, and a simulation study is carried out to compare the\nperformance of maximum pairwise likelihood with other limited information\nestimation methods. The methodology is illustrated throughout using a real data\nset about university paired comparisons performed by students.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 08:09:05 GMT"}], "update_date": "2012-10-04", "authors_parsed": [["Cattelan", "Manuela", ""]]}, {"id": "1210.1031", "submitter": "Dayue Chen", "authors": "Dayue Chen, Ingram Olkin", "title": "Pao-Lu Hsu (Xu, Bao-lu): The Grandparent of Probability and Statistics\n  in China", "comments": "Published in at http://dx.doi.org/10.1214/12-STS387 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 3, 434-445", "doi": "10.1214/12-STS387", "report-no": "IMS-STS-STS387", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The years 1910-1911 are auspicious years in Chinese mathematics with the\nbirths of Pao-Lu Hsu, Luo-Keng Hua and Shiing-Shen Chern. These three began the\ndevelopment of modern mathematics in China: Hsu in probability and statistics,\nHua in number theory, and Chern in differential geometry. We here review some\nfacts about the life of P.-L. Hsu which have been uncovered recently, and then\ndiscuss some of his contributions. We have drawn heavily on three papers in the\n1979 Annals of Statistics (volume 7, pages 467-483) by T. W. Anderson, K. L.\nChung and E. L. Lehmann, as well as an article by Jiang Ze-Han and Duan Xue-Fu\nin Hsu's collected papers.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 08:59:32 GMT"}], "update_date": "2012-10-04", "authors_parsed": [["Chen", "Dayue", ""], ["Olkin", "Ingram", ""]]}, {"id": "1210.1072", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Wenceslao Gonz\\'alez-Manteiga, Gil Gonz\\'alez-Rodr\\'iguez, Adela\n  Mart\\'inez-Calvo, Eduardo Garc\\'ia-Portugu\\'es", "title": "Bootstrap independence test for functional linear models", "comments": "17 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Functional data have been the subject of many research works over the last\nyears. Functional regression is one of the most discussed issues. Specifically,\nsignificant advances have been made for functional linear regression models\nwith scalar response. Let $(\\mathcal{H},<\\cdot,\\cdot>)$ be a separable Hilbert\nspace. We focus on the model $Y=<\\Theta,X>+b+\\varepsilon$, where $Y$ and\n$\\varepsilon$ are real random variables, $X$ is an $\\mathcal{H}$-valued random\nelement, and the model parameters $b$ and $\\Theta$ are in $\\mathbb{R}$ and\n$\\mathcal{H}$, respectively. Furthermore, the error satisfies that\n$E(\\varepsilon|X)=0$ and $E(\\varepsilon^2|X)=\\sigma^2<\\infty$. A consistent\nbootstrap method to calibrate the distribution of statistics for testing $H_0:\n\\Theta=0$ versus $H_1: \\Theta\\neq 0$ is developed. The asymptotic theory, as\nwell as a simulation study and a real data application illustrating the\nusefulness of our proposed bootstrap in practice, is presented.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 11:43:07 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2012 10:31:23 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2014 11:24:05 GMT"}, {"version": "v4", "created": "Sun, 20 Sep 2020 22:40:15 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["Gonz\u00e1lez-Rodr\u00edguez", "Gil", ""], ["Mart\u00ednez-Calvo", "Adela", ""], ["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""]]}, {"id": "1210.1412", "submitter": "Dominik Wied", "authors": "Dominik Wied", "title": "A nonparametric test for a constant correlation matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric procedure to test for changes in correlation\nmatrices at an unknown point in time. The new test requires only mild\nassumptions on the serial dependence structure and has considerable power in\nfinite samples. We derive the asymptotic distribution under the null hypothesis\nof no change as well as local power results and apply the test to stock\nreturns.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2012 12:29:35 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2012 12:53:16 GMT"}, {"version": "v3", "created": "Mon, 3 Dec 2012 13:57:06 GMT"}, {"version": "v4", "created": "Mon, 27 Oct 2014 20:30:36 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Wied", "Dominik", ""]]}, {"id": "1210.1473", "submitter": "Dennis Wei", "authors": "Dennis Wei and Alfred O. Hero III", "title": "Multistage Adaptive Estimation of Sparse Signals", "comments": "15 pages, 8 figures, minor revisions", "journal-ref": null, "doi": "10.1109/JSTSP.2013.2256105", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers sequential adaptive estimation of sparse signals under a\nconstraint on the total sensing effort. The advantage of adaptivity in this\ncontext is the ability to focus more resources on regions of space where signal\ncomponents exist, thereby improving performance. A dynamic programming\nformulation is derived for the allocation of sensing effort to minimize the\nexpected estimation loss. Based on the method of open-loop feedback control,\nallocation policies are then developed for a variety of loss functions. The\npolicies are optimal in the two-stage case, generalizing an optimal two-stage\npolicy proposed by Bashan et al., and improve monotonically thereafter with the\nnumber of stages. Numerical simulations show gains up to several dB as compared\nto recently proposed adaptive methods, and dramatic gains compared to\nnon-adaptive estimation. An application to radar imaging is also presented.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2012 14:46:16 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2013 17:05:53 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Wei", "Dennis", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1210.1766", "submitter": "Jun Zhu", "authors": "Jun Zhu, Ning Chen, and Eric P. Xing", "title": "Bayesian Inference with Posterior Regularization and applications to\n  Infinite Latent SVMs", "comments": "49 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Bayesian models, especially nonparametric Bayesian methods, rely on\nspecially conceived priors to incorporate domain knowledge for discovering\nimproved latent representations. While priors can affect posterior\ndistributions through Bayes' rule, imposing posterior regularization is\narguably more direct and in some cases more natural and general. In this paper,\nwe present regularized Bayesian inference (RegBayes), a novel computational\nframework that performs posterior inference with a regularization term on the\ndesired post-data posterior distribution under an information theoretical\nformulation. RegBayes is more flexible than the procedure that elicits expert\nknowledge via priors, and it covers both directed Bayesian networks and\nundirected Markov networks whose Bayesian formulation results in hybrid chain\ngraph models. When the regularization is induced from a linear operator on the\nposterior distributions, such as the expectation operator, we present a general\nconvex-analysis theorem to characterize the solution of RegBayes. Furthermore,\nwe present two concrete examples of RegBayes, infinite latent support vector\nmachines (iLSVM) and multi-task infinite latent support vector machines\n(MT-iLSVM), which explore the large-margin idea in combination with a\nnonparametric Bayesian model for discovering predictive latent features for\nclassification and multi-task learning, respectively. We present efficient\ninference methods and report empirical studies on several benchmark datasets,\nwhich appear to demonstrate the merits inherited from both large-margin\nlearning and Bayesian nonparametrics. Such results were not available until\nnow, and contribute to push forward the interface between these two important\nsubfields, which have been largely treated as isolated in the community.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 14:10:20 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2013 09:33:44 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2014 06:31:12 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Ning", ""], ["Xing", "Eric P.", ""]]}, {"id": "1210.1871", "submitter": "Robert Kohn", "authors": "Arnaud Doucet (Department of Statistics, University of Oxford),\n  Michael Pitt (Department of Economics, University of Warwick), George\n  Deligiannidis (Department of Statistics, University of Oxford) and Robert\n  Kohn (Department of Economics, University of New South Wales)", "title": "Efficient implementation of Markov chain Monte Carlo when using an\n  unbiased likelihood estimator", "comments": "34 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an unbiased estimator of the likelihood is used within a\nMetropolis--Hastings chain, it is necessary to trade off the number of Monte\nCarlo samples used to construct this estimator against the asymptotic variances\nof averages computed under this chain. Many Monte Carlo samples will typically\nresult in Metropolis--Hastings averages with lower asymptotic variances than\nthe corresponding Metropolis--Hastings averages using fewer samples. However,\nthe computing time required to construct the likelihood estimator increases\nwith the number of Monte Carlo samples. Under the assumption that the\ndistribution of the additive noise introduced by the log-likelihood estimator\nis Gaussian with variance inversely proportional to the number of Monte Carlo\nsamples and independent of the parameter value at which it is evaluated, we\nprovide guidelines on the number of samples to select. We demonstrate our\nresults by considering a stochastic volatility model applied to stock index\nreturns.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 21:18:21 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2012 10:38:07 GMT"}, {"version": "v3", "created": "Mon, 31 Mar 2014 18:30:04 GMT"}, {"version": "v4", "created": "Sat, 13 Sep 2014 08:11:09 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Doucet", "Arnaud", "", "Department of Statistics, University of Oxford"], ["Pitt", "Michael", "", "Department of Economics, University of Warwick"], ["Deligiannidis", "George", "", "Department of Statistics, University of Oxford"], ["Kohn", "Robert", "", "Department of Economics, University of New South Wales"]]}, {"id": "1210.2029", "submitter": "Georgios Fellouris Dr.", "authors": "Georgios Fellouris and George V. Moustakides", "title": "Bandwidth and Energy Efficient Decentralized Sequential Change Detection", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of decentralized sequential change detection is considered, where\nan abrupt change occurs in an area monitored by a number of sensors; the\nsensors transmit their data to a fusion center, subject to bandwidth and energy\nconstraints, and the fusion center is responsible for detecting the change as\nsoon as possible. A novel sequential detection rule is proposed that requires\ncommunication from the sensors at random times and transmission of only low-bit\nmessages, on which the fusion center runs in parallel a CUSUM test. The\nsecond-order asymptotic optimality of the proposed scheme is established both\nin discrete and in continuous time. Specifically, it is shown that the\ninflicted performance loss (with respect to the optimal detection rule that\nuses the complete sensor observations) is asymptotically bounded as the rate of\nfalse alarms goes to 0, for any fixed rate of communication. When the rate of\ncommunication from the sensors is asymptotically low, the proposed scheme\nremains first-order asymptotically optimal. Finally, simulation experiments\nillustrate its efficiency and its superiority over a decentralized detection\nrule that relies on communication at deterministic times.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2012 06:48:44 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2013 02:18:34 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Fellouris", "Georgios", ""], ["Moustakides", "George V.", ""]]}, {"id": "1210.2211", "submitter": "Raul Mondrag\\'on Dr", "authors": "R. J. Mondragon", "title": "Network Null Model based on Maximal Entropy and the Rich-Club", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to construct a network null-model based on the maximum\nentropy principle and where the restrictions that the rich-club and the degree\nsequence impose are conserved. We show that the probability that two nodes\nshare a link can be described with a simple probability function. The\nnull-model closely approximates the assortative properties of the network.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 10:07:33 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2014 17:47:20 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Mondragon", "R. J.", ""]]}, {"id": "1210.2294", "submitter": "Ccsd", "authors": "Guillaume Allain (MM), Fabrice Gamboa (IMT), Philippe Goudal (MM),\n  Jean-No\\\"el Kien (MM, IMT), Jean-Michel Loubes (IMT)", "title": "Modeling Weather Conditions Consequences on Road Trafficking Behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a model to understand how adverse weather conditions modify\ntraffic flow dynamic. We first prove that the microscopic Free Flow Speed of\nthe vehicles is changed and then provide a rule to model this change. For this,\nwe consider a thresholded linear model, corresponding to an application of a\nMARS model to road trafficking. This model adapts itself locally to the whole\nroad network and provides accurate unbiased forecasted speed using live or\nshort term forecasted weather data information.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 14:25:16 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Allain", "Guillaume", "", "MM"], ["Gamboa", "Fabrice", "", "IMT"], ["Goudal", "Philippe", "", "MM"], ["Kien", "Jean-No\u00ebl", "", "MM, IMT"], ["Loubes", "Jean-Michel", "", "IMT"]]}, {"id": "1210.2354", "submitter": "Jo\\~ao Strapasson E.", "authors": "Sueli I. R. Costa and Sandra A. Santos and Jo\\~ao E. Strapasson", "title": "Fisher information distance: a geometrical reading", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math-ph math.IT math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a strongly geometrical approach to the Fisher distance, which\nis a measure of dissimilarity between two probability distribution functions.\nThe Fisher distance, as well as other divergence measures, are also used in\nmany applications to establish a proper data average. The main purpose is to\nwiden the range of possible interpretations and relations of the Fisher\ndistance and its associated geometry for the prospective applications. It\nfocuses on statistical models of the normal probability distribution functions\nand takes advantage of the connection with the classical hyperbolic geometry to\nderive closed forms for the Fisher distance in several cases. Connections with\nthe well-known Kullback-Leibler divergence measure are also devised.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 17:34:17 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2013 17:18:24 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2014 17:55:58 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Costa", "Sueli I. R.", ""], ["Santos", "Sandra A.", ""], ["Strapasson", "Jo\u00e3o E.", ""]]}, {"id": "1210.2464", "submitter": "Ashin Mukherjee", "authors": "Ashin Mukherjee, Kun Chen, Naisyin Wang and Ji Zhu", "title": "On The Degrees of Freedom of Reduced-rank Estimators in Multivariate\n  Regression", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the effective degrees of freedom of a general class of\nreduced rank estimators for multivariate regression in the framework of Stein's\nunbiased risk estimation (SURE). We derive a finite-sample exact unbiased\nestimator that admits a closed-form expression in terms of the singular values\nor thresholded singular values of the least squares solution and hence readily\ncomputable. The results continue to hold in the high-dimensional scenario when\nboth the predictor and response dimensions are allowed to be larger than the\nsample size. The derived analytical form facilitates the investigation of its\ntheoretical properties and provides new insights into the empirical behaviors\nof the degrees of freedom. In particular, we examine the differences and\nconnections between the proposed estimator and a commonly-used naive estimator,\ni.e., the number of free parameters. The use of the proposed estimator leads to\nefficient and accurate prediction risk estimation and model selection, as\ndemonstrated by simulation studies and a data example.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 01:12:50 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2013 23:34:12 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Mukherjee", "Ashin", ""], ["Chen", "Kun", ""], ["Wang", "Naisyin", ""], ["Zhu", "Ji", ""]]}, {"id": "1210.2503", "submitter": "Antti Honkela", "authors": "Hande Topa and Antti Honkela", "title": "Gaussian process modelling of multiple short time series", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present techniques for effective Gaussian process (GP) modelling of\nmultiple short time series. These problems are common when applying GP models\nindependently to each gene in a gene expression time series data set. Such sets\ntypically contain very few time points. Naive application of common GP\nmodelling techniques can lead to severe over-fitting or under-fitting in a\nsignificant fraction of the fitted models, depending on the details of the data\nset. We propose avoiding over-fitting by constraining the GP length-scale to\nvalues that focus most of the energy spectrum to frequencies below the Nyquist\nfrequency corresponding to the sampling frequency in the data set.\nUnder-fitting can be avoided by more informative priors on observation noise.\nCombining these methods allows applying GP methods reliably automatically to\nlarge numbers of independent instances of short time series. This is\nillustrated with experiments with both synthetic data and real gene expression\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 06:36:09 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Topa", "Hande", ""], ["Honkela", "Antti", ""]]}, {"id": "1210.2575", "submitter": "Eero Liski", "authors": "Eero Liski, Klaus Nordhausen, Hannu Oja, Anne Ruiz-Gazen", "title": "Averaging orthogonal projectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality is a major concern in analyzing large data sets. Some well\nknown dimension reduction methods are for example principal component analysis\n(PCA), invariant coordinate selection (ICS), sliced inverse regression (SIR),\nsliced average variance estimate (SAVE), principal hessian directions (PHD) and\ninverse regression estimator (IRE). However, these methods are usually adequate\nof finding only certain types of structures or dependencies within the data.\nThis calls the need to combine information coming from several different\ndimension reduction methods. We propose a generalization of the Crone and\nCrosby distance, a weighted distance that allows to combine subspaces of\ndifferent dimensions. Some natural choices of weights are considered in detail.\nBased on the weighted distance metric we discuss the concept of averages of\nsubspaces as well to combine various dimension reduction methods. The\nperformance of the weighted distances and the combining approach is illustrated\nvia simulations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 12:04:02 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Liski", "Eero", ""], ["Nordhausen", "Klaus", ""], ["Oja", "Hannu", ""], ["Ruiz-Gazen", "Anne", ""]]}, {"id": "1210.2601", "submitter": "R\\'{e}mi Bardenet", "authors": "R\\'emi Bardenet, Olivier Capp\\'e, Gersende Fort, Bal\\'azs K\\'egl", "title": "Adaptive MCMC with online relabeling", "comments": "Published at http://dx.doi.org/10.3150/13-BEJ578 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 3, 1304-1340", "doi": "10.3150/13-BEJ578", "report-no": "IMS-BEJ-BEJ578", "categories": "stat.CO math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When targeting a distribution that is artificially invariant under some\npermutations, Markov chain Monte Carlo (MCMC) algorithms face the\nlabel-switching problem, rendering marginal inference particularly cumbersome.\nSuch a situation arises, for example, in the Bayesian analysis of finite\nmixture models. Adaptive MCMC algorithms such as adaptive Metropolis (AM),\nwhich self-calibrates its proposal distribution using an online estimate of the\ncovariance matrix of the target, are no exception. To address the\nlabel-switching issue, relabeling algorithms associate a permutation to each\nMCMC sample, trying to obtain reasonable marginals. In the case of adaptive\nMetropolis (Bernoulli 7 (2001) 223-242), an online relabeling strategy is\nrequired. This paper is devoted to the AMOR algorithm, a provably consistent\nvariant of AM that can cope with the label-switching problem. The idea is to\nnest relabeling steps within the MCMC algorithm based on the estimation of a\nsingle covariance matrix that is used both for adapting the covariance of the\nproposal distribution in the Metropolis algorithm step and for online\nrelabeling. We compare the behavior of AMOR to similar relabeling methods. In\nthe case of compactly supported target distributions, we prove a strong law of\nlarge numbers for AMOR and its ergodicity. These are the first results on the\nconsistency of an online relabeling algorithm to our knowledge. The proof\nunderlines latent relations between relabeling and vector quantization.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 14:05:38 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2013 15:17:07 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2015 11:54:09 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["Bardenet", "R\u00e9mi", ""], ["Capp\u00e9", "Olivier", ""], ["Fort", "Gersende", ""], ["K\u00e9gl", "Bal\u00e1zs", ""]]}, {"id": "1210.2667", "submitter": "Kyle Vincent Ph. D", "authors": "Kyle Vincent, Steve Thompson", "title": "Estimating Population Size with Link-Tracing Sampling", "comments": "23 pages, 2 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new design and inference method for estimating population size\nof a hidden population best reached through a link-tracing design. The strategy\ninvolves the Rao-Blackwell Theorem applied to a sufficient statistic markedly\ndifferent from the usual one that arises in sampling from a finite population.\nAn empirical application is described. The result demonstrates that the\nstrategy can efficiently incorporate adaptively selected members of the sample\ninto the inference procedure.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 17:06:12 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2012 17:33:13 GMT"}, {"version": "v3", "created": "Wed, 24 Oct 2012 00:39:21 GMT"}, {"version": "v4", "created": "Thu, 10 Oct 2013 13:07:12 GMT"}, {"version": "v5", "created": "Mon, 17 Feb 2014 13:02:17 GMT"}, {"version": "v6", "created": "Tue, 25 Nov 2014 17:49:11 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Vincent", "Kyle", ""], ["Thompson", "Steve", ""]]}, {"id": "1210.2953", "submitter": "Saikat Mukherjee", "authors": "Saikat Mukherjee, Farhad Jafari, Jong-Min Kim", "title": "Characterization of Differentiable Copulas", "comments": "12 pages, 5 figures, Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new class of copulas which characterize the set of all\ntwice continuously differentiable copulas. We show that our proposed new class\nof copulas is a new generalized copula family that include not only asymmetric\ncopulas but also all smooth copula families available in the current\nliterature. Spearman's rho and Kendall's tau for our new Fourier copulas which\nare asymmetric are introduced. Furthermore, an approximation method is\ndiscussed in order to optimize Spearman's rho and the corresponding Kendall's\ntau.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 20:39:48 GMT"}], "update_date": "2012-10-11", "authors_parsed": [["Mukherjee", "Saikat", ""], ["Jafari", "Farhad", ""], ["Kim", "Jong-Min", ""]]}, {"id": "1210.2954", "submitter": "Rajesh  Singh", "authors": "Jayant Singh, Housila P. Singh, Rajesh Singh", "title": "Unbiased Ratio-Type Estimator Using Transformed Auxiliary Variable In\n  Negative Correlation Case", "comments": "6 pages", "journal-ref": "Jour. Raj. Stat. Assoc. 1(1), 1-8 (2012)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to propose an unbiased ratio-type estimator\nfor finite population mean when the variables are negatively correlated.\nHartley and Ross[2] and Singh and Singh [6] estimators are identified as\nparticular cases of the proposed unbiased estimator. The variance expression of\nthe proposed estimator to the first degree of approximation has been obtained.\nAn empirical study is carried out to demonstrate the performance of the\nproposed estimator over, Robson [5] estimator and Singh and Singh [6]\nestimator.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2012 05:56:30 GMT"}], "update_date": "2012-10-11", "authors_parsed": [["Singh", "Jayant", ""], ["Singh", "Housila P.", ""], ["Singh", "Rajesh", ""]]}, {"id": "1210.3023", "submitter": "Rajesh  Singh", "authors": "Rajesh Singh, Mukesh Kumar, P. Chauhan, N. Sawan, S. Florentin", "title": "A general family of dual to ratio-cum-product estimator in sample\n  surveys", "comments": "I would like to with draw this paper because there are serious\n  mistake in results", "journal-ref": "SIT_newseries, 12(3), 587-594 (2011)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a family of dual to ratio-cum-product estimators for the\nfinite population mean. Under simple random sampling without replacement\n(SRSWOR) scheme, expressions of the bias and mean-squared error (MSE) up to the\nfirst order of approximation are derived. We show that the proposed family is\nmore efficient than usual unbiased estimator, ratio estimator, product\nestimator, Singh estimator (1967), Srivenkataramana (1980) and Bandyopadhyaya\nestimator (1980) and Singh et.al. (2005) estimator. An empirical study is\ncarried out to illustrate the performance of the constructed estimator over\nothers.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2012 11:46:33 GMT"}, {"version": "v2", "created": "Tue, 28 May 2013 18:21:12 GMT"}, {"version": "v3", "created": "Fri, 31 May 2013 07:29:46 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Singh", "Rajesh", ""], ["Kumar", "Mukesh", ""], ["Chauhan", "P.", ""], ["Sawan", "N.", ""], ["Florentin", "S.", ""]]}, {"id": "1210.3087", "submitter": "Grace Chiu", "authors": "Shahedul A Khan, Grace S Chiu, Joel A Dubin", "title": "Therapeutic hypothermia: quantification of the transition of core body\n  temperature using the flexible mixture bent-cable model for longitudinal data", "comments": "On 2013-04-14, a revised version of this manuscript was accepted for\n  publication in the Australian and New Zealand Journal of Statistics", "journal-ref": null, "doi": "10.1111/anzs.12047", "report-no": null, "categories": "stat.ME q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By reducing core body temperature, T_c, induced hypothermia is a therapeutic\ntool to prevent brain damage resulting from physical trauma. However, all\nphysiological systems begin to slow down due to hypothermia that in turn can\nresult in increased risk of mortality. Therefore, quantification of the\ntransition of T_c to early hypothermia is of great clinical interest.\nConceptually, T_c may exhibit an either gradual or abrupt transition.\nBent-cable regression is an appealing statistical tool to model such data due\nto the model's flexibility and greatly interpretable regression coefficients.\nIt handles more flexibly models that traditionally have been handled by\nlow-order polynomial models (for gradual transition) or piecewise linear\nchangepoint models (for abrupt change). We consider a rat model for humans to\nquantify the temporal trend of T_c to primarily address the question: What is\nthe critical time point associated with a breakdown in the compensatory\nmechanisms following the start of hypothermia therapy? To this end, we develop\na Bayesian modelling framework for bent-cable regression of longitudinal data\nto simultaneously account for gradual and abrupt transitions. Our analysis\nreveals that: (a) about 39% of rats exhibit a gradual transition in T_c; (b)\nthe critical time point is approximately the same regardless of transition\ntype; (c) both transition types show a significant increase of T_c followed by\na significant decrease.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2012 23:21:31 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2013 01:50:22 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Khan", "Shahedul A", ""], ["Chiu", "Grace S", ""], ["Dubin", "Joel A", ""]]}, {"id": "1210.3190", "submitter": "Gane Samb Lo", "authors": "Pape Djiby Mergane, Gane Samb LO", "title": "On the influence of the Theil-like inequality measure on the growth", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We set in this paper a coherent theory based on functional empirical\nprocesses to consider both the poverty and the inequality indices in one\nGaussian field enabling to study the influence of the one on the other. We use\nthe General Poverty Index (\\textit{GPI}), that is a class of poverty indices\ncovering the most common ones and a functional class of inequality measure\nincluding the Entropy Measure, the Mean Logarithmic Deviation, the different\ninequality measures of Atkinson, Champernowne, Kolm and Theil called Theil-like\nInequality Measures \\textit{TLIM}. Our results are given in a unified approach\nwith respect to the two classes instead of their particular elements. We\nprovide the asymptotic laws of the variations of each class over two given\nperiods and the ratio of the variation and derive confidence intervals for\nthem. Although the variances may seem somehow complicated, we provide R codes\nfor their computations and apply the results for the pseudo-panel data for\nSenegal with simple analysis.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 11:49:01 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Mergane", "Pape Djiby", ""], ["LO", "Gane Samb", ""]]}, {"id": "1210.3214", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Rosa M. Crujeiras, Wenceslao\n  Gonz\\'alez-Manteiga", "title": "Kernel density estimation for directional-linear data", "comments": "34 pages, 4 figures", "journal-ref": "Journal of Multivariate Analysis, 121:152-175, 2013", "doi": "10.1016/j.jmva.2013.06.009", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A nonparametric kernel density estimator for directional-linear data is\nintroduced. The proposal is based on a product kernel accounting for the\ndifferent nature of both (directional and linear) components of the random\nvector. Expressions for bias, variance and Mean Integrated Squared Error (MISE)\nare derived, jointly with an asymptotic normality result for the proposed\nestimator. For some particular distributions, an explicit formula for the MISE\nis obtained and compared with its asymptotic version, both for directional and\ndirectional-linear kernel density estimators. In this same setting a closed\nexpression for the bootstrap MISE is also derived.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 12:56:21 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2013 16:02:33 GMT"}, {"version": "v3", "created": "Fri, 14 Jun 2013 09:37:36 GMT"}, {"version": "v4", "created": "Wed, 26 Feb 2014 10:48:15 GMT"}, {"version": "v5", "created": "Sat, 27 Sep 2014 21:26:30 GMT"}, {"version": "v6", "created": "Sun, 20 Sep 2020 23:06:38 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Crujeiras", "Rosa M.", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""]]}, {"id": "1210.3313", "submitter": "John Storey", "authors": "Troels T. Marstrand, John D. Storey", "title": "Identifying and Mapping Cell-type Specific Chromatin Programming of Gene\n  Expression", "comments": "First version completed December 2010. Last modified August 2011. We\n  remain in the submission and publication process, so the content of this\n  manuscript may change in the future. With the recent publication of 30 ENCODE\n  papers, we would like to share our related work with the research community.\n  The Supplementary Information may be found among the source files,\n  specifically in arxiv_SI.pdf", "journal-ref": "Proceedings of the National Academy of Sciences (2014), 111(6),\n  E645-E654", "doi": "10.1073/pnas.1312523111", "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem of substantial interest is to systematically map variation in\nchromatin structure to gene expression regulation across conditions,\nenvironments, or differentiated cell types. We developed and applied a\nquantitative framework for determining the existence, strength, and type of\nrelationship between high-resolution chromatin structure in terms of DNaseI\nhypersensitivity (DHS) and genome-wide gene expression levels in 20 diverse\nhuman cell lines. We show that ~25% of genes show cell-type specific expression\nexplained by alterations in chromatin structure. We find that distal regions of\nchromatin structure (e.g., +/- 200kb) capture more genes with this relationship\nthan local regions (e.g., +/- 2.5kb), yet the local regions show a more\npronounced effect. By exploiting variation across cell-types, we were capable\nof pinpointing the most likely hypersensitive sites related to cell-type\nspecific expression, which we show have a range of contextual usages. This\nquantitative framework is likely applicable to other settings aimed at relating\ncontinuous genomic measurements to gene expression variation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 18:25:26 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Marstrand", "Troels T.", ""], ["Storey", "John D.", ""]]}, {"id": "1210.3405", "submitter": "Scott Sisson", "authors": "P. Menendez, Y. Fan, P. H. Garthwaite, S. A. Sisson", "title": "Simultaneous adjustment of bias and coverage probabilities for\n  confidence intervals", "comments": "To appear in Computational Statistics and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed for the correction of confidence intervals when the\noriginal interval does not have the correct nominal coverage probabilities in\nthe frequentist sense. The proposed method is general and does not require any\ndistributional assumptions. It can be applied to both frequentist and Bayesian\ninference where interval estimates are desired. We provide theoretical results\nfor the consistency of the proposed estimator, and give two complex examples,\non confidence interval correction for composite likelihood estimators and in\napproximate Bayesian computation (ABC), to demonstrate the wide applicability\nof the new method. Comparison is made with the double-bootstrap and other\nmethods of improving confidence interval coverage.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2012 00:48:34 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2013 03:40:58 GMT"}], "update_date": "2013-08-30", "authors_parsed": [["Menendez", "P.", ""], ["Fan", "Y.", ""], ["Garthwaite", "P. H.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1210.3420", "submitter": "A.C. Thomas", "authors": "Bin Zhang, A. C. Thomas, Patrick Doreian, David Krackhardt, Ramayya\n  Krishnan", "title": "Contrasting Multiple Social Network Autocorrelations for Binary\n  Outcomes, With Applications To Technology Adoption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of socially targeted marketing suggests that decisions made by\nconsumers can be predicted not only from their personal tastes and\ncharacteristics, but also from the decisions of people who are close to them in\ntheir networks. One obstacle to consider is that there may be several different\nmeasures for \"closeness\" that are appropriate, either through different types\nof friendships, or different functions of distance on one kind of friendship,\nwhere only a subset of these networks may actually be relevant. Another is that\nthese decisions are often binary and more difficult to model with conventional\napproaches, both conceptually and computationally. To address these issues, we\npresent a hierarchical model for individual binary outcomes that uses and\nextends the machinery of the auto-probit method for binary data. We demonstrate\nthe behavior of the parameters estimated by the multiple network-regime\nauto-probit model (m-NAP) under various sensitivity conditions, such as the\nimpact of the prior distribution and the nature of the structure of the\nnetwork, and demonstrate on several examples of correlated binary data in\nnetworks of interest to Information Systems, including the adoption of Caller\nRing-Back Tones, whose use is governed by direct connection but explained by\nadditional network topologies.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2012 02:49:16 GMT"}], "update_date": "2012-10-15", "authors_parsed": [["Zhang", "Bin", ""], ["Thomas", "A. C.", ""], ["Doreian", "Patrick", ""], ["Krackhardt", "David", ""], ["Krishnan", "Ramayya", ""]]}, {"id": "1210.3711", "submitter": "Sumanta Basu", "authors": "Sumanta Basu, Ali Shojaie and George Michailidis", "title": "Network Granger Causality with Inherent Grouping Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating high-dimensional network models arises naturally in\nthe analysis of many physical, biological and socio-economic systems. Examples\ninclude stock price fluctuations in financial markets and gene regulatory\nnetworks representing effects of regulators (transcription factors) on\nregulated genes in genetics. We aim to learn the structure of the network over\ntime employing the framework of Granger causal models under the assumptions of\nsparsity of its edges and inherent grouping structure among its nodes. We\nintroduce a thresholded variant of the Group Lasso estimator for discovering\nGranger causal interactions among the nodes of the network. Asymptotic results\non the consistency of the new estimation procedure are developed. The\nperformance of the proposed methodology is assessed through an extensive set of\nsimulation studies and comparisons with existing techniques.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2012 14:46:02 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2012 04:57:21 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2013 00:52:59 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Basu", "Sumanta", ""], ["Shojaie", "Ali", ""], ["Michailidis", "George", ""]]}, {"id": "1210.3739", "submitter": "Giles Hooker", "authors": "Giles Hooker, Kevin K. Lin, Bruce Rogers", "title": "Control Theory and Experimental Design in Diffusion Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of designing time-dependent, real-time\ncontrol policies for controllable nonlinear diffusion processes, with the goal\nof obtaining maximally-informative observations about parameters of interest.\nMore precisely, we maximize the expected Fisher information for the parameter\nobtained over the duration of the experiment, conditional on observations made\nup to that time. We propose to accomplish this with a two-step strategy: when\nthe full state vector of the diffusion process is observable continuously, we\nformulate this as an optimal control problem and apply numerical techniques\nfrom stochastic optimal control to solve it. When observations are incomplete,\ninfrequent, or noisy, we propose using standard filtering techniques to first\nestimate the state of the system, then apply the optimal control policy using\nthe posterior expectation of the state. We assess the effectiveness of these\nmethods in 3 situations: a paradigmatic bistable model from statistical\nphysics, a model of action potential generation in neurons, and a model of a\nsimple ecological system.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2012 20:32:45 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2013 01:14:20 GMT"}, {"version": "v3", "created": "Wed, 15 Oct 2014 16:25:47 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Hooker", "Giles", ""], ["Lin", "Kevin K.", ""], ["Rogers", "Bruce", ""]]}, {"id": "1210.3831", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Graphical Modelling in Genetics and Systems Biology", "comments": "Workshop on Foundations of Biomedical Knowledge Representation,\n  October 29 - November 2, 2012 Leiden, The Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical modelling has a long history in statistics as a tool for the\nanalysis of multivariate data, starting from Wright's path analysis and Gibbs'\napplications to statistical physics at the beginning of the last century. In\nits modern form, it was pioneered by Lauritzen and Wermuth and Pearl in the\n1980s, and has since found applications in fields as diverse as bioinformatics,\ncustomer satisfaction surveys and weather forecasts.\n  Genetics and systems biology are unique among these fields in the dimension\nof the data sets they study, which often contain several hundreds of variables\nand only a few tens or hundreds of observations. This raises problems in both\ncomputational complexity and the statistical significance of the resulting\nnetworks, collectively known as the \"curse of dimensionality\". Furthermore, the\ndata themselves are difficult to model correctly due to the limited\nunderstanding of the underlying mechanisms. In the following, we will\nillustrate how such challenges affect practical graphical modelling and some\npossible solutions.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2012 20:15:08 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2014 17:40:14 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1210.3931", "submitter": "Rajesh  Singh", "authors": "Rajesh Singh and Mukesh Kumar", "title": "A note on transformations on auxiliary variable in survey sampling", "comments": "4 pages", "journal-ref": "MASA, 6:1, 17-19 (2011)", "doi": "10.3233/MAS-2011-0154", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we address the doubts of Singh (2001) and Gupta and Shabbir\n(2008) on the transformations of auxiliary variables by adding unit free\nconstants. The original contribution by Sisodia and Dwivedi (1981) is correct.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 07:59:58 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Singh", "Rajesh", ""], ["Kumar", "Mukesh", ""]]}, {"id": "1210.4052", "submitter": "Saralees Nadarajah", "authors": "C. S. Withers, S. Nadarajah", "title": "Expansions about the gamma for the distribution and quantiles of a\n  standard estimate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give expansions for the distribution, density, and quantiles of an\nestimate, building on results of Cornish, Fisher, Hill, Davis and the authors.\nThe estimate is assumed to be non-lattice with the standard expansions for its\ncumulants. By expanding about a skew variable with matched skewness, one can\ndrastically reduce the number of terms needed for a given level of accuracy.\nThe building blocks generalize the Hermite polynomials. We demonstrate with\nexpansions about the gamma.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 14:44:56 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Withers", "C. S.", ""], ["Nadarajah", "S.", ""]]}, {"id": "1210.4154", "submitter": "Alejandro Frery", "authors": "Alejandro C. Frery, Renato J. Cintra and Abra\\~ao D. C. Nascimento", "title": "Entropy-based Statistical Analysis of PolSAR Data", "comments": "Accepted for publication on IEEE Transactions on Geoscience and\n  Remote Sensing", "journal-ref": null, "doi": "10.1109/TGRS.2012.2222029", "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images obtained from coherent illumination processes are contaminated with\nspeckle noise, with polarimetric synthetic aperture radar (PolSAR) imagery as a\nprominent example. With an adequacy widely attested in the literature, the\nscaled complex Wishart distribution is an acceptable model for PolSAR data. In\nthis perspective, we derive analytic expressions for the Shannon, R\\'enyi, and\nrestricted Tsallis entropies under this model. Relationships between the\nderived measures and the parameters of the scaled Wishart law (i.e., the\nequivalent number of looks and the covariance matrix) are discussed. In\naddition, we obtain the asymptotic variances of the Shannon and R\\'enyi\nentropies when replacing distribution parameters by maximum likelihood\nestimators. As a consequence, confidence intervals based on these two entropies\nare also derived and proposed as new ways of capturing contrast. New hypothesis\ntests are additionally proposed using these results, and their performance is\nassessed using simulated and real data. In general terms, the test based on the\nShannon entropy outperforms those based on R\\'enyi's.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 20:39:26 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Frery", "Alejandro C.", ""], ["Cintra", "Renato J.", ""], ["Nascimento", "Abra\u00e3o D. C.", ""]]}, {"id": "1210.4377", "submitter": "Patrick J. Wolfe", "authors": "Sofia C. Olhede, Patrick J. Wolfe", "title": "Order statistics of observed network degrees", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses the properties of extremes of degree sequences\ncalculated from network data. We introduce the notion of a normalized degree,\nin order to permit a comparison of degree sequences between networks with\ndiffering numbers of nodes. We model each normalized degree as a bounded\ncontinuous random variable, and determine the properties of the ordered\nk-maxima and minima of the normalized network degrees when they comprise a\nrandom sample from a Beta distribution. In this setting, their means and\nvariances take a simplified form given by their ordering, and we discuss the\nrelation of these quantities to other prescribed decays such as power laws. We\nverify the derived properties from simulated sets of normalized degrees, and\ndiscuss possible extensions to more flexible classes of distributions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 11:58:47 GMT"}], "update_date": "2012-10-17", "authors_parsed": [["Olhede", "Sofia C.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "1210.4524", "submitter": "Pulastya Bandyopadhyay", "authors": "B. N. Pandey and Pulastya Bandyopadhyay", "title": "Bayesian Estimation of Inverse Gaussian Distribution", "comments": "17 pages, 2 figures, submitted to the Journal of Statistical\n  Computation and Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider Bayesian estimation for the parameters of inverse\nGaussian distribution. Our emphasis is on Markov Chain Monte Carlo methods. We\nprovide complete implementation of the Gibbs sampler algorithm. Assuming an\ninformative prior, Bayes estimates are computed using the output of the Gibbs\nsampler and also from Lindley's approximation method. Maximum Likelihood and\nUniformly Minimum Variance Unbiased estimates are obtained as well. We also\ncompute Highest Posterior Density credible intervals, exact confidence\nintervals as well as \"percentile\" and \"percentile-t\" bootstrap approximations\nto the exact intervals. A simulation study was conducted to compare the\nlong-run performance of the various point and interval estimation methods\nconsidered. One real data illustration has been provided which brings out some\nsalient features of sampling-based approach to inference.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 18:47:39 GMT"}], "update_date": "2012-10-17", "authors_parsed": [["Pandey", "B. N.", ""], ["Bandyopadhyay", "Pulastya", ""]]}, {"id": "1210.4584", "submitter": "Nicolas Stadler N.S.", "authors": "Nicolas St\\\"adler and Sach Mukherjee", "title": "Two-Sample Testing in High-Dimensional Models", "comments": "28 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose novel methodology for testing equality of model parameters between\ntwo high-dimensional populations. The technique is very general and applicable\nto a wide range of models. The method is based on sample splitting: the data is\nsplit into two parts; on the first part we reduce the dimensionality of the\nmodel to a manageable size; on the second part we perform significance testing\n(p-value calculation) based on a restricted likelihood ratio statistic.\nAssuming that both populations arise from the same distribution, we show that\nthe restricted likelihood ratio statistic is asymptotically distributed as a\nweighted sum of chi-squares with weights which can be efficiently estimated\nfrom the data. In high-dimensional problems, a single data split can result in\na \"p-value lottery\". To ameliorate this effect, we iterate the splitting\nprocess and aggregate the resulting p-values. This multi-split approach\nprovides improved p-values. We illustrate the use of our general approach in\ntwo-sample comparisons of high-dimensional regression models (\"differential\nregression\") and graphical models (\"differential network\"). In both cases we\nshow results on simulated data as well as real data from recent,\nhigh-throughput cancer studies.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 22:05:33 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2013 17:53:54 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["St\u00e4dler", "Nicolas", ""], ["Mukherjee", "Sach", ""]]}, {"id": "1210.4630", "submitter": "Eben Kenah", "authors": "Eben Kenah", "title": "Semiparametric Relative-risk Regression for Infectious Disease Data", "comments": "38 pages, 5 figures", "journal-ref": "Journal of the American Statistical Association 110(509): 313-325\n  (2015)", "doi": "10.1080/01621459.2014.896807", "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces semiparametric relative-risk regression models for\ninfectious disease data based on contact intervals, where the contact interval\nfrom person i to person j is the time between the onset of infectiousness in i\nand infectious contact from i to j. The hazard of infectious contact from i to\nj is \\lambda_0(\\tau)r(\\beta_0^T X_{ij}), where \\lambda_0(\\tau) is an\nunspecified baseline hazard function, r is a relative risk function, \\beta_0 is\nan unknown covariate vector, and X_{ij} is a covariate vector. When\nwho-infects-whom is observed, the Cox partial likelihood is a profile\nlikelihood for \\beta maximized over all possible \\lambda_0(\\tau). When\nwho-infects-whom is not observed, we use an EM algorithm to maximize the\nprofile likelihood for \\beta integrated over all possible combinations of\nwho-infected-whom. This extends the most important class of regression models\nin survival analysis to infectious disease epidemiology.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 05:28:12 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Kenah", "Eben", ""]]}, {"id": "1210.4842", "submitter": "Elias Bareinboim", "authors": "Elias Bareinboim, Judea Pearl", "title": "Causal Inference by Surrogate Experiments: z-Identifiability", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-113-120", "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating the effect of intervening on a set of\nvariables X from experiments on a different set, Z, that is more accessible to\nmanipulation. This problem, which we call z-identifiability, reduces to\nordinary identifiability when Z = empty and, like the latter, can be given\nsyntactic characterization using the do-calculus [Pearl, 1995; 2000]. We\nprovide a graphical necessary and sufficient condition for z-identifiability\nfor arbitrary sets X,Z, and Y (the outcomes). We further develop a complete\nalgorithm for computing the causal effect of X on Y using information provided\nby experiments on Z. Finally, we use our results to prove completeness of\ndo-calculus relative to z-identifiability, a result that does not follow from\ncompleteness relative to ordinary identifiability.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:32:47 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Bareinboim", "Elias", ""], ["Pearl", "Judea", ""]]}, {"id": "1210.4844", "submitter": "Cedric Archambeau", "authors": "Cedric Archambeau, Francois Caron", "title": "Plackett-Luce regression: A new Bayesian model for polychotomous data", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-84-92", "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multinomial logistic regression is one of the most popular models for\nmodelling the effect of explanatory variables on a subject choice between a set\nof specified options. This model has found numerous applications in machine\nlearning, psychology or economy. Bayesian inference in this model is non\ntrivial and requires, either to resort to a MetropolisHastings algorithm, or\nrejection sampling within a Gibbs sampler. In this paper, we propose an\nalternative model to multinomial logistic regression. The model builds on the\nPlackett-Luce model, a popular model for multiple comparisons. We show that the\nintroduction of a suitable set of auxiliary variables leads to an\nExpectation-Maximization algorithm to find Maximum A Posteriori estimates of\nthe parameters. We further provide a full Bayesian treatment by deriving a\nGibbs sampler, which only requires to sample from highly standard\ndistributions. We also propose a variational approximate inference scheme. All\nare very simple to implement. One property of our Plackett-Luce regression\nmodel is that it learns a sparse set of feature weights. We compare our method\nto sparse Bayesian multinomial logistic regression and show that it is\ncompetitive, especially in presence of polychotomous data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:34:18 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Archambeau", "Cedric", ""], ["Caron", "Francois", ""]]}, {"id": "1210.4852", "submitter": "Judea Pearl", "authors": "Judea Pearl", "title": "The Do-Calculus Revisited", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-3-11", "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The do-calculus was developed in 1995 to facilitate the identification of\ncausal effects in non-parametric models. The completeness proofs of [Huang and\nValtorta, 2006] and [Shpitser and Pearl, 2006] and the graphical criteria of\n[Tian and Shpitser, 2010] have laid this identification problem to rest. Recent\nexplorations unveil the usefulness of the do-calculus in three additional\nareas: mediation analysis [Pearl, 2012], transportability [Pearl and\nBareinboim, 2011] and metasynthesis. Meta-synthesis (freshly coined) is the\ntask of fusing empirical results from several diverse studies, conducted on\nheterogeneous populations and under different conditions, so as to synthesize\nan estimate of a causal relation in some target environment, potentially\ndifferent from those under study. The talk surveys these results with emphasis\non the challenges posed by meta-synthesis. For background material, see\nhttp://bayes.cs.ucla.edu/csl_papers.html\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:36:07 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Pearl", "Judea", ""]]}, {"id": "1210.4866", "submitter": "Tom Claassen", "authors": "Tom Claassen, Tom Heskes", "title": "A Bayesian Approach to Constraint Based Causal Inference", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-207-216", "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We target the problem of accuracy and robustness in causal inference from\nfinite data sets. Some state-of-the-art algorithms produce clear output\ncomplete with solid theoretical guarantees but are susceptible to propagating\nerroneous decisions, while others are very adept at handling and representing\nuncertainty, but need to rely on undesirable assumptions. Our aim is to combine\nthe inherent robustness of the Bayesian approach with the theoretical strength\nand clarity of constraint-based methods. We use a Bayesian score to obtain\nprobability estimates on the input statements used in a constraint-based\nprocedure. These are subsequently processed in decreasing order of reliability,\nletting more reliable decisions take precedence in case of con icts, until a\nsingle output model is obtained. Tests show that a basic implementation of the\nresulting Bayesian Constraint-based Causal Discovery (BCCD) algorithm already\noutperforms established procedures such as FCI and Conservative PC. It can also\nindicate which causal decisions in the output have high reliability and which\ndo not.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:39:28 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Claassen", "Tom", ""], ["Heskes", "Tom", ""]]}, {"id": "1210.4868", "submitter": "Jie Liu", "authors": "Jie Liu, Chunming Zhang, Catherine McCarty, Peggy Peissig, Elizabeth\n  Burnside, David Page", "title": "Graphical-model Based Multiple Testing under Dependence, with\n  Applications to Genome-wide Association Studies", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-511-522", "categories": "stat.ME cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale multiple testing tasks often exhibit dependence, and leveraging\nthe dependence between individual tests is still one challenging and important\nproblem in statistics. With recent advances in graphical models, it is feasible\nto use them to perform multiple testing under dependence. We propose a multiple\ntesting procedure which is based on a Markov-random-field-coupled mixture\nmodel. The ground truth of hypotheses is represented by a latent binary Markov\nrandom field, and the observed test statistics appear as the coupled mixture\nvariables. The parameters in our model can be automatically learned by a novel\nEM algorithm. We use an MCMC algorithm to infer the posterior probability that\neach hypothesis is null (termed local index of significance), and the false\ndiscovery rate can be controlled accordingly. Simulations show that the\nnumerical performance of multiple testing can be improved substantially by\nusing our procedure. We apply the procedure to a real-world genome-wide\nassociation study on breast cancer, and we identify several SNPs with strong\nassociation evidence.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:40:38 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Liu", "Jie", ""], ["Zhang", "Chunming", ""], ["McCarty", "Catherine", ""], ["Peissig", "Peggy", ""], ["Burnside", "Elizabeth", ""], ["Page", "David", ""]]}, {"id": "1210.4879", "submitter": "Antti Hyttinen", "authors": "Antti Hyttinen, Frederick Eberhardt, Patrik O. Hoyer", "title": "Causal Discovery of Linear Cyclic Models from Multiple Experimental Data\n  Sets with Overlapping Variables", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-387-396", "categories": "stat.ME cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of scientific data is collected as randomized experiments intervening on\nsome and observing other variables of interest. Quite often, a given phenomenon\nis investigated in several studies, and different sets of variables are\ninvolved in each study. In this article we consider the problem of integrating\nsuch knowledge, inferring as much as possible concerning the underlying causal\nstructure with respect to the union of observed variables from such\nexperimental or passive observational overlapping data sets. We do not assume\nacyclicity or joint causal sufficiency of the underlying data generating model,\nbut we do restrict the causal relationships to be linear and use only second\norder statistics of the data. We derive conditions for full model\nidentifiability in the most generic case, and provide novel techniques for\nincorporating an assumption of faithfulness to aid in inference. In each case\nwe seek to establish what is and what is not determined by the data at hand.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:43:35 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Hyttinen", "Antti", ""], ["Eberhardt", "Frederick", ""], ["Hoyer", "Patrik O.", ""]]}, {"id": "1210.4903", "submitter": "Mathieu Sinn", "authors": "Mathieu Sinn, Ali Ghodsi, Karsten Keller", "title": "Detecting Change-Points in Time Series by Maximum Mean Discrepancy of\n  Ordinal Pattern Distributions", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-786-794", "categories": "stat.ME cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a new method for detecting change-points in high-resolution time series,\nwe apply Maximum Mean Discrepancy to the distributions of ordinal patterns in\ndifferent parts of a time series. The main advantage of this approach is its\ncomputational simplicity and robustness with respect to (non-linear) monotonic\ntransformations, which makes it particularly well-suited for the analysis of\nlong biophysical time series where the exact calibration of measurement devices\nis unknown or varies with time. We establish consistency of the method and\nevaluate its performance in simulation studies. Furthermore, we demonstrate the\napplication to the analysis of electroencephalography (EEG) and\nelectrocardiography (ECG) recordings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:51:29 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Sinn", "Mathieu", ""], ["Ghodsi", "Ali", ""], ["Keller", "Karsten", ""]]}, {"id": "1210.5073", "submitter": "Yvik Swan", "authors": "Marc Hallin, Yvik Swan, Thomas Verdebout and David Veredas", "title": "One-Step R-Estimation in Linear Models with Stable Errors", "comments": null, "journal-ref": "Journal of Econometrics (2012)", "doi": "10.1016/j.jeconom.2012.08.016", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical estimation techniques for linear models either are inconsistent, or\nperform rather poorly, under $\\alpha$-stable error densities; most of them are\nnot even rate-optimal. In this paper, we propose an original one-step\nR-estimation method and investigate its asymptotic performances under stable\ndensities. Contrary to traditional least squares, the proposed R-estimators\nremain root-$n$ consistent (the optimal rate) under the whole family of stable\ndistributions, irrespective of their asymmetry and tail index. While parametric\nstable-likelihood estimation, due to the absence of a closed form for stable\ndensities, is quite cumbersome, our method allows us to construct estimators\nreaching the parametric efficiency bounds associated with any prescribed values\n$(\\alpha_0, \\ b_0)$ of the tail index $\\alpha$ and skewness parameter $b$,\nwhile preserving root-$n$ consistency under any $(\\alpha, \\ b)$ as well as\nunder usual light-tailed densities. The method furthermore avoids all forms of\nmultidimensional argmin computation. Simulations confirm its excellent\nfinite-sample performances.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 09:57:53 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Hallin", "Marc", ""], ["Swan", "Yvik", ""], ["Verdebout", "Thomas", ""], ["Veredas", "David", ""]]}, {"id": "1210.5371", "submitter": "Reza Mohammadi", "authors": "A. Mohammadi, E. C. Wit", "title": "Bayesian Structure Learning in Sparse Gaussian Graphical Models", "comments": "Published at http://dx.doi.org/10.1214/14-BA889 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 1, 109-138", "doi": "10.1214/14-BA889", "report-no": "VTeX-BA-BA889", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoding complex relationships among large numbers of variables with\nrelatively few observations is one of the crucial issues in science. One\napproach to this problem is Gaussian graphical modeling, which describes\nconditional independence of variables through the presence or absence of edges\nin the underlying graph. In this paper, we introduce a novel and efficient\nBayesian framework for Gaussian graphical model determination which is a\ntrans-dimensional Markov Chain Monte Carlo (MCMC) approach based on a\ncontinuous-time birth-death process. We cover the theory and computational\ndetails of the method. It is easy to implement and computationally feasible for\nhigh-dimensional graphs. We show our method outperforms alternative Bayesian\napproaches in terms of convergence, mixing in the graph space and computing\ntime. Unlike frequentist approaches, it gives a principled and, in practice,\nsensible approach for structure learning. We illustrate the efficiency of the\nmethod on a broad range of simulated data. We then apply the method on\nlarge-scale real applications from human and mammary gland gene expression\nstudies to show its empirical usefulness. In addition, we implemented the\nmethod in the R package BDgraph which is freely available at\nhttp://CRAN.R-project.org/package=BDgraph\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 10:40:42 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2012 10:44:27 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2012 13:20:39 GMT"}, {"version": "v4", "created": "Tue, 5 Feb 2013 13:02:35 GMT"}, {"version": "v5", "created": "Wed, 22 Jan 2014 15:37:15 GMT"}, {"version": "v6", "created": "Thu, 22 May 2014 09:57:00 GMT"}, {"version": "v7", "created": "Mon, 30 Mar 2015 11:45:22 GMT"}, {"version": "v8", "created": "Thu, 25 Apr 2019 15:17:50 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Mohammadi", "A.", ""], ["Wit", "E. C.", ""]]}, {"id": "1210.5378", "submitter": "{\\O}ystein S{\\o}rensen", "authors": "{\\O}ystein S{\\o}rensen, Arnoldo Frigessi, Magne Thoresen", "title": "Measurement Error in Lasso: Impact and Correction", "comments": null, "journal-ref": null, "doi": "10.5705/ss.2013.180", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression with the lasso penalty is a popular tool for performing dimension\nreduction when the number of covariates is large. In many applications of the\nlasso, like in genomics, covariates are subject to measurement error. We study\nthe impact of measurement error on linear regression with the lasso penalty,\nboth analytically and in simulation experiments. A simple method of correction\nfor measurement error in the lasso is then considered. In the large sample\nlimit, the corrected lasso yields sign consistent covariate selection under\nconditions very similar to the lasso with perfect measurements, whereas the\nuncorrected lasso requires much more stringent conditions on the covariance\nstructure of the data. Finally, we suggest methods to correct for measurement\nerror in generalized linear models with the lasso penalty, which we study\nempirically in simulation experiments with logistic regression, and also apply\nto a classification problem with microarray data. We see that the corrected\nlasso selects less false positives than the standard lasso, at a similar level\nof true positives. The corrected lasso can therefore be used to obtain more\nconservative covariate selection in genomic analysis.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 11:10:33 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2013 07:18:52 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2014 21:52:48 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["S\u00f8rensen", "\u00d8ystein", ""], ["Frigessi", "Arnoldo", ""], ["Thoresen", "Magne", ""]]}, {"id": "1210.5418", "submitter": "Nikolai Krivulin", "authors": "Nikolai Krivulin", "title": "Unbiased estimates for gradients of stochastic network performance\n  measures", "comments": null, "journal-ref": "Acta Applicandae Mathematicae, October 1993, Volume 33, Issue 1,\n  pp. 21-43. ISSN: 0167-8019", "doi": "10.1007/BF00995493", "report-no": null, "categories": "math.OC stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three classes of stochastic networks and their performance measures are\nconsidered. These performance measures are defined as the expected value of\nsome random variables and cannot normally be obtained analytically as functions\nof network parameters in a closed form. We give similar representations for the\nrandom variables to provide a useful way of analytical study of these functions\nand their gradients. The representations are used to obtain sufficient\nconditions for the gradient estimates to be unbiased. The conditions are rather\ngeneral and usually met in simulation study of the stochastic networks.\nApplications of the results are discussed and some practical algorithms of\ncalculating unbiased estimates of the gradients are also presented.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 13:38:06 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Krivulin", "Nikolai", ""]]}, {"id": "1210.5500", "submitter": "Yasser Gonz\\'alez-Fern\\'andez", "authors": "Marta Soto, Yasser Gonz\\'alez-Fern\\'andez, Alberto Ochoa", "title": "Modeling with Copulas and Vines in Estimation of Distribution Algorithms", "comments": null, "journal-ref": "Investigaci\\'on Operacional, 36(1), 1-23", "doi": null, "report-no": null, "categories": "cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is studying the use of copulas and vines in the\noptimization with Estimation of Distribution Algorithms (EDAs). Two EDAs are\nbuilt around the multivariate product and normal copulas, and other two are\nbased on pair-copula decomposition of vine models. Empirically we study the\neffect of both marginal distributions and dependence structure separately, and\nshow that both aspects play a crucial role in the success of the optimization.\nThe results show that the use of copulas and vines opens new opportunities to a\nmore appropriate modeling of search distributions in EDAs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 19:03:11 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Soto", "Marta", ""], ["Gonz\u00e1lez-Fern\u00e1ndez", "Yasser", ""], ["Ochoa", "Alberto", ""]]}, {"id": "1210.5578", "submitter": "Hung Hung", "authors": "Peng-Wen Chen, Hung Hung, Osamu Komori, Su-Yun Huang, Shinto Eguchi", "title": "Robust Independent Component Analysis via Minimum Divergence Estimation", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) has been shown to be useful in many\napplications. However, most ICA methods are sensitive to data contamination and\noutliers. In this article we introduce a general minimum U-divergence framework\nfor ICA, which covers some standard ICA methods as special cases. Within the\nU-family we further focus on the gamma-divergence due to its desirable property\nof super robustness, which gives the proposed method gamma-ICA. Statistical\nproperties and technical conditions for the consistency of gamma-ICA are\nrigorously studied. In the limiting case, it leads to a necessary and\nsufficient condition for the consistency of MLE-ICA. This necessary and\nsufficient condition is weaker than the condition known in the literature.\nSince the parameter of interest in ICA is an orthogonal matrix, a geometrical\nalgorithm based on gradient flows on special orthogonal group is introduced to\nimplement gamma-ICA. Furthermore, a data-driven selection for the gamma value,\nwhich is critical to the achievement of gamma-ICA, is developed. The\nperformance, especially the robustness, of gamma-ICA in comparison with\nstandard ICA methods is demonstrated through experimental studies using\nsimulated data and image data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2012 04:22:56 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Chen", "Peng-Wen", ""], ["Hung", "Hung", ""], ["Komori", "Osamu", ""], ["Huang", "Su-Yun", ""], ["Eguchi", "Shinto", ""]]}, {"id": "1210.6003", "submitter": "Ioannis Papastathopoulos", "authors": "Ioannis Papastathopoulos and Jonathan A. Tawn", "title": "Stochastic Ordering under Conditional Modelling of Extreme Values:\n  Drug-Induced Liver Injury", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug-induced liver injury (DILI) is a major public health issue and of\nserious concern for the pharmaceutical industry. Early detection of signs of a\ndrug's potential for DILI is vital for pharmaceutical companies' evaluation of\nnew drugs. A combination of extreme values of liver specific variables indicate\npotential DILI (Hy's Law). We estimate the probability of severe DILI using the\nHeffernan and Tawn (2004) conditional dependence model which arises naturally\nin applications where a multidimensional random variable is extreme in at least\none component. We extend the current model by including the assumption of\nstochastically ordered survival curves for different doses in a Phase 3 study.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 19:06:48 GMT"}, {"version": "v2", "created": "Mon, 26 May 2014 13:47:48 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Papastathopoulos", "Ioannis", ""], ["Tawn", "Jonathan A.", ""]]}, {"id": "1210.6269", "submitter": "Piyush Tagade", "authors": "Piyush M. Tagade and Han-Lim Choi", "title": "A Dynamic Bi-orthogonality based Approach for Uncertainty Quantification\n  of Stochastic Systems with Discontinuities", "comments": "The paper is submitted to SIAM/ASA Journal on Uncertainty\n  Quantification for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of spectral projection based methods for simulation of a stochastic\nsystem with discontinuous solution exhibits the Gibbs phenomenon, which is\ncharacterized by oscillations near discontinuities. This paper investigates a\ndynamic bi-orthogonality based approach with appropriate post-processing for\nmitigating the effects of the Gibbs phenomenon. The proposed approach uses\nspectral decomposition of the spatial and stochastic fields in appropriate\northogonal bases, while the dynamic orthogonality condition is used to derive\nthe resultant closed form evolution equations. The orthogonal decomposition of\nthe spatial field is exploited to propose a Gegenbauer reprojection based\npost-processing approach, where the orthogonal bases in spatial dimension are\nreprojected on the Gegenbauer polynomials in the domain of analyticity. The\nresultant spectral expansion in Gegenbauer series is shown to mitigate the\nGibbs phenomenon. Efficacy of the proposed method is demonstrated for\nsimulation of a one-dimensional stochastic Burgers equation with uncertain\ninitial condition.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 15:34:18 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["Tagade", "Piyush M.", ""], ["Choi", "Han-Lim", ""]]}, {"id": "1210.6703", "submitter": "Anthony Lee", "authors": "Anthony Lee and Krzysztof Latuszynski", "title": "Variance bounding and geometric ergodicity of Markov chain Monte Carlo\n  kernels for approximate Bayesian computation", "comments": "26 pages, 10 figures", "journal-ref": null, "doi": "10.1093/biomet/asu027", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation has emerged as a standard computational tool\nwhen dealing with the increasingly common scenario of completely intractable\nlikelihood functions in Bayesian inference. We show that many common Markov\nchain Monte Carlo kernels used to facilitate inference in this setting can fail\nto be variance bounding, and hence geometrically ergodic, which can have\nconsequences for the reliability of estimates in practice. This phenomenon is\ntypically independent of the choice of tolerance in the approximation. We then\nprove that a recently introduced Markov kernel in this setting can inherit\nvariance bounding and geometric ergodicity from its intractable\nMetropolis--Hastings counterpart, under reasonably weak and manageable\nconditions. We show that the computational cost of this alternative kernel is\nbounded whenever the prior is proper, and present indicative results on an\nexample where spectral gaps and asymptotic variances can be computed, as well\nas an example involving inference for a partially and discretely observed,\ntime-homogeneous, pure jump Markov process. We also supply two general\ntheorems, one of which provides a simple sufficient condition for lack of\nvariance bounding for reversible kernels and the other provides a positive\nresult concerning inheritance of variance bounding and geometric ergodicity for\nmixtures of reversible kernels.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 23:14:51 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2013 13:16:10 GMT"}, {"version": "v3", "created": "Tue, 25 Mar 2014 21:57:19 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Lee", "Anthony", ""], ["Latuszynski", "Krzysztof", ""]]}, {"id": "1210.6799", "submitter": "Jonathan Bartlett", "authors": "Jonathan W. Bartlett, Shaun R. Seaman, Ian R. White, James R.\n  Carpenter", "title": "Multiple imputation of covariates by fully conditional specification:\n  accommodating the substantive model", "comments": "In the original version we defined the concept of congeniality\n  differently from Meng (1994). In this revised version we refrain from using\n  the term congeniality, and instead refer only to compatibility between\n  imputation and substantive models. In the discussion we explain how\n  imputation and substantive model compatibility and congeniality are related", "journal-ref": null, "doi": "10.1177/0962280214521348", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing covariate data commonly occur in epidemiological and clinical\nresearch, and are often dealt with using multiple imputation (MI). Imputation\nof partially observed covariates is complicated if the substantive model is\nnon-linear (e.g. Cox proportional hazards model), or contains non-linear (e.g.\nsquared) or interaction terms, and standard software implementations of MI may\nimpute covariates from models that are incompatible with such substantive\nmodels. We show how imputation by fully conditional specification, a popular\napproach for performing MI, can be modified so that covariates are imputed from\nmodels which are compatible with the substantive model. We investigate through\nsimulation the performance of this proposal, and compare it to existing\napproaches. Simulation results suggest our proposal gives consistent estimates\nfor a range of common substantive models, including models which contain\nnon-linear covariate effects or interactions, provided data are missing at\nrandom and the assumed imputation models are correctly specified and mutually\ncompatible.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 11:47:58 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2012 18:21:50 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2013 10:43:18 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Bartlett", "Jonathan W.", ""], ["Seaman", "Shaun R.", ""], ["White", "Ian R.", ""], ["Carpenter", "James R.", ""]]}, {"id": "1210.6805", "submitter": "Paul Blanche", "authors": "Paul Blanche, Aur\\'elien Latouche, Vivian Viallon", "title": "Time-dependent AUC with right-censored data: a survey study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ROC curve and the corresponding AUC are popular tools for the evaluation\nof diagnostic tests. They have been recently extended to assess prognostic\nmarkers and predictive models. However, due to the many particularities of\ntime-to-event outcomes, various definitions and estimators have been proposed\nin the literature. This review article aims at presenting the ones that\naccommodate to right-censoring, which is common when evaluating such prognostic\nmarkers.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 12:12:01 GMT"}], "update_date": "2012-10-26", "authors_parsed": [["Blanche", "Paul", ""], ["Latouche", "Aur\u00e9lien", ""], ["Viallon", "Vivian", ""]]}, {"id": "1210.6866", "submitter": "Niels Oppermann", "authors": "Niels Oppermann, Marco Selig, Michael R. Bell, Torsten A. En{\\ss}lin", "title": "Reconstruction of Gaussian and log-normal fields with spectral\n  smoothness", "comments": "18 pages, 12 figures", "journal-ref": "Phys.Rev.E 87, 032136 (2013)", "doi": "10.1103/PhysRevE.87.032136", "report-no": null, "categories": "astro-ph.IM physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method to infer log-normal random fields from measurement data\naffected by Gaussian noise. The log-normal model is well suited to describe\nstrictly positive signals with fluctuations whose amplitude varies over several\norders of magnitude. We use the formalism of minimum Gibbs free energy to\nderive an algorithm that uses the signal's correlation structure to regularize\nthe reconstruction. The correlation structure, described by the signal's power\nspectrum, is thereby reconstructed from the same data set. We show that the\nminimization of the Gibbs free energy, corresponding to a Gaussian\napproximation to the posterior marginalized over the power spectrum, is\nequivalent to the empirical Bayes ansatz, in which the power spectrum is fixed\nto its maximum a posteriori value. We further introduce a prior for the power\nspectrum that enforces spectral smoothness. The appropriateness of this prior\nin different scenarios is discussed and its effects on the reconstruction's\nresults are demonstrated. We validate the performance of our reconstruction\nalgorithm in a series of one- and two-dimensional test cases with varying\ndegrees of non-linearity and different noise levels.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 14:59:55 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2013 16:42:53 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Oppermann", "Niels", ""], ["Selig", "Marco", ""], ["Bell", "Michael R.", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1210.6950", "submitter": "Runlong Tang", "authors": "Jianqing Fan, Runlong Tang and Xiaofeng Shi", "title": "Partial Consistency with Sparse Incidental Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized estimation principle is fundamental to high-dimensional problems.\nIn the literature, it has been extensively and successfully applied to various\nmodels with only structural parameters. As a contrast, in this paper, we apply\nthis penalization principle to a linear regression model with a\nfinite-dimensional vector of structural parameters and a high-dimensional\nvector of sparse incidental parameters. For the estimators of the structural\nparameters, we derive their consistency and asymptotic normality, which reveals\nan oracle property. However, the penalized estimators for the incidental\nparameters possess only partial selection consistency but not consistency. This\nis an interesting partial consistency phenomenon: the structural parameters are\nconsistently estimated while the incidental ones cannot. For the structural\nparameters, also considered is an alternative two-step penalized estimator,\nwhich has fewer possible asymptotic distributions and thus is more suitable for\nstatistical inferences. We further extend the methods and results to the case\nwhere the dimension of the structural parameter vector diverges with but slower\nthan the sample size. A data-driven approach for selecting a penalty\nregularization parameter is provided. The finite-sample performance of the\npenalized estimators for the structural parameters is evaluated by simulations\nand a real data set is analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 19:16:02 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 08:24:33 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 02:34:29 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Fan", "Jianqing", ""], ["Tang", "Runlong", ""], ["Shi", "Xiaofeng", ""]]}, {"id": "1210.6958", "submitter": "Sami Stouli", "authors": "Richard Spady and Sami Stouli", "title": "Dual Regression", "comments": "Version accepted for publication, 39 pages, 4 figures", "journal-ref": "Biometrika. Vol. 105(1), pp.1-18 (2018)", "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose dual regression as an alternative to the quantile regression\nprocess for the global estimation of conditional distribution functions under\nminimal assumptions. Dual regression provides all the interpretational power of\nthe quantile regression process while avoiding the need for repairing the\nintersecting conditional quantile surfaces that quantile regression often\nproduces in practice. Our approach introduces a mathematical programming\ncharacterization of conditional distribution functions which, in its simplest\nform, is the dual program of a simultaneous estimator for linear location-scale\nmodels. We apply our general characterization to the specification and\nestimation of a flexible class of conditional distribution functions, and\npresent asymptotic theory for the corresponding empirical dual regression\nprocess.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 19:25:04 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2013 01:04:49 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 00:17:13 GMT"}, {"version": "v4", "created": "Sun, 23 Sep 2018 13:10:48 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Spady", "Richard", ""], ["Stouli", "Sami", ""]]}, {"id": "1210.7052", "submitter": "Rachel Ward", "authors": "Rachel Ward and Raymond J. Carroll", "title": "Testing Hardy-Weinberg equilibrium with a simple root-mean-square\n  statistic", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide evidence that a root-mean-square test of goodness-of-fit can be\nsignificantly more powerful than state-of-the-art exact tests in detecting\ndeviations from Hardy-Weinberg equilibrium. Unlike Pearson's chi-square test,\nthe log--likelihood-ratio test, and Fisher's exact test, which are sensitive to\nrelative discrepancies between genotypic frequencies, the root-mean-square test\nis sensitive to absolute discrepancies. This can increase statistical power, as\nwe demonstrate using benchmark datasets and through asymptotic analysis. With\nthe aid of computers, exact P-values for the root-mean-square statistic can be\ncalculated eeffortlessly, and can be easily implemented using the author's\nfreely available code.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 05:00:03 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2013 22:22:25 GMT"}, {"version": "v3", "created": "Thu, 30 May 2013 22:37:55 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Ward", "Rachel", ""], ["Carroll", "Raymond J.", ""]]}, {"id": "1210.7053", "submitter": "Khoat Than", "authors": "Khoat Than and Tu Bao Ho", "title": "Managing sparsity, time, and quality of inference in topic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference is an integral part of probabilistic topic models, but is often\nnon-trivial to derive an efficient algorithm for a specific model. It is even\nmuch more challenging when we want to find a fast inference algorithm which\nalways yields sparse latent representations of documents. In this article, we\nintroduce a simple framework for inference in probabilistic topic models,\ndenoted by FW. This framework is general and flexible enough to be easily\nadapted to mixture models. It has a linear convergence rate, offers an easy way\nto incorporate prior knowledge, and provides us an easy way to directly trade\noff sparsity against quality and time. We demonstrate the goodness and\nflexibility of FW over existing inference methods by a number of tasks.\nFinally, we show how inference in topic models with nonconjugate priors can be\ndone efficiently.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 05:23:25 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2013 00:09:46 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Than", "Khoat", ""], ["Ho", "Tu Bao", ""]]}, {"id": "1210.7192", "submitter": "Siegfried Hoermann", "authors": "Siegfried H\\\"ormann, {\\L}ukasz Kidzi\\'nski, Marc Hallin", "title": "Dynamic Functional Principal Component", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of dimension reduction for time series\nof functional data $(X_t\\colon t\\in\\mathbb{Z})$. Such {\\it functional time\nseries} frequently arise, e.g., when a continuous-time process is segmented\ninto some smaller natural units, such as days. Then each~$X_t$ represents one\nintraday curve. We argue that functional principal component analysis (FPCA),\nthough a key technique in the field and a benchmark for any competitor, does\nnot provide an adequate dimension reduction in a time-series setting. FPCA\nindeed is a {\\it static} procedure which ignores the essential information\nprovided by the serial dependence structure of the functional data under study.\nTherefore, inspired by Brillinger's theory of {\\it dynamic principal\ncomponents}, we propose a {\\it dynamic} version of FPCA, which is based on a\nfrequency-domain approach. By means of a simulation study and an empirical\nillustration, we show the considerable improvement the dynamic approach entails\nwhen compared to the usual static procedure.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 16:57:25 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2012 10:42:08 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2013 12:24:55 GMT"}, {"version": "v4", "created": "Wed, 16 Apr 2014 10:45:29 GMT"}, {"version": "v5", "created": "Tue, 2 Jun 2015 18:20:05 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["H\u00f6rmann", "Siegfried", ""], ["Kidzi\u0144ski", "\u0141ukasz", ""], ["Hallin", "Marc", ""]]}, {"id": "1210.7351", "submitter": "Christopher Paciorek", "authors": "Adam A. Szpiro, Christopher J. Paciorek", "title": "Measurement error in two-stage analyses, with application to air\n  pollution epidemiology", "comments": "35 pages, 4 figures, 2 tables", "journal-ref": "Environmetrics (2013) 24: 501-517", "doi": "10.1002/env.2233", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public health researchers often estimate health effects of exposures (e.g.,\npollution, diet, lifestyle) that cannot be directly measured for study\nsubjects. A common strategy in environmental epidemiology is to use a\nfirst-stage (exposure) model to estimate the exposure based on covariates\nand/or spatio-temporal proximity and to use predictions from the exposure model\nas the covariate of interest in the second-stage (health) model. This induces a\ncomplex form of measurement error. We propose an analytical framework and\nmethodology that is robust to misspecification of the first-stage model and\nprovides valid inference for the second-stage model parameter of interest.\n  We decompose the measurement error into components analogous to classical and\nBerkson error and characterize properties of the estimator in the second-stage\nmodel if the first-stage model predictions are plugged in without correction.\nSpecifically, we derive conditions for compatibility between the first- and\nsecond-stage models that guarantee consistency (and have direct and important\nreal-world design implications), and we derive an asymptotic estimate of\nfinite-sample bias when the compatibility conditions are satisfied. We propose\na methodology that (1) corrects for finite-sample bias and (2) correctly\nestimates standard errors. We demonstrate the utility of our methodology in\nsimulations and an example from air pollution epidemiology.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2012 17:57:57 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2013 20:00:52 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Szpiro", "Adam A.", ""], ["Paciorek", "Christopher J.", ""]]}, {"id": "1210.7447", "submitter": "Robert Stelzer", "authors": "Eckhard Schlemm and Robert Stelzer", "title": "Quasi maximum likelihood estimation for strongly mixing state space\n  models and multivariate L\\'evy-driven CARMA processes", "comments": null, "journal-ref": "2012, Electronic Journal of Statistics, 6, pp 2185-2234", "doi": "10.1214/12-EJS743", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider quasi maximum likelihood (QML) estimation for general\nnon-Gaussian discrete-ime linear state space models and equidistantly observed\nmultivariate L\\'evy-driven continuoustime autoregressive moving average\n(MCARMA) processes. In the discrete-time setting, we prove strong consistency\nand asymptotic normality of the QML estimator under standard moment assumptions\nand a strong-mixing condition on the output process of the state space model.\nIn the second part of the paper, we investigate probabilistic and analytical\nproperties of equidistantly sampled continuous-time state space models and\napply our results from the discrete-time setting to derive the asymptotic\nproperties of the QML estimator of discretely recorded MCARMA processes. Under\nnatural identifiability conditions, the estimators are again consistent and\nasymptotically normally distributed for any sampling frequency. We also\ndemonstrate the practical applicability of our method through a simulation\nstudy and a data example from econometrics.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2012 12:00:00 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Schlemm", "Eckhard", ""], ["Stelzer", "Robert", ""]]}, {"id": "1210.7508", "submitter": "Nico Riedel", "authors": "Nico Riedel and Johannes Berg", "title": "A statistical mechanics approach to the sample deconvolution problem", "comments": "8 pages, 4 figures", "journal-ref": "Phys. Rev. E 87, 042715 (2013)", "doi": "10.1103/PhysRevE.87.042715", "report-no": null, "categories": "q-bio.QM cond-mat.dis-nn physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multicellular organism different cell types express a gene in different\namounts. Samples from which gene expression levels can be measured typically\ncontain a mixture of different cell types, the resulting measurements thus give\nonly averages over the different cell types present. Based on fluctuations in\nthe mixture proportions from sample to sample it is in principle possible to\nreconstruct the underlying expression levels of each cell type: to deconvolute\nthe sample. We use a statistical mechanics approach to the problem of\ndeconvoluting such partial concentrations from mixed samples, give analytical\nresults for when and how well samples can be unmixed, and suggest an algorithm\nfor sample deconvolution.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2012 20:43:03 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Riedel", "Nico", ""], ["Berg", "Johannes", ""]]}, {"id": "1210.7642", "submitter": "J. Martin van Zyl", "authors": "J. Martin van Zyl", "title": "Estimation of the shape parameter of a generalized Pareto distribution\n  based on a transformation to Pareto distributed variables", "comments": null, "journal-ref": "Journal of Statistical Theory and Practice, 2015, 9(1), pp.\n  171-183", "doi": null, "report-no": null, "categories": "q-fin.CP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random variables of the generalized Pareto distribution, can be transformed\nto that of the Pareto distribution. Explicit expressions exist for the maximum\nlikelihood estimators of the parameters of the Pareto distribution. The\nperformance of the estimation of the shape parameter of generalized Pareto\ndistributed using transformed observations, based on the probability weighted\nmethod is tested. It was found to improve the performance of the probability\nweighted estimator and performs good with respect to bias and MSE.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2012 12:37:56 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2012 09:36:50 GMT"}, {"version": "v3", "created": "Wed, 21 Nov 2012 10:37:04 GMT"}, {"version": "v4", "created": "Fri, 14 Dec 2012 10:49:08 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["van Zyl", "J. Martin", ""]]}]