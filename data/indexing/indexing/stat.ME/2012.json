[{"id": "2012.00054", "submitter": "Maria Jose Lombardia", "authors": "M.D. Esteban, M.J. Lombard\\'ia, E. L\\'opez-Vizca\\'ino, D. Morales and\n  A. P\\'erez", "title": "Empirical best prediction of small area bivariate parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces empirical best predictors of small area bivariate\nparameters, like ratios of sums or sums of ratios, by assuming that the target\nunit-level vector follows a bivariate nested error regression model. The\ncorresponding means squared errors are estimated by parametric bootstrap.\nSeveral simulation experiments empirically study the behavior of the introduced\nstatistical methodology. An application to real data from the Spanish household\nbudget survey gives estimators of ratios of food household expenditures by\nprovinces.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 19:12:56 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Esteban", "M. D.", ""], ["Lombard\u00eda", "M. J.", ""], ["L\u00f3pez-Vizca\u00edno", "E.", ""], ["Morales", "D.", ""], ["P\u00e9rez", "A.", ""]]}, {"id": "2012.00064", "submitter": "Mar\\'ia Jos\\'e Lombard\\'ia", "authors": "M.J. Lombard\\'ia, E. L\\'opez-Vizca\\'ino and C. Rueda", "title": "A new approach to the gender pay gap decomposition by economic activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this paper is to present an original approach to estimate the\ngender pay gap. We propose a model-based decomposition, similar to the most\npopular approaches, where the first component measures differences in group\ncharacteristics and the second component measures the unexplained effect; the\nlatter being the real gap. The novel approach incorporates model selection and\nbias correction. %, avoiding the main limitation of standard approaches, which\nis the dependence on the choice of explanatory variables and the functional\nform in regression. The pay gap problem in a small area context is considered\nin this paper, although the approach is flexible to be applied to other\ncontexts.\n  Specifically, the methodology is validated for analysing wage differentials\nby economic activities in the region of Galicia (Spain) and by analysing\nsimulated data from an experimental design that imitates the generation of real\ndata. The good performance of the proposed estimators is shown in both cases,\nspecifically when compared with those obtained from the widely used\nOaxaca-Blinder approach.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 19:32:31 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Lombard\u00eda", "M. J.", ""], ["L\u00f3pez-Vizca\u00edno", "E.", ""], ["Rueda", "C.", ""]]}, {"id": "2012.00069", "submitter": "Mar\\'ia Jos\\'e Lombard\\'ia", "authors": "M. Boubeta, M.J. Lombard\\'ia, F. Marey-P\\'erez and D. Morales", "title": "Area-level spatio-temporal Poisson mixed models for predicting domain\n  counts and proportions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces area-level Poisson mixed models with temporal and\nSAR(1) spatially correlated random effects. Small area predictors of the\nproportions and counts of a dichotomic variable are derived from the new models\nand the corresponding mean squared errors are estimated by parametric\nbootstrap. The paper illustrates the introduced methodology with two\napplications to real data. The first one deals with data of forest fires in\nGalicia (Spain) during 2007-2008 and the target is modeling and predicting\ncounts of fires. The second one treats data from the Spanish living conditions\nsurvey of Galicia of 2013 and the target is the estimation of county\nproportions of women under the poverty line.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 19:44:32 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Boubeta", "M.", ""], ["Lombard\u00eda", "M. J.", ""], ["Marey-P\u00e9rez", "F.", ""], ["Morales", "D.", ""]]}, {"id": "2012.00081", "submitter": "Jannik Schaller", "authors": "Florian Meinfelder and Jannik Schaller", "title": "Data Fusion for Joining Income and Consumption Information Using\n  Different Donor-Recipient Distance Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data fusion describes the method of combining data from (at least) two\ninitially independent data sources to allow for joint analysis of variables\nwhich are not jointly observed. The fundamental idea is to base inference on\nidentifying assumptions, and on common variables which provide information that\nis jointly observed in all the data sources. A popular class of methods dealing\nwith this particular missing-data problem is based on nearest neighbour\nmatching. However, exact matches become unlikely with increasing common\ninformation, and the specification of the distance function can influence the\nresults of the data fusion. In this paper we compare two different approaches\nof nearest neighbour hot deck matching: One, Random Hot Deck, is a variant of\nthe covariate-based matching methods which was proposed by Eurostat, and can be\nconsidered as a 'classical' statistical matching method, whereas the\nalternative approach is based on Predictive Mean Matching. We discuss results\nfrom a simulation study to investigate benefits and potential drawbacks of both\nvariants, and our findings suggest that Predictive Mean Matching tends to\noutperform Random Hot Deck.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 20:10:40 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Meinfelder", "Florian", ""], ["Schaller", "Jannik", ""]]}, {"id": "2012.00174", "submitter": "Andrew Gelman", "authors": "Andrew Gelman and Aki Vehtari", "title": "What are the most important statistical ideas of the past 50 years?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review the most important statistical ideas of the past half century,\nwhich we categorize as: counterfactual causal inference, bootstrapping and\nsimulation-based inference, overparameterized models and regularization,\nBayesian multilevel models, generic computation algorithms, adaptive decision\nanalysis, robust inference, and exploratory data analysis. We discuss key\ncontributions in these subfields, how they relate to modern computing and big\ndata, and how they might be developed and extended in future decades. The goal\nof this article is to provoke thought and discussion regarding the larger\nthemes of research in statistics and data science.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 23:54:59 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 15:52:22 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 13:53:16 GMT"}, {"version": "v4", "created": "Thu, 27 May 2021 12:24:54 GMT"}, {"version": "v5", "created": "Thu, 3 Jun 2021 15:44:39 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Gelman", "Andrew", ""], ["Vehtari", "Aki", ""]]}, {"id": "2012.00180", "submitter": "John R.J. Thompson", "authors": "John R.J. Thompson, W. John Braun", "title": "Anisotropic local constant smoothing for change-point regression\n  function estimation", "comments": "30 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Understanding forest fire spread in any region of Canada is critical to\npromoting forest health, and protecting human life and infrastructure.\nQuantifying fire spread from noisy images, where regions of a fire are\nseparated by change-point boundaries, is critical to faithfully estimating fire\nspread rates. In this research, we develop a statistically consistent smooth\nestimator that allows us to denoise fire spread imagery from micro-fire\nexperiments. We develop an anisotropic smoothing method for change-point data\nthat uses estimates of the underlying data generating process to inform\nsmoothing. We show that the anisotropic local constant regression estimator is\nconsistent with convergence rate $O\\left(n^{-1/{(q+2)}}\\right)$. We demonstrate\nits effectiveness on simulated one- and two-dimensional change-point data and\nfire spread imagery from micro-fire experiments.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 00:11:04 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Thompson", "John R. J.", ""], ["Braun", "W. John", ""]]}, {"id": "2012.00370", "submitter": "Martin Huber", "authors": "Hugo Bodory, Martin Huber, Luk\\'a\\v{s} Laff\\'ers", "title": "Evaluating (weighted) dynamic treatment effects by double machine\n  learning", "comments": "arXiv admin note: text overlap with arXiv:2002.12710", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider evaluating the causal effects of dynamic treatments, i.e. of\nmultiple treatment sequences in various periods, based on double machine\nlearning to control for observed, time-varying covariates in a data-driven way\nunder a selection-on-observables assumption. To this end, we make use of\nso-called Neyman-orthogonal score functions, which imply the robustness of\ntreatment effect estimation to moderate (local) misspecifications of the\ndynamic outcome and treatment models. This robustness property permits\napproximating outcome and treatment models by double machine learning even\nunder high dimensional covariates and is combined with data splitting to\nprevent overfitting. In addition to effect estimation for the total population,\nwe consider weighted estimation that permits assessing dynamic treatment\neffects in specific subgroups, e.g. among those treated in the first treatment\nperiod. We demonstrate that the estimators are asymptotically normal and\n$\\sqrt{n}$-consistent under specific regularity conditions and investigate\ntheir finite sample properties in a simulation study. Finally, we apply the\nmethods to the Job Corps study in order to assess different sequences of\ntraining programs under a large set of covariates.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:55:40 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 19:05:21 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 13:28:53 GMT"}, {"version": "v4", "created": "Tue, 16 Feb 2021 16:05:40 GMT"}, {"version": "v5", "created": "Sat, 19 Jun 2021 14:56:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Bodory", "Hugo", ""], ["Huber", "Martin", ""], ["Laff\u00e9rs", "Luk\u00e1\u0161", ""]]}, {"id": "2012.00394", "submitter": "Swapnil Mishra", "authors": "Samir Bhatt, Neil Ferguson, Seth Flaxman, Axel Gandy, Swapnil Mishra,\n  James A. Scott", "title": "Semi-Mechanistic Bayesian Modeling of COVID-19 with Renewal Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a general Bayesian approach to modeling epidemics such as\nCOVID-19. The approach grew out of specific analyses conducted during the\npandemic, in particular an analysis concerning the effects of\nnon-pharmaceutical interventions (NPIs) in reducing COVID-19 transmission in 11\nEuropean countries. The model parameterizes the time varying reproduction\nnumber $R_t$ through a regression framework in which covariates can e.g be\ngovernmental interventions or changes in mobility patterns. This allows a joint\nfit across regions and partial pooling to share strength. This innovation was\ncritical to our timely estimates of the impact of lockdown and other NPIs in\nthe European epidemics, whose validity was borne out by the subsequent course\nof the epidemic. Our framework provides a fully generative model for latent\ninfections and observations deriving from them, including deaths, cases,\nhospitalizations, ICU admissions and seroprevalence surveys. One issue\nsurrounding our model's use during the COVID-19 pandemic is the confounded\nnature of NPIs and mobility. We use our framework to explore this issue. We\nhave open sourced an R package epidemia implementing our approach in Stan.\nVersions of the model are used by New York State, Tennessee and Scotland to\nestimate the current situation and make policy decisions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 10:51:09 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 09:00:05 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Bhatt", "Samir", ""], ["Ferguson", "Neil", ""], ["Flaxman", "Seth", ""], ["Gandy", "Axel", ""], ["Mishra", "Swapnil", ""], ["Scott", "James A.", ""]]}, {"id": "2012.00457", "submitter": "Mamadou Yauck", "authors": "Mamadou Yauck, Erica E. M. Moodie, Herak Apelian, Alain Fourmigue,\n  Daniel Grace, Trevor Hart, Gilles Lambert, Joseph Cox", "title": "General Regression Methods for Respondent-Driven Sampling Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-Driven Sampling (RDS) is a variant of link-tracing sampling\ntechniques that aim to recruit hard-to-reach populations by leveraging\nindividuals' social relationships. As such, an RDS sample has a graphical\ncomponent which represents a partially observed network of unknown structure.\nMoreover, it is common to observe homophily, or the tendency to form\nconnections with individuals who share similar traits. Currently, there is a\nlack of principled guidance on multivariate modeling strategies for RDS to\naddress homophilic covariates and the dependence between observations within\nthe network. In this work, we propose a methodology for general regression\ntechniques using RDS data. This is used to study the socio-demographic\npredictors of HIV treatment optimism (about the value of antiretroviral\ntherapy) among gay, bisexual and other men who have sex with men, recruited\ninto an RDS study in Montreal, Canada.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:05:18 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Yauck", "Mamadou", ""], ["Moodie", "Erica E. M.", ""], ["Apelian", "Herak", ""], ["Fourmigue", "Alain", ""], ["Grace", "Daniel", ""], ["Hart", "Trevor", ""], ["Lambert", "Gilles", ""], ["Cox", "Joseph", ""]]}, {"id": "2012.00460", "submitter": "Yi Yu", "authors": "Daren Wang, Zifeng Zhao, Yi Yu and Rebecca Willett", "title": "Functional Linear Regression with Mixed Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a functional linear regression model that deals with functional\nresponses and allows for both functional covariates and high-dimensional vector\ncovariates. The proposed model is flexible and nests several functional\nregression models in the literature as special cases. Based on the theory of\nreproducing kernel Hilbert spaces (RKHS), we propose a penalized least squares\nestimator that can accommodate functional variables observed on discrete grids.\nBesides the conventional smoothness penalties, a group Lasso-type penalty is\nfurther imposed to induce sparsity in the high-dimensional vector predictors.\nWe derive finite sample theoretical guarantees and show that the excess\nprediction risk of our estimator is minimax optimal. Furthermore, our analysis\nreveals an interesting phase transition phenomenon that the optimal excess risk\nis determined jointly by the smoothness and the sparsity of the functional\nregression coefficients. A novel efficient optimization algorithm based on\niterative coordinate descent is devised to handle the smoothness and sparsity\npenalties simultaneously. Simulation studies and real data applications\nillustrate the promising performance of the proposed approach compared to the\nstate-of-the-art methods in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:10:15 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 09:34:28 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Wang", "Daren", ""], ["Zhao", "Zifeng", ""], ["Yu", "Yi", ""], ["Willett", "Rebecca", ""]]}, {"id": "2012.00579", "submitter": "Lingjing Jiang", "authors": "Lingjing Jiang, Yuan Zhong, Chris Elrod, Loki Natarajan, Rob Knight,\n  Wesley K. Thompson", "title": "BayesTime: Bayesian Functional Principal Components for Sparse\n  Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling non-linear temporal trajectories is of fundamental interest in many\napplication areas, such as in longitudinal microbiome analysis. Many existing\nmethods focus on estimating mean trajectories, but it is also often of value to\nassess temporal patterns of individual subjects. Sparse principal components\nanalysis (SFPCA) serves as a useful tool for assessing individual variation in\nnon-linear trajectories; however its application to real data often requires\ncareful model selection criteria and diagnostic tools. Here, we propose a\nBayesian approach to SFPCA, which allows users to use the efficient\nleave-one-out cross-validation (LOO) with Pareto-smoothed importance sampling\n(PSIS) for model selection, and to utilize the estimated shape parameter from\nPSIS-LOO and also the posterior predictive checks for graphical model\ndiagnostics. This Bayesian implementation thus enables careful application of\nSFPCA to a wide range of longitudinal data applications.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 15:35:19 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Jiang", "Lingjing", ""], ["Zhong", "Yuan", ""], ["Elrod", "Chris", ""], ["Natarajan", "Loki", ""], ["Knight", "Rob", ""], ["Thompson", "Wesley K.", ""]]}, {"id": "2012.00663", "submitter": "Pankaj Mehta", "authors": "Wenping Cui, Jason W. Rocks, Pankaj Mehta", "title": "The Perturbative Resolvent Method: spectral densities of random matrix\n  ensembles via perturbation theory", "comments": "15 page, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simple, perturbative approach for calculating spectral densities\nfor random matrix ensembles in the thermodynamic limit we call the Perturbative\nResolvent Method (PRM). The PRM is based on constructing a linear system of\nequations and calculating how the solutions to these equation change in\nresponse to a small perturbation using the zero-temperature cavity method. We\nillustrate the power of the method by providing simple analytic derivations of\nthe Wigner Semi-circle Law for symmetric matrices, the Marchenko-Pastur Law for\nWishart matrices, the spectral density for a product Wishart matrix composed of\ntwo square matrices, and the Circle and elliptic laws for real random matrices.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:35:28 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Cui", "Wenping", ""], ["Rocks", "Jason W.", ""], ["Mehta", "Pankaj", ""]]}, {"id": "2012.00673", "submitter": "Andrzej Jaszkiewicz", "authors": "Andrzej Jaszkiewicz", "title": "Modified Dorfman procedure for pool tests with dilution -- COVID-19 case\n  study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of the global COVID-19 pandemic results in unprecedented demand\nfor fast and efficient testing of large numbers of patients for the presence of\nSARS-CoV-2 coronavirus. Beside technical improvements of the cost and speed of\nindividual tests, pool testing may be used to improve efficiency and throughput\nof a population test. Dorfman pool testing procedure is one of the best known\nand studied methods of this kind. This procedure is, however, based on\nunrealistic assumptions that the pool test has perfect sensitivity and the only\nobjective is to minimize the number of tests, and is not well adapted to the\ncase of imperfect pool tests. We propose and analyze a simple modification of\nthis procedure in which test of a pool with negative result is independently\nrepeated up to several times. The proposed procedure is evaluated in a\ncomputational study using recent data about dilution effect for SARS-CoV-2 PCR\ntests, showing that the proposed approach significantly reduces the number of\nfalse negatives with a relatively small increase of the number of tests,\nespecially for small prevalence rates. For example, for prevalence rate 0.001\nthe number of tests could be reduced to 22.1% of individual tests, increasing\nthe expected number of false negatives by no more than 1%, and to 16.8% of\nindividual tests increasing the expected number of false negatives by no more\nthan 10%. At the same time, a similar reduction of the expected number of tests\nin the standard Dorfman procedure would yield 675% and 821% increase of the\nexpected number of false negatives, respectively. This makes the proposed\nprocedure an interesting choice for screening tests in the case of diseases\nlike COVID-19.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:53:08 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 08:15:32 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Jaszkiewicz", "Andrzej", ""]]}, {"id": "2012.00745", "submitter": "Martin Huber", "authors": "Michela Bia, Martin Huber, Luk\\'a\\v{s} Laff\\'ers", "title": "Double machine learning for sample selection models", "comments": "arXiv admin note: text overlap with arXiv:2012.00370,\n  arXiv:2002.12710", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the evaluation of discretely distributed treatments when\noutcomes are only observed for a subpopulation due to sample selection or\noutcome attrition. For identification, we combine a selection-on-observables\nassumption for treatment assignment with either selection-on-observables or\ninstrumental variable assumptions concerning the outcome attrition/sample\nselection process. We also consider dynamic confounding, meaning that\ncovariates that jointly affect sample selection and the outcome may (at least\npartly) be influenced by the treatment. To control in a data-driven way for a\npotentially high dimensional set of pre- and/or post-treatment covariates, we\nadapt the double machine learning framework for treatment evaluation to sample\nselection problems. We make use of (a) Neyman-orthogonal, doubly robust, and\nefficient score functions, which imply the robustness of treatment effect\nestimation to moderate regularization biases in the machine learning-based\nestimation of the outcome, treatment, or sample selection models and (b) sample\nsplitting (or cross-fitting) to prevent overfitting bias. We demonstrate that\nthe proposed estimators are asymptotically normal and root-n consistent under\nspecific regularity conditions concerning the machine learners and investigate\ntheir finite sample properties in a simulation study. We also apply our\nproposed methodology to the Job Corps data for evaluating the effect of\ntraining on hourly wages which are only observed conditional on employment. The\nestimator is available in the causalweight package for the statistical software\nR.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 19:40:21 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 12:44:10 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 09:12:02 GMT"}, {"version": "v4", "created": "Sat, 19 Jun 2021 14:53:34 GMT"}, {"version": "v5", "created": "Thu, 15 Jul 2021 15:55:40 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Bia", "Michela", ""], ["Huber", "Martin", ""], ["Laff\u00e9rs", "Luk\u00e1\u0161", ""]]}, {"id": "2012.00943", "submitter": "Michele Peruzzi", "authors": "Michele Peruzzi, David B. Dunson", "title": "Spatial Multivariate Trees for Big Data Bayesian Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  High resolution geospatial data are challenging because standard\ngeostatistical models based on Gaussian processes are known to not scale to\nlarge data sizes. While progress has been made towards methods that can be\ncomputed more efficiently, considerably less attention has been devoted to big\ndata methods that allow the description of complex relationships between\nseveral outcomes recorded at high resolutions by different sensors. Our\nBayesian multivariate regression models based on spatial multivariate trees\n(SpamTrees) achieve scalability via conditional independence assumptions on\nlatent random effects following a treed directed acyclic graph.\nInformation-theoretic arguments and considerations on computational efficiency\nguide the construction of the tree and the related efficient sampling\nalgorithms in imbalanced multivariate settings. In addition to simulated data\nexamples, we illustrate SpamTrees using a large climate data set which combines\nsatellite data with land-based station data. Source code is available at\nhttps://github.com/mkln/spamtree\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 03:10:28 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Peruzzi", "Michele", ""], ["Dunson", "David B.", ""]]}, {"id": "2012.00960", "submitter": "Gwo Dong Lin", "authors": "Gwo Dong Lin and Xiaoling Dou", "title": "An Identity for Two Integral Transforms Applied to the Uniqueness of a\n  Distribution via its Laplace-Stieltjes Transform", "comments": "22 pages", "journal-ref": "Statistics 2021", "doi": "10.1080/02331888.2021.1893728", "report-no": null, "categories": "math.PR math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is well known that the Laplace-Stieltjes transform of a nonnegative random\nvariable (or random vector) uniquely determines its distribution function. We\nextend this uniqueness theorem by using the Muntz-Szasz Theorem and the\nidentity for the Laplace-Stieltjes and Laplace-Carson transforms of a\ndistribution function. The latter appears for the first time to the best of our\nknowledge. In particular, if X and Y are two nonnegative random variables with\njoint distribution H, then H can be characterized by a suitable set of\ncountably many values of its bivariate Laplace-Stieltjes transform. The general\nhigh-dimensional case is also investigated. Besides, Lerch's uniqueness theorem\nfor conventional Laplace transforms is extended as well. The identity can be\nused to simplify the calculation of Laplace-Stieltjes transforms when the\nunderlying distributions have singular parts. Finally, some examples are given\nto illustrate the characterization results via the uniqueness theorem.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 04:20:34 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 04:31:44 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Lin", "Gwo Dong", ""], ["Dou", "Xiaoling", ""]]}, {"id": "2012.00990", "submitter": "Natalia Nolde", "authors": "Natalia Nolde and Jennifer L. Wadsworth", "title": "Connections between representations for multivariate extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The study of multivariate extremes is dominated by multivariate regular\nvariation, although it is well known that this approach does not provide\nadequate distinction between random vectors whose components are not always\nsimultaneously large. Various alternative dependence measures and\nrepresentations have been proposed, with the most well-known being hidden\nregular variation and the conditional extreme value model. These varying\ndepictions of extremal dependence arise through consideration of different\nparts of the multivariate domain, and particularly exploring what happens when\nextremes of one variable may grow at different rates to other variables. Thus\nfar, these alternative representations have come from distinct sources and\nlinks between them are limited. In this work we elucidate many of the relevant\nconnections through a geometrical approach. In particular, the shape of the\nlimit set of scaled sample clouds in light-tailed margins is shown to provide a\ndescription of several different extremal dependence representations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 06:49:38 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Nolde", "Natalia", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "2012.01099", "submitter": "Steven Nijman", "authors": "Steven WJ Nijman, Jeroen Hoogland, T Katrien J Groenhof, Menno\n  Brandjes, John JL Jacobs, Michiel L Bots, Folkert W Asselbergs, Karel GM\n  Moons, Thomas PA Debray", "title": "Real-time imputation of missing predictor values in clinical practice", "comments": "17 pages, 6 figures, to be published in European Heart Journal -\n  Digital Health, accepted for MEMTAB 2020 conference", "journal-ref": null, "doi": "10.1093/ehjdh/ztaa016", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Use of prediction models is widely recommended by clinical guidelines, but\nusually requires complete information on all predictors that is not always\navailable in daily practice. We describe two methods for real-time handling of\nmissing predictor values when using prediction models in practice. We compare\nthe widely used method of mean imputation (M-imp) to a method that personalizes\nthe imputations by taking advantage of the observed patient characteristics.\nThese characteristics may include both prediction model variables and other\ncharacteristics (auxiliary variables). The method was implemented using\nimputation from a joint multivariate normal model of the patient\ncharacteristics (joint modeling imputation; JMI). Data from two different\ncardiovascular cohorts with cardiovascular predictors and outcome were used to\nevaluate the real-time imputation methods. We quantified the prediction model's\noverall performance (mean squared error (MSE) of linear predictor),\ndiscrimination (c-index), calibration (intercept and slope) and net benefit\n(decision curve analysis). When compared with mean imputation, JMI\nsubstantially improved the MSE (0.10 vs. 0.13), c-index (0.70 vs 0.68) and\ncalibration (calibration-in-the-large: 0.04 vs. 0.06; calibration slope: 1.01\nvs. 0.92), especially when incorporating auxiliary variables. When the\nimputation method was based on an external cohort, calibration deteriorated,\nbut discrimination remained similar. We recommend JMI with auxiliary variables\nfor real-time imputation of missing values, and to update imputation models\nwhen implementing them in new settings or (sub)populations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 11:35:54 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Nijman", "Steven WJ", ""], ["Hoogland", "Jeroen", ""], ["Groenhof", "T Katrien J", ""], ["Brandjes", "Menno", ""], ["Jacobs", "John JL", ""], ["Bots", "Michiel L", ""], ["Asselbergs", "Folkert W", ""], ["Moons", "Karel GM", ""], ["Debray", "Thomas PA", ""]]}, {"id": "2012.01149", "submitter": "Qiwei Li", "authors": "Cong Zhang and Guanghua Xiao and Chul Moon and Min Chen and Qiwei Li", "title": "Bayesian Landmark-based Shape Analysis of Tumor Pathology Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging is a form of technology that has revolutionized the medical\nfield in the past century. In addition to radiology imaging of tumor tissues,\ndigital pathology imaging, which captures histological details in high spatial\nresolution, is fast becoming a routine clinical procedure for cancer diagnosis\nsupport and treatment planning. Recent developments in deep-learning methods\nfacilitate the segmentation of tumor regions at almost the cellular level from\ndigital pathology images. The traditional shape features that were developed\nfor characterizing tumor boundary roughness in radiology are not applicable.\nReliable statistical approaches to modeling tumor shape in pathology images are\nin urgent need. In this paper, we consider the problem of modeling a tumor\nboundary with a closed polygonal chain. A Bayesian landmark-based shape\nanalysis (BayesLASA) model is proposed to partition the polygonal chain into\nmutually exclusive segments to quantify the boundary roughness piecewise. Our\nfully Bayesian inference framework provides uncertainty estimates of both the\nnumber and locations of landmarks. The BayesLASA outperforms a recently\ndeveloped landmark detection model for planar elastic curves in terms of\naccuracy and efficiency. We demonstrate how this model-based analysis can lead\nto sharper inferences than ordinary approaches through a case study on the 246\npathology images from 143 non-small cell lung cancer patients. The case study\nshows that the heterogeneity of tumor boundary roughness predicts patient\nprognosis (p-value < 0.001). This statistical methodology not only presents a\nnew model for characterizing a digitized object's shape features by using its\nlandmarks, but also provides a new perspective for understanding the role of\ntumor surface in cancer progression.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 00:04:09 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Zhang", "Cong", ""], ["Xiao", "Guanghua", ""], ["Moon", "Chul", ""], ["Chen", "Min", ""], ["Li", "Qiwei", ""]]}, {"id": "2012.01236", "submitter": "Denis Agniel", "authors": "Denis Agniel, Layla Parast, Boris Hejblum", "title": "Doubly-robust evaluation of high-dimensional surrogate markers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When evaluating the effectiveness of a treatment, policy, or intervention,\nthe desired measure of effectiveness may be expensive to collect, not routinely\navailable, or may take a long time to occur. In these cases, it is sometimes\npossible to identify a surrogate outcome that can more easily/quickly/cheaply\ncapture the effect of interest. Theory and methods for evaluating the strength\nof surrogate markers have been well studied in the context of a single\nsurrogate marker measured in the course of a randomized clinical study.\nHowever, methods are lacking for quantifying the utility of surrogate markers\nwhen the dimension of the surrogate grows and/or when study data are\nobservational. We propose an efficient nonparametric method for evaluating\nhigh-dimensional surrogate markers in studies where the treatment need not be\nrandomized. Our approach draws on a connection between quantifying the utility\nof a surrogate marker and the most fundamental tools of causal inference --\nnamely, methods for estimating the average treatment effect. We show that\nrecently developed methods for incorporating machine learning methods into the\nestimation of average treatment effects can be used for evaluating surrogate\nmarkers. This allows us to derive limiting asymptotic distributions for key\nquantities, and we demonstrate their good performance in simulation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:24:12 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 01:44:28 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Agniel", "Denis", ""], ["Parast", "Layla", ""], ["Hejblum", "Boris", ""]]}, {"id": "2012.01238", "submitter": "Mehmet Niyazi Cankaya mehmetn", "authors": "Roberto Vila and Mehmet Niyazi \\c{C}ankaya", "title": "A Bimodal Weibull Distribution: Properties and Inference", "comments": "36 pages; 9 figures;5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling is a challenging topic and using parametric models is an important\nstage to reach flexible function for modeling. Weibull distribution has two\nparameters which are shape $\\alpha$ and scale $\\beta$. In this study,\nbimodality parameter is added and so bimodal Weibull distribution is proposed\nby using a quadratic transformation technique used to generate bimodal\nfunctions produced due to using the quadratic expression. The analytical\nsimplicity of Weibull and quadratic form give an advantage to derive a bimodal\nWeibull via constructing normalizing constant. The characteristics and\nproperties of the proposed distribution are examined to show its usability in\nmodeling. After examination as first stage in modeling issue, it is appropriate\nto use bimodal Weibull for modeling data sets. Two estimation methods which are\nmaximum $\\log_q$ likelihood and its special form including objective functions\n$\\log_q(f)$ and $\\log(f)$ are used to estimate the parameters of shape, scale\nand bimodality parameters of the function. The second stage in modeling is\novercome by using heuristic algorithm for optimization of function according to\nparameters due to fact that converging to global point of objective function is\nperformed by heuristic algorithm based on the stochastic optimization. Real\ndata sets are provided to show the modeling competence of the proposed\ndistribution.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:26:59 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Vila", "Roberto", ""], ["\u00c7ankaya", "Mehmet Niyazi", ""]]}, {"id": "2012.01540", "submitter": "Matias Salibian-Barrera", "authors": "Graciela Boente and Matias Salibian-Barrera", "title": "Robust functional principal components for sparse longitudinal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we review existing methods for robust functional principal\ncomponent analysis (FPCA) and propose a new method for FPCA that can be applied\nto longitudinal data where only a few observations per trajectory are\navailable. This method is robust against the presence of atypical observations,\nand can also be used to derive a new non-robust FPCA approach for sparsely\nobserved functional data. We use local regression to estimate the values of the\ncovariance function, taking advantage of the fact that for elliptically\ndistributed random vectors the conditional location parameter of some of its\ncomponents given others is a linear function of the conditioning set. This\nobservation allows us to obtain robust FPCA estimators by using robust local\nregression methods. The finite sample performance of our proposal is explored\nthrough a simulation study that shows that, as expected, the robust method\noutperforms existing alternatives when the data are contaminated. Furthermore,\nwe also see that for samples that do not contain outliers the non-robust\nvariant of our proposal compares favourably to the existing alternative in the\nliterature. A real data example is also presented.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 21:27:20 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Boente", "Graciela", ""], ["Salibian-Barrera", "Matias", ""]]}, {"id": "2012.01615", "submitter": "Zhichao Jiang", "authors": "Zhichao Jiang, Shu Yang, Peng Ding", "title": "Multiply robust estimation of causal effects under principal\n  ignorability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference concerns not only the average effect of the treatment on the\noutcome but also the underlying mechanism through an intermediate variable of\ninterest. Principal stratification characterizes such mechanism by targeting\nsubgroup causal effects within principal strata, which are defined by the joint\npotential values of an intermediate variable. Due to the fundamental problem of\ncausal inference, principal strata are inherently latent, rendering it\nchallenging to identify and estimate subgroup effects within them. A line of\nresearch leverages the principal ignorability assumption that the latent\nprincipal strata are mean independent of the potential outcomes conditioning on\nthe observed covariates. Under principal ignorability, we derive various\nnonparametric identification formulas for causal effects within principal\nstrata in observational studies, which motivate estimators relying on the\ncorrect specifications of different parts of the observed-data distribution.\nAppropriately combining these estimators further yields new triply robust\nestimators for the causal effects within principal strata. These new estimators\nare consistent if two of the treatment, intermediate variable, and outcome\nmodels are correctly specified, and they are locally efficient if all three\nmodels are correctly specified. We show that these estimators arise naturally\nfrom either the efficient influence functions in the semiparametric theory or\nthe model-assisted estimators in the survey sampling theory. We evaluate\ndifferent estimators based on their finite-sample performance through\nsimulation, apply them to two observational studies, and implement them in an\nopen-source software package.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 00:42:45 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Jiang", "Zhichao", ""], ["Yang", "Shu", ""], ["Ding", "Peng", ""]]}, {"id": "2012.01619", "submitter": "Nicholas Tierney", "authors": "Nicholas J Tierney, Dianne Cook, Tania Prvan", "title": "brolgar: An R package to BRowse Over Longitudinal Data Graphically and\n  Analytically in R", "comments": "19 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal (panel) data provide the opportunity to examine temporal\npatterns of individuals, because measurements are collected on the same person\nat different, and often irregular, time points. The data is typically\nvisualised using a \"spaghetti plot\", where a line plot is drawn for each\nindividual. When overlaid in one plot, it can have the appearance of a bowl of\nspaghetti. With even a small number of subjects, these plots are too overloaded\nto be read easily. The interesting aspects of individual differences are lost\nin the noise. Longitudinal data is often modelled with a hierarchical linear\nmodel to capture the overall trends, and variation among individuals, while\naccounting for various levels of dependence. However, these models can be\ndifficult to fit, and can miss unusual individual patterns. Better visual tools\ncan help to diagnose longitudinal models, and better capture the individual\nexperiences. This paper introduces the R package, brolgar (BRowse over\nLongitudinal data Graphically and Analytically in R), which provides tools to\nidentify and summarise interesting individual patterns in longitudinal data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 00:55:42 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Tierney", "Nicholas J", ""], ["Cook", "Dianne", ""], ["Prvan", "Tania", ""]]}, {"id": "2012.01643", "submitter": "Yanfei Kang", "authors": "Yanfei Kang, Wei Cao, Fotios Petropoulos, Feng Li", "title": "Forecast with Forecasts: Diversity Matters", "comments": "23 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Forecast combination has been widely applied in the last few decades to\nimprove forecast accuracy. In recent years, the idea of using time series\nfeatures to construct forecast combination model has flourished in the\nforecasting area. Although this idea has been proved to be beneficial in\nseveral forecast competitions such as the M3 and M4 competitions, it may not be\npractical in many situations. For example, the task of selecting appropriate\nfeatures to build forecasting models can be a big challenge for many\nresearchers. Even if there is one acceptable way to define the features,\nexisting features are estimated based on the historical patterns, which are\ndoomed to change in the future, or infeasible in the case of limited historical\ndata. In this work, we suggest a change of focus from the historical data to\nthe produced forecasts to extract features. We calculate the diversity of a\npool of models based on the corresponding forecasts as a decisive feature and\nuse meta-learning to construct diversity-based forecast combination models. A\nrich set of time series are used to evaluate the performance of the proposed\nmethod. Experimental results show that our diversity-based forecast combination\nframework not only simplifies the modelling process but also achieves superior\nforecasting performance.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:14:02 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 12:09:43 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Kang", "Yanfei", ""], ["Cao", "Wei", ""], ["Petropoulos", "Fotios", ""], ["Li", "Feng", ""]]}, {"id": "2012.01697", "submitter": "Yanbo Tang", "authors": "Yanbo Tang, Radu Craiu and Lei Sun", "title": "General Behaviour of P-Values Under the Null and Alternative", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hypothesis testing results often rely on simple, yet important assumptions\nabout the behaviour of the distribution of p-values under the null and the\nalternative. We examine tests for one dimensional parameters of interest that\nconverge to a normal distribution, possibly in the presence of nuisance\nparameters, and characterize the distribution of the p-values using techniques\nfrom the higher order asymptotics literature. We show that commonly held\nbeliefs regarding the distribution of p-values are misleading when the variance\nand location of the test statistic are not well-calibrated or when the higher\norder cumulants of the test statistic are not negligible. Corrected tests are\nproposed and are shown to perform better than their first order counterparts in\ncertain settings.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 04:36:39 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Tang", "Yanbo", ""], ["Craiu", "Radu", ""], ["Sun", "Lei", ""]]}, {"id": "2012.01758", "submitter": "Steven Siwei Ye", "authors": "Steven Siwei Ye and Oscar Hernan Madrid Padilla", "title": "Non-parametric Quantile Regression via the K-NN Fused Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Quantile regression is a statistical method for estimating conditional\nquantiles of a response variable. In addition, for mean estimation, it is well\nknown that quantile regression is more robust to outliers than $l_2$-based\nmethods. By using the fused lasso penalty over a $K$-nearest neighbors graph,\nwe propose an adaptive quantile estimator in a non-parametric setup. We show\nthat the estimator attains optimal rate of $n^{-1/d}$ up to a logarithmic\nfactor, under mild assumptions on the data generation mechanism of the\n$d$-dimensional data. We develop algorithms to compute the estimator and\ndiscuss methodology for model selection. Numerical experiments on simulated and\nreal data demonstrate clear advantages of the proposed estimator over state of\nthe art methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 08:43:20 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 03:03:31 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 10:27:42 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ye", "Steven Siwei", ""], ["Padilla", "Oscar Hernan Madrid", ""]]}, {"id": "2012.01807", "submitter": "Wagner Barreto-Souza", "authors": "Fernando de S. Bastos, Wagner Barreto-Souza and Marc G. Genton", "title": "A Generalized Heckman Model With Varying Sample Selection Bias and\n  Dispersion Parameters", "comments": "Paper submitted for publication", "journal-ref": "Statistica Sinica (2021)", "doi": "10.5705/ss.202021.0068", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many proposals have emerged as alternatives to the Heckman selection model,\nmainly to address the non-robustness of its normal assumption. The 2001 Medical\nExpenditure Panel Survey data is often used to illustrate this non-robustness\nof the Heckman model. In this paper, we propose a generalization of the Heckman\nsample selection model by allowing the sample selection bias and dispersion\nparameters to depend on covariates. We show that the non-robustness of the\nHeckman model may be due to the assumption of the constant sample selection\nbias parameter rather than the normality assumption. Our proposed methodology\nallows us to understand which covariates are important to explain the sample\nselection bias phenomenon rather than to only form conclusions about its\npresence. We explore the inferential aspects of the maximum likelihood\nestimators (MLEs) for our proposed generalized Heckman model. More\nspecifically, we show that this model satisfies some regularity conditions such\nthat it ensures consistency and asymptotic normality of the MLEs. Proper score\nresiduals for sample selection models are provided, and model adequacy is\naddressed. Simulated results are presented to check the finite-sample behavior\nof the estimators and to verify the consequences of not considering varying\nsample selection bias and dispersion parameters. We show that the normal\nassumption for analyzing medical expenditure data is suitable and that the\nconclusions drawn using our approach are coherent with findings from prior\nliterature. Moreover, we identify which covariates are relevant to explain the\npresence of sample selection bias in this important dataset.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 10:19:12 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bastos", "Fernando de S.", ""], ["Barreto-Souza", "Wagner", ""], ["Genton", "Marc G.", ""]]}, {"id": "2012.01937", "submitter": "Rajdip Nayek", "authors": "Rajdip Nayek, Ramon Fuentes, Keith Worden, Elizabeth J. Cross", "title": "On spike-and-slab priors for Bayesian equation discovery of nonlinear\n  dynamical systems via sparse linear regression", "comments": null, "journal-ref": null, "doi": "10.1016/j.ymssp.2021.107986", "report-no": null, "categories": "stat.ME cs.SY eess.SY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the use of spike-and-slab (SS) priors for discovering\ngoverning differential equations of motion of nonlinear structural dynamic\nsystems. The problem of discovering governing equations is cast as that of\nselecting relevant variables from a predetermined dictionary of basis variables\nand solved via sparse Bayesian linear regression. The SS priors, which belong\nto a class of discrete-mixture priors and are known for their strong\nsparsifying (or shrinkage) properties, are employed to induce sparse solutions\nand select relevant variables. Three different variants of SS priors are\nexplored for performing Bayesian equation discovery. As the posteriors with SS\npriors are analytically intractable, a Markov chain Monte Carlo (MCMC)-based\nGibbs sampler is employed for drawing posterior samples of the model\nparameters; the posterior samples are used for variable selection and parameter\nestimation in equation discovery. The proposed algorithm has been applied to\nfour systems of engineering interest, which include a baseline linear system,\nand systems with cubic stiffness, quadratic viscous damping, and Coulomb\ndamping. The results demonstrate the effectiveness of the SS priors in\nidentifying the presence and type of nonlinearity in the system. Additionally,\ncomparisons with the Relevance Vector Machine (RVM) - that uses a Student's-t\nprior - indicate that the SS priors can achieve better model selection\nconsistency, reduce false discoveries, and derive models that have superior\npredictive accuracy. Finally, the Silverbox experimental benchmark is used to\nvalidate the proposed methodology.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 14:14:31 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 08:45:57 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Nayek", "Rajdip", ""], ["Fuentes", "Ramon", ""], ["Worden", "Keith", ""], ["Cross", "Elizabeth J.", ""]]}, {"id": "2012.02021", "submitter": "S. Yaser Samadi", "authors": "Hadi Safari-Katesari, S. Yaser Samadi, Samira Zaroudi", "title": "Modeling Count Data via Copulas", "comments": "33 pages", "journal-ref": "Statistics 2020", "doi": "10.1080/02331888.2020.1867140", "report-no": "2020", "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Copula models have been widely used to model the dependence between\ncontinuous random variables, but modeling count data via copulas has recently\nbecome popular in the statistics literature. Spearman's rho is an appropriate\nand effective tool to measure the degree of dependence between two random\nvariables. In this paper, we derived the population version of Spearman's rho\ncorrelation via copulas when both random variables are discrete. The\nclosed-form expressions of the Spearman correlation are obtained for some\ncopulas of simple structure such as Archimedean copulas with different marginal\ndistributions. We derive the upper bound and the lower bound of the Spearman's\nrho for Bernoulli random variables. Then, the proposed Spearman's rho\ncorrelations are compared with their corresponding Kendall's tau values. We\ncharacterize the functional relationship between these two measures of\ndependence in some special cases. An extensive simulation study is conducted to\ndemonstrate the validity of our theoretical results. Finally, we propose a\nbivariate copula regression model to analyze the count data of a \\emph{cervical\ncancer} dataset.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 16:04:51 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Safari-Katesari", "Hadi", ""], ["Samadi", "S. Yaser", ""], ["Zaroudi", "Samira", ""]]}, {"id": "2012.02025", "submitter": "Rohit Patra", "authors": "Rohit K. Patra, Moulinath Banerjee, and George Michailidis", "title": "A semi-parametric model for target localization in distributed systems", "comments": "54 pages and 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed systems serve as a key technological infrastructure for\nmonitoring diverse systems across space and time. Examples of their widespread\napplications include: precision agriculture, surveillance, ecosystem and\nphysical infrastructure monitoring, animal behavior and tracking, disaster\nresponse and recovery to name a few. Such systems comprise of a large number of\nsensor devices at fixed locations, wherein each individual sensor obtains\nmeasurements that are subsequently fused and processed at a central processing\nnode. A key problem for such systems is to detect targets and identify their\nlocations, for which a large body of literature has been developed focusing\nprimarily on employing parametric models for signal attenuation from target to\ndevice. In this paper, we adopt a nonparametric approach that only assumes that\nthe signal is nonincreasing as function of the distance between the sensor and\nthe target. We propose a simple tuning parameter free estimator for the target\nlocation, namely, the simple score estimator (SSCE). We show that the SSCE is\n$\\sqrt{n}$ consistent and has a Gaussian limit distribution which can be used\nto construct asymptotic confidence regions for the location of the target. We\nstudy the performance of the SSCE through extensive simulations, and finally\ndemonstrate an application to target detection in a video surveillance data\nset.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 16:09:16 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Patra", "Rohit K.", ""], ["Banerjee", "Moulinath", ""], ["Michailidis", "George", ""]]}, {"id": "2012.02074", "submitter": "Xinyue Qi", "authors": "Xinyue Qi, Shouhao Zhou, Martyn Plummer", "title": "A Note on Bayesian Modeling Specification of Censored Data in JAGS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Just Another Gibbs Sampling (JAGS) is a convenient tool to draw posterior\nsamples using Markov Chain Monte Carlo for Bayesian modeling. However, the\nbuilt-in function dinterval() to model censored data misspecifies the\ncomputation of deviance function, which may limit its usage to perform\nlikelihood based model comparison. To establish an automatic approach to\nspecify the correct deviance function in JAGS, we propose a simple alternative\nmodeling strategy to implement Bayesian model selection for analysis of\ncensored outcomes. The proposed approach is applicable to a broad spectrum of\ndata types, which include survival data and many other right-, left- and\ninterval-censored Bayesian model structures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:06:11 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Qi", "Xinyue", ""], ["Zhou", "Shouhao", ""], ["Plummer", "Martyn", ""]]}, {"id": "2012.02100", "submitter": "Mikael Mieskolainen", "authors": "Mikael Mieskolainen, Robert Bainbridge, Oliver Buchmueller, Louis\n  Lyons, Nicholas Wardle", "title": "Statistical techniques to estimate the SARS-CoV-2 infection fatality\n  rate", "comments": "50 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determination of the infection fatality rate (IFR) for the novel\nSARS-CoV-2 coronavirus is a key aim for many of the field studies that are\ncurrently being undertaken in response to the pandemic. The IFR together with\nthe basic reproduction number $R_0$, are the main epidemic parameters\ndescribing severity and transmissibility of the virus, respectively. The IFR\ncan be also used as a basis for estimating and monitoring the number of\ninfected individuals in a population, which may be subsequently used to inform\npolicy decisions relating to public health interventions and lockdown\nstrategies. The interpretation of IFR measurements requires the calculation of\nconfidence intervals. We present a number of statistical methods that are\nrelevant in this context and develop an inverse problem formulation to\ndetermine correction factors to mitigate time-dependent effects that can lead\nto biased IFR estimates. We also review a number of methods to combine IFR\nestimates from multiple independent studies, provide example calculations\nthroughout this note and conclude with a summary and \"best practice\"\nrecommendations. The developed code is available online.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:15:13 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Mieskolainen", "Mikael", ""], ["Bainbridge", "Robert", ""], ["Buchmueller", "Oliver", ""], ["Lyons", "Louis", ""], ["Wardle", "Nicholas", ""]]}, {"id": "2012.02103", "submitter": "Tim Friede", "authors": "Jan Beyersmann and Tim Friede and Claudia Schmoor", "title": "Design aspects of COVID-19 treatment trials: Improving probability and\n  time of favourable events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a reaction to the pandemic of the severe acute respiratory syndrome\ncoronavirus 2 (SARS-CoV-2), a multitude of clinical trials for the treatment of\nSARS-CoV-2 or the resulting corona disease (COVID-19) are globally at various\nstages from planning to completion. Although some attempts were made to\nstandardize study designs, this was hindered by the ferocity of the pandemic\nand the need to set up trials quickly. We take the view that a successful\ntreatment of COVID-19 patients (i) increases the probability of a recovery or\nimprovement within a certain time interval, say 28 days; (ii) aims to expedite\nfavourable events within this time frame; and (iii) does not increase mortality\nover this time period. On this background we discuss the choice of endpoint and\nits analysis. Furthermore, we consider consequences of this choice for other\ndesign aspects including sample size and power and provide some guidance on the\napplication of adaptive designs in this particular context.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 12:35:15 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Beyersmann", "Jan", ""], ["Friede", "Tim", ""], ["Schmoor", "Claudia", ""]]}, {"id": "2012.02122", "submitter": "Nir Keret", "authors": "Nir Keret and Malka Gorfine", "title": "Optimal Cox Regression Subsampling Procedure with Rare Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Massive sized survival datasets are becoming increasingly prevalent with the\ndevelopment of the healthcare industry. Such datasets pose computational\nchallenges unprecedented in traditional survival analysis use-cases. A popular\nway for coping with massive datasets is downsampling them to a more manageable\nsize, such that the computational resources can be afforded by the researcher.\nCox proportional hazards regression has remained one of the most popular\nstatistical models for the analysis of survival data to-date. This work\naddresses the settings of right censored and possibly left truncated data with\nrare events, such that the observed failure times constitute only a small\nportion of the overall sample. We propose Cox regression subsampling-based\nestimators that approximate their full-data partial-likelihood-based\ncounterparts, by assigning optimal sampling probabilities to censored\nobservations, and including all observed failures in the analysis. Asymptotic\nproperties of the proposed estimators are established under suitable regularity\nconditions, and simulation studies are carried out to evaluate the finite\nsample performance of the estimators. We further apply our procedure on\nUK-biobank colorectal cancer genetic and environmental risk factors.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:58:27 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 15:15:17 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Keret", "Nir", ""], ["Gorfine", "Malka", ""]]}, {"id": "2012.02130", "submitter": "Tianfang Zhang", "authors": "Tianfang Zhang and Rasmus Bokrantz and Jimmy Olsson", "title": "A similarity-based Bayesian mixture-of-experts model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new nonparametric mixture-of-experts model for multivariate\nregression problems, inspired by the probabilistic $k$-nearest neighbors\nalgorithm. Using a conditionally specified model, predictions for out-of-sample\ninputs are based on similarities to each observed data point, yielding\npredictive distributions represented by Gaussian mixtures. Posterior inference\nis performed on the parameters of the mixture components as well as the\ndistance metric using a mean-field variational Bayes algorithm accompanied with\na stochastic gradient-based optimization procedure. The proposed method is\nespecially advantageous in settings where inputs are of relatively high\ndimension in comparison to the data size, where input--output relationships are\ncomplex, and where predictive distributions may be skewed or multimodal.\nComputational studies on two synthetic datasets and one dataset comprising dose\nstatistics of radiation therapy treatment plans show that our\nmixture-of-experts method performs similarly or better than a conditional\nDirichlet process mixture model both in terms of validation metrics and visual\ninspection.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:08:30 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 10:14:23 GMT"}, {"version": "v3", "created": "Sun, 4 Jul 2021 12:33:04 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhang", "Tianfang", ""], ["Bokrantz", "Rasmus", ""], ["Olsson", "Jimmy", ""]]}, {"id": "2012.02155", "submitter": "Kristian Bj{\\o}rn Hessellund", "authors": "Kristian Bj{\\o}rn Hessellund, Ganggang Xu, Yongtao Guan and Rasmus\n  Waagepetersen", "title": "Second order semi-parametric inference for multivariate log Gaussian Cox\n  processes", "comments": "32 pages including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a new approach to inferring the second order properties\nof a multivariate log Gaussian Cox process (LGCP) with a complex intensity\nfunction. We assume a semi-parametric model for the multivariate intensity\nfunction containing an unspecified complex factor common to all types of\npoints. Given this model we exploit the availability of several types of points\nto construct a second-order conditional composite likelihood to infer the pair\ncorrelation and cross pair correlation functions of the LGCP. Crucially this\nlikelihood does not depend on the unspecified part of the intensity function.\nWe also introduce a cross validation method for model selection and an\nalgorithm for regularized inference that can be used to obtain sparse models\nfor cross pair correlation functions. The methodology is applied to simulated\ndata as well as data examples from microscopy and criminology. This shows how\nthe new approach outperforms existing alternatives where the intensity\nfunctions are estimated non-parametrically.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:42:09 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Hessellund", "Kristian Bj\u00f8rn", ""], ["Xu", "Ganggang", ""], ["Guan", "Yongtao", ""], ["Waagepetersen", "Rasmus", ""]]}, {"id": "2012.02168", "submitter": "Andres Christen", "authors": "Marcos A. Capistr\\'an, Antonio Capella and J. Andr\\'es Christen", "title": "Filtering and improved Uncertainty Quantification in the dynamic\n  estimation of effective reproduction numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The effective reproduction number $R_t$ measures an infectious disease's\ntransmissibility as the number of secondary infections in one reproduction time\nin a population having both susceptible and non-susceptible hosts. Current\napproaches do not quantify the uncertainty correctly in estimating $R_t$, as\nexpected by the observed variability in contagion patterns. We elaborate on the\nBayesian estimation of $R_t$ by improving on the Poisson sampling model of Cori\net al. (2013). By adding an autoregressive latent process, we build a Dynamic\nLinear Model on the log of observed $R_t$s, resulting in a filtering type\nBayesian inference. We use a conjugate analysis, and all calculations are\nexplicit. Results show an improved uncertainty quantification on the estimation\nof $R_t$'s, with a reliable method that could safely be used by non-experts and\nwithin other forecasting systems. We illustrate our approach with recent data\nfrom the current COVID19 epidemic in Mexico.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:48:46 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Capistr\u00e1n", "Marcos A.", ""], ["Capella", "Antonio", ""], ["Christen", "J. Andr\u00e9s", ""]]}, {"id": "2012.02196", "submitter": "Chibuzor Nnanatu", "authors": "Chibuzor C. Nnanatu, Murray S. A. Thompson, Michael A. Spence, Elena\n  Couce, Jeroen van der Kooij and Christopher P. Lynam", "title": "Bayesian hierarchical space-time models to improve multispecies\n  assessment by combining observations from disparate fish surveys", "comments": "32 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many wild species affected by human activities require multiple surveys with\ndiffering designs to capture behavioural response to wide ranging habitat\nconditions and map and quantify them. While data from for example intersecting\nbut disparate fish surveys using different gear, are widely available,\ndifferences in design and methodology often limit their integration. Novel\nstatistical approaches which can draw on observations from diverse sources\ncould enhance our understanding of multiple species distributions\nsimultaneously and thus provide vital evidence needed to conserve their\npopulations and biodiversity at large. Using a novel Bayesian hierarchical\nbinomial-lognormal hurdle modelling approach within the INLA-SPDE framework, we\ncombined and analysed acoustic and bottom trawl survey data for herring, sprat\nand northeast Atlantic mackerel in the North Sea. These models were implemented\nusing INLA-SPDE techniques. By accounting for gear-specific efficiencies across\nsurveys in addition to increased spatial coverage, we gained larger statistical\npower with greatly minimised uncertainties in estimation. Our statistical\napproach provides a methodological development to improve the evidence base for\nmultispecies assessment and marine ecosystem-based management. And on a broader\nscale, it could be readily applied where disparate biological surveys and\nsampling methods intersect, e.g. to provide information on biodiversity\npatterns using global datasets of species distributions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 22:04:35 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Nnanatu", "Chibuzor C.", ""], ["Thompson", "Murray S. A.", ""], ["Spence", "Michael A.", ""], ["Couce", "Elena", ""], ["van der Kooij", "Jeroen", ""], ["Lynam", "Christopher P.", ""]]}, {"id": "2012.02293", "submitter": "Felipe Medina-Aguayo", "authors": "Felipe J Medina-Aguayo, J Andr\\'es Christen", "title": "Penalised t-walk MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Handling multimodality that commonly arises from complicated statistical\nmodels remains a challenge. Current Markov chain Monte Carlo (MCMC) methodology\ntackling this subject is based on an ensemble of chains targeting a product of\npower-tempered distributions. Despite the theoretical validity of such methods,\npractical implementations typically suffer from bad mixing and slow convergence\ndue to the high-computation cost involved. In this work we study novel\nextensions of the t-walk algorithm, an existing MCMC method that is inexpensive\nand invariant to affine transformations of the state space, for dealing with\nmultimodal distributions. We acknowledge that the effectiveness of the new\nmethod will be problem dependent and might struggle in complex scenarios; for\nsuch cases we propose a post-processing technique based on pseudo-marginal\ntheory for combining isolated samples.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 21:56:13 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Medina-Aguayo", "Felipe J", ""], ["Christen", "J Andr\u00e9s", ""]]}, {"id": "2012.02302", "submitter": "Cai Li", "authors": "Cai Li, Luo Xiao, Sheng Luo", "title": "Joint Model for Survival and Multivariate Sparse Functional Data with\n  Application to a Study of Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": "10.1111/biom.13427", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Studies of Alzheimer's disease (AD) often collect multiple longitudinal\nclinical outcomes, which are correlated and predictive of AD progression. It is\nof great scientific interest to investigate the association between the\noutcomes and time to AD onset. We model the multiple longitudinal outcomes as\nmultivariate sparse functional data and propose a functional joint model\nlinking multivariate functional data to event time data. In particular, we\npropose a multivariate functional mixed model (MFMM) to identify the shared\nprogression pattern and outcome-specific progression patterns of the outcomes,\nwhich enables more interpretable modeling of associations between outcomes and\nAD onset. The proposed method is applied to the Alzheimer's Disease\nNeuroimaging Initiative study (ADNI) and the functional joint model sheds new\nlight on inference of five longitudinal outcomes and their associations with AD\nonset. Simulation studies also confirm the validity of the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 22:04:04 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Cai", ""], ["Xiao", "Luo", ""], ["Luo", "Sheng", ""]]}, {"id": "2012.02307", "submitter": "Juan Sosa", "authors": "Juan Sosa, Lina Buitrago", "title": "A Review of Latent Space Models for Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a review on both fundamentals of social networks\nand latent space modeling. The former discusses important topics related to\nnetwork description, including vertex characteristics and network structure;\nwhereas the latter articulates relevant advances in network modeling, including\nrandom graph models, generalized random graph models, exponential random graph\nmodels, and social space models. We discuss in detail several latent space\nmodels provided in literature, providing special attention to distance, class,\nand eigen models in the context of undirected, binary networks. In addition, we\nalso examine empirically the behavior of these models in terms of prediction\nand goodness-of-fit using more than twenty popular datasets of the network\nliterature.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 22:17:22 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Sosa", "Juan", ""], ["Buitrago", "Lina", ""]]}, {"id": "2012.02378", "submitter": "Ying Yuan", "authors": "Liyun Jiang, Lei Nie, Fangrong Yan, and Ying Yuan", "title": "Optimal Bayesian hierarchical model to accelerate the development of\n  tissue-agnostic drugs and basket trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tissue-agnostic trials enroll patients based on their genetic biomarkers, not\ntumor type, in an attempt to determine if a new drug can successfully treat\ndisease conditions based on biomarkers. The Bayesian hierarchical model (BHM)\nprovides an attractive approach to design phase II tissue-agnostic trials by\nallowing information borrowing across multiple disease types. In this article,\nwe elucidate two intrinsic and inevitable issues that may limit the use of BHM\nto tissue-agnostic trials: sensitivity to the prior specification of the\nshrinkage parameter and the competing \"interest\" among disease types in\nincreasing power and controlling type I error. To address these issues, we\npropose the optimal BHM (OBHM) approach. With OBHM, we first specify a flexible\nutility function to quantify the tradeoff between type I error and power across\ndisease type based on the study objectives, and then we select the prior of the\nshrinkage parameter to optimize the utility function of clinical and regulatory\ninterest. OBMH effectively balances type I and II errors, addresses the\nsensitivity of the prior selection, and reduces the \"unwarranted\" subjectivity\nin the prior selection. Simulation study shows that the resulting OBHM and its\nextensions, clustered OBHM (COBHM) and adaptive OBHM (AOBHM), have desirable\noperating characteristics, outperforming some existing methods with better\nbalanced power and type I error control. Our method provides a systematic,\nrigorous way to apply BHM and solve the common problem of blindingly using a\nnon-informative inverse-gamma prior (with a large variance) or priors\narbitrarily chosen that may lead to pathological statistical properties.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 03:12:09 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Jiang", "Liyun", ""], ["Nie", "Lei", ""], ["Yan", "Fangrong", ""], ["Yuan", "Ying", ""]]}, {"id": "2012.02395", "submitter": "Peter Hansen", "authors": "Ilya Archakov and Peter Reinhard Hansen", "title": "A New Parametrization of Correlation Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM q-fin.ST stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel parametrization of the correlation matrix. The\nreparametrization facilitates modeling of correlation and covariance matrices\nby an unrestricted vector, where positive definiteness is an innate property.\nThis parametrization can be viewed as a generalization of Fisther's\nZ-transformation to higher dimensions and has a wide range of potential\napplications. An algorithm for reconstructing the unique n x n correlation\nmatrix from any d-dimensional vector (with d = n(n-1)/2) is provided, and we\nderive its numerical complexity.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 04:14:47 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Archakov", "Ilya", ""], ["Hansen", "Peter Reinhard", ""]]}, {"id": "2012.02635", "submitter": "Fatemeh Asgari", "authors": "Fatemeh Asgari and Mohammad Hossein Alamatsaz and Valeria Vitelli and\n  Saeed Hayati", "title": "Latent function-on-scalar regression models for observed sequences of\n  binary data: a restricted likelihood approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a functional regression setting where the random\nresponse curve is unobserved, and only its dichotomized version observed at a\nsequence of correlated binary data is available. We propose a practical\ncomputational framework for maximum likelihood analysis via the parameter\nexpansion technique. Compared to existing methods, our proposal relies on the\nuse of a complete data likelihood, with the advantage of being able to handle\nnon-equally spaced and missing observations effectively. The proposed method is\nused in the Function-on-Scalar regression setting, with the latent response\nvariable being a Gaussian random element taking values in a separable Hilbert\nspace. Smooth estimations of functional regression coefficients and principal\ncomponents are provided by introducing an adaptive MCEM algorithm that\ncircumvents selecting the smoothing parameters. Finally, the performance of our\nnovel method is demonstrated by various simulation studies and on a real case\nstudy. The proposed method is implemented in the R package dfrr.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 14:52:26 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Asgari", "Fatemeh", ""], ["Alamatsaz", "Mohammad Hossein", ""], ["Vitelli", "Valeria", ""], ["Hayati", "Saeed", ""]]}, {"id": "2012.02698", "submitter": "Peter Hansen", "authors": "Ilya Archakov and Peter Reinhard Hansen", "title": "A Canonical Representation of Block Matrices with Applications to\n  Covariance and Correlation Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM q-fin.CP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We obtain a canonical representation for block matrices. The representation\nfacilitates simple computation of the determinant, the matrix inverse, and\nother powers of a block matrix, as well as the matrix logarithm and the matrix\nexponential. These results are particularly useful for block covariance and\nblock correlation matrices, where evaluation of the Gaussian log-likelihood and\nestimation are greatly simplified. We illustrate this with an empirical\napplication using a large panel of daily asset returns. Moreover, the\nrepresentation paves new ways to regularizing large covariance/correlation\nmatrices and to test block structures in matrices.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 16:06:03 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Archakov", "Ilya", ""], ["Hansen", "Peter Reinhard", ""]]}, {"id": "2012.02717", "submitter": "Zhimei Ren", "authors": "Zhimei Ren, Yuting Wei and Emmanuel Cand\\`es", "title": "Derandomizing Knockoffs", "comments": "35 pages, 32 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-X knockoffs is a general procedure that can leverage any feature\nimportance measure to produce a variable selection algorithm, which discovers\ntrue effects while rigorously controlling the number or fraction of false\npositives. Model-X knockoffs is a randomized procedure which relies on the\none-time construction of synthetic (random) variables. This paper introduces a\nderandomization method by aggregating the selection results across multiple\nruns of the knockoffs algorithm. The derandomization step is designed to be\nflexible and can be adapted to any variable selection base procedure to yield\nstable decisions without compromising statistical power. When applied to the\nbase procedure of Janson et al. (2016), we prove that derandomized knockoffs\ncontrols both the per family error rate (PFER) and the k family-wise error rate\n(k-FWER). Further, we carry out extensive numerical studies demonstrating tight\ntype-I error control and markedly enhanced power when compared with alternative\nvariable selection algorithms. Finally, we apply our approach to multi-stage\ngenome-wide association studies of prostate cancer and report locations on the\ngenome that are significantly associated with the disease. When\ncross-referenced with other studies, we find that the reported associations\nhave been replicated.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 17:06:24 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Ren", "Zhimei", ""], ["Wei", "Yuting", ""], ["Cand\u00e8s", "Emmanuel", ""]]}, {"id": "2012.02845", "submitter": "Zhichao Jiang", "authors": "Kosuke Imai, Zhichao Jiang, James Greiner, Ryan Halen, Sooahn Shin", "title": "Experimental Evaluation of Algorithm-Assisted Human Decision-Making:\n  Application to Pretrial Public Safety Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite an increasing reliance on fully-automated algorithmic decision making\nin our day-to-day lives, human beings still make highly consequential\ndecisions. As frequently seen in business, healthcare, and public policy,\nrecommendations produced by algorithms are provided to human decision-makers in\norder to guide their decisions. While there exists a fast-growing literature\nevaluating the bias and fairness of such algorithmic recommendations, an\noverlooked question is whether they help humans make better decisions. We\ndevelop a statistical methodology for experimentally evaluating the causal\nimpacts of algorithmic recommendations on human decisions. We also show how to\nexamine whether algorithmic recommendations improve the fairness of human\ndecisions and derive the optimal decisions under various settings. We apply the\nproposed methodology to the first-ever randomized controlled trial that\nevaluates the pretrial Public Safety Assessment (PSA) in the criminal justice\nsystem. A goal of the PSA is to help judges decide which arrested individuals\nshould be released. We find that the PSA provision has little overall impact on\nthe judge's decisions and subsequent arrestee behavior. However, our analysis\nprovides some potentially suggestive evidence that the PSA may help avoid\nunnecessarily harsh decisions for female arrestees regardless of their risk\nlevels while it encourages the judge to make stricter decisions for male\narrestees who are deemed to be risky. In terms of fairness, the PSA appears to\nincrease the gender bias against males while having little effect on the\nexisting racial biases of the judge's decisions against non-white males.\nFinally, we find that the PSA's recommendations might be too severe unless the\ncost of a new crime is sufficiently higher than the cost of a decision that may\nresult in an unnecessary incarceration.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 20:48:44 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Imai", "Kosuke", ""], ["Jiang", "Zhichao", ""], ["Greiner", "James", ""], ["Halen", "Ryan", ""], ["Shin", "Sooahn", ""]]}, {"id": "2012.02901", "submitter": "Dmitrii Ostrovskii", "authors": "Dmitrii M. Ostrovskii, Mohamed Ndaoud, Adel Javanmard, Meisam\n  Razaviyayn", "title": "Near-Optimal Procedures for Model Discrimination with Non-Disclosure\n  Properties", "comments": "52 pages, 2 figures; corrected the proof of the lower bound; added\n  new applications and the Fisher information-based argument in Appendix F", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $\\theta_0,\\theta_1 \\in \\mathbb{R}^d$ be the population risk minimizers\nassociated to some loss $\\ell:\\mathbb{R}^d\\times \\mathcal{Z}\\to\\mathbb{R}$ and\ntwo distributions $\\mathbb{P}_0,\\mathbb{P}_1$ on $\\mathcal{Z}$. The models\n$\\theta_0,\\theta_1$ are unknown, and $\\mathbb{P}_0,\\mathbb{P}_1$ can be\naccessed by drawing i.i.d samples from them. Our work is motivated by the\nfollowing model discrimination question: \"What sizes of the samples from\n$\\mathbb{P}_0$ and $\\mathbb{P}_1$ allow to distinguish between the two\nhypotheses $\\theta^*=\\theta_0$ and $\\theta^*=\\theta_1$ for given\n$\\theta^*\\in\\{\\theta_0,\\theta_1\\}$?\" Making the first steps towards answering\nit in full generality, we first consider the case of a well-specified linear\nmodel with squared loss. Here we provide matching upper and lower bounds on the\nsample complexity as given by $\\min\\{1/\\Delta^2,\\sqrt{r}/\\Delta\\}$ up to a\nconstant factor; here $\\Delta$ is a measure of separation between\n$\\mathbb{P}_0$ and $\\mathbb{P}_1$ and $r$ is the rank of the design covariance\nmatrix. We then extend this result in two directions: (i) for general\nparametric models in asymptotic regime; (ii) for generalized linear models in\nsmall samples ($n\\le r$) under weak moment assumptions. In both cases we derive\nsample complexity bounds of a similar form while allowing for model\nmisspecification. In fact, our testing procedures only access $\\theta^*$ via a\ncertain functional of empirical risk. In addition, the number of observations\nthat allows us to reach statistical confidence does not allow to \"resolve\" the\ntwo models $-$ that is, recover $\\theta_0,\\theta_1$ up to $O(\\Delta)$\nprediction accuracy. These two properties allow to use our framework in applied\ntasks where one would like to $\\textit{identify}$ a prediction model, which can\nbe proprietary, while guaranteeing that the model cannot be actually\n$\\textit{inferred}$ by the identifying agent.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 23:52:54 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 04:56:43 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 12:46:22 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ostrovskii", "Dmitrii M.", ""], ["Ndaoud", "Mohamed", ""], ["Javanmard", "Adel", ""], ["Razaviyayn", "Meisam", ""]]}, {"id": "2012.02914", "submitter": "Marios Papamichalis V.", "authors": "Marios Papamichalis, Simon Lunagomez and Patrick J. Wolfe", "title": "Robustness on Networks", "comments": "34 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We adopt the statistical framework on robustness proposed by Watson and\nHolmes in 2016 and then tackle the practical challenges that hinder its\napplicability to network models. The goal is to evaluate how the quality of an\ninference for a network feature degrades when the assumed model is\nmisspecified. Decision theory methods aimed to identify model\nmissespecification are applied in the context of network data with the goal of\ninvestigating the stability of optimal actions to perturbations to the assumed\nmodel. Here the modified versions of the model are contained within a well\ndefined neighborhood of model space. Our main challenge is to combine\nstochastic optimization and graph limits tools to explore the model space. As a\nresult, a method for robustness on exchangeable random networks is developed.\nOur approach is inspired by recent developments in the context of robustness\nand recent works in the robust control, macroeconomics and financial\nmathematics literature and more specifically and is based on the concept of\ngraphon approximation through its empirical graphon.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 01:18:01 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Papamichalis", "Marios", ""], ["Lunagomez", "Simon", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "2012.02936", "submitter": "Lucy Gao", "authors": "Lucy L. Gao, Jacob Bien and Daniela Witten", "title": "Selective Inference for Hierarchical Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing for a difference in means between two groups is fundamental to\nanswering research questions across virtually every scientific area. Classical\ntests control the Type I error rate when the groups are defined a priori.\nHowever, when the groups are instead defined via a clustering algorithm, then\napplying a classical test for a difference in means between the groups yields\nan extremely inflated Type I error rate. Notably, this problem persists even if\ntwo separate and independent data sets are used to define the groups and to\ntest for a difference in their means. To address this problem, in this paper,\nwe propose a selective inference approach to test for a difference in means\nbetween two clusters obtained from any clustering method. Our procedure\ncontrols the selective Type I error rate by accounting for the fact that the\nnull hypothesis was generated from the data. We describe how to efficiently\ncompute exact p-values for clusters obtained using agglomerative hierarchical\nclustering with many commonly used linkages. We apply our method to simulated\ndata and to single-cell RNA-seq data.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 03:03:19 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Gao", "Lucy L.", ""], ["Bien", "Jacob", ""], ["Witten", "Daniela", ""]]}, {"id": "2012.02985", "submitter": "David Hong", "authors": "David Hong, Yue Sheng, Edgar Dobriban", "title": "Selecting the number of components in PCA via random signflips", "comments": "46 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction via PCA and factor analysis is an important tool of\ndata analysis. A critical step is selecting the number of components. However,\nexisting methods (such as the scree plot, likelihood ratio, parallel analysis,\netc) do not have statistical guarantees in the increasingly common setting\nwhere the data are heterogeneous. There each noise entry can have a different\ndistribution. To address this problem, we propose the Signflip Parallel\nAnalysis (Signflip PA) method: it compares data singular values to those of\n\"empirical null\" data generated by flipping the sign of each entry randomly\nwith probability one-half. We show that Signflip PA consistently selects\nfactors above the noise level in high-dimensional signal-plus-noise models\n(including spiked models and factor models) under heterogeneous settings. Here\nclassical parallel analysis is no longer effective. To do this, we propose to\nleverage recent breakthroughs in random matrix theory, such as dimension-free\noperator norm bounds [Latala et al, 2018, Inventiones Mathematicae], and large\ndeviations for the top eigenvalues of nonhomogeneous matrices [Husson, 2020].\nTo our knowledge, some of these results have not yet been used in statistics.\nWe also illustrate that Signflip PA performs well in numerical simulations and\non empirical data examples.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 09:29:21 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Hong", "David", ""], ["Sheng", "Yue", ""], ["Dobriban", "Edgar", ""]]}, {"id": "2012.03294", "submitter": "Hunyong Cho", "authors": "Hunyong Cho, Shannon T. Holloway, and Michael R. Kosorok", "title": "Multi-stage optimal dynamic treatment regimes for survival outcomes with\n  dependent censoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a reinforcement learning method for estimating an optimal dynamic\ntreatment regime for survival outcomes with dependent censoring. The estimator\nallows the treatment decision times to be dependent on the failure time and\nconditionally independent of censoring, supports a flexible number of treatment\narms and treatment stages, and can maximize either the mean survival time or\nthe survival probability at a certain time point. The estimator is constructed\nusing generalized random survival forests, and its consistency is shown using\nempirical process theory. Simulations and leukemia data analysis results\nsuggest that the new estimator brings higher expected outcomes than existing\nmethods in various settings. An R package dtrSurv is available on CRAN.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 15:52:35 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Cho", "Hunyong", ""], ["Holloway", "Shannon T.", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "2012.03326", "submitter": "Qiwei Li", "authors": "Qiwei Li, Minzhe Zhang, Yang Xie, Guanghua Xiao", "title": "Bayesian Modeling of Spatial Molecular Profiling Data via Gaussian\n  Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The location, timing, and abundance of gene expression (both mRNA and\nproteins) within a tissue define the molecular mechanisms of cell functions.\nRecent technology breakthroughs in spatial molecular profiling, including\nimaging-based technologies and sequencing-based technologies, have enabled the\ncomprehensive molecular characterization of single cells while preserving their\nspatial and morphological contexts. This new bioinformatics scenario calls for\neffective and robust computational methods to identify genes with spatial\npatterns. We represent a novel Bayesian hierarchical model to analyze spatial\ntranscriptomics data, with several unique characteristics. It models the\nzero-inflated and over-dispersed counts by deploying a zero-inflated negative\nbinomial model that greatly increases model stability and robustness. Besides,\nthe Bayesian inference framework allows us to borrow strength in parameter\nestimation in a de novo fashion. As a result, the proposed model shows\ncompetitive performances in accuracy and robustness over existing methods in\nboth simulation studies and two real data applications. The related R/C++\nsource code is available at https://github.com/Minzhe/BOOST-GP.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 17:15:55 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Li", "Qiwei", ""], ["Zhang", "Minzhe", ""], ["Xie", "Yang", ""], ["Xiao", "Guanghua", ""]]}, {"id": "2012.03330", "submitter": "Adam Kapelner", "authors": "Abba M. Krieger and David Azriel and Adam Kapelner", "title": "Better Experimental Design by Hybridizing Binary Matching with Imbalance\n  Optimization", "comments": "18 pages, 2 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new experimental design procedure that divides a set of\nexperimental units into two groups in order to minimize error in estimating an\nadditive treatment effect. One concern is minimizing error at the experimental\ndesign stage is large covariate imbalance between the two groups. Another\nconcern is robustness of design to misspecification in response models. We\naddress both concerns in our proposed design: we first place subjects into\npairs using optimal nonbipartite matching, making our estimator robust to\ncomplicated non-linear response models. Our innovation is to keep the matched\npairs extant, take differences of the covariate values within each matched pair\nand then we use the greedy switching heuristic of Krieger et al. (2019) or\nrerandomization on these differences. This latter step greatly reduce covariate\nimbalance to the rate $O_p(n^{-4})$ in the case of one covariate that are\nuniformly distributed. This rate benefits from the greedy switching heuristic\nwhich is $O_p(n^{-3})$ and the rate of matching which is $O_p(n^{-1})$.\nFurther, our resultant designs are shown to be as random as matching which is\nrobust to unobserved covariates. When compared to previous designs, our\napproach exhibits significant improvement in the mean squared error of the\ntreatment effect estimator when the response model is nonlinear and performs at\nleast as well when it the response model is linear. Our design procedure is\nfound as a method in the open source R package available on CRAN called\nGreedyExperimentalDesign.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 17:32:30 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 14:10:28 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Krieger", "Abba M.", ""], ["Azriel", "David", ""], ["Kapelner", "Adam", ""]]}, {"id": "2012.03355", "submitter": "Kengo Nagashima", "authors": "Kengo Nagashima, Hisashi Noma, Yasunori Sato, Masahiko Gosho", "title": "Sample size calculations for single-arm survival studies using\n  transformations of the Kaplan-Meier estimator", "comments": "22 pages", "journal-ref": "Pharmaceutical Statistics 2021; 20(3): 499-511", "doi": "10.1002/pst.2090", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In single-arm clinical trials with survival outcomes, the Kaplan-Meier\nestimator and its confidence interval are widely used to assess survival\nprobability and median survival time. Since the asymptotic normality of the\nKaplan-Meier estimator is a common result, the sample size calculation methods\nhave not been studied in depth. An existing sample size calculation method is\nfounded on the asymptotic normality of the Kaplan-Meier estimator using the log\ntransformation. However, the small sample properties of the log transformed\nestimator are quite poor in small sample sizes (which are typical situations in\nsingle-arm trials), and the existing method uses an inappropriate standard\nnormal approximation to calculate sample sizes. These issues can seriously\ninfluence the accuracy of results. In this paper, we propose alternative\nmethods to determine sample sizes based on a valid standard normal\napproximation with several transformations that may give an accurate normal\napproximation even with small sample sizes. In numerical evaluations via\nsimulations, some of the proposed methods provided more accurate results, and\nthe empirical power of the proposed method with the arcsine square-root\ntransformation tended to be closer to a prescribed power than the other\ntransformations. These results were supported when methods were applied to data\nfrom three clinical trials.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 19:09:44 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 10:37:24 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Nagashima", "Kengo", ""], ["Noma", "Hisashi", ""], ["Sato", "Yasunori", ""], ["Gosho", "Masahiko", ""]]}, {"id": "2012.03371", "submitter": "Jacob Spertus", "authors": "Amanda K. Glazer and Jacob V. Spertus and Philip B. Stark", "title": "More style, less work: card-style data decrease risk-limiting audit\n  sample sizes", "comments": "19 pages, 9 figures. In submission at Digital Threats: Research and\n  Practice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  U.S. elections rely heavily on computers such as voter registration\ndatabases, electronic pollbooks, voting machines, scanners, tabulators, and\nresults reporting websites. These introduce digital threats to election\noutcomes. Risk-limiting audits (RLAs) mitigate threats to some of these systems\nby manually inspecting random samples of ballot cards. RLAs have a large chance\nof correcting wrong outcomes (by conducting a full manual tabulation of a\ntrustworthy record of the votes), but can save labor when reported outcomes are\ncorrect. This efficiency is eroded when sampling cannot be targeted to ballot\ncards that contain the contest(s) under audit. If the sample is drawn from all\ncast cards, RLA sample sizes scale like the reciprocal of the fraction of\nballot cards that contain the contest(s) under audit. That fraction shrinks as\nthe number of cards per ballot grows (i.e., when elections contain more\ncontests) and as the fraction of ballots that contain the contest decreases\n(i.e., when a smaller percentage of voters are eligible to vote in the\ncontest). States that conduct RLAs of contests on multi-card ballots or of\nsmall contests can dramatically reduce sample sizes by using information about\nwhich ballot cards contain which contests -- by keeping track of card-style\ndata (CSD). For instance, CSD reduces the expected number of draws needed to\naudit a single countywide contest on a 4-card ballot by 75%. Similarly, CSD\nreduces the expected number of draws by 95% or more for an audit of two\ncontests with the same margin on a 4-card ballot if one contest is on every\nballot and the other is on 10% of ballots. In realistic examples, the savings\ncan be several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 20:35:12 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Glazer", "Amanda K.", ""], ["Spertus", "Jacob V.", ""], ["Stark", "Philip B.", ""]]}, {"id": "2012.03391", "submitter": "Edmondo Trentin", "authors": "Edmondo Trentin (DIISM, University of Siena, Italy)", "title": "Multivariate Density Estimation with Deep Neural Mixture Models", "comments": "Extended journal version of E. Trentin, \"Maximum-Likelihood\n  Estimation of Neural Mixture Densities: Model, Algorithm, and Preliminary\n  Experimental Evaluation\". In Proc. of ANNPR 2018: 178-189, Springer, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Albeit worryingly underrated in the recent literature on machine learning in\ngeneral (and, on deep learning in particular), multivariate density estimation\nis a fundamental task in many applications, at least implicitly, and still an\nopen issue. With a few exceptions, deep neural networks (DNNs) have seldom been\napplied to density estimation, mostly due to the unsupervised nature of the\nestimation task, and (especially) due to the need for constrained training\nalgorithms that ended up realizing proper probabilistic models that satisfy\nKolmogorov's axioms. Moreover, in spite of the well-known improvement in terms\nof modeling capabilities yielded by mixture models over plain single-density\nstatistical estimators, no proper mixtures of multivariate DNN-based component\ndensities have been investigated so far. The paper fills this gap by extending\nour previous work on Neural Mixture Densities (NMMs) to multivariate DNN\nmixtures. A maximum-likelihood (ML) algorithm for estimating Deep NMMs (DNMMs)\nis handed out, which satisfies numerically a combination of hard and soft\nconstraints aimed at ensuring satisfaction of Kolmogorov's axioms. The class of\nprobability density functions that can be modeled to any degree of precision\nvia DNMMs is formally defined. A procedure for the automatic selection of the\nDNMM architecture, as well as of the hyperparameters for its ML training\nalgorithm, is presented (exploiting the probabilistic nature of the DNMM).\nExperimental results on univariate and multivariate data are reported on,\ncorroborating the effectiveness of the approach and its superiority to the most\npopular statistical estimation techniques.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 23:03:48 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Trentin", "Edmondo", "", "DIISM, University of Siena, Italy"]]}, {"id": "2012.03432", "submitter": "Maozhu Dai", "authors": "Maozhu Dai, Hal S. Stern", "title": "A U-statistic-based test of treatment effect heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies include a goal of determining whether there is treatment effect\nheterogeneity across different subpopulations. In this paper, we propose a\nU-statistic-based non-parametric test of the null hypothesis that the treatment\neffects are identical in different subgroups. The proposed test provides more\npower than the standard parametric test when the underlying distribution\nassumptions of the latter are violated. We apply the method to data from an\neconomic study of program effectiveness and find that there is treatment effect\nheterogeneity in different subpopulations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 03:05:39 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Dai", "Maozhu", ""], ["Stern", "Hal S.", ""]]}, {"id": "2012.03451", "submitter": "Zhiqiang Tan", "authors": "Zhiqiang Tan", "title": "Consistent and robust inference in hazard probability and odds models\n  with discrete-time survival data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For discrete-time survival data, conditional likelihood inference in Cox's\nhazard odds model is theoretically desirable but exact calculation is numerical\nintractable with a moderate to large number of tied events. Unconditional\nmaximum likelihood estimation over both regression coefficients and baseline\nhazard probabilities can be problematic with a large number of time intervals.\nWe develop new methods and theory using numerically simple estimating\nfunctions, along with model-based and model-robust variance estimation, in\nhazard probability and odds models. For the probability hazard model, we derive\nas a consistent estimator the Breslow-Peto estimator, previously known as an\napproximation to the conditional likelihood estimator in the hazard odds model.\nFor the odds hazard model, we propose a weighted Mantel-Haenszel estimator,\nwhich satisfies conditional unbiasedness given the numbers of events in\naddition to the risk sets and covariates, similarly to the conditional\nlikelihood estimator. Our methods are expected to perform satisfactorily in a\nbroad range of settings, with small or large numbers of tied events\ncorresponding to a large or small number of time intervals. The methods are\nimplemented in the R package dSurvival.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 05:23:30 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Tan", "Zhiqiang", ""]]}, {"id": "2012.03475", "submitter": "Kengo Nagashima", "authors": "Kengo Nagashima, Yasunori Sato, Chikuma Hamada", "title": "A modified maximum contrast method for unequal sample sizes in\n  pharmacogenomic studies", "comments": "22 pages, 10 figures", "journal-ref": "Statistical Applications in Genetics and Molecular Biology 2011;\n  10(1): Article 41", "doi": "10.2202/1544-6115.1560", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In pharmacogenomic studies, biomedical researchers commonly analyze the\nassociation between genotype and biological response by using the\nKruskal--Wallis test or one-way analysis of variance (ANOVA) after logarithmic\ntransformation of the obtained data. However, because these methods detect\nunexpected biological response patterns, the power for detecting the expected\npattern is reduced. Previously, we proposed a combination of the maximum\ncontrast method and the permuted modified maximum contrast method for unequal\nsample sizes in pharmacogenomic studies. However, we noted that the\ndistribution of the permuted modified maximum contrast statistic depends on a\nnuisance parameter $\\sigma^2$, which is the population variance. In this paper,\nwe propose a modified maximum contrast method with a statistic that does not\ndepend on the nuisance parameter. Furthermore, we compare the performance of\nthese methods via simulation studies. The simulation results showed that the\nmodified maximum contrast method gave the lowest false-positive rate;\ntherefore, this method is powerful for detecting the true response patterns in\nsome conditions. Further, it is faster and more accurate than the permuted\nmodified maximum contrast method. On the basis of these results, we suggest a\nrule of thumb to select the appropriate method in a given situation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 06:45:28 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Nagashima", "Kengo", ""], ["Sato", "Yasunori", ""], ["Hamada", "Chikuma", ""]]}, {"id": "2012.03593", "submitter": "Eliana Duarte", "authors": "Eliana Duarte, Liam Solus", "title": "Algebraic geometry of discrete interventional models", "comments": "The previous version of this paper was divided into two distinct\n  papers. This is one of them", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AC math.AG math.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the algebra and geometry of general interventions in discrete\nDAG models. To this end, we introduce a theory for modeling soft interventions\nin the more general family of staged tree models and develop the formalism to\nstudy these models as parametrized subvarieties of a product of probability\nsimplices. We then consider the problem of finding their defining equations,\nand we derive a combinatorial criterion for identifying interventional staged\ntree models for which the defining ideal is toric. We apply these results to\nthe class of discrete interventional DAG models and establish a criteria to\ndetermine when these models are toric varieties.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 11:29:21 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 08:36:01 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Duarte", "Eliana", ""], ["Solus", "Liam", ""]]}, {"id": "2012.03700", "submitter": "Lucie Biard", "authors": "Moreno Ursino, Lucie Biard, Sylvie Chevret", "title": "Bayesian model for early dose-finding in phase I trials with multiple\n  treatment courses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dose-finding clinical trials in oncology aim to determine the maximum\ntolerated dose (MTD) of a new drug, generally defined by the proportion of\npatients with short-term dose-limiting toxicities (DLTs). Model-based\napproaches for such phase I oncology trials have been widely designed and are\nmostly restricted to the DLTs occurring during the first cycle of treatment,\nalthough patients continue to receive treatment for multiple cycles. We aim to\nestimate the probability of DLTs over sequences of treatment cycles via a\nBayesian cumulative modeling approach, where the probability of DLT is modeled\ntaking into account the cumulative effect of the administered drug and the DLT\ncycle of occurrence. We propose a design, called DICE (Dose-fInding\nCumulativE), for dose escalation and de-escalation according to previously\nobserved toxicities, which aims at finding the MTD sequence (MTS). We performed\nan extensive simulation study comparing this approach to the time-to-event\ncontinual reassessment method (TITE-CRM) and to a benchmark. In general, our\napproach achieved a better or comparable percentage of correct MTS selection.\nMoreover, we investigated the DICE prediction ability.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 13:58:58 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Ursino", "Moreno", ""], ["Biard", "Lucie", ""], ["Chevret", "Sylvie", ""]]}, {"id": "2012.03786", "submitter": "Jack Bowden Professor", "authors": "Jack Bowden, Bjoern Bornkamp, Ekkehard Glimm and Frank Bretz", "title": "Connecting Instrumental Variable methods for causal inference to the\n  Estimand Framework", "comments": "29 pages, 9 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal inference methods are gaining increasing prominence in pharmaceutical\ndrug development in light of the recently published addendum on estimands and\nsensitivity analysis in clinical trials to the E9 guideline of the\nInternational Council for Harmonisation. The E9 addendum emphasises the need to\naccount for post-randomization or `intercurrent' events that can potentially\ninfluence the interpretation of a treatment effect estimate at a trial's\nconclusion. Instrumental Variables (IV) methods have been used extensively in\neconomics, epidemiology and academic clinical studies for `causal inference',\nbut less so in the pharmaceutical industry setting until now. In this tutorial\npaper we review the basic tools for causal inference, including graphical\ndiagrams and potential outcomes, as well as several conceptual frameworks that\nan IV analysis can sit within. We discuss in detail how to map these approaches\nto the Treatment Policy, Principal Stratum and Hypothetical `estimand\nstrategies' introduced in the E9 addendum, and provide details of their\nimplementation using standard regression models. Specific attention is given to\ndiscussing the assumptions each estimation strategy relies on in order to be\nconsistent, the extent to which they can be empirically tested and sensitivity\nanalyses in which specific assumptions can be relaxed. We finish by applying\nthe methods described to simulated data closely matching two recent\npharmaceutical trials to further motivate and clarify the ideas\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:28:39 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 06:53:51 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Bowden", "Jack", ""], ["Bornkamp", "Bjoern", ""], ["Glimm", "Ekkehard", ""], ["Bretz", "Frank", ""]]}, {"id": "2012.04137", "submitter": "Dhruva Kartik", "authors": "Dhruva Kartik, Neeraj Sood, Urbashi Mitra, Tara Javidi", "title": "Adaptive Sampling for Estimating Distributions: A Bayesian Upper\n  Confidence Bound Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of adaptive sampling for estimating probability mass functions\n(pmf) uniformly well is considered. Performance of the sampling strategy is\nmeasured in terms of the worst-case mean squared error. A Bayesian variant of\nthe existing upper confidence bound (UCB) based approaches is proposed. It is\nshown analytically that the performance of this Bayesian variant is no worse\nthan the existing approaches. The posterior distribution on the pmfs in the\nBayesian setting allows for a tighter computation of upper confidence bounds\nwhich leads to significant performance gains in practice. Using this approach,\nadaptive sampling protocols are proposed for estimating SARS-CoV-2\nseroprevalence in various groups such as location and ethnicity. The\neffectiveness of this strategy is discussed using data obtained from a\nseroprevalence survey in Los Angeles county.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 00:53:34 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kartik", "Dhruva", ""], ["Sood", "Neeraj", ""], ["Mitra", "Urbashi", ""], ["Javidi", "Tara", ""]]}, {"id": "2012.04200", "submitter": "Yan Li", "authors": "Yan Li, Kun Chen, Jun Yan, Xuebin Zhang", "title": "Regularized Fingerprinting in Detection and Attribution of Climate\n  Change with Weight Matrix Optimizing the Efficiency in Scaling Factor\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal fingerprinting method for detection and attribution of climate\nchange is based on a multiple regression where each covariate has measurement\nerror whose covariance matrix is the same as that of the regression error up to\na known scale. Inferences about the regression coefficients are critical not\nonly for making statements about detection and attribution but also for\nquantifying the uncertainty in important outcomes derived from detection and\nattribution analyses. When there is no errors-in-variables (EIV), the optimal\nweight matrix in estimating the regression coefficients is the precision matrix\nof the regression error which, in practice, is never known and has to be\nestimated from climate model simulations. We construct a weight matrix by\ninverting a nonlinear shrinkage estimate of the error covariance matrix that\nminimizes loss functions directly targeting the uncertainty of the resulting\nregression coefficient estimator. The resulting estimator of the regression\ncoefficients is asymptotically optimal as the sample size of the climate model\nsimulations and the matrix dimension go to infinity together with a limiting\nratio. When EIVs are present, the estimator of the regression coefficients\nbased on the proposed weight matrix is asymptotically more efficient than that\nbased on the inverse of the existing linear shrinkage estimator of the error\ncovariance matrix. The performance of the method is confirmed in finite sample\nsimulation studies mimicking realistic situations in terms of the length of the\nconfidence intervals and empirical coverage rates for the regression\ncoefficients. An application to detection and attribution analyses of the mean\ntemperature at different spatial scales illustrates the utility of the method.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 04:07:59 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Li", "Yan", ""], ["Chen", "Kun", ""], ["Yan", "Jun", ""], ["Zhang", "Xuebin", ""]]}, {"id": "2012.04271", "submitter": "Gilbert Saporta", "authors": "Ruiping Liu, Ndeye Niang, Gilbert Saporta, Huiwen Wang", "title": "Sparse Correspondence Analysis for Contingency Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the introduction of the lasso in regression, various sparse methods\nhave been developed in an unsupervised context like sparse principal component\nanalysis (s-PCA), sparse canonical correlation analysis (s-CCA) and sparse\nsingular value decomposition (s-SVD). These sparse methods combine feature\nselection and dimension reduction. One advantage of s-PCA is to simplify the\ninterpretation of the (pseudo) principal components since each one is expressed\nas a linear combination of a small number of variables. The disadvantages lie\non the one hand in the difficulty of choosing the number of non-zero\ncoefficients in the absence of a well established criterion and on the other\nhand in the loss of orthogonality for the components and/or the loadings. In\nthis paper we propose sparse variants of correspondence analysis (CA)for large\ncontingency tables like documents-terms matrices used in text mining, together\nwith pPMD, a deation technique derived from projected deflation in s-PCA. We\nuse the fact that CA is a double weighted PCA (for rows and columns) or a\nweighted SVD, as well as a canonical correlation analysis of indicator\nvariables. Applying s-CCA or s-SVD allows to sparsify both rows and columns\nweights. The user may tune the level of sparsity of rows and columns and\noptimize it according to some criterium, and even decide that no sparsity is\nneeded for rows (or columns) by relaxing one sparsity constraint. The latter is\nequivalent to apply s-PCA to matrices of row (or column) profiles.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 08:28:08 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Liu", "Ruiping", ""], ["Niang", "Ndeye", ""], ["Saporta", "Gilbert", ""], ["Wang", "Huiwen", ""]]}, {"id": "2012.04277", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn", "title": "Comparisons of multiple treatment groups with a negative control or\n  placebo group: Dunnett test vs. closed test procedur", "comments": "Appendix with simulation results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Several treatments are usually compared with a control using the Dunnett\ntest. As an alternative, three variants of the closed testing approach are\nconsidered, one with ANOVA-F-tests, one with MCT-GrandMean and one with global\nDunnett-tests in the partition hypotheses.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 08:43:31 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Hothorn", "Ludwig A.", ""]]}, {"id": "2012.04315", "submitter": "Jaejoon Lee", "authors": "Jaejoon Lee, Jaeyong Lee", "title": "Robust Sparse Bayesian Infinite Factor Models", "comments": "added code github repository in section 4. added data source of\n  breast carcinoma data in section 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most of previous works and applications of Bayesian factor model have assumed\nthe normal likelihood regardless of its validity. We propose a Bayesian factor\nmodel for heavy-tailed high-dimensional data based on multivariate Student-$t$\nlikelihood to obtain better covariance estimation. We use multiplicative gamma\nprocess shrinkage prior and factor number adaptation scheme proposed in\nBhattacharya & Dunson [Biometrika (2011) 291-306]. Since a naive Gibbs sampler\nfor the proposed model suffers from slow mixing, we propose a Markov Chain\nMonte Carlo algorithm where fast mixing of Hamiltonian Monte Carlo is exploited\nfor some parameters in proposed model. Simulation results illustrate the gain\nin performance of covariance estimation for heavy-tailed high-dimensional data.\nWe also provide a theoretical result that the posterior of the proposed model\nis weakly consistent under reasonable conditions. We conclude the paper with\nthe application of proposed factor model on breast cancer metastasis prediction\ngiven DNA signature data of cancer cell.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 09:37:16 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 07:52:44 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Lee", "Jaejoon", ""], ["Lee", "Jaeyong", ""]]}, {"id": "2012.04455", "submitter": "Giulio D'Agostini", "authors": "Giulio D'Agostini", "title": "Ratio of counts vs ratio of rates in Poisson processes", "comments": "73 pages, 24 figures. The scripts of Appendix B are available for\n  download from https://www.roma1.infn.it/~dagos/prob+stat.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The often debated issue of `ratios of small numbers of events' is approached\nfrom a probabilistic perspective, making a clear distinction between the\npredictive problem (forecasting numbers of events we might count under well\nstated assumptions, and therefore of their ratios) and inferential problem\n(learning about the relevant parameters of the related probability\ndistribution, in the light of the observed number of events). The quantities of\ninterests and their relations are visualized in a graphical model (`Bayesian\nnetwork'), very useful to understand how to approach the problem following the\nrules of probability theory. In this paper, written with didactic intent, we\ndiscuss in detail the basic ideas, however giving some hints of how real life\ncomplications, like (uncertain) efficiencies and possible background and\nsystematics, can be included in the analysis, as well as the possibility that\nthe ratio of rates might depend on some physical quantity. The simple models\nconsidered in this paper allow to obtain, under reasonable assumptions, closed\nexpressions for the rates and their ratios. Monte Carlo methods are also used,\nboth to cross check the exact results and to evaluate by sampling the ratios of\ncounts in the cases in which large number approximation does not hold. In\nparticular it is shown how to make approximate inferences using a Markov Chain\nMonte Carlo using JAGS/rjags. Some examples of R and JAGS code are provided.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 16:23:02 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["D'Agostini", "Giulio", ""]]}, {"id": "2012.04464", "submitter": "Suzanne Thornton", "authors": "Suzanne Thornton and Minge Xie", "title": "Bridging Bayesian, frequentist and fiducial (BFF) inferences using\n  confidence distribution", "comments": "30 pages, 5 figures, Handbook on Bayesian Fiducial and Frequentist\n  (BFF) Inferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian, frequentist and fiducial (BFF) inferences are much more congruous\nthan they have been perceived historically in the scientific community (cf.,\nReid and Cox 2015; Kass 2011; Efron 1998). Most practitioners are probably more\nfamiliar with the two dominant statistical inferential paradigms, Bayesian\ninference and frequentist inference. The third, lesser known fiducial inference\nparadigm was pioneered by R.A. Fisher in an attempt to define an inversion\nprocedure for inference as an alternative to Bayes' theorem. Although each\nparadigm has its own strengths and limitations subject to their different\nphilosophical underpinnings, this article intends to bridge these different\ninferential methodologies through the lenses of confidence distribution theory\nand Monte-Carlo simulation procedures. This article attempts to understand how\nthese three distinct paradigms, Bayesian, frequentist, and fiducial inference,\ncan be unified and compared on a foundational level, thereby increasing the\nrange of possible techniques available to both statistical theorists and\npractitioners across all fields.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 15:03:07 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Thornton", "Suzanne", ""], ["Xie", "Minge", ""]]}, {"id": "2012.04505", "submitter": "Nicholas Syring", "authors": "Nicholas Syring and Ryan Martin", "title": "Gibbs posterior concentration rates under sub-exponential type losses", "comments": "60 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian posterior distributions are widely used for inference, but their\ndependence on a statistical model creates some challenges. In particular, there\nmay be lots of nuisance parameters that require prior distributions and\nposterior computations, plus a potentially serious risk of model\nmisspecification bias. Gibbs posterior distributions, on the other hand, offer\ndirect, principled, probabilistic inference on quantities of interest through a\nloss function, not a model-based likelihood. Here we provide simple sufficient\nconditions for establishing Gibbs posterior concentration rates when the loss\nfunction is of a sub-exponential type. We apply these general results in a\nrange of practically relevant examples, including mean regression, quantile\nregression, and sparse high-dimensional classification. We also apply these\ntechniques in an important problem in medical statistics, namely, estimation of\na personalized minimum clinically important difference.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 15:44:26 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 19:58:45 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2020 14:19:26 GMT"}, {"version": "v4", "created": "Mon, 26 Jul 2021 17:47:07 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Syring", "Nicholas", ""], ["Martin", "Ryan", ""]]}, {"id": "2012.04570", "submitter": "Adel Daoud", "authors": "Adel Daoud and Devdatt Dubhashi", "title": "Statistical modeling: the three cultures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two decades ago, Leo Breiman identified two cultures for statistical\nmodeling. The data modeling culture (DMC) refers to practices aiming to conduct\nstatistical inference on one or several quantities of interest. The algorithmic\nmodeling culture (AMC) refers to practices defining a machine-learning (ML)\nprocedure that generates accurate predictions about an event of interest.\nBreiman argued that statisticians should give more attention to AMC than to\nDMC, because of the strengths of ML in adapting to data. While twenty years\nlater, DMC has lost some of its dominant role in statistics because of the\ndata-science revolution, we observe that this culture is still the leading\npractice in the natural and social sciences. DMC is the modus operandi because\nof the influence of the established scientific method, called the\nhypothetico-deductive scientific method. Despite the incompatibilities of AMC\nwith this scientific method, among some research groups, AMC and DMC cultures\nmix intensely. We argue that this mixing has formed a fertile spawning pool for\na mutated culture that we called the hybrid modeling culture (HMC) where\nprediction and inference have fused into new procedures where they reinforce\none another. This article identifies key characteristics of HMC, thereby\nfacilitating the scientific endeavor and fueling the evolution of statistical\ncultures towards better practices. By better, we mean increasingly reliable,\nvalid, and efficient statistical practices in analyzing causal relationships.\nIn combining inference and prediction, the result of HMC is that the\ndistinction between prediction and inference, taken to its limit, melts away.\nWe qualify our melting-away argument by describing three HMC practices, where\neach practice captures an aspect of the scientific cycle, namely, ML for causal\ninference, ML for data acquisition, and ML for theory prediction.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 17:15:02 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Daoud", "Adel", ""], ["Dubhashi", "Devdatt", ""]]}, {"id": "2012.04602", "submitter": "Samuel Duffield", "authors": "Samuel Duffield, Sumeetpal S. Singh", "title": "Online Particle Smoothing with Application to Map-matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel method for online smoothing in state-space models based\non a fixed-lag approximation. Unlike classical fixed-lag smoothing we\napproximate the joint posterior distribution rather than just the marginals. By\nonly partially resampling particles, our online particle smoothing technique\navoids path degeneracy as the length of the state-space model increases. We\ndemonstrate the utility of our method in the context of map-matching, the task\nof inferring a vehicle's trajectory given a road network and noisy GPS\nobservations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 18:02:42 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Duffield", "Samuel", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "2012.04620", "submitter": "Nicolas Jouvin", "authors": "Nicolas Jouvin, Charles Bouveyron and Pierre Latouche", "title": "A Bayesian Fisher-EM algorithm for discriminative Gaussian subspace\n  clustering", "comments": "The FisherEM package is available on CRAN, see\n  https://github.com/nicolasJouvin/FisherEM for additional information", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High-dimensional data clustering has become and remains a challenging task\nfor modern statistics and machine learning, with a wide range of applications.\nWe consider in this work the powerful discriminative latent mixture model, and\nwe extend it to the Bayesian framework. Modeling data as a mixture of Gaussians\nin a low-dimensional discriminative subspace, a Gaussian prior distribution is\nintroduced over the latent group means and a family of twelve submodels are\nderived considering different covariance structures. Model inference is done\nwith a variational EM algorithm, while the discriminative subspace is estimated\nvia a Fisher-step maximizing an unsupervised Fisher criterion. An empirical\nBayes procedure is proposed for the estimation of the prior hyper-parameters,\nand an integrated classification likelihood criterion is derived for selecting\nboth the number of clusters and the submodel. The performances of the resulting\nBayesian Fisher-EM algorithm are investigated in two thorough simulated\nscenarios, regarding both dimensionality as well as noise and assessing its\nsuperiority with respect to state-of-the-art Gaussian subspace clustering\nmodels. In addition to standard real data benchmarks, an application to single\nimage denoising is proposed, displaying relevant results. This work comes with\na reference implementation for the R software in the FisherEM package\naccompanying the paper.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 18:42:26 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Jouvin", "Nicolas", ""], ["Bouveyron", "Charles", ""], ["Latouche", "Pierre", ""]]}, {"id": "2012.04723", "submitter": "Tineke Blom", "authors": "Tineke Blom and Joris M. Mooij", "title": "Robustness of Model Predictions under Extension", "comments": "Accepted for oral presentation at the Causal Discovery &\n  Causality-Inspired Machine Learning Workshop at Neural Information Processing\n  Systems, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often, mathematical models of the real world are simplified representations\nof complex systems. A caveat to using models for analysis is that predicted\ncausal effects and conditional independences may not be robust under model\nextensions, and therefore applicability of such models is limited. In this\nwork, we consider conditions under which qualitative model predictions are\npreserved when two models are combined. We show how to use the technique of\ncausal ordering to efficiently assess the robustness of qualitative model\npredictions and characterize a large class of model extensions that preserve\nthese predictions. For dynamical systems at equilibrium, we demonstrate how\nnovel insights help to select appropriate model extensions and to reason about\nthe presence of feedback loops. We apply our ideas to a viral infection model\nwith immune responses.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 20:21:03 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Blom", "Tineke", ""], ["Mooij", "Joris M.", ""]]}, {"id": "2012.04756", "submitter": "Michael Weylandt", "authors": "Michael Weylandt and George Michailidis", "title": "Automatic Registration and Clustering of Time Series", "comments": "To appear in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Clustering of time series data exhibits a number of challenges not present in\nother settings, notably the problem of registration (alignment) of observed\nsignals. Typical approaches include pre-registration to a user-specified\ntemplate or time warping approaches which attempt to optimally align series\nwith a minimum of distortion. For many signals obtained from recording or\nsensing devices, these methods may be unsuitable as a template signal is not\navailable for pre-registration, while the distortion of warping approaches may\nobscure meaningful temporal information. We propose a new method for automatic\ntime series alignment within a clustering problem. Our approach, Temporal\nRegistration using Optimal Unitary Transformations (TROUT), is based on a novel\ndissimilarity measure between time series that is easy to compute and\nautomatically identifies optimal alignment between pairs of time series. By\nembedding our new measure in a optimization formulation, we retain well-known\nadvantages of computational and statistical performance. We provide an\nefficient algorithm for TROUT-based clustering and demonstrate its superior\nperformance over a range of competitors.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 21:51:21 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 18:30:11 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Weylandt", "Michael", ""], ["Michailidis", "George", ""]]}, {"id": "2012.04762", "submitter": "Michael Weylandt", "authors": "Michael Weylandt and T. Mitchell Roddenberry and Genevera I. Allen", "title": "Simultaneous Grouping and Denoising via Sparse Convex Wavelet Clustering", "comments": "To appear in IEEE DSLW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Clustering is a ubiquitous problem in data science and signal processing. In\nmany applications where we observe noisy signals, it is common practice to\nfirst denoise the data, perhaps using wavelet denoising, and then to apply a\nclustering algorithm. In this paper, we develop a sparse convex wavelet\nclustering approach that simultaneously denoises and discovers groups. Our\napproach utilizes convex fusion penalties to achieve agglomeration and\ngroup-sparse penalties to denoise through sparsity in the wavelet domain. In\ncontrast to common practice which denoises then clusters, our method is a\nunified, convex approach that performs both simultaneously. Our method yields\ndenoised (wavelet-sparse) cluster centroids that both improve interpretability\nand data compression. We demonstrate our method on synthetic examples and in an\napplication to NMR spectroscopy.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 22:00:38 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 00:30:03 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Weylandt", "Michael", ""], ["Roddenberry", "T. Mitchell", ""], ["Allen", "Genevera I.", ""]]}, {"id": "2012.04798", "submitter": "Niloy Biswas", "authors": "Niloy Biswas, Anirban Bhattacharya, Pierre E. Jacob, James E. Johndrow", "title": "Coupling-based convergence assessment of some Gibbs samplers for\n  high-dimensional Bayesian regression with shrinkage priors", "comments": "61 pages, 9 figures. Alternative coupling strategies, a second GWAS\n  example, and some results on the meeting times of the coupling algorithms\n  have been added. R package available at\n  https://github.com/niloyb/CoupledHalfT", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider Markov chain Monte Carlo (MCMC) algorithms for Bayesian\nhigh-dimensional regression with continuous shrinkage priors. A common\nchallenge with these algorithms is the choice of the number of iterations to\nperform. This is critical when each iteration is expensive, as is the case when\ndealing with modern data sets, such as genome-wide association studies with\nthousands of rows and up to hundred of thousands of columns. We develop\ncoupling techniques tailored to the setting of high-dimensional regression with\nshrinkage priors, which enable practical, non-asymptotic diagnostics of\nconvergence without relying on traceplots or long-run asymptotics. By\nestablishing geometric drift and minorization conditions for the algorithm\nunder consideration, we prove that the proposed couplings have finite expected\nmeeting time. Focusing on a class of shrinkage priors which includes the\n'Horseshoe', we empirically demonstrate the scalability of the proposed\ncouplings. A highlight of our findings is that less than 1000 iterations can be\nenough for a Gibbs sampler to reach stationarity in a regression on 100,000\ncovariates. The numerical results also illustrate the impact of the prior on\nthe computational efficiency of the coupling, and suggest the use of priors\nwhere the local precisions are Half-t distributed with degree of freedom larger\nthan one.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 00:16:40 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 23:51:33 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 03:03:51 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Biswas", "Niloy", ""], ["Bhattacharya", "Anirban", ""], ["Jacob", "Pierre E.", ""], ["Johndrow", "James E.", ""]]}, {"id": "2012.04809", "submitter": "Aaron Sonabend", "authors": "Aaron Sonabend-W, Nilanjana Laha, Ashwin N. Ananthakrishnan, Tianxi\n  Cai, Rajarshi Mukherjee", "title": "Semi-Supervised Off Policy Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning (RL) has shown great success in estimating sequential\ntreatment strategies which take into account patient heterogeneity. However,\nhealth-outcome information, which is used as the reward for reinforcement\nlearning methods, is often not well coded but rather embedded in clinical\nnotes. Extracting precise outcome information is a resource intensive task, so\nmost of the available well-annotated cohorts are small. To address this issue,\nwe propose a semi-supervised learning (SSL) approach that efficiently leverages\na small sized labeled data with true outcome observed, and a large unlabeled\ndata with outcome surrogates. In particular, we propose a semi-supervised,\nefficient approach to Q-learning and doubly robust off policy value estimation.\nGeneralizing SSL to sequential treatment regimes brings interesting challenges:\n1) Feature distribution for Q-learning is unknown as it includes previous\noutcomes. 2) The surrogate variables we leverage in the modified SSL framework\nare predictive of the outcome but not informative to the optimal policy or\nvalue function. We provide theoretical results for our Q-function and value\nfunction estimators to understand to what degree efficiency can be gained from\nSSL. Our method is at least as efficient as the supervised approach, and\nmoreover safe as it robust to mis-specification of the imputation models.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 00:59:12 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 15:43:46 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 02:54:08 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2021 14:15:13 GMT"}, {"version": "v5", "created": "Tue, 23 Feb 2021 02:35:02 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Sonabend-W", "Aaron", ""], ["Laha", "Nilanjana", ""], ["Ananthakrishnan", "Ashwin N.", ""], ["Cai", "Tianxi", ""], ["Mukherjee", "Rajarshi", ""]]}, {"id": "2012.04831", "submitter": "Corwin Zigler", "authors": "Corwin Zigler, Laura Forastiere, Fabrizia Mealli", "title": "Bipartite Interference and Air Pollution Transport: Estimating Health\n  Effects of Power Plant Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evaluating air quality interventions is confronted with the challenge of\ninterference since interventions at a particular pollution source likely impact\nair quality and health at distant locations and air quality and health at any\ngiven location are likely impacted by interventions at many sources. The\nstructure of interference in this context is dictated by complex atmospheric\nprocesses governing how pollution emitted from a particular source is\ntransformed and transported across space, and can be cast with a bipartite\nstructure reflecting the two distinct types of units: 1) interventional units\non which treatments are applied or withheld to change pollution emissions; and\n2) outcome units on which outcomes of primary interest are measured. We propose\nnew estimands for bipartite causal inference with interference that construe\ntwo components of treatment: a \"key-associated\" (or \"individual\") treatment and\nan \"upwind\" (or \"neighborhood\") treatment. Estimation is carried out using a\nsemi-parametric adjustment approach based on joint propensity scores. A\nreduced-complexity atmospheric model is deployed to characterize the structure\nof the interference network by modeling the movement of air parcels through\ntime and space. The new methods are deployed to evaluate the effectiveness of\ninstalling flue-gas desulfurization scrubbers on 472 coal-burning power plants\n(the interventional units) in reducing Medicare hospitalizations among\n22,603,597 Medicare beneficiaries residing across 23,675 ZIP codes in the\nUnited States (the outcome units).\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 02:40:17 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Zigler", "Corwin", ""], ["Forastiere", "Laura", ""], ["Mealli", "Fabrizia", ""]]}, {"id": "2012.04879", "submitter": "Andrew Fowlie Assoc. Prof.", "authors": "Andrew Fowlie", "title": "Objective Bayesian approach to the Jeffreys-Lindley paradox", "comments": "6 pages, 1 figure, to appear in Communications in Statistics", "journal-ref": null, "doi": "10.1080/03610926.2020.1866206", "report-no": null, "categories": "stat.ME hep-ph physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the Jeffreys-Lindley paradox from an objective Bayesian\nperspective by attempting to find priors representing complete indifference to\nsample size in the problem. This means that we ensure that the prior for the\nunknown mean and the prior predictive for the $t$-statistic are independent of\nthe sample size. If successful, this would lead to Bayesian model comparison\nthat was independent of sample size and ameliorate the paradox. Unfortunately,\nit leads to an improper scale-invariant prior for the unknown mean. We show,\nhowever, that a truncated scale-invariant prior delays the dependence on sample\nsize, which could be practically significant. Lastly, we shed light on the\nparadox by relating it to the fact that the scale-invariant prior is improper.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 06:03:27 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Fowlie", "Andrew", ""]]}, {"id": "2012.04922", "submitter": "Adrian Perez-Suay", "authors": "Emiliano D\\'iaz, Adri\\'an P\\'erez-Suay, Valero Laparra, Gustau\n  Camps-Valls", "title": "Consistent regression of biophysical parameters with kernel methods", "comments": "arXiv admin note: substantial text overlap with arXiv:1710.05578", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces a novel statistical regression framework that allows\nthe incorporation of consistency constraints. A linear and nonlinear\n(kernel-based) formulation are introduced, and both imply closed-form\nanalytical solutions. The models exploit all the information from a set of\ndrivers while being maximally independent of a set of auxiliary, protected\nvariables. We successfully illustrate the performance in the estimation of\nchlorophyll content.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 08:59:16 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["D\u00edaz", "Emiliano", ""], ["P\u00e9rez-Suay", "Adri\u00e1n", ""], ["Laparra", "Valero", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.05072", "submitter": "Jarrad Courts", "authors": "Jarrad Courts, Adrian Wills, Thomas Sch\\\"on, Brett Ninness", "title": "Variational System Identification for Nonlinear State-Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY eess.SY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers parameter estimation for nonlinear state-space models,\nwhich is an important but challenging problem. We address this challenge by\nemploying a variational inference (VI) approach, which is a principled method\nthat has deep connections to maximum likelihood estimation. This VI approach\nultimately provides estimates of the model as solutions to an optimisation\nproblem, which is deterministic, tractable and can be solved using standard\noptimisation tools. A specialisation of this approach for systems with additive\nGaussian noise is also detailed. The proposed method is examined numerically on\na range of simulated and real examples focusing on the robustness to parameter\ninitialisation; additionally, favourable comparisons are performed against\nstate-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 05:43:50 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 04:39:01 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Courts", "Jarrad", ""], ["Wills", "Adrian", ""], ["Sch\u00f6n", "Thomas", ""], ["Ninness", "Brett", ""]]}, {"id": "2012.05127", "submitter": "Antonio Remiro-Az\\'ocar Mr.", "authors": "Antonio Remiro-Az\\'ocar, Anna Heath, Gianluca Baio", "title": "Principled selection of effect modifiers: Comments on 'Matching-adjusted\n  indirect comparisons: Application to time-to-event data'", "comments": "14 pages, submitted to Statistics in Medicine. Response to\n  `Matching-adjusted indirect comparisons: Application to time-to-event data'\n  by Aouni, Gaudel-Dedieu and Sebastien, published in Statistics in Medicine\n  (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this commentary, we raise our concerns about a recent simulation study\nconducted by Aouni, Gaudel-Dedieu and Sebastien, evaluating the performance of\ndifferent versions of matching-adjusted indirect comparison (MAIC). The\nfollowing points are highlighted: (1) making a clear distinction between\nprognostic and effect-modifying covariates is important; (2) in the anchored\nsetting, MAIC is necessary where there are cross-trial imbalances in effect\nmodifiers; (3) the standard indirect comparison provides greater precision and\naccuracy than MAIC if there are no effect modifiers in imbalance; (4) while the\ntarget estimand of the simulation study is a conditional treatment effect, MAIC\ntargets a marginal or population-average treatment effect; (5) in MAIC,\nvariable selection is a problem of low dimensionality and sparsity-inducing\nmethods like the LASSO may induce bias; and (6) individual studies are\nunderpowered to detect interactions and data-driven approaches do not obviate\nthe necessity for subject matter knowledge when selecting effect modifiers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 16:01:53 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 10:41:26 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Remiro-Az\u00f3car", "Antonio", ""], ["Heath", "Anna", ""], ["Baio", "Gianluca", ""]]}, {"id": "2012.05150", "submitter": "Adrian Perez-Suay", "authors": "Adri\\'an P\\'erez-Suay, Gustau Camps-Valls", "title": "Causal Inference in Geoscience and Remote Sensing from Observational\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Establishing causal relations between random variables from observational\ndata is perhaps the most important challenge in today's \\blue{science}. In\nremote sensing and geosciences this is of special relevance to better\nunderstand the Earth's system and the complex interactions between the\ngoverning processes. In this paper, we focus on observational causal inference,\nthus we try to estimate the correct direction of causation using a finite set\nof empirical data. In addition, we focus on the more complex bivariate scenario\nthat requires strong assumptions and no conditional independence tests can be\nused. In particular, we explore the framework of (non-deterministic) additive\nnoise models, which relies on the principle of independence between the cause\nand the generating mechanism. A practical algorithmic instantiation of such\nprinciple only requires 1) two regression models in the forward and backward\ndirections, and 2) the estimation of {\\em statistical independence} between the\nobtained residuals and the observations. The direction leading to more\nindependent residuals is decided to be the cause. We instead propose a\ncriterion that uses the {\\em sensitivity} (derivative) of the dependence\nestimator, the sensitivity criterion allows to identify samples most affecting\nthe dependence measure, and hence the criterion is robust to spurious\ndetections. We illustrate performance in a collection of 28 geoscience causal\ninference problems, in a database of radiative transfer models simulations and\nmachine learning emulators in vegetation parameter modeling involving 182\nproblems, and in assessing the impact of different regression models in a\ncarbon cycle problem. The criterion achieves state-of-the-art detection rates\nin all cases, it is generally robust to noise sources and distortions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 22:56:55 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["P\u00e9rez-Suay", "Adri\u00e1n", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.05158", "submitter": "Kean Ming Tan", "authors": "Yanxin Jin, Yang Ning, and Kean Ming Tan", "title": "Exponential Family Graphical Models: Correlated Replicates and\n  Unmeasured Confounders, with Applications to fMRI Data", "comments": "R package latentgraph for fitting the model can be found at CRAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models have been used extensively for modeling brain connectivity\nnetworks. However, unmeasured confounders and correlations among measurements\nare often overlooked during model fitting, which may lead to spurious\nscientific discoveries. Motivated by functional magnetic resonance imaging\n(fMRI) studies, we propose a novel method for constructing brain connectivity\nnetworks with correlated replicates and latent effects. In a typical fMRI\nstudy, each participant is scanned and fMRI measurements are collected across a\nperiod of time. In many cases, subjects may have different states of mind that\ncannot be measured during the brain scan: for instance, some subjects may be\nawake during the first half of the brain scan, and may fall asleep during the\nsecond half of the brain scan. To model the correlation among replicates and\nlatent effects induced by the different states of mind, we assume that the\ncorrelated replicates within each independent subject follow a one-lag vector\nautoregressive model, and that the latent effects induced by the unmeasured\nconfounders are piecewise constant. The proposed method results in a convex\noptimization problem which we solve using a block coordinate descent algorithm.\nTheoretical guarantees are established for parameter estimation. We demonstrate\nvia extensive numerical studies that our method is able to estimate latent\nvariable graphical models with correlated replicates more accurately than\nexisting methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 16:49:01 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Jin", "Yanxin", ""], ["Ning", "Yang", ""], ["Tan", "Kean Ming", ""]]}, {"id": "2012.05187", "submitter": "Kean Ming Tan", "authors": "Xuming He, Xiaoou Pan, Kean Ming Tan, and Wen-Xin Zhou", "title": "Smoothed Quantile Regression with Large-Scale Inference", "comments": "An R package conquer for fitting smoothed quantile regression is\n  available in CRAN, https://cran.r-project.org/web/packages/conquer/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is a powerful tool for learning the relationship between\na response variable and a multivariate predictor while exploring heterogeneous\neffects. In this paper, we consider statistical inference for quantile\nregression with large-scale data in the \"increasing dimension\" regime. We\nprovide a comprehensive and in-depth analysis of a convolution-type smoothing\napproach that achieves adequate approximation to computation and inference for\nquantile regression. This method, which we refer to as {\\it{conquer}}, turns\nthe non-differentiable quantile loss function into a twice-differentiable,\nconvex and locally strongly convex surrogate, which admits a fast and scalable\nBarzilai-Borwein gradient-based algorithm to perform optimization, and\nmultiplier bootstrap for statistical inference. Theoretically, we establish\nexplicit non-asymptotic bounds on both estimation and Bahadur-Kiefer\nlinearization errors, from which we show that the asymptotic normality of the\nconquer estimator holds under a weaker requirement on the number of the\nregressors than needed for conventional quantile regression. Moreover, we prove\nthe validity of multiplier bootstrap confidence constructions. Our numerical\nstudies confirm the conquer estimator as a practical and reliable approach to\nlarge-scale inference for quantile regression. Software implementing the\nmethodology is available in the \\texttt{R} package \\texttt{conquer}.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 17:27:20 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 00:45:00 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["He", "Xuming", ""], ["Pan", "Xiaoou", ""], ["Tan", "Kean Ming", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "2012.05269", "submitter": "Marco Scutari", "authors": "Andrea Ruggieri, Francesco Stranieri, Fabio Stella and Marco Scutari", "title": "Hard and Soft EM in Bayesian Network Learning from Incomplete Data", "comments": "16 pages, 5 figures", "journal-ref": "Algorithms 2020, 13(12):329, 1-16", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Incomplete data are a common feature in many domains, from clinical trials to\nindustrial applications. Bayesian networks (BNs) are often used in these\ndomains because of their graphical and causal interpretations. BN parameter\nlearning from incomplete data is usually implemented with the\nExpectation-Maximisation algorithm (EM), which computes the relevant sufficient\nstatistics (\"soft EM\") using belief propagation. Similarly, the Structural\nExpectation-Maximisation algorithm (Structural EM) learns the network structure\nof the BN from those sufficient statistics using algorithms designed for\ncomplete data. However, practical implementations of parameter and structure\nlearning often impute missing data (\"hard EM\") to compute sufficient statistics\ninstead of using belief propagation, for both ease of implementation and\ncomputational speed. In this paper, we investigate the question: what is the\nimpact of using imputation instead of belief propagation on the quality of the\nresulting BNs? From a simulation study using synthetic data and reference BNs,\nwe find that it is possible to recommend one approach over the other in several\nscenarios based on the characteristics of the data. We then use this\ninformation to build a simple decision tree to guide practitioners in choosing\nthe EM algorithm best suited to their problem.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 19:13:32 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Ruggieri", "Andrea", ""], ["Stranieri", "Francesco", ""], ["Stella", "Fabio", ""], ["Scutari", "Marco", ""]]}, {"id": "2012.05298", "submitter": "J. Cricelio Montesinos-L\\'opez", "authors": "J. Cricelio Montesinos-L\\'opez, Antonio Capella, J. Andr\\'es Christen\n  and Josu\\'e Tago", "title": "Uncertainty quantification for fault slip inversion", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.geo-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient Bayesian approach to infer a fault displacement from\ngeodetic data in a slow slip event. Our physical model of the slip process\nreduces to a multiple linear regression subject to constraints. Assuming a\nGaussian model for the geodetic data and considering a multivariate truncated\nnormal prior distribution for the unknown fault slip, the resulting posterior\ndistribution is also multivariate truncated normal. Regarding the posterior, we\npropose an algorithm based on Optimal Directional Gibbs that allows us to\nefficiently sample from the resulting high-dimensional posterior distribution\nof along dip and along strike movements of our fault grid division. A synthetic\nfault slip example illustrates the flexibility and accuracy of the proposed\napproach. The methodology is also applied to a real data set, for the 2006\nGuerrero, Mexico, Slow Slip Event, where the objective is to recover the fault\nslip on a known interface that produces displacements observed at ground\ngeodetic stations. As a by-product of our approach, we are able to estimate\nmoment magnitude for the 2006 Guerrero Event with uncertainty quantification.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 20:18:48 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 00:57:38 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Montesinos-L\u00f3pez", "J. Cricelio", ""], ["Capella", "Antonio", ""], ["Christen", "J. Andr\u00e9s", ""], ["Tago", "Josu\u00e9", ""]]}, {"id": "2012.05301", "submitter": "David Norris", "authors": "David C. Norris", "title": "What Were They Thinking? Pharmacologic priors implicit in a choice of\n  3+3 dose-escalation design", "comments": "5 pages, 3 figures, 1 table, 13 references with added hyperlinks.\n  Version 2 restores to the exhibit on page 2 a DCG clause which v1 had\n  mistakenly cropped; additionally, footnote 6 is expanded and a few minor\n  typographical or grammatical improvements are made", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  If explicit, formal consideration of clinical pharmacology at all informs the\ndesign and conduct of modern oncology dose-finding trials, the designs\nthemselves hardly attest to this. Yet in conducting a trial, investigators\naffirm that they hold reasonable expectations of participant safety -\nexpectations that necessarily depend on beliefs about how certain pharmacologic\nparameters are distributed in the study population. Thus, these beliefs are\nimplicit in a trial's presumed conformance to a community standard of safety,\nand may therefore to some extent be reverse-engineered from trial designs. For\none popular form of dose-escalation trial design, I demonstrate here how this\nmay be done.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 20:33:38 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 14:18:38 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Norris", "David C.", ""]]}, {"id": "2012.05313", "submitter": "Han Lin Shang", "authors": "Ufuk Beyaztas and Han Lin Shang", "title": "A partial least squares approach for function-on-function interaction\n  regression", "comments": "34 pages, 6 figures, 2 tables, to appear at Computational Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A partial least squares regression is proposed for estimating the\nfunction-on-function regression model where a functional response and multiple\nfunctional predictors consist of random curves with quadratic and interaction\neffects. The direct estimation of a function-on-function regression model is\nusually an ill-posed problem. To overcome this difficulty, in practice, the\nfunctional data that belong to the infinite-dimensional space are generally\nprojected into a finite-dimensional space of basis functions. The\nfunction-on-function regression model is converted to a multivariate regression\nmodel of the basis expansion coefficients. In the estimation phase of the\nproposed method, the functional variables are approximated by a\nfinite-dimensional basis function expansion method. We show that the partial\nleast squares regression constructed via a functional response, multiple\nfunctional predictors, and quadratic/interaction terms of the functional\npredictors is equivalent to the partial least squares regression constructed\nusing basis expansions of functional variables. From the partial least squares\nregression of the basis expansions of functional variables, we provide an\nexplicit formula for the partial least squares estimate of the coefficient\nfunction of the function-on-function regression model. Because the true forms\nof the models are generally unspecified, we propose a forward procedure for\nmodel selection. The finite sample performance of the proposed method is\nexamined using several Monte Carlo experiments and two empirical data analyses,\nand the results were found to compare favorably with an existing method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 20:51:00 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Beyaztas", "Ufuk", ""], ["Shang", "Han Lin", ""]]}, {"id": "2012.05351", "submitter": "Maikol Sol\\'is", "authors": "Alberto J Hern\\'andez and Maikol Sol\\'is and Ronald A.\n  Z\\'u\\~niga-Rojas", "title": "Estimation of first-order sensitivity indices based on symmetric\n  reflected Vietoris-Rips complexes areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we estimate the first-order sensitivity index of random\nvariables within a model by reconstructing the embedding manifold of a\ntwo-dimensional cloud point. The model assumed has p predictors and a\ncontinuous outcome Y . Our method gauges the manifold through a Vietoris-Rips\ncomplex with a fixed radius for each variable. With this object, and using the\narea and its symmetric reflection, we can estimate an index of relevance for\neach predictor. The index reveals the geometric nature of the data points.\nAlso, given the method used, we can decide whether a pair of non-correlated\nrandom variables have some structural pattern in their interaction.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 22:45:16 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Hern\u00e1ndez", "Alberto J", ""], ["Sol\u00eds", "Maikol", ""], ["Z\u00fa\u00f1iga-Rojas", "Ronald A.", ""]]}, {"id": "2012.05394", "submitter": "Cristina Tortora Dr", "authors": "Hung Tong, Cristina Tortora", "title": "Cluster analysis and outlier detection with missing data", "comments": "4 pages, presented at MBC2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of multivariate contaminated normal (MCN) distributions is a useful\nmodel-based clustering technique to accommodate data sets with mild outliers.\nHowever, this model only works when fitted to complete data sets, which is\noften not the case in real applications. In this paper, we develop a framework\nfor fitting a mixture of MCN distributions to incomplete data sets, i.e. data\nsets with some values missing at random. We employ the expectation-conditional\nmaximization algorithm for parameter estimation. We use a simulation study to\ncompare the results of our model and a mixture of Student's t distributions for\nincomplete data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 01:17:24 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Tong", "Hung", ""], ["Tortora", "Cristina", ""]]}, {"id": "2012.05405", "submitter": "Angus McLure", "authors": "Angus McLure, Ben O'Neill, Helen Mayfield, Colleen Lau, Brady\n  McPherson", "title": "PoolTestR: An R package for estimating prevalence and regression\n  modelling with pooled samples", "comments": "18 pages, 2 figures, 1 table, 2 boxes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooled testing (also known as group testing), where diagnostic tests are\nperformed on pooled samples, has broad applications in the surveillance of\ndiseases in animals and humans. An increasingly common use case is molecular\nxenomonitoring (MX), where surveillance of vector-borne diseases is conducted\nby capturing and testing large numbers of vectors (e.g. mosquitoes). The R\npackage PoolTestR was developed to meet the needs of increasingly large and\ncomplex molecular xenomonitoring surveys but can be applied to analyse any data\ninvolving pooled testing. PoolTestR includes simple and flexible tools to\nestimate prevalence and fit fixed- and mixed-effect generalised linear models\nfor pooled data in frequentist and Bayesian frameworks. Mixed-effect models\nallow users to account for the hierarchical sampling designs that are often\nemployed in surveys, including MX. We demonstrate the utility of PoolTestR by\napplying it to a large synthetic dataset that emulates a MX survey with a\nhierarchical sampling design.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 01:53:20 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["McLure", "Angus", ""], ["O'Neill", "Ben", ""], ["Mayfield", "Helen", ""], ["Lau", "Colleen", ""], ["McPherson", "Brady", ""]]}, {"id": "2012.05432", "submitter": "Xin Li", "authors": "Xin Li and Dongya Wu", "title": "Low-rank matrix estimation in multi-response regression with measurement\n  errors: Statistical and computational guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we investigate the matrix estimation problem in the\nmulti-response regression model with measurement errors. A nonconvex\nerror-corrected estimator based on a combination of the amended loss function\nand the nuclear norm regularizer is proposed to estimate the matrix parameter.\nThen under the (near) low-rank assumption, we analyse statistical and\ncomputational theoretical properties of global solutions of the nonconvex\nregularized estimator from a general point of view. In the statistical aspect,\nwe establish the nonasymptotic recovery bound for any global solution of the\nnonconvex estimator, under restricted strong convexity on the loss function. In\nthe computational aspect, we solve the nonconvex optimization problem via the\nproximal gradient method. The algorithm is proved to converge to a near-global\nsolution and achieve a linear convergence rate. In addition, we also verify\nsufficient conditions for the general results to be held, in order to obtain\nprobabilistic consequences for specific types of measurement errors, including\nthe additive noise and missing data. Finally, theoretical consequences are\ndemonstrated by several numerical experiments on corrupted errors-in-variables\nmulti-response regression models. Simulation results reveal excellent\nconsistency with our theory under high-dimensional scaling.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 03:03:20 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 02:27:52 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Xin", ""], ["Wu", "Dongya", ""]]}, {"id": "2012.05465", "submitter": "Hongxiang Qiu", "authors": "Hongxiang Qiu, Alex Luedtke", "title": "Leveraging vague prior information in general models via iteratively\n  constructed Gamma-minimax estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gamma-minimax estimation is an approach to incorporate prior information into\nan estimation procedure when it is implausible to specify one particular prior\ndistribution. In this approach, we aim for an estimator that minimizes the\nworst-case Bayes risk over a set $\\Gamma$ of prior distributions.\nTraditionally, Gamma-minimax estimation is defined for parametric models. In\nthis paper, we define Gamma-minimaxity for general models and propose iterative\nalgorithms with convergence guarantees to compute Gamma-minimax estimators for\na general model space and a set of prior distributions constrained by\ngeneralized moments. We also propose encoding the space of candidate estimators\nby neural networks to enable flexible estimation. We illustrate our method in\ntwo settings, namely entropy estimation and a problem that arises in\nbiodiversity studies.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 05:39:17 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Qiu", "Hongxiang", ""], ["Luedtke", "Alex", ""]]}, {"id": "2012.05577", "submitter": "Haidong Li", "authors": "Haidong Li, Henry Lam, Zhe Liang, and Yijie Peng", "title": "Context-dependent Ranking and Selection under a Bayesian Framework", "comments": "The article was published without the co-Author's notice, and it is\n  withdrawn due to his objection", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a context-dependent ranking and selection problem. The best\ndesign is not universal but depends on the contexts. Under a Bayesian\nframework, we develop a dynamic sampling scheme for context-dependent\noptimization (DSCO) to efficiently learn and select the best designs in all\ncontexts. The proposed sampling scheme is proved to be consistent. Numerical\nexperiments show that the proposed sampling scheme significantly improves the\nefficiency in context-dependent ranking and selection.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 10:53:51 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 05:39:19 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Li", "Haidong", ""], ["Lam", "Henry", ""], ["Liang", "Zhe", ""], ["Peng", "Yijie", ""]]}, {"id": "2012.05591", "submitter": "Haidong Li", "authors": "Haidong Li, Henry Lam, and Yijie Peng", "title": "Efficient Learning for Clustering and Optimizing Context-Dependent\n  Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a simulation optimization problem for a context-dependent\ndecision-making. A Gaussian mixture model is proposed to capture the\nperformance clustering phenomena of context-dependent designs. Under a Bayesian\nframework, we develop a dynamic sampling policy to efficiently learn both the\nglobal information of each cluster and local information of each design for\nselecting the best designs in all contexts. The proposed sampling policy is\nproved to be consistent and achieve the asymptotically optimal sampling ratio.\nNumerical experiments show that the proposed sampling policy significantly\nimproves the efficiency in context-dependent simulation optimization.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 11:26:06 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 03:31:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Haidong", ""], ["Lam", "Henry", ""], ["Peng", "Yijie", ""]]}, {"id": "2012.05677", "submitter": "Miaomiao Su", "authors": "Miaomiao Su and Qihua Wang", "title": "A Convex Programming Solution Based Debiased Estimator for Quantile with\n  Missing Response and High-dimensional Covariables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the estimating problem of response quantile with\nhigh dimensional covariates when response is missing at random. Some existing\nmethods define root-n consistent estimators for the response quantile. But\nthese methods require correct specifications of both the conditional\ndistribution of response given covariates and the selection probability\nfunction. In this paper, a debiased method is proposed by solving a convex\nprogramming. The estimator obtained by the proposed method is asymptotically\nnormal given a correctly specified parametric model for the condition\ndistribution function, without the requirement to specify and estimate the\nselection probability function. Moreover, the proposed estimator is\nasymptotically more efficient than the existing estimators. The proposed method\nis evaluated by a simulation study and is illustrated by a real data example.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 14:04:53 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 02:31:34 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 02:32:44 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Su", "Miaomiao", ""], ["Wang", "Qihua", ""]]}, {"id": "2012.05722", "submitter": "Maren Hackenberg", "authors": "Maren Hackenberg, Marlon Grodd, Clemens Kreutz, Martina Fischer,\n  Janina Esins, Linus Grabenhenrich, Christian Karagiannidis, Harald Binder", "title": "Using Differentiable Programming for Flexible Statistical Modeling", "comments": "16 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable programming has recently received much interest as a paradigm\nthat facilitates taking gradients of computer programs. While the corresponding\nflexible gradient-based optimization approaches so far have been used\npredominantly for deep learning or enriching the latter with modeling\ncomponents, we want to demonstrate that they can also be useful for statistical\nmodeling per se, e.g., for quick prototyping when classical maximum likelihood\napproaches are challenging or not feasible. In an application from a COVID-19\nsetting, we utilize differentiable programming to quickly build and optimize a\nflexible prediction model adapted to the data quality challenges at hand.\nSpecifically, we develop a regression model, inspired by delay differential\nequations, that can bridge temporal gaps of observations in the central German\nregistry of COVID-19 intensive care cases for predicting future demand. With\nthis exemplary modeling challenge, we illustrate how differentiable programming\ncan enable simple gradient-based optimization of the model by automatic\ndifferentiation. This allowed us to quickly prototype a model under time\npressure that outperforms simpler benchmark models. We thus exemplify the\npotential of differentiable programming also outside deep learning\napplications, to provide more options for flexible applied statistical\nmodeling.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 12:33:49 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Hackenberg", "Maren", ""], ["Grodd", "Marlon", ""], ["Kreutz", "Clemens", ""], ["Fischer", "Martina", ""], ["Esins", "Janina", ""], ["Grabenhenrich", "Linus", ""], ["Karagiannidis", "Christian", ""], ["Binder", "Harald", ""]]}, {"id": "2012.05764", "submitter": "Flavio Goncalves", "authors": "Flavio B. Gon\\c{c}alves and Barbara C. C. Dias", "title": "Exact Bayesian inference for level-set Cox processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new methodology to perform Bayesian inference for a\nclass of multidimensional Cox processes in which the intensity function is\npiecewise constant. Poisson processes with piecewise constant intensity\nfunctions are believed to be suitable to model a variety of point process\nphenomena and, given its simpler structure, are expected to provided more\nprecise inference when compared to processes with non-parametric intensity\nfunctions that vary continuously across the space. The piecewise constant\nproperty is determined by a level-set function of a latent Gaussian process so\nthat the regions in which the intensity function is constant are defined in a\nflexible manner. Despite the intractability of the likelihood function and the\ninfinite dimensionality of the parameter space, inference is performed exactly,\nin the sense that no space discretization approximation is used and MCMC error\nis the only source of inaccuracy. That is achieved by using retrospective\nsampling techniques and devising a pseudo-marginal infinite-dimensional MCMC\nalgorithm that converges to the exact target posterior distribution. An\nextension to consider spatiotemporal models is also proposed. The efficiency of\nthe proposed methodology is investigated in simulated examples and its\napplicability is illustrated in the analysis of some real point process\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 15:50:35 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 15:50:55 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Gon\u00e7alves", "Flavio B.", ""], ["Dias", "Barbara C. C.", ""]]}, {"id": "2012.05824", "submitter": "Siegfried H\\\"ormann", "authors": "Siegfried H\\\"ormann and Fatima Jammoul", "title": "Preprocessing noisy functional data using factor models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider functional data which are measured on a discrete set of\nobservation points. Often such data are measured with noise, and then the\ntarget is to recover the underlying signal. Most commonly, practitioners use\nsome smoothing approach, e.g.,\\ kernel smoothing or spline fitting towards this\ngoal. The drawback of such curve fitting techniques is that they act function\nby function, and don't take into account information from the entire sample. In\nthis paper we argue that signal and noise can be naturally represented as the\ncommon and idiosyncratic component, respectively, of a factor model.\nAccordingly, we propose to an estimation scheme which is based on factor\nmodels. The purpose of this paper is to explain the reasoning behind our\napproach and to compare its performance on simulated and on real data to\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 16:54:44 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["H\u00f6rmann", "Siegfried", ""], ["Jammoul", "Fatima", ""]]}, {"id": "2012.05849", "submitter": "Ying Zhou", "authors": "Ying Zhou, Dehan Kong, Linbo Wang", "title": "The Promises of Parallel Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unobserved confounding presents a major threat to the validity of causal\ninference from observational studies. In this paper, we introduce a novel\nframework that leverages the information in multiple parallel outcomes for\nidentification and estimation of causal effects. Under a conditional\nindependence structure among multiple parallel outcomes, we achieve\nnonparametric identification with at least three parallel outcomes. We further\nshow that under a set of linear structural equation models, causal inference is\npossible with two parallel outcomes. We develop accompanying estimating\nprocedures and evaluate their finite sample performance through simulation\nstudies and a data application studying the causal effect of the tau protein\nlevel on various types of behavioral deficits.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 17:39:59 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zhou", "Ying", ""], ["Kong", "Dehan", ""], ["Wang", "Linbo", ""]]}, {"id": "2012.05936", "submitter": "Jonathan P Williams", "authors": "Jonathan P Williams, Danica M Ommen, Jan Hannig", "title": "Generalized fiducial factor: an alternative to the Bayes factor for\n  forensic identification of source problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One formulation of forensic identification of source problems is to determine\nthe source of trace evidence, for instance, glass fragments found on a suspect\nfor a crime. The current state of the science is to compute a Bayes factor (BF)\ncomparing the marginal distribution of measurements of trace evidence under two\ncompeting propositions for whether or not the unknown source evidence\noriginated from a specific source. The obvious problem with such an approach is\nthe ability to tailor the prior distributions (placed on the\nfeatures/parameters of the statistical model for the measurements of trace\nevidence) in favor of the defense or prosecution, which is further complicated\nby the fact that the typical number of measurements of trace evidence is\ntypically sufficiently small that prior choice/specification has a strong\ninfluence on the value of the BF. To remedy this problem of prior specification\nand choice, we develop an alternative to the BF, within the framework of\ngeneralized fiducial inference (GFI), that we term a {\\em generalized fiducial\nfactor} (GFF). Furthermore, we demonstrate empirically, on the synthetic and\nreal Netherlands Forensic Institute (NFI) casework data, deficiencies in the BF\nand classical/frequentist likelihood ratio (LR) approaches.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 19:22:22 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Williams", "Jonathan P", ""], ["Ommen", "Danica M", ""], ["Hannig", "Jan", ""]]}, {"id": "2012.05967", "submitter": "Matthias Katzfuss", "authors": "Brian Kidd, Matthias Katzfuss", "title": "Bayesian nonstationary and nonparametric covariance estimation for large\n  spatial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spatial statistics, it is often assumed that the spatial field of interest\nis stationary and its covariance has a simple parametric form, but these\nassumptions are not appropriate in many applications. Given replicate\nobservations of a Gaussian spatial field, we propose nonstationary and\nnonparametric Bayesian inference on the spatial dependence. Instead of\nestimating the quadratic (in the number of spatial locations) entries of the\ncovariance matrix, the idea is to infer a near-linear number of nonzero entries\nin a sparse Cholesky factor of the precision matrix. Our prior assumptions are\nmotivated by recent results on the exponential decay of the entries of this\nCholesky factor for Matern-type covariances under a specific ordering scheme.\nOur methods are highly scalable and parallelizable. We conduct numerical\ncomparisons and apply our methodology to climate-model output, enabling\nstatistical emulation of an expensive physical model.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 20:48:43 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Kidd", "Brian", ""], ["Katzfuss", "Matthias", ""]]}, {"id": "2012.05968", "submitter": "Yan-Cheng Chao", "authors": "Yan-Cheng Chao (1), Thomas M. Braun (1), Roy N. Tamura (2), Kelley M.\n  Kidwell (1) ((1) Department of Biostatistics, School of Public Health,\n  University of Michigan, Ann Arbor, USA, (2) Health Informatics Institute,\n  University of South Florida, Tampa, USA)", "title": "Power prior models for treatment effect estimation in a small n,\n  sequential, multiple assignment, randomized trial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A small n, sequential, multiple assignment, randomized trial (snSMART) is a\nsmall sample, two-stage design where participants receive up to two treatments\nsequentially, but the second treatment depends on response to the first\ntreatment. The treatment effect of interest in an snSMART is the first-stage\nresponse rate, but outcomes from both stages can be used to obtain more\ninformation from a small sample. A novel way to incorporate the outcomes from\nboth stages applies power prior models, in which first stage outcomes from an\nsnSMART are regarded as the primary data and second stage outcomes are regarded\nas supplemental. We apply existing power prior models to snSMART data, and we\nalso develop new extensions of power prior models. All methods are compared to\neach other and to the Bayesian joint stage model (BJSM) via simulation studies.\nBy comparing the biases and the efficiency of the response rate estimates among\nall proposed power prior methods, we suggest application of Fisher's exact test\nor the Bhattacharyya's overlap measure to an snSMART to estimate the treatment\neffect in an snSMART, which both have performance mostly as good or better than\nthe BJSM. We describe the situations where each of these suggested approaches\nis preferred.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 20:48:55 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Chao", "Yan-Cheng", ""], ["Braun", "Thomas M.", ""], ["Tamura", "Roy N.", ""], ["Kidwell", "Kelley M.", ""]]}, {"id": "2012.06093", "submitter": "Liangyuan Hu", "authors": "Liangyuan Hu, Jungang Zou, Chenyang Gu, Jiayi Ji, Michael Lopez, Minal\n  Kale", "title": "A flexible sensitivity analysis approach for unmeasured confounding with\n  multiple treatments and a binary outcome with application to SEER-Medicare\n  lung cancer data", "comments": "36 pages, 12 figures, 9 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the absence of a randomized experiment, a key assumption for drawing\ncausal inference about treatment effects is the ignorable treatment assignment.\nViolations of the ignorability assumption may lead to biased treatment effect\nestimates. Sensitivity analysis helps gauge how causal conclusions will be\naltered in response to the potential magnitude of departure from the\nignorability assumption. However, sensitivity analysis approaches for\nunmeasured confounding in the context of multiple treatments and binary\noutcomes are scarce. We propose a flexible Monte Carlo sensitivity analysis\napproach for causal inference in such settings. We first derive the general\nform of the bias introduced by unmeasured confounding, with emphasis on\ntheoretical properties uniquely relevant to multiple treatments. We then\npropose methods to encode the impact of unmeasured confounding on potential\noutcomes and adjust the estimates of causal effects in which the presumed\nunmeasured confounding is removed. Our proposed methods embed nested multiple\nimputation within the Bayesian framework, which allow for seamless integration\nof the uncertainty about the values of the sensitivity parameters and the\nsampling variability, as well as use of the Bayesian Additive Regression Trees\nfor modeling flexibility. Expansive simulations validate our methods and gain\ninsight into sensitivity analysis with multiple treatments. We use the\nSEER-Medicare data to demonstrate sensitivity analysis using three treatments\nfor early stage non-small cell lung cancer. The methods developed in this work\nare readily available in the R package SAMTx.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 03:07:05 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 02:31:02 GMT"}, {"version": "v3", "created": "Sat, 26 Jun 2021 18:41:18 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Hu", "Liangyuan", ""], ["Zou", "Jungang", ""], ["Gu", "Chenyang", ""], ["Ji", "Jiayi", ""], ["Lopez", "Michael", ""], ["Kale", "Minal", ""]]}, {"id": "2012.06102", "submitter": "Arsalane Chouaib Guidoum", "authors": "Arsalane Chouaib Guidoum", "title": "Kernel Estimator and Bandwidth Selection for Density and its\n  Derivatives: The kedd Package", "comments": "22 pages, 9 figures, part of the kedd package", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The kedd package providing additional smoothing techniques to the R\nstatistical system. Although various packages on the Comprehensive R Archive\nNetwork (CRAN) provide functions useful to nonparametric statistics, kedd aims\nto serve as a central location for more specifically of a nonparametric\nfunctions and data sets. The current feature set of the package can be split in\nfour main categories: compute the convolutions and derivatives of a kernel\nfunction, compute the kernel estimators for a density of probability and its\nderivatives, computing the bandwidth selectors with different methods,\ndisplaying the kernel estimators and selection functions of the bandwidth.\nMoreover, the package follows the general R philosophy of working with model\nobjects. This means that instead of merely returning, say, a kernel estimator\nof rth derivative of a density, many functions will return an object\ncontaining, it's functions are S3 classes (S3method). The object can then be\nmanipulated at one's will using various extraction, summary or plotting\nfunctions. Whenever possible, we develop a graphical user interface of the\nvarious functions of a coherent whole, to facilitate the use of this package.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 03:49:27 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Guidoum", "Arsalane Chouaib", ""]]}, {"id": "2012.06179", "submitter": "Sebastian Engelke", "authors": "Sebastian Engelke and Stanislav Volgushev", "title": "Structure learning for extremal tree models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extremal graphical models are sparse statistical models for multivariate\nextreme events. The underlying graph encodes conditional independencies and\nenables a visual interpretation of the complex extremal dependence structure.\nFor the important case of tree models, we develop a data-driven methodology for\nlearning the graphical structure. We show that sample versions of the extremal\ncorrelation and a new summary statistic, which we call the extremal variogram,\ncan be used as weights for a minimum spanning tree to consistently recover the\ntrue underlying tree. Remarkably, this implies that extremal tree models can be\nlearned in a completely non-parametric fashion by using simple summary\nstatistics and without the need to assume discrete distributions, existence of\ndensities, or parametric models for bivariate distributions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 08:09:44 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Engelke", "Sebastian", ""], ["Volgushev", "Stanislav", ""]]}, {"id": "2012.06199", "submitter": "Hangjin Jiang", "authors": "Yinrui Sun and Hangjin Jiang", "title": "Bayesian Variable Selection for Single Index Logistic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, variable selection is a key technology for handling\nhigh-dimensional problems with a small sample size but a large number of\ncovariables. Different variable selection methods were proposed for different\nmodels, such as linear model, logistic model and generalized linear model.\nHowever, fewer works focused on variable selection for single index models,\nespecially, for single index logistic model, due to the difficulty arose from\nthe unknown link function and the slow mixing rate of MCMC algorithm for\ntraditional logistic model. In this paper, we proposed a Bayesian variable\nselection procedure for single index logistic model by taking the advantage of\nGaussian process and data augmentation. Numerical results from simulations and\nreal data analysis show the advantage of our method over the state of arts.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 09:11:55 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Sun", "Yinrui", ""], ["Jiang", "Hangjin", ""]]}, {"id": "2012.06287", "submitter": "Michael Stephanou", "authors": "Michael Stephanou and Melvin Varughese", "title": "Sequential estimation of Spearman rank correlation using Hermite series\n  estimators", "comments": "Revised article incorporating comments by the reviewers, associate\n  editor and editor of the Journal of Multivariate Analysis (accepted\n  manuscript). Changes include a modified title and a significantly shortened\n  article", "journal-ref": null, "doi": "10.1016/j.jmva.2021.104783", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this article we describe a new Hermite series based sequential estimator\nfor the Spearman rank correlation coefficient and provide algorithms applicable\nin both the stationary and non-stationary settings. To treat the non-stationary\nsetting, we introduce a novel, exponentially weighted estimator for the\nSpearman rank correlation, which allows the local nonparametric correlation of\na bivariate data stream to be tracked. To the best of our knowledge this is the\nfirst algorithm to be proposed for estimating a time varying Spearman rank\ncorrelation that does not rely on a moving window approach. We explore the\npractical effectiveness of the Hermite series based estimators through real\ndata and simulation studies demonstrating good practical performance. The\nsimulation studies in particular reveal competitive performance compared to an\nexisting algorithm. The potential applications of this work are manifold. The\nHermite series based Spearman rank correlation estimator can be applied to fast\nand robust online calculation of correlation which may vary over time. Possible\nmachine learning applications include, amongst others, fast feature selection\nand hierarchical clustering on massive data sets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 12:43:19 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 09:46:09 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Stephanou", "Michael", ""], ["Varughese", "Melvin", ""]]}, {"id": "2012.06397", "submitter": "Florian Heinemann", "authors": "Florian Heinemann, Axel Munk, Yoav Zemel", "title": "Randomised Wasserstein Barycenter Computation: Resampling with\n  Statistical Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid resampling method to approximate finitely supported\nWasserstein barycenters on large-scale datasets, which can be combined with any\nexact solver. Nonasymptotic bounds on the expected error of the objective value\nas well as the barycenters themselves allow to calibrate computational cost and\nstatistical accuracy. The rate of these upper bounds is shown to be optimal and\nindependent of the underlying dimension, which appears only in the constants.\nUsing a simple modification of the subgradient descent algorithm of Cuturi and\nDoucet, we showcase the applicability of our method on a myriad of simulated\ndatasets, as well as a real-data example from cell microscopy which are out of\nreach for state of the art algorithms for computing Wasserstein barycenters.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:54:04 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 13:45:18 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Heinemann", "Florian", ""], ["Munk", "Axel", ""], ["Zemel", "Yoav", ""]]}, {"id": "2012.06488", "submitter": "Juhyun Park", "authors": "Juhyun Park, Jeongyoun Ahn, Yongho Jeon", "title": "Sparse Functional Linear Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Functional linear discriminant analysis offers a simple yet efficient method\nfor classification, with the possibility of achieving a perfect classification.\nSeveral methods are proposed in the literature that mostly address the\ndimensionality of the problem. On the other hand, there is a growing interest\nin interpretability of the analysis, which favors a simple and sparse solution.\nIn this work, we propose a new approach that incorporates a type of sparsity\nthat identifies non-zero sub-domains in the functional setting, offering a\nsolution that is easier to interpret without compromising performance. With the\nneed to embed additional constraints in the solution, we reformulate the\nfunctional linear discriminant analysis as a regularization problem with an\nappropriate penalty. Inspired by the success of $\\ell_1$-type regularization at\ninducing zero coefficients for scalar variables, we develop a new\nregularization method for functional linear discriminant analysis that\nincorporates an $L^1$-type penalty, $\\int |f|$, to induce zero regions. We\ndemonstrate that our formulation has a well defined solution that contains zero\nregions, achieving a functional sparsity in the sense of domain selection. In\naddition, the misclassification probability of the regularized solution is\nshown to converge to the Bayes error if the data are Gaussian. Our method does\nnot presume that the underlying function has zero regions in the domain, but\nproduces a sparse estimator that consistently estimates the true function\nwhether or not the latter is sparse. Numerical comparisons with existing\nmethods demonstrate this property in finite samples with both simulated and\nreal data examples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 16:53:40 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Park", "Juhyun", ""], ["Ahn", "Jeongyoun", ""], ["Jeon", "Yongho", ""]]}, {"id": "2012.06564", "submitter": "Marcos Matabuena", "authors": "Marcos Matabuena, Paulo F\\'elix, Carlos Meijide-Garcia and Francisco\n  Gude", "title": "Glucose values prediction five years ahead with a new framework of\n  missing responses in reproducing kernel Hilbert spaces, and the use of\n  continuous glucose monitoring technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AEGIS study possesses unique information on longitudinal changes in\ncirculating glucose through continuous glucose monitoring technology (CGM).\nHowever, as usual in longitudinal medical studies, there is a significant\namount of missing data in the outcome variables. For example, 40 percent of\nglycosylated hemoglobin (A1C) biomarker data are missing five years ahead. With\nthe purpose to reduce the impact of this issue, this article proposes a new\ndata analysis framework based on learning in reproducing kernel Hilbert spaces\n(RKHS) with missing responses that allows to capture non-linear relations\nbetween variable studies in different supervised modeling tasks. First, we\nextend the Hilbert-Schmidt dependence measure to test statistical independence\nin this context introducing a new bootstrap procedure, for which we prove\nconsistency. Next, we adapt or use existing models of variable selection,\nregression, and conformal inference to obtain new clinical findings about\nglucose changes five years ahead with the AEGIS data. The most relevant\nfindings are summarized below: i) We identify new factors associated with\nlong-term glucose evolution; ii) We show the clinical sensibility of CGM data\nto detect changes in glucose metabolism; iii) We can improve clinical\ninterventions based on our algorithms' expected glucose changes according to\npatients' baseline characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:51:44 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 18:47:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Matabuena", "Marcos", ""], ["F\u00e9lix", "Paulo", ""], ["Meijide-Garcia", "Carlos", ""], ["Gude", "Francisco", ""]]}, {"id": "2012.06705", "submitter": "Nehali Mhatre", "authors": "Nehali Mhatre and Daniel Cooley", "title": "Transformed-Linear Models for Time Series Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to capture the dependence in the upper tail of a time series, we\ndevelop non-negative regularly-varying time series models that are constructed\nsimilarly to classical non-extreme ARMA models. We first investigate\nconsistency requirements among the finite-dimensional collections of the\nelements of a regularly-varying time series. We define the tail pairwise\ndependence function (TPDF) to quantify the extremal dependence between two\nelements of the regularly-varying time series, and use the TPDF to define the\nconcept of weak tail stationarity for regularly varying time series. To develop\nour non-negative regularly varying ARMA-like time series models, we use\ntransformed-linear operations. We show existence and stationarity of these\nmodels and develop their properties, such as the model TPDF's. Motivated by\ninvestigating conditions conducive to the spread of wildfires, we fit models to\nhourly windspeed data and find that the fitted transformed-linear models\nproduce better estimates of upper tail quantities than alternative models.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 02:36:40 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 21:28:02 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Mhatre", "Nehali", ""], ["Cooley", "Daniel", ""]]}, {"id": "2012.06725", "submitter": "Nikos Vlassis", "authors": "Nikos Vlassis, Phil Hebda, Stephan McBride, Athanasios Noulas", "title": "On Proximal Causal Learning with Many Hidden Confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the proximal g-formula of Miao, Geng, and Tchetgen Tchetgen\n(2018) for causal inference under unobserved confounding using proxy variables.\nSpecifically, we show that the formula holds true for all causal models in a\ncertain equivalence class, and this class contains models in which the total\nnumber of levels for the set of unobserved confounders can be arbitrarily\nlarger than the number of levels of each proxy variable. Although\nstraightforward to obtain, the result can be significant for applications.\nSimulations corroborate our formal arguments.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 04:55:34 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Vlassis", "Nikos", ""], ["Hebda", "Phil", ""], ["McBride", "Stephan", ""], ["Noulas", "Athanasios", ""]]}, {"id": "2012.06762", "submitter": "BaoLuo Sun", "authors": "BaoLuo Sun and Ting Ye", "title": "Semiparametric causal mediation with unmeasured mediator-outcome\n  confounding", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the exposure can be randomly assigned in studies of mediation\neffects, any form of direct intervention on the mediator is often infeasible.\nAs a result, unmeasured mediator-outcome confounding can seldom be ruled out.\nWe propose semiparametric identification of natural direct and indirect effects\nin the presence of unmeasured mediator-outcome confounding by leveraging\nheteroskedasticity restrictions on the observed data law. For inference, we\ndevelop semiparametric estimators that remain consistent under partial\nmisspecifications of the observed data model. We illustrate the proposed\nestimators through both simulations and an application to evaluate the effect\nof self-efficacy on fatigue among health care workers during the COVID-19\noutbreak.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 09:16:43 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 11:25:46 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 16:14:01 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Sun", "BaoLuo", ""], ["Ye", "Ting", ""]]}, {"id": "2012.06784", "submitter": "Lorenzo Fassina PhD", "authors": "Lorenzo Fassina, Alessandro Faragli, Francesco Paolo Lo Muzio,\n  Sebastian Kelle, Carlo Campana, Burkert Pieske, Frank Edelmann, Alessio\n  Alogna", "title": "A random shuffle method to expand a narrow dataset and overcome the\n  associated challenges in a clinical study: a heart failure cohort example", "comments": null, "journal-ref": "Frontiers in Cardiovascular Medicine 2020;7:599923", "doi": "10.3389/fcvm.2020.599923", "report-no": null, "categories": "q-bio.QM cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heart failure (HF) affects at least 26 million people worldwide, so\npredicting adverse events in HF patients represents a major target of clinical\ndata science. However, achieving large sample sizes sometimes represents a\nchallenge due to difficulties in patient recruiting and long follow-up times,\nincreasing the problem of missing data. To overcome the issue of a narrow\ndataset cardinality (in a clinical dataset, the cardinality is the number of\npatients in that dataset), population-enhancing algorithms are therefore\ncrucial. The aim of this study was to design a random shuffle method to enhance\nthe cardinality of an HF dataset while it is statistically legitimate, without\nthe need of specific hypotheses and regression models. The cardinality\nenhancement was validated against an established random repeated-measures\nmethod with regard to the correctness in predicting clinical conditions and\nendpoints. In particular, machine learning and regression models were employed\nto highlight the benefits of the enhanced datasets. The proposed random shuffle\nmethod was able to enhance the HF dataset cardinality (711 patients before\ndataset preprocessing) circa 10 times and circa 21 times when followed by a\nrandom repeated-measures approach. We believe that the random shuffle method\ncould be used in the cardiovascular field and in other data science problems\nwhen missing data and the narrow dataset cardinality represent an issue.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 10:59:38 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Fassina", "Lorenzo", ""], ["Faragli", "Alessandro", ""], ["Muzio", "Francesco Paolo Lo", ""], ["Kelle", "Sebastian", ""], ["Campana", "Carlo", ""], ["Pieske", "Burkert", ""], ["Edelmann", "Frank", ""], ["Alogna", "Alessio", ""]]}, {"id": "2012.06830", "submitter": "Jingxin Zhang", "authors": "Jingxin Zhang, Hao Chen, Songhang Chen, and Xia Hong", "title": "An improved mixture of probabilistic PCA for nonlinear data-driven\n  process monitoring", "comments": null, "journal-ref": "IEEE Transactions on Cybernetics, 2019, 49(1):198-210", "doi": "10.1109/TCYB.2017.2771229", "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An improved mixture of probabilistic principal component analysis (PPCA) has\nbeen introduced for nonlinear data-driven process monitoring in this paper. To\nrealize this purpose, the technique of a mixture of probabilistic principal\ncomponent analysers is utilized to establish the model of the underlying\nnonlinear process with local PPCA models, where a novel composite monitoring\nstatistic is proposed based on the integration of two monitoring statistics in\nmodified PPCA-based fault detection approach. Besides, the weighted mean of the\nmonitoring statistics aforementioned is utilised as a metrics to detect\npotential abnormalities. The virtues of the proposed algorithm have been\ndiscussed in comparison with several unsupervised algorithms. Finally,\nTennessee Eastman process and an autosuspension model are employed to\ndemonstrate the effectiveness of the proposed scheme further.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 14:29:07 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Jingxin", ""], ["Chen", "Hao", ""], ["Chen", "Songhang", ""], ["Hong", "Xia", ""]]}, {"id": "2012.06862", "submitter": "Kenneth Harris", "authors": "Kenneth D. Harris", "title": "A Shift Test for Independence in Generic Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a family of conservative statistical tests for independence of\ntwo autocorrelated time series. The series may take values in any sets, and one\nof them must be stationary. A user-specified function quantifying the\nassociation of a segment of the two series is compared to an ensemble obtained\nby time-shifting the stationary series -N to N steps. If the series are\nindependent, the unshifted value is in the top m shifted values with\nprobability at most m/(N+1). For large N, the probability approaches m/(2N+1).\nA conservative test rejects independence at significance {\\alpha} if the\nunshifted value is in the top {\\alpha}(N+1), and has half the power of an\napproximate test valid in the large N limit. We illustrate this framework with\na test for correlation of autocorrelated categorical time series.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 17:09:13 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Harris", "Kenneth D.", ""]]}, {"id": "2012.06865", "submitter": "Falco J. Bargagli Stoffi", "authors": "Francesca Dominici and Falco J. Bargagli-Stoffi and Fabrizia Mealli", "title": "From controlled to undisciplined data: estimating causal effects in the\n  era of data science using a potential outcome framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper discusses the fundamental principles of causal inference - the\narea of statistics that estimates the effect of specific occurrences,\ntreatments, interventions, and exposures on a given outcome from experimental\nand observational data. We explain the key assumptions required to identify\ncausal effects, and highlight the challenges associated with the use of\nobservational data. We emphasize that experimental thinking is crucial in\ncausal inference. The quality of the data (not necessarily the quantity), the\nstudy design, the degree to which the assumptions are met, and the rigor of the\nstatistical analysis allow us to credibly infer causal effects. Although we\nadvocate leveraging the use of big data and the application of machine learning\n(ML) algorithms for estimating causal effects, they are not a substitute of\nthoughtful study design. Concepts are illustrated via examples.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 17:15:46 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Dominici", "Francesca", ""], ["Bargagli-Stoffi", "Falco J.", ""], ["Mealli", "Fabrizia", ""]]}, {"id": "2012.06893", "submitter": "Sven Serneels", "authors": "Emmanuel Jordy Menvouta and Sven Serneels and Tim Verdonck", "title": "Sparse dimension reduction based on energy and ball statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As its name suggests, sufficient dimension reduction (SDR) targets to\nestimate a subspace from data that contains all information sufficient to\nexplain a dependent variable. Ample approaches exist to SDR, some of the most\nrecent of which rely on minimal to no model assumptions. These are defined\naccording to an optimization criterion that maximizes a nonparametric measure\nof association. The original estimators are nonsparse, which means that all\nvariables contribute to the model. However, in many practical applications, an\nSDR technique may be called for that is sparse and as such, intrinsically\nperforms sufficient variable selection (SVS). This paper examines how such a\nsparse SDR estimator can be constructed. Three variants are investigated,\ndepending on different measures of association: distance covariance, martingale\ndifference divergence and ball covariance. A simulation study shows that each\nof these estimators can achieve correct variable selection in highly nonlinear\ncontexts, yet are sensitive to outliers and computationally intensive. The\nstudy sheds light on the subtle differences between the methods. Two examples\nillustrate how these new estimators can be applied in practice, with a slight\npreference for the option based on martingale difference divergence in the\nbioinformatics example.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 19:23:36 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Menvouta", "Emmanuel Jordy", ""], ["Serneels", "Sven", ""], ["Verdonck", "Tim", ""]]}, {"id": "2012.07021", "submitter": "Jingxin Zhang", "authors": "Jingxin Zhang, Maoyin Chen, Hao Chen, Xia Hong, and Donghua Zhou", "title": "Process monitoring based on orthogonal locality preserving projection\n  with maximum likelihood estimation", "comments": null, "journal-ref": "Industrial and Engineering Chemistry Research, 58(14), 5579-5587,\n  2019", "doi": "10.1021/acs.iecr.8b05875", "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By integrating two powerful methods of density reduction and intrinsic\ndimensionality estimation, a new data-driven method, referred to as OLPP-MLE\n(orthogonal locality preserving projection-maximum likelihood estimation), is\nintroduced for process monitoring. OLPP is utilized for dimensionality\nreduction, which provides better locality preserving power than locality\npreserving projection. Then, the MLE is adopted to estimate intrinsic\ndimensionality of OLPP. Within the proposed OLPP-MLE, two new static measures\nfor fault detection $T_{\\scriptscriptstyle {OLPP}}^2$ and ${\\rm\nSPE}_{\\scriptscriptstyle {OLPP}}$ are defined. In order to reduce algorithm\ncomplexity and ignore data distribution, kernel density estimation is employed\nto compute thresholds for fault diagnosis. The effectiveness of the proposed\nmethod is demonstrated by three case studies.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 10:20:47 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Jingxin", ""], ["Chen", "Maoyin", ""], ["Chen", "Hao", ""], ["Hong", "Xia", ""], ["Zhou", "Donghua", ""]]}, {"id": "2012.07066", "submitter": "Jacques Balayla", "authors": "Jacques Balayla", "title": "Invariant Points on the Screening Plane: a Geometric Definition of the\n  Likelihood Ratio (LR+)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  From the fundamental theorem of screening we obtain the following\nmathematical relationship relaying the positive predictive value ($\\rho(\\phi)$)\nof a screening test to the prevalence ($\\phi$) of disease:\n  $\\displaystyle\\lim_{\\varepsilon \\to 2}{\\displaystyle\n\\int_{0}^{1}}{\\rho(\\phi)d\\phi} = 1$\n  where $\\varepsilon$ is equal to the sum of the sensitivity ($a$) and\nspecificity ($b$) parameters of the test in question. However, identical values\nof $\\varepsilon$ may yield different shapes of the screening curve since\n$\\varepsilon$ does not respect traditional commutative properties given the\ninvariant points of the screening plane. In order to distinguish between two\nscreening curves with identical $\\varepsilon$ values, we make use of the angle\n$\\beta$ created by the line between the origin invariant and the prevalence\nthreshold $\\phi_e$ to make a right-angle triangle. In this work, we provide\nderivation of this angle $\\beta$ and show its value to be:\n  $\\beta = arctan(\\Psi) = arctan\\left(\\sqrt{\\frac{1-b}{a}}\\right)$\n  From the above relationship, we derive the positive likelihood ratio (LR+),\ndefined as the likelihood of a test result in patients with the disease divided\nby the likelihood of the test result in patients without the disease, as\nfollows:\n  $LR+ = \\frac{a}{1-b} = cot^2{(\\beta)}$\n  Using the concepts of the prevalence threshold and the invariant points on\nthe screening plane, the work herein presented provides a new geometric\ndefinition of the positive likelihood ratio (LR+) throughout the prevalence\nspectrum and describes a formal measure to compare the performance of two\nscreening tests whose $\\varepsilon$ are equal.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 14:10:41 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Balayla", "Jacques", ""]]}, {"id": "2012.07092", "submitter": "Pengfei Li", "authors": "Meng Yuan, Chunlin Wang, Boxi Lin, and Pengfei Li", "title": "Semiparametric inference on general functionals of two semicontinuous\n  populations", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose new semiparametric procedures for making inference\non linear functionals and their functions of two semicontinuous populations.\nThe distribution of each population is usually characterized by a mixture of a\ndiscrete point mass at zero and a continuous skewed positive component, and\nhence such distribution is semicontinuous in the nature. To utilize the\ninformation from both populations, we model the positive components of the two\nmixture distributions via a semiparametric density ratio model. Under this\nmodel setup, we construct the maximum empirical likelihood estimators of the\nlinear functionals and their functions, and establish the asymptotic normality\nof the proposed estimators. We show the proposed estimators of the linear\nfunctionals are more efficient than the fully nonparametric ones. The developed\nasymptotic results enable us to construct confidence regions and perform\nhypothesis tests for the linear functionals and their functions. We further\napply these results to several important summary quantities such as the\nmoments, the mean ratio, the coefficient of variation, and the generalized\nentropy class of inequality measures. Simulation studies demonstrate the\nadvantages of our proposed semiparametric method over some existing methods.\nTwo real data examples are provided for illustration.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 16:24:34 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 03:53:58 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Yuan", "Meng", ""], ["Wang", "Chunlin", ""], ["Lin", "Boxi", ""], ["Li", "Pengfei", ""]]}, {"id": "2012.07133", "submitter": "Prabrisha Rakshit", "authors": "Zijian Guo, Prabrisha Rakshit, Daniel S. Herman and Jinbo Chen", "title": "Inference for the Case Probability in High-dimensional Logistic\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling patients in electronic health records with respect to their statuses\nof having a disease or condition, i.e. case or control statuses, has\nincreasingly relied on prediction models using high-dimensional variables\nderived from structured and unstructured electronic health record data. A major\nhurdle currently is a lack of valid statistical inference methods for the case\nprobability. In this paper, considering high-dimensional sparse logistic\nregression models for prediction, we propose a novel bias-corrected estimator\nfor the case probability through the development of linearization and variance\nenhancement techniques. We establish asymptotic normality of the proposed\nestimator for any loading vector in high dimensions. We construct a confidence\ninterval for the case probability and propose a hypothesis testing procedure\nfor patient case-control labelling. We demonstrate the proposed method via\nextensive simulation studies and application to real-world electronic health\nrecord data.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 19:58:33 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 04:08:39 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Guo", "Zijian", ""], ["Rakshit", "Prabrisha", ""], ["Herman", "Daniel S.", ""], ["Chen", "Jinbo", ""]]}, {"id": "2012.07182", "submitter": "Bo Zhang", "authors": "Bo Zhang, Emily J. Mackay, Mike Baiocchi", "title": "Statistical matching and subclassification with a continuous dose:\n  characterization, algorithms, and inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Subclassification and matching are often used to adjust for observed\ncovariates in observational studies; however, they are largely restricted to\nrelatively simple study designs with a binary treatment. One important\nexception is Lu et al.(2001), who considered optimal pair matching with a\ncontinuous treatment dose. In this article, we propose two criteria for optimal\nsubclassification/full matching based on subclass homogeneity with a continuous\ntreatment dose, and propose an efficient polynomial-time algorithm that is\nguaranteed to find an optimal subclassification with respect to one criterion\nand serves as a 2-approximation algorithm for the other criterion. We discuss\nhow to incorporate treatment dose and use appropriate penalties to control the\nnumber of subclasses in the design. Via extensive simulations, we\nsystematically examine the performance of our proposed method, and demonstrate\nthat combining our proposed subclassification scheme with regression adjustment\nhelps reduce model dependence for parametric causal inference with a continuous\ntreatment dose. We illustrate the new design and how to conduct\nrandomization-based statistical inference under the new design using Medicare\nand Medicaid claims data to study the effect of transesophageal\nechocardiography (TEE) during CABG surgery on patients' 30-day mortality rate.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 23:39:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Bo", ""], ["Mackay", "Emily J.", ""], ["Baiocchi", "Mike", ""]]}, {"id": "2012.07230", "submitter": "Min Wang", "authors": "Min Wang, Li Sheng, Donghua Zhou, and Maoyin Chen", "title": "A Feature Weighted Mixed Naive Bayes Model for Monitoring Anomalies in\n  the Fan System of a Thermal Power Plant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increasing intelligence and integration, a great number of\ntwo-valued variables (generally stored in the form of 0 or 1 value) often exist\nin large-scale industrial processes. However, these variables cannot be\neffectively handled by traditional monitoring methods such as LDA, PCA and PLS.\nRecently, a mixed hidden naive Bayesian model (MHNBM) is developed for the\nfirst time to utilize both two-valued and continuous variables for abnormality\nmonitoring. Although MHNBM is effective, it still has some shortcomings that\nneed to be improved. For MHNBM, the variables with greater correlation to other\nvariables have greater weights, which cannot guarantee greater weights are\nassigned to the more discriminating variables. In addition, the conditional\nprobability must be computed based on the historical data. When the training\ndata is scarce, the conditional probability between continuous variables tends\nto be uniformly distributed, which affects the performance of MHNBM. Here a\nnovel feature weighted mixed naive Bayes model (FWMNBM) is developed to\novercome the above shortcomings. For FWMNBM, the variables that are more\ncorrelated to the class have greater weights, which makes the more\ndiscriminating variables contribute more to the model. At the same time, FWMNBM\ndoes not have to calculate the conditional probability between variables, thus\nit is less restricted by the number of training data samples. Compared with\nMHNBM, FWMNBM has better performance, and its effectiveness is validated\nthrough the numerical cases of a simulation example and a practical case of\nZhoushan thermal power plant (ZTPP), China.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 03:05:54 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 08:47:37 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 02:14:11 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Wang", "Min", ""], ["Sheng", "Li", ""], ["Zhou", "Donghua", ""], ["Chen", "Maoyin", ""]]}, {"id": "2012.07363", "submitter": "Debarghya Mukherjee", "authors": "Debarghya Mukherjee, Aritra Guha, Justin Solomon, Yuekai Sun and\n  Mikhail Yurochkin", "title": "Outlier-Robust Optimal Transport", "comments": "Accepted in Proceedings of the 38th International Conference on\n  Machine Learning, PMLR 139, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal transport (OT) measures distances between distributions in a way that\ndepends on the geometry of the sample space. In light of recent advances in\ncomputational OT, OT distances are widely used as loss functions in machine\nlearning. Despite their prevalence and advantages, OT loss functions can be\nextremely sensitive to outliers. In fact, a single adversarially-picked outlier\ncan increase the standard $W_2$-distance arbitrarily. To address this issue, we\npropose an outlier-robust formulation of OT. Our formulation is convex but\nchallenging to scale at a first glance. Our main contribution is deriving an\n\\emph{equivalent} formulation based on cost truncation that is easy to\nincorporate into modern algorithms for computational OT. We demonstrate the\nbenefits of our formulation in mean estimation problems under the Huber\ncontamination model in simulations and outlier detection tasks on real data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 09:28:16 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 14:05:12 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Mukherjee", "Debarghya", ""], ["Guha", "Aritra", ""], ["Solomon", "Justin", ""], ["Sun", "Yuekai", ""], ["Yurochkin", "Mikhail", ""]]}, {"id": "2012.07385", "submitter": "Mehdi Dagdoug", "authors": "Mehdi Dagdoug, Camelia Goga, David Haziza", "title": "Model-assisted estimation in high-dimensional settings for survey data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-assisted estimators have attracted a lot of attention in the last three\ndecades. These estimators attempt to make an efficient use of auxiliary\ninformation available at the estimation stage. A working model linking the\nsurvey variable to the auxiliary variables is specified and fitted on the\nsample data to obtain a set of predictions, which are then incorporated in the\nestimation procedures. A nice feature of model-assisted procedures is that they\nmaintain important design properties such as consistency and asymptotic\nunbiasedness irrespective of whether or not the working model is correctly\nspecified. In this article, we examine several model-assisted estimators from a\ndesign-based point of view and in a high-dimensional setting, including\npenalized estimators and tree-based estimators. We conduct an extensive\nsimulation study using data from the Irish Commission for Energy Regulation\nSmart Metering Project, in order to assess the performance of several\nmodel-assisted estimators in terms of bias and efficiency in this\nhigh-dimensional data set.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 10:13:58 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 14:50:16 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Dagdoug", "Mehdi", ""], ["Goga", "Camelia", ""], ["Haziza", "David", ""]]}, {"id": "2012.07429", "submitter": "David Rossell", "authors": "David Rossell, Oriol Abril, Anirban Bhattacharya", "title": "Approximate Laplace approximations for scalable model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the approximate Laplace approximation (ALA) to evaluate integrated\nlikelihoods, a bottleneck in Bayesian model selection. The Laplace\napproximation (LA) is a popular tool that speeds up such computation and equips\nstrong model selection properties. However, when the sample size is large or\none considers many models the cost of the required optimizations becomes\nimpractical. ALA reduces the cost to that of solving a least-squares problem\nfor each model. Further, it enables efficient computation across models such as\nsharing pre-computed sufficient statistics and certain operations in matrix\ndecompositions. We prove that in generalized (possibly non-linear) models ALA\nachieves a strong form of model selection consistency for a suitably-defined\noptimal model, at the same functional rates as exact computation. We consider\nfixed- and high-dimensional problems, group and hierarchical constraints, and\nthe possibility that all models are misspecified. We also obtain ALA rates for\nGaussian regression under non-local priors, an important example where the LA\ncan be costly and does not consistently estimate the integrated likelihood. Our\nexamples include non-linear regression, logistic, Poisson and survival models.\nWe implement the methodology in the R package mombf.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 11:29:11 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 08:06:01 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Rossell", "David", ""], ["Abril", "Oriol", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "2012.07485", "submitter": "Jai-Hua Yen", "authors": "Jai-Hua Yen, Chun-Huo Chiu", "title": "Richness estimation with species identity error", "comments": "6 pages, ISI WSC 2019 conference", "journal-ref": "Proceedings 62th International Statistical Institute (ISI) World\n  Statistics Congress, Volume 6, 401-408 (2020)", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Richness estimation of an interesting area is always a challenge statistical\nwork due to small sample size or species identity error. In the literatures,\nmost richness estimators were only proposed to tackle the underestimation of\nthe size-limited sample. However, species identity error almost occurs in each\nspecies survey and seriously reduces the accuracy of observed, singleton, and\ndoubleton richness in turns to influence the behavior of richness estimator.\nTherefore, to estimate the true richness, the biased collected data due to\nspecies identity error should be modified before processing the richness\nestimation work. In the manuscript, we propose a new approach to correct the\nbias of richness estimation due to species identity error. First, a species\nlist inventory from a subplot obtained by the investigator was used to estimate\nthe species identity error rate. Then, we can correct the biased observed,\nsingleton, and doubleton richness of the raw sampling data from the interesting\narea. Finally, the richness estimators proposed in the literature could be\nsupplied to get the more correct estimates based on adjusted observed data. To\ninvestigate the behavior of the proposed method, we performed simulations by\ngenerating data sets from various species models with different species\nidentity error rates. For the purpose of illustration, the real data was\nsupplied to demonstrate our proposed approach. A presence/absence weeds species\nwas surveyed in the organic farmland located at Soft Bridge County in the North\nof Taiwan.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 13:04:13 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 13:04:46 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Yen", "Jai-Hua", ""], ["Chiu", "Chun-Huo", ""]]}, {"id": "2012.07941", "submitter": "Yi Zuo", "authors": "Yi Zuo, Thomas G. Stewart, Jeffrey D. Blume", "title": "Variable Selection with Second-Generation P-Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many statistical methods have been proposed for variable selection in the\npast century, but few balance inference and prediction tasks well. Here we\nreport on a novel variable selection approach called Penalized regression with\nSecond-Generation P-Values (ProSGPV). It captures the true model at the best\nrate achieved by current standards, is easy to implement in practice, and often\nyields the smallest parameter estimation error. The idea is to use an l0\npenalization scheme with second-generation p-values (SGPV), instead of\ntraditional ones, to determine which variables remain in a model. The approach\nyields tangible advantages for balancing support recovery, parameter\nestimation, and prediction tasks. The ProSGPV algorithm can maintain its good\nperformance even when there is strong collinearity among features or when a\nhigh dimensional feature space with p > n is considered. We present extensive\nsimulations and a real-world application comparing the ProSGPV approach with\nsmoothly clipped absolute deviation (SCAD), adaptive lasso (AL), and mini-max\nconcave penalty with penalized linear unbiased selection (MC+). While the last\nthree algorithms are among the current standards for variable selection,\nProSGPV has superior inference performance and comparable prediction\nperformance in certain scenarios. Supplementary materials are available online.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 21:08:18 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 13:22:41 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zuo", "Yi", ""], ["Stewart", "Thomas G.", ""], ["Blume", "Jeffrey D.", ""]]}, {"id": "2012.07977", "submitter": "Didong Li", "authors": "Didong Li, Andrew Jones and Barbara Engelhardt", "title": "Probabilistic Contrastive Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction is useful for exploratory data analysis. In many\napplications, it is of interest to discover variation that is enriched in a\n\"foreground\" dataset relative to a \"background\" dataset. Recently, contrastive\nprincipal component analysis (CPCA) was proposed for this setting. However, the\nlack of a formal probabilistic model makes it difficult to reason about CPCA\nand to tune its hyperparameter. In this work, we propose probabilistic\ncontrastive principal component analysis (PCPCA), a model-based alternative to\nCPCA. We discuss how to set the hyperparameter in theory and in practice, and\nwe show several of PCPCA's advantages over CPCA, including greater\ninterpretability, uncertainty quantification and principled inference,\nrobustness to noise and missing data, and the ability to generate data from the\nmodel. We demonstrate PCPCA's performance through a series of simulations and\ncase-control experiments with datasets of gene expression, protein expression,\nand images.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 22:21:50 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 22:53:49 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Li", "Didong", ""], ["Jones", "Andrew", ""], ["Engelhardt", "Barbara", ""]]}, {"id": "2012.08015", "submitter": "Annie Sauer", "authors": "Annie Sauer, Robert B. Gramacy, David Higdon", "title": "Active Learning for Deep Gaussian Process Surrogates", "comments": "34 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Gaussian processes (DGPs) are increasingly popular as predictive models\nin machine learning (ML) for their non-stationary flexibility and ability to\ncope with abrupt regime changes in training data. Here we explore DGPs as\nsurrogates for computer simulation experiments whose response surfaces exhibit\nsimilar characteristics. In particular, we transport a DGP's automatic warping\nof the input space and full uncertainty quantification (UQ), via a novel\nelliptical slice sampling (ESS) Bayesian posterior inferential scheme, through\nto active learning (AL) strategies that distribute runs non-uniformly in the\ninput space -- something an ordinary (stationary) GP could not do. Building up\nthe design sequentially in this way allows smaller training sets, limiting both\nexpensive evaluation of the simulator code and mitigating cubic costs of DGP\ninference. When training data sizes are kept small through careful acquisition,\nand with parsimonious layout of latent layers, the framework can be both\neffective and computationally tractable. Our methods are illustrated on\nsimulation data and two real computer experiments of varying input\ndimensionality. We provide an open source implementation in the \"deepgp\"\npackage on CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 00:09:37 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Sauer", "Annie", ""], ["Gramacy", "Robert B.", ""], ["Higdon", "David", ""]]}, {"id": "2012.08089", "submitter": "Pratishtha Batra", "authors": "Pratishtha Batra, Neil A. Spencer and Pritam Ranjan", "title": "IsoCheck: An R Package to check Isomorphism for Two-level Factorial\n  Designs with Randomization Restrictions", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorial designs are often used in various industrial and sociological\nexperiments to identify significant factors and factor combinations that may\naffect the process response. In the statistics literature, several studies have\ninvestigated the analysis, construction, and isomorphism of factorial and\nfractional factorial designs. When there are multiple choices for a design, it\nis helpful to have an easy-to-use tool for identifying which are distinct, and\nwhich of those can be efficiently analyzed/has good theoretical properties. For\nthis task, we present an R library called IsoCheck that checks the isomorphism\nof multi-stage 2^n factorial experiments with randomization restrictions.\nThrough representing the factors and their combinations as a finite projective\ngeometry, IsoCheck recasts the problem of searching over all possible\nrelabelings as a search over collineations, then exploits projective geometric\nproperties of the space to make the search much more efficient. Furthermore, a\nbitstring representation of the factorial effects is used to characterize all\npossible rearrangements of designs, thus facilitating quick comparisons after\nrelabeling. We present several examples with R code to illustrate the usage of\nthe main functions in IsoCheck. Besides checking equivalence and isomorphism of\n2^n multi-stage factorial designs, we demonstrate how the functions of the\npackage can be used to create a catalog of all non-isomorphic designs, and\nsubsequently rank these designs based on a suitably defined ranking criterion.\nIsoCheck is free software and distributed under the General Public License and\navailable from the Comprehensive R Archive Network.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 04:55:59 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Batra", "Pratishtha", ""], ["Spencer", "Neil A.", ""], ["Ranjan", "Pritam", ""]]}, {"id": "2012.08154", "submitter": "Ludvig Hult", "authors": "Ludvig Hult and Dave Zachariah", "title": "Inference of Causal Effects when Control Variables are Unknown", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional methods in causal effect inferencetypically rely on specifying a\nvalid set of control variables. When this set is unknown or misspecified,\ninferences will be erroneous. We propose a method for inferring average causal\neffects when all potential confounders are observed, but thecontrol variables\nare unknown. When the data-generating process belongs to the class of acyclical\nlinear structural causal models, we prove that themethod yields asymptotically\nvalid confidence intervals. Our results build upon a smooth characterization of\nlinear directed acyclic graphs. We verify the capability of the method to\nproduce valid confidence intervals for average causal effects using synthetic\ndata, even when the appropriate specification of control variables is unknown.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 08:54:26 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 14:59:07 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 14:39:52 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hult", "Ludvig", ""], ["Zachariah", "Dave", ""]]}, {"id": "2012.08223", "submitter": "Sayar Karmakar", "authors": "Sayar Karmakar, Marek Chudy and Wei Biao Wu", "title": "Long-term prediction intervals with many covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate forecasting is one of the fundamental focus in the literature of\neconometric time-series. Often practitioners and policy makers want to predict\noutcomes of an entire time horizon in the future instead of just a single\n$k$-step ahead prediction. These series, apart from their own possible\nnon-linear dependence, are often also influenced by many external predictors.\nIn this paper, we construct prediction intervals of time-aggregated forecasts\nin a high-dimensional regression setting. Our approach is based on quantiles of\nresiduals obtained by the popular LASSO routine. We allow for general\nheavy-tailed, long-memory, and nonlinear stationary error process and\nstochastic predictors. Through a series of systematically arranged consistency\nresults we provide theoretical guarantees of our proposed quantile-based method\nin all of these scenarios. After validating our approach using simulations we\nalso propose a novel bootstrap based method that can boost the coverage of the\ntheoretical intervals. Finally analyzing the EPEX Spot data, we construct\nprediction intervals for hourly electricity prices over horizons spanning 17\nweeks and contrast them to selected Bayesian and bootstrap interval forecasts.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 11:26:08 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Karmakar", "Sayar", ""], ["Chudy", "Marek", ""], ["Wu", "Wei Biao", ""]]}, {"id": "2012.08257", "submitter": "Sangita Das", "authors": "Sangita Das and Suchandan Kayal", "title": "Ordering results of extreme order statistics from multiple-outlier scale\n  models with dependence", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on stochastic comparisons of extreme order statistics\nstemming from multiple-outlier scale models with dependence. Archimedean copula\nis used to model dependence structure among nonnegative random variables.\nSufficient conditions are obtained for comparison of the largest order\nstatistics in the sense of the usual stochastic, reversed hazard rate, star and\nLorenz orders. The smallest order statistics are also compared with respect to\nthe usual stochastic, hazard rate, star and Lorenz orders. To illustrate the\ntheoretical establishments, some examples are provided.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:47:29 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Das", "Sangita", ""], ["Kayal", "Suchandan", ""]]}, {"id": "2012.08294", "submitter": "Mehmet Niyazi Cankaya mehmetn", "authors": "Mehmet Niyazi \\c{C}ankaya, Roberto Vila", "title": "Maximum $\\log_q$ Likelihood Estimation for Parameters of Weibull\n  Distribution and Properties: Monte Carlo Simulation", "comments": "36 pages, 12 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The maximum ${\\log}_q$ likelihood estimation method is a generalization of\nthe known maximum $\\log$ likelihood method to overcome the problem for modeling\nnon-identical observations (inliers and outliers). The parameter $q$ is a\ntuning constant to manage the modeling capability. Weibull is a flexible and\npopular distribution for problems in engineering. In this study, this method is\nused to estimate the parameters of Weibull distribution when non-identical\nobservations exist. Since the main idea is based on modeling capability of\nobjective function\n$\\rho(x;\\boldsymbol{\\theta})=\\log_q\\big[f(x;\\boldsymbol{\\theta})\\big]$, we\nobserve that the finiteness of score functions cannot play a role in the robust\nestimation for inliers. The properties of Weibull distribution are examined. In\nthe numerical experiment, the parameters of Weibull distribution are estimated\nby $\\log_q$ and its special form, $\\log$, likelihood methods if the different\ndesigns of contamination into underlying Weibull distribution are applied. The\noptimization is performed via genetic algorithm. The modeling competence of\n$\\rho(x;\\boldsymbol{\\theta})$ and insensitiveness to non-identical observations\nare observed by Monte Carlo simulation. The value of $q$ can be chosen by use\nof the mean squared error in simulation and the $p$-value of Kolmogorov-Smirnov\ntest statistic used for evaluation of fitting competence. Thus, we can overcome\nthe problem about determining of the value of $q$ for real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 14:01:05 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["\u00c7ankaya", "Mehmet Niyazi", ""], ["Vila", "Roberto", ""]]}, {"id": "2012.08371", "submitter": "Emma Jingfei Zhang", "authors": "Jianwei Hu, Jingfei Zhang, Ji Zhu and Jianhua Guo", "title": "Limiting laws and consistent estimation criteria for fixed and diverging\n  number of spiked eigenvalues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study limiting laws and consistent estimation criteria for\nthe extreme eigenvalues in a spiked covariance model of dimension $p$. Firstly,\nfor fixed $p$, we propose a generalized estimation criterion that can\nconsistently estimate, $k$, the number of spiked eigenvalues. Compared with the\nexisting literature, we show that consistency can be achieved under weaker\nconditions on the penalty term. Next, allowing both $p$ and $k$ to diverge, we\nderive limiting distributions of the spiked sample eigenvalues using random\nmatrix theory techniques. Notably, our results do not require the spiked\neigenvalues to be uniformly bounded from above or tending to infinity, as have\nbeen assumed in the existing literature. Based on the above derived results, we\nformulate a generalized estimation criterion and show that it can consistently\nestimate $k$, while $k$ can be fixed or grow at an order of $k=o(n^{1/3})$. We\nfurther show that the results in our work continue to hold under a general\npopulation distribution without assuming normality. The efficacy of the\nproposed estimation criteria is illustrated through comparative simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 15:36:03 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Hu", "Jianwei", ""], ["Zhang", "Jingfei", ""], ["Zhu", "Ji", ""], ["Guo", "Jianhua", ""]]}, {"id": "2012.08391", "submitter": "Catherine Medlock", "authors": "Catherine Medlock and Alan Oppenheim", "title": "Optimal ROC Curves from Score Variable Threshold Tests", "comments": null, "journal-ref": "In 2019 IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP) (pp. 5327-5330). IEEE", "doi": "10.1109/ICASSP.2019.8683187", "report-no": null, "categories": "math.ST eess.SP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Receiver Operating Characteristic (ROC) is a well-established\nrepresentation of the tradeoff between detection and false alarm probabilities\nin binary hypothesis testing. In many practical contexts ROC's are generated by\nthresholding a measured score variable -- applying score variable threshold\ntests (SVT's). In many cases the resulting curve is different from the\nlikelihood ratio test (LRT) ROC and is therefore not Neyman-Pearson optimal.\nWhile it is well-understood that concavity is a necessary condition for an ROC\nto be Neyman-Pearson optimal, this paper establishes that it is also a\nsufficient condition in the case where the ROC was generated using SVT's. It\nfurther defines a constructive procedure by which the LRT ROC can be generated\nfrom a non-concave SVT ROC, without requiring explicit knowledge of the\nconditional PDF's of the score variable. If the conditional PDF's are known,\nthe procedure implicitly provides a way of redesigning the test so that it is\nequivalent to an LRT.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 16:08:24 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Medlock", "Catherine", ""], ["Oppenheim", "Alan", ""]]}, {"id": "2012.08397", "submitter": "Yunyi Shen", "authors": "Yunyi Shen and Claudia Solis-Lemus", "title": "Bayesian Conditional Auto-Regressive LASSO Models to Learn Sparse\n  Microbial Networks with Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Microbiome data analyses require statistical models that can simultaneously\ndecode microbes' reactions to the environment and interactions among microbes.\nWhile a multiresponse linear regression model seems like a straightforward\nsolution, we argue that treating it as a graphical model is flawed given that\nthe regression coefficient matrix does not encode the conditional dependence\nstructure between response and predictor nodes because it does not represent\nthe adjacency matrix. This observation is especially important in biological\nsettings when we have prior knowledge on the edges from specific experimental\ninterventions that can only be properly encoded under a conditional dependence\nmodel. Here, we propose a chain graph model with two sets of nodes (predictors\nand responses) whose solution yields a graph with edges that indeed represent\nconditional dependence and thus, agrees with the experimenter's intuition on\nthe average behavior of nodes under treatment. The solution to our model is\nsparse via Bayesian LASSO and is also guaranteed to be the sparse solution to a\nConditional Auto-Regressive (CAR) model. In addition, we propose an adaptive\nextension so that different shrinkage can be applied to different edges to\nincorporate edge-specific prior knowledge. Our model is computationally\ninexpensive through an efficient Gibbs sampling algorithm and can account for\nbinary, counting, and compositional responses via appropriate hierarchical\nstructure. We apply our model to a human gut and a soil microbial compositional\ndatasets and we highlight that CAR-LASSO can estimate biologically meaningful\nnetwork structures in the data. The CAR-LASSO software is available as an R\npackage at https://github.com/YunyiShen/CAR-LASSO.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 16:12:54 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 23:02:19 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 22:04:03 GMT"}, {"version": "v4", "created": "Tue, 16 Feb 2021 06:54:30 GMT"}, {"version": "v5", "created": "Tue, 4 May 2021 15:19:42 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Shen", "Yunyi", ""], ["Solis-Lemus", "Claudia", ""]]}, {"id": "2012.08591", "submitter": "Brian Karrer", "authors": "Brian Karrer, Liang Shi, Monica Bhole, Matt Goldman, Tyrone Palmer,\n  Charlie Gelman, Mikael Konutgan, Feng Sun", "title": "Network experimentation at scale", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our framework, deployed at Facebook, that accounts for\ninterference between experimental units through cluster-randomized experiments.\nWe document this system, including the design and estimation procedures, and\ndetail insights we have gained from the many experiments that have used this\nsystem at scale. We introduce a cluster-based regression adjustment that\nsubstantially improves precision for estimating global treatment effects as\nwell as testing for interference as part of our estimation procedure. With this\nregression adjustment, we find that imbalanced clusters can better account for\ninterference than balanced clusters without sacrificing accuracy. In addition,\nwe show how logging exposure to a treatment can be used for additional variance\nreduction. Interference is a widely acknowledged issue with online field\nexperiments, yet there is less evidence from real-world experiments\ndemonstrating interference in online settings. We fill this gap by describing\ntwo case studies that capture significant network effects and highlight the\nvalue of this experimentation framework.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 20:02:19 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Karrer", "Brian", ""], ["Shi", "Liang", ""], ["Bhole", "Monica", ""], ["Goldman", "Matt", ""], ["Palmer", "Tyrone", ""], ["Gelman", "Charlie", ""], ["Konutgan", "Mikael", ""], ["Sun", "Feng", ""]]}, {"id": "2012.08647", "submitter": "Adam Kashlak", "authors": "Adam B Kashlak and Weicong Yuan", "title": "Computation-free Nonparametric testing for Local and Global Spatial\n  Autocorrelation with application to the Canadian Electorate", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measures of local and global spatial association are key tools for\nexploratory spatial data analysis. Many such measures exist including Moran's\n$I$, Geary's $C$, and the Getis-Ord $G$ and $G^*$ statistics. A parametric\napproach to testing for significance relies on strong assumptions, which are\noften not met by real world data. Alternatively, the most popular nonparametric\napproach, the permutation test, imposes a large computational burden especially\nfor massive graphical networks. Hence, we propose a computation-free approach\nto nonparametric permutation testing for local and global measures of spatial\nautocorrelation stemming from generalizations of the Khintchine inequality from\nfunctional analysis and the theory of $L^p$ spaces. Our methodology is\ndemonstrated on the results of the 2019 federal Canadian election in the\nprovince of Alberta. We recorded the percentage of the vote gained by the\nconservative candidate in each riding. This data is not normal, and the sample\nsize is fixed at $n=34$ ridings making the parametric approach invalid. In\ncontrast, running a classic permutation test for every riding, for multiple\ntest statistics, with various neighbourhood structures, and multiple testing\ncorrection would require the simulation of millions of permutations. We are\nable to achieve similar statistical power on this dataset to the permutation\ntest without the need for tedious simulation. We also consider data simulated\nacross the entire electoral map of Canada.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 22:12:25 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kashlak", "Adam B", ""], ["Yuan", "Weicong", ""]]}, {"id": "2012.08649", "submitter": "Enrique Barrajon MDPhD", "authors": "Enrique Barraj\\'on, Laura Barraj\\'on", "title": "Effect of right censoring bias on survival analysis", "comments": "12 pages, 5 figures, supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kaplan-Meier survival analysis represents the most objective measure of\ntreatment efficacy in oncology, though subjected to potential bias, which is\nworrisome in an era of precision medicine. Independent of the bias inherent to\nthe design and execution of clinical trials, bias may be the result of patient\ncensoring, or incomplete observation. Unlike disease/progression free survival,\noverall survival is based on a well defined time point and thus avoids interval\ncensoring, but right-censoring, due to incomplete follow-up, may still be a\nsource of bias. We study three mechanisms of right-censoring and find that one\nof them, surrogate of patient lost to follow-up, is able to impact Kaplan-Meier\nsurvival, improving significantly the estimation of survival in comparison with\ncomplete follow-up datasets, as measured by the hazard ratio. We also present\ntwo bias indexes able to signal datasets with right-censoring associated\noverestimation of survival. These bias indexes can detect bias in public\navailable datasets\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 22:17:31 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Barraj\u00f3n", "Enrique", ""], ["Barraj\u00f3n", "Laura", ""]]}, {"id": "2012.08843", "submitter": "Kory Johnson", "authors": "Kory D. Johnson, Mathias Beiglb\\\"ock, Manuel Eder, Annemarie Grass,\n  Joachim Hermisson, Gudmund Pammer, Jitka Polechov\\'a, Daniel Toneian,\n  Benjamin W\\\"olfl", "title": "Disease Momentum: Estimating the Reproduction Number in the Presence of\n  Superspreading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A primary quantity of interest in the study of infectious diseases is the\naverage number of new infections that an infected person produces. This\nso-called reproduction number has significant implications for the disease\nprogression. There has been increasing literature suggesting that\nsuperspreading, the significant variability in number of new infections caused\nby individuals, plays an important role in the spread of SARS-CoV-2. In this\npaper, we consider the effect that such superspreading has on the estimation of\nthe reproduction number and subsequent estimates of future cases. Accordingly,\nwe employ a simple extension to models currently used in the literature to\nestimate the reproduction number and present a case-study of the progression of\nCOVID-19 in Austria. Our models demonstrate that the estimation uncertainty of\nthe reproduction number increases with superspreading and that this improves\nthe performance of prediction intervals. Of independent interest is the\nderivation of a transparent formula that connects the extent of superspreading\nto the width of credible intervals for the reproduction number. This serves as\na valuable heuristic for understanding the uncertainty surrounding diseases\nwith superspreading.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 10:34:15 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 13:07:23 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Johnson", "Kory D.", ""], ["Beiglb\u00f6ck", "Mathias", ""], ["Eder", "Manuel", ""], ["Grass", "Annemarie", ""], ["Hermisson", "Joachim", ""], ["Pammer", "Gudmund", ""], ["Polechov\u00e1", "Jitka", ""], ["Toneian", "Daniel", ""], ["W\u00f6lfl", "Benjamin", ""]]}, {"id": "2012.08848", "submitter": "JInglai Li", "authors": "Jiangqi Wu, Linjie Wen, Peter L Green, Jinglai Li, Simon Maskell", "title": "Ensemble Kalman filter based Sequential Monte Carlo Sampler for\n  sequential Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many real-world problems require one to estimate parameters of interest, in a\nBayesian framework, from data that are collected sequentially in time.\nConventional methods for sampling from posterior distributions, such as {Markov\nChain Monte Carlo} can not efficiently address such problems as they do not\ntake advantage of the data's sequential structure. To this end, sequential\nmethods which seek to update the posterior distribution whenever a new\ncollection of data become available are often used to solve these types of\nproblems. Two popular choices of sequential method are the Ensemble Kalman\nfilter (EnKF) and the sequential Monte Carlo sampler (SMCS). An advantage of\nthe SMCS method is that, unlike the EnKF method that only computes a Gaussian\napproximation of the posterior distribution, SMCS can draw samples directly\nfrom the posterior. Its performance, however, depends critically upon the\nkernels that are used. In this work, we present a method that constructs the\nkernels of SMCS using an EnKF formulation, and we demonstrate the performance\nof the method with numerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 10:37:15 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Wu", "Jiangqi", ""], ["Wen", "Linjie", ""], ["Green", "Peter L", ""], ["Li", "Jinglai", ""], ["Maskell", "Simon", ""]]}, {"id": "2012.08988", "submitter": "Chirok Han", "authors": "Chirok Han", "title": "Exact Trend Control in Estimating Treatment Effects Using Panel Data\n  with Heterogenous Trends", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  For a panel model considered by Abadie et al. (2010), the counterfactual\noutcomes constructed by Abadie et al., Hsiao et al. (2012), and Doudchenko and\nImbens (2017) may all be confounded by uncontrolled heterogenous trends. Based\non exact-matching on the trend predictors, I propose new methods of estimating\nthe model-specific treatment effects, which are free from heterogenous trends.\nWhen applied to Abadie et al.'s (2010) model and data, the new estimators\nsuggest considerably smaller effects of California's tobacco control program.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 14:33:53 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Han", "Chirok", ""]]}, {"id": "2012.09246", "submitter": "Peter Cohen", "authors": "Peter L. Cohen, Colin B. Fogarty", "title": "No-harm calibration for generalized Oaxaca-Blinder estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In randomized experiments, linear regression with baseline features can be\nused to form an estimate of the sample average treatment effect that is\nasymptotically no less efficient than the treated-minus-control difference in\nmeans. Randomization alone provides this \"do-no-harm\" property, with neither\ntruth of a linear model nor a generative model for the outcomes being required.\nWe present a general calibration step which confers the same no-harm property\nonto estimators leveraging a broad class of nonlinear models. The process\nrecovers the usual regression-adjusted estimator when ordinary least squares is\nused, and further provides non-inferior treatment effect estimators using\nmethods such as logistic and Poisson regression. The resulting estimators are\nnon-inferior with respect to both the difference in means estimator and with\nrespect to treatment effect estimators that have not undergone calibration.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 20:22:50 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Cohen", "Peter L.", ""], ["Fogarty", "Colin B.", ""]]}, {"id": "2012.09395", "submitter": "Pan Shang", "authors": "Pan Shang and Lingchen Kong", "title": "l1-norm quantile regression screening rule via the dual circumscribed\n  sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  l1-norm quantile regression is a common choice if there exists outlier or\nheavy-tailed error in high-dimensional data sets. However, it is\ncomputationally expensive to solve this problem when the feature size of data\nis ultra high. As far as we know, existing screening rules can not speed up the\ncomputation of the l1-norm quantile regression, which dues to the\nnon-differentiability of the quantile function/pinball loss. In this paper, we\nintroduce the dual circumscribed sphere technique and propose a novel l1-norm\nquantile regression screening rule. Our rule is expressed as the closed-form\nfunction of given data and eliminates inactive features with a low\ncomputational cost. Numerical experiments on some simulation and real data sets\nshow that this screening rule can be used to eliminate almost all inactive\nfeatures. Moreover, this rule can help to reduce up to 23 times of\ncomputational time, compared with the computation without our screening rule.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 04:59:18 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Shang", "Pan", ""], ["Kong", "Lingchen", ""]]}, {"id": "2012.09448", "submitter": "Yiyan Huang", "authors": "Yiyan Huang, Cheuk Hang Leung, Xing Yan, Qi Wu, Nanbo Peng, Dongdong\n  Wang, Zhixiang Huang", "title": "The Causal Learning of Retail Delinquency", "comments": "This paper was accepted and will be published in the Thirty-Fifth\n  AAAI Conference on Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the expected difference in borrower's repayment when\nthere is a change in the lender's credit decisions. Classical estimators\noverlook the confounding effects and hence the estimation error can be\nmagnificent. As such, we propose another approach to construct the estimators\nsuch that the error can be greatly reduced. The proposed estimators are shown\nto be unbiased, consistent, and robust through a combination of theoretical\nanalysis and numerical testing. Moreover, we compare the power of estimating\nthe causal quantities between the classical estimators and the proposed\nestimators. The comparison is tested across a wide range of models, including\nlinear regression models, tree-based models, and neural network-based models,\nunder different simulated datasets that exhibit different levels of causality,\ndifferent degrees of nonlinearity, and different distributional properties.\nMost importantly, we apply our approaches to a large observational dataset\nprovided by a global technology firm that operates in both the e-commerce and\nthe lending business. We find that the relative reduction of estimation error\nis strikingly substantial if the causal effects are accounted for correctly.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 08:46:01 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Huang", "Yiyan", ""], ["Leung", "Cheuk Hang", ""], ["Yan", "Xing", ""], ["Wu", "Qi", ""], ["Peng", "Nanbo", ""], ["Wang", "Dongdong", ""], ["Huang", "Zhixiang", ""]]}, {"id": "2012.09598", "submitter": "Owen G. Ward", "authors": "Owen G. Ward, Jing Wu, Tian Zheng, Anna L. Smith, James P. Curley", "title": "Network Hawkes Process Models for Exploring Latent Hierarchy in Social\n  Animal Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group-based social dominance hierarchies are of essential interest in animal\nbehavior research. Studies often record aggressive interactions observed over\ntime, and models that can capture such dynamic hierarchy are therefore crucial.\nTraditional ranking methods summarize interactions across time, using only\naggregate counts. Instead, we take advantage of the interaction timestamps,\nproposing a series of network point process models with latent ranks. We\ncarefully design these models to incorporate important characteristics of\nanimal interaction data, including the winner effect, bursting and pair-flip\nphenomena. Through iteratively constructing and evaluating these models we\narrive at the final cohort Markov-Modulated Hawkes process (C-MMHP), which best\ncharacterizes all aforementioned patterns observed in interaction data. We\ncompare all models using simulated and real data. Using statistically developed\ndiagnostic perspectives, we demonstrate that the C-MMHP model outperforms other\nmethods, capturing relevant latent ranking structures that lead to meaningful\npredictions for real data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 14:16:16 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Ward", "Owen G.", ""], ["Wu", "Jing", ""], ["Zheng", "Tian", ""], ["Smith", "Anna L.", ""], ["Curley", "James P.", ""]]}, {"id": "2012.09623", "submitter": "Emma Simpson", "authors": "Emma S. Simpson, Jennifer L. Wadsworth and Jonathan A. Tawn", "title": "A geometric investigation into the tail dependence of vine copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vine copulas are a type of multivariate dependence model, composed of a\ncollection of bivariate copulas that are combined according to a specific\nunderlying graphical structure. Their flexibility and practicality in moderate\nand high dimensions have contributed to the popularity of vine copulas, but\nrelatively little attention has been paid to their extremal properties. To\naddress this issue, we present results on the tail dependence properties of\nsome of the most widely studied vine copula classes. We focus our study on the\ncoefficient of tail dependence and the asymptotic shape of the sample cloud,\nwhich we calculate using the geometric approach of Nolde (2014). We offer new\ninsights by presenting results for trivariate vine copulas constructed from\nasymptotically dependent and asymptotically independent bivariate copulas,\nfocusing on bivariate extreme value and inverted extreme value copulas, with\nadditional detail provided for logistic and inverted logistic examples. We also\npresent new theory for a class of higher dimensional vine copulas, constructed\nfrom bivariate inverted extreme value copulas.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 14:49:02 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Simpson", "Emma S.", ""], ["Wadsworth", "Jennifer L.", ""], ["Tawn", "Jonathan A.", ""]]}, {"id": "2012.09659", "submitter": "Francisco Cuevas-Pacheco Mr.", "authors": "Francisco Cuevas-Pacheco, Jean-Fran\\c{c}ois Coeurjolly,\n  Marie-H\\'el\\`ene Descary", "title": "Fast estimation of a convolution type model for the intensity of spatial\n  point processes", "comments": "Submitted for journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the first-order intensity function in point pattern analysis is an\nimportant problem, and it has been approached so far from different\nperspectives: parametrically, semiparametrically or nonparametrically. Our\napproach is close to a semiparametric one. Motivated by eye-movement data, we\nintroduce a convolution type model where the log-intensity is modelled as the\nconvolution of a function $\\beta(\\cdot)$, to be estimated, and a single spatial\ncovariate (the image an individual is looking at for eye-movement data). Based\non a Fourier series expansion, we show that the proposed model is related to\nthe log-linear model with infinite number of coefficients, which correspond to\nthe spectral decomposition of $\\beta(\\cdot)$. After truncation, we estimate\nthese coefficients through a penalized Poisson likelihood and prove infill\nasymptotic results for a large class of spatial point processes. We illustrate\nthe efficiency of the proposed methodology on simulated data and real data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 15:17:51 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 12:58:14 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Cuevas-Pacheco", "Francisco", ""], ["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Descary", "Marie-H\u00e9l\u00e8ne", ""]]}, {"id": "2012.09731", "submitter": "Samuel Livingstone", "authors": "Max Hird, Samuel Livingstone and Giacomo Zanella", "title": "A fresh take on 'Barker dynamics' for MCMC", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a recently introduced gradient-based Markov chain Monte Carlo method\nbased on 'Barker dynamics'. We provide a full derivation of the method from\nfirst principles, placing it within a wider class of continuous-time Markov\njump processes. We then evaluate the Barker approach numerically on a\nchallenging ill-conditioned logistic regression example with imbalanced data,\nshowing in particular that the algorithm is remarkably robust to irregularity\n(in this case a high degree of skew) in the target distribution.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 16:49:08 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 17:05:40 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hird", "Max", ""], ["Livingstone", "Samuel", ""], ["Zanella", "Giacomo", ""]]}, {"id": "2012.09746", "submitter": "Sudipta Bhattacharya", "authors": "Sudipta Bhattacharya", "title": "Non-parametric estimation of Expectation and Variance of event count and\n  of incidence rate in a recurrent process -- where intensity of\n  event-occurrence changes with the occurrence of each higher order event", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a novel non-parametric method for estimation of expectation\nand maximum value of the variance function is proposed for recurrent events\nwhere intensity of event occurrence changes with the occurrence of each higher\norder event. These kinds of recurrent events are often observed in clinical\ntrials for cardio-vascular events and also in many social experiments involving\ndrug addiction, armed robberies, etc. Simulated data is used to demonstrate the\nnovel approach for estimating the mean and variance of such recurrent events\nand the results are compared with the result of Nelson Aalen estimator.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 17:03:03 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Bhattacharya", "Sudipta", ""]]}, {"id": "2012.09775", "submitter": "Fabian Bach", "authors": "Fabian Bach", "title": "Differential privacy and noisy confidentiality concepts for European\n  population statistics", "comments": "37 pages, 7 figures, extended abstract accepted for NTTS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper aims to give an overview of various approaches to statistical\ndisclosure control based on random noise that are currently being discussed for\nofficial population statistics and censuses. A particular focus is on a\nstringent delineation between different concepts influencing the discussion: we\nseparate clearly between risk measures, noise distributions and output\nmechanisms - putting these concepts into scope and into relation with each\nother.\n  After recapitulating differential privacy as a risk measure, the paper also\nremarks on utility and risk aspects of some specific output mechanisms and\nparameter setups, with special attention on static outputs that are rather\ntypical in official population statistics. In particular, it is argued that\nunbounded noise distributions, such as plain Laplace, may jeopardise key unique\ncensus features without a clear need from a risk perspective. On the other\nhand, bounded noise distributions, such as the truncated Laplace or the cell\nkey method, can be set up to keep unique census features while controlling\ndisclosure risks in census-like outputs.\n  Finally, the paper analyses some typical attack scenarios to constrain\ngeneric noise parameter ranges that suggest a good risk/utility compromise for\nthe 2021 EU census output scenario. The analysis also shows that strictly\ndifferentially private mechanisms would be severely constrained in this\nscenario.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 17:34:53 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Bach", "Fabian", ""]]}, {"id": "2012.09833", "submitter": "Georgios Papageorgiou", "authors": "Georgios Papageorgiou", "title": "Bayesian semiparametric modelling of covariance matrices for\n  multivariate longitudinal data", "comments": "37 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The article develops marginal models for multivariate longitudinal responses.\nOverall, the model consists of five regression submodels, one for the mean and\nfour for the covariance matrix, with the latter resulting by considering\nvarious matrix decompositions. The decompositions that we employ are intuitive,\neasy to understand, and they do not rely on any assumptions such as the\npresence of an ordering among the multivariate responses. The regression\nsubmodels are semiparametric, with unknown functions represented by basis\nfunction expansions. We use spike-slap priors for the regression coefficients\nto achieve variable selection and function regularization, and to obtain\nparameter estimates that account for model uncertainty. An efficient Markov\nchain Monte Carlo algorithm for posterior sampling is developed. The simulation\nstudies presented investigate the effects of priors on posteriors, the gains\nthat one may have when considering multivariate longitudinal analyses instead\nof univariate ones, and whether these gains can counteract the negative effects\nof missing data. We apply the methods on a highly unbalanced longitudinal\ndataset with four responses observed over of period of 20 years\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:53:03 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Papageorgiou", "Georgios", ""]]}, {"id": "2012.09915", "submitter": "Mar\\'ia Alonso-Pena", "authors": "Mar\\'ia Alonso-Pena and Rosa M. Crujeiras", "title": "Nonparametric multimodal regression for circular data", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal regression estimation methods are introduced for regression models\ninvolving circular response and/or covariate. The regression estimators are\nbased on the maximization of the conditional densities of the response variable\nover the covariate. Conditional versions of the mean shift and the circular\nmean shift algorithms are used to obtain the regression estimators. The\nasymptotic properties of the estimators are studied and the problem of\nbandwidth selection is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 20:19:48 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Alonso-Pena", "Mar\u00eda", ""], ["Crujeiras", "Rosa M.", ""]]}, {"id": "2012.09920", "submitter": "Miguel Angel Luque-Fernandez", "authors": "Matthew J. Smith, Camille Maringe, Bernard Rachet, Mohammad A.\n  Mansournia, Paul N. Zivich, Stephen R. Cole, Miguel Angel Luque-Fernandez", "title": "Tutorial: Introduction to computational causal inference using\n  reproducible Stata, R and Python code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The purpose of many health studies is to estimate the effect of an exposure\non an outcome. It is not always ethical to assign an exposure to individuals in\nrandomised controlled trials, instead observational data and appropriate study\ndesign must be used. There are major challenges with observational studies, one\nof which is confounding that can lead to biased estimates of the causal\neffects. Controlling for confounding is commonly performed by simple adjustment\nfor measured confounders; although, often this is not enough. Recent advances\nin the field of causal inference have dealt with confounding by building on\nclassical standardisation methods. However, these recent advances have\nprogressed quickly with a relative paucity of computational-oriented applied\ntutorials contributing to some confusion in the use of these methods among\napplied researchers. In this tutorial, we show the computational implementation\nof different causal inference estimators from a historical perspective where\ndifferent estimators were developed to overcome the limitations of the previous\none. Furthermore, we also briefly introduce the potential outcomes framework,\nillustrate the use of different methods using an illustration from the health\ncare setting, and most importantly, we provide reproducible and commented code\nin Stata, R and Python for researchers to apply in their own observational\nstudy. The code can be accessed at\nhttps://github.com/migariane/TutorialCausalInferenceEstimators\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 20:31:30 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 17:01:06 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Smith", "Matthew J.", ""], ["Maringe", "Camille", ""], ["Rachet", "Bernard", ""], ["Mansournia", "Mohammad A.", ""], ["Zivich", "Paul N.", ""], ["Cole", "Stephen R.", ""], ["Luque-Fernandez", "Miguel Angel", ""]]}, {"id": "2012.09935", "submitter": "Alejandro Schuler", "authors": "Alejandro Schuler, David Walsh, Diana Hall, Jon Walsh, Charles Fisher", "title": "Increasing the efficiency of randomized trial estimates via linear\n  adjustment for a prognostic score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating causal effects from randomized experiments is central to clinical\nresearch. Reducing the statistical uncertainty in these analyses is an\nimportant objective for statisticians. Registries, prior trials, and health\nrecords constitute a growing compendium of historical data on patients under\nstandard-of-care conditions that may be exploitable to this end. However, most\nmethods for historical borrowing achieve reductions in variance by sacrificing\nstrict type-I error rate control. Here, we propose a use of historical data\nthat exploits linear covariate adjustment to improve the efficiency of trial\nanalyses without incurring bias. Specifically, we train a prognostic model on\nthe historical data, then estimate the treatment effect using a linear\nregression while adjusting for the trial subjects' predicted outcomes (their\nprognostic scores). We prove that, under certain conditions, this prognostic\ncovariate adjustment procedure attains the minimum variance possible among a\nlarge class of estimators. When those conditions are not met, prognostic\ncovariate adjustment is still more efficient than raw covariate adjustment and\nthe gain in efficiency is proportional to a measure of the predictive accuracy\nof the prognostic model. We demonstrate the approach using simulations and a\nreanalysis of an Alzheimer's Disease clinical trial and observe meaningful\nreductions in mean-squared error and the estimated variance. Lastly, we provide\na simplified formula for asymptotic variance that enables power and sample size\ncalculations that account for the gains from the prognostic model for clinical\ntrial design. Sample size reductions between 10% and 30% are attainable when\nusing prognostic models that explain a clinically realistic percentage of the\noutcome variance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 21:10:10 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 01:46:36 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Schuler", "Alejandro", ""], ["Walsh", "David", ""], ["Hall", "Diana", ""], ["Walsh", "Jon", ""], ["Fisher", "Charles", ""]]}, {"id": "2012.09996", "submitter": "Anru R. Zhang", "authors": "Rungang Han, Yuetian Luo, Miaoyan Wang, and Anru R. Zhang", "title": "Exact Clustering in Tensor Block Model: Statistical Optimality and\n  Computational Limit", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-order clustering aims to identify heterogeneous substructure in multiway\ndataset that arises commonly in neuroimaging, genomics, and social network\nstudies. The non-convex and discontinuous nature of the problem poses\nsignificant challenges in both statistics and computation. In this paper, we\npropose a tensor block model and the computationally efficient methods,\n\\emph{high-order Lloyd algorithm} (HLloyd) and \\emph{high-order spectral\nclustering} (HSC), for high-order clustering in tensor block model. The\nconvergence of the proposed procedure is established, and we show that our\nmethod achieves exact clustering under reasonable assumptions. We also give the\ncomplete characterization for the statistical-computational trade-off in\nhigh-order clustering based on three different signal-to-noise ratio regimes.\nFinally, we show the merits of the proposed procedures via extensive\nexperiments on both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 00:48:27 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 14:55:54 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Han", "Rungang", ""], ["Luo", "Yuetian", ""], ["Wang", "Miaoyan", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2012.09999", "submitter": "Eric Dunipace", "authors": "Eric Dunipace and Lorenzo Trippa", "title": "Interpretable Model Summaries Using the Wasserstein Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical models often include thousands of parameters. However, large\nmodels decrease the investigator's ability to interpret and communicate the\nestimated parameters. Reducing the dimensionality of the parameter space in the\nestimation phase is a commonly used approach, but less work has focused on\nselecting subsets of the parameters for interpreting the estimated model --\nespecially in settings such as Bayesian inference and model averaging.\nImportantly, many models do not have straightforward interpretations and create\nanother layer of obfuscation. To solve this gap, we introduce a new method that\nuses the Wasserstein distance to identify a low-dimensional interpretable model\nprojection. After the estimation of complex models, users can budget how many\nparameters they wish to interpret and the proposed generates a simplified model\nof the desired dimension minimizing the distance to the full model. We provide\nsimulation results to illustrate the method and apply it to cancer datasets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 00:55:36 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 22:12:44 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dunipace", "Eric", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "2012.10009", "submitter": "Jiaming Qiu", "authors": "Jiaming Qiu, Xiongtao Dai, Zhengyuan Zhu", "title": "Nonparametric Estimation of Repeated Densities with Heterogeneous Sample\n  Sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of densities in multiple subpopulations, where the\navailable sample size in each subpopulation greatly varies. For example, in\nepidemiology, different diseases may share similar pathogenic mechanism but\ndiffer in their prevalence. Without specifying a parametric form, our proposed\napproach pools information from the population and estimate the density in each\nsubpopulation in a data-driven fashion. Low-dimensional approximating density\nfamilies in the form of exponential families are constructed from the principal\nmodes of variation in the log-densities, within which subpopulation densities\nare then fitted based on likelihood principles and shrinkage. The approximating\nfamilies increase in their flexibility as the number of components increases\nand can approximate arbitrary infinite-dimensional densities with discrete\nobservations, for which we derived convergence results. The proposed methods\nare shown to be interpretable and efficient in simulation as well as\napplications to electronic medical record and rainfall data.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 01:45:11 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Qiu", "Jiaming", ""], ["Dai", "Xiongtao", ""], ["Zhu", "Zhengyuan", ""]]}, {"id": "2012.10021", "submitter": "Paul Patrone", "authors": "Paul N. Patrone and Anthony J. Kearsley", "title": "Classification Under Uncertainty: Data Analysis for Diagnostic Antibody\n  Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formulating accurate and robust classification strategies is a key challenge\nof developing diagnostic and antibody tests. Methods that do not explicitly\naccount for disease prevalence and uncertainty therein can lead to significant\nclassification errors. We present a novel method that leverages optimal\ndecision theory to address this problem. As a preliminary step, we develop an\nanalysis that uses an assumed prevalence and conditional probability models of\ndiagnostic measurement outcomes to define optimal (in the sense of minimizing\nrates of false positives and false negatives) classification domains.\nCritically, we demonstrate how this strategy can be generalized to a setting in\nwhich the prevalence is unknown by either: (i) defining a third class of\nhold-out samples that require further testing; or (ii) using an adaptive\nalgorithm to estimate prevalence prior to defining classification domains. We\nalso provide examples for a recently published SARS-CoV-2 serology test and\ndiscuss how measurement uncertainty (e.g. associated with instrumentation) can\nbe incorporated into the analysis. We find that our new strategy decreases\nclassification error by up to a decade relative to more traditional methods\nbased on confidence intervals. Moreover, it establishes a theoretical\nfoundation for generalizing techniques such as receiver operating\ncharacteristics (ROC) by connecting them to the broader field of optimization.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 02:39:46 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 01:38:21 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Patrone", "Paul N.", ""], ["Kearsley", "Anthony J.", ""]]}, {"id": "2012.10030", "submitter": "Zhenzhong Wang", "authors": "Zhenzhong Wang, Abolfazl Safikhani, Zhengyuan Zhu, David S. Matteson", "title": "Regularized Estimation in High-Dimensional Vector Auto-Regressive Models\n  using Spatio-Temporal Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Vector Auto-Regressive (VAR) model is commonly used to model multivariate\ntime series, and there are many penalized methods to handle high\ndimensionality. However in terms of spatio-temporal data, most methods do not\ntake the spatial and temporal structure of the data into consideration, which\nmay lead to unreliable network detection and inaccurate forecasts. This paper\nproposes a data-driven weighted l1 regularized approach for spatio-temporal VAR\nmodel. Extensive simulation studies are carried out to compare the proposed\nmethod with four existing methods of high-dimensional VAR model, demonstrating\nimprovements of our method over others in parameter estimation, network\ndetection and out-of-sample forecasts. We also apply our method on a traffic\ndata set to evaluate its performance in real application. In addition, we\nexplore the theoretical properties of l1 regularized estimation of VAR model\nunder the weakly sparse scenario, in which the exact sparsity can be viewed as\na special case. To the best of our knowledge, this direction has not been\nconsidered yet in the literature. For general stationary VAR process, we derive\nthe non-asymptotic upper bounds on l1 regularized estimation errors under the\nweakly sparse scenario, provide the conditions of estimation consistency, and\nfurther simplify these conditions for a special VAR(1) case.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 03:09:56 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Wang", "Zhenzhong", ""], ["Safikhani", "Abolfazl", ""], ["Zhu", "Zhengyuan", ""], ["Matteson", "David S.", ""]]}, {"id": "2012.10032", "submitter": "Xin Zhang", "authors": "Qing Mai, Xin Zhang, Yuqing Pan, Kai Deng", "title": "A Doubly-Enhanced EM Algorithm for Model-Based Tensor Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern scientific studies often collect data sets in the forms of tensors,\nwhich call for innovative statistical analysis methods. In particular, there is\na pressing need for tensor clustering methods to understand the heterogeneity\nin the data. We propose a tensor normal mixture model (TNMM) approach to enable\nprobabilistic interpretation and computational tractability. Our statistical\nmodel leverages the tensor covariance structure to reduce the number of\nparameters for parsimonious modeling, and at the same time explicitly exploits\nthe correlations for better variable selection and clustering. We propose a\ndoubly-enhanced expectation-maximization (DEEM) algorithm to perform clustering\nunder this model. Both the E-step and the M-step are carefully tailored for\ntensor data in order to account for statistical accuracy and computational cost\nin high dimensions. Theoretical studies confirm that DEEM achieves consistent\nclustering even when the dimension of each mode of the tensors grows at an\nexponential rate of the sample size. Numerical studies demonstrate favorable\nperformance of DEEM in comparison to existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 03:16:28 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 03:16:27 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mai", "Qing", ""], ["Zhang", "Xin", ""], ["Pan", "Yuqing", ""], ["Deng", "Kai", ""]]}, {"id": "2012.10124", "submitter": "Ayokunle Osuntuyi", "authors": "Roberto Casarin, Mauro Costantini and Anthony Osuntuyi", "title": "Bayesian nonparametric panel Markov-switching GARCH models", "comments": "38 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new model for panel data with Markov-switching GARCH\neffects. The model incorporates a series-specific hidden Markov chain process\nthat drives the GARCH parameters. To cope with the high-dimensionality of the\nparameter space, the paper exploits the cross-sectional clustering of the\nseries by first assuming a soft parameter pooling through a hierarchical prior\ndistribution with two-step procedure, and then introducing clustering effects\nin the parameter space through a nonparametric prior distribution. The model\nand the proposed inference are evaluated through a simulation experiment. The\nresults suggest that the inference is able to recover the true value of the\nparameters and the number of groups in each regime. An empirical application to\n78 assets of the SP\\&100 index from $6^{th}$ January 2000 to $3^{rd}$ October\n2020 is also carried out by using a two-regime Markov switching GARCH model.\nThe findings shows the presence of 2 and 3 clusters among the constituents in\nthe first and second regime, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 09:29:30 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Casarin", "Roberto", ""], ["Costantini", "Mauro", ""], ["Osuntuyi", "Anthony", ""]]}, {"id": "2012.10167", "submitter": "Ioan Gabriel Bucur", "authors": "Ioan Gabriel Bucur, Tom Claassen and Tom Heskes", "title": "Inferring the Direction of a Causal Link and Estimating Its Effect via a\n  Bayesian Mendelian Randomization Approach", "comments": "26 pages, 22 figures, published in Statistical Methods in Medical\n  Research", "journal-ref": "Statistical Methods in Medical Research, Vol 29, Issue 4, 2020", "doi": "10.1177/0962280219851817", "report-no": null, "categories": "stat.ME q-bio.GN q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of genetic variants as instrumental variables - an approach known as\nMendelian randomization - is a popular epidemiological method for estimating\nthe causal effect of an exposure (phenotype, biomarker, risk factor) on a\ndisease or health-related outcome from observational data. Instrumental\nvariables must satisfy strong, often untestable assumptions, which means that\nfinding good genetic instruments among a large list of potential candidates is\nchallenging. This difficulty is compounded by the fact that many genetic\nvariants influence more than one phenotype through different causal pathways, a\nphenomenon called horizontal pleiotropy. This leads to errors not only in\nestimating the magnitude of the causal effect but also in inferring the\ndirection of the putative causal link. In this paper, we propose a Bayesian\napproach called BayesMR that is a generalization of the Mendelian randomization\ntechnique in which we allow for pleiotropic effects and, crucially, for the\npossibility of reverse causation. The output of the method is a posterior\ndistribution over the target causal effect, which provides an immediate and\neasily interpretable measure of the uncertainty in the estimation. More\nimportantly, we use Bayesian model averaging to determine how much more likely\nthe inferred direction is relative to the reverse direction.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 11:01:52 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Bucur", "Ioan Gabriel", ""], ["Claassen", "Tom", ""], ["Heskes", "Tom", ""]]}, {"id": "2012.10194", "submitter": "Martin Law MSc", "authors": "Martin Law, Michael J. Grayling, Adrian P. Mander", "title": "Multi-outcome trials with a generalised number of efficacious outcomes", "comments": "38 pages, 11 figures, 4 tables. Submitted for the degree of Doctor of\n  Philosophy at University of Cambridge", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing multi-outcome designs focus almost entirely on evaluating whether\nall outcomes show evidence of efficacy or whether at least one outcome shows\nevidence of efficacy. While a small number of authors have provided\nmulti-outcome designs that evaluate when a general number of outcomes show\npromise, these designs have been single-stage in nature only. We therefore\npropose two designs, of group-sequential and drop the loser form, that provide\nthis design characteristic in a multi-stage setting. Previous such\nmulti-outcome multi-stage designs have allowed only for a maximum of two\noutcomes; our designs thus also extend previous related proposals by permitting\nany number of outcomes.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 12:34:04 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Law", "Martin", ""], ["Grayling", "Michael J.", ""], ["Mander", "Adrian P.", ""]]}, {"id": "2012.10202", "submitter": "M{\\aa}rten Schultzberg PhD", "authors": "M{\\aa}rten Schultzberg, Oskar Kjellin and Johan Rydberg", "title": "Statistical Properties of Exclusive and Non-exclusive Online Randomized\n  Experiments using Bucket Reuse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments is a key part of product development in the tech\nindustry. It is often necessary to run programs of exclusive experiments, i.e.,\nexperiments that cannot be run on the same units during the same time. These\nprograms implies restriction on the random sampling, as units that are\ncurrently in an experiment cannot be sampled into a new one. Moreover, to\ntechnically enable this type of coordination with large populations, the units\nin the population are often grouped into 'buckets' and sampling is then\nperformed on the bucket level. This paper investigates some statistical\nimplications of both the restricted sampling and the bucket-level sampling. The\ncontribution of this paper is threefold: First, bucket sampling is connected to\nthe existing literature on randomized experiments in complex sampling designs\nwhich enables establishing properties of the difference-in-means estimator of\nthe average treatment effect. These properties are needed for inference to the\npopulation under random sampling of buckets. Second, the bias introduced by\nrestricting the sampling as imposed by programs of exclusive experiments, is\nderived. Finally, simulation results supporting the theoretical findings are\npresented together with recommendations on how to empirically evaluate and\nhandle this bias.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 12:46:40 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Schultzberg", "M\u00e5rten", ""], ["Kjellin", "Oskar", ""], ["Rydberg", "Johan", ""]]}, {"id": "2012.10249", "submitter": "Ranjan Maitra", "authors": "Carlos Llosa-Vite and Ranjan Maitra", "title": "Reduced-Rank Tensor-on-Tensor Regression and Tensor-variate Analysis of\n  Variance", "comments": "30 pages, 12 figures, 2 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.data-an stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting regression models with many multivariate responses and covariates can\nbe challenging, but such responses and covariates sometimes have tensor-variate\nstructure. We extend the classical multivariate regression model to exploit\nsuch structure in two ways: first, we impose four types of low-rank tensor\nformats on the regression coefficients. Second, we model the errors using the\ntensor-variate normal distribution that imposes a Kronecker separable format on\nthe covariance matrix. We obtain maximum likelihood estimators via\nblock-relaxation algorithms and derive their asymptotic distributions. Our\nregression framework enables us to formulate tensor-variate analysis of\nvariance (TANOVA) methodology. Application of our methodology in a one-way\nTANOVA layout enables us to identify cerebral regions significantly associated\nwith the interaction of suicide attempters or non-attemptor ideators and\npositive-, negative- or death-connoting words. A separate application performs\nthree-way TANOVA on the Labeled Faces in the Wild image database to distinguish\nfacial characteristics related to ethnic origin, age group and gender.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 14:04:41 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 15:57:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Llosa-Vite", "Carlos", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2012.10300", "submitter": "Matthias Templ", "authors": "Matthias Templ", "title": "Artificial Neural Networks to Impute Rounded Zeros in Compositional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Methods of deep learning have become increasingly popular in recent years,\nbut they have not arrived in compositional data analysis. Imputation methods\nfor compositional data are typically applied on additive, centered or isometric\nlog-ratio representations of the data. Generally, methods for compositional\ndata analysis can only be applied to observed positive entries in a data\nmatrix. Therefore one tries to impute missing values or measurements that were\nbelow a detection limit. In this paper, a new method for imputing rounded zeros\nbased on artificial neural networks is shown and compared with conventional\nmethods. We are also interested in the question whether for ANNs, a\nrepresentation of the data in log-ratios for imputation purposes, is relevant.\nIt can be shown, that ANNs are competitive or even performing better when\nimputing rounded zeros of data sets with moderate size. They deliver better\nresults when data sets are big. Also, we can see that log-ratio transformations\nwithin the artificial neural network imputation procedure nevertheless help to\nimprove the results. This proves that the theory of compositional data analysis\nand the fulfillment of all properties of compositional data analysis is still\nvery important in the age of deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 15:31:23 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Templ", "Matthias", ""]]}, {"id": "2012.10403", "submitter": "Jeffrey Wong", "authors": "Eskil Forsell, Julie Beckley, Simon Ejdemyr, Veronica Hannan, Andy\n  Rhines, Martin Tingley, Matthew Wardrop, Jeffrey Wong", "title": "Success Stories from a Democratized Experimentation Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We demonstrate the effectiveness of democratization and efficient computation\nas key concepts of our experimentation platform (XP) by presenting four new\nmodels supported by the platform: 1) Weighted least squares, 2) Quantile\nbootstrapping, 3) Bayesian shrinkage, and 4) Dynamic treatment effects. Each\nmodel is motivated by a specific business problem but is generalizable and\nextensible. The modular structure of our platform allows independent innovation\non statistical and computational methods. In practice, a technical symbiosis is\ncreated where increasingly advanced user contributions inspire innovations to\nthe software that in turn enable further methodological improvements. This\ncycle adds further value to how the XP contributes to business solutions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 04:28:06 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Forsell", "Eskil", ""], ["Beckley", "Julie", ""], ["Ejdemyr", "Simon", ""], ["Hannan", "Veronica", ""], ["Rhines", "Andy", ""], ["Tingley", "Martin", ""], ["Wardrop", "Matthew", ""], ["Wong", "Jeffrey", ""]]}, {"id": "2012.10541", "submitter": "Tianyu Pan", "authors": "Tianyu Pan, Guanyu Hu and Weining Shen", "title": "Identifying latent groups in spatial panel data using a Markov random\n  field constrained product partition model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the heterogeneity over spatial locations is an important\nproblem that has been widely studied in many applications such as economics and\nenvironmental science. In this paper, we focus on regression models for spatial\npanel data analysis, where repeated measurements are collected over time at\nvarious spatial locations. We propose a novel class of nonparametric priors\nthat combines Markov random field (MRF) with the product partition model (PPM),\nand show that the resulting prior, called by MRF-PPM, is capable of identifying\nthe latent group structure among the spatial locations while efficiently\nutilizing the spatial dependence information. We derive a closed-form\nconditional distribution for the proposed prior and introduce a new way to\ncompute the marginal likelihood that renders efficient Bayesian inference. We\nfurther study the theoretical properties of the proposed MRF-PPM prior and show\na clustering consistency result for the posterior distribution. We demonstrate\nthe excellent empirical performance of our method via extensive simulation\nstudies and applications to a US precipitation data and a California median\nhousehold income data study.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 22:23:26 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Pan", "Tianyu", ""], ["Hu", "Guanyu", ""], ["Shen", "Weining", ""]]}, {"id": "2012.10559", "submitter": "Tyler McCormick", "authors": "Shane Lubold, Arun G. Chandrasekhar, Tyler H. McCormick", "title": "Identifying the latent space geometry of network models through analysis\n  of curvature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.GT stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Statistically modeling networks, across numerous disciplines and contexts, is\nfundamentally challenging because of (often high-order) dependence between\nconnections. A common approach assigns each person in the graph to a position\non a low-dimensional manifold. Distance between individuals in this (latent)\nspace is inversely proportional to the likelihood of forming a connection. The\nchoice of the latent geometry (the manifold class, dimension, and curvature)\nhas consequential impacts on the substantive conclusions of the model. More\npositive curvature in the manifold, for example, encourages more and tighter\ncommunities; negative curvature induces repulsion among nodes. Currently,\nhowever, the choice of the latent geometry is an a priori modeling assumption\nand there is limited guidance about how to make these choices in a data-driven\nway. In this work, we present a method to consistently estimate the manifold\ntype, dimension, and curvature from an empirically relevant class of latent\nspaces: simply connected, complete Riemannian manifolds of constant curvature.\nOur core insight comes by representing the graph as a noisy distance matrix\nbased on the ties between cliques. Leveraging results from statistical\ngeometry, we develop hypothesis tests to determine whether the observed\ndistances could plausibly be embedded isometrically in each of the candidate\ngeometries. We explore the accuracy of our approach with simulations and then\napply our approach to data-sets from economics and sociology as well as\nneuroscience.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 00:35:29 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 22:40:45 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 21:47:53 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Lubold", "Shane", ""], ["Chandrasekhar", "Arun G.", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "2012.10579", "submitter": "Zhengwu Zhang", "authors": "Zhengwu Zhang, Xiao Wang, Linglong Kong, Hongtu Zhu", "title": "High-Dimensional Spatial Quantile Function-on-Scalar Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper develops a novel spatial quantile function-on-scalar regression\nmodel, which studies the conditional spatial distribution of a high-dimensional\nfunctional response given scalar predictors. With the strength of both quantile\nregression and copula modeling, we are able to explicitly characterize the\nconditional distribution of the functional or image response on the whole\nspatial domain. Our method provides a comprehensive understanding of the effect\nof scalar covariates on functional responses across different quantile levels\nand also gives a practical way to generate new images for given covariate\nvalues. Theoretically, we establish the minimax rates of convergence for\nestimating coefficient functions under both fixed and random designs. We\nfurther develop an efficient primal-dual algorithm to handle high-dimensional\nimage data. Simulations and real data analysis are conducted to examine the\nfinite-sample performance.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 03:00:59 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhang", "Zhengwu", ""], ["Wang", "Xiao", ""], ["Kong", "Linglong", ""], ["Zhu", "Hongtu", ""]]}, {"id": "2012.10637", "submitter": "Xiao Chen", "authors": "Xiao Chen", "title": "Robust mixture regression with Exponential Power distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assuming an exponential power distribution is one way to deal with outliers\nin regression and clustering, which can increase the robustness of the\nanalysis. Gaussian distribution is a special case of an exponential\ndistribution. And an exponential power distribution can be viewed as a scale\nmixture of normal distributions. Thus, model selection methods developed for\nthe Gaussian mixture model can be easily extended for the exponential power\nmixture model. Moreover, Gaussian mixture models tend to select more components\nthan exponential power mixture models in real-world cases, which means\nexponential power mixture models are easier to interpret. In this paper, We\ndevelop analyses for mixture regression models when the errors are assumed to\nfollow an exponential power distribution. It will be robust to outliers, and\nmodel selection for it is easy to implement.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 09:30:42 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Chen", "Xiao", ""]]}, {"id": "2012.10686", "submitter": "Mattias Nordin", "authors": "Per Johansson and Mattias Nordin", "title": "Inference in experiments conditional on observed imbalances in\n  covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Double blind randomized controlled trials are traditionally seen as the gold\nstandard for causal inferences as the difference-in-means estimator is an\nunbiased estimator of the average treatment effect in the experiment. The fact\nthat this estimator is unbiased over all possible randomizations does not,\nhowever, mean that any given estimate is close to the true treatment effect.\nSimilarly, while pre-determined covariates will be balanced between treatment\nand control groups on average, large imbalances may be observed in a given\nexperiment and the researcher may therefore want to condition on such\ncovariates using linear regression. This paper studies the theoretical\nproperties of both the difference-in-means and OLS estimators conditional on\nobserved differences in covariates. By deriving the statistical properties of\nthe conditional estimators, we can establish guidance for how to deal with\ncovariate imbalances. We study both inference with OLS, as well as with a new\nversion of Fisher's exact test, where the randomization distribution comes from\na small subset of all possible assignment vectors.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 13:45:53 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Johansson", "Per", ""], ["Nordin", "Mattias", ""]]}, {"id": "2012.10745", "submitter": "St\\'ephane Guerrier", "authors": "St\\'ephane Guerrier and Christoph Kuzmics and Maria-Pia Victoria-Feser", "title": "Prevalence Estimation from Random Samples and Census Data with\n  Participation Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Countries officially record the number of COVID-19 cases based on medical\ntests of a subset of the population with unknown participation bias. For\nprevalence estimation, the official information is typically discarded and,\ninstead, small random survey samples are taken. We derive (maximum likelihood\nand method of moment) prevalence estimators, based on a survey sample, that\nadditionally utilize the official information, and that are substantially more\naccurate than the simple sample proportion of positive cases. Put differently,\nusing our estimators, the same level of precision can be obtained with\nsubstantially smaller survey samples. We take into account the possibility of\nmeasurement errors due to the sensitivity and specificity of the medical\ntesting procedure. The proposed estimators and associated confidence intervals\nare implemented in the companion open source R package cape.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 18:02:51 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 21:40:08 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Kuzmics", "Christoph", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "2012.10760", "submitter": "Helton Saulo", "authors": "Kessys L. P. Oliveira, Bruno S. Castro, Helton Saulo and Roberto Vila", "title": "On a length-biased Birnbaum-Saunders regression model applied to\n  meteorological data", "comments": "24 pages; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The length-biased Birnbaum-Saunders distribution is both useful and practical\nfor environmental sciences. In this paper, we initially derive some new\nproperties for the length-biased Birnbaum-Saunders distribution, showing that\none of its parameters is the mode and that it is bimodal. We then introduce a\nnew regression model based on this distribution. We implement use the maximum\nlikelihood method for parameter estimation, approach interval estimation and\nconsider three types of residuals. An elaborate Monte Carlo study is carried\nout for evaluating the performance of the likelihood-based estimates, the\nconfidence intervals and the empirical distribution of the residuals. Finally,\nwe illustrate the proposed regression model with the use of a real\nmeteorological data set.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 19:06:00 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 15:00:43 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Oliveira", "Kessys L. P.", ""], ["Castro", "Bruno S.", ""], ["Saulo", "Helton", ""], ["Vila", "Roberto", ""]]}, {"id": "2012.10763", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Ruofan Xu", "title": "Functional time series forecasting of extreme values", "comments": "21 pages, 4 figures, 1 table, to appear at Communication in\n  Statistics: Case Studies and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider forecasting functional time series of extreme values within a\ngeneralised extreme value distribution (GEV). The GEV distribution can be\ncharacterised using the three parameters (location, scale and shape). As a\nresult, the forecasts of the GEV density can be accomplished by forecasting\nthese three latent parameters. Depending on the underlying data structure, some\nof the three parameters can either be modelled as scalars or functions. We\nprovide two forecasting algorithms to model and forecast these parameters. To\nassess the forecast uncertainty, we apply a sieve bootstrap method to construct\npointwise and simultaneous prediction intervals of the forecasted extreme\nvalues. Illustrated by a daily maximum temperature dataset, we demonstrate the\nadvantages of modelling these parameters as functions. Further, the\nfinite-sample performance of our methods is quantified using several\nMonte-Carlo simulated data under a range of scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 19:31:05 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Shang", "Han Lin", ""], ["Xu", "Ruofan", ""]]}, {"id": "2012.10796", "submitter": "Yongming Qu", "authors": "Yongming Qu, Ilya Lipkovich", "title": "Revisiting ICH E9 (R1) during the COVID-19 pandemic", "comments": "37 Page", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The current COVID-19 pandemic poses numerous challenges for ongoing clinical\ntrials and provides a stress-testing environment for the existing principles\nand practice of estimands in clinical trials. The pandemic may increase the\nrate of intercurrent events (ICEs) and missing values, spurring a great deal of\ndiscussion on amending protocols and statistical analysis plans to address\nthese issues. In this article we revisit recent research on estimands and\nhandling of missing values, especially the ICH E9 (R1) on Estimands and\nSensitivity Analysis in Clinical Trials. Based on an in-depth discussion of the\nstrategies for handling ICEs using a causal inference framework, we suggest\nsome improvements in applying the estimand and estimation framework in ICH E9\n(R1). Specifically, we discuss a mix of strategies allowing us to handle ICEs\ndifferentially based on reasons for ICEs. We also suggest ICEs should be\nhandled primarily by hypothetical strategies and provide examples of different\nhypothetical strategies for different types of ICEs as well as a road map for\nestimation and sensitivity analyses. We conclude that the proposed framework\nhelps streamline translating clinical objectives into targets of statistical\ninference and automatically resolves many issues with defining estimands and\nchoosing estimation procedures arising from events such as the pandemic.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 22:09:16 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Qu", "Yongming", ""], ["Lipkovich", "Ilya", ""]]}, {"id": "2012.10903", "submitter": "Lorenzo Pacchiardi", "authors": "Lorenzo Pacchiardi, Ritabrata Dutta", "title": "Score Matched Conditional Exponential Families for Likelihood-Free\n  Inference", "comments": "48 pages, 12 figures. Code for reproducing the experiments is\n  available at http://github.com/LoryPack/SM-ExpFam-LFI", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform Bayesian inference for stochastic simulator models for which the\nlikelihood is not accessible, Likelihood-Free Inference (LFI) relies on\nsimulations from the model. Standard LFI methods can be split according to how\nthese simulations are used: to build an explicit Surrogate Likelihood, or to\naccept/reject parameter values according to a measure of distance from the\nobservations (Approximate Bayesian Computation (ABC)). In both cases,\nsimulations are adaptively tailored to the value of the observation. Here, we\ngenerate parameter-simulation pairs from the model independently on the\nobservation, and use them to learn a conditional exponential family likelihood\napproximation; to parametrize it, we use Neural Networks whose weights are\ntuned with Score Matching. With our likelihood approximation, we can employ\nMCMC for doubly intractable distributions to draw samples from the posterior\nfor any number of observations without additional model simulations, with\nperformance competitive to comparable approaches. Further, the sufficient\nstatistics of the exponential family can be used as summaries in ABC,\noutperforming the state-of-the-art method in five different models with known\nlikelihood. Finally, we apply our method to a challenging model from\nmeteorology.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 11:57:30 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 09:18:48 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Pacchiardi", "Lorenzo", ""], ["Dutta", "Ritabrata", ""]]}, {"id": "2012.10943", "submitter": "Torben Sell", "authors": "Torben Sell, Sumeetpal S. Singh", "title": "Dimension-robust Function Space MCMC With Neural Network Priors", "comments": "24 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a new prior on functions spaces which scales more\nfavourably in the dimension of the function's domain compared to the usual\nKarhunen-Lo\\'eve function space prior, a property we refer to as\ndimension-robustness. The proposed prior is a Bayesian neural network prior,\nwhere each weight and bias has an independent Gaussian prior, but with the key\ndifference that the variances decrease in the width of the network, such that\nthe variances form a summable sequence and the infinite width limit neural\nnetwork is well defined. We show that our resulting posterior of the unknown\nfunction is amenable to sampling using Hilbert space Markov chain Monte Carlo\nmethods. These sampling methods are favoured because they are stable under\nmesh-refinement, in the sense that the acceptance probability does not shrink\nto 0 as more parameters are introduced to better approximate the well-defined\ninfinite limit. We show that our priors are competitive and have distinct\nadvantages over other function space priors. Upon defining a suitable\nlikelihood for continuous value functions in a Bayesian approach to\nreinforcement learning, our new prior is used in numerical examples to\nillustrate its performance and dimension-robustness.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 14:52:57 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Sell", "Torben", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "2012.10980", "submitter": "Yingjie Zheng", "authors": "Yijie Li, Wei Fan, Miao Zhang, Lili Liu, Jiangbo Bao, Yingjie Zheng", "title": "Measurement bias: a structural perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal structure for measurement bias (MB) remains controversial. Aided\nby the Directed Acyclic Graph (DAG), this paper proposes a new structure for\nmeasuring one singleton variable whose MB arises in the selection of an\nimperfect I/O device-like measurement system. For effect estimation, however,\nan extra source of MB arises from any redundant association between a measured\nexposure and a measured outcome. The misclassification will be bidirectionally\ndifferential for a common outcome, unidirectionally differential for a causal\nrelation, and non-differential for a common cause between the measured exposure\nand the measured outcome or a null effect. The measured exposure can actually\naffect the measured outcome, or vice versa. Reverse causality is a concept\ndefined at the level of measurement. Our new DAGs have clarified the structures\nand mechanisms of MB.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 17:28:54 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 06:43:43 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Li", "Yijie", ""], ["Fan", "Wei", ""], ["Zhang", "Miao", ""], ["Liu", "Lili", ""], ["Bao", "Jiangbo", ""], ["Zheng", "Yingjie", ""]]}, {"id": "2012.11016", "submitter": "Manuel Carlan", "authors": "Manuel Carlan, Thomas Kneib and Nadja Klein", "title": "Bayesian Conditional Transformation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in statistical regression methodology establish flexible\nrelationships between all parameters of the response distribution and the\ncovariates. This shift away from pure mean regression is just one example and\nis further intensified by conditional transformation models (CTMs). They aim to\ninfer the entire conditional distribution directly by applying a transformation\nfunction that transforms the response conditionally on a set of covariates\ntowards a simple log-concave reference distribution. Thus, CTMs allow not only\nvariance, kurtosis and skewness but the complete conditional distribution\nfunction to depend on the explanatory variables. In this article, we propose a\nBayesian notion of conditional transformation models (BCTM) for discrete and\ncontinuous responses in the presence of random censoring. Rather than relying\non simple polynomials, we implement a spline-based parametrization for\nmonotonic effects that are supplemented with smoothness penalties. Furthermore,\nwe are able to benefit from the Bayesian paradigm directly via easily\nobtainable credible intervals and other quantities without relying on large\nsample approximations. A simulation study demonstrates the competitiveness of\nour approach against its likelihood-based counterpart, most likely\ntransformations (MLTs) and Bayesian additive models of location, scale and\nshape (BAMLSS). Three applications illustrate the versatility of the BCTMs in\nproblems involving real world data.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 20:27:50 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 14:57:39 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Carlan", "Manuel", ""], ["Kneib", "Thomas", ""], ["Klein", "Nadja", ""]]}, {"id": "2012.11026", "submitter": "Kenric Nelson Ph.D.", "authors": "Kenric P. Nelson", "title": "Independent Approximates enable closed-form estimation of heavy-tailed\n  distributions", "comments": "30 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Approximates (IAs) are proven to enable a closed-form estimation\nof heavy-tailed distributions with an analytical density such as the\ngeneralized Pareto and Student's t distributions. A broader proof using\nconvolution of the characteristic function is described for future research.\n(IAs) are selected from independent, identically distributed samples by\npartitioning the samples into groups of size n and retaining the median of the\nsamples in those groups which have approximately equal samples. The marginal\ndistribution along the diagonal of equal values has a density proportional to\nthe nth power of the original density. This nth power density, which the IAs\napproximate, has faster tail decay enabling closed-form estimation of its\nmoments and retains a functional relationship with the original density.\nComputational experiments with between 1000 to 100,000 Student's t samples are\nreported for over a range of location, scale, and shape (inverse of degree of\nfreedom) parameters. IA pairs are used to estimate the location, IA triplets\nfor the scale, and the geometric mean of the original samples for the shape.\nWith 10,000 samples the relative bias of the parameter estimates is less than\n0.01 and a relative precision is less than plus or minus 0.1. The theoretical\nbias is zero for the location and the finite bias for the scale can be\nsubtracted out. The theoretical precision has a finite range when the shape is\nless than 2 for the location estimate and less than 3/2 for the scale estimate.\nThe boundary of finite precision can be extended using higher-order IAs.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 21:31:39 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 02:42:22 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Nelson", "Kenric P.", ""]]}, {"id": "2012.11100", "submitter": "Liu Wei", "authors": "Wei Liu, Huazhen Lin, Jin Liu, Shurong Zheng", "title": "Two-directional simultaneous inference for high-dimensional models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a general two directional simultaneous inference (TOSI)\nframework for high-dimensional models with a manifest variable or latent\nvariable structure, for example, high-dimensional mean models, high-dimensional\nsparse regression models, and high-dimensional latent factors models. TOSI\nperforms simultaneous inference on a set of parameters from two directions, one\nto test whether the assumed zero parameters indeed are zeros and one to test\nwhether exist zeros in the parameter set of nonzeros. As a result, we can\nexactly identify whether the parameters are zeros, thereby keeping the data\nstructure fully and parsimoniously expressed. We theoretically prove that the\nproposed TOSI method asymptotically controls the Type I error at the\nprespecified significance level and that the testing power converges to one.\nSimulations are conducted to examine the performance of the proposed method in\nfinite sample situations and two real datasets are analyzed. The results show\nthat the TOSI method is more predictive and has more interpretable estimators\nthan existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 03:40:01 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 12:06:58 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Liu", "Wei", ""], ["Lin", "Huazhen", ""], ["Liu", "Jin", ""], ["Zheng", "Shurong", ""]]}, {"id": "2012.11122", "submitter": "Pritam Ranjan", "authors": "M. Harshvardhan, Pritam Ranjan", "title": "Statistical Modelling and Analysis of the Computer-Simulated Datasets", "comments": null, "journal-ref": null, "doi": "10.4018/978-1-5225-8407-0.ch011", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last two decades, the science has come a long way from relying on\nonly physical experiments and observations to experimentation using computer\nsimulators. This chapter focusses on the modelling and analysis of data arising\nfrom computer simulators. It turns out that traditional statistical metamodels\nare often not very useful for analyzing such datasets. For deterministic\ncomputer simulators, the realizations of Gaussian Process (GP) models are\ncommonly used for fitting a surrogate statistical metamodel of the simulator\noutput. The chapter starts with a quick review of the standard GP based\nstatistical surrogate model. The chapter also emphasizes on the numerical\ninstability due to near-singularity of the spatial correlation structure in the\nGP model fitting process. The authors also present a few generalizations of the\nGP model, reviews methods and algorithms specifically developed for analyzing\nbig data obtained from computer model runs, and reviews the popular analysis\ngoals of such computer experiments. A few real-life computer simulators are\nalso briefly outlined here.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 05:17:50 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Harshvardhan", "M.", ""], ["Ranjan", "Pritam", ""]]}, {"id": "2012.11124", "submitter": "Pritam Ranjan", "authors": "Pritam Ranjan, M. Harshvardhan", "title": "The Evolution of Dynamic Gaussian Process Model with Applications to\n  Malaria Vaccine Coverage Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) based statistical surrogates are popular, inexpensive\nsubstitutes for emulating the outputs of expensive computer models that\nsimulate real-world phenomena or complex systems. Here, we discuss the\nevolution of dynamic GP model - a computationally efficient statistical\nsurrogate for a computer simulator with time series outputs. The main idea is\nto use a convolution of standard GP models, where the weights are guided by a\nsingular value decomposition (SVD) of the response matrix over the time\ncomponent. The dynamic GP model also adopts a localized modeling approach for\nbuilding a statistical model for large datasets.\n  In this chapter, we use several popular test function based computer\nsimulators to illustrate the evolution of dynamic GP models. We also use this\nmodel for predicting the coverage of Malaria vaccine worldwide. Malaria is\nstill affecting more than eighty countries concentrated in the tropical belt.\nIn 2019 alone, it was the cause of more than 435,000 deaths worldwide. The\nmalice is easy to cure if diagnosed in time, but the common symptoms make it\ndifficult. We focus on a recently discovered reliable vaccine called Mos-Quirix\n(RTS,S) which is currently going under human trials. With the help of publicly\navailable data on dosages, efficacy, disease incidence and communicability of\nother vaccines obtained from the World Health Organisation, we predict vaccine\ncoverage for 78 Malaria-prone countries.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 05:27:42 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Ranjan", "Pritam", ""], ["Harshvardhan", "M.", ""]]}, {"id": "2012.11281", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Towards Conditional Path Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend path analysis by giving sufficient conditions for computing the\npartial covariance of two random variables from their covariance. This is\nspecifically done by correcting the covariance with the product of some partial\nvariance ratios. As a result, the partial covariance retains the covariance's\nsalient feature of factorizing over the edges in the paths between the two\nvariables of interest.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 12:20:51 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "2012.11291", "submitter": "Michail Katsoulis", "authors": "Michail Katsoulis, Alvina G Lai, Dimitra-Kleio Kipourou, Reecha Sofat,\n  Manuel Gomes, Amitava Banerjee, Spiros Denaxas, Thomas R Lumbers, Kostas\n  Tsilidis, Harry Hemingway, Karla Diaz-Ordaz", "title": "How to estimate the association between change in a risk factor and a\n  health outcome?", "comments": "13 pages, 2 Tables, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the effect of a change in a particular risk factor and a chronic\ndisease requires information on the risk factor from two time points; the\nenrolment and the first follow-up. When using observational data to study the\neffect of such an exposure (change in risk factor) extra complications arise,\nnamely (i) when is time zero? and (ii) which information on confounders should\nwe account for in this type of analysis? From enrolment or the 1st follow-up?\nOr from both?. The combination of these questions has proven to be very\nchallenging. Researchers have applied different methodologies with mixed\nsuccess, because the different choices made when answering these questions\ninduce systematic bias. Here we review these methodologies and highlight the\nsources of bias in each type of analysis. We discuss the advantages and the\nlimitations of each method ending by making our recommendations on the analysis\nplan.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 12:33:56 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Katsoulis", "Michail", ""], ["Lai", "Alvina G", ""], ["Kipourou", "Dimitra-Kleio", ""], ["Sofat", "Reecha", ""], ["Gomes", "Manuel", ""], ["Banerjee", "Amitava", ""], ["Denaxas", "Spiros", ""], ["Lumbers", "Thomas R", ""], ["Tsilidis", "Kostas", ""], ["Hemingway", "Harry", ""], ["Diaz-Ordaz", "Karla", ""]]}, {"id": "2012.11349", "submitter": "Ryan Martin", "authors": "Pei-Shien Wu and Ryan Martin", "title": "A comparison of learning rate selection methods in generalized Bayesian\n  inference", "comments": "22 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generalized Bayes posterior distributions are formed by putting a fractional\npower on the likelihood before combining with the prior via Bayes's formula.\nThis fractional power, which is often viewed as a remedy for potential model\nmisspecification bias, is called the learning rate, and a number of data-driven\nlearning rate selection methods have been proposed in the recent literature.\nEach of these proposals has a different focus, a different target they aim to\nachieve, which makes them difficult to compare. In this paper, we provide a\ndirect head-to-head comparison of these learning rate selection methods in\nvarious misspecified model scenarios, in terms of several relevant metrics, in\nparticular, coverage probability of the generalized Bayes credible regions. In\nsome examples all the methods perform well, while in others the\nmisspecification is too severe to be overcome, but we find that the so-called\ngeneralized posterior calibration algorithm tends to outperform the others in\nterms of credible region coverage probability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 14:02:19 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Wu", "Pei-Shien", ""], ["Martin", "Ryan", ""]]}, {"id": "2012.11369", "submitter": "Oskar Allerbo", "authors": "Oskar Allerbo, Rebecka J\\\"ornsten", "title": "Flexible, Non-parametric Modeling Using Regularized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric regression, such as generalized additive models (GAMs), is\nable to capture complex data dependencies in a flexible, yet interpretable way.\nHowever, choosing the format of the additive components often requires\nnon-trivial data exploration. Here, we propose an alternative to GAMs,\nPrAda-net, which uses a one hidden layer neural network, trained with proximal\ngradient descent and adaptive lasso. PrAda-net automatically adjusts the size\nand architecture of the neural network to capture the complexity and structure\nof the underlying data generative model. The compact network obtained by\nPrAda-net can be translated to additive model components, making it suitable\nfor non-parametric statistical modelling with automatic model selection. We\ndemonstrate PrAda-net on simulated data, where we compare the test error\nperformance, variable importance and variable subset identification properties\nof PrAda-net to other lasso-based approaches. We also apply Prada-net to the\nmassive U.K. black smoke data set, to demonstrate the capability of using\nPrada-net as an alternative to GAMs. In contrast to GAMs, which often require\ndomain knowledge to select the functional forms of the additive components,\nPrada-net requires no such pre-selection while still resulting in interpretable\nadditive components.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 08:49:04 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 09:36:55 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Allerbo", "Oskar", ""], ["J\u00f6rnsten", "Rebecka", ""]]}, {"id": "2012.11470", "submitter": "Laura Freijeiro-Gonz\\'alez", "authors": "Laura Freijeiro-Gonz\\'alez, Manuel Febrero-Bande and Wenceslao\n  Gonz\\'alez-Manteiga", "title": "A critical review of LASSO and its derivatives for variable selection\n  under dependence among covariates", "comments": "26 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the limitations of the well known LASSO regression as a variable\nselector when there exists dependence structures among covariates. We analyze\nboth the classic situation with $n\\geq p$ and the high dimensional framework\nwith $p>n$. Restrictive properties of this methodology to guarantee optimality,\nas well as the inconveniences in practice, are analyzed. Examples of these\ndrawbacks are showed by means of a extensive simulation study, making use of\ndifferent dependence scenarios. In order to search for improvements, a broad\ncomparison with LASSO derivatives and alternatives is carried out. Eventually,\nwe give some guidance about what procedures are the best in terms of the data\nnature.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 16:38:07 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Freijeiro-Gonz\u00e1lez", "Laura", ""], ["Febrero-Bande", "Manuel", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""]]}, {"id": "2012.11501", "submitter": "Michael Griebel", "authors": "Alexandros Gilch and Michael Griebel and Jens Oettershagen", "title": "Sparse tensor product approximation for a class of generalized method of\n  moments estimators", "comments": "33 pages, 4 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generalized Method of Moments (GMM) estimators in their various forms,\nincluding the popular Maximum Likelihood (ML) estimator, are frequently applied\nfor the evaluation of complex econometric models with not analytically\ncomputable moment or likelihood functions. As the objective functions of GMM-\nand ML-estimators themselves constitute the approximation of an integral, more\nprecisely of the expected value over the real world data space, the question\narises whether the approximation of the moment function and the simulation of\nthe entire objective function can be combined. Motivated by the popular Probit\nand Mixed Logit models, we consider double integrals with a linking function\nwhich stems from the considered estimator, e.g. the logarithm for Maximum\nLikelihood, and apply a sparse tensor product quadrature to reduce the\ncomputational effort for the approximation of the combined integral. Given\nH\\\"older continuity of the linking function, we prove that this approach can\nimprove the order of the convergence rate of the classical GMM- and\nML-estimator by a factor of two, even for integrands of low regularity or high\ndimensionality. This result is illustrated by numerical simulations of Mixed\nLogit and Multinomial Probit integrals which are estimated by ML- and\nGMM-estimators, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 17:15:29 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 16:59:43 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Gilch", "Alexandros", ""], ["Griebel", "Michael", ""], ["Oettershagen", "Jens", ""]]}, {"id": "2012.11542", "submitter": "Sean Elliott", "authors": "Sean Elliott and Christian Gourieroux", "title": "Uncertainty on the Reproduction Ratio in the SIR Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this paper is to understand the extreme variability on the\nestimated reproduction ratio $R_0$ observed in practice. For expository purpose\nwe consider a discrete time stochastic version of the\nSusceptible-Infected-Recovered (SIR) model, and introduce different approximate\nmaximum likelihood (AML) estimators of $R_0$. We carefully discuss the\nproperties of these estimators and illustrate by a Monte-Carlo study the width\nof confidence intervals on $R_0$.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:21:36 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Elliott", "Sean", ""], ["Gourieroux", "Christian", ""]]}, {"id": "2012.11574", "submitter": "Nikola Bani\\'c", "authors": "Nikola Bani\\'c and Neven Elezovi\\'c", "title": "TVOR: Finding Discrete Total Variation Outliers among Histograms", "comments": "28 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pearson's chi-squared test can detect outliers in the data distribution of a\ngiven set of histograms. However, in fields such as demographics (for e.g.\nbirth years), outliers may be more easily found in terms of the histogram\nsmoothness where techniques such as Whipple's or Myers' indices handle\nsuccessfully only specific anomalies. This paper proposes smoothness outliers\ndetection among histograms by using the relation between their discrete total\nvariations (DTV) and their respective sample sizes. This relation is\nmathematically derived to be applicable in all cases and simplified by an\naccurate linear model. The deviation of the histogram's DTV from the value\npredicted by the model is used as the outlier score and the proposed method is\nnamed Total Variation Outlier Recognizer (TVOR). TVOR requires no prior\nassumptions about the histograms' samples' distribution, it has no\nhyperparameters that require tuning, it is not limited to only specific\npatterns, and it is applicable to histograms with the same bins. Each bin can\nhave an arbitrary interval that can also be unbounded. TVOR finds DTV outliers\neasier than Pearson's chi-squared test. In case of distribution outliers, the\nopposite holds. TVOR is tested on real census data and it successfully finds\nsuspicious histograms. The source code is given at\nhttps://github.com/DiscreteTotalVariation/TVOR.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:52:07 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bani\u0107", "Nikola", ""], ["Elezovi\u0107", "Neven", ""]]}, {"id": "2012.11676", "submitter": "Zhou Fan", "authors": "Xinyi Zhong and Chang Su and Zhou Fan", "title": "Empirical Bayes PCA in high dimensions", "comments": "v2: Shorten exposition, clarify some theoretical details", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the dimension of data is comparable to or larger than the number of data\nsamples, Principal Components Analysis (PCA) may exhibit problematic\nhigh-dimensional noise. In this work, we propose an Empirical Bayes PCA method\nthat reduces this noise by estimating a joint prior distribution for the\nprincipal components. EB-PCA is based on the classical Kiefer-Wolfowitz\nnonparametric MLE for empirical Bayes estimation, distributional results\nderived from random matrix theory for the sample PCs, and iterative refinement\nusing an Approximate Message Passing (AMP) algorithm. In theoretical \"spiked\"\nmodels, EB-PCA achieves Bayes-optimal estimation accuracy in the same settings\nas an oracle Bayes AMP procedure that knows the true priors. Empirically,\nEB-PCA significantly improves over PCA when there is strong prior structure,\nboth in simulation and on quantitative benchmarks constructed from the 1000\nGenomes Project and the International HapMap Project. An illustration is\npresented for analysis of gene expression data obtained by single-cell RNA-seq.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 20:43:44 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 15:20:16 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhong", "Xinyi", ""], ["Su", "Chang", ""], ["Fan", "Zhou", ""]]}, {"id": "2012.11735", "submitter": "Abhijit Mandal", "authors": "Pushpinder Singh, Abhijit Mandal, Ayanendranath Basu", "title": "Robust Inference Using the Exponential-Polynomial Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Density-based minimum divergence procedures represent popular techniques in\nparametric statistical inference. They combine strong robustness properties\nwith high (sometimes full) asymptotic efficiency. Among density-based minimum\ndistance procedures, the methods based on the Bregman-divergence have the\nattractive property that the empirical formulation of the divergence does not\nrequire the use of any non-parametric smoothing technique such as kernel\ndensity estimation. The methods based on the density power divergence (DPD)\nrepresent the current standard in this area of research. In this paper, we will\npresent a more generalized divergence that subsumes the DPD as a special case\nand produces several new options providing better compromises between\nrobustness and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 23:10:13 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Singh", "Pushpinder", ""], ["Mandal", "Abhijit", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "2012.11767", "submitter": "Garritt Page", "authors": "Yawen Guan, Garritt L. Page, Brian J Reich, Massimo Ventrucci, Shu\n  Yang", "title": "A spectral adjustment for spatial confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adjusting for an unmeasured confounder is generally an intractable problem,\nbut in the spatial setting it may be possible under certain conditions. In this\npaper, we derive necessary conditions on the coherence between the treatment\nvariable of interest and the unmeasured confounder that ensure the causal\neffect of the treatment is estimable. We specify our model and assumptions in\nthe spectral domain to allow for different degrees of confounding at different\nspatial resolutions. The key assumption that ensures identifiability is that\nconfounding present at global scales dissipates at local scales. We show that\nthis assumption in the spectral domain is equivalent to adjusting for\nglobal-scale confounding in the spatial domain by adding a spatially smoothed\nversion of the treatment variable to the mean of the response variable. Within\nthis general framework, we propose a sequence of confounder adjustment methods\nthat range from parametric adjustments based on the Matern coherence function\nto more robust semi-parametric methods that use smoothing splines. These ideas\nare applied to areal and geostatistical data for both simulated and real\ndatasets\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 00:39:11 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Guan", "Yawen", ""], ["Page", "Garritt L.", ""], ["Reich", "Brian J", ""], ["Ventrucci", "Massimo", ""], ["Yang", "Shu", ""]]}, {"id": "2012.11798", "submitter": "Jialei Chen", "authors": "Jialei Chen, Zhehui Chen, Chuck Zhang, C. F. Jeff Wu", "title": "APIK: Active Physics-Informed Kriging Model with Partial Differential\n  Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kriging (or Gaussian process regression) is a popular machine learning method\nfor its flexibility and closed-form prediction expressions. However, one of the\nkey challenges in applying kriging to engineering systems is that the available\nmeasurement data is scarce due to the measurement limitations and high sensing\ncosts. On the other hand, physical knowledge of the engineering system is often\navailable and represented in the form of partial differential equations (PDEs).\nWe present in this work a PDE Informed Kriging model (PIK), which introduces\nPDE information via a set of PDE points and conducts posterior prediction\nsimilar to the standard kriging method. The proposed PIK model can incorporate\nphysical knowledge from both linear and nonlinear PDEs. To further improve\nlearning performance, we propose an Active PIK framework (APIK) that designs\nPDE points to leverage the PDE information based on the PIK model and\nmeasurement data. The selected PDE points not only explore the whole input\nspace but also exploit the locations where the PDE information is critical in\nreducing predictive uncertainty. Finally, an expectation-maximization algorithm\nis developed for parameter estimation. We demonstrate the effectiveness of APIK\nin two synthetic examples, a shock wave case study, and a laser heating case\nstudy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 02:31:26 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Chen", "Jialei", ""], ["Chen", "Zhehui", ""], ["Zhang", "Chuck", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "2012.11826", "submitter": "Anupam Kundu", "authors": "Anupam Kundu and Mohsen Pourahmadi", "title": "MLE of Jointly Constrained Mean-Covariance of Multivariate Normal\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Estimating the unconstrained mean and covariance matrix is a popular topic in\nstatistics. However, estimation of the parameters of $N_p(\\mu,\\Sigma)$ under\njoint constraints such as $\\Sigma\\mu = \\mu$ has not received much attention. It\ncan be viewed as a multivariate counterpart of the classical estimation problem\nin the $N(\\theta,\\theta^2)$ distribution. In addition to the usual inference\nchallenges under such non-linear constraints among the parameters (curved\nexponential family), one has to deal with the basic requirements of symmetry\nand positive definiteness when estimating a covariance matrix. We derive the\nnon-linear likelihood equations for the constrained maximum likelihood\nestimator of $(\\mu,\\Sigma)$ and solve them using iterative methods. Generally,\nthe MLE of covariance matrices computed using iterative methods do not satisfy\nthe constraints. We propose a novel algorithm to modify such (infeasible)\nestimators or any other (reasonable) estimator. The key step is to re-align the\nmean vector along the eigenvectors of the covariance matrix using the idea of\nregression. In using the Lagrangian function for constrained MLE (Aitchison et\nal. 1958), the Lagrange multiplier entangles with the parameters of interest\nand presents another computational challenge. We handle this by either\niterative or explicit calculation of the Lagrange multiplier. The existence and\nnature of location of the constrained MLE are explored within a data-dependent\nconvex set using recent results from random matrix theory. A simulation study\nillustrates our methodology and shows that the modified estimators perform\nbetter than the initial estimators from the iterative methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 04:32:46 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Kundu", "Anupam", ""], ["Pourahmadi", "Mohsen", ""]]}, {"id": "2012.11915", "submitter": "Andreas Kryger Jensen", "authors": "Claus Thorn Ekstr{\\o}m and Andreas Kryger Jensen", "title": "Having a Ball: evaluating scoring streaks and game excitement using\n  in-match trend estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many popular sports involve matches between two teams or players where each\nteam have the possibility of scoring points throughout the match. While the\noverall match winner and result is interesting, it conveys little information\nabout the underlying scoring trends throughout the match. Modeling approaches\nthat accommodate a finer granularity of the score difference throughout the\nmatch is needed to evaluate in-game strategies, discuss scoring streaks, teams\nstrengths, and other aspects of the game.\n  We propose a latent Gaussian process to model the score difference between\ntwo teams and introduce the Trend Direction Index as an easily interpretable\nprobabilistic measure of the current trend in the match as well as a measure of\npost-game trend evaluation. In addition we propose the Excitement Trend Index -\nthe expected number of monotonicity changes in the running score difference -\nas a measure of overall game excitement.\n  Our proposed methodology is applied to all 1143 matches from the 2019-2020\nNational Basketball Association (NBA) season. We show how the trends can be\ninterpreted in individual games and how the excitement score can be used to\ncluster teams according to how exciting they are to watch.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 10:30:12 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Ekstr\u00f8m", "Claus Thorn", ""], ["Jensen", "Andreas Kryger", ""]]}, {"id": "2012.12102", "submitter": "Chul Moon", "authors": "Chul Moon, Qiwei Li, Guanghua Xiao", "title": "Using Persistent Homology Topological Features to Characterize Medical\n  Images: Case Studies on Lung and Brain Cancers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tumor shape is a key factor that affects tumor growth and metastasis. This\npaper proposes a topological feature computed by persistent homology to\ncharacterize tumor progression from digital pathology and radiology images and\nexamines its effect on the time-to-event data. The proposed topological\nfeatures are invariant to scale-preserving transformation and can summarize\nvarious tumor shape patterns. The topological features are represented in\nfunctional space and used as functional predictors in a functional Cox\nproportional hazards model. The proposed model enables interpretable inference\nabout the association between topological shape features and survival risks.\nTwo case studies are conducted using consecutive 143 lung cancer and 77 brain\ntumor patients. The results of both studies show that the topological features\npredict survival prognosis after adjusting clinical variables, and the\npredicted high-risk groups have significantly (at the level of 0.01) worse\nsurvival outcomes than the low-risk groups. Also, the topological shape\nfeatures found to be positively associated with survival hazards are irregular\nand heterogeneous shape patterns, which are known to be related to tumor\nprogression.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:16:59 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 18:48:40 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Moon", "Chul", ""], ["Li", "Qiwei", ""], ["Xiao", "Guanghua", ""]]}, {"id": "2012.12198", "submitter": "Geoffrey Stewart Morrison", "authors": "Geoffrey Stewart Morrison, Reinoud D Stoel", "title": "Forensic strength of evidence statements should preferably be likelihood\n  ratios calculated using relevant data, quantitative measurements, and\n  statistical models", "comments": null, "journal-ref": "Aus J Forensic Sciences, 46, 282-292 (2014)", "doi": "10.1080/00450618.2013.833648", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Lennard (2013) [Fingerprint identification: how far have we come? Aus J\nForensic Sci. doi:10.1080/00450618.2012.752037] proposes that the numeric\noutput of statistical models should not be presented in court (except \"if\nnecessary\" / \"if required\"). Instead he argues in favour of an \"expert opinion\"\nwhich may be informed by a statistical model but which is not itself the output\nof a statistical model. We argue that his proposed procedure lacks the\ntransparency, the ease of testing of validity and reliability, and the relative\nrobustness to cognitive bias that are the strengths of a likelihood-ratio\napproach based on relevant data, quantitative measurements, and statistical\nmodels, and that the latter is therefore preferable.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 12:32:18 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Morrison", "Geoffrey Stewart", ""], ["Stoel", "Reinoud D", ""]]}, {"id": "2012.12337", "submitter": "Gertraud Malsiner-Walli", "authors": "Jan Greve, Bettina Gr\\\"un, Gertraud Malsiner-Walli, and Sylvia\n  Fr\\\"uhwirth-Schnatter", "title": "Spying on the prior of the number of data clusters and the partition\n  distribution in Bayesian cluster analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mixture models represent the key modelling approach for Bayesian cluster\nanalysis. Different likelihood and prior specifications are required to capture\nthe prototypical shape of the clusters. In addition, the mixture modelling\napproaches also crucially differ in the specification of the prior on the\nnumber of components and the prior on the component weight distribution. We\ninvestigate how these specifications impact on the implicitly induced prior on\nthe number of 'filled' components, i.e., data clusters, and the prior on the\npartitions. We derive computationally feasible calculations to obtain these\nimplicit priors for reasonable data analysis settings and make a reference\nimplementation available in the R package 'fipp'. In many applications the\nimplicit priors are of more practical relevance than the explicit priors\nimposed and thus suitable prior specifications depend on the implicit priors\ninduced. We highlight the insights which may be gained from inspecting these\nimplicit priors by analysing them for three different modelling approaches\npreviously proposed for Bayesian cluster analysis. These modelling approaches\nconsist of the Dirichlet process mixture and the static and dynamic mixture of\nfinite mixtures model. The default priors suggested in the literature for these\nmodelling approaches are used and the induced priors compared. Based on the\nimplicit priors, we discuss the suitability of these modelling approaches and\nprior specifications when aiming at sparse cluster solutions and flexibility in\nthe prior on the partitions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 20:17:47 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Greve", "Jan", ""], ["Gr\u00fcn", "Bettina", ""], ["Malsiner-Walli", "Gertraud", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""]]}, {"id": "2012.12449", "submitter": "Roy Adams", "authors": "Noam Finkelstein, Roy Adams, Suchi Saria, Ilya Shpitser", "title": "Partial Identifiability in Discrete Data With Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data contains measurement errors, it is necessary to make assumptions\nrelating the observed, erroneous data to the unobserved true phenomena of\ninterest. These assumptions should be justifiable on substantive grounds, but\nare often motivated by mathematical convenience, for the sake of exactly\nidentifying the target of inference. We adopt the view that it is preferable to\npresent bounds under justifiable assumptions than to pursue exact\nidentification under dubious ones. To that end, we demonstrate how a broad\nclass of modeling assumptions involving discrete variables, including common\nmeasurement error and conditional independence assumptions, can be expressed as\nlinear constraints on the parameters of the model. We then use linear\nprogramming techniques to produce sharp bounds for factual and counterfactual\ndistributions under measurement error in such models. We additionally propose a\nprocedure for obtaining outer bounds on non-linear models. Our method yields\nsharp bounds in a number of important settings -- such as the instrumental\nvariable scenario with measurement error -- for which no bounds were previously\nknown.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 02:11:08 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Finkelstein", "Noam", ""], ["Adams", "Roy", ""], ["Saria", "Suchi", ""], ["Shpitser", "Ilya", ""]]}, {"id": "2012.12461", "submitter": "Janice Scealy", "authors": "Janice L. Scealy and Andrew T. A. Wood", "title": "Score matching for compositional distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Compositional data and multivariate count data with known totals are\nchallenging to analyse due to the non-negativity and sum-to-one constraints on\nthe sample space. It is often the case that many of the compositional\ncomponents are highly right-skewed, with large numbers of zeros. A major\nlimitation of currently available estimators for compositional models is that\nthey either cannot handle many zeros in the data or are not computationally\nfeasible in moderate to high dimensions. We derive a new set of novel score\nmatching estimators applicable to distributions on a Riemannian manifold with\nboundary, of which the standard simplex is a special case. The score matching\nmethod is applied to estimate the parameters in a new flexible truncation model\nfor compositional data and we show that the estimators are scalable and\navailable in closed form. Through extensive simulation studies, the scoring\nmethodology is demonstrated to work well for estimating the parameters in the\nnew truncation model and also for the Dirichlet distribution. We apply the new\nmodel and estimators to real microbiome compositional data and show that the\nmodel provides a good fit to the data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 02:53:15 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Scealy", "Janice L.", ""], ["Wood", "Andrew T. A.", ""]]}, {"id": "2012.12499", "submitter": "Hailiang Du", "authors": "Hailiang Du", "title": "Beyond Strictly Proper Scoring Rules: The Importance of Being Local", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The evaluation of probabilistic forecasts plays a central role both in the\ninterpretation and in the use of forecast systems and their development.\nProbabilistic scores (scoring rules) provide statistical measures to assess the\nquality of probabilistic forecasts. Often, many probabilistic forecast systems\nare available while evaluations of their performance are not standardized, with\ndifferent scoring rules being used to measure different aspects of forecast\nperformance. Even when the discussion is restricted to strictly proper scoring\nrules, there remains considerable variability between them; indeed strictly\nproper scoring rules need not rank competing forecast systems in the same order\nwhen none of these systems are perfect. The locality property is explored to\nfurther distinguish scoring rules. The nonlocal strictly proper scoring rules\nconsidered are shown to have a property that can produce \"unfortunate\"\nevaluations. Particularly the fact that Continuous Rank Probability Score\nprefers the outcome close to the median of the forecast distribution regardless\nthe probability mass assigned to the value at/near the median raises concern to\nits use. The only local strictly proper scoring rules, the logarithmic score,\nhas direct interpretations in terms of probabilities and bits of information.\nThe nonlocal strictly proper scoring rules, on the other hand, lack meaningful\ndirect interpretation for decision support. The logarithmic score is also shown\nto be invariant under smooth transformation of the forecast variable, while the\nnonlocal strictly proper scoring rules considered may, however, change their\npreferences due to the transformation. It is therefore suggested that the\nlogarithmic score always be included in the evaluation of probabilistic\nforecasts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 05:48:35 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Du", "Hailiang", ""]]}, {"id": "2012.12550", "submitter": "Roger Koenker", "authors": "Jiaying Gu and Roger Koenker", "title": "Invidious Comparisons: Ranking and Selection as Compound Decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an innate human tendency, one might call it the \"league table\nmentality,\" to construct rankings. Schools, hospitals, sports teams, movies,\nand myriad other objects are ranked even though their inherent\nmulti-dimensionality would suggest that -- at best -- only partial orderings\nwere possible. We consider a large class of elementary ranking problems in\nwhich we observe noisy, scalar measurements of merit for $n$ objects of\npotentially heterogeneous precision and are asked to select a group of the\nobjects that are \"most meritorious.\" The problem is naturally formulated in the\ncompound decision framework of Robbins's (1956) empirical Bayes theory, but it\nalso exhibits close connections to the recent literature on multiple testing.\nThe nonparametric maximum likelihood estimator for mixture models (Kiefer and\nWolfowitz (1956)) is employed to construct optimal ranking and selection rules.\nPerformance of the rules is evaluated in simulations and an application to\nranking U.S kidney dialysis centers.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 09:21:38 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 09:37:24 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Gu", "Jiaying", ""], ["Koenker", "Roger", ""]]}, {"id": "2012.12583", "submitter": "Pedro Cardoso-Leite", "authors": "Aur\\'elien Defossez, Morteza Ansarinia, Brice Clocher, Emmanuel\n  Schm\\\"uck, Paul Schrater and Pedro Cardoso-Leite", "title": "The structure of behavioral data", "comments": "12 pages, 1 table, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For more than a century, scientists have been collecting behavioral data--an\nincreasing fraction of which is now being publicly shared so other researchers\ncan reuse them to replicate, integrate or extend past results. Although\nbehavioral data is fundamental to many scientific fields, there is currently no\nwidely adopted standard for formatting, naming, organizing, describing or\nsharing such data. This lack of standardization is a major bottleneck for\nscientific progress. Not only does it prevent the effective reuse of data, it\nalso affects how behavioral data in general are processed, as non-standard data\ncalls for custom-made data analysis code and prevents the development of\nefficient tools. To address this problem, we develop the Behaverse Data Model\n(BDM), a standard for structuring behavioral data. Here we focus on major\nconcepts in behavioral data, leaving further details and developments to the\nproject's website (https://behaverse.github.io/data-model/).\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 10:22:00 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Defossez", "Aur\u00e9lien", ""], ["Ansarinia", "Morteza", ""], ["Clocher", "Brice", ""], ["Schm\u00fcck", "Emmanuel", ""], ["Schrater", "Paul", ""], ["Cardoso-Leite", "Pedro", ""]]}, {"id": "2012.12615", "submitter": "Jon Cockayne", "authors": "Jon Cockayne and Ilse C.F. Ipsen and Chris J. Oates and Tim W. Reid", "title": "Probabilistic Iterative Methods for Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a probabilistic perspective on iterative methods for\napproximating the solution $\\mathbf{x}_* \\in \\mathbb{R}^d$ of a nonsingular\nlinear system $\\mathbf{A} \\mathbf{x}_* = \\mathbf{b}$. In the approach a\nstandard iterative method on $\\mathbb{R}^d$ is lifted to act on the space of\nprobability distributions $\\mathcal{P}(\\mathbb{R}^d)$. Classically, an\niterative method produces a sequence $\\mathbf{x}_m$ of approximations that\nconverge to $\\mathbf{x}_*$. The output of the iterative methods proposed in\nthis paper is, instead, a sequence of probability distributions $\\mu_m \\in\n\\mathcal{P}(\\mathbb{R}^d)$. The distributional output both provides a \"best\nguess\" for $\\mathbf{x}_*$, for example as the mean of $\\mu_m$, and also\nprobabilistic uncertainty quantification for the value of $\\mathbf{x}_*$ when\nit has not been exactly determined. Theoretical analysis is provided in the\nprototypical case of a stationary linear iterative method. In this setting we\ncharacterise both the rate of contraction of $\\mu_m$ to an atomic measure on\n$\\mathbf{x}_*$ and the nature of the uncertainty quantification being provided.\nWe conclude with an empirical illustration that highlights the insight into\nsolution uncertainty that can be provided by probabilistic iterative methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 11:55:57 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 14:30:53 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Cockayne", "Jon", ""], ["Ipsen", "Ilse C. F.", ""], ["Oates", "Chris J.", ""], ["Reid", "Tim W.", ""]]}, {"id": "2012.12814", "submitter": "Haeran Cho Dr", "authors": "Haeran Cho and Claudia Kirch", "title": "Data segmentation algorithms: Univariate mean change and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data segmentation a.k.a. multiple change point analysis has received\nconsiderable attention due to its importance in time series analysis and signal\nprocessing, with applications in a variety of fields including natural and\nsocial sciences, medicine, engineering and finance.\n  In the first part of this survey, we review the existing literature on the\ncanonical data segmentation problem which aims at detecting and localising\nmultiple change points in the mean of univariate time series. We provide an\noverview of popular methodologies on their computational complexity and\ntheoretical properties. In particular, our theoretical discussion focuses on\nthe separation rate relating to which change points are detectable by a given\nprocedure, and the localisation rate quantifying the precision of corresponding\nchange point estimators, and we distinguish between whether a homogeneous or\nmultiscale viewpoint has been adopted in their derivation. We further highlight\nthat the latter viewpoint provides the most general setting for investigating\nthe optimality of data segmentation algorithms.\n  Arguably, the canonical segmentation problem has been the most popular\nframework to propose new data segmentation algorithms and study their\nefficiency in the last decades. In the second part of this survey, we motivate\nthe importance of attaining an in-depth understanding of strengths and\nweaknesses of methodologies for the change point problem in a simpler,\nunivariate setting, as a stepping stone for the development of methodologies\nfor more complex problems. We illustrate this with a range of examples\nshowcasing the connections between complex distributional changes and those in\nthe mean. We also discuss extensions towards high-dimensional change point\nproblems where we demonstrate that the challenges arising from high\ndimensionality are orthogonal to those in dealing with multiple change points.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 17:21:33 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 08:41:49 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Cho", "Haeran", ""], ["Kirch", "Claudia", ""]]}, {"id": "2012.13025", "submitter": "Kang Du", "authors": "Kang Du and Yu Xiang", "title": "Causal Inference Using Linear Time-Varying Filters with Additive Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference using the restricted structural causal model framework\nhinges largely on the asymmetry between cause and effect from the data\ngenerating mechanisms. For linear non-Gaussian noise models and nonlinear\nadditive noise models, the asymmetry arises from non-Gaussianity or\nnonlinearity, respectively. Despite the fact that this methodology can be\nadapted to stationary time series, inferring causal relationships from\nnonstationary time series remains a challenging task. In this work, we focus on\nslowly-varying nonstationary processes and propose to break the symmetry by\nexploiting the nonstationarity of the data. Our main theoretical result shows\nthat the causal direction is identifiable in generic cases when cause and\neffect are connected via a time-varying filter. We propose a causal discovery\nprocedure by leveraging powerful estimates of the bivariate evolutionary\nspectra. Both synthetic and real-world data simulations that involve high-order\nand non-smooth filters are provided to demonstrate the effectiveness of our\nproposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 23:35:58 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 20:56:03 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Du", "Kang", ""], ["Xiang", "Yu", ""]]}, {"id": "2012.13075", "submitter": "Zhou Lan", "authors": "Zhou Lan", "title": "Correlated Wishart Matrices Classification via an\n  Expectation-Maximization Composite Likelihood-Based Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Positive-definite matrix-variate data is becoming popular in computer vision.\nThe computer vision data descriptors in the form of Region Covariance\nDescriptors (RCD) are positive definite matrices, which extract the key\nfeatures of the images. The RCDs are extensively used in image set\nclassification. Some classification methods treating RCDs as Wishart\ndistributed random matrices are being proposed. However, the majority of the\ncurrent methods preclude the potential correlation among the RCDs caused by the\nso-called non-voxel information (e.g., subjects' ages and nose widths, etc).\nModeling correlated Wishart matrices is difficult since the joint density\nfunction of correlated Wishart matrices is difficult to be obtained. In this\npaper, we propose an Expectation-Maximization composite likelihood-based\nalgorithm of Wishart matrices to tackle this issue. Given the numerical studies\nbased on the synthetic data and the real data (Chicago face data-set), our\nproposed algorithm performs better than the alternative methods which do not\nconsider the correlation caused by the so-called non-voxel information. All\nthese above demonstrate our algorithm's compelling potential in image set\nclassification in the coming future.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 02:49:59 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Lan", "Zhou", ""]]}, {"id": "2012.13112", "submitter": "David Walsh", "authors": "David Walsh, Alejandro Schuler, Diana Hall, Jon Walsh, Charles Fisher", "title": "Bayesian prognostic covariate adjustment", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical data about disease outcomes can be integrated into the analysis of\nclinical trials in many ways. We build on existing literature that uses\nprognostic scores from a predictive model to increase the efficiency of\ntreatment effect estimates via covariate adjustment. Here we go further,\nutilizing a Bayesian framework that combines prognostic covariate adjustment\nwith an empirical prior distribution learned from the predictive performances\nof the prognostic model on past trials. The Bayesian approach interpolates\nbetween prognostic covariate adjustment with strict type I error control when\nthe prior is diffuse, and a single-arm trial when the prior is sharply peaked.\nThis method is shown theoretically to offer a substantial increase in\nstatistical power, while limiting the type I error rate under reasonable\nconditions. We demonstrate the utility of our method in simulations and with an\nanalysis of a past Alzheimer's disease clinical trial.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 05:19:03 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Walsh", "David", ""], ["Schuler", "Alejandro", ""], ["Hall", "Diana", ""], ["Walsh", "Jon", ""], ["Fisher", "Charles", ""]]}, {"id": "2012.13133", "submitter": "Suman Majumder", "authors": "Suman Majumder, Yawen Guan, Brian J. Reich and Arvind K. Saibaba", "title": "Kryging: Geostatistical analysis of large-scale datasets using Krylov\n  subspace methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing massive spatial datasets using Gaussian process model poses\ncomputational challenges. This is a problem prevailing heavily in applications\nsuch as environmental modeling, ecology, forestry and environmental heath. We\npresent a novel approximate inference methodology that uses profile likelihood\nand Krylov subspace methods to estimate the spatial covariance parameters and\nmakes spatial predictions with uncertainty quantification. The proposed method,\nKryging, applies for both observations on regular grid and irregularly-spaced\nobservations, and for any Gaussian process with a stationary covariance\nfunction, including the popular $\\Matern$ covariance family. We make use of the\nblock Toeplitz structure with Toeplitz blocks of the covariance matrix and use\nfast Fourier transform methods to alleviate the computational and memory\nbottlenecks. We perform extensive simulation studies to show the effectiveness\nof our model by varying sample sizes, spatial parameter values and sampling\ndesigns. A real data application is also performed on a dataset consisting of\nland surface temperature readings taken by the MODIS satellite. Compared to\nexisting methods, the proposed method performs satisfactorily with much less\ncomputation time and better scalability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 06:35:17 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Majumder", "Suman", ""], ["Guan", "Yawen", ""], ["Reich", "Brian J.", ""], ["Saibaba", "Arvind K.", ""]]}, {"id": "2012.13295", "submitter": "Ioannis Kalogridis Mr", "authors": "Ioannis Kalogridis and Stefan Van Aelst", "title": "Robust penalized spline estimation with difference penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Penalized spline estimation with discrete difference penalties (P-splines) is\na popular estimation method for semiparametric models, but the classical\nleast-squares estimator is highly sensitive to deviations from its ideal model\nassumptions. To remedy this deficiency, a broad class of P-spline estimators\nbased on general loss functions is introduced and studied. Robust estimators\nare obtained by well-chosen loss functions, such as the Huber or Tukey loss\nfunction. A preliminary scale estimator can also be included in the loss\nfunction. It is shown that this class of P-spline estimators enjoys the same\noptimal asymptotic properties as least-squares P-splines, thereby providing\nstrong theoretical motivation for its use. The proposed estimators may be\ncomputed very efficiently through a simple adaptation of well-established\niterative least squares algorithms and exhibit excellent performance even in\nfinite samples, as evidenced by a numerical study and a real-data example.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 15:33:36 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 19:59:18 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Kalogridis", "Ioannis", ""], ["Van Aelst", "Stefan", ""]]}, {"id": "2012.13425", "submitter": "Vasiliki Koutra Dr", "authors": "Vasiliki Koutra, Steven G. Gilmour, Ben M. Parker, Andrew Mead", "title": "Designs with complex blocking structures and network effects for use in\n  agricultural field experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel model-based approach for constructing optimal designs with\ncomplex blocking structures and network effects, for application in\nagricultural field experiments. The potential interference among plots is\nviewed as a network structure, defined via the adjacency matrix, which performs\ntwo functions: capturing the spatial structure to reflect distances between\nneighbouring plots across space and adjusting for farmer operations. We\nconsider a field trial run at Rothamsted Research and provide a comparison of\noptimal designs under various different models, including the commonly used\ndesigns in such situations. It is shown that when there is interference between\ntreatments on neighbouring plots, due to the spatial arrangement of the plots,\nsuch designs are at least as good as, and often even more efficient than,\nrandomised row-column designs. The advantage of network designs is that we can\nconstruct the neighbouring structure even for an irregular layout by means of a\ngraph to address the particular characteristics of the experiment. The need for\nsuch designs arises when control of heterogeneity is required. Ignoring the\nneighbouring structure can lead to imprecise estimates of the treatment\nparameters and invalid conclusions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 19:12:18 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Koutra", "Vasiliki", ""], ["Gilmour", "Steven G.", ""], ["Parker", "Ben M.", ""], ["Mead", "Andrew", ""]]}, {"id": "2012.13696", "submitter": "Sayantan Banerjee", "authors": "Sayantan Banerjee and Weining Shen", "title": "Graph signal denoising using $t$-shrinkage priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the graph signal denoising problem by estimating a piecewise\nconstant signal over an undirected graph. We propose a new Bayesian approach\nthat first converts a general graph to a chain graph via the depth-first search\nalgorithm, and then imposes a heavy-tailed $t$-shrinkage prior on the\ndifferences between consecutive signals over the induced chain graph. We show\nthat the posterior computation can be conveniently conducted by fully exploring\nthe conjugacy structure in the model. We also derive the posterior contraction\nrate for the proposed estimator, and show that this rate is optimal up to a\nlogarithmic factor, besides automatically adapting to the unknown edge sparsity\nlevel of the graph. We demonstrate the excellent empirical performance of the\nproposed method via extensive simulation studies and applications to stock\nmarket data.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 07:59:48 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Banerjee", "Sayantan", ""], ["Shen", "Weining", ""]]}, {"id": "2012.13769", "submitter": "Chaofan Huang", "authors": "Chaofan Huang, V. Roshan Joseph, Simon Mak", "title": "Population Quasi-Monte Carlo", "comments": "Submitted to Journal of Computational and Graphical Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo methods are widely used for approximating complicated,\nmultidimensional integrals for Bayesian inference. Population Monte Carlo (PMC)\nis an important class of Monte Carlo methods, which utilizes a population of\nproposals to generate weighted samples that approximate the target\ndistribution. The generic PMC framework iterates over three steps: samples are\nsimulated from a set of proposals, weights are assigned to such samples to\ncorrect for mismatch between the proposal and target distributions, and the\nproposals are then adapted via resampling from the weighted samples. When the\ntarget distribution is expensive to evaluate, the PMC has its computational\nlimitation since the convergence rate is $\\mathcal{O}(N^{-1/2})$. To address\nthis, we propose in this paper a new Population Quasi-Monte Carlo (PQMC)\nframework, which integrates Quasi-Monte Carlo ideas within the sampling and\nadaptation steps of PMC. A key novelty in PQMC is the idea of importance\nsupport points resampling, a deterministic method for finding an \"optimal\"\nsubsample from the weighted proposal samples. Moreover, within the PQMC\nframework, we develop an efficient covariance adaptation strategy for\nmultivariate normal proposals. Lastly, a new set of correction weights is\nintroduced for the weighted PMC estimator to improve the efficiency from the\nstandard PMC estimator. We demonstrate the improved empirical convergence of\nPQMC over PMC in extensive numerical simulations and a friction drilling\napplication.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 16:10:54 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Huang", "Chaofan", ""], ["Joseph", "V. Roshan", ""], ["Mak", "Simon", ""]]}, {"id": "2012.13837", "submitter": "Jeong Eun Lee Dr", "authors": "Jeong Eun. Lee and Geoff K. Nicholls", "title": "Tree based credible set estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a joint Highest Posterior Density credible set for a multivariate\nposterior density is challenging as dimension gets larger. Credible intervals\nfor univariate marginals are usually presented for ease of computation and\nvisualisation. There are often two layers of approximation, as we may need to\ncompute a credible set for a target density which is itself only an\napproximation to the true posterior density. We obtain joint Highest Posterior\nDensity credible sets for density estimation trees given by Li et al. (2016)\napproximating a density truncated to a compact subset of R^d as this is\npreferred to a copula construction. These trees approximate a joint posterior\ndistribution from posterior samples using a piecewise constant function defined\nby sequential binary splits. We use a consistent estimator to measure of the\nsymmetric difference between our credible set estimate and the true HPD set of\nthe target density samples. This quality measure can be computed without the\nneed to know the true set. We show how the true-posterior-coverage of an\napproximate credible set estimated for an approximate target density may be\nestimated in doubly intractable cases where posterior samples are not\navailable. We illustrate our methods with simulation studies and find that our\nestimator is competitive with existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 00:30:57 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 04:54:08 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Lee", "Jeong Eun.", ""], ["Nicholls", "Geoff K.", ""]]}, {"id": "2012.13867", "submitter": "Gregory Watson", "authors": "Gregory L. Watson, Colleen E. Reid, Michael Jerrett, Donatello Telesca", "title": "Prediction & Model Evaluation for Space-Time Data", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation metrics for prediction error, model selection and model averaging\non space-time data are understudied and poorly understood. The absence of\nindependent replication makes prediction ambiguous as a concept and renders\nevaluation procedures developed for independent data inappropriate for most\nspace-time prediction problems. Motivated by air pollution data collected\nduring California wildfires in 2008, this manuscript attempts a formalization\nof the true prediction error associated with spatial interpolation. We\ninvestigate a variety of cross-validation (CV) procedures employing both\nsimulations and case studies to provide insight into the nature of the estimand\ntargeted by alternative data partition strategies. Consistent with recent best\npractice, we find that location-based cross-validation is appropriate for\nestimating spatial interpolation error as in our analysis of the California\nwildfire data. Interestingly, commonly held notions of bias-variance trade-off\nof CV fold size do not trivially apply to dependent data, and we recommend\nleave-one-location-out (LOLO) CV as the preferred prediction error metric for\nspatial interpolation.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 05:53:23 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Watson", "Gregory L.", ""], ["Reid", "Colleen E.", ""], ["Jerrett", "Michael", ""], ["Telesca", "Donatello", ""]]}, {"id": "2012.13885", "submitter": "Chan Park", "authors": "Chan Park and Hyunseung Kang", "title": "Assumption-Lean Analysis of Cluster Randomized Trials in Infectious\n  Diseases for Intent-to-Treat Effects and Network Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Cluster randomized trials (CRTs) are a popular design to study the effect of\ninterventions in infectious disease settings. However, standard analysis of\nCRTs primarily relies on strong parametric methods, usually a Normal mixed\neffect models to account for the clustering structure, and focus on the overall\nintent-to-treat (ITT) effect to evaluate effectiveness. The paper presents two\nassumption-lean methods to analyze two types of effects in CRTs, ITT effects\nand network effects among well-known compliance groups. For the ITT effects, we\nstudy the overall and the heterogeneous ITT effects among the observed\ncovariates where we do not impose parametric models or asymptotic restrictions\non cluster size. For the network effects among compliance groups, we propose a\nnew bound-based method that uses pre-treatment covariates, classification\nalgorithms, and a linear program to obtain sharp bounds. A key feature of our\nmethod is that the bounds can become narrower as the classification algorithm\nimproves and the method may also be useful for studies of partial\nidentification with instrumental variables. We conclude by reanalyzing a CRT\nstudying the effect of face masks and hand sanitizers on transmission of 2008\ninterpandemic influenza in Hong Kong.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 07:35:26 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 21:40:39 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 03:24:54 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Park", "Chan", ""], ["Kang", "Hyunseung", ""]]}, {"id": "2012.13926", "submitter": "Michael Crowther", "authors": "Caroline E. Weibull, Paul C. Lambert, Sandra Eloranta, Therese M.L.\n  Andersson, Paul W. Dickman, Michael J. Crowther", "title": "A multi-state model incorporating estimation of excess hazards and\n  multiple time scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As cancer patient survival improves, late effects from treatment are becoming\nthe next clinical challenge. Chemotherapy and radiotherapy, for example,\npotentially increase the risk of both morbidity and mortality from second\nmalignancies and cardiovascular disease. To provide clinically relevant\npopulation-level measures of late effects, it is of importance to (1)\nsimultaneously estimate the risks of both morbidity and mortality, (2)\npartition these risks into the component expected in the absence of cancer and\nthe component due to the cancer and its treatment, and (3) incorporate the\nmultiple time scales of attained age, calendar time, and time since diagnosis.\nMulti-state models provide a framework for simultaneously studying morbidity\nand mortality, but do not solve the problem of partitioning the risks. However,\nthis partitioning can be achieved by applying a relative survival framework, by\nallowing is to directly quantify the excess risk. This paper proposes a\ncombination of these two frameworks, providing one approach to address (1)-(3).\nUsing recently developed methods in multi-state modeling, we incorporate\nestimation of excess hazards into a multi-state model. Both intermediate and\nabsorbing state risks can be partitioned and different transitions are allowed\nto have different and/or multiple time scales. We illustrate our approach using\ndata on Hodgkin lymphoma patients and excess risk of diseases of the\ncirculatory system, and provide user-friendly Stata software with accompanying\nexample code.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 11:49:40 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Weibull", "Caroline E.", ""], ["Lambert", "Paul C.", ""], ["Eloranta", "Sandra", ""], ["Andersson", "Therese M. L.", ""], ["Dickman", "Paul W.", ""], ["Crowther", "Michael J.", ""]]}, {"id": "2012.14127", "submitter": "Myung Geun Kim", "authors": "Myung Geun Kim", "title": "On deletion diagnostic statistic in regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A brief review about choosing a normalizing or scaling matrix for deletion\ndiagnostic statistic in regression is made. Some results and comments are\nadded.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 07:36:27 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 10:12:46 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kim", "Myung Geun", ""]]}, {"id": "2012.14159", "submitter": "Matthieu Marbac", "authors": "Matthieu Marbac, Mohammed Sedki, Christophe Biernacki and Vincent\n  Vandewalle", "title": "Simultaneous semi-parametric estimation of clustering and regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate the parameter estimation of regression models with fixed group\neffects, when the group variable is missing while group related variables are\navailable. This problem involves clustering to infer the missing group variable\nbased on the group related variables, and regression to build a model on the\ntarget variable given the group and eventually additional variables. Thus, this\nproblem can be formulated as the joint distribution modeling of the target and\nof the group related variables. The usual parameter estimation strategy for\nthis joint model is a two-step approach starting by learning the group variable\n(clustering step) and then plugging in its estimator for fitting the regression\nmodel (regression step). However, this approach is suboptimal (providing in\nparticular biased regression estimates) since it does not make use of the\ntarget variable for clustering. Thus, we claim for a simultaneous estimation\napproach of both clustering and regression, in a semi-parametric framework.\nNumerical experiments illustrate the benefits of our proposition by considering\nwide ranges of distributions and regression models. The relevance of our new\nmethod is illustrated on real data dealing with problems associated with high\nblood pressure prevention.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 09:36:49 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Marbac", "Matthieu", ""], ["Sedki", "Mohammed", ""], ["Biernacki", "Christophe", ""], ["Vandewalle", "Vincent", ""]]}, {"id": "2012.14303", "submitter": "Adrian Perez-Suay", "authors": "Adri\\'an P\\'erez-Suay and Gustau Camps-Valls", "title": "Causal Inference in Geosciences with Kernel Sensitivity Maps", "comments": "arXiv admin note: substantial text overlap with arXiv:1611.00555,\n  arXiv:2012.05150", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Establishing causal relations between random variables from observational\ndata is perhaps the most important challenge in today's Science. In remote\nsensing and geosciences this is of special relevance to better understand the\nEarth's system and the complex and elusive interactions between processes. In\nthis paper we explore a framework to derive cause-effect relations from pairs\nof variables via regression and dependence estimation. We propose to focus on\nthe sensitivity (curvature) of the dependence estimator to account for the\nasymmetry of the forward and inverse densities of approximation residuals.\nResults in a large collection of 28 geoscience causal inference problems\ndemonstrate the good capabilities of the method.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 21:13:21 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["P\u00e9rez-Suay", "Adri\u00e1n", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.14409", "submitter": "Peter MacDonald", "authors": "Peter W. MacDonald, Elizaveta Levina, Ji Zhu", "title": "Latent space models for multiplex networks with shared structure", "comments": "41 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent space models are frequently used for modeling single-layer networks\nand include many popular special cases, such as the stochastic block model and\nthe random dot product graph. However, they are not well-developed for more\ncomplex network structures, which are becoming increasingly common in practice.\nHere we propose a new latent space model for multiplex networks: multiple,\nheterogeneous networks observed on a shared node set. Multiplex networks can\nrepresent a network sample with shared node labels, a network evolving over\ntime, or a network with multiple types of edges. The key feature of our model\nis that it learns from data how much of the network structure is shared between\nlayers and pools information across layers as appropriate. We establish\nidentifiability, develop a fitting procedure using convex optimization in\ncombination with a nuclear norm penalty, and prove a guarantee of recovery for\nthe latent positions as long as there is sufficient separation between the\nshared and the individual latent subspaces. We compare the model to competing\nmethods in the literature on simulated networks and on a multiplex network\ndescribing the worldwide trade of agricultural products.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 18:42:19 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 20:02:30 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["MacDonald", "Peter W.", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "2012.14482", "submitter": "Nhat Ho", "authors": "Nhat Ho and Stephen G. Walker", "title": "Multivariate Smoothing via the Fourier Integral Theorem and Fourier\n  Kernel", "comments": "58 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting with the Fourier integral theorem, we present natural Monte Carlo\nestimators of multivariate functions including densities, mixing densities,\ntransition densities, regression functions, and the search for modes of\nmultivariate density functions (modal regression). Rates of convergence are\nestablished and, in many cases, provide superior rates to current standard\nestimators such as those based on kernels, including kernel density estimators\nand kernel regression functions. Numerical illustrations are presented.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 20:59:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ho", "Nhat", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2012.14530", "submitter": "S. Novak", "authors": "S.Y. Novak", "title": "On the T-test", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $T$-test is probably the most popular statistical test; it is routinely\nrecommended by the textbooks. The applicability of the test relies upon the\nvalidity of normal or Student's approximation to the distribution of Student's\nstatistic $\\,t_n$. However, the latter assumption is not valid as often as\nassumed. We show that normal or Student's approximation to $\\,\\L(t_n)\\,$ does\nnot hold uniformly even in the class $\\,{\\cal P}_n\\,$ of samples from zero-mean\nunit-variance bounded distributions. We present lower bounds to the\ncorresponding error. The fact that a non-parametric test is not applicable\nuniformly to samples from the class $\\,{\\cal P}_n\\,$ seems to be established\nfor the first time. It means the $T$-test can be misleading, and should not be\nrecommended in its present form. We suggest a generalisation of the test that\nallows for variability of possible limiting/approximating distributions to\n$\\,\\L(t_n)$.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 23:33:44 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Novak", "S. Y.", ""]]}, {"id": "2012.14589", "submitter": "Guanyu Hu", "authors": "Qingyang Liu, Guanyu Hu, Binqi Ye, Susan Wang, Yaoshi Wu", "title": "Sample Size Re-estimation Design in Phase II Dose Finding Study with\n  Multiple Dose Groups: Frequentist and Bayesian Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unblinded sample size re-estimation (SSR) is often planned in a clinical\ntrial when there is large uncertainty about the true treatment effect. For\nProof-of Concept (PoC) in a Phase II dose finding study, contrast test can be\nadopted to leverage information from all treatment groups. In this article, we\npropose two-stage SSR designs using frequentist conditional power and Bayesian\nposterior predictive power for both single and multiple contrast tests. The\nBayesian SSR can be implemented under a wide range of prior settings to\nincorporate different prior knowledge. Taking the adaptivity into account, all\ntype I errors of final analysis in this paper are rigorously protected.\nSimulation studies are carried out to demonstrate the advantages of unblinded\nSSR in multi-arm trials.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 03:48:42 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 02:04:29 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liu", "Qingyang", ""], ["Hu", "Guanyu", ""], ["Ye", "Binqi", ""], ["Wang", "Susan", ""], ["Wu", "Yaoshi", ""]]}, {"id": "2012.14614", "submitter": "Azam Asanjarani", "authors": "Azam Asanjarani, Yoni Nazarathy, Peter Taylor", "title": "A Survey of Parameter and State Estimation in Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a broad literature survey of parameter and state estimation for\nqueueing systems. Our approach is based on various inference activities,\nqueueing models, observations schemes, and statistical methods. We categorize\nthese into branches of research that we call estimation paradigms. These\ninclude: the classical sampling approach, inverse problems, inference for\nnon-interacting systems, inference with discrete sampling, inference with\nqueueing fundamentals, queue inference engine problems, Bayesian approaches,\nonline prediction, implicit models, and control, design, and uncertainty\nquantification. For each of these estimation paradigms, we outline the\nprinciples and ideas, while surveying key references. We also present various\nsimple numerical experiments. In addition to some key references mentioned\nhere, a periodically-updated comprehensive list of references dealing with\nparameter and state estimation of queues will be kept in an accompanying\nannotated bibliography.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 05:31:41 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Asanjarani", "Azam", ""], ["Nazarathy", "Yoni", ""], ["Taylor", "Peter", ""]]}, {"id": "2012.14708", "submitter": "Weichi Wu", "authors": "Weichi Wu, Zhou Zhou", "title": "Adaptive Estimation for Non-stationary Factor Models And A Test for\n  Static Factor Loadings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the estimation and testing of a class of\nhigh-dimensional non-stationary time series factor models with evolutionary\ntemporal dynamics. In particular, the entries and the dimension of the factor\nloading matrix are allowed to vary with time while the factors and the\nidiosyncratic noise components are locally stationary. We propose an adaptive\nsieve estimator for the span of the varying loading matrix and the locally\nstationary factor processes. A uniformly consistent estimator of the effective\nnumber of factors is investigated via eigenanalysis of a non-negative definite\ntime-varying matrix. A high-dimensional bootstrap-assisted test for the\nhypothesis of static factor loadings is proposed by comparing the kernels of\nthe covariance matrices of the whole time series with their local counterparts.\nWe examine our estimator and test via simulation studies and a real data\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 11:26:08 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wu", "Weichi", ""], ["Zhou", "Zhou", ""]]}, {"id": "2012.14804", "submitter": "Zhen Huang", "authors": "Zhen Huang, Nabarun Deb, Bodhisattva Sen", "title": "Kernel Partial Correlation Coefficient -- a Measure of Conditional\n  Dependence", "comments": "63 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and study a class of simple, nonparametric, yet\ninterpretable measures of conditional dependence between two random variables\n$Y$ and $Z$ given a third variable $X$, all taking values in general\ntopological spaces. The population version of any of these measures captures\nthe strength of conditional dependence and it is 0 if and only if $Y$ and $Z$\nare conditionally independent given $X$, and 1 if and only if $Y$ is a\nmeasurable function of $Z$ and $X$. Thus, our measure -- which we call kernel\npartial correlation (KPC) coefficient -- can be thought of as a nonparametric\ngeneralization of the partial correlation coefficient that possesses the above\nproperties when $(X,Y,Z)$ is jointly normal. We describe two consistent methods\nof estimating KPC. Our first method utilizes the general framework of geometric\ngraphs, including $K$-nearest neighbor graphs and minimum spanning trees. A\nsub-class of these estimators can be computed in near linear time and converges\nat a rate that automatically adapts to the intrinsic dimension(s) of the\nunderlying distribution(s). Our second strategy involves direct estimation of\nconditional mean embeddings using cross-covariance operators in the reproducing\nkernel Hilbert spaces. Using these empirical measures we develop forward\nstepwise (high-dimensional) nonlinear variable selection algorithms. We show\nthat our algorithm, using the graph-based estimator, yields a provably\nconsistent model-free variable selection procedure, even in the\nhigh-dimensional regime when the number of covariates grows exponentially with\nthe sample size, under suitable sparsity assumptions. Extensive simulation and\nreal-data examples illustrate the superior performance of our methods compared\nto existing procedures. The recent conditional dependence measure proposed by\nAzadkia and Chatterjee (2019) can be viewed as a special case of our general\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 15:33:54 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Huang", "Zhen", ""], ["Deb", "Nabarun", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "2012.14817", "submitter": "Hsien-Wei Chen", "authors": "Hsien-Wei Chen", "title": "Spatial Resolution Enhancement of Oversampled Images Using Regression\n  Decomposition and Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new statistical model designed for regression analysis with a sparse design\nmatrix is proposed. This new model utilizes the positions of the limited\nnon-zero elements in the design matrix to decompose the regression model into\nsub-regression models. Statistical inferences are further made on the values of\nthese limited non-zero elements to provide a reference for synthesizing these\nsub-regression models. With this concept of the regression decomposition and\nsynthesis, the information on the structure of the design matrix can be\nincorporated into the regression analysis to provide a more reliable\nestimation. The proposed model is then applied to resolve the spatial\nresolution enhancement problem for spatially oversampled images. To\nsystematically evaluate the performance of the proposed model in enhancing the\nspatial resolution, the proposed approach is applied to the oversampled images\nthat are reproduced via random field simulations. These application results\nbased on different generated scenarios then conclude the effectiveness and the\nfeasibility of the proposed approach in enhancing the spatial resolution of\nspatially oversampled images.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 15:55:11 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Chen", "Hsien-Wei", ""]]}, {"id": "2012.14820", "submitter": "Justyna Wr\\'oblewska", "authors": "Justyna Wr\\'oblewska", "title": "Bayesian analysis of seasonally cointegrated VAR model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The paper aims at developing the Bayesian seasonally cointegrated model for\nquarterly data. We propose the prior structure, derive the set of full\nconditional posterior distributions, and propose the sampling scheme. The\nidentification of cointegrating spaces is obtained \\emph{via} orthonormality\nrestrictions imposed on vectors spanning them. In the case of annual frequency,\nthe cointegrating vectors are complex, which should be taken into account when\nidentifying them. The point estimation of the cointegrating spaces is also\ndiscussed. The presented methods are illustrated by a simulation experiment and\nare employed in the analysis of money and prices in the Polish economy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 15:57:54 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 12:10:32 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Wr\u00f3blewska", "Justyna", ""]]}, {"id": "2012.14823", "submitter": "Michal Koles\\'ar", "authors": "Timothy B. Armstrong and Michal Koles\\'ar and Soonwoo Kwon", "title": "Bias-Aware Inference in Regularized Regression Models", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference on a regression coefficient under a constraint on the\nmagnitude of the control coefficients. We show that a class of estimators based\non an auxiliary regularized regression of the regressor of interest on control\nvariables exactly solves a tradeoff between worst-case bias and variance. We\nderive \"bias-aware\" confidence intervals (CIs) based on these estimators, which\ntake into account possible bias when forming the critical value. We show that\nthese estimators and CIs are near-optimal in finite samples for mean squared\nerror and CI length. Our finite-sample results are based on an idealized\nsetting with normal regression errors with known homoskedastic variance, and we\nprovide conditions for asymptotic validity with unknown and possibly\nheteroskedastic error distribution. Focusing on the case where the constraint\non the magnitude of control coefficients is based on an $\\ell_p$ norm ($p\\ge\n1$), we derive rates of convergence for optimal estimators and CIs under\nhigh-dimensional asymptotics that allow the number of regressors to increase\nmore quickly than the number of observations.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 16:06:43 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Armstrong", "Timothy B.", ""], ["Koles\u00e1r", "Michal", ""], ["Kwon", "Soonwoo", ""]]}, {"id": "2012.14844", "submitter": "Anru R. Zhang", "authors": "Dong Xia and Anru R. Zhang and Yuchen Zhou", "title": "Inference for Low-rank Tensors -- No Need to Debias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we consider the statistical inference for several low-rank\ntensor models. Specifically, in the Tucker low-rank tensor PCA or regression\nmodel, provided with any estimates achieving some attainable error rate, we\ndevelop the data-driven confidence regions for the singular subspace of the\nparameter tensor based on the asymptotic distribution of an updated estimate by\ntwo-iteration alternating minimization. The asymptotic distributions are\nestablished under some essential conditions on the signal-to-noise ratio (in\nPCA model) or sample size (in regression model). If the parameter tensor is\nfurther orthogonally decomposable, we develop the methods and theory for\ninference on each individual singular vector. For the rank-one tensor PCA\nmodel, we establish the asymptotic distribution for general linear forms of\nprincipal components and confidence interval for each entry of the parameter\ntensor. Finally, numerical simulations are presented to corroborate our\ntheoretical discoveries.\n  In all these models, we observe that different from many matrix/vector\nsettings in existing work, debiasing is not required to establish the\nasymptotic distribution of estimates or to make statistical inference on\nlow-rank tensors. In fact, due to the widely observed\nstatistical-computational-gap for low-rank tensor estimation, one usually\nrequires stronger conditions than the statistical (or information-theoretic)\nlimit to ensure the computationally feasible estimation is achievable.\nSurprisingly, such conditions ``incidentally\" render a feasible low-rank tensor\ninference without debiasing.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 16:48:02 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Xia", "Dong", ""], ["Zhang", "Anru R.", ""], ["Zhou", "Yuchen", ""]]}, {"id": "2012.14881", "submitter": "Christophe Andrieu", "authors": "Christophe Andrieu and Anthony Lee and Sam Livingstone", "title": "A general perspective on the Metropolis-Hastings kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its inception the Metropolis-Hastings kernel has been applied in\nsophisticated ways to address ever more challenging and diverse sampling\nproblems. Its success stems from the flexibility brought by the fact that its\nverification and sampling implementation rests on a local ``detailed balance''\ncondition, as opposed to a global condition in the form of a typically\nintractable integral equation. While checking the local condition is routine in\nthe simplest scenarios, this proves much more difficult for complicated\napplications involving auxiliary structures and variables. Our aim is to\ndevelop a framework making establishing correctness of complex Markov chain\nMonte Carlo kernels a purely mechanical or algebraic exercise, while making\ncommunication of ideas simpler and unambiguous by allowing a stronger focus on\nessential features -- a choice of embedding distribution, an involution and\noccasionally an acceptance function -- rather than the induced, boilerplate\nstructure of the kernels that often tends to obscure what is important. This\nframework can also be used to validate kernels that do not satisfy detailed\nbalance, i.e. which are not reversible, but a modified version thereof.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 18:15:21 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Andrieu", "Christophe", ""], ["Lee", "Anthony", ""], ["Livingstone", "Sam", ""]]}, {"id": "2012.14954", "submitter": "Qi Long", "authors": "Yi Deng, Xiaoqian Jiang, Qi Long", "title": "Privacy-Preserving Methods for Vertically Partitioned Incomplete Data", "comments": null, "journal-ref": "2020 AMIA Annual Symposium Proceedings", "doi": null, "report-no": null, "categories": "cs.CR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed health data networks that use information from multiple sources\nhave drawn substantial interest in recent years. However, missing data are\nprevalent in such networks and present significant analytical challenges. The\ncurrent state-of-the-art methods for handling missing data require pooling data\ninto a central repository before analysis, which may not be possible in a\ndistributed health data network. In this paper, we propose a privacy-preserving\ndistributed analysis framework for handling missing data when data are\nvertically partitioned. In this framework, each institution with a particular\ndata source utilizes the local private data to calculate necessary intermediate\naggregated statistics, which are then shared to build a global model for\nhandling missing data. To evaluate our proposed methods, we conduct simulation\nstudies that clearly demonstrate that the proposed privacy-preserving methods\nperform as well as the methods using the pooled data and outperform several\nna\\\"ive methods. We further illustrate the proposed methods through the\nanalysis of a real dataset. The proposed framework for handling vertically\npartitioned incomplete data is substantially more privacy-preserving than\nmethods that require pooling of the data, since no individual-level data are\nshared, which can lower hurdles for collaboration across multiple institutions\nand build stronger public trust.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 21:46:32 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Deng", "Yi", ""], ["Jiang", "Xiaoqian", ""], ["Long", "Qi", ""]]}, {"id": "2012.15278", "submitter": "Nora M. Villanueva", "authors": "Nora M. Villanueva and Marta Sestelo and Celestino Ord\\'o\\~nez and\n  Javier Roca-Pardi\\~nas", "title": "An automatic procedure to determine groups of nonparametric regression\n  curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many situations it could be interesting to ascertain whether nonparametric\nregression curves can be grouped, especially when confronted with a\nconsiderable number of curves. The proposed testing procedure allows to\ndetermine groups with an automatic selection of their number. A simulation\nstudy is presented in order to investigate the finite sample properties of the\nproposed methods when compared to existing alternative procedures. Finally, the\napplicability of the procedure to study the geometry of a tunnel by analysing a\nset of cross-sections is demonstrated. The results obtained show the existence\nof some heterogeneity in the tunnel geometry.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 18:49:21 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 16:45:04 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Villanueva", "Nora M.", ""], ["Sestelo", "Marta", ""], ["Ord\u00f3\u00f1ez", "Celestino", ""], ["Roca-Pardi\u00f1as", "Javier", ""]]}, {"id": "2012.15306", "submitter": "Theodore Papamarkou", "authors": "Adam Spannaus, Theodore Papamarkou, Samantha Erwin, J. Blair Christian", "title": "Bayesian state space modelling for COVID-19: with Tennessee and New York\n  case studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian inferential framework for the spread of COVID-19 using\nmechanistic epidemiological models, such as SIR or SEIR, and allow the\neffective contact rate to vary in time. A novel aspect of our approach is the\nincorporation of a time-varying reporting rate accounting for the initial phase\nof the pandemic before testing was widely available. By varying both the\nreporting rate and the effective contact rate in time, our models can capture\nchanges in the data induced by external influences, such as public health\nintervention measures, for example. We view COVID-19 incidence data as the\nobserved measurements of a hidden Markov model, with latent space represented\nby the underlying epidemiological model, and employ a particle Markov chain\nMonte Carlo (PMCMC) sampling scheme for Bayesian inference. Parameter inference\nis performed via PMCMC on incidence data collated by the New York Times from\nthe states of New York and Tennessee from March 1, 2020 through August 30,\n2020. Lastly, we perform Bayesian model selection on the different formulations\nof the epidemiological models, make predictions from our fitted models, and\nvalidate our predictions against the true incidence data for the week between\nAugust 31, 2020 and September 7, 2020.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 19:52:22 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 07:52:34 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Spannaus", "Adam", ""], ["Papamarkou", "Theodore", ""], ["Erwin", "Samantha", ""], ["Christian", "J. Blair", ""]]}, {"id": "2012.15313", "submitter": "Iain Carmichael", "authors": "Iain Carmichael", "title": "Learning Sparsity and Block Diagonal Structure in Multi-View Mixture\n  Models", "comments": "First upload, 39 pages and 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific studies increasingly collect multiple modalities of data to\ninvestigate a phenomenon from several perspectives. In integrative data\nanalysis it is important to understand how information is heterogeneously\nspread across these different data sources. To this end, we consider a\nparametric clustering model for the subjects in a multi-view data set (i.e.\nmultiple sources of data from the same set of subjects) where each view\nmarginally follows a mixture model. In the case of two views, the dependence\nbetween them is captured by a cluster membership matrix parameter and we aim to\nlearn the structure of this matrix (e.g. the zero pattern). First, we develop a\npenalized likelihood approach to estimate the sparsity pattern of the cluster\nmembership matrix. For the specific case of block diagonal structures, we\ndevelop a constrained likelihood formulation where this matrix is constrained\nto be block diagonal up to permutations of the rows and columns. To enforce\nblock diagonal constraints we propose a novel optimization approach based on\nthe symmetric graph Laplacian. We demonstrate the performance of these methods\nthrough both simulations and applications to data sets from cancer genetics and\nneuroscience. Both methods naturally extend to multiple views.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 20:30:25 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Carmichael", "Iain", ""]]}, {"id": "2012.15367", "submitter": "Brad Ross", "authors": "Billy Ferguson and Brad Ross", "title": "Assessing the Sensitivity of Synthetic Control Treatment Effect\n  Estimates to Misspecification Error", "comments": "36 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sensitivity analysis for Synthetic Control (SC) treatment effect\nestimates to interrogate the assumption that the SC method is well-specified,\nnamely that choosing weights to minimize pre-treatment prediction error yields\naccurate predictions of counterfactual post-treatment outcomes. Our data-driven\nprocedure recovers the set of treatment effects consistent with the assumption\nthat the misspecification error incurred by the SC method is at most the\nobservable misspecification error incurred when using the SC estimator to\npredict the outcomes of some control unit. We show that under one definition of\nmisspecification error, our procedure provides a simple, geometric motivation\nfor comparing the estimated treatment effect to the distribution of placebo\nresiduals to assess estimate credibility. When we apply our procedure to\nseveral canonical studies that report SC estimates, we broadly confirm the\nconclusions drawn by the source papers.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 23:35:30 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 23:57:20 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 07:16:54 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ferguson", "Billy", ""], ["Ross", "Brad", ""]]}, {"id": "2012.15664", "submitter": "Snigdha Panigrahi", "authors": "Snigdha Panigrahi, Peter W. MacDonald, Daniel Kessler", "title": "Inference post Selection of Group-sparse Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a post-selective Bayesian framework to jointly and consistently\nestimate parameters within automatic group-sparse regression models. Selected\nthrough an indispensable class of learning algorithms, e.g. the Group LASSO,\nthe overlapping Group LASSO, the sparse Group LASSO etc., uncertainty estimates\nfor the matched parameters are unreliable in the absence of adjustments for\nselection bias. Limiting however the application of state of the art tools for\nthe group-sparse problem include estimation strictly tailored to (i)\nreal-valued projections onto very specific selected subspaces, (ii) selection\nevents admitting representations as linear inequalities in the data variables.\nOur Bayesian methods address these gaps by deriving an adjustment factor in an\neasily feasible analytic form that eliminates bias from the selection of\npromising groups. Paying a very nominal price for this adjustment, experiments\non simulated data and the Human Connectome Project demonstrate the efficacy of\nour methods at a joint estimation of group-sparse parameters learned from data.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 15:43:26 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 03:58:49 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Panigrahi", "Snigdha", ""], ["MacDonald", "Peter W.", ""], ["Kessler", "Daniel", ""]]}, {"id": "2012.15716", "submitter": "Matthew Masten", "authors": "Matthew A. Masten, Alexandre Poirier, and Linqi Zhang", "title": "Assessing Sensitivity to Unconfoundedness: Estimation and Inference", "comments": "37 pages with 41 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a set of methods for quantifying the robustness of\ntreatment effects estimated using the unconfoundedness assumption (also known\nas selection on observables or conditional independence). Specifically, we\nestimate and do inference on bounds on various treatment effect parameters,\nlike the average treatment effect (ATE) and the average effect of treatment on\nthe treated (ATT), under nonparametric relaxations of the unconfoundedness\nassumption indexed by a scalar sensitivity parameter c. These relaxations allow\nfor limited selection on unobservables, depending on the value of c. For large\nenough c, these bounds equal the no assumptions bounds. Using a non-standard\nbootstrap method, we show how to construct confidence bands for these bound\nfunctions which are uniform over all values of c. We illustrate these methods\nwith an empirical application to effects of the National Supported Work\nDemonstration program. We implement these methods in a companion Stata module\nfor easy use in practice.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 17:14:38 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Masten", "Matthew A.", ""], ["Poirier", "Alexandre", ""], ["Zhang", "Linqi", ""]]}, {"id": "2012.15726", "submitter": "Geovani Rizk", "authors": "Geovani Rizk and Igor Colin and Albert Thomas and Moez Draief", "title": "Refined bounds for randomized experimental design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Experimental design is an approach for selecting samples among a given set so\nas to obtain the best estimator for a given criterion. In the context of linear\nregression, several optimal designs have been derived, each associated with a\ndifferent criterion: mean square error, robustness, \\emph{etc}. Computing such\ndesigns is generally an NP-hard problem and one can instead rely on a convex\nrelaxation that considers probability distributions over the samples. Although\ngreedy strategies and rounding procedures have received a lot of attention,\nstraightforward sampling from the optimal distribution has hardly been\ninvestigated. In this paper, we propose theoretical guarantees for randomized\nstrategies on E and G-optimal design. To this end, we develop a new\nconcentration inequality for the eigenvalues of random matrices using a refined\nversion of the intrinsic dimension that enables us to quantify the performance\nof such randomized strategies. Finally, we evidence the validity of our\nanalysis through experiments, with particular attention on the G-optimal design\napplied to the best arm identification problem for linear bandits.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 20:37:57 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Rizk", "Geovani", ""], ["Colin", "Igor", ""], ["Thomas", "Albert", ""], ["Draief", "Moez", ""]]}]