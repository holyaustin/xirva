[{"id": "1111.0317", "submitter": "Jared Murray", "authors": "Jared S. Murray, David B. Dunson, Lawrence Carin and Joseph E. Lucas", "title": "Bayesian Gaussian Copula Factor Models for Mixed Data", "comments": "To appear in JASA Theory & Methods. This revision corrects the\n  simulation study in the previous version (and adds another), adds new figures\n  and edits some of the previous figures, corrects some typographical errors\n  and has been edited for style and length", "journal-ref": null, "doi": "10.1080/01621459.2012.762328", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian factor models have proven widely useful for parsimoniously\ncharacterizing dependence in multivariate data. There is a rich literature on\ntheir extension to mixed categorical and continuous variables, using latent\nGaussian variables or through generalized latent trait models acommodating\nmeasurements in the exponential family. However, when generalizing to\nnon-Gaussian measured variables the latent variables typically influence both\nthe dependence structure and the form of the marginal distributions,\ncomplicating interpretation and introducing artifacts. To address this problem\nwe propose a novel class of Bayesian Gaussian copula factor models which\ndecouple the latent factors from the marginal distributions. A semiparametric\nspecification for the marginals based on the extended rank likelihood yields\nstraightforward implementation and substantial computational gains, critical\nfor scaling to high-dimensional applications. We provide new theoretical and\nempirical justifications for using this likelihood in Bayesian inference. We\npropose new default priors for the factor loadings and develop efficient\nparameter-expanded Gibbs sampling for posterior computation. The methods are\nevaluated through simulations and applied to a dataset in political science.\nThe methods in this paper are implemented in the R package bfa.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2011 21:06:48 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2013 18:45:49 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Murray", "Jared S.", ""], ["Dunson", "David B.", ""], ["Carin", "Lawrence", ""], ["Lucas", "Joseph E.", ""]]}, {"id": "1111.0324", "submitter": "Patrick Danaher", "authors": "Patrick Danaher, Pei Wang, Daniela M. Witten", "title": "The joint graphical lasso for inverse covariance estimation across\n  multiple classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating multiple related but distinct graphical\nmodels on the basis of a high-dimensional data set with observations that\nbelong to distinct classes. A motivating example occurs in the analysis of gene\nexpression data for tissue samples with and without cancer. In this case, we\nmight wish to estimate a gene expression network for the normal tissue and a\ngene expression network for the tumor tissue. We expect the two gene expression\nnetworks to be similar but not identical to each other, and so more accurate\nestimation of these two networks may be possible using a joint approach. We\npropose the joint graphical lasso for this purpose. Rather than estimating a\ngraphical model for each class separately, or a single graphical model across\nall classes, we borrow strength across the classes in order to estimate\nmultiple graphical models that share certain characteristics, such as the\nlocations or weights of nonzero edges. Our approach is based upon maximizing a\npenalized log likelihood. We employ fused lasso or group lasso penalties, and\nimplement a very fast computational approach that solves the joint graphical\nlasso problem. In a simulation study we demonstrate that our proposed approach\nleads to more accurate estimation of networks and covariance structure than\ncompeting approaches. We further illustrate our proposal on a\npublicly-available lung cancer gene expression data set.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2011 21:34:56 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2011 22:47:10 GMT"}, {"version": "v3", "created": "Thu, 26 Jan 2012 17:59:31 GMT"}, {"version": "v4", "created": "Wed, 11 Jul 2012 16:54:16 GMT"}], "update_date": "2012-07-12", "authors_parsed": [["Danaher", "Patrick", ""], ["Wang", "Pei", ""], ["Witten", "Daniela M.", ""]]}, {"id": "1111.0328", "submitter": "Guenther Walther", "authors": "Guenther Walther", "title": "The Average Likelihood Ratio for Large-scale Multiple Testing and\n  Detecting Sparse Mixtures", "comments": null, "journal-ref": "From Probability to Statistics and Back: High-Dimensional Models\n  and Processes - A Festschrift in Honor of Jon A. Wellner. M. Bannerjee, F.\n  Bunea, J. Huang, V. Koltchinskii, M.H. Maathuis (eds.), Inst. Math.\n  Statistics (2013), 317-326", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale multiple testing problems require the simultaneous assessment of\nmany p-values. This paper compares several methods to assess the evidence in\nmultiple binomial counts of p-values: the maximum of the binomial counts after\nstandardization (the `higher-criticism statistic'), the maximum of the binomial\ncounts after a log-likelihood ratio transformation (the `Berk-Jones\nstatistic'), and a newly introduced average of the binomial counts after a\nlikelihood ratio transformation. Simulations show that the higher criticism\nstatistic has a superior performance to the Berk-Jones statistic in the case of\nvery sparse alternatives (sparsity coefficient $\\beta \\gtrapprox 0.75$), while\nthe situation is reversed for $\\beta \\lessapprox 0.75$. The average likelihood\nratio is found to combine the favorable performance of higher criticism in the\nvery sparse case with that of the Berk-Jones statistic in the less sparse case\nand thus appears to dominate both statistics. Some asymptotic optimality theory\nis considered but found to set in too slowly to illuminate the above findings,\nat least for sample sizes up to one million. In contrast, asymptotic\napproximations to the critical values of the Berk-Jones statistic that have\nbeen developed by Wellner and Koltchinskii (2003) and Jager and Wellner (2007)\nare found to give surprisingly accurate approximations even for quite small\nsample sizes.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2011 22:10:18 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Walther", "Guenther", ""]]}, {"id": "1111.0576", "submitter": "Christian Sch\\\"afer", "authors": "Christian Sch\\\"afer", "title": "On parametric families for sampling binary data with specified mean and\n  correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a class of binary parametric families with conditional\nprobabilities taking the form of generalized linear models and show that this\napproach allows to model high-dimensional random binary vectors with arbitrary\nmean and correlation. We derive the special case of logistic conditionals as an\napproximation to the Ising-type exponential distribution and provide empirical\nevidence that this parametric family indeed outperforms competing approaches in\nterms of feasible correlations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 17:36:27 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2011 18:14:46 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2012 16:58:40 GMT"}, {"version": "v4", "created": "Fri, 6 Apr 2012 16:33:28 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Sch\u00e4fer", "Christian", ""]]}, {"id": "1111.1133", "submitter": "Xi Luo", "authors": "Xi Luo", "title": "Recovering Model Structures from Large Low Rank and Sparse Covariance\n  Matrix Estimation", "comments": "35 pages, 3 figures. Presented at JSM 2011 and various invited\n  seminars since February, 2011. R package available from\n  http://cran.r-project.org/web/packages/lorec/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.PM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular statistical models, such as factor and random effects models,\ngive arise a certain type of covariance structures that is a summation of low\nrank and sparse matrices. This paper introduces a penalized approximation\nframework to recover such model structures from large covariance matrix\nestimation. We propose an estimator based on minimizing a non-likelihood loss\nwith separable non-smooth penalty functions. This estimator is shown to recover\nexactly the rank and sparsity patterns of these two components, and thus\npartially recovers the model structures. Convergence rates under various matrix\nnorms are also presented. To compute this estimator, we further develop a\nfirst-order iterative algorithm to solve a convex optimization problem that\ncontains separa- ble non-smooth functions, and the algorithm is shown to\nproduce a solution within O(1/t^2) of the optimal, after any finite t\niterations. Numerical performance is illustrated using simulated data and stock\nportfolio selection on S&P 100.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2011 14:12:49 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2013 20:21:23 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Luo", "Xi", ""]]}, {"id": "1111.1210", "submitter": "Xiaoquan Wen", "authors": "Xiaoquan Wen, Matthew Stephens", "title": "Bayesian methods for genetic association analysis with heterogeneous\n  subgroups: From meta-analyses to gene-environment interactions", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS695 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 176-203", "doi": "10.1214/13-AOAS695", "report-no": "IMS-AOAS-AOAS695", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic association analyses often involve data from multiple\npotentially-heterogeneous subgroups. The expected amount of heterogeneity can\nvary from modest (e.g., a typical meta-analysis) to large (e.g., a strong\ngene--environment interaction). However, existing statistical tools are limited\nin their ability to address such heterogeneity. Indeed, most genetic\nassociation meta-analyses use a \"fixed effects\" analysis, which assumes no\nheterogeneity. Here we develop and apply Bayesian association methods to\naddress this problem. These methods are easy to apply (in the simplest case,\nrequiring only a point estimate for the genetic effect and its standard error,\nfrom each subgroup) and effectively include standard frequentist meta-analysis\nmethods, including the usual \"fixed effects\" analysis, as special cases. We\napply these tools to two large genetic association studies: one a meta-analysis\nof genome-wide association studies from the Global Lipids consortium, and the\nsecond a cross-population analysis for expression quantitative trait loci\n(eQTLs). In the Global Lipids data we find, perhaps surprisingly, that effects\nare generally quite homogeneous across studies. In the eQTL study we find that\neQTLs are generally shared among different continental groups, and discuss\nconsequences of this for study design.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2011 18:48:43 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2011 13:22:28 GMT"}, {"version": "v3", "created": "Mon, 14 Apr 2014 13:42:20 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Wen", "Xiaoquan", ""], ["Stephens", "Matthew", ""]]}, {"id": "1111.1687", "submitter": "Noah Simon", "authors": "Noah Simon and Rob Tibshirani", "title": "Discriminant Analysis with Adaptively Pooled Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear and Quadratic Discriminant analysis (LDA/QDA) are common tools for\nclassification problems. For these methods we assume observations are normally\ndistributed within group. We estimate a mean and covariance matrix for each\ngroup and classify using Bayes theorem. With LDA, we estimate a single, pooled\ncovariance matrix, while for QDA we estimate a separate covariance matrix for\neach group. Rarely do we believe in a homogeneous covariance structure between\ngroups, but often there is insufficient data to separately estimate covariance\nmatrices. We propose L1- PDA, a regularized model which adaptively pools\nelements of the precision matrices. Adaptively pooling these matrices decreases\nthe variance of our estimates (as in LDA), without overly biasing them. In this\npaper, we propose and discuss this method, give an efficient algorithm to fit\nit for moderate sized problems, and show its efficacy on real and simulated\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2011 19:30:13 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2011 23:45:34 GMT"}], "update_date": "2011-12-08", "authors_parsed": [["Simon", "Noah", ""], ["Tibshirani", "Rob", ""]]}, {"id": "1111.1714", "submitter": "Yakir Berchenko", "authors": "Yakir Berchenko, Richard G. White, Cyprian Wejnert, Simon D.W. Frost", "title": "Analysis of a capture-recapture estimator for the size of populations\n  with heterogenous catchability, and its evaluation on RDS data from rural\n  Uganda", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider capture-recapture experiments with heterogenous\ncatchability. In the setting we consider, the widespread Huggins-Alho estimator\nis not very suitable and we introduce and study a new generalized\nHorvitz-Thompson estimator. Our motivation is Respondent Driven Sampling (RDS),\na prime example for such a setting where the capture probability is dependent\non both the unknown population size as well as on an observable covariate, the\nnetwork degree of an individual, due to peer recruitment. After discussing the\ntheoretical properties of the new estimator, with full details given in the\nappendix, we evaluate it on various empirical and simulated data-sets, focusing\non an RDS survey in a population in rural Uganda in which the population size\nis known a priori. The results thus obtained demonstrate that the adjusted\nestimator is less biased than the naive Lincoln-Petersen estimator.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2011 20:51:18 GMT"}], "update_date": "2011-11-09", "authors_parsed": [["Berchenko", "Yakir", ""], ["White", "Richard G.", ""], ["Wejnert", "Cyprian", ""], ["Frost", "Simon D. W.", ""]]}, {"id": "1111.1802", "submitter": "Tamara Broderick", "authors": "Tamara Broderick, Lester Mackey, John Paisley, Michael I. Jordan", "title": "Combinatorial clustering and the beta negative binomial process", "comments": "56 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian nonparametric approach to a general family of latent\nclass problems in which individuals can belong simultaneously to multiple\nclasses and where each class can be exhibited multiple times by an individual.\nWe introduce a combinatorial stochastic process known as the negative binomial\nprocess (NBP) as an infinite-dimensional prior appropriate for such problems.\nWe show that the NBP is conjugate to the beta process, and we characterize the\nposterior distribution under the beta-negative binomial process (BNBP) and\nhierarchical models based on the BNBP (the HBNBP). We study the asymptotic\nproperties of the BNBP and develop a three-parameter extension of the BNBP that\nexhibits power-law behavior. We derive MCMC algorithms for posterior inference\nunder the HBNBP, and we present experiments using these algorithms in the\ndomains of image segmentation, object recognition, and document analysis.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2011 04:46:01 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2011 21:49:49 GMT"}, {"version": "v3", "created": "Fri, 17 Feb 2012 00:17:58 GMT"}, {"version": "v4", "created": "Mon, 15 Oct 2012 18:27:13 GMT"}, {"version": "v5", "created": "Mon, 10 Jun 2013 13:40:24 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Broderick", "Tamara", ""], ["Mackey", "Lester", ""], ["Paisley", "John", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1111.1830", "submitter": "Robert Hable", "authors": "Robert Hable and Andreas Christmann", "title": "Estimation of scale functions to model heteroscedasticity by support\n  vector machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main goal of regression is to derive statistical conclusions on the\nconditional distribution of the output variable Y given the input values x. Two\nof the most important characteristics of a single distribution are location and\nscale. Support vector machines (SVMs) are well established to estimate location\nfunctions like the conditional median or the conditional mean. We investigate\nthe estimation of scale functions by SVMs when the conditional median is\nunknown, too. Estimation of scale functions is important e.g. to estimate the\nvolatility in finance. We consider the median absolute deviation (MAD) and the\ninterquantile range (IQR) as measures of scale. Our main result shows the\nconsistency of MAD-type SVMs.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2011 08:51:17 GMT"}], "update_date": "2011-11-09", "authors_parsed": [["Hable", "Robert", ""], ["Christmann", "Andreas", ""]]}, {"id": "1111.1957", "submitter": "Nial Friel", "authors": "Nial Friel and Jason Wyse", "title": "Estimating the evidence -- a review", "comments": "21 pages. To appear in a special issue, \"All Models Are Wrong...\", of\n  Statistica Neerlandica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model evidence is a vital quantity in the comparison of statistical\nmodels under the Bayesian paradigm. This paper presents a review of commonly\nused methods. We outline some guidelines and offer some practical advice. The\nreviewed methods are compared for two examples; non-nested Gaussian linear\nregression and covariate subset selection in logistic regression.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2011 16:28:05 GMT"}], "update_date": "2011-11-09", "authors_parsed": [["Friel", "Nial", ""], ["Wyse", "Jason", ""]]}, {"id": "1111.2113", "submitter": "Paul Kabaila", "authors": "Paul Kabaila and Khageswor Giri", "title": "Further properties of frequentist confidence intervals in regression\n  that utilize uncertain prior information", "comments": "The exposition has been improved", "journal-ref": "Further properties of frequentist confidence intervals in\n  regression that utilize uncertain prior information. Australian & New Zealand\n  Journal of Statistics, 55, 259-270 (2013)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a linear regression model with n-dimensional response vector,\nregression parameter \\beta = (\\beta_1, ..., \\beta_p) and independent and\nidentically N(0, \\sigma^2) distributed errors. Suppose that the parameter of\ninterest is \\theta = a^T \\beta where a is a specified vector. Define the\nparameter \\tau = c^T \\beta - t where c and t are specified. Also suppose that\nwe have uncertain prior information that \\tau = 0. Part of our evaluation of a\nfrequentist confidence interval for \\theta is the ratio (expected length of\nthis confidence interval)/(expected length of standard 1-\\alpha confidence\ninterval), which we call the scaled expected length of this interval. We say\nthat a 1-\\alpha confidence interval for \\theta utilizes this uncertain prior\ninformation if (a) the scaled expected length of this interval is significantly\nless than 1 when \\tau = 0, (b) the maximum value of the scaled expected length\nis not too much larger than 1 and (c) this confidence interval reverts to the\nstandard 1-\\alpha confidence interval when the data happen to strongly\ncontradict the prior information. Kabaila and Giri, 2009, JSPI present a new\nmethod for finding such a confidence interval. Let \\hat\\beta denote the least\nsquares estimator of \\beta. Also let \\hat\\Theta = a^T \\hat\\beta and \\hat\\tau =\nc^T \\hat\\beta - t. Using computations and new theoretical results, we show that\nthe performance of this confidence interval improves as |Corr(\\hat\\Theta,\n\\hat\\tau)| increases and n-p decreases.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2011 06:45:22 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2012 01:25:37 GMT"}, {"version": "v3", "created": "Mon, 2 Apr 2012 04:11:53 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Kabaila", "Paul", ""], ["Giri", "Khageswor", ""]]}, {"id": "1111.2255", "submitter": "Antonio Forcina", "authors": "Antonio Forcina", "title": "Modelling sources of ecological fallacy within a revised Brown and Payne\n  model of voting transitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model of voting behaviour based on a version of aggregated\noverdispersed multinomial distributions; relative to a similar model by\n\\citet{BP86}, our model is based on more realistic assumptions and free from\ncertain shortcomings of the previous model. We show that, within this model, it\nis possible to test for certain confounding effects due to observable\ncovariates measured at the aggregate level; such effects, if ignored, might\ncause substantial bias in the estimated relation between voting decisions in\ntwo close in time elections, a phenomenon known as {\\em Ecological Fallacy}. An\napplication to a referendum following an election for the major in the town of\nMilan, which was interpreted as a defeat for the Berlusconi gouvernment, is\nused as an illustration.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2011 16:15:08 GMT"}], "update_date": "2011-11-10", "authors_parsed": [["Forcina", "Antonio", ""]]}, {"id": "1111.2609", "submitter": "Jeong Eun  Lee Dr", "authors": "Jeong Lee, Kerrie Mengersen, Christian Robert, Ross McVinish", "title": "Issues in designing hybrid algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Bayesian community, an ongoing imperative is to develop efficient\nalgorithms. An appealing approach is to form a hybrid algorithm by combining\nideas from competing existing techniques. This paper addresses issues in\ndesigning hybrid methods by considering selected case studies: the delayed\nrejection algorithm, the pinball sampler, the Metropolis adjusted Langevin\nalgorithm, and the population Monte Carlo algorithm. We observe that even if\neach component of a hybrid algorithm has individual strengths, they may not\ncontribute equally or even positively when they are combined. Moreover, even if\nthe statistical efficiency is improved, from a practical perspective there are\ntechnical issues to be considered such as applicability and computational\nworkload. In order to optimize performance of the algorithm in real time, these\nissues should be taken into account.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2011 21:01:19 GMT"}], "update_date": "2011-11-14", "authors_parsed": [["Lee", "Jeong", ""], ["Mengersen", "Kerrie", ""], ["Robert", "Christian", ""], ["McVinish", "Ross", ""]]}, {"id": "1111.3988", "submitter": "Gane Lo samb", "authors": "Gane Samb Lo, El Hadji Deme", "title": "A functional Generalized Hill process and applications", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned in this paper with the functional asymptotic behaviour of\nthe sequence of stochastic processes T_{n}(f)=\\sum_{j=1}^{j=k}f(j)(\\log\nX_{n-j+1,n}-\\log X_{n-j,n}), indexed by some classes $\\mathcal{F}$ of functions\n$f:\\mathbb{N} \\backslash {0} \\longmapsto \\mathbb{R}_{+}$ and where $k=k(n)$\nsatisfies 1\\leq k\\leq n,k/n\\rightarrow 0\\text{as}n\\rightarrow \\infty. This is a\nfunctional generalized Hill process including as many new estimators of the\nextremal index when $F$ is in the extremal domain. We focus in this paper on\nits functional and uniform asymptotic law in the new setting of weak\nconvergence in the space of bounded real functions. The results are next\nparticularized for explicit examples of classes $\\mathcal{F} $.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2011 22:39:40 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Lo", "Gane Samb", ""], ["Deme", "El Hadji", ""]]}, {"id": "1111.4148", "submitter": "Surya Tokdar Surya Tokdar", "authors": "Surya T. Tokdar", "title": "Adaptive Convergence Rates of a Dirichlet Process Mixture of\n  Multivariate Normals", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that a simple Dirichlet process mixture of multivariate normals\noffers Bayesian density estimation with adaptive posterior convergence rates.\nToward this, a novel sieve for non-parametric mixture densities is explored,\nand its rate adaptability to various smoothness classes of densities in\narbitrary dimension is demonstrated. This sieve construction is expected to\noffer a substantial technical advancement in studying Bayesian non-parametric\nmixture models based on stick-breaking priors.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2011 16:58:39 GMT"}], "update_date": "2011-11-18", "authors_parsed": [["Tokdar", "Surya T.", ""]]}, {"id": "1111.4180", "submitter": "Axel Gandy", "authors": "Axel Gandy and Jan Terje Kval{\\o}y", "title": "Guaranteed Conditional Performance of Control Charts via Bootstrap\n  Methods", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": "10.1002/sjos.12006", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To use control charts in practice, the in-control state usually has to be\nestimated. This estimation has a detrimental effect on the performance of\ncontrol charts, which is often measured for example by the false alarm\nprobability or the average run length. We suggest an adjustment of the\nmonitoring schemes to overcome these problems. It guarantees, with a certain\nprobability, a conditional performance given the estimated in-control state.\nThe suggested method is based on bootstrapping the data used to estimate the\nin-control state. The method applies to different types of control charts, and\nalso works with charts based on regression models, survival models, etc. If a\nnonparametric bootstrap is used, the method is robust to model errors. We show\nlarge sample properties of the adjustment. The usefulness of our approach is\ndemonstrated through simulation studies.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2011 18:59:04 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Gandy", "Axel", ""], ["Kval\u00f8y", "Jan Terje", ""]]}, {"id": "1111.4226", "submitter": "Emily Fox", "authors": "Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, and Alan S. Willsky", "title": "Joint Modeling of Multiple Related Time Series via the Beta Process", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric approach to the problem of jointly\nmodeling multiple related time series. Our approach is based on the discovery\nof a set of latent, shared dynamical behaviors. Using a beta process prior, the\nsize of the set and the sharing pattern are both inferred from data. We develop\nefficient Markov chain Monte Carlo methods based on the Indian buffet process\nrepresentation of the predictive distribution of the beta process, without\nrelying on a truncated model. In particular, our approach uses the sum-product\nalgorithm to efficiently compute Metropolis-Hastings acceptance probabilities,\nand explores new dynamical behaviors via birth and death proposals. We examine\nthe benefits of our proposed feature-based model on several synthetic datasets,\nand also demonstrate promising results on unsupervised segmentation of visual\nmotion capture data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2011 21:28:04 GMT"}], "update_date": "2011-11-21", "authors_parsed": [["Fox", "Emily B.", ""], ["Sudderth", "Erik B.", ""], ["Jordan", "Michael I.", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1111.4296", "submitter": "Asohan Amarasingham", "authors": "Asohan Amarasingham, Matthew T. Harrison, Nicholas G. Hatsopoulos,\n  Stuart Geman", "title": "Conditional Modeling and the Jitter Method of Spike Re-sampling:\n  Supplement", "comments": "39 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report accompanies the manuscript \"Conditional Modeling and\nthe Jitter Method of Spike Re-sampling.\" It contains further details, comments,\nreferences, and equations concerning various simulations and data analyses\npresented in that manuscript, as well as a self-contained Mathematical Appendix\nthat provides a formal treatment of jitter-based spike re-sampling methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2011 08:04:32 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Amarasingham", "Asohan", ""], ["Harrison", "Matthew T.", ""], ["Hatsopoulos", "Nicholas G.", ""], ["Geman", "Stuart", ""]]}, {"id": "1111.4416", "submitter": "Zhou Fang", "authors": "Zhou Fang", "title": "Sparse Group Selection Through Co-Adaptive Penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has focused on the problem of conducting linear regression when\nthe number of covariates is very large, potentially greater than the sample\nsize. To facilitate this, one useful tool is to assume that the model can be\nwell approximated by a fit involving only a small number of covariates -- a so\ncalled sparsity assumption, which leads to the Lasso and other methods. In many\nsituations, however, the covariates can be considered to be structured, in that\nthe selection of some variables favours the selection of others -- with\nvariables organised into groups entering or leaving the model simultaneously as\na special case. This structure creates a different form of sparsity. In this\npaper, we suggest the Co-adaptive Lasso to fit models accommodating this form\nof `group sparsity'. The Co-adaptive Lasso is fast and simple to calculate, and\nwe show that it holds theoretical advantages over the Lasso, performs well\nunder a broad set of conclusions, and is very competitive in empirical\nsimulations in comparison with previously suggested algorithms like the Group\nLasso and the Adaptive Lasso.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2011 16:38:18 GMT"}], "update_date": "2011-11-21", "authors_parsed": [["Fang", "Zhou", ""]]}, {"id": "1111.4469", "submitter": "Gane Lo samb", "authors": "Gane Samb Lo and Adja Mbarka Fall", "title": "On the Pickands stochastic process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Pickands process {equation*} P_{n}(s)=\\log (1/s)^{-1}\\log\n\\frac{X_{n-k+1,n}-X_{n-[k/s]+1,n}}{% X_{n-[k/s]+1,n}-X_{n-[k/s^{2}]+1,n}},\n{equation*} {equation*} (\\frac{k}{n}\\leq s^2 \\leq 1), {equation*} which is a\ngeneralization of the classical Pickands estimate $P_{n}(1/2)$ of the extremal\nindex. We undertake here a purely stochastic process view for the asymptotic\ntheory of that process by using the\nCs\\\"{o}rg\\H{o}-Cs\\\"{o}rg\\H{o}-Horv\\'{a}th-Mason (1986) \\cite{cchm} weighted\napproximation of the empirical and quantile processes to suitable Brownian\nbridges. This leads to the uniform convergence of the margins of this process\nto the extremal index and a complete theory of weak convergence of $P_n$ in\n$\\ell^{\\infty}([a,b])$ to some Gaussian process $$\\{\\mathbb{G},a\\leq s \\leq b\\}\n$$ for all $[a,b] \\subset]0,1[$. This frame greatly simplifies the former\nresults and enable applications based on stochastic processes methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2011 20:23:12 GMT"}], "update_date": "2011-11-21", "authors_parsed": [["Lo", "Gane Samb", ""], ["Fall", "Adja Mbarka", ""]]}, {"id": "1111.4494", "submitter": "Daniel Percival", "authors": "Daniel Percival", "title": "Structured Sparse Aggregation", "comments": "34 Pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for aggregating many least squares estimator so that\nthe resulting estimate has two properties: sparsity and structure. That is,\nonly a few candidate covariates are used in the resulting model, and the\nselected covariates follow some structure over the candidate covariates that is\nassumed to be known a priori. While sparsity is well studied in many settings,\nincluding aggregation, structured sparse methods are still emerging. We\ndemonstrate a general framework for structured sparse aggregation that allows\nfor a wide variety of structures, including overlapping grouped structures and\ngeneral structural penalties defined as set functions on the set of covariates.\nWe show that such estimators satisfy structured sparse oracle inequalities ---\ntheir finite sample risk adapts to the structured sparsity of the target. These\ninequalities reveal that under suitable settings, the structured sparse\nestimator performs at least as well as, and potentially much better than, a\nsparse aggregation estimator. We empirically establish the effectiveness of the\nmethod using simulation and an application to HIV drug resistance.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2011 21:29:57 GMT"}], "update_date": "2011-11-22", "authors_parsed": [["Percival", "Daniel", ""]]}, {"id": "1111.4564", "submitter": "Gane  Samb Lo", "authors": "Gane Samb Lo, El Hadji Deme and Aliou Diop", "title": "On the Generalized Hill Process for Small Parameters and Applications", "comments": "19 pages; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_{1},X_{2},...$ be a sequence of independent copies (s.i.c) of a real\nrandom variable (r.v.) $X\\geq 1$, with distribution function $df$\n$F(x)=\\mathbb{P}% (X\\leq x)$ and let $X_{1,n}\\leq X_{2,n} \\leq ... \\leq\nX_{n,n}$ be the order statistics based on the $n\\geq 1$ first of these\nobservations. The following continuous generalized Hill process {equation*}\nT_{n}(\\tau)=k^{-\\tau}\\sum_{j=1}^{j=k}j^{\\tau}(\\log X_{n-j+1,n}-\\log X_{n-j,n}),\n\\label{dl02} {equation*} $\\tau >0$, $1\\leq k \\leq n$, has been introduced as a\ncontinuous family of estimators of the extreme value index, and largely studied\nfor statistical purposes with asymptotic normality results restricted to $\\tau\n> 1/2$. We extend those results to $0 < \\tau \\leq 1/2$ and show that asymptotic\nnormality is still valid for $\\tau=1/2$. For $0 < \\tau <1/2$, we get non\nGaussian asymptotic laws which are closely related to the Riemann function $%\n\\zeta(s)=\\sum_{n=1}^{\\infty} n^{-s},s>1$\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2011 15:06:25 GMT"}], "update_date": "2011-11-22", "authors_parsed": [["Lo", "Gane Samb", ""], ["Deme", "El Hadji", ""], ["Diop", "Aliou", ""]]}, {"id": "1111.4639", "submitter": "Leo Lahti", "authors": "Leo Lahti, Martin Sch\\\"afer, Hans-Ulrich Klein, Silvio Bicciato, and\n  Martin Dugas", "title": "Cancer gene prioritization by integrative analysis of mRNA expression\n  and DNA copy number data: a comparative review", "comments": "PDF file including supplementary material. 9 pages. Preprint", "journal-ref": null, "doi": "10.1093/bib/bbs005", "report-no": null, "categories": "cs.CE q-bio.GN stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A variety of genome-wide profiling techniques are available to probe\ncomplementary aspects of genome structure and function. Integrative analysis of\nheterogeneous data sources can reveal higher-level interactions that cannot be\ndetected based on individual observations. A standard integration task in\ncancer studies is to identify altered genomic regions that induce changes in\nthe expression of the associated genes based on joint analysis of genome-wide\ngene expression and copy number profiling measurements. In this review, we\nprovide a comparison among various modeling procedures for integrating\ngenome-wide profiling data of gene copy number and transcriptional alterations\nand highlight common approaches to genomic data integration. A transparent\nbenchmarking procedure is introduced to quantitatively compare the cancer gene\nprioritization performance of the alternative methods. The benchmarking\nalgorithms and data sets are available at http://intcomp.r-forge.r-project.org\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2011 15:23:09 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Lahti", "Leo", ""], ["Sch\u00e4fer", "Martin", ""], ["Klein", "Hans-Ulrich", ""], ["Bicciato", "Silvio", ""], ["Dugas", "Martin", ""]]}, {"id": "1111.4954", "submitter": "Forrest Crawford", "authors": "Forrest W. Crawford and Vladimir N. Minin and Marc A. Suchard", "title": "Estimation for general birth-death processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Birth-death processes (BDPs) are continuous-time Markov chains that track the\nnumber of \"particles\" in a system over time. While widely used in population\nbiology, genetics and ecology, statistical inference of the instantaneous\nparticle birth and death rates remains largely limited to restrictive linear\nBDPs in which per-particle birth and death rates are constant. Researchers\noften observe the number of particles at discrete times, necessitating data\naugmentation procedures such as expectation-maximization (EM) to find maximum\nlikelihood estimates. The E-step in the EM algorithm is available in\nclosed-form for some linear BDPs, but otherwise previous work has resorted to\napproximation or simulation. Remarkably, the E-step conditional expectations\ncan also be expressed as convolutions of computable transition probabilities\nfor any general BDP with arbitrary rates. This important observation, along\nwith a convenient continued fraction representation of the Laplace transforms\nof the transition probabilities, allows novel and efficient computation of the\nconditional expectations for all BDPs, eliminating the need for approximation\nor costly simulation. We use this insight to derive EM algorithms that yield\nmaximum likelihood estimation for general BDPs characterized by various rate\nmodels, including generalized linear models. We show that our Laplace\nconvolution technique outperforms competing methods when available and\ndemonstrate a technique to accelerate EM algorithm convergence. Finally, we\nvalidate our approach using synthetic data and then apply our methods to\nestimation of mutation parameters in microsatellite evolution.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2011 17:55:48 GMT"}], "update_date": "2011-11-22", "authors_parsed": [["Crawford", "Forrest W.", ""], ["Minin", "Vladimir N.", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1111.5028", "submitter": "Shuang Li", "authors": "Shuang Li, Li Hsu, Jie Peng, Pei Wang", "title": "Bootstrap inference for network construction with an application to a\n  breast cancer microarray study", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS589 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 391-417", "doi": "10.1214/12-AOAS589", "report-no": "IMS-AOAS-AOAS589", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Graphical Models (GGMs) have been used to construct genetic\nregulatory networks where regularization techniques are widely used since the\nnetwork inference usually falls into a high-dimension-low-sample-size scenario.\nYet, finding the right amount of regularization can be challenging, especially\nin an unsupervised setting where traditional methods such as BIC or\ncross-validation often do not work well. In this paper, we propose a new method\n- Bootstrap Inference for Network COnstruction (BINCO) - to infer networks by\ndirectly controlling the false discovery rates (FDRs) of the selected edges.\nThis method fits a mixture model for the distribution of edge selection\nfrequencies to estimate the FDRs, where the selection frequencies are\ncalculated via model aggregation. This method is applicable to a wide range of\napplications beyond network construction. When we applied our proposed method\nto building a gene regulatory network with microarray expression breast cancer\ndata, we were able to identify high-confidence edges and well-connected hub\ngenes that could potentially play important roles in understanding the\nunderlying biological processes of breast cancer.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2011 21:11:21 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2013 12:34:50 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Li", "Shuang", ""], ["Hsu", "Li", ""], ["Peng", "Jie", ""], ["Wang", "Pei", ""]]}, {"id": "1111.5110", "submitter": "Ting Yan", "authors": "Ting Yan, Jinfeng Xu and Yaning Yang", "title": "Grouped sparse paired comparisons in the Bradley-Terry model", "comments": "This paper has been withdrawn by the author due to that it needs to\n  be revised greatly", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide class of paired comparisons, especially in the sports games, in\nwhich all subjects are divided into several groups, the intragroup comparisons\nare dense and the intergroup comparisons are sparse. Typical examples include\nthe NFL regular season. Motivated by these situations, we propose group\nsparsity for paired comparisons and show the consistency and asymptotical\nnormality of the maximum likelihood estimate in the Bradley-Terry model when\nthe number of parameters goes to infinity in this paper. Simulations are\ncarried out to illustrate the group sparsity and asymptotical results.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2011 06:12:16 GMT"}, {"version": "v2", "created": "Wed, 16 May 2012 08:30:16 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2012 10:46:55 GMT"}], "update_date": "2012-06-11", "authors_parsed": [["Yan", "Ting", ""], ["Xu", "Jinfeng", ""], ["Yang", "Yaning", ""]]}, {"id": "1111.5421", "submitter": "Christophe Andrieu", "authors": "Christophe Andrieu, Matti Vihola", "title": "Markovian stochastic approximation with expanding projections", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJ497 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2014, Vol. 20, No. 2, 545-585", "doi": "10.3150/12-BEJ497", "report-no": "IMS-BEJ-BEJ497", "categories": "math.PR stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic approximation is a framework unifying many random iterative\nalgorithms occurring in a diverse range of applications. The stability of the\nprocess is often difficult to verify in practical applications and the process\nmay even be unstable without additional stabilisation techniques. We study a\nstochastic approximation procedure with expanding projections similar to\nAndrad\\'{o}ttir [Oper. Res. 43 (1995) 1037-1048]. We focus on Markovian noise\nand show the stability and convergence under general conditions. Our framework\nalso incorporates the possibility to use a random step size sequence, which\nallows us to consider settings with a non-smooth family of Markov kernels. We\napply the theory to stochastic approximation expectation maximisation with\nparticle independent Metropolis-Hastings sampling.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2011 07:31:58 GMT"}, {"version": "v2", "created": "Fri, 7 Mar 2014 10:14:28 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Andrieu", "Christophe", ""], ["Vihola", "Matti", ""]]}, {"id": "1111.5827", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universite Paris-Dauphine, IUF, and CREST)", "title": "Error and Inference: an outsider stand on a frequentist philosophy", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is an extended review of the book Error and Inference, edited by\nDeborah Mayo and Aris Spanos, about their frequentist and philosophical\nperspective on testing of hypothesis and on the criticisms of alternatives like\nthe Bayesian approach.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2011 18:03:37 GMT"}], "update_date": "2011-11-28", "authors_parsed": [["Robert", "Christian P.", "", "Universite Paris-Dauphine, IUF, and CREST"]]}, {"id": "1111.5989", "submitter": "Djamal Louani DL", "authors": "Djamal Louani and Sidi Mohamed Ould Maouloud", "title": "Large Deviation Results for the Nonparametric Regression Function\n  Estimator on Functional Data", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the study of large deviation behaviors in the\nsetting of the estimation of the regression function on functional data. A\nlarge deviation principle is stated for a process Zn, defined below, allowing\nto derive a pointwise large deviation principle for the Nadaraya-Watson-type\nl-indexed regression function estimator as a by-product. Moreover, a uniform\nover VC-classes Cherno? type large deviation result is stated for the deviation\nof the l-indexed regression estimator.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2011 14:06:11 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Louani", "Djamal", ""], ["Maouloud", "Sidi Mohamed Ould", ""]]}, {"id": "1111.6085", "submitter": "Vincent Tan", "authors": "Vincent Y. F. Tan and C\\'edric F\\'evotte", "title": "Automatic Relevance Determination in Nonnegative Matrix Factorization\n  with the \\beta-Divergence", "comments": "Accepted by the IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the estimation of the latent dimensionality in\nnonnegative matrix factorization (NMF) with the \\beta-divergence. The\n\\beta-divergence is a family of cost functions that includes the squared\nEuclidean distance, Kullback-Leibler and Itakura-Saito divergences as special\ncases. Learning the model order is important as it is necessary to strike the\nright balance between data fidelity and overfitting. We propose a Bayesian\nmodel based on automatic relevance determination in which the columns of the\ndictionary matrix and the rows of the activation matrix are tied together\nthrough a common scale parameter in their prior. A family of\nmajorization-minimization algorithms is proposed for maximum a posteriori (MAP)\nestimation. A subset of scale parameters is driven to a small lower bound in\nthe course of inference, with the effect of pruning the corresponding spurious\ncomponents. We demonstrate the efficacy and robustness of our algorithms by\nperforming extensive experiments on synthetic data, the swimmer dataset, a\nmusic decomposition example and a stock price prediction task.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2011 19:03:21 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2012 09:21:21 GMT"}, {"version": "v3", "created": "Fri, 5 Oct 2012 11:39:35 GMT"}], "update_date": "2012-10-08", "authors_parsed": [["Tan", "Vincent Y. F.", ""], ["F\u00e9votte", "C\u00e9dric", ""]]}, {"id": "1111.6174", "submitter": "David R. Bickel", "authors": "David R. Bickel", "title": "Resolving conflicts between statistical methods by probability\n  combination: Application to empirical Bayes analyses of genomic data", "comments": null, "journal-ref": "D. R. Bickel, Game-theoretic probability combination with\n  applications to resolving conflicts between statistical methods,\n  International Journal of Approximate Reasoning 53, 880-891 (2012)", "doi": "10.1016/j.ijar.2012.04.002", "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the typical analysis of a data set, a single method is selected for\nstatistical reporting even when equally applicable methods yield very different\nresults. Examples of equally applicable methods can correspond to those of\ndifferent ancillary statistics in frequentist inference and of different prior\ndistributions in Bayesian inference. More broadly, choices are made between\nparametric and nonparametric methods and between frequentist and Bayesian\nmethods.\n  Rather than choosing a single method, it can be safer, in a game-theoretic\nsense, to combine those that are equally appropriate in light of the available\ninformation. Since methods of combining subjectively assessed probability\ndistributions are not objective enough for that purpose, this paper introduces\na method of distribution combination that does not require any assignment of\ndistribution weights. It does so by formalizing a hedging strategy in terms of\na game between three players: nature, a statistician combining distributions,\nand a statistician refusing to combine distributions. The optimal move of the\nfirst statistician reduces to the solution of a simpler problem of selecting an\nestimating distribution that minimizes the Kullback-Leibler loss maximized over\nthe plausible distributions to be combined. The resulting combined distribution\nis a linear combination of the most extreme of the distributions to be combined\nthat are scientifically plausible. The optimal weights are close enough to each\nother that no extreme distribution dominates the others.\n  The new methodology is illustrated by combining conflicting empirical Bayes\nmethodologies in the context of gene expression data analysis.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2011 16:31:56 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Bickel", "David R.", ""]]}, {"id": "1111.6291", "submitter": "Yining Chen", "authors": "Yining Chen", "title": "Semiparametric Time Series Models with Log-concave Innovations: Maximum\n  Likelihood Estimation and its Consistency", "comments": "38 pages, 4 figures", "journal-ref": "Scandinavian Journal of Statistics, 2015, 42(1), 1-31", "doi": "10.1111/sjos.12092", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study semiparametric time series models with innovations following a\nlog-concave distribution. We propose a general maximum likelihood framework\nwhich allows us to estimate simultaneously the parameters of the model and the\ndensity of the innovations. This framework can be easily adapted to many\nwell-known models, including ARMA, GARCH and ARMA-GARCH. Furthermore, we show\nthat the estimator under our new framework is consistent in both ARMA and\nARMA-GARCH settings. We demonstrate its finite sample performance via a\nthorough simulation study and apply it to model the daily log-return of FTSE\n100 index and the rabbit population.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2011 20:20:15 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2012 18:00:15 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2012 22:31:12 GMT"}, {"version": "v4", "created": "Wed, 10 Jul 2013 23:36:41 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Chen", "Yining", ""]]}, {"id": "1111.6308", "submitter": "Koby Todros", "authors": "Koby Todros and Alfred O. Hero", "title": "On Measure Transformed Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2012.2203816", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper linear canonical correlation analysis (LCCA) is generalized by\napplying a structured transform to the joint probability distribution of the\nconsidered pair of random vectors, i.e., a transformation of the joint\nprobability measure defined on their joint observation space. This framework,\ncalled measure transformed canonical correlation analysis (MTCCA), applies LCCA\nto the data after transformation of the joint probability measure. We show that\njudicious choice of the transform leads to a modified canonical correlation\nanalysis, which, in contrast to LCCA, is capable of detecting non-linear\nrelationships between the considered pair of random vectors. Unlike kernel\ncanonical correlation analysis, where the transformation is applied to the\nrandom vectors, in MTCCA the transformation is applied to their joint\nprobability distribution. This results in performance advantages and reduced\nimplementation complexity. The proposed approach is illustrated for graphical\nmodel selection in simulated data having non-linear dependencies, and for\nmeasuring long-term associations between companies traded in the NASDAQ and\nNYSE stock markets.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2011 22:25:36 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2011 15:02:10 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2012 17:06:52 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Todros", "Koby", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1111.6899", "submitter": "Ioannis Papastathopoulos", "authors": "Ioannis Papastathopoulos and Jonathan A. Tawn", "title": "Extended Generalised Pareto Models for Tail Estimation", "comments": "18 pages, 7 figures", "journal-ref": "J. Statist. Plann. and Inf., (2013), 143", "doi": "10.1016/j.jspi.2012.07.001", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most popular approach in extreme value statistics is the modelling of\nthreshold exceedances using the asymptotically motivated generalised Pareto\ndistribution. This approach involves the selection of a high threshold above\nwhich the model fits the data well. Sometimes, few observations of a\nmeasurement process might be recorded in applications and so selecting a high\nquantile of the sample as the threshold leads to almost no exceedances. In this\npaper we propose extensions of the generalised Pareto distribution that\nincorporate an additional shape parameter while keeping the tail behaviour\nunaffected. The inclusion of this parameter offers additional structure for the\nmain body of the distribution, improves the stability of the modified scale,\ntail index and return level estimates to threshold choice and allows a lower\nthreshold to be selected. We illustrate the benefits of the proposed models\nwith a simulation study and two case studies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 17:04:26 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Papastathopoulos", "Ioannis", ""], ["Tawn", "Jonathan A.", ""]]}]