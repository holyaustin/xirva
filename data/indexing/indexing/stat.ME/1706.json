[{"id": "1706.00103", "submitter": "Shan-Yu Liu", "authors": "Hsieh Fushing, Shan-Yu Liu, Yin-Chen Hsieh, Brenda McCowan", "title": "From patterned response dependency to structured covariate dependency:\n  categorical-pattern-matching", "comments": "32 pages, 10 figures, 3 box pictures", "journal-ref": null, "doi": "10.1371/journal.pone.0198253", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data generated from a system of interest typically consists of measurements\nfrom an ensemble of subjects across multiple response and covariate features,\nand is naturally represented by one response-matrix against one\ncovariate-matrix. Likely each of these two matrices simultaneously embraces\nheterogeneous data types: continuous, discrete and categorical. Here a matrix\nis used as a practical platform to ideally keep hidden dependency among/between\nsubjects and features intact on its lattice. Response and covariate dependency\nis individually computed and expressed through mutliscale blocks via a newly\ndeveloped computing paradigm named Data Mechanics. We propose a categorical\npattern matching approach to establish causal linkages in a form of information\nflows from patterned response dependency to structured covariate dependency.\nThe strength of an information flow is evaluated by applying the combinatorial\ninformation theory. This unified platform for system knowledge discovery is\nillustrated through five data sets. In each illustrative case, an information\nflow is demonstrated as an organization of discovered knowledge loci via\nemergent visible and readable heterogeneity. This unified approach\nfundamentally resolves many long standing issues, including statistical\nmodeling, multiple response, renormalization and feature selections, in data\nanalysis, but without involving man-made structures and distribution\nassumptions. The results reported here enhance the idea that linking patterns\nof response dependency to structures of covariate dependency is the true\nphilosophical foundation underlying data-driven computing and learning in\nsciences.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 21:43:36 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Fushing", "Hsieh", ""], ["Liu", "Shan-Yu", ""], ["Hsieh", "Yin-Chen", ""], ["McCowan", "Brenda", ""]]}, {"id": "1706.00204", "submitter": "Mathieu Carri\\`ere", "authors": "Mathieu Carri\\`ere and Bertrand Michel and Steve Oudot", "title": "Statistical Analysis and Parameter Selection for Mapper", "comments": "Minor modifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study the question of the statistical convergence of the\n1-dimensional Mapper to its continuous analogue, the Reeb graph. We show that\nthe Mapper is an optimal estimator of the Reeb graph, which gives, as a\nbyproduct, a method to automatically tune its parameters and compute confidence\nregions on its topological features, such as its loops and flares. This allows\nto circumvent the issue of testing a large grid of parameters and keeping the\nmost stable ones in the brute-force setting, which is widely used in\nvisualization, clustering and feature selection with the Mapper.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 08:34:01 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 13:36:23 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Carri\u00e8re", "Mathieu", ""], ["Michel", "Bertrand", ""], ["Oudot", "Steve", ""]]}, {"id": "1706.00473", "submitter": "Vadim Sokolov", "authors": "Nicholas Polson and Vadim Sokolov", "title": "Deep Learning: A Bayesian Perspective", "comments": null, "journal-ref": null, "doi": "10.1214/17-BA1082", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a form of machine learning for nonlinear high dimensional\npattern matching and prediction. By taking a Bayesian probabilistic\nperspective, we provide a number of insights into more efficient algorithms for\noptimisation and hyper-parameter tuning. Traditional high-dimensional data\nreduction techniques, such as principal component analysis (PCA), partial least\nsquares (PLS), reduced rank regression (RRR), projection pursuit regression\n(PPR) are all shown to be shallow learners. Their deep learning counterparts\nexploit multiple deep layers of data reduction which provide predictive\nperformance gains. Stochastic gradient descent (SGD) training optimisation and\nDropout (DO) regularization provide estimation and variable selection. Bayesian\nregularization is central to finding weights and connections in networks to\noptimize the predictive bias-variance trade-off. To illustrate our methodology,\nwe provide an analysis of international bookings on Airbnb. Finally, we\nconclude with directions for future research.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 19:50:37 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 00:57:42 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 01:33:09 GMT"}, {"version": "v4", "created": "Tue, 14 Nov 2017 03:36:51 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Polson", "Nicholas", ""], ["Sokolov", "Vadim", ""]]}, {"id": "1706.00599", "submitter": "Fabrizio Leisen", "authors": "Fabrizio Leisen, Cristiano Villa, Stephen G. Walker", "title": "On a Class of Objective Priors from Scoring Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective prior distributions represent an important tool that allows one to\nhave the advantages of using the Bayesian framework even when information about\nthe parameters of a model is not available. The usual objective approaches work\noff the chosen statistical model and in the majority of cases the resulting\nprior is improper, which can pose limitations to a practical implementation,\neven when the complexity of the model is moderate. In this paper we propose to\ntake a novel look at the construction of objective prior distributions, where\nthe connection with a chosen sampling distribution model is removed. We explore\nthe notion of defining objective prior distributions which allow one to have\nsome degree of flexibility, in particular in exhibiting some desirable\nfeatures, such as being proper, or centered on specific values which would be\nof interest in nested model comparisons. The basic tool we use are proper\nscoring rules and the main result is a class of objective prior distributions\nthat can be employed in scenarios where the usual model based priors fail, such\nas mixture models and model selection via Bayes factors. In addition, we show\nthat the proposed class of priors is the result of minimising the information\nit contains, providing solid interpretation to the method.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 09:17:54 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 19:53:43 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Leisen", "Fabrizio", ""], ["Villa", "Cristiano", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1706.00853", "submitter": "Ning Dai", "authors": "Ning Dai, Galin L. Jones", "title": "Multivariate initial sequence estimators in Markov chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is a simulation method commonly used for\nestimating expectations with respect to a given distribution. We consider\nestimating the covariance matrix of the asymptotic multivariate normal\ndistribution of a vector of sample means. Geyer (1992) developed a Monte Carlo\nerror estimation method for estimating a univariate mean. We propose a novel\nmultivariate version of Geyer's method that provides an asymptotically valid\nestimator for the covariance matrix and results in stable Monte Carlo\nestimates. The finite sample properties of the proposed method are investigated\nvia simulation experiments.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 21:05:23 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Dai", "Ning", ""], ["Jones", "Galin L.", ""]]}, {"id": "1706.00865", "submitter": "Ning Dai", "authors": "Ning Dai", "title": "Inference for penalized spline regression: Improving confidence\n  intervals by reducing the penalty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized spline regression is a popular method for scatterplot smoothing,\nbut there has long been a debate on how to construct confidence intervals for\npenalized spline fits. Due to the penalty, the fitted smooth curve is a biased\nestimate of the target function. Many methods, including Bayesian intervals and\nthe simple-shift bias-reduction, have been proposed to upgrade the coverage of\nthe confidence intervals, but these methods usually fail to adequately improve\nthe situation at predictor values where the function is sharply curved. In this\npaper, we develop a novel approach to improving the confidence intervals by\nusing a smaller smoothing strength than that of the spline fits. With a\ncarefully selected amount of reduction in smoothing strength, the confidence\nintervals achieve nearly nominal coverage without being excessively wide or\nwiggly. The coverage performance of the proposed method is investigated via\nsimulation experiments in comparison with the bias-correction techniques\nproposed by Hodges (2013) and Kuusela and Panaretos (2015).\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 22:13:48 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Dai", "Ning", ""]]}, {"id": "1706.01083", "submitter": "Richard Darlington", "authors": "Richard B. Darlington", "title": "In elections, irrelevant alternatives provide relevant data", "comments": "8 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electoral criterion of independence of irrelevant alternatives, or IIA,\nstates that a voting system is unacceptable if it would choose a different\nwinner if votes were recounted after one of the losers had dropped out. But IIA\nconfuses the candidate who withdrew with the data which was generated by that\ncandidate. This paper reports a wide variety of simulation studies which\nconsistently show that data from dropout candidates can be very useful in\nchoosing the best of the remaining candidates. These studies use well-validated\nspatial models in which the most centrist candidates are considered to be the\nbest candidates. Thus IIA should be abandoned. The majority judgment or MJ\nvoting system was created specifically to satisfy IIA. Some of these studies\nalso show the substantial inferiority of MJ to other voting systems.\nDiscussions of IIA have usually treated dropouts as strictly hypothetical, but\nour conclusions about the usefulness of dropout data may apply even to real\ndropouts.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 14:43:52 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Darlington", "Richard B.", ""]]}, {"id": "1706.01155", "submitter": "Haeran Cho Dr", "authors": "Haeran Cho, Karolos Korkas", "title": "High-dimensional GARCH process segmentation with an application to\n  Value-at-Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for financial risk often assume that underlying asset returns are\nstationary. However, there is strong evidence that multivariate financial time\nseries entail changes not only in their within-series dependence structure, but\nalso in the cross-sectional dependence among them. In particular, the stressed\nValue-at-Risk of a portfolio, a popularly adopted measure of market risk,\ncannot be gauged adequately unless such structural breaks are taken into\naccount in its estimation. We propose a method for consistent detection of\nmultiple change points in high-dimensional GARCH panel data set where both\nindividual GARCH processes and their correlations are allowed to change over\ntime. We prove its consistency in multiple change point estimation, and\ndemonstrate its good performance through simulation studies and an application\nto the Value-at-Risk problem on a real dataset. Our methodology is implemented\nin the R package segMGarch, available from CRAN.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 22:20:22 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 09:42:10 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2020 20:06:45 GMT"}, {"version": "v4", "created": "Tue, 2 Mar 2021 17:26:26 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Cho", "Haeran", ""], ["Korkas", "Karolos", ""]]}, {"id": "1706.01240", "submitter": "Guanhua Fang", "authors": "Guanhua Fang, Jingchen Liu, and Zhiliang Ying", "title": "On the Identifiability of Diagnostic Classification Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes fundamental results for statistical inference of\ndiagnostic classification models (DCM). The results are developed at a high\nlevel of generality, applicable to essentially all diagnostic classification\nmodels. In particular, we establish identifiability results of various modeling\nparameters, notably item response probabilities, attribute distribution, and\nQ-matrix-induced partial information structure. Consistent estimators are\nconstructed. Simulation results show that these estimators perform well under\nvarious modeling settings. We also use a real example to illustrate the new\nmethod. The results are stated under the setting of general latent class\nmodels. For DCM with a specific parameterization, the conditions may be adapted\naccordingly.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 08:56:25 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Fang", "Guanhua", ""], ["Liu", "Jingchen", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1706.01252", "submitter": "Takeru Matsuda", "authors": "Takeru Matsuda and Fumiyasu Komaki", "title": "Empirical Bayes Matrix Completion", "comments": "15 pages", "journal-ref": "Computational Statistics & Data Analysis, Vol. 137, pp. 195--210,\n  2019", "doi": "10.1016/j.csda.2019.02.006", "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an empirical Bayes (EB) algorithm for the matrix completion\nproblems. The EB algorithm is motivated from the singular value shrinkage\nestimator for matrix means by Efron and Morris (1972). Since the EB algorithm\nis essentially the EM algorithm applied to a simple model, it does not require\nheuristic parameter tuning other than tolerance. Numerical results demonstrated\nthat the EB algorithm achieves a good trade-off between accuracy and efficiency\ncompared to existing algorithms and that it works particularly well when the\ndifference between the number of rows and columns is large. Application to real\ndata also shows the practical utility of the EB algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 09:50:59 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 10:42:32 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Matsuda", "Takeru", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "1706.01389", "submitter": "Sai Li", "authors": "Sai Li", "title": "Mendelian Randomization when Many Instruments are Invalid: Hierarchical\n  Empirical Bayes Estimation", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the causal effect of an exposure on an outcome is an important\ntask in many economical and biological studies. Mendelian randomization, in\nparticular, uses genetic variants as instruments to estimate causal effects in\nepidemiological studies. However, conventional instrumental variable methods\nrely on some untestable assumptions, which may be violated in real problems. In\nthis paper, we adopt a Bayesian framework and build hierarchical models to\nincorporate invalid effects of instruments. We introduce an empirical Bayes\nestimator for which some of the instruments are invalid by utilizing a Gaussian\nmixture prior. Theoretical performance and algorithm implementations are\nprovided and illustrated. The reliable performance of the proposed method is\ndemonstrated in various simulation settings and on real datasets concerning the\ncausal effects of HDL cholesterol and LDL cholesterol on type 2 diabetes.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 16:02:45 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Li", "Sai", ""]]}, {"id": "1706.01426", "submitter": "Jingxiang Chen", "authors": "Jingxiang Chen, Chong Zhang, Michael R. Kosorok, and Yufeng Liu", "title": "Double Sparsity Kernel Learning with Automatic Variable Selection and\n  Data Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with Reproducing Kernel Hilbert Spaces (RKHS) has been widely used\nin many scientific disciplines. Because a RKHS can be very flexible, it is\ncommon to impose a regularization term in the optimization to prevent\noverfitting. Standard RKHS learning employs the squared norm penalty of the\nlearning function. Despite its success, many challenges remain. In particular,\none cannot directly use the squared norm penalty for variable selection or data\nextraction. Therefore, when there exists noise predictors, or the underlying\nfunction has a sparse representation in the dual space, the performance of\nstandard RKHS learning can be suboptimal. In the literature,work has been\nproposed on how to perform variable selection in RKHS learning, and a data\nsparsity constraint was considered for data extraction. However, how to learn\nin a RKHS with both variable selection and data extraction simultaneously\nremains unclear. In this paper, we propose a unified RKHS learning method,\nnamely, DOuble Sparsity Kernel (DOSK) learning, to overcome this challenge. An\nefficient algorithm is provided to solve the corresponding optimization\nproblem. We prove that under certain conditions, our new method can\nasymptotically achieve variable selection consistency. Simulated and real data\nresults demonstrate that DOSK is highly competitive among existing approaches\nfor RKHS learning.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 17:15:37 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Chen", "Jingxiang", ""], ["Zhang", "Chong", ""], ["Kosorok", "Michael R.", ""], ["Liu", "Yufeng", ""]]}, {"id": "1706.01507", "submitter": "Cornelis Potgieter", "authors": "Cornelis J. Potgieter", "title": "Density Deconvolution for Generalized Skew-Symmetric Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a density deconvolution estimator that assumes the\ndensity of interest is a member of the generalized skew-symmetric (GSS) family\nof distributions. Estimation occurs in two parts: a skewing function, as well\nas location and scale parameters must be estimated. A kernel method is proposed\nfor estimating the skewing function. The mean integrated square error (MISE) of\nthe resulting GSS deconvolution estimator is derived. Based on derivation of\nthe MISE, two bandwidth estimation methods for estimating the skewing function\nare also proposed. A generalized method of moments (GMM) approach is developed\nfor estimation of the location and scale parameters. The question of multiple\nsolutions in applying the GMM is also considered, and two solution selection\ncriteria are proposed. The GSS deconvolution estimator is further investigated\nin simulation studies and is compared to the nonparametric deconvolution\nestimator. For most simulation settings considered, the GSS estimator has\nperformance superior to the nonparametric estimator.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 19:19:50 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Potgieter", "Cornelis J.", ""]]}, {"id": "1706.01520", "submitter": "Michael Betancourt", "authors": "Michael Betancourt", "title": "The Convergence of Markov chain Monte Carlo Methods: From the Metropolis\n  method to Hamiltonian Monte Carlo", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.hist-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From its inception in the 1950s to the modern frontiers of applied\nstatistics, Markov chain Monte Carlo has been one of the most ubiquitous and\nsuccessful methods in statistical computing. In that time its development has\nbeen fueled by increasingly difficult problems and novel techniques from\nphysics. In this article I will review the history of Markov chain Monte Carlo\nfrom its inception with the Metropolis method to today's state-of-the-art in\nHamiltonian Monte Carlo. Along the way I will focus on the evolving interplay\nbetween the statistical and physical perspectives of the method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 20:02:04 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 16:07:35 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Betancourt", "Michael", ""]]}, {"id": "1706.01752", "submitter": "Erlis Ruli", "authors": "Erlis Ruli, Nicola Sartori and Laura Ventura", "title": "Robust approximate Bayesian inference", "comments": "This is a revised and personal manuscript version of the article that\n  has been accepted for publication by Journal of Statistical Planning and\n  Inference", "journal-ref": null, "doi": "10.1016/j.jspi.2019.05.006", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss an approach for deriving robust posterior distributions from\n$M$-estimating functions using Approximate Bayesian Computation (ABC) methods.\nIn particular, we use $M$-estimating functions to construct suitable summary\nstatistics in ABC algorithms. The theoretical properties of the robust\nposterior distributions are discussed. Special attention is given to the\napplication of the method to linear mixed models. Simulation results and an\napplication to a clinical study demonstrate the usefulness of the method. An R\nimplementation is also provided in the robustBLME package.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 13:29:43 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 07:30:27 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 09:21:09 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Ruli", "Erlis", ""], ["Sartori", "Nicola", ""], ["Ventura", "Laura", ""]]}, {"id": "1706.01841", "submitter": "Richard Darlington", "authors": "Richard B. Darlington", "title": "Why Condorcet Consistency is Essential", "comments": "8 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a single winner election with several candidates and ranked choice or\nrating scale ballots, a Condorcet winner is one who wins all their two way\nraces by majority rule or MR. A voting system has Condorcet consistency or CC\nif it names any Condorcet winner the winner. Many voting systems lack CC, but a\nthree step line of reasoning is used here to show why it is necessary. In step\n1 we show that we can dismiss all the electoral criteria which conflict with\nCC. In step 2 we point out that CC follows almost automatically if we can agree\nthat MR is the only acceptable system for elections with two candidates. In\nstep 3 we make that argument for MR. This argument itself has three parts.\nFirst, in races with two candidates, the only well known alternatives to MR can\nsometimes name as winner a candidate who is preferred over their opponent by\nonly one voter, with all others preferring the opponent. That is unacceptable.\nSecond, those same systems are also extremely susceptible to strategic\ninsincere voting. Third, in simulation studies using spatial models with two\ncandidates, the best known alternative to MR picks the best or most centrist\ncandidate significantly less often than MR does.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 16:18:00 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Darlington", "Richard B.", ""]]}, {"id": "1706.01954", "submitter": "Tomaso Aste", "authors": "Tomaso Aste and T. Di Matteo", "title": "Sparse causality network retrieval from short time series", "comments": "17 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how efficiently a known underlying sparse causality structure\nof a simulated multivariate linear process can be retrieved from the analysis\nof time-series of short lengths. Causality is quantified from conditional\ntransfer entropy and the network is constructed by retaining only the\nstatistically validated contributions. We compare results from three\nmethodologies: two commonly used regularization methods, Glasso and ridge, and\na newly introduced technique, LoGo, based on the combination of information\nfiltering network and graphical modelling. For these three methodologies we\nexplore the regions of time series lengths and model-parameters where a\nsignificant fraction of true causality links is retrieved. We conclude that,\nwhen time-series are short, with their lengths shorter than the number of\nvariables, sparse models are better suited to uncover true causality links with\nLoGo retrieving the true causality network more accurately than Glasso and\nridge.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 20:10:53 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 13:48:08 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Aste", "Tomaso", ""], ["Di Matteo", "T.", ""]]}, {"id": "1706.01960", "submitter": "Matthew Dunlop", "authors": "Oliver R. A. Dunbar, Matthew M. Dunlop, Charles M. Elliott, Viet Ha\n  Hoang, Andrew M. Stuart", "title": "Reconciling Bayesian and perimeter regularization for binary inversion", "comments": "30 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central theme in classical algorithms for the reconstruction of\ndiscontinuous functions from observational data is perimeter regularization via\nthe use of the total variation. On the other hand, sparse or noisy data often\ndemands a probabilistic approach to the reconstruction of images, to enable\nuncertainty quantification; the Bayesian approach to inversion, which itself\nintroduces a form of regularization, is a natural framework in which to carry\nthis out. In this paper the link between Bayesian inversion methods and\nperimeter regularization is explored. In this paper two links are studied: (i)\nthe maximum a posteriori (MAP) objective function of a suitably chosen Bayesian\nphase-field approach is shown to be closely related to a least squares plus\nperimeter regularization objective; (ii) sample paths of a suitably chosen\nBayesian level set formulation are shown to possess finite perimeter and to\nhave the ability to learn about the true perimeter.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 20:23:06 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 00:14:25 GMT"}, {"version": "v3", "created": "Tue, 17 Dec 2019 19:30:43 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2020 11:36:51 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Dunbar", "Oliver R. A.", ""], ["Dunlop", "Matthew M.", ""], ["Elliott", "Charles M.", ""], ["Hoang", "Viet Ha", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "1706.02046", "submitter": "Michail Tsagris", "authors": "Michail Tsagris", "title": "Conditional independence test for categorical data using Poisson\n  log-linear model", "comments": "11 pages and 1 Figure", "journal-ref": "Journal of Data Science, 2017, Volume 15(2): 347-356", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate how to test for conditional independence of two variables with\ncategorical data using Poisson log-linear models. The size of the conditioning\nset of variables can vary from 0 (simple independence) up to many variables. We\nalso provide a function in R for performing the test. Instead of calculating\nall possible tables with for loop we perform the test using the log-linear\nmodels and thus speeding up the process. Time comparison simulation studies are\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 04:56:25 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Tsagris", "Michail", ""]]}, {"id": "1706.02150", "submitter": "Hanzhong Liu", "authors": "Hanzhong Liu, Xin Xu and Jingyi Jessica Li", "title": "A Bootstrap Lasso + Partial Ridge Method to Construct Confidence\n  Intervals for Parameters in High-dimensional Sparse Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing confidence intervals for the coefficients of high-dimensional\nsparse linear models remains a challenge, mainly because of the complicated\nlimiting distributions of the widely used estimators, such as the lasso.\nSeveral methods have been developed for constructing such intervals. Bootstrap\nlasso+ols is notable for its technical simplicity, good interpretability, and\nperformance that is comparable with that of other more complicated methods.\nHowever, bootstrap lasso+ols depends on the beta-min assumption, a theoretic\ncriterion that is often violated in practice. Thus, we introduce a new method,\ncalled bootstrap lasso+partial ridge, to relax this assumption. Lasso+partial\nridge is a two-stage estimator. First, the lasso is used to select features.\nThen, the partial ridge is used to refit the coefficients. Simulation results\nshow that bootstrap lasso+partial ridge outperforms bootstrap lasso+ols when\nthere exist small, but nonzero coefficients, a common situation that violates\nthe beta-min assumption. For such coefficients, the confidence intervals\nconstructed using bootstrap lasso+partial ridge have, on average, $50\\%$ larger\ncoverage probabilities than those of bootstrap lasso+ols. Bootstrap\nlasso+partial ridge also has, on average, $35\\%$ shorter confidence interval\nlengths than those of the de-sparsified lasso methods, regardless of whether\nthe linear models are misspecified. Additionally, we provide theoretical\nguarantees for bootstrap lasso+partial ridge under appropriate conditions, and\nimplement it in the R package \"HDCI.\"\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 12:29:25 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 15:23:23 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Liu", "Hanzhong", ""], ["Xu", "Xin", ""], ["Li", "Jingyi Jessica", ""]]}, {"id": "1706.02178", "submitter": "Hassan Maatouk", "authors": "Hassan Maatouk (Inria, GdR MASCOT-NUM)", "title": "Finite-dimensional approximation of Gaussian processes with inequality\n  constraints", "comments": "arXiv admin note: text overlap with arXiv:1606.01265", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their flexibility, Gaussian processes (GPs) have been widely used in\nnonparametric function estimation. A prior information about the underlying\nfunction is often available. For instance, the physical system (computer model\noutput) may be known to satisfy inequality constraints with respect to some or\nall inputs. We develop a finite-dimensional approximation of GPs capable of\nincorporating inequality constraints and noisy observations for computer model\nemulators. It is based on a linear combination between Gaussian random\ncoefficients and deterministic basis functions. By this methodology , the\ninequality constraints are respected in the entire domain. The mean and the\nmaximum of the posterior distribution are well defined. A simulation study to\nshow the efficiency and the performance of the proposed model in term of\npredictive accuracy and uncertainty quantification is included.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 12:36:44 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 07:37:05 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Maatouk", "Hassan", "", "Inria, GdR MASCOT-NUM"]]}, {"id": "1706.02283", "submitter": "Hwanhee Hong", "authors": "Hwanhee Hong, David A. Aaby, Juned Siddique, Elizabeth A. Stuart", "title": "Propensity score-based estimators with multiple error-prone covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score methods are an important tool to help reduce confounding in\nnon-experimental studies. Most propensity score methods assume that covariates\nare measured without error. However, covariates are often measured with error,\nwhich leads to biased causal effect estimates if the true underlying covariates\nare the actual confounders. Although some studies have investigated the impact\nof a single mismeasured covariate on estimating a causal effect and proposed\nmethods for handling the measurement error, almost no work exists investigating\nthe case where multiple covariates are mismeasured. In this paper, we examine\nthe consequences of multiple error-prone covariates when estimating causal\neffects using propensity score-based estimators via extensive simulation\nstudies and real data analyses. We find that causal effect estimates are less\nbiased when the propensity score model includes mismeasured covariates whose\ntrue underlying values are strongly correlated with each other. However, when\nthe measurement \\emph{errors} are correlated with each other, additional bias\nis introduced. In addition, it is beneficial to include correctly measured\nauxiliary variables that are correlated with confounders whose true underlying\nvalues are mismeasured in the propensity score model.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 17:55:15 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Hong", "Hwanhee", ""], ["Aaby", "David A.", ""], ["Siddique", "Juned", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "1706.02289", "submitter": "Evgeny Burnaev", "authors": "Smolyakov Dmitry, Alexander Korotin, Pavel Erofeev, Artem Papanov,\n  Evgeny Burnaev", "title": "Meta-Learning for Resampling Recommendation Systems", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One possible approach to tackle the class imbalance in classification tasks\nis to resample a training dataset, i.e., to drop some of its elements or to\nsynthesize new ones. There exist several widely-used resampling methods. Recent\nresearch showed that the choice of resampling method significantly affects the\nquality of classification, which raises resampling selection problem.\nExhaustive search for optimal resampling is time-consuming and hence it is of\nlimited use. In this paper, we describe an alternative approach to the\nresampling selection. We follow the meta-learning concept to build resampling\nrecommendation systems, i.e., algorithms recommending resampling for datasets\non the basis of their properties.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 22:02:27 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 21:19:48 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 08:08:13 GMT"}, {"version": "v4", "created": "Mon, 17 Sep 2018 07:50:39 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Dmitry", "Smolyakov", ""], ["Korotin", "Alexander", ""], ["Erofeev", "Pavel", ""], ["Papanov", "Artem", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1706.02338", "submitter": "Malte S. Kurz", "authors": "Malte S. Kurz and Fabian Spanhel", "title": "Testing the simplifying assumption in high-dimensional vine copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing the simplifying assumption in high-dimensional vine copulas is a\ndifficult task. Tests must be based on estimated observations and amount to\nchecking constraints on high-dimensional distributions. So far, corresponding\ntests have been limited to single conditional copulas with a low-dimensional\nset of conditioning variables. We propose a novel testing procedure that is\ncomputationally feasible for high-dimensional data sets and that exhibits a\npower that decreases only slightly with the dimension. By discretizing the\nsupport of the conditioning variables and incorporating a penalty in the test\nstatistic, we mitigate the curse of dimensions by looking for the possibly\nstrongest deviation from the simplifying assumption. The use of a decision tree\nrenders the test computationally feasible for large dimensions. We derive the\nasymptotic distribution of the test and analyze its finite sample performance\nin an extensive simulation study. The utility of the test is demonstrated by\nits application to six data sets with up to 49 dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 18:54:09 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 15:32:57 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 09:53:51 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kurz", "Malte S.", ""], ["Spanhel", "Fabian", ""]]}, {"id": "1706.02348", "submitter": "Chencheng Cai", "authors": "Chencheng Cai, Rong Chen and Ming Lin", "title": "Resampling Strategy in Sequential Monte Carlo for Constrained Sampling\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC) methods are a class of Monte Carlo methods that\nare used to obtain random samples of a high dimensional random variable in a\nsequential fashion. Many problems encountered in applications often involve\ndifferent types of constraints. These constraints can make the problem much\nmore challenging. In this paper, we formulate a general framework of using SMC\nfor constrained sampling problems based on forward and backward pilot\nresampling strategies. We review some existing methods under the framework and\ndevelop several new algorithms. It is noted that all information observed or\nimposed on the underlying system can be viewed as constraints. Hence the\napproach outlined in this paper can be useful in many applications.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 19:11:31 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 14:02:06 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Cai", "Chencheng", ""], ["Chen", "Rong", ""], ["Lin", "Ming", ""]]}, {"id": "1706.02353", "submitter": "Dengdeng Yu", "authors": "Dengdeng Yu, Li Zhang, Ivan Mizera, Bei Jiang, and Linglong Kong", "title": "Sparse Wavelet Estimation in Quantile Regression with Multiple\n  Functional Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we study quantile regression in partial functional linear\nmodel where response is scalar and predictors include both scalars and multiple\nfunctions. Wavelet basis are adopted to better approximate functional slopes\nwhile effectively detect local features. The sparse group lasso penalty is\nimposed to select important functional predictors while capture shared\ninformation among them. The estimation problem can be reformulated into a\nstandard second-order cone program and then solved by an interior point method.\nWe also give a novel algorithm by using alternating direction method of\nmultipliers (ADMM) which was recently employed by many researchers in solving\npenalized quantile regression problems. The asymptotic properties such as the\nconvergence rate and prediction error bound have been established. Simulations\nand a real data from ADHD-200 fMRI data are investigated to show the\nsuperiority of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 19:28:53 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 23:34:51 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Yu", "Dengdeng", ""], ["Zhang", "Li", ""], ["Mizera", "Ivan", ""], ["Jiang", "Bei", ""], ["Kong", "Linglong", ""]]}, {"id": "1706.02380", "submitter": "Anru Zhang", "authors": "Yuanpei Cao and Anru Zhang and Hongzhe Li", "title": "Multi-sample Estimation of Bacterial Composition Matrix in Metagenomics\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Metagenomics sequencing is routinely applied to quantify bacterial abundances\nin microbiome studies, where the bacterial composition is estimated based on\nthe sequencing read counts. Due to limited sequencing depth and DNA dropouts,\nmany rare bacterial taxa might not be captured in the final sequencing reads,\nwhich results in many zero counts. Naive composition estimation using count\nnormalization leads to many zero proportions, which tend to result in\ninaccurate estimates of bacterial abundance and diversity. This paper takes a\nmulti-sample approach to the estimation of bacterial abundances in order to\nborrow information across samples and across species. Empirical results from\nreal data sets suggest that the composition matrix over multiple samples is\napproximately low rank, which motivates a regularized maximum likelihood\nestimation with a nuclear norm penalty. An efficient optimization algorithm\nusing the generalized accelerated proximal gradient and Euclidean projection\nonto simplex space is developed. The theoretical upper bounds and the minimax\nlower bounds of the estimation errors, measured by the Kullback-Leibler\ndivergence and the Frobenius norm, are established. Simulation studies\ndemonstrate that the proposed estimator outperforms the naive estimators. The\nmethod is applied to an analysis of a human gut microbiome dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 21:05:26 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 05:50:09 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2018 01:44:19 GMT"}, {"version": "v4", "created": "Wed, 24 Apr 2019 18:41:04 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Cao", "Yuanpei", ""], ["Zhang", "Anru", ""], ["Li", "Hongzhe", ""]]}, {"id": "1706.02419", "submitter": "Artemy Kolchinsky", "authors": "Artemy Kolchinsky, Brendan D. Tracey", "title": "Estimating Mixture Entropy with Pairwise Distances", "comments": "Corrects several errata in published version, in particular in\n  Section V (bounds on mutual information)", "journal-ref": "Entropy 2017, 19(7), 361", "doi": "10.3390/e19070361", "report-no": null, "categories": "cs.IT math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture distributions arise in many parametric and non-parametric settings --\nfor example, in Gaussian mixture models and in non-parametric estimation. It is\noften necessary to compute the entropy of a mixture, but, in most cases, this\nquantity has no closed-form expression, making some form of approximation\nnecessary. We propose a family of estimators based on a pairwise distance\nfunction between mixture components, and show that this estimator class has\nmany attractive properties. For many distributions of interest, the proposed\nestimators are efficient to compute, differentiable in the mixture parameters,\nand become exact when the mixture components are clustered. We prove this\nfamily includes lower and upper bounds on the mixture entropy. The Chernoff\n$\\alpha$-divergence gives a lower bound when chosen as the distance function,\nwith the Bhattacharyya distance providing the tightest lower bound for\ncomponents that are symmetric and members of a location family. The\nKullback-Leibler divergence gives an upper bound when used as the distance\nfunction. We provide closed-form expressions of these bounds for mixtures of\nGaussians, and discuss their applications to the estimation of mutual\ninformation. We then demonstrate that our bounds are significantly tighter than\nwell-known existing bounds using numeric simulations. This estimator class is\nvery useful in optimization problems involving maximization/minimization of\nentropy and mutual information, such as MaxEnt and rate distortion problems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 00:47:46 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 20:06:34 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 01:25:04 GMT"}, {"version": "v4", "created": "Wed, 22 Aug 2018 05:33:38 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Kolchinsky", "Artemy", ""], ["Tracey", "Brendan D.", ""]]}, {"id": "1706.02429", "submitter": "Subhabrata Majumdar", "authors": "Subhabrata Majumdar, Snigdhansu Chatterjee", "title": "Fast and General Model Selection using Data Depth and Resampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique using data depth functions and resampling to perform\nbest subset variable selection for a wide range of statistical models. We do\nthis by assigning a score, called an $e$-value, to a candidate model, and use a\nfast bootstrap method to approximate sample versions of these $e$-values. Under\ngeneral conditions, $e$-values can separate statistical models that adequately\nexplain properties of the data from those that do not. This results in a fast\nalgorithm that fits only a single model and evaluates $p +1$ models, $p$ being\nthe number of predictors under consideration, as opposed to the traditional\nrequirement of fitting and evaluating $2^{p}$ models. We illustrate in\nsimulation experiments that our proposed method typically performs better than\nan array of currently used methods for variable selection in linear models and\nfixed effect selection in linear mixed models. As a real data application, we\nuse our procedure to elicit climatic drivers of Indian summer monsoon\nprecipitation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 02:15:46 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 21:30:38 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 16:39:29 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Majumdar", "Subhabrata", ""], ["Chatterjee", "Snigdhansu", ""]]}, {"id": "1706.02563", "submitter": "Clara Grazian", "authors": "Clara Grazian and Christian P. Robert", "title": "Jeffreys priors for mixture estimation: properties and alternatives", "comments": "arXiv admin note: substantial text overlap with arXiv:1511.03145", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Jeffreys priors usually are well-defined for the parameters of mixtures\nof distributions, they are not available in closed form. Furthermore, they\noften are improper priors. Hence, they have never been used to draw inference\non the mixture parameters. The implementation and the properties of Jeffreys\npriors in several mixture settings are studied. It is shown that the associated\nposterior distributions most often are improper. Nevertheless, the Jeffreys\nprior for the mixture weights conditionally on the parameters of the mixture\ncomponents will be shown to have the property of conservativeness with respect\nto the number of components, in case of overfitted mixture and it can be\ntherefore used as a default priors in this context.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 18:42:09 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 11:06:53 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 14:28:35 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Grazian", "Clara", ""], ["Robert", "Christian P.", ""]]}, {"id": "1706.02624", "submitter": "Anne Presanis", "authors": "Paul J Birrell, Daniela De Angelis and Anne M Presanis", "title": "Evidence synthesis for stochastic epidemic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the role of epidemic models in informing public health\npolicies has progressively grown. Models have become increasingly realistic and\nmore complex, requiring the use of multiple data sources to estimate all\nquantities of interest. This review summarises the different types of\nstochastic epidemic models that use evidence synthesis and highlights current\nchallenges.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 14:57:22 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Birrell", "Paul J", ""], ["De Angelis", "Daniela", ""], ["Presanis", "Anne M", ""]]}, {"id": "1706.02675", "submitter": "Laura Balzer PhD", "authors": "Laura B. Balzer, Wenjing Zheng, Mark J. van der Laan, Maya L. Petersen\n  (for the SEARCH Collaboration)", "title": "A new approach to hierarchical data analysis: Targeted maximum\n  likelihood estimation for the causal effect of a cluster-level exposure", "comments": null, "journal-ref": "Statistical Methods in Medical Research, 28, 2019, 1761-1780", "doi": "10.1177/0962280218774936", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We often seek to estimate the impact of an exposure naturally occurring or\nrandomly assigned at the cluster-level. For example, the literature on\nneighborhood determinants of health continues to grow. Likewise, community\nrandomized trials are applied to learn about real-world implementation,\nsustainability, and population effects of interventions with proven\nindividual-level efficacy. In these settings, individual-level outcomes are\ncorrelated due to shared cluster-level factors, including the exposure, as well\nas social or biological interactions between individuals. To flexibly and\nefficiently estimate the effect of a cluster-level exposure, we present two\ntargeted maximum likelihood estimators (TMLEs). The first TMLE is developed\nunder a non-parametric causal model, which allows for arbitrary interactions\nbetween individuals within a cluster. These interactions include direct\ntransmission of the outcome (i.e. contagion) and influence of one individual's\ncovariates on another's outcome (i.e. covariate interference). The second TMLE\nis developed under a causal sub-model assuming the cluster-level and\nindividual-specific covariates are sufficient to control for confounding.\nSimulations compare the alternative estimators and illustrate the potential\ngains from pairing individual-level risk factors and outcomes during\nestimation, while avoiding unwarranted assumptions. Our results suggest that\nestimation under the sub-model can result in bias and misleading inference in\nan observational setting. Incorporating working assumptions during estimation\nis more robust than assuming they hold in the underlying causal model. We\nillustrate our approach with an application to HIV prevention and treatment.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 16:49:49 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 17:18:23 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Balzer", "Laura B.", "", "for the SEARCH Collaboration"], ["Zheng", "Wenjing", "", "for the SEARCH Collaboration"], ["van der Laan", "Mark J.", "", "for the SEARCH Collaboration"], ["Petersen", "Maya L.", "", "for the SEARCH Collaboration"]]}, {"id": "1706.02692", "submitter": "Sebastian Vollmer", "authors": "Tigran Nagapetyan, Andrew B. Duncan, Leonard Hasenclever, Sebastian J.\n  Vollmer, Lukasz Szpruch, Konstantinos Zygalakis", "title": "The True Cost of Stochastic Gradient Langevin Dynamics", "comments": "6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of posterior inference is central to Bayesian statistics and a\nwealth of Markov Chain Monte Carlo (MCMC) methods have been proposed to obtain\nasymptotically correct samples from the posterior. As datasets in applications\ngrow larger and larger, scalability has emerged as a central problem for MCMC\nmethods. Stochastic Gradient Langevin Dynamics (SGLD) and related stochastic\ngradient Markov Chain Monte Carlo methods offer scalability by using stochastic\ngradients in each step of the simulated dynamics. While these methods are\nasymptotically unbiased if the stepsizes are reduced in an appropriate fashion,\nin practice constant stepsizes are used. This introduces a bias that is often\nignored. In this paper we study the mean squared error of Lipschitz functionals\nin strongly log- concave models with i.i.d. data of growing data set size and\nshow that, given a batchsize, to control the bias of SGLD the stepsize has to\nbe chosen so small that the computational cost of reaching a target accuracy is\nroughly the same for all batchsizes. Using a control variate approach, the cost\ncan be reduced dramatically. The analysis is performed by considering the\nalgorithms as noisy discretisations of the Langevin SDE which correspond to the\nEuler method if the full data set is used. An important observation is that the\n1scale of the step size is determined by the stability criterion if the\naccuracy is required for consistent credible intervals. Experimental results\nconfirm our theoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 17:51:01 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Nagapetyan", "Tigran", ""], ["Duncan", "Andrew B.", ""], ["Hasenclever", "Leonard", ""], ["Vollmer", "Sebastian J.", ""], ["Szpruch", "Lukasz", ""], ["Zygalakis", "Konstantinos", ""]]}, {"id": "1706.02781", "submitter": "Alexander Tank", "authors": "Alex Tank, Emily B. Fox, Ali Shojaie", "title": "Granger Causality Networks for Categorical Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework for learning Granger causality networks for\nmultivariate categorical time series, based on the mixture transition\ndistribution (MTD) model. Traditionally, MTD is plagued by a nonconvex\nobjective, non-identifiability, and presence of many local optima. To\ncircumvent these problems, we recast inference in the MTD as a convex problem.\nThe new formulation facilitates the application of MTD to high-dimensional\nmultivariate time series. As a baseline, we also formulate a multi-output\nlogistic autoregressive model (mLTD), which while a straightforward extension\nof autoregressive Bernoulli generalized linear models, has not been previously\napplied to the analysis of multivariate categorial time series. We develop\nnovel identifiability conditions of the MTD model and compare them to those for\nmLTD. We further devise novel and efficient optimization algorithm for the MTD\nbased on the new convex formulation, and compare the MTD and mLTD in both\nsimulated and real data experiments. Our approach simultaneously provides a\ncomparison of methods for network inference in categorical time series and\nopens the door to modern, regularized inference with the MTD model.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 22:02:06 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Tank", "Alex", ""], ["Fox", "Emily B.", ""], ["Shojaie", "Ali", ""]]}, {"id": "1706.02940", "submitter": "Theodore  Kypraios", "authors": "Theodore Kypraios and Philip D. O'Neill", "title": "Bayesian nonparametrics for stochastic epidemic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of models for the spread of communicable diseases are\nparametric in nature and involve underlying assumptions about how the disease\nspreads through a population. In this article we consider the use of Bayesian\nnonparametric approaches to analysing data from disease outbreaks. Specifically\nwe focus on methods for estimating the infection process in simple models under\nthe assumption that this process has an explicit time-dependence.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 13:12:27 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Kypraios", "Theodore", ""], ["O'Neill", "Philip D.", ""]]}, {"id": "1706.02946", "submitter": "Anna Klimova", "authors": "Anna Klimova, Tam\\'as Rudas", "title": "On the role of the overall effect in exponential families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential families of discrete probability distributions when the\nnormalizing constant (or overall effect) is added or removed are compared in\nthis paper. The latter setup, in which the exponential family is curved, is\nparticularly relevant when the sample space is an incomplete Cartesian product\nor when it is very large, so that the computational burden is significant. The\nlack or presence of the overall effect has a fundamental impact on the\nproperties of the exponential family. When the overall effect is added, the\nfamily becomes the smallest regular exponential family containing the curved\none. The procedure is related to the homogenization of an inhomogeneous variety\ndiscussed in algebraic geometry, of which a statistical interpretation is given\nas an augmentation of the sample space. The changes in the kernel basis\nrepresentation when the overall effect is included or removed are derived. The\ngeometry of maximum likelihood estimates, also allowing zero observed\nfrequencies, is described with and without the overall effect, and various\nalgorithms are compared. The importance of the results is illustrated by an\nexample from cell biology, showing that routinely including the overall effect\nleads to estimates which are not in the model intended by the researchers.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 13:39:53 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 11:09:08 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Klimova", "Anna", ""], ["Rudas", "Tam\u00e1s", ""]]}, {"id": "1706.02972", "submitter": "Tasmin Symons", "authors": "N. H. Bingham and Tasmin L. Symons", "title": "Probability, Statistics and Planet Earth, I: Geotemporal covariances", "comments": "20 pages; corrected statement of GMP Theorem", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of covariances (or positive definite functions) on the sphere (the\nEarth, in our motivation) goes back to Bochner and Schoenberg (1940--42) and to\nthe first author (1969, 1973), among others. Extending to the geotemporal case\n(sphere cross line, for position and time) was for a long time an obstacle to\ngeostatistical modelling. The characterisation question here was raised by the\nauthors and Mijatovi\\'c in 2016, and answered by Berg and Porcu in 2017.\nExtensions to multiple products (of spheres and lines) follows similarly\n(Guella, Menegatto and Peron, 2016). We survey results of this type, and\nrelated applications e.g. in numerical weather prediction.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 14:29:45 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 15:45:08 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Bingham", "N. H.", ""], ["Symons", "Tasmin L.", ""]]}, {"id": "1706.03156", "submitter": "Denis Agniel", "authors": "Denis Agniel, Wen Xie, Myron Essex, Tianxi Cai", "title": "Functional principal variance component testing for a genetic\n  association study of HIV progression", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HIV-1C is the most prevalent subtype of HIV-1 and accounts for over half of\nHIV-1 infections worldwide. Host genetic influence of HIV infection has been\npreviously studied in HIV-1B, but little attention has been paid to the more\nprevalent subtype C. To understand the role of host genetics in HIV-1C disease\nprogression, we perform a study to assess the association between\nlongitudinally collected measures of disease and more than 100,000 genetic\nmarkers located on chromosome 6. The most common approach to analyzing\nlongitudinal data in this context is linear mixed effects models, which may be\noverly simplistic in this case. On the other hand, existing non-parametric\nmethods may suffer from low power due to high degrees of freedom (DF) and may\nbe computationally infeasible at the large scale. We propose a functional\nprincipal variance component (FPVC) testing framework which captures the\nnonlinearity in the CD4 and viral load with potentially low DF and is fast\nenough to carry out thousands or millions of times. The FPVC testing unfolds in\ntwo stages. In the first stage, we summarize the markers of disease progression\naccording to their major patterns of variation via functional principal\ncomponents analysis (FPCA). In the second stage, we employ a simple working\nmodel and variance component testing to examine the association between the\nsummaries of disease progression and a set of single nucleotide polymorphisms.\nWe supplement this analysis with simulation results which indicate that FPVC\ntesting can offer large power gains over the standard linear mixed effects\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 00:12:58 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Agniel", "Denis", ""], ["Xie", "Wen", ""], ["Essex", "Myron", ""], ["Cai", "Tianxi", ""]]}, {"id": "1706.03277", "submitter": "Yuan Ji", "authors": "Yuan Ji and Shengjie Yang", "title": "On the Interval-Based Dose-Finding Designs", "comments": "This is the second version with typos corrected and an incorrect\n  reference removed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The landscape of dose-finding designs for phase I clinical trials is rapidly\nshifting in the recent years, noticeably marked by the emergence of\ninterval-based designs. We categorize them as the iDesigns and the IB-Designs.\nThe iDesigns are originated by the toxicity probability inter- val (TPI)\ndesigns and its two modifications, the mTPI and mTPI-2 designs. The IB-Designs\nstarted as the cumulative cohort design (CCD) and is recently extended by the\nBOIN design. We discuss the differences and similarities between these two\nclasses of interval-based designs, and compare their simulation performance\nwith popular non-interval designs, such as the CRM and 3+3 designs. We also\nshow that in addition to the population-level operating characteristics from\nsimulated trials, investigators should also assess the dose-finding decision\ntables from the implemented designs to better understand the per-trial and\nper-patient behavior. This is particularly important for nonstatisticians to\nassess the designs with transparency. We pro- vide, to our knowledge, the most\ncomprehensive simulation-based comparative study on various interval-based\ndose-finding designs.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 20:19:56 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 17:19:34 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Ji", "Yuan", ""], ["Yang", "Shengjie", ""]]}, {"id": "1706.03278", "submitter": "Yuan Ji", "authors": "Jiaying Lyu, Yuan Ji, Naiqing Zhao, Daniel V.T. Catenacci", "title": "AAA: Triple-adaptive Bayesian designs for the identification of optimal\n  dose combinations in dual-agent dose-finding trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible design for the identification of optimal dose\ncombinations in dual-agent dose-finding clinical trials. The design is called\nAAA, standing for three adaptations: adaptive model selection, adaptive dose\ninsertion, and adaptive cohort divi- sion. The adaptations highlight the need\nand opportunity for innovation for dual-agent dose finding, and are supported\nby the numerical results presented in the proposed simulation studies. To our\nknowledge, this is the first design that allows for all three adaptations at\nthe same time. We find that AAA improves the statistical inference, enhances\nthe chance of finding the optimal dose combinations, and shortens the trial\nduration. A clinical trial is being planned to apply the AAA design.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 20:27:54 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Lyu", "Jiaying", ""], ["Ji", "Yuan", ""], ["Zhao", "Naiqing", ""], ["Catenacci", "Daniel V. T.", ""]]}, {"id": "1706.03343", "submitter": "Michiel de Kock Mr", "authors": "MB de Kock and HC Eggers", "title": "Bayesian Model Selection for Misspecified Models in Linear Regression", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the Bayesian Information Criterion (BIC) and Akaike Information\nCriterion (AIC) are powerful tools for model selection in linear regression,\nthey are built on different prior assumptions and thereby apply to different\ndata generation scenarios. We show that in the finite-dimensional case their\nrespective assumptions can be unified within an augmented model-plus-noise\nspace and construct a prior in this space which inherits the beneficial\nproperties of both AIC and BIC. This allows us to adapt the BIC to be robust\nagainst misspecified models where the signal to noise ratio is low.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 11:03:09 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 16:56:58 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["de Kock", "MB", ""], ["Eggers", "HC", ""]]}, {"id": "1706.03388", "submitter": "Christopher Paciorek", "authors": "Christopher J. Paciorek, D\\'aith\\'i A. Stone, Michael F. Wehner", "title": "Quantifying statistical uncertainty in the attribution of human\n  influence on severe weather", "comments": "41 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event attribution in the context of climate change seeks to understand the\nrole of anthropogenic greenhouse gas emissions on extreme weather events,\neither specific events or classes of events. A common approach to event\nattribution uses climate model output under factual (real-world) and\ncounterfactual (world that might have been without anthropogenic greenhouse gas\nemissions) scenarios to estimate the probabilities of the event of interest\nunder the two scenarios. Event attribution is then quantified by the ratio of\nthe two probabilities. While this approach has been applied many times in the\nlast 15 years, the statistical techniques used to estimate the risk ratio based\non climate model ensembles have not drawn on the full set of methods available\nin the statistical literature and have in some cases used and interpreted the\nbootstrap method in non-standard ways. We present a precise frequentist\nstatistical framework for quantifying the effect of sampling uncertainty on\nestimation of the risk ratio, propose the use of statistical methods that are\nnew to event attribution, and evaluate a variety of methods using statistical\nsimulations. We conclude that existing statistical methods not yet in use for\nevent attribution have several advantages over the widely-used bootstrap,\nincluding better statistical performance in repeated samples and robustness to\nsmall estimated probabilities. Software for using the methods is available\nthrough the climextRemes package available for R or Python. While we focus on\nfrequentist statistical methods, Bayesian methods are likely to be particularly\nuseful when considering sources of uncertainty beyond sampling uncertainty.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 18:25:47 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 19:44:25 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Paciorek", "Christopher J.", ""], ["Stone", "D\u00e1ith\u00ed A.", ""], ["Wehner", "Michael F.", ""]]}, {"id": "1706.03400", "submitter": "Anthony Hou", "authors": "Jiajie Chen, Anthony Hou, Thomas Y. Hou", "title": "A Prototype Knockoff Filter for Group Selection with FDR Control", "comments": "16 pages, 5 figures", "journal-ref": "Information and Inference: A Journal of the IMA, iaz012 (2019)", "doi": "10.1093/imaiai/iaz012", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, we need to study a linear regression model that\nconsists of a response variable and a large number of potential explanatory\nvariables and determine which variables are truly associated with the response.\nIn 2015, Barber and Candes introduced a new variable selection procedure called\nthe knockoff filter to control the false discovery rate (FDR) and proved that\nthis method achieves exact FDR control. In this paper, we propose a prototype\nknockoff filter for group selection by extending the Reid-Tibshirani prototype\nmethod. Our prototype knockoff filter improves the computational efficiency and\nstatistical power of the Reid-Tibshirani prototype method when it is applied\nfor group selection. In some cases when the group features are spanned by one\nor a few hidden factors, we demonstrate that the PCA prototype knockoff filter\noutperforms the Dai-Barber group knockoff filter. We present several numerical\nexperiments to compare our prototype knockoff filter with the Reid-Tibshirani\nprototype method and the group knockoff filter. We have also conducted some\nanalysis of the knockoff filter. Our analysis reveals that some knockoff path\nmethod statistics, including the Lasso path statistic, may lead to loss of\npower for certain design matrices and a specially designed response even if\ntheir signal strengths are still relatively strong.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 19:56:06 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 04:15:03 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Chen", "Jiajie", ""], ["Hou", "Anthony", ""], ["Hou", "Thomas Y.", ""]]}, {"id": "1706.03411", "submitter": "Massil Achab", "authors": "Massil Achab, Emmanuel Bacry, Jean-Fran\\c{c}ois Muzy, Marcello\n  Rambaldi", "title": "Analysis of order book flows using a nonparametric estimation of the\n  branching ratio matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new non parametric method that allows for a direct, fast and\nefficient estimation of the matrix of kernel norms of a multivariate Hawkes\nprocess, also called branching ratio matrix. We demonstrate the capabilities of\nthis method by applying it to high-frequency order book data from the EUREX\nexchange. We show that it is able to uncover (or recover) various relationships\nbetween all the first level order book events associated with some asset when\nmapped to a 12-dimensional process. We then scale up the model so as to account\nfor events on two assets simultaneously and we discuss the joint high-frequency\ndynamics.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 21:39:11 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Achab", "Massil", ""], ["Bacry", "Emmanuel", ""], ["Muzy", "Jean-Fran\u00e7ois", ""], ["Rambaldi", "Marcello", ""]]}, {"id": "1706.03412", "submitter": "Evgeny Burnaev", "authors": "Vladislav Ishimtsev, Ivan Nazarov, Alexander Bernstein and Evgeny\n  Burnaev", "title": "Conformal k-NN Anomaly Detector for Univariate Data Streams", "comments": "15 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies in time-series data give essential and often actionable information\nin many applications. In this paper we consider a model-free anomaly detection\nmethod for univariate time-series which adapts to non-stationarity in the data\nstream and provides probabilistic abnormality scores based on the conformal\nprediction paradigm. Despite its simplicity the method performs on par with\ncomplex prediction-based models on the Numenta Anomaly Detection benchmark and\nthe Yahoo! S5 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 21:45:24 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Ishimtsev", "Vladislav", ""], ["Nazarov", "Ivan", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1706.03415", "submitter": "Evgeny Burnaev", "authors": "Denis Volkhonskiy, Ilia Nouretdinov, Alexander Gammerman, Vladimir\n  Vovk, Evgeny Burnaev", "title": "Inductive Conformal Martingales for Change-Point Detection", "comments": "22 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of quickest change-point detection in data streams.\nClassical change-point detection procedures, such as CUSUM, Shiryaev-Roberts\nand Posterior Probability statistics, are optimal only if the change-point\nmodel is known, which is an unrealistic assumption in typical applied problems.\nInstead we propose a new method for change-point detection based on Inductive\nConformal Martingales, which requires only the independence and identical\ndistribution of observations. We compare the proposed approach to standard\nmethods, as well as to change-point detection oracles, which model a typical\npractical situation when we have only imprecise (albeit parametric) information\nabout pre- and post-change data distributions. Results of comparison provide\nevidence that change-point detection based on Inductive Conformal Martingales\nis an efficient tool, capable to work under quite general conditions unlike\ntraditional approaches.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 21:49:19 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Volkhonskiy", "Denis", ""], ["Nouretdinov", "Ilia", ""], ["Gammerman", "Alexander", ""], ["Vovk", "Vladimir", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1706.03461", "submitter": "S\\\"oren R. K\\\"unzel", "authors": "S\\\"oren R. K\\\"unzel, Jasjeet S. Sekhon, Peter J. Bickel, Bin Yu", "title": "Meta-learners for Estimating Heterogeneous Treatment Effects using\n  Machine Learning", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.1804597116", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in estimating and analyzing heterogeneous treatment\neffects in experimental and observational studies. We describe a number of\nmeta-algorithms that can take advantage of any supervised learning or\nregression method in machine learning and statistics to estimate the\nConditional Average Treatment Effect (CATE) function. Meta-algorithms build on\nbase algorithms---such as Random Forests (RF), Bayesian Additive Regression\nTrees (BART) or neural networks---to estimate the CATE, a function that the\nbase algorithms are not designed to estimate directly. We introduce a new\nmeta-algorithm, the X-learner, that is provably efficient when the number of\nunits in one treatment group is much larger than in the other, and can exploit\nstructural properties of the CATE function. For example, if the CATE function\nis linear and the response functions in treatment and control are Lipschitz\ncontinuous, the X-learner can still achieve the parametric rate under\nregularity conditions. We then introduce versions of the X-learner that use RF\nand BART as base learners. In extensive simulation studies, the X-learner\nperforms favorably, although none of the meta-learners is uniformly the best.\nIn two persuasion field experiments from political science, we demonstrate how\nour new X-learner can be used to target treatment regimes and to shed light on\nunderlying mechanisms. A software package is provided that implements our\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 04:10:09 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 16:44:35 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 07:18:59 GMT"}, {"version": "v4", "created": "Fri, 16 Mar 2018 02:13:30 GMT"}, {"version": "v5", "created": "Fri, 28 Sep 2018 04:31:26 GMT"}, {"version": "v6", "created": "Wed, 24 Apr 2019 03:01:02 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["K\u00fcnzel", "S\u00f6ren R.", ""], ["Sekhon", "Jasjeet S.", ""], ["Bickel", "Peter J.", ""], ["Yu", "Bin", ""]]}, {"id": "1706.03462", "submitter": "Siliang Gong", "authors": "Siliang Gong, Kai Zhang and Yufeng Liu", "title": "Efficient Test-based Variable Selection for High-dimensional Linear\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection plays a fundamental role in high-dimensional data\nanalysis. Various methods have been developed for variable selection in recent\nyears. Well-known examples are forward stepwise regression (FSR) and least\nangle regression (LARS), among others. These methods typically add variables\ninto the model one by one. For such selection procedures, it is crucial to find\na stopping criterion that controls model complexity. One of the most commonly\nused techniques to this end is cross-validation (CV) which, in spite of its\npopularity, has two major drawbacks: expensive computational cost and lack of\nstatistical interpretation. To overcome these drawbacks, we introduce a\nflexible and efficient test-based variable selection approach that can be\nincorporated into any sequential selection procedure. The test, which is on the\noverall signal in the remaining inactive variables, is based on the maximal\nabsolute partial correlation between the inactive variables and the response\ngiven active variables. We develop the asymptotic null distribution of the\nproposed test statistic as the dimension tends to infinity uniformly in the\nsample size. We also show that the test is consistent. With this test, at each\nstep of the selection, a new variable is included if and only if the $p$-value\nis below some pre-defined level. Numerical studies show that the proposed\nmethod delivers very competitive performance in terms of variable selection\naccuracy and computational complexity compared to CV.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 04:17:25 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 01:21:16 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Gong", "Siliang", ""], ["Zhang", "Kai", ""], ["Liu", "Yufeng", ""]]}, {"id": "1706.03597", "submitter": "Said el Bouhaddani", "authors": "Said el Bouhaddani, Hae-Won Uh, Caroline Hayward, Geurt Jongbloed,\n  Jeanine Houwing-Duistermaat", "title": "Probabilistic partial least squares model: Identifiability, estimation\n  and application", "comments": "Accepted in Journal of Multivariate Analysis", "journal-ref": null, "doi": "10.1016/j.jmva.2018.05.009", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a rapid increase in volume and complexity of data sets, there is a need\nfor methods that can extract useful information, for example the relationship\nbetween two data sets measured for the same persons. The Partial Least Squares\n(PLS) method can be used for this dimension reduction task. Within life\nsciences, results across studies are compared and combined. Therefore,\nparameters need to be identifiable, which is not the case for PLS. In addition,\nPLS is an algorithm, while epidemiological study designs are often\noutcome-dependent and methods to analyze such data require a probabilistic\nformulation. Moreover, a probabilistic model provides a statistical framework\nfor inference. To address these issues, we develop Probabilistic PLS (PPLS). We\nderive maximum likelihood estimators that satisfy the identifiability\nconditions by using an EM algorithm with a constrained optimization in the M\nstep. We show that the PPLS parameters are identifiable up to sign. A\nsimulation study is conducted to study the performance of PPLS compared to\nexisting methods. The PPLS estimates performed well in various scenarios, even\nin high dimensions. Most notably, the estimates seem to be robust against\ndepartures from normality. To illustrate our method, we applied it to IgG\nglycan data from two cohorts. Our PPLS model provided insight as well as\ninterpretable results across the two cohorts.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 12:31:58 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 11:23:11 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Bouhaddani", "Said el", ""], ["Uh", "Hae-Won", ""], ["Hayward", "Caroline", ""], ["Jongbloed", "Geurt", ""], ["Houwing-Duistermaat", "Jeanine", ""]]}, {"id": "1706.03649", "submitter": "Umut \\c{S}im\\c{s}ekli", "authors": "Umut \\c{S}im\\c{s}ekli", "title": "Fractional Langevin Monte Carlo: Exploring L\\'{e}vy Driven Stochastic\n  Differential Equations for Markov Chain Monte Carlo", "comments": "Published in the International Conference on Machine Learning (ICML\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the recent advances in scalable Markov Chain Monte Carlo methods,\nsampling techniques that are based on Langevin diffusions have started\nreceiving increasing attention. These so called Langevin Monte Carlo (LMC)\nmethods are based on diffusions driven by a Brownian motion, which gives rise\nto Gaussian proposal distributions in the resulting algorithms. Even though\nthese approaches have proven successful in many applications, their performance\ncan be limited by the light-tailed nature of the Gaussian proposals. In this\nstudy, we extend classical LMC and develop a novel Fractional LMC (FLMC)\nframework that is based on a family of heavy-tailed distributions, called\n$\\alpha$-stable L\\'{e}vy distributions. As opposed to classical approaches, the\nproposed approach can possess large jumps while targeting the correct\ndistribution, which would be beneficial for efficient exploration of the state\nspace. We develop novel computational methods that can scale up to large-scale\nproblems and we provide formal convergence analysis of the proposed scheme. Our\nexperiments support our theory: FLMC can provide superior performance in\nmulti-modal settings, improved convergence rates, and robustness to algorithm\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 14:07:00 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["\u015eim\u015fekli", "Umut", ""]]}, {"id": "1706.03665", "submitter": "Daniel Ahfock", "authors": "Daniel Ahfock, William J. Astle and Sylvia Richardson", "title": "Statistical properties of sketching algorithms", "comments": "added central limit theorem under weaker conditions, unconditional\n  results, corrected expression for variance of partial sketch", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketching is a probabilistic data compression technique that has been largely\ndeveloped in the computer science community. Numerical operations on big\ndatasets can be intolerably slow; sketching algorithms address this issue by\ngenerating a smaller surrogate dataset. Typically, inference proceeds on the\ncompressed dataset. Sketching algorithms generally use random projections to\ncompress the original dataset and this stochastic generation process makes them\namenable to statistical analysis. We argue that the sketched data can be\nmodelled as a random sample, thus placing this family of data compression\nmethods firmly within an inferential framework. In particular, we focus on the\nGaussian, Hadamard and Clarkson-Woodruff sketches, and their use in single pass\nsketching algorithms for linear regression with huge $n$. We explore the\nstatistical properties of sketched regression algorithms and derive new\ndistributional results for a large class of sketched estimators. A key result\nis a conditional central limit theorem for data oblivious sketches. An\nimportant finding is that the best choice of sketching algorithm in terms of\nmean square error is related to the signal to noise ratio in the source\ndataset. Finally, we demonstrate the theory and the limits of its applicability\non two real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 14:44:47 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 23:51:39 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ahfock", "Daniel", ""], ["Astle", "William J.", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1706.03768", "submitter": "Kun Zhang", "authors": "Kun Zhang, Mingming Gong, Joseph Ramsey, Kayhan Batmanghelich, Peter\n  Spirtes, Clark Glymour", "title": "Causal Discovery in the Presence of Measurement Error: Identifiability\n  Conditions", "comments": "15 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Measurement error in the observed values of the variables can greatly change\nthe output of various causal discovery methods. This problem has received much\nattention in multiple fields, but it is not clear to what extent the causal\nmodel for the measurement-error-free variables can be identified in the\npresence of measurement error with unknown variance. In this paper, we study\nprecise sufficient identifiability conditions for the measurement-error-free\ncausal model and show what information of the causal model can be recovered\nfrom observed data. In particular, we present two different sets of\nidentifiability conditions, based on the second-order statistics and\nhigher-order statistics of the data, respectively. The former was inspired by\nthe relationship between the generating model of the\nmeasurement-error-contaminated data and the factor analysis model, and the\nlatter makes use of the identifiability result of the over-complete independent\ncomponent analysis problem.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 17:22:26 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Zhang", "Kun", ""], ["Gong", "Mingming", ""], ["Ramsey", "Joseph", ""], ["Batmanghelich", "Kayhan", ""], ["Spirtes", "Peter", ""], ["Glymour", "Clark", ""]]}, {"id": "1706.03883", "submitter": "Nhat Ho", "authors": "Nhat Ho, XuanLong Nguyen, Mikhail Yurochkin, Hung Hai Bui, Viet Huynh,\n  Dinh Phung", "title": "Multilevel Clustering via Wasserstein Means", "comments": "Proceedings of the ICML, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to the problem of multilevel clustering, which\naims to simultaneously partition data in each group and discover grouping\npatterns among groups in a potentially large hierarchically structured corpus\nof data. Our method involves a joint optimization formulation over several\nspaces of discrete probability measures, which are endowed with Wasserstein\ndistance metrics. We propose a number of variants of this problem, which admit\nfast optimization algorithms, by exploiting the connection to the problem of\nfinding Wasserstein barycenters. Consistency properties are established for the\nestimates of both local and global clusters. Finally, experiment results with\nboth synthetic and real data are presented to demonstrate the flexibility and\nscalability of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 01:15:04 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Ho", "Nhat", ""], ["Nguyen", "XuanLong", ""], ["Yurochkin", "Mikhail", ""], ["Bui", "Hung Hai", ""], ["Huynh", "Viet", ""], ["Phung", "Dinh", ""]]}, {"id": "1706.03901", "submitter": "Fred Lombard Prof", "authors": "F. Lombard and C. Van Zyl", "title": "Signed Sequential Rank CUSUMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CUSUMs based on the signed sequential ranks of observations are developed for\ndetecting location and scale changes in symmetric distributions. The CUSUMs are\ndistribution free and fully self-starting: given a specified in-control median\nand nominal in-control average run length, no parametric specification of the\nunderlying distribution is required in order to find the correct control\nlimits. If the underlying distribution is normal with unknown variance, a CUSUM\nbased on the Van der Waerden signed rank score produces out-of-control average\nrun lengths that are commensurate with those produced by the standard CUSUM for\na normal distribution with known variance. For heavier tailed distributions,\nuse of a CUSUM based on the Wilcoxon signed rank score is indicated. The\nmethodology is illustrated by application to real data from an industrial\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 04:48:46 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Lombard", "F.", ""], ["Van Zyl", "C.", ""]]}, {"id": "1706.03938", "submitter": "Robert Kohn", "authors": "David Gunawan and Chris Carter and Robert Kohn", "title": "Efficient Bayesian inference for multivariate factor stochastic\n  volatility models with leverage", "comments": "4 figures and 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the efficient Bayesian estimation of a multivariate\nfactor stochastic volatility (Factor MSV) model with leverage. We propose a\nnovel approach to construct the sampling schemes that converges to the\nposterior distribution of the latent volatilities and the parameters of\ninterest of the Factor MSV model based on recent advances in Particle Markov\nchain Monte Carlo (PMCMC). As opposed to the approach of Chib et al. (2006} and\nOmori et al. (2007}, our approach does not require approximating the joint\ndistribution of outcome and volatility innovations by a mixture of bivariate\nnormal distributions. To sample the free elements of the loading matrix we\nemploy the interweaving method used in Kastner et al. (2017} in the Particle\nMetropolis within Gibbs (PMwG) step. The proposed method is illustrated\nempirically using a simulated dataset and a sample of daily US stock returns.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 07:46:22 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Gunawan", "David", ""], ["Carter", "Chris", ""], ["Kohn", "Robert", ""]]}, {"id": "1706.03953", "submitter": "Robert Kohn", "authors": "David Gunawan and Chris carter and Denzil Fiebig and Robert Kohn", "title": "Efficient Bayesian estimation for flexible panel models for multivariate\n  outcomes: Impact of life events on mental health and excessive alcohol\n  consumption", "comments": "48 pages, 14 tables and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem we consider considers estimating a multivariate longitudinal\npanel data model whose outcomes can be a combination of discrete and continuous\nvariables. This problem is challenging because the likelihood is usually\nanalytically intractable. Our article makes both a methodological contribution\nand also a substantive contribution to the application. The methodological\ncontribution is to introduce into the panel data literature a particle\nMetropolis within Gibbs method to carry out Bayesian inference, using a\nHamiltonian Monte Carlo (Neal 2011} proposal for sampling the vector of unknown\nparameters. We note that in panel data models the Our second contribution is to\napply our method to carry out a serious analysis of the impact of serious life\nevents on mental health and excessive alcohol consumption. The dependence\nbetween these two outcomes may be more pronounced when consumption of alcohol\nis excessive and mental health poor, which in turn has implications for how\nlife events impact the joint distribution of the outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 08:28:48 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 01:01:23 GMT"}, {"version": "v3", "created": "Sat, 23 Sep 2017 09:25:26 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Gunawan", "David", ""], ["carter", "Chris", ""], ["Fiebig", "Denzil", ""], ["Kohn", "Robert", ""]]}, {"id": "1706.04032", "submitter": "Tijana Radivojevic", "authors": "Tijana Radivojevi\\'c and Elena Akhmatskaya", "title": "Modified Hamiltonian Monte Carlo for Bayesian inference", "comments": "30 pages, 17 figures", "journal-ref": "Stat Comput (2019). https://doi.org/10.1007/s11222-019-09885-x", "doi": "10.1007/s11222-019-09885-x", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hamiltonian Monte Carlo (HMC) method has been recognized as a powerful\nsampling tool in computational statistics. We show that performance of HMC can\nbe significantly improved by incorporating importance sampling and an\nirreversible part of the dynamics into a chain. This is achieved by replacing\nHamiltonians in the Metropolis test with modified Hamiltonians, and a complete\nmomentum update with a partial momentum refreshment. We call the resulting\ngeneralized HMC importance sampler---Mix & Match Hamiltonian Monte Carlo\n(MMHMC). The method is irreversible by construction and further benefits from\n(i) the efficient algorithms for computation of modified Hamiltonians; (ii) the\nimplicit momentum update procedure and (iii) the multi-stage splitting\nintegrators specially derived for the methods sampling with modified\nHamiltonians. MMHMC has been implemented, tested on the popular statistical\nmodels and compared in sampling efficiency with HMC, Riemann Manifold\nHamiltonian Monte Carlo, Generalized Hybrid Monte Carlo, Generalized Shadow\nHybrid Monte Carlo, Metropolis Adjusted Langevin Algorithm and Random Walk\nMetropolis-Hastings. To make a fair comparison, we propose a metric that\naccounts for correlations among samples and weights, and can be readily used\nfor all methods which generate such samples. The experiments reveal the\nsuperiority of MMHMC over popular sampling techniques, especially in solving\nhigh dimensional problems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 12:58:41 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 04:18:05 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Radivojevi\u0107", "Tijana", ""], ["Akhmatskaya", "Elena", ""]]}, {"id": "1706.04059", "submitter": "Yohann De Castro", "authors": "Yohann De Castro, Fabrice Gamboa, Didier Henrion, Roxana Hess,\n  Jean-Bernard Lasserre", "title": "Approximate Optimal Designs for Multivariate Polynomial Regression", "comments": "30 Pages, 8 Figures. arXiv admin note: substantial text overlap with\n  arXiv:1703.01777", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.NA stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach aiming at computing approximate optimal designs\nfor multivariate polynomial regressions on compact (semi-algebraic) design\nspaces. We use the moment-sum-of-squares hierarchy of semidefinite programming\nproblems to solve numerically the approximate optimal design problem. The\ngeometry of the design is recovered via semidefinite programming duality\ntheory. This article shows that the hierarchy converges to the approximate\noptimal design as the order of the hierarchy increases. Furthermore, we provide\na dual certificate ensuring finite convergence of the hierarchy and showing\nthat the approximate optimal design can be computed numerically with our\nmethod. As a byproduct, we revisit the equivalence theorem of the experimental\ndesign theory: it is linked to the Christoffel polynomial and it characterizes\nfinite convergence of the moment-sum-of-square hierarchies.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 19:19:36 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 13:03:59 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 12:50:56 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["De Castro", "Yohann", ""], ["Gamboa", "Fabrice", ""], ["Henrion", "Didier", ""], ["Hess", "Roxana", ""], ["Lasserre", "Jean-Bernard", ""]]}, {"id": "1706.04152", "submitter": "Joseph Futoma", "authors": "Joseph Futoma, Sanjay Hariharan, Katherine Heller", "title": "Learning to Detect Sepsis with a Multitask Gaussian Process RNN\n  Classifier", "comments": "Presented at 34th International Conference on Machine Learning (ICML\n  2017), Sydney, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable end-to-end classifier that uses streaming physiological\nand medication data to accurately predict the onset of sepsis, a\nlife-threatening complication from infections that has high mortality and\nmorbidity. Our proposed framework models the multivariate trajectories of\ncontinuous-valued physiological time series using multitask Gaussian processes,\nseamlessly accounting for the high uncertainty, frequent missingness, and\nirregular sampling rates typically associated with real clinical data. The\nGaussian process is directly connected to a black-box classifier that predicts\nwhether a patient will become septic, chosen in our case to be a recurrent\nneural network to account for the extreme variability in the length of patient\nencounters. We show how to scale the computations associated with the Gaussian\nprocess in a manner so that the entire system can be discriminatively trained\nend-to-end using backpropagation. In a large cohort of heterogeneous inpatient\nencounters at our university health system we find that it outperforms several\nbaselines at predicting sepsis, and yields 19.4% and 55.5% improved areas under\nthe Receiver Operating Characteristic and Precision Recall curves as compared\nto the NEWS score currently used by our hospital.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 16:42:01 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Futoma", "Joseph", ""], ["Hariharan", "Sanjay", ""], ["Heller", "Katherine", ""]]}, {"id": "1706.04370", "submitter": "Giona Casiraghi", "authors": "Giona Casiraghi, Vahan Nanumyan, Ingo Scholtes, Frank Schweitzer", "title": "From Relational Data to Graphs: Inferring Significant Links using\n  Generalized Hypergeometric Ensembles", "comments": "10 pages, 8 figures, accepted at SocInfo2017", "journal-ref": "Social Informatics. SocInfo (2017) Lecture Notes in Computer\n  Science, vol 10540", "doi": "10.1007/978-3-319-67256-4_11", "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inference of network topologies from relational data is an important\nproblem in data analysis. Exemplary applications include the reconstruction of\nsocial ties from data on human interactions, the inference of gene\nco-expression networks from DNA microarray data, or the learning of semantic\nrelationships based on co-occurrences of words in documents. Solving these\nproblems requires techniques to infer significant links in noisy relational\ndata. In this short paper, we propose a new statistical modeling framework to\naddress this challenge. It builds on generalized hypergeometric ensembles, a\nclass of generative stochastic models that give rise to analytically tractable\nprobability spaces of directed, multi-edge graphs. We show how this framework\ncan be used to assess the significance of links in noisy relational data. We\nillustrate our method in two data sets capturing spatio-temporal proximity\nrelations between actors in a social system. The results show that our\nanalytical framework provides a new approach to infer significant links from\nrelational data, with interesting perspectives for the mining of data on social\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 09:11:56 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 10:38:56 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Casiraghi", "Giona", ""], ["Nanumyan", "Vahan", ""], ["Scholtes", "Ingo", ""], ["Schweitzer", "Frank", ""]]}, {"id": "1706.04416", "submitter": "Reza Mohammadi", "authors": "Reza Mohammadi, Helene Massam, Gerard Letac", "title": "Accelerating Bayesian Structure Learning in Sparse Gaussian Graphical\n  Models", "comments": "59 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models are relevant tools to learn conditional\nindependence structure between variables. In this class of models, Bayesian\nstructure learning is often done by search algorithms over the graph space. The\nconjugate prior for the precision matrix satisfying graphical constraints is\nthe well-known G-Wishart. With this prior, the transition probabilities in the\nsearch algorithms necessitate evaluating the ratios of the prior normalizing\nconstants of G-Wishart. In moderate to high-dimensions, this ratio is often\napproximated using sampling-based methods as computationally expensive updates\nin the search algorithm. Calculating this ratio so far has been a major\ncomputational bottleneck. We overcome this issue by representing a search\nalgorithm in which the ratio of normalizing constant is carried out by an\nexplicit closed-form approximation. Using this approximation within our search\nalgorithm yields significant improvement in the scalability of structure\nlearning without sacrificing structure learning accuracy. We study the\nconditions under which the approximation is valid. We also evaluate the\nefficacy of our method with simulation studies. We show that the new search\nalgorithm with our approximation outperforms state-of-the-art methods in both\ncomputational efficiency and accuracy. The implementation of our work is\navailable in the R package BDgraph.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 11:41:06 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 12:26:11 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 14:38:50 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Mohammadi", "Reza", ""], ["Massam", "Helene", ""], ["Letac", "Gerard", ""]]}, {"id": "1706.04651", "submitter": "John Hughes", "authors": "John Hughes", "title": "Spatial Regression and the Bayesian Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression for spatially dependent outcomes poses many challenges, for\ninference and for computation. Non-spatial models and traditional spatial\nmixed-effects models each have their advantages and disadvantages, making it\ndifficult for practitioners to determine how to carry out a spatial regression\nanalysis. We discuss the data-generating mechanisms implicitly assumed by\nvarious popular spatial regression models, and discuss the implications of\nthese assumptions. We propose Bayesian spatial filtering as an approximate\nmiddle way between non-spatial models and traditional spatial mixed models. We\nshow by simulation that our Bayesian spatial filtering model has several\ndesirable properties and hence may be a useful addition to a spatial\nstatistician's toolkit.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 19:42:35 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 21:33:45 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Hughes", "John", ""]]}, {"id": "1706.04677", "submitter": "Emmanuel Candes", "authors": "Matteo Sesia, Chiara Sabatti, Emmanuel J. Cand\\`es", "title": "Gene Hunting with Knockoffs for Hidden Markov Models", "comments": "35 pages, 13 figues, 9 tables", "journal-ref": "Biometrika, Volume 106, Issue 1, 1 March 2019, Pages 1-18", "doi": "10.1093/biomet/asy033", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern scientific studies often require the identification of a subset of\nrelevant explanatory variables, in the attempt to understand an interesting\nphenomenon. Several statistical methods have been developed to automate this\ntask, but only recently has the framework of model-free knockoffs proposed a\ngeneral solution that can perform variable selection under rigorous type-I\nerror control, without relying on strong modeling assumptions. In this paper,\nwe extend the methodology of model-free knockoffs to a rich family of problems\nwhere the distribution of the covariates can be described by a hidden Markov\nmodel (HMM). We develop an exact and efficient algorithm to sample knockoff\ncopies of an HMM. We then argue that combined with the knockoffs selective\nframework, they provide a natural and powerful tool for performing principled\ninference in genome-wide association studies with guaranteed FDR control.\nFinally, we apply our methodology to several datasets aimed at studying the\nCrohn's disease and several continuous phenotypes, e.g. levels of cholesterol.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 21:42:12 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Sesia", "Matteo", ""], ["Sabatti", "Chiara", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1706.04692", "submitter": "Dean Eckles", "authors": "Dean Eckles, Eytan Bakshy", "title": "Bias and high-dimensional adjustment in observational studies of peer\n  effects", "comments": "25 pages, 3 figures, 2 tables; supplementary information as ancillary\n  file", "journal-ref": "Journal of the American Statistical Association (2020)", "doi": "10.1080/01621459.2020.1796393", "report-no": null, "categories": "stat.ME cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer effects, in which the behavior of an individual is affected by the\nbehavior of their peers, are posited by multiple theories in the social\nsciences. Other processes can also produce behaviors that are correlated in\nnetworks and groups, thereby generating debate about the credibility of\nobservational (i.e. nonexperimental) studies of peer effects. Randomized field\nexperiments that identify peer effects, however, are often expensive or\ninfeasible. Thus, many studies of peer effects use observational data, and\nprior evaluations of causal inference methods for adjusting observational data\nto estimate peer effects have lacked an experimental \"gold standard\" for\ncomparison. Here we show, in the context of information and media diffusion on\nFacebook, that high-dimensional adjustment of a nonexperimental control group\n(677 million observations) using propensity score models produces estimates of\npeer effects statistically indistinguishable from those from using a large\nrandomized experiment (220 million observations). Naive observational\nestimators overstate peer effects by 320% and commonly used variables (e.g.,\ndemographics) offer little bias reduction, but adjusting for a measure of prior\nbehaviors closely related to the focal behavior reduces bias by 91%.\nHigh-dimensional models adjusting for over 3,700 past behaviors provide\nadditional bias reduction, such that the full model reduces bias by over 97%.\nThis experimental evaluation demonstrates that detailed records of individuals'\npast behavior can improve studies of social influence, information diffusion,\nand imitation; these results are encouraging for the credibility of some\nstudies but also cautionary for studies of rare or new behaviors. More\ngenerally, these results show how large, high-dimensional data sets and\nstatistical learning techniques can be used to improve causal inference in the\nbehavioral sciences.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 23:21:37 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Eckles", "Dean", ""], ["Bakshy", "Eytan", ""]]}, {"id": "1706.04919", "submitter": "Benjamin Deonovic", "authors": "Benjamin E. Deonovic and Brian J. Smith", "title": "Convergence diagnostics for MCMC draws of a categorical variable", "comments": "36 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) is a popular class of statistical methods for\nsimulating autocorrelated draws from target distributions, including posterior\ndistributions in Bayesian analysis. An important consideration in using\nsimulated MCMC draws for inference is that the sampling algorithm has converged\nto the distribution of interest. Since the distribution is typically of a\nnon-standard form, convergence cannot generally be proven and, instead, is\nassessed with convergence diagnostics. Although parameters used in the MCMC\nframework are typically continuous, there are many situations in which\nsimulating a categorical variable is desired. Examples include indicators for\nmodel inclusion in Bayesian variable selection and latent categorical component\nvariables in mixture modeling. Traditional convergence diagnostics are designed\nfor continuous variables and may be inappropriate for categorical variables. In\nthis paper two convergence diagnostic methods are considered which are\nappropriate for MCMC data. The diagnostics discussed in the paper utilize\nchi-squared test statistics for dependent data. Performance of the convergence\ndiagnostics is evaluated under various simulations. Finally, the diagnostics\nare applied to a real data set where reversible jump MCMC is used to sample\nfrom a finite mixture model.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 15:18:14 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Deonovic", "Benjamin E.", ""], ["Smith", "Brian J.", ""]]}, {"id": "1706.05029", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "M\\'onica Benito, Eduardo Garc\\'ia-Portugu\\'es, J. S. Marron, Daniel\n  Pe\\~na", "title": "Distance weighted discrimination of face images for gender\n  classification", "comments": "9 pages, 4 figures, 1 table", "journal-ref": "Stat, 6:231-240, 2017", "doi": "10.1002/sta4.151", "report-no": null, "categories": "stat.AP cs.CV stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We illustrate the advantages of distance weighted discrimination for\nclassification and feature extraction in a High Dimension Low Sample Size\n(HDLSS) situation. The HDLSS context is a gender classification problem of face\nimages in which the dimension of the data is several orders of magnitude larger\nthan the sample size. We compare distance weighted discrimination with Fisher's\nlinear discriminant, support vector machines, and principal component analysis\nby exploring their classification interpretation through insightful\nvisuanimations and by examining the classifiers' discriminant errors. This\nanalysis enables us to make new contributions to the understanding of the\ndrivers of human discrimination between males and females.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 18:25:50 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 10:14:26 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Benito", "M\u00f3nica", ""], ["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Marron", "J. S.", ""], ["Pe\u00f1a", "Daniel", ""]]}, {"id": "1706.05030", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Davy Paindaveine, Thomas Verdebout", "title": "On optimal tests for rotational symmetry against new classes of\n  hyperspherical distributions", "comments": "22 pages, 6 figures. Supplementary material: 14 pages, 2 figures", "journal-ref": "Journal of the American Statistical Association,\n  115(532):1873-1887, 2020", "doi": "10.1080/01621459.2019.1665527", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivated by the central role played by rotationally symmetric distributions\nin directional statistics, we consider the problem of testing rotational\nsymmetry on the hypersphere. We adopt a semiparametric approach and tackle\nproblems where the location of the symmetry axis is either specified or\nunspecified. For each problem, we define two tests and study their asymptotic\nproperties under very mild conditions. We introduce two new classes of\ndirectional distributions that extend the rotationally symmetric class and are\nof independent interest. We prove that each test is locally asymptotically\nmaximin, in the Le Cam sense, for one kind of the alternatives given by the new\nclasses of distributions, both for specified and unspecified symmetry axis. The\ntests, aimed to detect location-like and scatter-like alternatives, are\ncombined into convenient hybrid tests that are consistent against both\nalternatives. We perform Monte Carlo experiments that illustrate the\nfinite-sample performances of the proposed tests and their agreement with the\nasymptotic results. Finally, the practical relevance of our tests is\nillustrated on a real data application from astronomy. The R package rotasym\nimplements the proposed tests and allows practitioners to reproduce the data\napplication.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 18:26:52 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 13:40:01 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 16:32:59 GMT"}, {"version": "v4", "created": "Mon, 21 Sep 2020 10:18:19 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Paindaveine", "Davy", ""], ["Verdebout", "Thomas", ""]]}, {"id": "1706.05128", "submitter": "Alvaro E. Cordero-Franco", "authors": "Mar\\'ia I. Salazar-Alvarez, V\\'ictor G. Tercero-G\\'omez, Alvaro E.\n  Cordero-Franco, William J. Conover and Mario G. Beruvides", "title": "A Quantile Estimate Based on Local Curve Fitting", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile estimation is a problem presented in fields such as quality control,\nhydrology, and economics. There are different techniques to estimate such\nquantiles. Nevertheless, these techniques use an overall fit of the sample when\nthe quantiles of interest are usually located in the tails of the distribution.\nRegression Approach for Quantile Estimation (RAQE) is a method based on\nregression techniques and the properties of the empirical distribution to\naddress this problem. The method was first presented for the problem of\ncapability analysis. In this paper, a generalization of the method is\npresented, extended to the multiple sample scenario, and data from real\nexamples is used to illustrate the proposed approaches. In addition,\ntheoretical framework is presented to support the extension for multiple\nhomogeneous samples and the use of the uncertainty of the estimated\nprobabilities as a weighting factor in the analysis.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 01:53:20 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Salazar-Alvarez", "Mar\u00eda I.", ""], ["Tercero-G\u00f3mez", "V\u00edctor G.", ""], ["Cordero-Franco", "Alvaro E.", ""], ["Conover", "William J.", ""], ["Beruvides", "Mario G.", ""]]}, {"id": "1706.05280", "submitter": "Gregor Kastner", "authors": "Gregor Kastner, Sylvia Fr\\\"uhwirth-Schnatter", "title": "Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC\n  Estimation of Stochastic Volatility Models", "comments": null, "journal-ref": "Computational Statistics & Data Analysis 76, 408-423 (2014)", "doi": "10.1016/j.csda.2013.01.002", "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for stochastic volatility models using MCMC methods highly\ndepends on actual parameter values in terms of sampling efficiency. While draws\nfrom the posterior utilizing the standard centered parameterization break down\nwhen the volatility of volatility parameter in the latent state equation is\nsmall, non-centered versions of the model show deficiencies for highly\npersistent latent variable series. The novel approach of\nancillarity-sufficiency interweaving has recently been shown to aid in\novercoming these issues for a broad class of multilevel models. In this paper,\nwe demonstrate how such an interweaving strategy can be applied to stochastic\nvolatility models in order to greatly improve sampling efficiency for all\nparameters and throughout the entire parameter range. Moreover, this method of\n\"combining best of different worlds\" allows for inference for parameter\nconstellations that have previously been infeasible to estimate without the\nneed to select a particular parameterization beforehand.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 14:11:43 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Kastner", "Gregor", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""]]}, {"id": "1706.05378", "submitter": "Fanny Yang", "authors": "Fanny Yang, Aaditya Ramdas, Kevin Jamieson, Martin J. Wainwright", "title": "A framework for Multi-A(rmed)/B(andit) testing with online FDR control", "comments": "Published as a conference paper at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an alternative framework to existing setups for controlling false\nalarms when multiple A/B tests are run over time. This setup arises in many\npractical applications, e.g. when pharmaceutical companies test new treatment\noptions against control pills for different diseases, or when internet\ncompanies test their default webpages versus various alternatives over time.\nOur framework proposes to replace a sequence of A/B tests by a sequence of\nbest-arm MAB instances, which can be continuously monitored by the data\nscientist. When interleaving the MAB tests with an an online false discovery\nrate (FDR) algorithm, we can obtain the best of both worlds: low sample\ncomplexity and any time online FDR control. Our main contributions are: (i) to\npropose reasonable definitions of a null hypothesis for MAB instances; (ii) to\ndemonstrate how one can derive an always-valid sequential p-value that allows\ncontinuous monitoring of each MAB test; and (iii) to show that using rejection\nthresholds of online-FDR algorithms as the confidence levels for the MAB\nalgorithms results in both sample-optimality, high power and low FDR at any\npoint in time. We run extensive simulations to verify our claims, and also\nreport results on real data collected from the New Yorker Cartoon Caption\ncontest.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 18:00:00 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 07:25:12 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Yang", "Fanny", ""], ["Ramdas", "Aaditya", ""], ["Jamieson", "Kevin", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1706.05566", "submitter": "Philip Dawid", "authors": "Philip Dawid, Monica Musio, Rossella Murtas", "title": "The Probability of Causation", "comments": "14 pages, 2 tables, 7 figures", "journal-ref": "Law, Probability and Risk (2017) 16, 163-179", "doi": "10.1093/lpr/mgx012", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many legal cases require decisions about causality, responsibility or blame,\nand these may be based on statistical data. However, causal inferences from\nsuch data are beset by subtle conceptual and practical difficulties, and in\ngeneral it is, at best, possible to identify the \"probability of causation\" as\nlying between certain empirically informed limits. These limits can be refined\nand improved if we can obtain additional information, from statistical or\nscientific data, relating to the internal workings of the causal processes. In\nthis paper we review and extend recent work in this area, where additional\ninformation may be available on covariate and/or mediating variables.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 17:41:54 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dawid", "Philip", ""], ["Musio", "Monica", ""], ["Murtas", "Rossella", ""]]}, {"id": "1706.05661", "submitter": "Zeda Li", "authors": "Zeda Li and Robert T. Krafty", "title": "Adaptive Bayesian Power Spectrum Analysis of Multivariate Nonstationary\n  Time Series", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a nonparametric approach to multivariate time-varying\npower spectrum analysis. The procedure adaptively partitions a time series into\nan unknown number of approximately stationary segments, where some spectral\ncomponents may remain unchanged across segments, allowing components to evolve\ndifferently over time. Local spectra within segments are fit through Whittle\nlikelihood based penalized spline models of modified Cholesky components, which\nprovide flexible nonparametric estimates that preserve positive definite\nstructures of spectral matrices. The approach is formulated in a Bayesian\nframework, in which the number and location of partitions are random, and\nrelies on reversible jump Markov chain and Hamiltonian Monte Carlo methods that\ncan adapt to the unknown number of segments and parameters. By averaging over\nthe distribution of partitions, the approach can approximate both abrupt and\nslow-varying changes in spectral matrices. Empirical performance is evaluated\nin simulation studies and illustrated through analyses of\nelectroencephalography during sleep and of the El Ni\\~no-Southern Oscillation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 14:30:01 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 16:51:13 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Li", "Zeda", ""], ["Krafty", "Robert T.", ""]]}, {"id": "1706.05717", "submitter": "Vahid Tadayon", "authors": "Vahid Tadayon", "title": "Bayesian Analysis of Censored Spatial Data Based on a Non-Gaussian Model", "comments": null, "journal-ref": "Journal of Statistical Research of Iran. 2017, 13 (2) :155-180", "doi": "10.18869/acadpub.jsri.13.2.155", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we suggest using a skew Gaussian-log Gaussian model for the\nanalysis of spatial censored data from a Bayesian point of view. This approach\nfurnishes an extension of the skew log Gaussian model to accommodate to both\nskewness and heavy tails and also censored data. All of the characteristics\nmentioned are three pervasive features of spatial data. We utilize data\naugmentation method and Markov chain Monte Carlo (MCMC) algorithms to do\nposterior calculations. The methodology is illustrated using simulated data, as\nwell as applying it to a real data set. Keywords: Censored data, data\naugmentation, non-Gaussian spatial models, outlier, unified skew Gaussian.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 20:17:43 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 08:10:48 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Tadayon", "Vahid", ""]]}, {"id": "1706.05745", "submitter": "Somabha Mukherjee", "authors": "Arun Kumar Kuchibhotla, Somabha Mukherjee and Ayanendranath Basu", "title": "Statistical Inference based on Bridge Divergences", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  M-estimators offer simple robust alternatives to the maximum likelihood\nestimator. Much of the robustness literature, however, has focused on the\nproblems of location, location-scale and regression estimation rather than on\nestimation of general parameters. The density power divergence (DPD) and the\nlogarithmic density power divergence (LDPD) measures provide two classes of\ncompetitive M-estimators (obtained from divergences) in general parametric\nmodels which contain the MLE as a special case. In each of these families, the\nrobustness of the estimator is achieved through a density power down-weighting\nof outlying observations. Both the families have proved to be very useful tools\nin the area of robust inference. However, the relation and hierarchy between\nthe minimum distance estimators of the two families are yet to be\ncomprehensively studied or fully established. Given a particular set of real\ndata, how does one choose an optimal member from the union of these two classes\nof divergences? In this paper, we present a generalized family of divergences\nincorporating the above two classes; this family provides a smooth bridge\nbetween the DPD and the LDPD measures. This family helps to clarify and settle\nseveral longstanding issues in the relation between the important families of\nDPD and LDPD, apart from being an important tool in different areas of\nstatistical inference in its own right.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 23:23:10 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""], ["Mukherjee", "Somabha", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1706.05876", "submitter": "Claudio Agostinelli", "authors": "Claudio Agostinelli and Luca Greco", "title": "Weighted likelihood estimation of multivariate location and scatter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach to obtain weighted likelihood estimates of multivariate\nlocation and scatter is discussed. A weighting scheme is proposed that is based\non the distribution of the Mahalanobis distances rather than the distribution\nof the data at the assumed model. This strategy allows to avoid the curse of\ndimensionality affecting non-parametric density estimation, that is involved in\nthe construction of the weights through the Pearson residuals Markatou et al\n(1998). Then, weighted likelihood based outlier detection rules and robust\ndimensionality reduction techniques are developed. The effectiveness of the\nmethodology is illustrated through some numerical studies and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 10:59:54 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Agostinelli", "Claudio", ""], ["Greco", "Luca", ""]]}, {"id": "1706.05982", "submitter": "Christopher Walters", "authors": "Patrick Kline and Christopher R. Walters", "title": "On Heckits, LATE, and Numerical Equivalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural econometric methods are often criticized for being sensitive to\nfunctional form assumptions. We study parametric estimators of the local\naverage treatment effect (LATE) derived from a widely used class of latent\nthreshold crossing models and show they yield LATE estimates algebraically\nequivalent to the instrumental variables (IV) estimator. Our leading example is\nHeckman's (1979) two-step (\"Heckit\") control function estimator which, with\ntwo-sided non-compliance, can be used to compute estimates of a variety of\ncausal parameters. Equivalence with IV is established for a semi-parametric\nfamily of control function estimators and shown to hold at interior solutions\nfor a class of maximum likelihood estimators. Our results suggest differences\nbetween structural and IV estimates often stem from disagreements about the\ntarget parameter rather than from functional form assumptions per se. In cases\nwhere equivalence fails, reporting structural estimates of LATE alongside IV\nprovides a simple means of assessing the credibility of structural\nextrapolation exercises.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 14:21:38 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 04:17:12 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 01:00:35 GMT"}, {"version": "v4", "created": "Wed, 24 Oct 2018 23:54:29 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Kline", "Patrick", ""], ["Walters", "Christopher R.", ""]]}, {"id": "1706.06006", "submitter": "Ville Satopaa", "authors": "Ville A. Satop\\\"a\\\"a", "title": "Combining Information from Multiple Forecasters: Inefficiency of Central\n  Tendency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though the forecasting literature agrees that aggregating multiple\npredictions of some future outcome typically outperforms the individual\npredictions, there is no general consensus about the right way to do this. Most\ncommon aggregators are means, defined loosely as aggregators that always remain\nbetween the smallest and largest predictions. Examples include the arithmetic\nmean, trimmed means, median, mid-range, and many other measures of central\ntendency. If the forecasters use different information, the aggregator ideally\ncombines their information into a consensus without losing or distorting any of\nit. An aggregator that achieves this is considered efficient. Unfortunately,\nour results show that if the forecasters use their information accurately, an\naggregator that always remains strictly between the smallest and largest\npredictions is never efficient in practice. A similar result holds even if the\nideal predictions are distorted with random error that is centered at zero. If\nthese noisy predictions are aggregated with a similar notion of centrality,\nthen, under some mild conditions, the aggregator is asymptotically inefficient.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 15:18:54 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 17:49:43 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Satop\u00e4\u00e4", "Ville A.", ""]]}, {"id": "1706.06159", "submitter": "Dominik Rothenh\\\"ausler", "authors": "Dominik Rothenh\\\"ausler, Peter B\\\"uhlmann, Nicolai Meinshausen", "title": "Causal Dantzig: fast inference in linear structural equation models with\n  hidden variables under additive interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference is known to be very challenging when only observational data\nare available. Randomized experiments are often costly and impractical and in\ninstrumental variable regression the number of instruments has to exceed the\nnumber of causal predictors. It was recently shown in Peters et al. [2016] that\ncausal inference for the full model is possible when data from distinct\nobservational environments are available, exploiting that the conditional\ndistribution of a response variable is invariant under the correct causal\nmodel. Two shortcomings of such an approach are the high computational effort\nfor large-scale data and the assumed absence of hidden confounders. Here we\nshow that these two shortcomings can be addressed if one is willing to make a\nmore restrictive assumption on the type of interventions that generate\ndifferent environments. Thereby, we look at a different notion of invariance,\nnamely inner-product invariance. By avoiding a computationally cumbersome\nreverse-engineering approach such as in Peters et al. [2016], it allows for\nlarge-scale causal inference in linear structural equation models. We discuss\nidentifiability conditions for the causal parameter and derive asymptotic\nconfidence intervals in the low-dimensional setting. In the case of\nnon-identifiability we show that the solution set of causal Dantzig has\npredictive guarantees under certain interventions. We derive finite-sample\nbounds in the high-dimensional setting and investigate its performance on\nsimulated datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 20:10:57 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 17:01:46 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 18:25:16 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Rothenh\u00e4usler", "Dominik", ""], ["B\u00fchlmann", "Peter", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1706.06185", "submitter": "Paul McNicholas", "authors": "Yuhong Wei, Yang Tang and Paul D. McNicholas", "title": "Flexible High-Dimensional Unsupervised Learning with Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixture of factor analyzers (MFA) model is a famous mixture model-based\napproach for unsupervised learning with high-dimensional data. It can be\nuseful, inter alia, in situations where the data dimensionality far exceeds the\nnumber of observations. In recent years, the MFA model has been extended to\nnon-Gaussian mixtures to account for clusters with heavier tail weight and/or\nasymmetry. The generalized hyperbolic factor analyzers (MGHFA) model is one\nsuch extension, which leads to a flexible modelling paradigm that accounts for\nboth heavier tail weight and cluster asymmetry. In many practical applications,\nthe occurrence of missing values often complicates data analyses. A\ngeneralization of the MGHFA is presented to accommodate missing values. Under a\nmissing-at-random mechanism, we develop a computationally efficient alternating\nexpectation conditional maximization algorithm for parameter estimation of the\nMGHFA model with different patterns of missing values. The imputation of\nmissing values under an incomplete-data structure of MGHFA is also\ninvestigated. The performance of our proposed methodology is illustrated\nthrough the analysis of simulated and real data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 21:37:56 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 20:30:16 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Wei", "Yuhong", ""], ["Tang", "Yang", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1706.06288", "submitter": "Javier \\'Alvarez  Li\\'ebana", "authors": "J. \\'Alvarez-Li\\'ebana", "title": "A review and comparative study on functional time series techniques", "comments": "38 pages with 16 figures. Supplementary material is also included (30\n  pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA math.HO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper reviews the main estimation and prediction results derived in the\ncontext of functional time series, when Hilbert and Banach spaces are\nconsidered, specially, in the context of autoregressive processes of order one\n(ARH(1) and ARB(1) processes, for H and B being a Hilbert and Banach space,\nrespectively). Particularly, we pay attention to the estimation and prediction\nresults, and statistical tests, derived in both parametric and non-parametric\nframeworks. A comparative study between different ARH(1) prediction approaches\nis developed in the simulation study undertaken.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 07:09:19 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["\u00c1lvarez-Li\u00e9bana", "J.", ""]]}, {"id": "1706.06469", "submitter": "Colin Fogarty", "authors": "Colin B. Fogarty", "title": "On mitigating the analytical limitations of finely stratified\n  experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While attractive from a theoretical perspective, finely stratified\nexperiments such as paired designs suffer from certain analytical limitations\nnot present in block-randomized experiments with multiple treated and control\nindividuals in each block. In short, when using an appropriately weighted\ndifference-in-means to estimated the sample average treatment effect, the\ntraditional variance estimator in a paired experiment is conservative unless\nthe pairwise average treatment effects are constant across pairs; however, in\nmore coarsely stratified experiments, the corresponding variance estimator is\nunbiased if treatment effects are constant within blocks, even if they vary\nacross blocks. Using insights from classical least squares theory, we present\nan improved variance estimator appropriate in finely stratified experiments.\nThe variance estimator is still conservative in expectation for the true\nvariance of the difference-in-means estimator, but is asymptotically no larger\nthan the classical variance estimator under mild conditions. The improvements\nstem from the exploitation of effect modification, and thus the magnitude of\nthe improvement depends upon on the extent to which effect heterogeneity can be\nexplained by observed covariates. Aided by these estimators, a new test for the\nnull hypothesis of a constant treatment effect is proposed. These findings\nextend to some, but not all, super-population models, depending on whether or\nnot the covariates are viewed as fixed across samples in the super-population\nformulation under consideration.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 14:24:27 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Fogarty", "Colin B.", ""]]}, {"id": "1706.06611", "submitter": "Nicholas Henderson", "authors": "Nicholas C. Henderson, Thomas A.Louis, Gary L. Rosner and Ravi\n  Varadhan", "title": "Individualized Treatment Effects with Censored Data via Fully\n  Nonparametric Bayesian Accelerated Failure Time Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals often respond differently to identical treatments, and\ncharacterizing such variability in treatment response is an important aim in\nthe practice of personalized medicine. In this article, we describe a\nnon-parametric accelerated failure time model that can be used to analyze\nheterogeneous treatment effects (HTE) when patient outcomes are time-to-event.\nBy utilizing Bayesian additive regression trees and a mean-constrained\nDirichlet process mixture model, our approach offers a flexible model for the\nregression function while placing few restrictions on the baseline hazard. Our\nnon-parametric method leads to natural estimates of individual treatment effect\nand has the flexibility to address many major goals of HTE assessment.\nMoreover, our method requires little user input in terms of tuning parameter\nselection or subgroup specification. We illustrate the merits of our proposed\napproach with a detailed analysis of two large clinical trials for the\nprevention and treatment of congestive heart failure using an\nangiotensin-converting enzyme inhibitor. The analysis revealed considerable\nevidence for the presence of HTE in both trials as demonstrated by substantial\nestimated variation in treatment effect and by high proportions of patients\nexhibiting strong evidence of having treatment effects which differ from the\noverall treatment effect.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 18:07:48 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Henderson", "Nicholas C.", ""], ["Louis", "Thomas A.", ""], ["Rosner", "Gary L.", ""], ["Varadhan", "Ravi", ""]]}, {"id": "1706.06703", "submitter": "Hossein Moradi Rekabdarkolaee", "authors": "Hossein Moradi Rekabdarkolaee, Qin Wang, Zahra Naji, and Montserrat\n  Fuentes", "title": "New Parsimonious Multivariate Spatial Model: Spatial Envelope", "comments": null, "journal-ref": "Statistica Sinica, 2018", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction provides a useful tool for analyzing high dimensional\ndata. The recently developed \\textit{Envelope} method is a parsimonious version\nof the classical multivariate regression model through identifying a minimal\nreducing subspace of the responses. However, existing envelope methods assume\nan independent error structure in the model. While the assumption of\nindependence is convenient, it does not address the additional complications\nassociated with spatial or temporal correlations in the data. In this article,\nwe introduce a \\textit{Spatial Envelope} method for dimension reduction in the\npresence of dependencies across space. We study the asymptotic properties of\nthe proposed estimators and show that the asymptotic variance of the estimated\nregression coefficients under the spatial envelope model is smaller than that\nfrom the traditional maximum likelihood estimation. Furthermore, we present a\ncomputationally efficient approach for inference. The efficacy of the new\napproach is investigated through simulation studies and an analysis of an Air\nQuality Standard (AQS) dataset from the Environmental Protection Agency (EPA).\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 23:34:33 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 19:43:27 GMT"}, {"version": "v3", "created": "Fri, 20 Oct 2017 19:35:12 GMT"}, {"version": "v4", "created": "Mon, 4 Mar 2019 21:48:30 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Rekabdarkolaee", "Hossein Moradi", ""], ["Wang", "Qin", ""], ["Naji", "Zahra", ""], ["Fuentes", "Montserrat", ""]]}, {"id": "1706.06742", "submitter": "Stephane Robin", "authors": "Xiaoqiang Wang, Emilie Lebarbier, Julie Aubert and St\\'ephane Robin", "title": "Variational inference for coupled Hidden Markov Models applied to the\n  joint detection of copy number variations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models provide a natural statistical framework for the\ndetection of the copy number variations (CNV) in genomics. In this paper, we\nconsider a Hidden Markov Model involving several correlated hidden processes at\nthe same time. When dealing with a large number of series, maximum likelihood\ninference (performed classically using the EM algorithm) becomes intractable.\nWe thus propose an approximate inference algorithm based on a variational\napproach (VEM). A simulation study is performed to assess the performance of\nthe proposed method and an application to the detection of structural\nvariations in plant genomes is presented.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 05:27:29 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Wang", "Xiaoqiang", ""], ["Lebarbier", "Emilie", ""], ["Aubert", "Julie", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1706.06763", "submitter": "Brodie Lawson", "authors": "Brodie A. J. Lawson, Christopher C. Drovandi, Nicole Cusimano, Pamela\n  Burrage, Blanca Rodriguez, Kevin Burrage", "title": "Unlocking datasets by calibrating populations of models to data density:\n  a study in atrial electrophysiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The understanding of complex physical or biological systems nearly always\nrequires a characterisation of the variability that underpins these processes.\nIn addition, the data used to calibrate such models may also often exhibit\nconsiderable variability. A recent approach to deal with these issues has been\nto calibrate populations of models (POMs), that is multiple copies of a single\nmathematical model but with different parameter values. To date this\ncalibration has been limited to selecting models that produce outputs that fall\nwithin the ranges of the dataset, ignoring any trends that might be present in\nthe data. We present here a novel and general methodology for calibrating POMs\nto the distributions of a set of measured values in a dataset. We demonstrate\nthe benefits of our technique using a dataset from a cardiac atrial\nelectrophysiology study based on the differences in atrial action potential\nreadings between patients exhibiting sinus rhythm (SR) or chronic atrial\nfibrillation (cAF) and the Courtemanche--Ramirez--Nattel model for human atrial\naction potentials. Our approach accurately captures the variability inherent in\nthe experimental population, and allows us to identify the differences\nunderlying stratified data as well as the effects of drug block.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 07:08:25 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Lawson", "Brodie A. J.", ""], ["Drovandi", "Christopher C.", ""], ["Cusimano", "Nicole", ""], ["Burrage", "Pamela", ""], ["Rodriguez", "Blanca", ""], ["Burrage", "Kevin", ""]]}, {"id": "1706.06991", "submitter": "Qiang Sun", "authors": "Qiang Sun, Wenxin Zhou, and Jianqing Fan", "title": "Adaptive Huber Regression", "comments": "final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data can easily be contaminated by outliers or contain variables with\nheavy-tailed distributions, which makes many conventional methods inadequate.\nTo address this challenge, we propose the adaptive Huber regression for robust\nestimation and inference. The key observation is that the robustification\nparameter should adapt to the sample size, dimension and moments for optimal\ntradeoff between bias and robustness. Our theoretical framework deals with\nheavy-tailed distributions with bounded $(1+\\delta)$-th moment for any $\\delta\n> 0$. We establish a sharp phase transition for robust estimation of regression\nparameters in both low and high dimensions: when $\\delta \\geq 1$, the estimator\nadmits a sub-Gaussian-type deviation bound without sub-Gaussian assumptions on\nthe data, while only a slower rate is available in the regime $0<\\delta< 1$.\nFurthermore, this transition is smooth and optimal. In addition, we extend the\nmethodology to allow both heavy-tailed predictors and observation noise.\nSimulation studies lend further support to the theory. In a genetic study of\ncancer cell lines that exhibit heavy-tailedness, the proposed methods are shown\nto be more robust and predictive.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 16:28:15 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 04:54:05 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Sun", "Qiang", ""], ["Zhou", "Wenxin", ""], ["Fan", "Jianqing", ""]]}, {"id": "1706.07136", "submitter": "Daniele Marinazzo", "authors": "Luca Faes, Daniele Marinazzo, Sebastiano Stramaglia", "title": "Multiscale Information Decomposition: Exact Computation for Multivariate\n  Gaussian Processes", "comments": null, "journal-ref": "Entropy, 19, 408 (2017)", "doi": "10.3390/e19080408", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting the theory of state space models, we derive the exact expressions\nof the information transfer, as well as redundant and synergistic transfer, for\ncoupled Gaussian processes observed at multiple temporal scales. All of the\nterms, constituting the frameworks known as interaction information\ndecomposition and partial information decomposition, can thus be analytically\nobtained for different time scales from the parameters of the VAR model that\nfits the processes. We report the application of the proposed methodology\nfirstly to benchmark Gaussian systems, showing that this class of systems may\ngenerate patterns of information decomposition characterized by mainly\nredundant or synergistic information transfer persisting across multiple time\nscales or even by the alternating prevalence of redundant and synergistic\nsource interaction depending on the time scale. Then, we apply our method to an\nimportant topic in neuroscience, i.e., the detection of causal interactions in\nhuman epilepsy networks, for which we show the relevance of partial information\ndecomposition to the detection of multiscale information transfer spreading\nfrom the seizure onset zone.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 23:02:11 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 08:02:15 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Faes", "Luca", ""], ["Marinazzo", "Daniele", ""], ["Stramaglia", "Sebastiano", ""]]}, {"id": "1706.07194", "submitter": "Sylvia Fr\\\"uhwirth-Schnatter", "authors": "Sylvia Fr\\\"uhwirth-Schnatter and Gertraud Malsiner-Walli", "title": "From here to infinity - sparse finite versus Dirichlet process mixtures\n  in model-based clustering", "comments": "Accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model-based-clustering mixture models are used to group data points into\nclusters. A useful concept introduced for Gaussian mixtures by Malsiner Walli\net al (2016) are sparse finite mixtures, where the prior distribution on the\nweight distribution of a mixture with $K$ components is chosen in such a way\nthat a priori the number of clusters in the data is random and is allowed to be\nsmaller than $K$ with high probability. The number of cluster is then inferred\na posteriori from the data.\n  The present paper makes the following contributions in the context of sparse\nfinite mixture modelling. First, it is illustrated that the concept of sparse\nfinite mixture is very generic and easily extended to cluster various types of\nnon-Gaussian data, in particular discrete data and continuous multivariate data\narising from non-Gaussian clusters. Second, sparse finite mixtures are compared\nto Dirichlet process mixtures with respect to their ability to identify the\nnumber of clusters. For both model classes, a random hyper prior is considered\nfor the parameters determining the weight distribution. By suitable matching of\nthese priors, it is shown that the choice of this hyper prior is far more\ninfluential on the cluster solution than whether a sparse finite mixture or a\nDirichlet process mixture is taken into consideration.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 07:44:23 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 15:16:57 GMT"}, {"version": "v3", "created": "Wed, 22 Aug 2018 07:53:19 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Fr\u00fchwirth-Schnatter", "Sylvia", ""], ["Malsiner-Walli", "Gertraud", ""]]}, {"id": "1706.07408", "submitter": "Aur\\'elien Bibaut", "authors": "Aurelien F. Bibaut and Mark J. van der Laan", "title": "Data-adaptive smoothing for optimal-rate estimation of possibly\n  non-regular parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric inference of finite dimensional, potentially\nnon-pathwise differentiable target parameters. In a nonparametric model, some\nexamples of such parameters that are always non pathwise differentiable target\nparameters include probability density functions at a point, or regression\nfunctions at a point. In causal inference, under appropriate causal\nassumptions, mean counterfactual outcomes can be pathwise differentiable or\nnot, depending on the degree at which the positivity assumption holds.\n  In this paper, given a potentially non-pathwise differentiable target\nparameter, we introduce a family of approximating parameters, that are pathwise\ndifferentiable. This family is indexed by a scalar. In kernel regression or\ndensity estimation for instance, a natural choice for such a family is obtained\nby kernel smoothing and is indexed by the smoothing level. For the\ncounterfactual mean outcome, a possible approximating family is obtained\nthrough truncation of the propensity score, and the truncation level then plays\nthe role of the index.\n  We propose a method to data-adaptively select the index in the family, so as\nto optimize mean squared error. We prove an asymptotic normality result, which\nallows us to derive confidence intervals. Under some conditions, our estimator\nachieves an optimal mean squared error convergence rate. Confidence intervals\nare data-adaptive and have almost optimal width.\n  A simulation study demonstrates the practical performance of our estimators\nfor the inference of a causal dose-response curve at a given treatment dose.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 17:23:39 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 23:29:12 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Bibaut", "Aurelien F.", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1706.07449", "submitter": "Moritz Schauer", "authors": "Shota Gugushvili, Frank van der Meulen, Moritz Schauer, Peter Spreij", "title": "Nonparametric Bayesian estimation of a H\\\"older continuous diffusion\n  coefficient", "comments": null, "journal-ref": "Braz. J. Probab. Stat., 34(3): 537-579. 2020", "doi": "10.1214/19-BJPS433", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a nonparametric Bayesian approach to estimate the diffusion\ncoefficient of a stochastic differential equation given discrete time\nobservations over a fixed time interval. As a prior on the diffusion\ncoefficient, we employ a histogram-type prior with piecewise constant\nrealisations on bins forming a partition of the time interval. Specifically,\nthese constants are realizations of independent inverse Gamma distributed\nrandoma variables. We justify our approach by deriving the rate at which the\ncorresponding posterior distribution asymptotically concentrates around the\ndata-generating diffusion coefficient. This posterior contraction rate turns\nout to be optimal for estimation of a H\\\"older-continuous diffusion coefficient\nwith smoothness parameter $0<\\lambda\\leq 1.$ Our approach is straightforward to\nimplement, as the posterior distributions turn out to be inverse Gamma again,\nand leads to good practical results in a wide range of simulation examples.\nFinally, we apply our method on exchange rate data sets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 18:16:50 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 12:32:40 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 18:43:54 GMT"}, {"version": "v4", "created": "Thu, 19 Jul 2018 12:24:55 GMT"}, {"version": "v5", "created": "Wed, 30 Jan 2019 13:49:04 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Gugushvili", "Shota", ""], ["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""], ["Spreij", "Peter", ""]]}, {"id": "1706.07511", "submitter": "Muhammad Naveed Tabassum", "authors": "Muhammad Naveed Tabassum and Esa Ollila (Aalto University, Dept. of\n  Signal Processing and Acoustics)", "title": "Pathwise Least Angle Regression and a Significance Test for the Elastic\n  Net", "comments": "5 pages, 25th European Signal Processing Conference (EUSIPCO 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Least angle regression (LARS) by Efron et al. (2004) is a novel method for\nconstructing the piece-wise linear path of Lasso solutions. For several years,\nit remained also as the de facto method for computing the Lasso solution before\nmore sophisticated optimization algorithms preceded it. LARS method has\nrecently again increased its popularity due to its ability to find the values\nof the penalty parameters, called knots, at which a new parameter enters the\nactive set of non-zero coefficients. Significance test for the Lasso by\nLockhart et al. (2014), for example, requires solving the knots via the LARS\nalgorithm. Elastic net (EN), on the other hand, is a highly popular extension\nof Lasso that uses a linear combination of Lasso and ridge regression\npenalties. In this paper, we propose a new novel algorithm, called pathwise\n(PW-)LARS-EN, that is able to compute the EN knots over a grid of EN tuning\nparameter {\\alpha} values. The developed PW-LARS-EN algorithm decreases the EN\ntuning parameter and exploits the previously found knot values and the original\nLARS algorithm. A covariance test statistic for the Lasso is then generalized\nto the EN for testing the significance of the predictors. Our simulation\nstudies validate the fact that the test statistic has an asymptotic Exp(1)\ndistribution.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 22:24:11 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Tabassum", "Muhammad Naveed", "", "Aalto University, Dept. of\n  Signal Processing and Acoustics"], ["Ollila", "Esa", "", "Aalto University, Dept. of\n  Signal Processing and Acoustics"]]}, {"id": "1706.07581", "submitter": "Gael Varoquaux", "authors": "Ga\\\"el Varoquaux (PARIETAL)", "title": "Cross-validation failure: small sample sizes lead to large error bars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models ground many state-of-the-art developments in statistical\nbrain image analysis: decoding, MVPA, searchlight, or extraction of biomarkers.\nThe principled approach to establish their validity and usefulness is\ncross-validation, testing prediction on unseen data. Here, I would like to\nraise awareness on error bars of cross-validation, which are often\nunderestimated. Simple experiments show that sample sizes of many neuroimaging\nstudies inherently lead to large error bars, eg $\\pm$10% for 100 samples. The\nstandard error across folds strongly underestimates them. These large error\nbars compromise the reliability of conclusions drawn with predictive models,\nsuch as biomarkers or methods developments where, unlike with cognitive\nneuroimaging MVPA approaches, more samples cannot be acquired by repeating the\nexperiment across many subjects. Solutions to increase sample size must be\ninvestigated, tackling possible increases in heterogeneity of the data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 06:47:16 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Varoquaux", "Ga\u00ebl", "", "PARIETAL"]]}, {"id": "1706.07664", "submitter": "Lixing Zhu", "authors": "Falong Tan and Lixing Zhu", "title": "Estimation and adaptive-to-model testing for regressions with diverging\n  number of predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research described in this paper is motivated by model checking for\nparametric single-index models with diverging number of predictors. To\nconstruct a test statistic, we first study the asymptotic property of the\nestimators of involved parameters of interest under the null and alternative\nhypothesis when the dimension is divergent to infinity as the sample size goes\nto infinity. For the testing problem, we study an adaptive-to-model\nresidual-marked empirical process as the basis for constructing a test\nstatistic. By modifying the approach in the literature to suit the diverging\ndimension settings, we construct a martingale transformation. Under the null,\nlocal and global alternative hypothesis, the weak limits of the empirical\nprocess are derived and then the asymptotic properties of the test statistic\nare investigated. Simulation studies are carried out to examine the performance\nof the test.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 12:23:42 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Tan", "Falong", ""], ["Zhu", "Lixing", ""]]}, {"id": "1706.07681", "submitter": "Debasis Kundu Professor", "authors": "Debasis Kundu", "title": "Multivariate Geometric Skew-Normal Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Azzalini (1985) introduced a skew-normal distribution of which normal\ndistribution is a special case. Recently Kundu (2014) introduced a geometric\nskew-normal distribution and showed that it has certain advantages over\nAzzalini's skew-normal distribution. In this paper we discuss about the\nmultivariate geometric skew-normal distribution. It can be used as an\nalternative to Azzalini's skew normal distribution. We discuss different\nproperties of the proposed distribution. It is observed that the joint\nprobability density function of the multivariate geometric skew normal\ndistribution can take variety of shapes. Several characterization results have\nbeen established. Generation from a multivariate geometric skew normal\ndistribution is quite simple, hence the simulation experiments can be performed\nquite easily. The maximum likelihood estimators of the unknown parameters can\nbe obtained quite conveniently using expectation maximization (EM) algorithm.\nWe perform some simulation experiments and it is observed that the performances\nof the proposed EM algorithm are quite satisfactory. Further, the analyses of\ntwo data sets have been performed, and it is observed that the proposed methods\nand the model work very well.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 13:07:25 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Kundu", "Debasis", ""]]}, {"id": "1706.07682", "submitter": "Debasis Kundu Professor", "authors": "Shuvashree Mondal and Debasis Kundu", "title": "Point and Interval Estimation of Weibull Parameters Based on Joint\n  Progressively Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of progressively censored data has received considerable\nattention in the last few years. In this paper we consider the joint\nprogressive censoring scheme for two populations. It is assumed that the\nlifetime distribution of the items from the two populations follow Weibull\ndistribution with the same shape but different scale parameters. Based on the\njoint progressive censoring scheme first we consider the maximum likelihood\nestimators of the unknown parameters whenever they exist. We provide the\nBayesian inferences of the unknown parameters under a fairly general priors on\nthe shape and scale parameters. The Bayes estimators and the associated\ncredible intervals cannot be obtained in closed form, and we propose to use the\nimportance sampling technique to compute the same. Further, we consider the\nproblem when it is known apriori that the expected lifetime of one population\nis smaller than the other. We provide the order restricted classical and\nBayesian inferences of the unknown parameters. Monte Carlo simulations are\nperformed to observe the performances of the different estimators and the\nassociated confidence and credible intervals. One real data set has been\nanalyzed for illustrative purpose.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 13:15:46 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Mondal", "Shuvashree", ""], ["Kundu", "Debasis", ""]]}, {"id": "1706.07685", "submitter": "Cachimo Assane", "authors": "Cachimo Combo Assane, Basilio de Bragan\\c{c}a Pereira, Carlos Alberto\n  de Bragan\\c{c}a Pereira", "title": "Model choice in separate families: A comparison between the FBST and the\n  Cox test", "comments": "20 pages. arXiv admin note: substantial text overlap with\n  arXiv:1705.11073", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tests of separate families of hypotheses were initially considered by Cox\n(1961,1962) In this work, the Fully Bayesian Significance Test, FBST, is\nevaluated for discriminating between the lognormal, gamma and Weibull models\nwhose families of distributions are separate. Considering a linear mixture\nmodel including all candidate distributions, the FBST tests the hypotheses on\nthe mixture weights in order to calculate the evidence measure in favor of each\none. Additionally, the density functions of the mixture components are\nreparametrized in terms of the common parameters, the mean and the variance of\nthe population, since the comparison between the models is based on the same\ndataset, i.e, on the same population. Reparametrizing the models in terms of\nthe common parameters also allows one to reduce the number of the parameters to\nbe estimated. In order to evaluate the performance of the procedure, some\nnumerical results based on simulated sample points are given. In these\nsimulations, the results of FBST are compared with those of the Cox test. Two\napplications examples illustrating the procedure for uncensored dataset are\nalso presented.\n  Keywords: Model choice; Separate Models; Mixture model; Significance test;\nFBST; Cox Test\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 17:52:00 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Assane", "Cachimo Combo", ""], ["Pereira", "Basilio de Bragan\u00e7a", ""], ["Pereira", "Carlos Alberto de Bragan\u00e7a", ""]]}, {"id": "1706.07705", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami, Hajime Seya", "title": "Spatially filtered unconditional quantile regression: Application to a\n  hedonic analysis", "comments": "Paper accepted to Environmetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconditional quantile regression (UQR) attracts attention in various fields\nto investigate the impacts of explanatory variables on quantiles of the\nmarginal distribution of an explained variable. This study attempts to\nintroduce spatial dependence into the UQR within the framework of random\neffects eigenvector spatial filtering, resulting in the model that we term the\nspatially filtered UQR (SF-UQR). We then develop a computationally efficient\napproach for SF-UQR estimation. Finally, the performance of the SF-UQR is\ntested with a hedonic land price model for the Tokyo metropolitan area. SF-UQR\nis implemented in an R package, \"spmoran.\"\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 13:51:52 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 08:48:38 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 15:24:45 GMT"}, {"version": "v4", "created": "Tue, 18 Dec 2018 04:08:49 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Murakami", "Daisuke", ""], ["Seya", "Hajime", ""]]}, {"id": "1706.07712", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead", "title": "Asymptotics of ABC", "comments": "This document is due to appear as a chapter of the forthcoming\n  Handbook of Approximate Bayesian Computation (ABC) edited by S. Sisson, Y.\n  Fan, and M. Beaumont", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an informal review of recent work on the asymptotics of\nApproximate Bayesian Computation (ABC). In particular we focus on how does the\nABC posterior, or point estimates obtained by ABC, behave in the limit as we\nhave more data? The results we review show that ABC can perform well in terms\nof point estimation, but standard implementations will over-estimate the\nuncertainty about the parameters. If we use the regression correction of\nBeaumont et al. then ABC can also accurately quantify this uncertainty. The\ntheoretical results also have practical implications for how to implement ABC.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 14:01:34 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Fearnhead", "Paul", ""]]}, {"id": "1706.07767", "submitter": "Ding Xiang", "authors": "Ding Xiang, Galin L. Jones", "title": "Fully Bayesian Penalized Regression with a Generalized Bridge Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider penalized regression models under a unified framework where the\nparticular method is determined by the form of the penalty term. We propose a\nfully Bayesian approach that incorporates both sparse and dense settings and\nshow how to use a type of model averaging approach to eliminate the nuisance\npenalty parameters and perform inference through the marginal posterior\ndistribution of the regression coefficients. We establish tail robustness of\nthe resulting estimator as well as conditional and marginal posterior\nconsistency. We develop an efficient component-wise Markov chain Monte Carlo\nalgorithm for sampling. Numerical results show that the method tends to select\nthe optimal penalty and performs well in both variable selection and prediction\nand is comparable to, and often better than alternative methods. Both simulated\nand real data examples are provided.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 16:19:10 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 01:27:44 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 21:49:18 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Xiang", "Ding", ""], ["Jones", "Galin L.", ""]]}, {"id": "1706.07840", "submitter": "Neil Shephard", "authors": "Iavor Bojinov and Neil Shephard", "title": "Time series experiments and causal estimands: exact randomization tests\n  and trading", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2018.1527225", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define causal estimands for experiments on single time series, extending\nthe potential outcome framework to dealing with temporal data. Our approach\nallows the estimation of some of these estimands and exact randomization based\np-values for testing causal effects, without imposing stringent assumptions. We\ntest our methodology on simulated \"potential autoregressions,\"which have a\ncausal interpretation. Our methodology is partially inspired by data from a\nlarge number of experiments carried out by a financial company who compared the\nimpact of two different ways of trading equity futures contracts. We use our\nmethodology to make causal statements about their trading methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 19:14:38 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 17:35:30 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Bojinov", "Iavor", ""], ["Shephard", "Neil", ""]]}, {"id": "1706.08037", "submitter": "Simon Mak", "authors": "Simon Mak, Henry Shaowu Yushi, Yao Xie", "title": "Information-Guided Sampling for Low-Rank Matrix Completion", "comments": "ICML 2021 Workshop on Information-Theoretic Methods for Rigorous,\n  Responsible, and Reliable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The noisy matrix completion problem, which aims to recover a low-rank matrix\n$\\mathbf{X}$ from a partial, noisy observation of its entries, arises in many\nstatistical, machine learning, and engineering applications. In this paper, we\npresent a new, information-theoretic approach for active sampling (or\ndesigning) of matrix entries for noisy matrix completion, based on the maximum\nentropy design principle. One novelty of our method is that it implicitly makes\nuse of uncertainty quantification (UQ) -- a measure of uncertainty for\nunobserved matrix entries -- to guide the active sampling procedure. The\nproposed framework reveals several novel insights on the role of compressive\nsensing (e.g., coherence) and coding design (e.g., Latin squares) on the\nsampling performance and UQ for noisy matrix completion. Using such insights,\nwe develop an efficient posterior sampler for UQ, which is then used to guide a\nclosed-form sampling scheme for matrix entries. Finally, we illustrate the\neffectiveness of this integrated sampling / UQ methodology in simulation\nstudies and two applications to collaborative filtering.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 05:01:59 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 18:35:23 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 02:34:33 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Mak", "Simon", ""], ["Yushi", "Henry Shaowu", ""], ["Xie", "Yao", ""]]}, {"id": "1706.08058", "submitter": "Niklas Pfister", "authors": "Niklas Pfister, Peter B\\\"uhlmann and Jonas Peters", "title": "Invariant Causal Prediction for Sequential Data", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of inferring the causal predictors of a response\n$Y$ from a set of $d$ explanatory variables $(X^1,\\dots,X^d)$. Classical\nordinary least squares regression includes all predictors that reduce the\nvariance of $Y$. Using only the causal predictors instead leads to models that\nhave the advantage of remaining invariant under interventions, loosely speaking\nthey lead to invariance across different \"environments\" or \"heterogeneity\npatterns\". More precisely, the conditional distribution of $Y$ given its causal\npredictors remains invariant for all observations. Recent work exploits such a\nstability to infer causal relations from data with different but known\nenvironments. We show that even without having knowledge of the environments or\nheterogeneity pattern, inferring causal relations is possible for time-ordered\n(or any other type of sequentially ordered) data. In particular, this allows\ndetecting instantaneous causal relations in multivariate linear time series\nwhich is usually not the case for Granger causality. Besides novel methodology,\nwe provide statistical confidence bounds and asymptotic detection results for\ninferring causal predictors, and present an application to monetary policy in\nmacroeconomics.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 08:25:25 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 15:35:51 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Pfister", "Niklas", ""], ["B\u00fchlmann", "Peter", ""], ["Peters", "Jonas", ""]]}, {"id": "1706.08234", "submitter": "Hamdi Raissi", "authors": "Hamdi Ra\\\"issi", "title": "Testing normality for unconditionally heteroscedastic macroeconomic\n  variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the testing of normality for unconditionally heteroscedastic\nmacroeconomic time series is studied. It is underlined that the classical\nJarque-Bera test (JB hereafter) for normality is inadequate in our framework.\nOn the other hand it is found that the approach which consists in correcting\nthe heteroscedasticity by kernel smoothing for testing normality is justified\nasymptotically. Nevertheless it appears from Monte Carlo experiments that such\nmethodology can noticeably suffer from size distortion for samples that are\ntypical for macroeconomic variables. As a consequence a parametric bootstrap\nmethodology for correcting the problem is proposed. The innovations\ndistribution of a set of inflation measures for the U.S., Korea and Australia\nare analyzed.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 05:05:49 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Ra\u00efssi", "Hamdi", ""]]}, {"id": "1706.08269", "submitter": "Torsten Hothorn", "authors": "Torsten Hothorn", "title": "Top-down Transformation Choice", "comments": null, "journal-ref": "Statistical Modelling 2018", "doi": "10.1177/1471082X17748081", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simple models are preferred over complex models, but over-simplistic models\ncould lead to erroneous interpretations. The classical approach is to start\nwith a simple model, whose shortcomings are assessed in residual-based model\ndiagnostics. Eventually, one increases the complexity of this initial overly\nsimple model and obtains a better-fitting model. I illustrate how\ntransformation analysis can be used as an alternative approach to model choice.\nInstead of adding complexity to simple models, step-wise complexity reduction\nis used to help identify simpler and better-interpretable models. As an\nexample, body mass index distributions in Switzerland are modelled by means of\ntransformation models to understand the impact of sex, age, smoking and other\nlifestyle factors on a person's body mass index. In this process, I searched\nfor a compromise between model fit and model interpretability. Special emphasis\nis given to the understanding of the connections between transformation models\nof increasing complexity. The models used in this analysis ranged from\nevergreens, such as the normal linear regression model with constant variance,\nto novel models with extremely flexible conditional distribution functions,\nsuch as transformation trees and transformation forests.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 08:08:01 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 13:09:24 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hothorn", "Torsten", ""]]}, {"id": "1706.08289", "submitter": "Joris Chau", "authors": "Joris Chau, Hernando Ombao, Rainer von Sachs", "title": "Intrinsic data depth for Hermitian positive definite matrices", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics 28:2 (2019),\n  427-439", "doi": "10.1080/10618600.2018.1537926", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nondegenerate covariance, correlation and spectral density matrices are\nnecessarily symmetric or Hermitian and positive definite. The main contribution\nof this paper is the development of statistical data depths for collections of\nHermitian positive definite matrices by exploiting the geometric structure of\nthe space as a Riemannian manifold. The depth functions allow one to naturally\ncharacterize most central or outlying matrices, but also provide a practical\nframework for inference in the context of samples of positive definite\nmatrices. First, the desired properties of an intrinsic data depth function\nacting on the space of Hermitian positive definite matrices are presented.\nSecond, we propose two computationally fast pointwise and integrated data depth\nfunctions that satisfy each of these requirements and investigate several\nrobustness and efficiency aspects. As an application, we construct depth-based\nconfidence regions for the intrinsic mean of a sample of positive definite\nmatrices, which is applied to the exploratory analysis of a collection of\ncovariance matrices associated to a multicenter research trial.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 09:03:33 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 18:02:02 GMT"}, {"version": "v3", "created": "Mon, 9 Apr 2018 18:14:22 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Chau", "Joris", ""], ["Ombao", "Hernando", ""], ["von Sachs", "Rainer", ""]]}, {"id": "1706.08327", "submitter": "Florian Maire", "authors": "Florian Maire, Nial Friel, Pierre Alquier", "title": "Informed Sub-Sampling MCMC: Approximate Bayesian Inference for Large\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a framework for speeding up Bayesian inference\nconducted in presence of large datasets. We design a Markov chain whose\ntransition kernel uses an (unknown) fraction of (fixed size) of the available\ndata that is randomly refreshed throughout the algorithm. Inspired by the\nApproximate Bayesian Computation (ABC) literature, the subsampling process is\nguided by the fidelity to the observed data, as measured by summary statistics.\nThe resulting algorithm, Informed Sub-Sampling MCMC (ISS-MCMC), is a generic\nand flexible approach which, contrary to existing scalable methodologies,\npreserves the simplicity of the Metropolis-Hastings algorithm. Even though\nexactness is lost, i.e. the chain distribution approximates the posterior, we\nstudy and quantify theoretically this bias and show on a diverse set of\nexamples that it yields excellent performances when the computational budget is\nlimited. If available and cheap to compute, we show that setting the summary\nstatistics as the maximum likelihood estimator is supported by theoretical\narguments.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 11:24:51 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 00:43:36 GMT"}, {"version": "v3", "created": "Thu, 31 May 2018 09:20:50 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Maire", "Florian", ""], ["Friel", "Nial", ""], ["Alquier", "Pierre", ""]]}, {"id": "1706.08418", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Iv\\'an Fern\\'andez-Val and Whitney Newey", "title": "Nonseparable Multinomial Choice Models in Cross-Section and Panel Data", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multinomial choice models are fundamental for empirical modeling of economic\nchoices among discrete alternatives. We analyze identification of binary and\nmultinomial choice models when the choice utilities are nonseparable in\nobserved attributes and multidimensional unobserved heterogeneity with\ncross-section and panel data. We show that derivatives of choice probabilities\nwith respect to continuous attributes are weighted averages of utility\nderivatives in cross-section models with exogenous heterogeneity. In the\nspecial case of random coefficient models with an independent additive effect,\nwe further characterize that the probability derivative at zero is proportional\nto the population mean of the coefficients. We extend the identification\nresults to models with endogenous heterogeneity using either a control function\nor panel data. In time stationary panel models with two periods, we find that\ndifferences over time of derivatives of choice probabilities identify utility\nderivatives \"on the diagonal,\" i.e. when the observed attributes take the same\nvalues in the two periods. We also show that time stationarity does not\nidentify structural derivatives \"off the diagonal\" both in continuous and\nmultinomial choice panel models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 14:46:45 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 15:07:50 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Newey", "Whitney", ""]]}, {"id": "1706.08576", "submitter": "Christina Heinze-Deml", "authors": "Christina Heinze-Deml, Jonas Peters and Nicolai Meinshausen", "title": "Invariant Causal Prediction for Nonlinear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in many domains is to predict how a system will respond\nto interventions. This task is inherently linked to estimating the system's\nunderlying causal structure. To this end, Invariant Causal Prediction (ICP)\n(Peters et al., 2016) has been proposed which learns a causal model exploiting\nthe invariance of causal relations using data from different environments. When\nconsidering linear models, the implementation of ICP is relatively\nstraightforward. However, the nonlinear case is more challenging due to the\ndifficulty of performing nonparametric tests for conditional independence. In\nthis work, we present and evaluate an array of methods for nonlinear and\nnonparametric versions of ICP for learning the causal parents of given target\nvariables. We find that an approach which first fits a nonlinear model with\ndata pooled over all environments and then tests for differences between the\nresidual distributions across environments is quite robust across a large\nvariety of simulation settings. We call this procedure \"invariant residual\ndistribution test\". In general, we observe that the performance of all\napproaches is critically dependent on the true (unknown) causal structure and\nit becomes challenging to achieve high power if the parental set includes more\nthan two variables. As a real-world example, we consider fertility rate\nmodelling which is central to world population projections. We explore\npredicting the effect of hypothetical interventions using the accepted models\nfrom nonlinear ICP. The results reaffirm the previously observed central causal\nrole of child mortality rates.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 19:58:09 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 20:07:01 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Heinze-Deml", "Christina", ""], ["Peters", "Jonas", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1706.08757", "submitter": "Lizhen Lin", "authors": "Lizhen Lin, Mu Niu, Pokman Cheung and David Dunson", "title": "Extrinsic Gaussian processes for regression and classification on\n  manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are very widely used for modeling of unknown\nfunctions or surfaces in applications ranging from regression to classification\nto spatial processes. Although there is an increasingly vast literature on\napplications, methods, theory and algorithms related to GPs, the overwhelming\nmajority of this literature focuses on the case in which the input domain\ncorresponds to a Euclidean space. However, particularly in recent years with\nthe increasing collection of complex data, it is commonly the case that the\ninput domain does not have such a simple form. For example, it is common for\nthe inputs to be restricted to a non-Euclidean manifold, a case which forms the\nmotivation for this article. In particular, we propose a general extrinsic\nframework for GP modeling on manifolds, which relies on embedding of the\nmanifold into a Euclidean space and then constructing extrinsic kernels for GPs\non their images. These extrinsic Gaussian processes (eGPs) are used as prior\ndistributions for unknown functions in Bayesian inferences. Our approach is\nsimple and general, and we show that the eGPs inherit fine theoretical\nproperties from GP models in Euclidean spaces. We consider applications of our\nmodels to regression and classification problems with predictors lying in a\nlarge class of manifolds, including spheres, planar shape spaces, a space of\npositive definite matrices, and Grassmannians. Our models can be readily used\nby practitioners in biological sciences for various regression and\nclassification problems, such as disease diagnosis or detection. Our work is\nalso likely to have impact in spatial statistics when spatial locations are on\nthe sphere or other geometric spaces.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 09:53:03 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Lin", "Lizhen", ""], ["Niu", "Mu", ""], ["Cheung", "Pokman", ""], ["Dunson", "David", ""]]}, {"id": "1706.08822", "submitter": "Przemyslaw Biecek", "authors": "Przemyslaw Biecek, Marcin Kosinski", "title": "archivist: An R Package for Managing, Recording and Restoring Data\n  Analysis Results", "comments": "Submitted to JSS in 2015, conditionally accepted", "journal-ref": "Journal of Statistical Software (2017) vol 82 (11)", "doi": "10.18637/jss.v082.i11", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Everything that exists in R is an object [Chambers2016]. This article\nexamines what would be possible if we kept copies of all R objects that have\never been created. Not only objects but also their properties, meta-data,\nrelations with other objects and information about context in which they were\ncreated.\n  We introduce archivist, an R package designed to improve the management of\nresults of data analysis. Key functionalities of this package include: (i)\nmanagement of local and remote repositories which contain R objects and their\nmeta-data (objects' properties and relations between them); (ii) archiving R\nobjects to repositories; (iii) sharing and retrieving objects (and it's\npedigree) by their unique hooks; (iv) searching for objects with specific\nproperties or relations to other objects; (v) verification of object's identity\nand context of it's creation.\n  The presented archivist package extends, in a combination with packages such\nas knitr and Sweave, the reproducible research paradigm by creating new ways to\nretrieve and validate previously calculated objects. These new features give a\nvariety of opportunities such as: sharing R objects within reports or articles;\nadding hooks to R objects in table or figure captions; interactive exploration\nof object repositories; caching function calls with their results; retrieving\nobject's pedigree (information about how the object was created); automated\ntracking of the performance of considered models, restoring R libraries to the\nstate in which object was archived.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 12:44:39 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Biecek", "Przemyslaw", ""], ["Kosinski", "Marcin", ""]]}, {"id": "1706.08881", "submitter": "Joshua Chang", "authors": "Joshua C. Chang", "title": "Predictive Bayesian selection of multistep Markov chains, applied to the\n  detection of the hot hand and other statistical dependencies in free throws", "comments": "Preprint before final acceptance to RSOS. arXiv admin note: text\n  overlap with arXiv:1702.06221", "journal-ref": "R. Soc. open sci, 6(182174) 2019", "doi": "10.1098/rsos.182174", "report-no": null, "categories": "stat.ME physics.pop-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of modeling memory effects in discrete-state random\nwalks using higher-order Markov chains. This paper explores cross validation\nand information criteria as proxies for a model's predictive accuracy. Our\nobjective is to select, from data, the number of prior states of recent history\nupon which a trajectory is statistically dependent. Through simulations, I\nevaluate these criteria in the case where data are drawn from systems with\nfixed orders of history, noting trends in the relative performance of the\ncriteria. As a real-world illustrative example of these methods, this\nmanuscript evaluates the problem of detecting statistical dependencies in shot\noutcomes in free throw shooting. Over three NBA seasons analyzed, several\nplayers exhibited statistical dependencies in free throw hitting probability of\nvarious types - hot handedness, cold handedness, and error correction. For the\n2013-2014 through 2015-2016 NBA seasons, I detected statistical dependencies in\n23% of all player-seasons. Focusing on a single player, in two of these three\nseasons, LeBron James shot a better percentage after an immediate miss than\notherwise. In those seasons, conditioning on the previous outcome makes for a\nmore predictive model than treating free throw makes as independent. When\nextended to data from the 2016-2017 NBA season specifically for LeBron James, a\nmodel depending on the previous shot (single-step Markovian) does not clearly\nbeat a model with independent outcomes. An error-correcting variable length\nmodel of two parameters, where James shoots a higher percentage after a missed\nfree throw than otherwise, is more predictive than either model.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 14:35:36 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 16:54:14 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2019 20:32:39 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Chang", "Joshua C.", ""]]}, {"id": "1706.08894", "submitter": "Mohamed Laib", "authors": "Mohamed Laib and Mikhail Kanevski", "title": "Unsupervised Feature Selection Based on Space Filling Concept", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with the adaptation of a new measure for the unsupervised\nfeature selection problems. The proposed measure is based on space filling\nconcept and is called the coverage measure. This measure was used for judging\nthe quality of an experimental space filling design. In the present work, the\ncoverage measure is adapted for selecting the smallest informative subset of\nvariables by reducing redundancy in data. This paper proposes a simple analogy\nto apply this measure. It is implemented in a filter algorithm for unsupervised\nfeature selection problems.\n  The proposed filter algorithm is robust with high dimensional data and can be\nimplemented without extra parameters. Further, it is tested with simulated data\nand real world case studies including environmental data and hyperspectral\nimage. Finally, the results are evaluated by using random forest algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 14:48:39 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Laib", "Mohamed", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1706.08927", "submitter": "Angelina Pesevski", "authors": "Angelina Pesevski, Brian C. Franczak, Paul D. McNicholas", "title": "Subspace Clustering with the Multivariate-t Distribution", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering procedures suitable for the analysis of very high-dimensional data\nare needed for many modern data sets. In model-based clustering, a method\ncalled high-dimensional data clustering (HDDC) uses a family of Gaussian\nmixture models for clustering. HDDC is based on the idea that high-dimensional\ndata usually exists in lower-dimensional subspaces; as such, an intrinsic\ndimension for each sub-population of the observed data can be estimated and\ncluster analysis can be performed in this lower-dimensional subspace. As a\nresult, only a fraction of the total number of parameters need to be estimated\nand a computationally efficient parameter estimation scheme based on the EM\nalgorithm was developed. This family of models has gained attention due to its\nsuperior classification performance compared to other families of mixture\nmodels; however, it still suffers from the usual limitations of Gaussian\nmixture model-based approaches. In this paper, a robust analogue of the HDDC\napproach is proposed. This approach, which extends the HDDC procedure to\ninclude the mulitvariate-t distribution, encompasses 28 models that rectify the\naforementioned shortcomings of the HDDC procedure. Our tHDDC procedure is\nfitted to both simulated and real data sets and is compared to the HDDC\nprocedure using an image reconstruction problem that arose from satellite\nimagery of Mars' surface.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 16:19:03 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Pesevski", "Angelina", ""], ["Franczak", "Brian C.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1706.09072", "submitter": "Shahryar Minhas", "authors": "Shahryar Minhas, Peter D. Hoff, Michael D. Ward", "title": "Influence Networks in International Relations", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring influence and determining what drives it are persistent questions\nin political science and in network analysis more generally. Herein we focus on\nthe domain of international relations. Our major substantive question is: How\ncan we determine what characteristics make an actor influential? To address the\ntopic of influence, we build on a multilinear tensor regression framework\n(MLTR) that captures influence relationships using a tensor generalization of a\nvector autoregression model. Influence relationships in that approach are\ncaptured in a pair of n x n matrices and provide measurements of how the\nnetwork actions of one actor may influence the future actions of another. A\nlimitation of the MLTR and earlier latent space approaches is that there are no\ndirect mechanisms through which to explain why a certain actor is more or less\ninfluential than others. Our new framework, social influence regression,\nprovides a way to statistically model the influence of one actor on another as\na function of characteristics of the actors. Thus we can move beyond just\nestimating that an actor influences another to understanding why. To highlight\nthe utility of this approach, we apply it to studying monthly-level conflictual\nevents between countries as measured through the Integrated Crisis Early\nWarning System (ICEWS) event data project.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 23:12:16 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Minhas", "Shahryar", ""], ["Hoff", "Peter D.", ""], ["Ward", "Michael D.", ""]]}, {"id": "1706.09141", "submitter": "Nicolai Meinshausen", "authors": "Christina Heinze-Deml, Marloes H. Maathuis, and Nicolai Meinshausen", "title": "Causal Structure Learning", "comments": "to appear in `Annual Review of Statistics and Its Application', 30\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models can represent a multivariate distribution in a convenient\nand accessible form as a graph. Causal models can be viewed as a special class\nof graphical models that not only represent the distribution of the observed\nsystem but also the distributions under external interventions. They hence\nenable predictions under hypothetical interventions, which is important for\ndecision making. The challenging task of learning causal models from data\nalways relies on some underlying assumptions. We discuss several recently\nproposed structure learning algorithms and their assumptions, and compare their\nempirical performance under various scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 07:11:09 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Heinze-Deml", "Christina", ""], ["Maathuis", "Marloes H.", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1706.09187", "submitter": "Ruth Keogh", "authors": "Ruth H. Keogh, Tim P. Morris", "title": "Multiple imputation in Cox regression when there are time-varying\n  effects of exposures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Cox regression it is sometimes of interest to study time-varying effects\n(TVE) of exposures and to test the proportional hazards assumption. TVEs can be\ninvestigated with log hazard ratios modelled as a function of time. Missing\ndata on exposures are common and multiple imputation (MI) is a popular approach\nto handling this, to avoid the potential bias and loss of efficiency resulting\nfrom a 'complete-case' analysis. Two MI methods have been proposed for when the\nsubstantive model is a Cox proportional hazards regression: an approximate\nmethod (White and Royston, Statist. Med. 2009;28:1982-98) and a\nsubstantive-model-compatible method (Bartlett et al., SMMR 2015;24:462-87). At\npresent, neither method accommodates TVEs of exposures. We extend them to do so\nfor a general form for the TVEs and give specific details for TVEs modelled\nusing restricted cubic splines. Simulation studies assess the performance of\nthe methods under several underlying shapes for TVEs. Our proposed methods give\napproximately unbiased TVE estimates for binary exposures with missing data,\nbut for continuous exposures the substantive-model-compatible method performs\nbetter. The methods also give approximately correct type I errors in the test\nfor proportional hazards when there is no TVE, and gain power to detect TVEs\nrelative to complete-case analysis. Ignoring TVEs at the imputation stage\nresults in biased TVE estimates, incorrect type I errors and substantial loss\nof power in detecting TVEs. We also propose a multivariable TVE model selection\nalgorithm. The methods are illustrated using data from the Rotterdam Breast\nCancer Study. Example R code is provided.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 09:43:48 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Keogh", "Ruth H.", ""], ["Morris", "Tim P.", ""]]}, {"id": "1706.09375", "submitter": "Eugene Katsevich", "authors": "Eugene Katsevich and Chiara Sabatti", "title": "Multilayer Knockoff Filter: Controlled variable selection at multiple\n  resolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of selecting from among a large number of variables\nthose that are 'important' for an outcome. We consider situations where groups\nof variables are also of interest in their own right. For example, each\nvariable might be a genetic polymorphism and we might want to study how a trait\ndepends on variability in genes, segments of DNA that typically contain\nmultiple such polymorphisms. Or, variables might quantify various aspects of\nthe functioning of individual internet servers owned by a company, and we might\nbe interested in assessing the importance of each server as a whole on the\naverage download speed for the company's customers. In this context, to\ndiscover that a variable is relevant for the outcome implies discovering that\nthe larger entity it represents is also important. To guarantee meaningful and\nreproducible results, we suggest controlling the rate of false discoveries for\nfindings at the level of individual variables and at the level of groups.\nBuilding on the knockoff construction of Barber and Candes (2015) and the\nmultilayer testing framework of Barber and Ramdas (2016), we introduce the\nmultilayer knockoff filter (MKF). We prove that MKF simultaneously controls the\nFDR at each resolution and use simulations to show that it incurs little power\nloss compared to methods that provide guarantees only for the discoveries of\nindividual variables. We apply MKF to analyze a genetic dataset and find that\nit successfully reduces the number of false gene discoveries without a\nsignificant reduction in power.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 17:35:51 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 22:35:48 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Katsevich", "Eugene", ""], ["Sabatti", "Chiara", ""]]}, {"id": "1706.09523", "submitter": "P. Richard Hahn", "authors": "P. Richard Hahn, Jared S. Murray, and Carlos Carvalho", "title": "Bayesian regression tree models for causal inference: regularization,\n  confounding, and heterogeneous effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel nonlinear regression model for estimating\nheterogeneous treatment effects from observational data, geared specifically\ntowards situations with small effect sizes, heterogeneous effects, and strong\nconfounding. Standard nonlinear regression models, which may work quite well\nfor prediction, have two notable weaknesses when used to estimate heterogeneous\ntreatment effects. First, they can yield badly biased estimates of treatment\neffects when fit to data with strong confounding. The Bayesian causal forest\nmodel presented in this paper avoids this problem by directly incorporating an\nestimate of the propensity function in the specification of the response model,\nimplicitly inducing a covariate-dependent prior on the regression function.\nSecond, standard approaches to response surface modeling do not provide\nadequate control over the strength of regularization over effect heterogeneity.\nThe Bayesian causal forest model permits treatment effect heterogeneity to be\nregularized separately from the prognostic effect of control variables, making\nit possible to informatively \"shrink to homogeneity\". We illustrate these\nbenefits via the reanalysis of an observational study assessing the causal\neffects of smoking on medical expenditures as well as extensive simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 00:20:37 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 04:50:27 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 07:30:20 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 06:16:38 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Hahn", "P. Richard", ""], ["Murray", "Jared S.", ""], ["Carvalho", "Carlos", ""]]}, {"id": "1706.09603", "submitter": "Aasthaa Bansal", "authors": "Aasthaa Bansal, Patrick J. Heagerty", "title": "A tutorial on evaluating time-varying discrimination accuracy for\n  survival models used in dynamic decision-making", "comments": "54 pages, 3 tables, 4 figures, presented at Society for Medical\n  Decision Making annual meeting 2015 and American Statistical Association\n  Joint Statistical Meetings 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many medical decisions involve the use of dynamic information collected on\nindividual patients toward predicting likely transitions in their future health\nstatus. If accurate predictions are developed, then a prognostic mode can\nidentify patients at greatest risk for future adverse events, and may be used\nclinically to define populations appropriate for targeted intervention. In\npractice, a prognostic model is often used to guide decisions at multiple time\npoints over the course of disease, and classification performance, i.e.\nsensitivity and specificity, for distinguishing high-risk versus low-risk\nindividuals may vary over time as an individual's disease status and prognostic\ninformation change. In this tutorial, we detail contemporary statistical\nmethods that can characterize the time-varying accuracy of prognostic survival\nmodels when used for dynamic decision-making. Although statistical methods for\nevaluating prognostic models with simple binary outcomes are well established,\nmethods appropriate for survival outcomes are less well known and require\ntime-dependent extensions of sensitivity and specificity to fully characterize\nlongitudinal biomarkers or models. The methods we review are particularly\nimportant in that they allow for appropriate handling of censored outcomes\ncommonly encountered with event-time data. We highlight the importance of\ndetermining whether clinical interest is in predicting cumulative (or\nprevalent) cases over a fixed future time interval versus predicting incident\ncases over a range of follow-up time, and whether patient information is static\nor updated over time. We discuss implementation of time-dependent ROC\napproaches using relevant R statistical software packages. The statistical\nsummaries are illustrated using a liver prognostic model to guide\ntransplantation in primary biliary cirrhosis.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 07:27:29 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 16:00:32 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Bansal", "Aasthaa", ""], ["Heagerty", "Patrick J.", ""]]}, {"id": "1706.09796", "submitter": "David R\\\"ugamer", "authors": "David R\\\"ugamer and Sonja Greven", "title": "Selective inference after likelihood- or test-based model selection in\n  linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference after model selection requires an inference framework\nthat takes the selection into account in order to be valid. Following recent\nwork on selective inference, we derive analytical expressions for inference\nafter likelihood- or test-based model selection for linear models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 15:09:33 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 11:42:31 GMT"}, {"version": "v3", "created": "Sat, 23 Sep 2017 16:37:27 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["R\u00fcgamer", "David", ""], ["Greven", "Sonja", ""]]}, {"id": "1706.09828", "submitter": "Subir Kumar Bhandari Dr.", "authors": "Anupam Kundu and Subir Kumar Bhandari", "title": "Classification of Population Using Voronoi Area Based Density Estimation", "comments": "20 pages, 30 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two sets of training samples, general method is to estimate the density\nfunction and classify the test sample according to higher values of estimated\ndensities. Natural way to estimate the density should be histogram tending to\nfrequency curve. But nature of different amount of data accumulation at\ndifferent points, obstruct us to take common interval length in drawing\nhistogram. Here we followed a method similar to drawing histogram with an\nintention to density estimation and classification, but without doing\narbitrarily we considered diagram of the scattered plot and has obtained\nestimate of density based on areas of the Voronoi cells. But area of arbitrary\nVoronoi cell may be difficult to find out particularly in two and higher\ndimensions, and also in manifolds. For this reason we estimate the area of the\nVoronoi cell by drawing many independent uniform variates covering the support\nof the training samples. Please note that though we have in mind density\nestimation and classification problem in higher dimension and in typical\nmanifolds, but our simulations and algebraic calculations have been mostly\nshown for densities on real line. We expect in higher dimension and in\nmanifolds the work will be in similar lines and plan to do in near future. Also\nwe expect discrete cases can be handled in slightly different but similar lines\nwhich are also left for future works.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 16:11:57 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Kundu", "Anupam", ""], ["Bhandari", "Subir Kumar", ""]]}, {"id": "1706.10029", "submitter": "Cheng Ju", "authors": "Cheng Ju and Richard Wyss and Jessica M. Franklin and Sebastian\n  Schneeweiss and Jenny H\\\"aggstr\\\"om and Mark J. van der Laan", "title": "Collaborative-controlled LASSO for Constructing Propensity Score-based\n  Estimators in High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score (PS) based estimators are increasingly used for causal\ninference in observational studies. However, model selection for PS estimation\nin high-dimensional data has received little attention. In these settings, PS\nmodels have traditionally been selected based on the goodness-of-fit for the\ntreatment mechanism itself, without consideration of the causal parameter of\ninterest. Collaborative minimum loss-based estimation (C-TMLE) is a novel\nmethodology for causal inference that takes into account information on the\ncausal parameter of interest when selecting a PS model. This \"collaborative\nlearning\" considers variable associations with both treatment and outcome when\nselecting a PS model in order to minimize a bias-variance trade off in the\nestimated treatment effect. In this study, we introduce a novel approach for\ncollaborative model selection when using the LASSO estimator for PS estimation\nin high-dimensional covariate settings. To demonstrate the importance of\nselecting the PS model collaboratively, we designed quasi-experiments based on\na real electronic healthcare database, where only the potential outcomes were\nmanually generated, and the treatment and baseline covariates remained\nunchanged. Results showed that the C-TMLE algorithm outperformed other\ncompeting estimators for both point estimation and confidence interval\ncoverage. In addition, the PS model selected by C-TMLE could be applied to\nother PS-based estimators, which also resulted in substantive improvement for\nboth point estimation and confidence interval coverage. We illustrate the\ndiscussed concepts through an empirical example comparing the effects of\nnon-selective nonsteroidal anti-inflammatory drugs with selective COX-2\ninhibitors on gastrointestinal complications in a population of Medicare\nbeneficiaries.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 05:44:00 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Ju", "Cheng", ""], ["Wyss", "Richard", ""], ["Franklin", "Jessica M.", ""], ["Schneeweiss", "Sebastian", ""], ["H\u00e4ggstr\u00f6m", "Jenny", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1706.10066", "submitter": "Esa Ollila", "authors": "Esa Ollila", "title": "Optimal High-Dimensional Shrinkage Covariance Estimation for Elliptical\n  Distributions", "comments": "Accepted in the 25th European Signal Processing Conference (EUSIPCO\n  2017), published by EURASIP, scheduled for Aug. 28 - Sep. 2 in Kos island,\n  Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an optimal shrinkage sample covariance matrix (SCM) estimator which\nis suitable for high dimensional problems and when sampling from an unspecified\nelliptically symmetric distribution. Specifically, we derive the optimal\n(oracle) shrinkage parameters that obtain the minimum mean-squared error (MMSE)\nbetween the shrinkage SCM and the true covariance matrix when sampling from an\nelliptical distribution. Subsequently, we show how the oracle shrinkage\nparameters can be consistently estimated under the random matrix theory regime.\nSimulations show the advantage of the proposed estimator over the conventional\nshrinkage SCM estimator due to Ledoit and Wolf (2004). The proposed shrinkage\nSCM estimator often provides significantly better performance than the\nLedoit-Wolf estimator and has the advantage that consistency is guaranteed over\nthe whole class of elliptical distributions with finite 4th order moments.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 08:49:00 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Ollila", "Esa", ""]]}, {"id": "1706.10179", "submitter": "Jyotishka Datta", "authors": "Anindya Bhadra, Jyotishka Datta, Nicholas G. Polson and Brandon T.\n  Willard", "title": "Lasso Meets Horseshoe : A Survey", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to contrast and survey the major advances in two of\nthe most commonly used high-dimensional techniques, namely, the Lasso and\nhorseshoe regularization. Lasso is a gold standard for predictor selection\nwhile horseshoe is a state-of-the-art Bayesian estimator for sparse signals.\nLasso is fast and scalable and uses convex optimization whilst the horseshoe is\nnon-convex. Our novel perspective focuses on three aspects: (i) theoretical\noptimality in high dimensional inference for the Gaussian sparse model and\nbeyond, (ii) efficiency and scalability of computation and (iii) methodological\ndevelopment and performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 13:02:14 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 20:51:34 GMT"}, {"version": "v3", "created": "Fri, 10 Aug 2018 15:30:12 GMT"}, {"version": "v4", "created": "Sun, 3 Mar 2019 16:35:27 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Bhadra", "Anindya", ""], ["Datta", "Jyotishka", ""], ["Polson", "Nicholas G.", ""], ["Willard", "Brandon T.", ""]]}, {"id": "1706.10273", "submitter": "Chong Chen", "authors": "Chong Chen (1), Ruibin Xi (1) and Nan Lin (2) ((1) School of\n  Mathematical Sciences, Peking University, (2) Department of Mathematics,\n  Washington University in St. Louis)", "title": "Community Detection by $L_0$-penalized Graph Laplacian", "comments": "40 pages, 15 Postscript figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection in network analysis aims at partitioning nodes in a\nnetwork into $K$ disjoint communities. Most currently available algorithms\nassume that $K$ is known, but choosing a correct $K$ is generally very\ndifficult for real networks. In addition, many real networks contain outlier\nnodes not belonging to any community, but currently very few algorithm can\nhandle networks with outliers. In this paper, we propose a novel model free\ntightness criterion and an efficient algorithm to maximize this criterion for\ncommunity detection. This tightness criterion is closely related with the graph\nLaplacian with $L_0$ penalty. Unlike most community detection methods, our\nmethod does not require a known $K$ and can properly detect communities in\nnetworks with outliers.\n  Both theoretical and numerical properties of the method are analyzed. The\ntheoretical result guarantees that, under the degree corrected stochastic block\nmodel, even for networks with outliers, the maximizer of the tightness\ncriterion can extract communities with small misclassification rates even when\nthe number of communities grows to infinity as the network size grows.\nSimulation study shows that the proposed method can recover true communities\nmore accurately than other methods. Applications to a college football data and\na yeast protein-protein interaction data also reveal that the proposed method\nperforms significantly better.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 17:01:03 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Chen", "Chong", ""], ["Xi", "Ruibin", ""], ["Lin", "Nan", ""]]}]