[{"id": "0711.0198", "submitter": "Ulrike von Luxburg", "authors": "Ulrike von Luxburg, Volker H. Franz", "title": "A Geometric Approach to Confidence Sets for Ratios: Fieller's Theorem,\n  Generalizations, and Bootstrap", "comments": null, "journal-ref": "Statistica Sinica, 19(3), 1095-1117. (2009)", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": null, "abstract": "  We present a geometric method to determine confidence sets for the ratio\nE(Y)/E(X) of the means of random variables X and Y. This method reduces the\nproblem of constructing confidence sets for the ratio of two random variables\nto the problem of constructing confidence sets for the means of one-dimensional\nrandom variables. It is valid in a large variety of circumstances. In the case\nof normally distributed random variables, the so constructed confidence sets\ncoincide with the standard Fieller confidence sets. Generalizations of our\nconstruction lead to definitions of exact and conservative confidence sets for\nvery general classes of distributions, provided the joint expectation of (X,Y)\nexists and the linear combinations of the form aX + bY are well-behaved.\nFinally, our geometric method allows to derive a very simple bootstrap approach\nfor constructing conservative confidence sets for ratios which perform\nfavorably in certain situations, in particular in the asymmetric heavy-tailed\nregime.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2007 19:32:37 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["von Luxburg", "Ulrike", ""], ["Franz", "Volker H.", ""]]}, {"id": "0711.0458", "submitter": "Agostino Nobile", "authors": "Agostino Nobile", "title": "Bayesian finite mixtures: a note on prior specification and posterior\n  computation", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": "Univ of Glasgow, Dept of Statistics, Tech report 05-3", "categories": "stat.ME stat.CO", "license": null, "abstract": "  A new method for the computation of the posterior distribution of the number\nk of components in a finite mixture is presented. Two aspects of prior\nspecification are also studied: an argument is made for the use of a Poisson(1)\ndistribution as the prior for k; and methods are given for the selection of\nhyperparameter values in the mixture of normals model, with natural conjugate\npriors on the components parameters.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2007 14:40:47 GMT"}], "update_date": "2007-11-06", "authors_parsed": [["Nobile", "Agostino", ""]]}, {"id": "0711.0660", "submitter": "Hannes Leeb", "authors": "Benedikt M. Potscher, Hannes Leeb", "title": "On the Distribution of Penalized Maximum Likelihood Estimators: The\n  LASSO, SCAD, and Thresholding", "comments": null, "journal-ref": "J. Multivariate Anal. 100 (2009) 2065-2082", "doi": "10.1016/j.jmva.2009.06.010", "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distributions of the LASSO, SCAD, and thresholding estimators,\nin finite samples and in the large-sample limit. The asymptotic distributions\nare derived for both the case where the estimators are tuned to perform\nconsistent model selection and for the case where the estimators are tuned to\nperform conservative model selection. Our findings complement those of Knight\nand Fu (2000) and Fan and Li (2001). We show that the distributions are\ntypically highly nonnormal regardless of how the estimator is tuned, and that\nthis property persists in large samples. The uniform convergence rate of these\nestimators is also obtained, and is shown to be slower than 1/root(n) in case\nthe estimator is tuned to perform consistent model selection. An impossibility\nresult regarding estimation of the estimators' distribution function is also\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2007 15:27:39 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2009 12:13:16 GMT"}], "update_date": "2009-09-07", "authors_parsed": [["Potscher", "Benedikt M.", ""], ["Leeb", "Hannes", ""]]}, {"id": "0711.0690", "submitter": "Arne Kovac", "authors": "P. L. Davies and A. Kovac and M. Meise", "title": "Nonparametric Regression, Confidence Regions and Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": null, "abstract": "  In this paper we offer a unified approach to the problem of nonparametric\nregression on the unit interval. It is based on a universal, honest and\nnon-asymptotic confidence region which is defined by a set of linear\ninequalities involving the values of the functions at the design points.\nInterest will typically centre on certain simplest functions in that region\nwhere simplicity can be defined in terms of shape (number of local extremes,\nintervals of convexity/concavity) or smoothness (bounds on derivatives) or a\ncombination of both. Once some form of regularization has been decided upon the\nconfidence region can be used to provide honest non-asymptotic confidence\nbounds which are less informative but conceptually much simpler.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2007 16:56:15 GMT"}], "update_date": "2007-11-06", "authors_parsed": [["Davies", "P. L.", ""], ["Kovac", "A.", ""], ["Meise", "M.", ""]]}, {"id": "0711.1036", "submitter": "Benedikt M. P\\\"otscher", "authors": "Benedikt M. P\\\"otscher", "title": "Confidence Sets Based on Sparse Estimators Are Necessarily Large", "comments": "Revision containing correction of some minor errors and typos; some\n  additional remarks added", "journal-ref": "Sankhya 71-A (2009), Part 1, 1-18", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence sets based on sparse estimators are shown to be large compared to\nmore standard confidence sets, demonstrating that sparsity of an estimator\ncomes at a substantial price in terms of the quality of the estimator. The\nresults are set in a general parametric or semiparametric framework.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2007 08:40:37 GMT"}, {"version": "v2", "created": "Thu, 7 May 2009 07:38:56 GMT"}], "update_date": "2010-01-09", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""]]}, {"id": "0711.1146", "submitter": "Peter Hoff", "authors": "Peter D. Hoff", "title": "Modeling homophily and stochastic equivalence in symmetric relational\n  data", "comments": "12 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": null, "abstract": "  This article discusses a latent variable model for inference and prediction\nof symmetric relational data.\n  The model, based on the idea of the eigenvalue decomposition, represents the\nrelationship between two nodes as the weighted inner-product of node-specific\nvectors of latent characteristics. This ``eigenmodel'' generalizes other\npopular latent variable models, such as latent class and distance models: It is\nshown mathematically that any latent class or distance model has a\nrepresentation as an eigenmodel, but not vice-versa. The practical implications\nof this are examined in the context of three real datasets, for which the\neigenmodel has as good or better out-of-sample predictive performance than the\nother two models.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2007 19:46:30 GMT"}], "update_date": "2007-11-08", "authors_parsed": [["Hoff", "Peter D.", ""]]}, {"id": "0711.1455", "submitter": "Roberto D. Pascual-Marqui", "authors": "Roberto D. Pascual-Marqui", "title": "Instantaneous and lagged measurements of linear and nonlinear dependence\n  between groups of multivariate time series: frequency decomposition", "comments": "KEY Institute - University of Zurich - Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": null, "abstract": "  Measures of linear dependence (coherence) and nonlinear dependence (phase\nsynchronization) between any number of multivariate time series are defined.\nThe measures are expressed as the sum of lagged dependence and instantaneous\ndependence. The measures are non-negative, and take the value zero only when\nthere is independence of the pertinent type. These measures are defined in the\nfrequency domain and are applicable to stationary and non-stationary time\nseries. These new results extend and refine significantly those presented in a\nprevious technical report (Pascual-Marqui 2007, arXiv:0706.1776 [stat.ME],\nhttp://arxiv.org/abs/0706.1776), and have been largely motivated by the seminal\npaper on linear feedback by Geweke (1982 JASA 77:304-313). One important field\nof application is neurophysiology, where the time series consist of electric\nneuronal activity at several brain locations. Coherence and phase\nsynchronization are interpreted as \"connectivity\" between locations. However,\nany measure of dependence is highly contaminated with an instantaneous,\nnon-physiological contribution due to volume conduction and low spatial\nresolution. The new techniques remove this confounding factor considerably.\nMoreover, the measures of dependence can be applied to any number of brain\nareas jointly, i.e. distributed cortical networks, whose activity can be\nestimated with eLORETA (Pascual-Marqui 2007, arXiv:0710.3341 [math-ph]).\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2007 19:32:16 GMT"}], "update_date": "2007-11-12", "authors_parsed": [["Pascual-Marqui", "Roberto D.", ""]]}, {"id": "0711.1594", "submitter": "Konstantinos Kalogeropoulos", "authors": "Konstantinos Kalogeropoulos, Gareth O. Roberts, Petros Dellaportas", "title": "Inference for stochastic volatility models using time change\n  transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-fin.CP stat.CO stat.TH", "license": null, "abstract": "  We address the problem of parameter estimation for diffusion driven\nstochastic volatility models through Markov chain Monte Carlo (MCMC). To avoid\ndegeneracy issues we introduce an innovative reparametrisation defined through\ntransformations that operate on the time scale of the diffusion. A novel MCMC\nscheme which overcomes the inherent difficulties of time change transformations\nis also presented. The algorithm is fast to implement and applies to models\nwith stochastic volatility. The methodology is tested through simulation based\nexperiments and illustrated on data consisting of US treasury bill rates.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2007 12:48:03 GMT"}], "update_date": "2008-12-02", "authors_parsed": [["Kalogeropoulos", "Konstantinos", ""], ["Roberts", "Gareth O.", ""], ["Dellaportas", "Petros", ""]]}, {"id": "0711.1595", "submitter": "Konstantinos Kalogeropoulos", "authors": "Konstantinos Kalogeropoulos, Petros Dellaportas, Gareth O. Roberts", "title": "Likelihood-based inference for correlated diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST stat.CO stat.ME stat.TH", "license": null, "abstract": "  We address the problem of likelihood based inference for correlated diffusion\nprocesses using Markov chain Monte Carlo (MCMC) techniques. Such a task\npresents two interesting problems. First, the construction of the MCMC scheme\nshould ensure that the correlation coefficients are updated subject to the\npositive definite constraints of the diffusion matrix. Second, a diffusion may\nonly be observed at a finite set of points and the marginal likelihood for the\nparameters based on these observations is generally not available. We overcome\nthe first issue by using the Cholesky factorisation on the diffusion matrix. To\ndeal with the likelihood unavailability, we generalise the data augmentation\nframework of Roberts and Stramer (2001 Biometrika 88(3):603-621) to\nd-dimensional correlated diffusions including multivariate stochastic\nvolatility models. Our methodology is illustrated through simulation based\nexperiments and with daily EUR /USD, GBP/USD rates together with their implied\nvolatilities.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2007 13:02:52 GMT"}], "update_date": "2008-12-02", "authors_parsed": [["Kalogeropoulos", "Konstantinos", ""], ["Dellaportas", "Petros", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "0711.1612", "submitter": "Michael Wakin", "authors": "Emmanuel J. Candes, Michael B. Wakin, and Stephen P. Boyd", "title": "Enhancing Sparsity by Reweighted L1 Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": null, "abstract": "  It is now well understood that (1) it is possible to reconstruct sparse\nsignals exactly from what appear to be highly incomplete sets of linear\nmeasurements and (2) that this can be done by constrained L1 minimization. In\nthis paper, we study a novel method for sparse signal recovery that in many\nsituations outperforms L1 minimization in the sense that substantially fewer\nmeasurements are needed for exact recovery. The algorithm consists of solving a\nsequence of weighted L1-minimization problems where the weights used for the\nnext iteration are computed from the value of the current solution. We present\na series of experiments demonstrating the remarkable performance and broad\napplicability of this algorithm in the areas of sparse signal recovery,\nstatistical estimation, error correction and image processing. Interestingly,\nsuperior gains are also achieved when our method is applied to recover signals\nwith assumed near-sparsity in overcomplete representations--not by reweighting\nthe L1 norm of the coefficient sequence as is common, but by reweighting the L1\nnorm of the transformed object. An immediate consequence is the possibility of\nhighly efficient data acquisition protocols by improving on a technique known\nas compressed sensing.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2007 20:52:58 GMT"}], "update_date": "2007-11-13", "authors_parsed": [["Candes", "Emmanuel J.", ""], ["Wakin", "Michael B.", ""], ["Boyd", "Stephen P.", ""]]}, {"id": "0711.1874", "submitter": "Geoff Nicholls", "authors": "Geoff K. Nicholls and Russell D. Gray", "title": "Dated ancestral trees from binary trait data and its application to the\n  diversification of languages", "comments": "The definitive version of this manuscript is available in the Journal\n  of the Royal Statistical Society at http://www3.interscience.wiley.com/", "journal-ref": "Journal of the Royal Statistical Society. Series B: Statistical\n  Methodology 70 (3), pp. 545-566 (2008)", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": null, "abstract": "  Binary trait data record the presence or absence of distinguishing traits in\nindividuals. We treat the problem of estimating ancestral trees with time depth\nfrom binary trait data. Simple analysis of such data is problematic. Each\nhomology class of traits has a unique birth event on the tree, and the birth\nevent of a trait visible at the leaves is biased towards the leaves. We propose\na model-based analysis of such data, and present an MCMC algorithm that can\nsample from the resulting posterior distribution. Our model is based on using a\nbirth-death process for the evolution of the elements of sets of traits. Our\nanalysis correctly accounts for the removal of singleton traits, which are\ncommonly discarded in real data sets. We illustrate Bayesian inference for two\nbinary-trait data sets which arise in historical linguistics. The Bayesian\napproach allows for the incorporation of information from ancestral languages.\nThe marginal prior distribution of the root time is uniform. We present a\nthorough analysis of the robustness of our results to model mispecification,\nthrough analysis of predictive distributions for external data, and fitting\ndata simulated under alternative observation models. The reconstructed ages of\ntree nodes are relatively robust, whilst posterior probabilities for topology\nare not reliable.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2007 21:58:17 GMT"}], "update_date": "2009-08-31", "authors_parsed": [["Nicholls", "Geoff K.", ""], ["Gray", "Russell D.", ""]]}, {"id": "0711.1918", "submitter": "Chenlei Leng", "authors": "Chenlei Leng", "title": "The Residual Information Criterion, Corrected", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": null, "abstract": "  Shi and Tsai (JRSSB, 2002) proposed an interesting residual information\ncriterion (RIC) for model selection in regression. Their RIC was motivated by\nthe principle of minimizing the Kullback-Leibler discrepancy between the\nresidual likelihoods of the true and candidate model. We show, however, under\nthis principle, RIC would always choose the full (saturated) model. The\nresidual likelihood therefore, is not appropriate as a discrepancy measure in\ndefining information criterion. We explain why it is so and provide a corrected\nresidual information criterion as a remedy.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2007 04:44:54 GMT"}], "update_date": "2007-11-14", "authors_parsed": [["Leng", "Chenlei", ""]]}, {"id": "0711.1930", "submitter": "I-Li Lu", "authors": "Roger D. Gibb, I-Li Lu, Walter H. Carter Jr", "title": "Bootstrap Confidence Regions for Optimal Operating Conditions in\n  Response Surface Methodology", "comments": "Submitted to the Electronic Journal of Statistics\n  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": null, "doi": null, "report-no": "IMS-EJS-EJS_2007_144", "categories": "stat.ME", "license": null, "abstract": "  This article concerns the application of bootstrap methodology to construct a\nlikelihood-based confidence region for operating conditions associated with the\nmaximum of a response surface constrained to a specified region. Unlike\nclassical methods based on the stationary point, proper interpretation of this\nconfidence region does not depend on unknown model parameters. In addition, the\nmethodology does not require the assumption of normally distributed errors. The\napproach is demonstrated for concave-down and saddle system cases in two\ndimensions. Simulation studies were performed to assess the coverage\nprobability of these regions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2007 07:55:22 GMT"}], "update_date": "2007-11-14", "authors_parsed": [["Gibb", "Roger D.", ""], ["Lu", "I-Li", ""], ["Carter", "Walter H.", "Jr"]]}, {"id": "0711.2345", "submitter": "Anne-Laure Fougeres", "authors": "Anne-Laure Foug\\`eres (MODAL'X), John P. Nolan, Holger Rootz\\'en", "title": "Models for dependent extremes using stable mixtures", "comments": null, "journal-ref": "Scandinavian Journal of Statistics 36 (2009) 42-59", "doi": "10.1111/j.1467-9469.2008.00613.x", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": null, "abstract": "  This paper unifies and extends results on a class of multivariate Extreme\nValue (EV) models studied by Hougaard, Crowder, and Tawn. In these models both\nunconditional and conditional distributions are EV, and all lower-dimensional\nmarginals and maxima belong to the class. This leads to substantial economies\nof understanding, analysis and prediction. One interpretation of the models is\nas size mixtures of EV distributions, where the mixing is by positive stable\ndistributions. A second interpretation is as exponential-stable location\nmixtures (for Gumbel) or as power-stable scale mixtures (for non-Gumbel EV\ndistributions). A third interpretation is through a Peaks over Thresholds model\nwith a positive stable intensity. The mixing variables are used as a modeling\ntool and for better understanding and model checking. We study extreme value\nanalogues of components of variance models, and new time series, spatial, and\ncontinuous parameter models for extreme values. The results are applied to data\nfrom a pitting corrosion investigation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2007 06:13:08 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["Foug\u00e8res", "Anne-Laure", "", "MODAL'X"], ["Nolan", "John P.", ""], ["Rootz\u00e9n", "Holger", ""]]}, {"id": "0711.2349", "submitter": "Samuel M\\\"uller", "authors": "Samuel Mueller, A.H. Welsh", "title": "Robust model selection in generalized linear models", "comments": "24 pages, 1 figure, submitted to JASA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": null, "abstract": "  In this paper, we extend to generalized linear models (including logistic and\nother binary regression models, Poisson regression and gamma regression models)\nthe robust model selection methodology developed by Mueller and Welsh (2005;\nJASA) for linear regression models. As in Mueller and Welsh (2005), we combine\na robust penalized measure of fit to the sample with a robust measure of out of\nsample predictive ability which is estimated using a post-stratified m-out-of-n\nbootstrap. A key idea is that the method can be used to compare different\nestimators (robust and nonrobust) as well as different models. Even when\nspecialized back to linear regression models, the methodology presented in this\npaper improves on that of Mueller and Welsh (2005). In particular, we use a new\nbias-adjusted bootstrap estimator which avoids the need to centre the\nexplanatory variables and to include an intercept in every model. We also use\nmore sophisticated arguments than Mueller and Welsh (2005) to establish an\nessential monotonicity condition.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2007 06:56:50 GMT"}], "update_date": "2007-11-16", "authors_parsed": [["Mueller", "Samuel", ""], ["Welsh", "A. H.", ""]]}, {"id": "0711.2893", "submitter": "Milan \\v{Z}ukovi\\v{c}", "authors": "Milan Zukovic, Dionissios T. Hristopulos", "title": "The Method of Normalized Correlations - A Fast Alternative to Maximum\n  Likelihood Estimation for Random Processes and Isotropic Random Fields with\n  Short-Range Dependence", "comments": "This paper has been withdrawn", "journal-ref": "Technometrics 51 173 (2009)", "doi": "10.1198/TECH.2009.0018", "report-no": null, "categories": "stat.CO stat.ME", "license": null, "abstract": "  This paper has been withdrawn by the authors, due the copyright policy of the\njournal it has been submited to.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2007 10:36:53 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2008 09:05:18 GMT"}], "update_date": "2012-12-24", "authors_parsed": [["Zukovic", "Milan", ""], ["Hristopulos", "Dionissios T.", ""]]}, {"id": "0711.3035", "submitter": "Jeffrey Picka", "authors": "Jeffrey Picka", "title": "Statistical Inference for Disordered Sphere Packings", "comments": "54 pages, no figures; revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sphere packings are essential to the development of physical models for\npowders, composite materials, and the atomic structure of the liquid state.\nThere is a strong scientific need to be able to assess the fit of packing\nmodels to data, but this is complicated by the lack of formal probabilistic\nmodels for packings. Without formal models, simulation algorithms and\ncollections of physical objects must be used as models. Identification of\ncommon aspects of different realizations of the same packing process requires\nthe use of new descriptive statistics, many of which have yet to be developed.\nModel assessment will require the use of large samples of independent and\nidentically distributed realizations, rather than the large single stationary\nrealizations found in conventional spatial statistics. The development of\nprocedures for model assessment will resemble the development of thermodynamic\nmodels, and will be based on much exploration and experimentation rather than\non extensions of established statistical methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2007 22:49:22 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2007 22:03:50 GMT"}, {"version": "v3", "created": "Fri, 31 Oct 2008 23:39:25 GMT"}, {"version": "v4", "created": "Fri, 30 Oct 2009 22:05:34 GMT"}], "update_date": "2009-10-31", "authors_parsed": [["Picka", "Jeffrey", ""]]}, {"id": "0711.3218", "submitter": "Vasiliy Krivtsov", "authors": "Mark Kaminskiy, Vasiliy Krivtsov", "title": "An Integral Measure of Aging/Rejuvenation for Repairable and\n  Non-repairable Systems", "comments": "12 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": null, "abstract": "  This paper introduces a simple index that helps to assess the degree of aging\nor rejuvenation of a (non)repairable system. The index ranges from -1 to 1 and\nis negative for the class of decreasing failure rate distributions (or\ndeteriorating point processes) and is positive for the increasing failure rate\ndistributions (or improving point processes). The introduced index is\ndistribution free.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2007 15:08:11 GMT"}], "update_date": "2007-11-22", "authors_parsed": [["Kaminskiy", "Mark", ""], ["Krivtsov", "Vasiliy", ""]]}, {"id": "0711.3236", "submitter": "Paul Kabaila", "authors": "Paul Kabaila, Khageswor Giri", "title": "Confidence intervals in regression utilizing prior information", "comments": "This version differs from v2 in 2 respects. Firstly, a few typos have\n  been corrected. Secondly, the paper has been shortened. This version has been\n  accepted for publication in Journal of Statistical Planning and Inference", "journal-ref": "Confidence intervals in regression utilizing prior information.\n  Journal of Statistical Planning and Inference, 139, 3419-3429 (2009)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a linear regression model with regression parameter\nbeta=(beta_1,...,beta_p) and independent and identically N(0,sigma^2)\ndistributed errors. Suppose that the parameter of interest is theta = a^T beta\nwhere a is a specified vector. Define the parameter tau=c^T beta-t where the\nvector c and the number t are specified and a and c are linearly independent.\nAlso suppose that we have uncertain prior information that tau = 0. We present\na new frequentist 1-alpha confidence interval for theta that utilizes this\nprior information. We require this confidence interval to (a) have endpoints\nthat are continuous functions of the data and (b) coincide with the standard\n1-alpha confidence interval when the data strongly contradicts this prior\ninformation. This interval is optimal in the sense that it has minimum weighted\naverage expected length where the largest weight is given to this expected\nlength when tau=0. This minimization leads to an interval that has the\nfollowing desirable properties. This interval has expected length that (a) is\nrelatively small when the prior information about tau is correct and (b) has a\nmaximum value that is not too large. The following problem will be used to\nillustrate the application of this new confidence interval. Consider a 2-by 2\nfactorial experiment with 20 replicates. Suppose that the parameter of interest\ntheta is a specified simple effect and that we have uncertain prior information\nthat the two-factor interaction is zero. Our aim is to find a frequentist 0.95\nconfidence interval for theta that utilizes this prior information.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2007 23:36:20 GMT"}, {"version": "v2", "created": "Tue, 2 Sep 2008 05:56:15 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2009 22:45:06 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Kabaila", "Paul", ""], ["Giri", "Khageswor", ""]]}, {"id": "0711.3271", "submitter": "J. Sacks", "authors": "M. J. Bayarri, J. O. Berger, J. Cafeo, G. Garcia-Donato, F. Liu, J.\n  Palomo, R. J. Parthasarathy, R. Paulo, J. Sacks, D. Walsh", "title": "Computer model validation with functional output", "comments": "Published in at http://dx.doi.org/10.1214/009053607000000163 the\n  Annals of Statistics (http://www.imstat.org/aos/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2007, Vol. 35, No. 5, 1874-1906", "doi": "10.1214/009053607000000163", "report-no": "IMS-AOS-AOS0246", "categories": "stat.ME", "license": null, "abstract": "  A key question in evaluation of computer models is Does the computer model\nadequately represent reality? A six-step process for computer model validation\nis set out in Bayarri et al. [Technometrics 49 (2007) 138--154] (and briefly\nsummarized below), based on comparison of computer model runs with field data\nof the process being modeled. The methodology is particularly suited to\ntreating the major issues associated with the validation process: quantifying\nmultiple sources of error and uncertainty in computer models; combining\nmultiple sources of information; and being able to adapt to different, but\nrelated scenarios. Two complications that frequently arise in practice are the\nneed to deal with highly irregular functional data and the need to acknowledge\nand incorporate uncertainty in the inputs. We develop methodology to deal with\nboth complications. A key part of the approach utilizes a wavelet\nrepresentation of the functional data, applies a hierarchical version of the\nscalar validation methodology to the wavelet coefficients, and transforms back,\nto ultimately compare computer model output with field output. The generality\nof the methodology is only limited by the capability of a combination of\ncomputational tools and the appropriateness of decompositions of the sort\n(wavelets) employed here. The methods and analyses we present are illustrated\nwith a test bed dynamic stress analysis for a particular engineering system.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2007 07:48:28 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Bayarri", "M. J.", ""], ["Berger", "J. O.", ""], ["Cafeo", "J.", ""], ["Garcia-Donato", "G.", ""], ["Liu", "F.", ""], ["Palomo", "J.", ""], ["Parthasarathy", "R. J.", ""], ["Paulo", "R.", ""], ["Sacks", "J.", ""], ["Walsh", "D.", ""]]}, {"id": "0711.3657", "submitter": "Artin Armagan", "authors": "Artin Armagan and Russell L. Zaretzki", "title": "Bayesian Shrinkage Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": null, "abstract": "  Withdrawn due to extensions and submission as another paper.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2007 16:01:51 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2007 00:44:07 GMT"}, {"version": "v3", "created": "Sun, 2 Dec 2007 19:49:03 GMT"}, {"version": "v4", "created": "Wed, 16 Apr 2008 20:37:59 GMT"}], "update_date": "2008-04-16", "authors_parsed": [["Armagan", "Artin", ""], ["Zaretzki", "Russell L.", ""]]}, {"id": "0711.3834", "submitter": "Jonathan Lilly", "authors": "Jonathan M. Lilly and Sofia C. Olhede", "title": "On the Analytic Wavelet Transform", "comments": null, "journal-ref": "Lilly, J. M., and S. C. Olhede (2010). On the analytic wavelet\n  transform. IEEE Transactions on Information Theory, 56 (8), 4135--4156", "doi": "10.1109/TIT.2010.2050935", "report-no": null, "categories": "math.ST math.FA stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An exact and general expression for the analytic wavelet transform of a\nreal-valued signal is constructed, resolving the time-dependent effects of\nnon-negligible amplitude and frequency modulation. The analytic signal is first\nlocally represented as a modulated oscillation, demodulated by its own\ninstantaneous frequency, and then Taylor-expanded at each point in time. The\nterms in this expansion, called the instantaneous modulation functions, are\ntime-varying functions which quantify, at increasingly higher orders, the local\ndepartures of the signal from a uniform sinusoidal oscillation. Closed-form\nexpressions for these functions are found in terms of Bell polynomials and\nderivatives of the signal's instantaneous frequency and bandwidth. The analytic\nwavelet transform is shown to depend upon the interaction between the signal's\ninstantaneous modulation functions and frequency-domain derivatives of the\nwavelet, inducing a hierarchy of departures of the transform away from a\nperfect representation of the signal. The form of these deviation terms\nsuggests a set of conditions for matching the wavelet properties to suit the\nvariability of the signal, in which case our expressions simplify considerably.\nOne may then quantify the time-varying bias associated with signal estimation\nvia wavelet ridge analysis, and choose wavelets to minimize this bias.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2007 12:32:35 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2009 13:17:36 GMT"}, {"version": "v3", "created": "Sat, 15 Oct 2011 16:33:06 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["Lilly", "Jonathan M.", ""], ["Olhede", "Sofia C.", ""]]}, {"id": "0711.3857", "submitter": "Hacene Belbachir", "authors": "Abdelhakim Aknouche, Fay\\c{c}al Hamdi", "title": "Periodic Chandrasekhar recursions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": null, "abstract": "  This paper extends the Chandrasekhar-type recursions due to Morf, Sidhu, and\nKailath \"Some new algorithms for recursive estimation in constant, linear,\ndiscrete-time systems, IEEE Trans. Autom. Control 19 (1974) 315-323\" to the\ncase of periodic time-varying state-space models. We show that the S-lagged\nincrements of the one-step prediction error covariance satisfy certain\nrecursions from which we derive some algorithms for linear least squares\nestimation for periodic state-space models. The proposed recursions may have\npotential computational advantages over the Kalman Filter and, in particular,\nthe periodic Riccati difference equation.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2007 19:11:33 GMT"}], "update_date": "2007-11-27", "authors_parsed": [["Aknouche", "Abdelhakim", ""], ["Hamdi", "Fay\u00e7al", ""]]}, {"id": "0711.4514", "submitter": "Pierre Etore", "authors": "Pierre Etore (CERMICS), Benjamin Jourdain (CERMICS)", "title": "Adaptive optimal allocation in stratified sampling methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": null, "abstract": "  In this paper, we propose a stratified sampling algorithm in which the random\ndrawings made in the strata to compute the expectation of interest are also\nused to adaptively modify the proportion of further drawings in each stratum.\nThese proportions converge to the optimal allocation in terms of variance\nreduction. And our stratified estimator is asymptotically normal with\nasymptotic variance equal to the minimal one. Numerical experiments confirm the\nefficiency of our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2007 14:52:08 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2007 12:59:04 GMT"}], "update_date": "2007-12-04", "authors_parsed": [["Etore", "Pierre", "", "CERMICS"], ["Jourdain", "Benjamin", "", "CERMICS"]]}, {"id": "0711.4983", "submitter": "Longhai Li", "authors": "Longhai Li and Radford M. Neal", "title": "A Method for Compressing Parameters in Bayesian Models with Application\n  to Logistic Sequence Prediction Models", "comments": "29 pages", "journal-ref": "Bayesian Analysis, 2008, 3(4), 793-822", "doi": "10.1214/08-BA330", "report-no": null, "categories": "stat.ML stat.ME", "license": null, "abstract": "  Bayesian classification and regression with high order interactions is\nlargely infeasible because Markov chain Monte Carlo (MCMC) would need to be\napplied with a great many parameters, whose number increases rapidly with the\norder. In this paper we show how to make it feasible by effectively reducing\nthe number of parameters, exploiting the fact that many interactions have the\nsame values for all training cases. Our method uses a single ``compressed''\nparameter to represent the sum of all parameters associated with a set of\npatterns that have the same value for all training cases. Using symmetric\nstable distributions as the priors of the original parameters, we can easily\nfind the priors of these compressed parameters. We therefore need to deal only\nwith a much smaller number of compressed parameters when training the model\nwith MCMC. The number of compressed parameters may have converged before\nconsidering the highest possible order. After training the model, we can split\nthese compressed parameters into the original ones as needed to make\npredictions for test cases. We show in detail how to compress parameters for\nlogistic sequence prediction models. Experiments on both simulated and real\ndata demonstrate that a huge number of parameters can indeed be reduced by our\ncompression method.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2007 17:24:41 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Li", "Longhai", ""], ["Neal", "Radford M.", ""]]}]