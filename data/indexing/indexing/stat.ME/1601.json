[{"id": "1601.00225", "submitter": "Michael Betancourt", "authors": "Michael Betancourt", "title": "Identifying the Optimal Integration Time in Hamiltonian Monte Carlo", "comments": "31 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By leveraging the natural geometry of a smooth probabilistic system,\nHamiltonian Monte Carlo yields computationally efficient Markov Chain Monte\nCarlo estimation. At least provided that the algorithm is sufficiently\nwell-tuned. In this paper I show how the geometric foundations of Hamiltonian\nMonte Carlo implicitly identify the optimal choice of these parameters,\nespecially the integration time. I then consider the practical consequences of\nthese principles in both existing algorithms and a new implementation called\n\\emph{Exhaustive Hamiltonian Monte Carlo} before demonstrating the utility of\nthese ideas in some illustrative examples.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 20:59:22 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Betancourt", "Michael", ""]]}, {"id": "1601.00237", "submitter": "Daniel Almirall", "authors": "Audrey Boruvka, Daniel Almirall, Katie Witkiewitz, Susan A. Murphy", "title": "Assessing Time-Varying Causal Effect Moderation in Mobile Health", "comments": "24 pages plus Supplemental Appendix (18 pages), Github link for R\n  code in Supplemental Appendix E", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mobile health interventions aimed at behavior change and maintenance,\ntreatments are provided in real time to manage current or impending high risk\nsituations or promote healthy behaviors in near real time. Currently there is\ngreat scientific interest in developing data analysis approaches to guide the\ndevelopment of mobile interventions. In particular data from mobile health\nstudies might be used to examine effect moderators-i.e., individual\ncharacteristics, time-varying context or past treatment response that moderate\nthe effect of current treatment on a subsequent response. This paper introduces\na formal definition for moderated effects in terms of potential outcomes, a\ndefinition that is particularly suited to mobile interventions, where treatment\noccasions are numerous, individuals are not always available for treatment, and\npotential moderators might be influenced by past treatment. Methods for\nestimating moderated effects are developed and compared. The proposed approach\nis illustrated using BASICS-Mobile, a smartphone-based intervention designed to\ncurb heavy drinking and smoking among college students.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 00:33:15 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 16:06:27 GMT"}, {"version": "v3", "created": "Wed, 17 Aug 2016 01:29:56 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Boruvka", "Audrey", ""], ["Almirall", "Daniel", ""], ["Witkiewitz", "Katie", ""], ["Murphy", "Susan A.", ""]]}, {"id": "1601.00341", "submitter": "Olusegun Ewemooje", "authors": "Olusegun S. Ewemooje and Godwin N. Amahia", "title": "Improved Randomized Response Technique for Two Sensitive Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed new and more efficient estimators for estimating population\nproportion of respondents belonging to two related sensitive attributes in\nsurvey sampling by extending the work of Mangat (1994). Our proposed estimators\nare more efficient than Lee et al (2013) simple and crossed model estimators as\nthe population proportion of possessing the sensitive attribute increases.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 21:22:30 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Ewemooje", "Olusegun S.", ""], ["Amahia", "Godwin N.", ""]]}, {"id": "1601.00389", "submitter": "Armeen Taeb", "authors": "Armeen Taeb, Venkat Chandrasekaran", "title": "Interpreting Latent Variables in Factor Models via Convex Optimization", "comments": null, "journal-ref": "Mathematical Programming 2018, Vol. 167, 129--154", "doi": "10.1007/s10107-017-1187-7", "report-no": null, "categories": "stat.ME math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent or unobserved phenomena pose a significant difficulty in data analysis\nas they induce complicated and confounding dependencies among a collection of\nobserved variables. Factor analysis is a prominent multivariate statistical\nmodeling approach that addresses this challenge by identifying the effects of\n(a small number of) latent variables on a set of observed variables. However,\nthe latent variables in a factor model are purely mathematical objects that are\nderived from the observed phenomena, and they do not have any interpretation\nassociated to them. A natural approach for attributing semantic information to\nthe latent variables in a factor model is to obtain measurements of some\nadditional plausibly useful covariates that may be related to the original set\nof observed variables, and to associate these auxiliary covariates to the\nlatent variables. In this paper, we describe a systematic approach for\nidentifying such associations. Our method is based on solving computationally\ntractable convex optimization problems, and it can be viewed as a\ngeneralization of the minimum-trace factor analysis procedure for fitting\nfactor models via convex optimization. We analyze the theoretical consistency\nof our approach in a high-dimensional setting as well as its utility in\npractice via experimental demonstrations with real data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 06:29:16 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 01:18:57 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Taeb", "Armeen", ""], ["Chandrasekaran", "Venkat", ""]]}, {"id": "1601.00439", "submitter": "Aidan O'Keeffe", "authors": "Panayiota Constantinou and Aidan G. O'Keeffe", "title": "Regression Discontinuity Designs: A Decision Theoretic Approach", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regression discontinuity design (RDD) is a quasi-experimental design that\ncan be used to identify and estimate the causal effect of a treatment using\nobservational data. In an RDD, a pre-specified rule is used for treatment\nassignment, whereby a subject is assigned to the treatment (control) group\nwhenever their observed value of a specific continuous variable is greater than\nor equal to (is less than) a fixed threshold. Sharp RDDs occur when guidelines\nare strictly adhered to and fuzzy RDDs occur when the guidelines or not\nstrictly adhered to. In this paper, we take a rigorous decision theoretic\napproach to formally study causal effect identification and estimation in both\nsharp and fuzzy RDDs. We use the language and calculus of conditional\nindependence to express and explore in a clear and precise manner the\nconditions implied by each RDD and investigate additional assumptions under\nwhich the identification of the average causal effect at the threshold can be\nachieved. We apply the methodology in an example concerning the relationship\nbetween statins (a class of cholesterol-lowering drugs) and low density\nlipoprotein (LDL) cholesterol using a real set of primary care data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 10:21:55 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Constantinou", "Panayiota", ""], ["O'Keeffe", "Aidan G.", ""]]}, {"id": "1601.00477", "submitter": "Stephen Bush", "authors": "Stephen Bush and Katya Ruggiero", "title": "Optimal block designs for experiments with responses drawn from a\n  Poisson distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal block designs for additive models achieve their efficiency by\ndividing experimental units among relatively homogenous blocks and allocating\ntreatments equally to blocks. Responses in many modern experiments, however,\nare drawn from distributions such as the one- and two-parameter exponential\nfamilies, e.g., RNA sequence counts from a negative binomial distribution.\nThese violate additivity. Yet, designs generated by assuming additivity\ncontinue to be used, because better approaches are not available, and because\nthe issues are not widely recognised. We solve this problem for single-factor\nexperiments in which treatments, taking categorical values only, are arranged\nin blocks and responses drawn from a Poisson distribution. We derive\nexpressions for two objective functions, based on D_A- and C-optimality, with\nefficient estimation of linear contrasts of the fixed effects parameters in a\nPoisson generalised linear mixed model (GLMM) being the objective. These\nobjective functions are shown to be computational efficient, requiring no\nmatrix inversion. Using simulated annealing to generate Poisson GLMM-based\nlocally optimal designs, we show that the replication numbers of treatments in\nthese designs are inversely proportional to the relative magnitudes of the\ntreatments' expected counts. Importantly, for non-negligible treatment effect\nsizes, Poisson GLMM-based optimal designs may be substantially more efficient\nthan their classically optimal counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 12:23:43 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Bush", "Stephen", ""], ["Ruggiero", "Katya", ""]]}, {"id": "1601.00736", "submitter": "Jiahe Lin", "authors": "Jiahe Lin, Sumanta Basu, Moulinath Banerjee, George Michailidis", "title": "Penalized Maximum Likelihood Estimation of Multi-layered Gaussian\n  Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing multi-layered graphical models provides insight into understanding\nthe conditional relationships among nodes within layers after adjusting for and\nquantifying the effects of nodes from other layers. We obtain the penalized\nmaximum likelihood estimator for Gaussian multi-layered graphical models, based\non a computational approach involving screening of variables, iterative\nestimation of the directed edges between layers and undirected edges within\nlayers and a final refitting and stability selection step that provides\nimproved performance in finite sample settings. We establish the consistency of\nthe estimator in a high-dimensional setting. To obtain this result, we develop\na strategy that leverages the biconvexity of the likelihood function to ensure\nconvergence of the developed iterative algorithm to a stationary point, as well\nas careful uniform error control of the estimates over iterations. The\nperformance of the maximum likelihood estimator is illustrated on synthetic\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 05:08:33 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Lin", "Jiahe", ""], ["Basu", "Sumanta", ""], ["Banerjee", "Moulinath", ""], ["Michailidis", "George", ""]]}, {"id": "1601.00797", "submitter": "Florian Heinrichs", "authors": "Holger Dette, Katrin Kettelhake, Kirsten Schorning, Weng Kee Wong,\n  Frank Bretz", "title": "Optimal designs for active controlled dose finding trials with\n  efficacy-toxicity outcomes", "comments": "Keywords and Phrases: Active controlled trials, dose finding, optimal\n  design, admissible design, Emax model, Equivalence theorem, Particle swarm\n  optimization, Tchebycheff system", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear regression models addressing both efficacy and toxicity outcomes\nare increasingly used in dose-finding trials, such as in pharmaceutical drug\ndevelopment. However, research on related experimental design problems for\ncorresponding active controlled trials is still scarce. In this paper we derive\noptimal designs to estimate efficacy and toxicity in an active controlled\nclinical dose finding trial when the bivariate continuous outcomes are modeled\neither by polynomials up to degree 2, the Michaelis- Menten model, the Emax\nmodel, or a combination thereof. We determine upper bounds on the number of\ndifferent doses levels required for the optimal design and provide conditions\nunder which the boundary points of the design space are included in the optimal\ndesign. We also provide an analytical description of the minimally supported\n$D$-optimal designs and show that they do not depend on the correlation between\nthe bivariate outcomes. We illustrate the proposed methods with numerical\nexamples and demonstrate the advantages of the $D$-optimal design for a trial,\nwhich has recently been considered in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 11:32:11 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Dette", "Holger", ""], ["Kettelhake", "Katrin", ""], ["Schorning", "Kirsten", ""], ["Wong", "Weng Kee", ""], ["Bretz", "Frank", ""]]}, {"id": "1601.00815", "submitter": "Jana Jankova", "authors": "Jana Jankova and Sara van de Geer", "title": "Semi-parametric efficiency bounds for high-dimensional models", "comments": "68 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymptotic lower bounds for estimation play a fundamental role in assessing\nthe quality of statistical procedures. In this paper we propose a framework for\nobtaining semi-parametric efficiency bounds for sparse high-dimensional models,\nwhere the dimension of the parameter is larger than the sample size. We adopt a\nsemi-parametric point of view: we concentrate on one dimensional functions of a\nhigh-dimensional parameter. We follow two different approaches to reach the\nlower bounds: asymptotic Cram\\'er-Rao bounds and Le Cam's type of analysis.\nBoth these approaches allow us to define a class of asymptotically unbiased or\n\"regular\" estimators for which a lower bound is derived. Consequently, we show\nthat certain estimators obtained by de-sparsifying (or de-biasing) an\n$\\ell_1$-penalized M-estimator are asymptotically unbiased and achieve the\nlower bound on the variance: thus in this sense they are asymptotically\nefficient. The paper discusses in detail the linear regression model and the\nGaussian graphical model.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 12:50:05 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 16:47:30 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 13:10:07 GMT"}, {"version": "v4", "created": "Thu, 12 Oct 2017 20:43:50 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Jankova", "Jana", ""], ["van de Geer", "Sara", ""]]}, {"id": "1601.00992", "submitter": "Bruce Desmarais", "authors": "Jake Bowers, Bruce A. Desmarais, Mark Frederickson, Nahomi Ichino,\n  Hsuan-Wei Lee, Simi Wang", "title": "Models, Methods and Network Topology: Experimental Design for the Study\n  of Interference", "comments": null, "journal-ref": "Jake Bowers, Bruce A. Desmarais, Mark Frederickson, Nahomi Ichino,\n  Hsuan-Wei Lee, Simi Wang, Models, methods and network topology: Experimental\n  design for the study of interference, Social Networks, Volume 54, 2018, Pages\n  196-208", "doi": "10.1016/j.socnet.2018.01.010", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should a network experiment be designed to achieve high statistical\npower? Ex- perimental treatments on networks may spread. Randomizing assignment\nof treatment to nodes enhances learning about the counterfactual causal effects\nof a social network experiment and also requires new methodology (ex. Aronow\nand Samii 2017a; Bow- ers et al. 2013; Toulis and Kao 2013). In this paper we\nshow that the way in which a treatment propagates across a social network\naffects the statistical power of an ex- perimental design. As such, prior\ninformation regarding treatment propagation should be incorporated into the\nexperimental design. Our findings justify reconsideration of standard practice\nin circumstances where units are presumed to be independent even in simple\nexperiments: information about treatment effects is not maximized when we\nassign half the units to treatment and half to control. We also present an\nexam- ple in which statistical power depends on the extent to which the network\ndegree of nodes is correlated with treatment assignment probability. We\nrecommend that re- searchers think carefully about the underlying treatment\npropagation model motivat- ing their study in designing an experiment on a\nnetwork.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 21:41:35 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 01:34:37 GMT"}, {"version": "v3", "created": "Fri, 30 Mar 2018 09:47:35 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Bowers", "Jake", ""], ["Desmarais", "Bruce A.", ""], ["Frederickson", "Mark", ""], ["Ichino", "Nahomi", ""], ["Lee", "Hsuan-Wei", ""], ["Wang", "Simi", ""]]}, {"id": "1601.01039", "submitter": "Jiguo Cao", "authors": "Baisen Liu and Jiguo Cao", "title": "Estimating Functional Linear Mixed-Effects Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The functional linear model is a popular tool to investigate the relationship\nbetween a scalar/functional response variable and a scalar/functional\ncovariate. We generalize this model to a functional linear mixed-effects model\nwhen repeated measurements are available on multiple subjects. Each subject has\nan individual intercept and slope function, while shares common population\nintercept and slope function. This model is flexible in the sense of allowing\nthe slope random effects to change with the time. We propose a penalized spline\nsmoothing method to estimate the population and random slope functions. A\nREML-based EM algorithm is developed to estimate the variance parameters for\nthe random effects and the data noise. Simulation studies show that our\nestimation method provides an accurate estimate for the functional linear\nmixed-effects model with the finite samples. The functional linear\nmixed-effects model is demonstrated by investigating the effect of the 24-hour\nnitrogen dioxide on the daily maximum ozone concentrations and also studying\nthe effect of the daily temperature on the annual precipitation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 01:04:59 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Liu", "Baisen", ""], ["Cao", "Jiguo", ""]]}, {"id": "1601.01125", "submitter": "Tore Selland Kleppe", "authors": "Oliver Grothe, Tore Selland Kleppe and Roman Liesenfeld", "title": "The Gibbs Sampler with Particle Efficient Importance Sampling for\n  State-Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Particle Gibbs (PG) as a tool for Bayesian analysis of non-linear\nnon-Gaussian state-space models. PG is a Monte Carlo (MC) approximation of the\nstandard Gibbs procedure which uses sequential MC (SMC) importance sampling\ninside the Gibbs procedure to update the latent and potentially\nhigh-dimensional state trajectories. We propose to combine PG with a generic\nand easily implementable SMC approach known as Particle Efficient Importance\nSampling (PEIS). By using SMC importance sampling densities which are\napproximately fully globally adapted to the targeted density of the states,\nPEIS can substantially improve the mixing and the efficiency of the PG draws\nfrom the posterior of the states and the parameters relative to existing PG\nimplementations. The efficiency gains achieved by PEIS are illustrated in PG\napplications to a univariate stochastic volatility model for asset returns, a\nnon-Gaussian nonlinear local-level model for interest rates, and a multivariate\nstochastic volatility model for the realized covariance matrix of asset\nreturns.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 10:19:15 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 10:38:05 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 10:53:35 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Grothe", "Oliver", ""], ["Kleppe", "Tore Selland", ""], ["Liesenfeld", "Roman", ""]]}, {"id": "1601.01126", "submitter": "Shravan Vasishth", "authors": "Shravan Vasishth and Bruno Nicenboim", "title": "Statistical methods for linguistic research: Foundational Ideas - Part I", "comments": "30 pages, 9 figures, 3 tables. Under review with Language and\n  Linguistics Compass. Comments and suggestions for improvement welcome.\n  (Replaced version corrects several typos)", "journal-ref": "Language and Linguistics Compass 2016", "doi": "10.1111/lnc3.12201", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the fundamental ideas underlying statistical hypothesis testing\nusing the frequentist framework. We begin with a simple example that builds up\nthe one-sample t-test from the beginning, explaining important concepts such as\nthe sampling distribution of the sample mean, and the iid assumption. Then we\nexamine the p-value in detail, and discuss several important misconceptions\nabout what a p-value does and does not tell us. This leads to a discussion of\nType I, II error and power, and Type S and M error. An important conclusion\nfrom this discussion is that one should aim to carry out appropriately powered\nstudies. Next, we discuss two common issues we have encountered in\npsycholinguistics and linguistics: running experiments until significance is\nreached, and the \"garden-of-forking-paths\" problem discussed by Gelman and\nothers, whereby the researcher attempts to find statistical significance by\nanalyzing the data in different ways. The best way to use frequentist methods\nis to run appropriately powered studies, check model assumptions, clearly\nseparate exploratory data analysis from confirmatory hypothesis testing, and\nalways attempt to replicate results.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 10:27:38 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 13:47:08 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Vasishth", "Shravan", ""], ["Nicenboim", "Bruno", ""]]}, {"id": "1601.01178", "submitter": "Kaniav Kamary", "authors": "Kaniav Kamary (Universit\\'e Paris-Dauphine and INRIA), Jeong Eun Lee\n  (Auckland University of Technology), and Christian P. Robert (Universit\\'e\n  Paris-Dauphine and University of Warwick)", "title": "Weakly informative reparameterisations for location-scale mixtures", "comments": "32 pages, 14 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While mixtures of Gaussian distributions have been studied for more than a\ncentury (Pearson, 1894), the construction of a reference Bayesian analysis of\nthose models still remains unsolved, with a general prohibition of the usage of\nimproper priors (Fruwirth-Schnatter, 2006) due to the ill-posed nature of such\nstatistical objects. This difficulty is usually bypassed by an empirical Bayes\nresolution (Richardson and Green, 1997). By creating a new parameterisation\ncantered on the mean and possibly the variance of the mixture distribution\nitself, we manage to develop here a weakly informative prior for a wide class\nof mixtures with an arbitrary number of components. We demonstrate that some\nposterior distributions associated with this prior and a minimal sample size\nare proper. We provide MCMC implementations that exhibit the expected\nexchangeability. We only study here the univariate case, the extension to\nmultivariate location-scale mixtures being currently under study. An R package\ncalled Ultimixt is associated with this paper.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 13:58:56 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 09:31:03 GMT"}, {"version": "v3", "created": "Mon, 13 Mar 2017 08:25:33 GMT"}, {"version": "v4", "created": "Mon, 31 Jul 2017 09:41:36 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Kamary", "Kaniav", "", "Universit\u00e9 Paris-Dauphine and INRIA"], ["Lee", "Jeong Eun", "", "Auckland University of Technology"], ["Robert", "Christian P.", "", "Universit\u00e9\n  Paris-Dauphine and University of Warwick"]]}, {"id": "1601.01180", "submitter": "Andrea Riebler", "authors": "Andrea Riebler, Sigrunn H. S{\\o}rbye, Daniel Simpson and H{\\aa}vard\n  Rue", "title": "An intuitive Bayesian spatial model for disease mapping that accounts\n  for scaling", "comments": "24 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, disease mapping studies have become a routine application\nwithin geographical epidemiology and are typically analysed within a Bayesian\nhierarchical model formulation. A variety of model formulations for the latent\nlevel have been proposed but all come with inherent issues. In the classical\nBYM model, the spatially structured component cannot be seen independently from\nthe unstructured component. This makes prior definitions for the\nhyperparameters of the two random effects challenging. There are alternative\nmodel formulations that address this confounding, however, the issue on how to\nchoose interpretable hyperpriors is still unsolved. Here, we discuss a recently\nproposed parameterisation of the BYM model that leads to improved parameter\ncontrol as the hyperparameters can be seen independently from each other.\nFurthermore, the need for a scaled spatial component is addressed, which\nfacilitates assignment of interpretable hyperpriors and make these transferable\nbetween spatial applications with different graph structures. We provide\nimplementation details for the new model formulation which preserve sparsity\nproperties, and we investigate systematically the model performance and compare\nit to existing parameterisations. Through a simulation study, we show that the\nnew model performs well, both showing good learning abilities and good\nshrinkage behaviour. In terms of model choice criteria, the proposed model\nperforms at least equally well as existing parameterisations, but only the new\nformulation offers parameters that are interpretable and hyperpriors that have\na clear meaning.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 14:06:13 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Riebler", "Andrea", ""], ["S\u00f8rbye", "Sigrunn H.", ""], ["Simpson", "Daniel", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1601.01413", "submitter": "Peter Aronow", "authors": "Peter M. Aronow", "title": "Local average causal effects and superefficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches in causal inference have proposed estimating average causal\neffects that are local to some subpopulation, often for reasons of efficiency.\nThese inferential targets are sometimes data-adaptive, in that they are\ndependent on the empirical distribution of the data. In this short note, we\nshow that if researchers are willing to adapt the inferential target on the\nbasis of efficiency, then extraordinary gains in precision can be obtained.\nSpecifically, when causal effects are heterogeneous, any asymptotically normal\nand root-$n$ consistent estimator of the population average causal effect is\nsuperefficient for a data-adaptive local average causal effect. Our result\nillustrates the fundamental gain in statistical certainty afforded by\nindifference about the inferential target.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 06:56:23 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 00:21:06 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Aronow", "Peter M.", ""]]}, {"id": "1601.01462", "submitter": "Simone Padoan PhD", "authors": "Giulia Marcon, Simone A. Padoan, Antoniano-Villalobos", "title": "Bayesian Inference for the Extremal Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple approach for modeling multivariate extremes is to consider the\nvector of component-wise maxima and their max-stable distributions. The\nextremal dependence can be inferred by estimating the angular measure or,\nalternatively, the Pickands dependence function. We propose a nonparametric\nBayesian model that allows, in the bivariate case, the simultaneous estimation\nof both functional representations through the use of polynomials in the\nBernstein form. The constraints required to provide a valid extremal dependence\nare addressed in a straightforward manner, by placing a prior on the\ncoefficients of the Bernstein polynomials which gives probability one to the\nset of valid functions. The prior is extended to the polynomial degree, making\nour approach fully nonparametric. Although the analytical expression of the\nposterior is unknown, inference is possible via a trans-dimensional MCMC\nscheme. We show the efficiency of the proposed methodology by means of a\nsimulation study. The extremal behaviour of log-returns of daily exchange rates\nbetween the Pound Sterling vs the U.S. Dollar and the Pound Sterling vs the\nJapanese Yen is analysed for illustrative purposes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 10:04:45 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 08:35:18 GMT"}, {"version": "v3", "created": "Thu, 2 Feb 2017 08:03:05 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Marcon", "Giulia", ""], ["Padoan", "Simone A.", ""], ["Antoniano-Villalobos", "", ""]]}, {"id": "1601.01700", "submitter": "Robert Murphy", "authors": "Robert A. Murphy", "title": "A Predictive Model using the Markov Property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a data set of numerical values which are sampled from some unknown\nprobability distribution, we will show how to check if the data set exhibits\nthe Markov property and we will show how to use the Markov property to predict\nfuture values from the same distribution, with probability 1.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 16:19:21 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Murphy", "Robert A.", ""]]}, {"id": "1601.01849", "submitter": "Matteo Fasiolo", "authors": "Matteo Fasiolo, Simon N. Wood, Florian Hartig, Mark V. Bravington", "title": "An Extended Empirical Saddlepoint Approximation for Intractable\n  Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenges posed by complex stochastic models used in computational\necology, biology and genetics have stimulated the development of approximate\napproaches to statistical inference. Here we focus on Synthetic Likelihood\n(SL), a procedure that reduces the observed and simulated data to a set of\nsummary statistics, and quantifies the discrepancy between them through a\nsynthetic likelihood function. SL requires little tuning, but it relies on the\napproximate normality of the summary statistics. We relax this assumption by\nproposing a novel, more flexible, density estimator: the Extended Empirical\nSaddlepoint approximation. In addition to proving the consistency of SL, under\neither the new or the Gaussian density estimator, we illustrate the method\nusing two examples. One of these is a complex individual-based forest model for\nwhich SL offers one of the few practical possibilities for statistical\ninference. The examples show that the new density estimator is able to capture\nlarge departures from normality, while being scalable to high dimensions, and\nthis in turn leads to more accurate parameter estimates, relative to the\nGaussian alternative. The new density estimator is implemented by the esaddle R\npackage, which can be found on the Comprehensive R Archive Network (CRAN).\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 12:20:46 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 15:31:47 GMT"}, {"version": "v3", "created": "Tue, 12 Apr 2016 20:34:46 GMT"}, {"version": "v4", "created": "Thu, 1 Dec 2016 14:52:31 GMT"}, {"version": "v5", "created": "Thu, 8 Jun 2017 16:44:11 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Fasiolo", "Matteo", ""], ["Wood", "Simon N.", ""], ["Hartig", "Florian", ""], ["Bravington", "Mark V.", ""]]}, {"id": "1601.01879", "submitter": "Matthias Kirchner", "authors": "Paul Embrechts and Matthias Kirchner", "title": "Hawkes graphs", "comments": "22 pages", "journal-ref": "Theory of Probability and Its Applications, 62(1):163-193 (2017)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Hawkes skeleton and the Hawkes graph. These objects\nsummarize the branching structure of a multivariate Hawkes point process in a\ncompact, yet meaningful way. We demonstrate how graph-theoretic vocabulary\n(`ancestor sets', `parent sets', `connectivity', `walks', `walk weights', ...)\nis very convenient for the discussion of multivariate Hawkes processes. For\nexample, we reformulate the classic eigenvalue-based subcriticality criterion\nof multitype branching processes in graph terms. Next to these more\nterminological contributions, we show how the graph view may be used for the\nspecification and estimation of Hawkes models from large, multitype event\nstreams. Based on earlier work, we give a nonparametric statistical procedure\nto estimate the Hawkes skeleton and the Hawkes graph from data. We show how the\ngraph estimation may then be used for specifying and fitting parametric Hawkes\nmodels. Our estimation method avoids the a priori assumptions on the model from\na straighforward MLE-approach and is numerically more flexible than the latter.\nOur method has two tuning parameters: one controlling numerical complexity, the\nother one controlling the sparseness of the estimated graph. A simulation study\nconfirms that the presented procedure works as desired. We pay special\nattention to computational issues in the implementation. This makes our results\napplicable to high-dimensional event-stream data, such as dozens of event\nstreams and thousands of events per component.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 14:05:40 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 09:24:44 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Embrechts", "Paul", ""], ["Kirchner", "Matthias", ""]]}, {"id": "1601.01955", "submitter": "Satya Singh P", "authors": "Satya Prakash Singh and Siuli Mukhopadhyay", "title": "Bayesian Crossover Designs for Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article discusses D-optimal Bayesian crossover designs for generalized\nlinear models. Crossover trials with t treatments and p periods, for $t <= p$,\nare considered. The designs proposed in this paper minimize the log determinant\nof the variance of the estimated treatment effects over all possible allocation\nof the n subjects to the treatment sequences. It is assumed that the p\nobservations from each subject are mutually correlated while the observations\nfrom different subjects are uncorrelated. Since main interest is in estimating\nthe treatment effects, the subject effect is assumed to be nuisance, and\ngeneralized estimating equations are used to estimate the marginal means. To\naddress the issue of parameter dependence a Bayesian approach is employed.\nPrior distributions are assumed on the model parameters which are then\nincorporated into the D-optimal design criterion by integrating it over the\nprior distribution. Three case studies, one with binary outcomes in a\n4$\\times$4 crossover trial, second one based on count data for a 2$\\times$2\ntrial and a third one with Gamma responses in a 3$times$2 crossover trial are\nused to illustrate the proposed method. The effect of the choice of prior\ndistributions on the designs is also studied.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 17:44:26 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 14:57:56 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Singh", "Satya Prakash", ""], ["Mukhopadhyay", "Siuli", ""]]}, {"id": "1601.01981", "submitter": "James Pustejovsky", "authors": "James E. Pustejovsky and Elizabeth Tipton", "title": "Small sample methods for cluster-robust variance estimation and\n  hypothesis testing in fixed effects models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In longitudinal panels and other regression models with unobserved effects,\nfixed effects estimation is often paired with cluster-robust variance\nestimation (CRVE) in order to account for heteroskedasticity and un-modeled\ndependence among the errors. CRVE is asymptotically consistent as the number of\nindependent clusters increases, but can be biased downward for sample sizes\noften found in applied work, leading to hypothesis tests with overly liberal\nrejection rates. One solution is to use bias-reduced linearization (BRL), which\ncorrects the CRVE so that it is unbiased under a working model, and t-tests\nwith Satterthwaite degrees of freedom. We propose a generalization of BRL that\ncan be applied in models with arbitrary sets of fixed effects, where the\noriginal BRL method is undefined, and describe how to apply the method when the\nregression is estimated after absorbing the fixed effects. We also propose a\nsmall-sample test for multiple-parameter hypotheses, which generalizes the\nSatterthwaite approximation for t-tests. In simulations covering a variety of\nstudy designs, we find that conventional cluster-robust Wald tests can severely\nunder-reject while the proposed small-sample test maintains Type I error very\nclose to nominal levels.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 19:20:29 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Pustejovsky", "James E.", ""], ["Tipton", "Elizabeth", ""]]}, {"id": "1601.01986", "submitter": "Qing Feng", "authors": "Qing Feng, Jan Hannig and J.S.Marron", "title": "A Note on Automatic Data Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data analysis frequently involves variables with highly non-Gaussian\nmarginal distributions. However, commonly used analysis methods are most\neffective with roughly Gaussian data. This paper introduces an automatic\ntransformation that improves the closeness of distributions to normality. For\neach variable, a new family of parametrizations of the shifted logarithm\ntransformation is proposed, which is unique in treating the data as\nreal-valued, and in allowing transformation for both left and right skewness\nwithin the single family. This also allows an automatic selection of the\nparameter value (which is crucial for high dimensional data with many variables\nto transform) by minimizing the Anderson-Darling test statistic of the\ntransformed data. An application to image features extracted from melanoma\nmicroscopy slides demonstrate the utility of the proposed transformation in\naddressing data with excessive skewness, heteroscedasticity and influential\nobservations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 19:29:38 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Feng", "Qing", ""], ["Hannig", "Jan", ""], ["Marron", "J. S.", ""]]}, {"id": "1601.02149", "submitter": "Luis Zuluaga", "authors": "Robert Howley and Robert Storer and Juan Vera and Luis F. Zuluaga", "title": "Computing semiparametric bounds on the expected payments of insurance\n  instruments via column generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been recently shown that numerical semiparametric bounds on the\nexpected payoff of fi- nancial or actuarial instruments can be computed using\nsemidefinite programming. However, this approach has practical limitations.\nHere we use column generation, a classical optimization technique, to address\nthese limitations. From column generation, it follows that practical univari-\nate semiparametric bounds can be found by solving a series of linear programs.\nIn addition to moment information, the column generation approach allows the\ninclusion of extra information about the random variable; for instance,\nunimodality and continuity, as well as the construction of corresponding\nworst/best-case distributions in a simple way.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 19:22:27 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Howley", "Robert", ""], ["Storer", "Robert", ""], ["Vera", "Juan", ""], ["Zuluaga", "Luis F.", ""]]}, {"id": "1601.02387", "submitter": "Guillaume Dehaene", "authors": "Guillaume P Dehaene and Simon Barthelm\\'e", "title": "Bounding errors of Expectation-Propagation", "comments": "Accepted and published at NIPS 2015", "journal-ref": "Advances in Neural Information Processing Systems 28, 244--252,\n  2015", "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation Propagation is a very popular algorithm for variational\ninference, but comes with few theoretical guarantees. In this article, we prove\nthat the approximation errors made by EP can be bounded. Our bounds have an\nasymptotic interpretation in the number $n$ of datapoints, which allows us to\nstudy EP's convergence with respect to the true posterior. In particular, we\nshow that EP converges at a rate of $\\mathcal{0}(n^{-2})$ for the mean, up to\nan order of magnitude faster than the traditional Gaussian approximation at the\nmode. We also give similar asymptotic expansions for moments of order 2 to 4,\nas well as excess Kullback-Leibler cost (defined as the additional KL cost\nincurred by using EP rather than the ideal Gaussian approximation). All these\nexpansions highlight the superior convergence properties of EP. Our approach\nfor deriving those results is likely applicable to many similar approximate\ninference methods. In addition, we introduce bounds on the moments of\nlog-concave distributions that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 10:34:21 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Dehaene", "Guillaume P", ""], ["Barthelm\u00e9", "Simon", ""]]}, {"id": "1601.02410", "submitter": "Wanchuang Zhu", "authors": "Wanchuang Zhu, Yanan Fan", "title": "A novel approach for Markov Random Field with intractable normalising\n  constant on large lattices", "comments": "24 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pseudo likelihood method of Besag(1974), has remained a popular method\nfor estimating Markov random field on a very large lattice, despite various\ndocumented deficiencies. This is partly because it remains the only\ncomputationally tractable method for large lattices. We introduce a novel\nmethod to estimate Markov random fields defined on a regular lattice. The\nmethod takes advantage of conditional independence structures and recursively\ndecomposes a large lattice into smaller sublattices. An approximation is made\nat each decomposition. Doing so completely avoids the need to compute the\ntroublesome normalising constant. The computational complexity is $O(N)$, where\n$N$ is the the number of pixels in lattice, making it computationally\nattractive for very large lattices. We show through simulation, that the\nproposed method performs well, even when compared to the methods using exact\nlikelihoods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 12:05:01 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Zhu", "Wanchuang", ""], ["Fan", "Yanan", ""]]}, {"id": "1601.02461", "submitter": "Brian Reich", "authors": "Beth A. Tidemann-Miller, Brian J. Reich, Ana-Maria Staicu", "title": "Modeling Multivariate Mixed-Response Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian modeling framework for jointly analyzing multiple\nfunctional responses of different types (e.g. binary and continuous data). Our\napproach is based on a multivariate latent Gaussian process and models the\ndependence among the functional responses through the dependence of the latent\nprocess. Our framework easily accommodates additional covariates. We offer a\nway to estimate the multivariate latent covariance, allowing for implementation\nof multivariate functional principal components analysis (FPCA) to specify\nbasis expansions and simplify computation. We demonstrate our method through\nboth simulation studies and an application to real data from a periodontal\nstudy.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 14:37:58 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Tidemann-Miller", "Beth A.", ""], ["Reich", "Brian J.", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "1601.02596", "submitter": "Rex Cheung", "authors": "Rex C. Y. Cheung, Alexander Aue, Thomas C.M. Lee", "title": "Consistent Estimation for Partition-wise Regression and Classification\n  Models", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": "10.1109/TSP.2017.2698407", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partition-wise models offer a flexible approach for modeling complex and\nmultidimensional data that are capable of producing interpretable results. They\nare based on partitioning the observed data into regions, each of which is\nmodeled with a simple submodel. The success of this approach highly depends on\nthe quality of the partition, as too large a region could lead to a non-simple\nsubmodel, while too small a region could inflate estimation variance. This\npaper proposes an automatic procedure for choosing the partition (i.e., the\nnumber of regions and the boundaries between regions) as well as the submodels\nfor the regions. It is shown that, under the assumption of the existence of a\ntrue partition, the proposed partition estimator is statistically consistent.\nThe methodology is demonstrated for both regression and classification\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 20:57:17 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Cheung", "Rex C. Y.", ""], ["Aue", "Alexander", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "1601.02739", "submitter": "Wen-Xin Zhou", "authors": "Aurore Delaigle, Peter Hall, Wen-Xin Zhou", "title": "Nonparametric covariate-adjusted regression", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric estimation of a regression curve when the data are\nobserved with multiplicative distortion which depends on an observed\nconfounding variable. We suggest several estimators, ranging from a relatively\nsimple one that relies on restrictive assumptions usually made in the\nliterature, to a sophisticated piecewise approach that involves reconstructing\na smooth curve from an estimator of a constant multiple of its absolute value,\nand which can be applied in much more general scenarios. We show that, although\nour nonparametric estimators are constructed from predictors of the unobserved\nundistorted data, they have the same first order asymptotic properties as the\nstandard estimators that could be computed if the undistorted data were\navailable. We illustrate the good numerical performance of our methods on both\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 06:06:42 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Delaigle", "Aurore", ""], ["Hall", "Peter", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1601.02801", "submitter": "Sokbae Lee", "authors": "Sokbae Lee, Ryo Okui, Yoon-Jae Whang", "title": "Doubly Robust Uniform Confidence Band for the Conditional Average\n  Treatment Effect Function", "comments": "52 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a doubly robust method to present the heterogeneity\nof the average treatment effect with respect to observed covariates of\ninterest. We consider a situation where a large number of covariates are needed\nfor identifying the average treatment effect but the covariates of interest for\nanalyzing heterogeneity are of much lower dimension. Our proposed estimator is\ndoubly robust and avoids the curse of dimensionality. We propose a uniform\nconfidence band that is easy to compute, and we illustrate its usefulness via\nMonte Carlo experiments and an application to the effects of smoking on birth\nweights.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 10:54:38 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 19:45:41 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Lee", "Sokbae", ""], ["Okui", "Ryo", ""], ["Whang", "Yoon-Jae", ""]]}, {"id": "1601.02820", "submitter": "Hamed Haselimashhadi", "authors": "Hamed Haselimashhadi, Veronica Vinciotti, Keming Yu", "title": "A new Bayesian regression model for counts in medicine", "comments": null, "journal-ref": null, "doi": "10.1080/02664763.2017.1342782", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Discrete data are collected in many application areas and are often\ncharacterised by highly skewed and power-lawlike distributions. An example of\nthis, which is considered in this paper, is the number of visits to a\nspecialist, often taken as a measure of demand in healthcare. A discrete\nWeibull regression model was recently proposed for regression problems with a\ndiscrete response and it was shown to possess two important features: the\nability to capture over and under-dispersion simultaneously and a closed-form\nanalytical expression of the quantiles of the conditional distribution. In this\npaper, we propose the first Bayesian implementation of a discrete Weibull\nregression model. The implementation considers a novel parameterization, where\nboth parameters of the discrete Weibull distribution can be made dependent on\nthe predictors. In addition, prior distributions can be imposed that encourage\nparameter shrinkage and that lead to variable selection. As with Bayesian\nprocedures, the full posterior distribution of the parameters is returned, from\nwhich credible intervals can be readily calculated. A simulation study and the\nanalysis of four real datasets of medical records show promises for the wide\napplicability of this approach to the analysis of count data. The method is\nimplemented in the R package BDWreg.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 12:03:22 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Haselimashhadi", "Hamed", ""], ["Vinciotti", "Veronica", ""], ["Yu", "Keming", ""]]}, {"id": "1601.03281", "submitter": "J\\'er\\'emy Magnanensi", "authors": "J\\'er\\'emy Magnanensi, Myriam Maumy-Bertrand, Nicolas Meyer,\n  Fr\\'ed\\'eric Bertrand", "title": "New developments in Sparse PLS regression", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods based on partial least squares (PLS) regression, which has recently\ngained much attention in the analysis of high-dimensional genomic datasets,\nhave been developed since the early 2000s for performing variable selection.\nMost of these techniques rely on tuning parameters that are often determined by\ncross-validation (CV) based methods, which raises important stability issues.\nTo overcome this, we have developed a new dynamic bootstrapbased method for\nsignificant predictor selection, suitable for both PLS regression and its\nincorporation into generalized linear models (GPLS). It relies on the\nestablishment of bootstrap confidence intervals, that allows testing of the\nsignificance of predictors at preset type I risk $\\alpha$, and avoids the use\nof CV. We have also developed adapted versions of sparse PLS (SPLS) and sparse\nGPLS regression (SGPLS), using a recently introduced non-parametric\nbootstrap-based technique for the determination of the numbers of components.\nWe compare their variable selection reliability and stability concerning tuning\nparameters determination, as well as their predictive ability, using simulated\ndata for PLS and real microarray gene expression data for PLS-logistic\nclassification. We observe that our new dynamic bootstrapbased method has the\nproperty of best separating random noise in y from the relevant information\nwith respect to other methods, leading to better accuracy and predictive\nabilities, especially for non-negligible noise levels.\n  Keywords: Variable selection, PLS, GPLS, Bootstrap, Stability\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 15:32:30 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Magnanensi", "J\u00e9r\u00e9my", ""], ["Maumy-Bertrand", "Myriam", ""], ["Meyer", "Nicolas", ""], ["Bertrand", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1601.03334", "submitter": "Lior Pachter", "authors": "Audrey Fu and Lior Pachter", "title": "Estimating intrinsic and extrinsic noise from single-cell gene\n  expression measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene expression is stochastic and displays variation (\"noise\") both within\nand between cells. Intracellular (intrinsic) variance can be distinguished from\nextracellular (extrinsic) variance by applying the law of total variance to\ndata from two-reporter assays that probe expression of identical gene pairs in\nsingle-cells. We examine established formulas for the estimation of intrinsic\nand extrinsic noise and provide interpretations of them in terms of a\nhierarchical model. This allows us to derive corrections that minimize the mean\nsquared error, an objective that may be important when sample sizes are small.\nThe statistical framework also highlights the need for quantile normalization,\nand provides justification for the use of the sample correlation between the\ntwo reporter expression levels to estimate the percent contribution of\nextrinsic noise to the total noise. Finally, we provide a geometric\ninterpretation of these results that clarifies the current interpretation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 18:08:51 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Fu", "Audrey", ""], ["Pachter", "Lior", ""]]}, {"id": "1601.03379", "submitter": "Georgios Fellouris Dr.", "authors": "Georgios Fellouris and Alexander G. Tartakovsky", "title": "Multichannel Sequential Detection- Part I: Non-i.i.d. Data", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sequential signal detection in a multichannel\nsystem where the number and location of signals is a priori unknown. We assume\nthat the data in each channel are sequentially observed and follow a general\nnon-i.i.d. stochastic model. Under the assumption that the local log-likelihood\nratio processes in the channels converge r-completely to positive and finite\nnumbers, we establish the asymptotic optimality of a generalized sequential\nlikelihood ratio test and a mixture-based sequential likelihood ratio test.\nSpecifically, we show that both tests minimize the first r moments of the\nstopping time distribution asymptotically as the probabilities of false alarm\nand missed detection approach zero. Moreover, we show that both tests\nasymptotically minimize all moments of the stopping time distribution when the\nlocal log-likelihood ratio processes have independent increments and simply\nobey the Strong Law of Large Numbers. This extends a result previously known in\nthe case of i.i.d. observations when only one channel is affected. We\nillustrate the general detection theory using several practical examples,\nincluding the detection of signals in Gaussian hidden Markov models, white\nGaussian noises with unknown intensity, and testing of the first-order\nautoregression's correlation coefficient. Finally, we illustrate the\nfeasibility of both sequential tests when assuming an upper and a lower bound\non the number of signals and compare their non-asymptotic performance using a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 20:46:29 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Fellouris", "Georgios", ""], ["Tartakovsky", "Alexander G.", ""]]}, {"id": "1601.03410", "submitter": "David N. Levin", "authors": "David N. Levin (University of Chicago)", "title": "Nonlinear Blind Source Separation Using Sensor-Independent Signal\n  Representations", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a time series of signal measurements $x(t)$, having components $x_k\n\\mbox{ for } k = 1,2, \\ldots ,N$. This paper shows how to determine if these\nsignals are equal to linear or nonlinear mixtures of the state variables of two\nor more statistically-independent subsystems. First, the local distribution of\nmeasurement velocities ($\\dot{x}$) is processed in order to derive $N$ local\nvectors at each $x$. If the data are separable, each of these vectors is\ndirected along a subspace traversed by varying the state variable of one\nsubsystem, while all other subsystems are kept constant. Because of this\nproperty, these vectors can be used to determine if the data are separable,\nand, if they are, $x(t)$ can be transformed into a separable coordinate system\nin order to recover the time series of the independent subsystems. The method\nis illustrated by using it to blindly recover the separate utterances of two\nspeakers from nonlinear combinations of their waveforms.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 21:16:53 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Levin", "David N.", "", "University of Chicago"]]}, {"id": "1601.03448", "submitter": "Ege Rubak", "authors": "Jesper M{\\o}ller and Ege Rubak", "title": "Functional summary statistics for point processes on the sphere with an\n  application to determinantal point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study point processes on $\\mathbb S^d$, the $d$-dimensional unit sphere\n$\\mathbb S^d$, considering both the isotropic and the anisotropic case, and\nfocusing mostly on the spherical case $d=2$. The first part studies reduced\nPalm distributions and functional summary statistics, including nearest\nneighbour functions, empty space functions, and Ripley's and inhomogeneous\n$K$-functions. The second part partly discusses the appealing properties of\ndeterminantal point process (DPP) models on the sphere and partly considers the\napplication of functional summary statistics to DPPs. In fact DPPs exhibit\nrepulsiveness, but we also use them together with certain dependent thinnings\nwhen constructing point process models on the sphere with aggregation on the\nlarge scale and regularity on the small scale. We conclude with a discussion on\nfuture work on statistics for spatial point processes on the sphere.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 00:11:19 GMT"}, {"version": "v2", "created": "Sun, 12 Jun 2016 09:28:31 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["M\u00f8ller", "Jesper", ""], ["Rubak", "Ege", ""]]}, {"id": "1601.03501", "submitter": "Zheng Zhang", "authors": "K.C.G. Chan, K. Imai, S.C.P. Yam, Z. Zhang", "title": "Efficient nonparametric estimation of causal mediation effects", "comments": "Nonparametric Estimation, Natural direct effects, Natural indirect\n  effects, Treatment effects, Semiparametric efficiency", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An essential goal of program evaluation and scientific research is the\ninvestigation of causal mechanisms. Over the past several decades, causal\nmediation analysis has been used in medical and social sciences to decompose\nthe treatment effect into the natural direct and indirect effects. However, all\nof the existing mediation analysis methods rely on parametric modeling\nassumptions in one way or another, typically requiring researchers to specify\nmultiple regression models involving the treatment, mediator, outcome, and\npre-treatment confounders. To overcome this limitation, we propose a novel\nnonparametric estimation method for causal mediation analysis that eliminates\nthe need for applied researchers to model multiple conditional distributions.\nThe proposed method balances a certain set of empirical moments between the\ntreatment and control groups by weighting each observation; in particular, we\nestablish that the proposed estimator is globally semiparametric efficient. We\nalso show how to consistently estimate the asymptotic variance of the proposed\nestimator without additional efforts. Finally, we extend the proposed method to\nother relevant settings including the causal mediation analysis with multiple\nmediators.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 06:33:14 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Chan", "K. C. G.", ""], ["Imai", "K.", ""], ["Yam", "S. C. P.", ""], ["Zhang", "Z.", ""]]}, {"id": "1601.03517", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen and Geoffrey J McLachlan and Jeremy F P Ullmann and\n  Andrew L Janke", "title": "Spatial Clustering of Time-Series via Mixture of Autoregressions Models\n  and Markov Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series data arise in many medical and biological imaging scenarios. In\nsuch images, a time-series is obtained at each of a large number of\nspatially-dependent data units. It is interesting to organize these data into\nmodel-based clusters. A two-stage procedure is proposed. In Stage 1, a mixture\nof autoregressions (MoAR) model is used to marginally cluster the data. The\nMoAR model is fitted using maximum marginal likelihood (MMaL) estimation via an\nMM (minorization--maximization) algorithm. In Stage 2, a Markov random field\n(MRF) model induces a spatial structure onto the Stage 1 clustering. The MRF\nmodel is fitted using maximum pseudolikelihood (MPL) estimation via an MM\nalgorithm. Both the MMaL and MPL estimators are proved to be consistent.\nNumerical properties are established for both MM algorithms. A simulation study\ndemonstrates the performance of the two-stage procedure. An application to the\nsegmentation of a zebrafish brain calcium image is presented.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 08:31:20 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Nguyen", "Hien D", ""], ["McLachlan", "Geoffrey J", ""], ["Ullmann", "Jeremy F P", ""], ["Janke", "Andrew L", ""]]}, {"id": "1601.03605", "submitter": "Matthew Dunlop", "authors": "Matthew M. Dunlop, Marco A. Iglesias and Andrew M. Stuart", "title": "Hierarchical Bayesian Level Set Inversion", "comments": "43 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The level set approach has proven widely successful in the study of inverse\nproblems for interfaces, since its systematic development in the 1990s.\nRecently it has been employed in the context of Bayesian inversion, allowing\nfor the quantification of uncertainty within the reconstruction of interfaces.\nHowever the Bayesian approach is very sensitive to the length and amplitude\nscales in the prior probabilistic model. This paper demonstrates how the\nscale-sensitivity can be circumvented by means of a hierarchical approach,\nusing a single scalar parameter. Together with careful consideration of the\ndevelopment of algorithms which encode probability measure equivalences as the\nhierarchical parameter is varied, this leads to well-defined Gibbs based MCMC\nmethods found by alternating Metropolis-Hastings updates of the level set\nfunction and the hierarchical parameter. These methods demonstrably outperform\nnon-hierarchical Bayesian level set methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 14:33:53 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 20:44:04 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Dunlop", "Matthew M.", ""], ["Iglesias", "Marco A.", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "1601.03610", "submitter": "Fani Tsapeli", "authors": "Fani Tsapeli, Mirco Musolesi, Peter Tino", "title": "Non-Parametric Causality Detection: An Application to Social Media and\n  Financial Data", "comments": "Physica A: Statistical Mechanics and its Applications 2017", "journal-ref": null, "doi": "10.1016/j.physa.2017.04.101", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to behavioral finance, stock market returns are influenced by\nemotional, social and psychological factors. Several recent works support this\ntheory by providing evidence of correlation between stock market prices and\ncollective sentiment indexes measured using social media data. However, a pure\ncorrelation analysis is not sufficient to prove that stock market returns are\ninfluenced by such emotional factors since both stock market prices and\ncollective sentiment may be driven by a third unmeasured factor. Controlling\nfor factors that could influence the study by applying multivariate regression\nmodels is challenging given the complexity of stock market data. False\nassumptions about the linearity or non-linearity of the model and inaccuracies\non model specification may result in misleading conclusions.\n  In this work, we propose a novel framework for causal inference that does not\nrequire any assumption about the statistical relationships among the variables\nof the study and can effectively control a large number of factors. We apply\nour method in order to estimate the causal impact that information posted in\nsocial media may have on stock market returns of four big companies. Our\nresults indicate that social media data not only correlate with stock market\nreturns but also influence them.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 14:52:21 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 15:47:04 GMT"}, {"version": "v3", "created": "Sun, 11 Jun 2017 15:22:18 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Tsapeli", "Fani", ""], ["Musolesi", "Mirco", ""], ["Tino", "Peter", ""]]}, {"id": "1601.03640", "submitter": "Nirian Mart\\'in", "authors": "Narayanaswamy Balakrishnan, Nirian Martin, Leandro Pardo", "title": "Empirical phi-divergence test-statistics for the equalityof means of two\n  populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical phi-divergence test-statistics have demostrated to be a useful\ntechnique for the simple null hypothesis to improve the finite sample behaviour\nof the classical likelihood ratio test-statistic, as well asfor model\nmisspecification problems, in both cases for the one population problem. This\npaper introduces this methodology for two sample problems. A simulation study\nillustrates situations in which the new test-statistics become a competitive\ntool with respect to the classical z-test and the likelihood ratio\ntest-statistic.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 16:04:58 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Balakrishnan", "Narayanaswamy", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1601.03670", "submitter": "Eardi Lila", "authors": "Eardi Lila, John A. D. Aston, Laura M. Sangalli", "title": "Smooth Principal Component Analysis over two-dimensional manifolds with\n  an application to Neuroimaging", "comments": "33 pages", "journal-ref": null, "doi": "10.1214/16-AOAS975", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the analysis of high-dimensional neuroimaging signals located\nover the cortical surface, we introduce a novel Principal Component Analysis\ntechnique that can handle functional data located over a two-dimensional\nmanifold. For this purpose a regularization approach is adopted, introducing a\nsmoothing penalty coherent with the geodesic distance over the manifold. The\nmodel introduced can be applied to any manifold topology, can naturally handle\nmissing data and functional samples evaluated in different grids of points. We\napproach the discretization task by means of finite element analysis and\npropose an efficient iterative algorithm for its resolution. We compare the\nperformances of the proposed algorithm with other approaches classically\nadopted in literature. We finally apply the proposed method to resting state\nfunctional magnetic resonance imaging data from the Human Connectome Project,\nwhere the method shows substantial differential variations between brain\nregions that were not apparent with other approaches.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 17:22:04 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 11:33:02 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Lila", "Eardi", ""], ["Aston", "John A. D.", ""], ["Sangalli", "Laura M.", ""]]}, {"id": "1601.03704", "submitter": "Florencia Leonardi", "authors": "Florencia Leonardi and Peter B\\\"uhlmann", "title": "Computationally efficient change point detection for high-dimensional\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale sequential data is often exposed to some degree of inhomogeneity\nin the form of sudden changes in the parameters of the data-generating process.\nWe consider the problem of detecting such structural changes in a\nhigh-dimensional regression setting. We propose a joint estimator of the number\nand the locations of the change points and of the parameters in the\ncorresponding segments. The estimator can be computed using dynamic programming\nor, as we emphasize here, it can be approximated using a binary search\nalgorithm with $O(n \\log(n) \\mathrm{Lasso}(n))$ computational operations while\nstill enjoying essentially the same theoretical properties; here\n$\\mathrm{Lasso}(n)$ denotes the computational cost of computing the Lasso for\nsample size $n$. We establish oracle inequalities for the estimator as well as\nfor its binary search approximation, covering also the case with a large\n(asymptotically growing) number of change points. We evaluate the performance\nof the proposed estimation algorithms on simulated data and apply the\nmethodology to real data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 19:30:44 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Leonardi", "Florencia", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1601.03868", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko and Grigory Sokolov", "title": "An Analytic Expression for the Distribution of the Generalized\n  Shiryaev-Roberts Diffusion", "comments": "45 pages; 8 figures; to appear in Methodology and Computing in\n  Applied Probability", "journal-ref": null, "doi": "10.1007/s11009-016-9478-7", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the quickest change-point detection problem where the aim is to\ndetect the onset of a pre-specified drift in \"live\"-monitored standard Brownian\nmotion; the change-point is assumed unknown (nonrandom). The topic of interest\nis the distribution of the Generalized Shryaev-Roberts (GSR) detection\nstatistic set up to \"sense\" the presence of the drift. Specifically, we derive\na closed-form formula for the transition probability density function (pdf) of\nthe time-homogeneous Markov diffusion process generated by the GSR statistic\nwhen the Brownian motion under surveillance is \"drift-free\", i.e., in the\npre-change regime; the GSR statistic's (deterministic) nonnegative headstart is\nassumed arbitrarily given. The transition pdf formula is found analytically,\nthrough direct solution of the respective Kolmogorov forward equation via the\nFourier spectral method to achieve separation of the spacial and temporal\nvariables. The obtained result generalizes the well-known formula for the\n(pre-change) stationary distribution of the GSR statistic: the latter's\nstationary distribution is the temporal limit of the distribution sought in\nthis work. To conclude, we exploit the obtained formula numerically and briefly\nstudy the pre-change behavior of the GSR statistic versus three factors: (a)\ndrift-shift magnitude, (b) time, and (c) the GSR statistic's headstart.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 10:46:31 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Polunchenko", "Aleksey S.", ""], ["Sokolov", "Grigory", ""]]}, {"id": "1601.04083", "submitter": "Alexander Volfovsky", "authors": "Guillaume W. Basse, Alexander Volfovsky and Edoardo M. Airoldi", "title": "Observational studies with unknown time of treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time plays a fundamental role in causal analyses, where the goal is to\nquantify the effect of a specific treatment on future outcomes. In a randomized\nexperiment, times of treatment, and when outcomes are observed, are typically\nwell defined. In an observational study, treatment time marks the point from\nwhich pre-treatment variables must be regarded as outcomes, and it is often\nstraightforward to establish. Motivated by a natural experiment in online\nmarketing, we consider a situation where useful conceptualizations of the\nexperiment behind an observational study of interest lead to uncertainty in the\ndetermination of times at which individual treatments take place. Of interest\nis the causal effect of heavy snowfall in several parts of the country on daily\nmeasures of online searches for batteries, and then purchases. The data\navailable give information on actual snowfall, whereas the natural treatment is\nthe anticipation of heavy snowfall, which is not observed. In this article, we\nintroduce formal assumptions and inference methodology centered around a novel\nnotion of plausible time of treatment. These methods allow us to explicitly\nbound the last plausible time of treatment in observational studies with\nunknown times of treatment, and ultimately yield valid causal estimates in such\nsituations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 21:39:43 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Basse", "Guillaume W.", ""], ["Volfovsky", "Alexander", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1601.04096", "submitter": "Michael GB Blum", "authors": "Louisiane Lemaire, Flora Jay, I-Hung Lee, Katalin Csill\\'ery, Michael\n  G. B. Blum", "title": "Goodness-of-fit statistics for approximate Bayesian computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation is a statistical framework that uses\nnumerical simulations to calibrate and compare models. Instead of computing\nlikelihood functions, Approximate Bayesian computation relies on numerical\nsimulations, which makes it applicable to complex models in ecology and\nevolution. As usual for statistical modeling, evaluating goodness-of-fit is a\nfundamental step for Approximate Bayesian Computation. Here, we introduce a\ngoodness-of-fit approach based on hypothesis-testing. We introduce two test\nstatistics based on the mean distance between numerical summaries of the data\nand simulated ones. One test statistic relies on summaries simulated with the\nprior predictive distribution whereas the other one relies on simulations from\nthe posterior predictive distribution. For different coalescent models, we find\nthat the statistics are well calibrated, meaning that the type I error can be\ncontrolled. However, the statistical power of the two statistics is extremely\nvariable across models ranging from 20% to 100%. The difference of power\nbetween the two statistics is negligible in models of demographic inference but\nsubstantial in an additional and purely statistical example. When analyzing\nresequencing data to evaluate models of human demography, the two statistics\nconfirm that an out-of-Africa bottleneck cannot be rejected for Asiatic and\nEuropean data. We also consider two speciation models in the context of a\nbutterfly species complex. One goodness-of-fit statistic indicates a poor fit\nfor both models, and the numerical summaries causing the poor fit were\nidentified using posterior predictive checks. Statistical tests for\ngoodness-of-fit should foster evaluation of model fit in Approximate Bayesian\nComputation. The test statistic based on simulations from the prior predictive\ndistribution is implemented in the gfit function of the R abc package.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 23:01:42 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Lemaire", "Louisiane", ""], ["Jay", "Flora", ""], ["Lee", "I-Hung", ""], ["Csill\u00e9ry", "Katalin", ""], ["Blum", "Michael G. B.", ""]]}, {"id": "1601.04262", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko", "title": "Exact Distribution of the Generalized Shiryaev-Roberts Stopping Time\n  Under the Minimax Brownian Motion Setup", "comments": "39 pages; 13 figures", "journal-ref": "Sequential Analysis, vol. 35, no. 1, pp. 108-143, 2016", "doi": "10.1080/07474946.2016.1132066", "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the quickest change-point detection problem where the aim is to\ndetect the onset of a pre-specified drift in \"live\"-monitored standard Brownian\nmotion; the change-point is assumed unknown (nonrandom). The object of interest\nis the distribution of the stopping time associated with the Generalized\nShryaev-Roberts (GSR) detection procedure set up to \"sense\" the presence of the\ndrift in the Brownian motion under surveillance. Specifically, we seek the GSR\nstopping time's survival function (the tail probability that no alarm is\ntriggered by the GSR procedure prior to a given point in time), and distinguish\ntwo scenarios: (a) when the drift never sets in (pre-change regime) and (b)\nwhen the drift is in effect ab initio (post-change regime). Under each\nscenario, we obtain a closed-form formula for the respective survival function,\nwith the GSR statistic's (deterministic) nonnegative headstart assumed\narbitrarily given. The two formulae are found analytically, through direct\nsolution of the respective Kolmogorov forward equation via the Fourier spectral\nmethod to achieve separation of the spacial and temporal variables. We then\nexploit the obtained formulae numerically and characterize the pre- and\npost-change distributions of the GSR stopping time depending on three factors:\n(a) magnitude of the drift, (b) detection threshold, and (c) the GSR\nstatistic's headstart.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 08:37:06 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 18:27:49 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Polunchenko", "Aleksey S.", ""]]}, {"id": "1601.04331", "submitter": "Maria DeYoreo", "authors": "Maria DeYoreo and Athanasios Kottas", "title": "A Bayesian Nonparametric Markovian Model for Nonstationary Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stationary time series models built from parametric distributions are, in\ngeneral, limited in scope due to the assumptions imposed on the residual\ndistribution and autoregression relationship. We present a modeling approach\nfor univariate time series data, which makes no assumptions of stationarity,\nand can accommodate complex dynamics and capture nonstandard distributions. The\nmodel for the transition density arises from the conditional distribution\nimplied by a Bayesian nonparametric mixture of bivariate normals. This implies\na flexible autoregressive form for the conditional transition density, defining\na time-homogeneous, nonstationary, Markovian model for real-valued data indexed\nin discrete-time. To obtain a more computationally tractable algorithm for\nposterior inference, we utilize a square-root-free Cholesky decomposition of\nthe mixture kernel covariance matrix. Results from simulated data suggest the\nmodel is able to recover challenging transition and predictive densities. We\nalso illustrate the model on time intervals between eruptions of the Old\nFaithful geyser. Extensions to accommodate higher order structure and to\ndevelop a state-space model are also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 19:37:58 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 18:19:24 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 16:23:20 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["DeYoreo", "Maria", ""], ["Kottas", "Athanasios", ""]]}, {"id": "1601.04397", "submitter": "Hongzhe Li", "authors": "Yuanpei Cao, Wei Lin, Hongzhe Li", "title": "Large Covariance Estimation for Compositional Data via\n  Composition-Adjusted Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional compositional data arise naturally in many applications such\nas metagenomic data analysis. The observed data lie in a high-dimensional\nsimplex, and conventional statistical methods often fail to produce sensible\nresults due to the unit-sum constraint. In this article, we address the problem\nof covariance estimation for high-dimensional compositional data, and introduce\na composition-adjusted thresholding (COAT) method under the assumption that the\nbasis covariance matrix is sparse. Our method is based on a decomposition\nrelating the compositional covariance to the basis covariance, which is\napproximately identifiable as the dimensionality tends to infinity. The\nresulting procedure can be viewed as thresholding the sample centered log-ratio\ncovariance matrix and hence is scalable for large covariance matrices. We\nrigorously characterize the identifiability of the covariance parameters,\nderive rates of convergence under the spectral norm, and provide theoretical\nguarantees on support recovery. Simulation studies demonstrate that the COAT\nestimator outperforms some naive thresholding estimators that ignore the unique\nfeatures of compositional data. We apply the proposed method to the analysis of\na microbiome dataset in order to understand the dependence structure among\nbacterial taxa in the human gut.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 04:34:25 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Cao", "Yuanpei", ""], ["Lin", "Wei", ""], ["Li", "Hongzhe", ""]]}, {"id": "1601.04405", "submitter": "Saeid Rezakhah", "authors": "S. Rezakhah and Y. Maleki", "title": "Discretization of Continuous Time Discrete Scale Invariant Processes:\n  Estimation and Spectra", "comments": "15 pages", "journal-ref": null, "doi": "10.1007/s10955-016-1541-9", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imposing some flexible sampling scheme we provide some discretization of\ncontinuous time discrete scale invariant (DSI) processes which is a subsidiary\ndiscrete time DSI process.\n  Then by introducing some simple random measure we provide a second continuous\ntime DSI process which provides a proper approximation of the first one. This\nenables us to provide a bilateral relation between covariance functions of the\nsubsidiary process and the new continuous time processes. The time varying\nspectral representation of such continuous time DSI process is characterized,\nand its spectrum is estimated. Also, a new method for estimation time dependent\nHurst parameter of such processes is provided which gives a more accurate\nestimation. The performance of this estimation method is studied via\nsimulation. Finally this method is applied to the real data of S$\\&$P500 and\nDow Jones indices for some special periods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 05:54:16 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Rezakhah", "S.", ""], ["Maleki", "Y.", ""]]}, {"id": "1601.04586", "submitter": "Binhuan Wang", "authors": "Binhuan Wang, Yilong Zhang, Will Wei Sun, Yixin Fang", "title": "Sparse Convex Clustering", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2017.1377081", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex clustering, a convex relaxation of k-means clustering and hierarchical\nclustering, has drawn recent attentions since it nicely addresses the\ninstability issue of traditional nonconvex clustering methods. Although its\ncomputational and statistical properties have been recently studied, the\nperformance of convex clustering has not yet been investigated in the\nhigh-dimensional clustering scenario, where the data contains a large number of\nfeatures and many of them carry no information about the clustering structure.\nIn this paper, we demonstrate that the performance of convex clustering could\nbe distorted when the uninformative features are included in the clustering. To\novercome it, we introduce a new clustering method, referred to as Sparse Convex\nClustering, to simultaneously cluster observations and conduct feature\nselection. The key idea is to formulate convex clustering in a form of\nregularization, with an adaptive group-lasso penalty term on cluster centers.\nIn order to optimally balance the tradeoff between the cluster fitting and\nsparsity, a tuning criterion based on clustering stability is developed. In\ntheory, we provide an unbiased estimator for the degrees of freedom of the\nproposed sparse convex clustering method. Finally, the effectiveness of the\nsparse convex clustering is examined through a variety of numerical experiments\nand a real data application.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 16:03:35 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 03:26:26 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2016 14:38:43 GMT"}, {"version": "v4", "created": "Fri, 10 Feb 2017 16:51:07 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Wang", "Binhuan", ""], ["Zhang", "Yilong", ""], ["Sun", "Will Wei", ""], ["Fang", "Yixin", ""]]}, {"id": "1601.04767", "submitter": "Gareth Williams", "authors": "K. Ryan, D. Gareth Williams, David J. Balding", "title": "Encoding of low-quality DNA profiles as genotype probability matrices\n  for improved profile comparisons, relatedness evaluation and database\n  searches", "comments": "28 pages. Accepted for publication 2-Sep-2016 - Forensic Science\n  International: Genetics", "journal-ref": "Forensic Science International: Genetics 25 (2016) 227-39", "doi": "10.1016/j.fsigen.2016.09.004", "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many DNA profiles recovered from crime scene samples are of a quality that\ndoes not allow them to be searched against, nor entered into, databases. We\npropose a method for the comparison of profiles arising from two DNA samples,\none or both of which can have multiple donors and be affected by low DNA\ntemplate or degraded DNA. We compute likelihood ratios to evaluate the\nhypothesis that the two samples have a common DNA donor, and hypotheses\nspecifying the relatedness of two donors. Our method uses a probability\ndistribution for the genotype of the donor of interest in each sample. This\ndistribution can be obtained from a statistical model, or we can exploit the\nability of trained human experts to assess genotype probabilities, thus\nextracting much information that would be discarded by standard interpretation\nrules. Our method is compatible with established methods in simple settings,\nbut is more widely applicable and can make better use of information than many\ncurrent methods for the analysis of mixed-source, low-template DNA profiles. It\ncan accommodate uncertainty arising from relatedness instead of or in addition\nto uncertainty arising from noisy genotyping. We describe a computer program\nGPMDNA, available under an open source license, to calculate LRs using the\nmethod presented in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 01:11:31 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 22:07:58 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Ryan", "K.", ""], ["Williams", "D. Gareth", ""], ["Balding", "David J.", ""]]}, {"id": "1601.04770", "submitter": "Enming Luo", "authors": "Enming Luo, Stanley H. Chan, Truong Q. Nguyen", "title": "Adaptive Image Denoising by Mixture Adaptation", "comments": "15 pages", "journal-ref": null, "doi": "10.1109/TIP.2016.2590318", "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adaptive learning procedure to learn patch-based image priors\nfor image denoising. The new algorithm, called the Expectation-Maximization\n(EM) adaptation, takes a generic prior learned from a generic external database\nand adapts it to the noisy image to generate a specific prior. Different from\nexisting methods that combine internal and external statistics in ad-hoc ways,\nthe proposed algorithm is rigorously derived from a Bayesian hyper-prior\nperspective. There are two contributions of this paper: First, we provide full\nderivation of the EM adaptation algorithm and demonstrate methods to improve\nthe computational complexity. Second, in the absence of the latent clean image,\nwe show how EM adaptation can be modified based on pre-filtering. Experimental\nresults show that the proposed adaptation algorithm yields consistently better\ndenoising results than the one without adaptation and is superior to several\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 01:41:36 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 19:27:36 GMT"}, {"version": "v3", "created": "Sat, 25 Jun 2016 02:37:45 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Luo", "Enming", ""], ["Chan", "Stanley H.", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1601.04826", "submitter": "Anna Kiriliouk", "authors": "John H.J. Einmahl and Anna Kiriliouk and Johan Segers", "title": "A continuous updating weighted least squares estimator of tail\n  dependence in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-based procedures are a common way to estimate tail dependence\nparameters. They are not applicable, however, in non-differentiable models such\nas those arising from recent max-linear structural equation models. Moreover,\nthey can be hard to compute in higher dimensions. An adaptive weighted\nleast-squares procedure matching nonparametric estimates of the stable tail\ndependence function with the corresponding values of a parametrically specified\nproposal yields a novel minimum-distance estimator. The estimator is easy to\ncalculate and applies to a wide range of sampling schemes and tail dependence\nmodels. In large samples, it is asymptotically normal with an explicit and\nestimable covariance matrix. The minimum distance obtained forms the basis of a\ngoodness-of-fit statistic whose asymptotic distribution is chi-square.\nExtensive Monte Carlo simulations confirm the excellent finite-sample\nperformance of the estimator and demonstrate that it is a strong competitor to\ncurrently available methods. The estimator is then applied to disentangle\nsources of tail dependence in European stock markets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 08:56:39 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Einmahl", "John H. J.", ""], ["Kiriliouk", "Anna", ""], ["Segers", "Johan", ""]]}, {"id": "1601.04841", "submitter": "Peter McCullagh", "authors": "Walter Dempsey and Peter McCullagh", "title": "Vital variables and survival processes", "comments": "13 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of a survival study is partly on the distribution of survival\ntimes, and partly on the health or quality of life of patients while they live.\nHealth varies over time, and survival is the most basic aspect of health, so\nthe two aspects are closely intertwined. Depending on the nature of the study,\na range of variables may be measured; some constant in time, others not; some\nregarded as responses, others as explanatory risk factors; some directly and\npersonally health-related, others less directly so. This paper begins by\nclassifying variables that may arise in such a setting, emphasizing in\nparticular, the mathematical distinction between vital and non-vital variables.\nWe examine also various types of probabilistic relationships that may exist\namong variables. Independent evolution is an asymmetric relation, which is\nintended to encapsulate the notion of one process driving the other; $X$~is a\ndriver of~$Y$ if $X$ evolves independently of the history of~$Y$. This concept\narises in several places in the study of survival processes.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 09:41:02 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Dempsey", "Walter", ""], ["McCullagh", "Peter", ""]]}, {"id": "1601.05078", "submitter": "Mandev Gill", "authors": "Mandev S. Gill, Philippe Lemey, Shannon N. Bennett, Roman Biek, and\n  Marc A. Suchard", "title": "Understanding Past Population Dynamics: Bayesian Coalescent-Based\n  Modeling with Covariates", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective population size characterizes the genetic variability in a\npopulation and is a parameter of paramount importance in population genetics.\nKingman's coalescent process enables inference of past population dynamics\ndirectly from molecular sequence data, and researchers have developed a number\nof flexible coalescent-based models for Bayesian nonparametric estimation of\nthe effective population size as a function of time. A major goal of\ndemographic reconstruction is understanding the association between the\neffective population size and potential explanatory factors. Building upon\nBayesian nonparametric coalescent-based approaches, we introduce a flexible\nframework that incorporates time-varying covariates through Gaussian Markov\nrandom fields. To approximate the posterior distribution, we adapt efficient\nMarkov chain Monte Carlo algorithms designed for highly structured Gaussian\nmodels. Incorporating covariates into the demographic inference framework\nenables the modeling of associations between the effective population size and\ncovariates while accounting for uncertainty in population histories.\nFurthermore, it can lead to more precise estimates of population dynamics. We\napply our model to four examples. We reconstruct the demographic history of\nraccoon rabies in North America and find a significant association with the\nspatiotemporal spread of the outbreak. Next, we examine the effective\npopulation size trajectory of the DENV-4 virus in Puerto Rico along with viral\nisolate count data and find similar cyclic patterns. We compare the population\nhistory of the HIV-1 CRF02_AG clade in Cameroon with HIV incidence and\nprevalence data and find that the effective population size is more reflective\nof incidence rate. Finally, we explore the hypothesis that the population\ndynamics of musk ox during the Late Quaternary period were related to climate\nchange.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 20:55:10 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Gill", "Mandev S.", ""], ["Lemey", "Philippe", ""], ["Bennett", "Shannon N.", ""], ["Biek", "Roman", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1601.05155", "submitter": "Peng Ding", "authors": "Peng Ding, Tyler J. VanderWeele", "title": "Sharp sensitivity bounds for mediation under unmeasured mediator-outcome\n  confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often of interest to decompose a total effect of an exposure into the\ncomponent that acts on the outcome through some mediator and the component that\nacts independently through other pathways. Said another way, we are interested\nin the direct and indirect effects of the exposure on the outcome. Even if the\nexposure is randomly assigned, it is often infeasible to randomize the\nmediator, leaving the mediator-outcome confounding not fully controlled. We\ndevelop a sensitivity analysis technique that can bound the direct and indirect\neffects without parametric assumptions about the unmeasured mediator-outcome\nconfounding.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 03:03:37 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Ding", "Peng", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "1601.05156", "submitter": "Boyu Ren", "authors": "Boyu Ren, Sergio Bacallado, Stefano Favaro, Susan Holmes, Lorenzo\n  Trippa", "title": "Bayesian Nonparametric Ordination for the Analysis of Microbial\n  Communities", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2017.1288631", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human microbiome studies use sequencing technologies to measure the abundance\nof bacterial species or Operational Taxonomic Units (OTUs) in samples of\nbiological material. Typically the data are organized in contingency tables\nwith OTU counts across heterogeneous biological samples. In the microbial\necology community, ordination methods are frequently used to investigate latent\nfactors or clusters that capture and describe variations of OTU counts across\nbiological samples. It remains important to evaluate how uncertainty in\nestimates of each biological sample's microbial distribution propagates to\nordination analyses, including visualization of clusters and projections of\nbiological samples on low dimensional spaces. We propose a Bayesian analysis\nfor dependent distributions to endow frequently used ordinations with estimates\nof uncertainty. A Bayesian nonparametric prior for dependent normalized random\nmeasures is constructed, which is marginally equivalent to the normalized\ngeneralized Gamma process, a well-known prior for nonparametric analyses. In\nour prior the dependence and similarity between microbial distributions is\nrepresented by latent factors that concentrate in a low dimensional space. We\nuse a shrinkage prior to tune the dimensionality of the latent factors. The\nresulting posterior samples of model parameters can be used to evaluate\nuncertainty in analyses routinely applied in microbiome studies. Specifically,\nby combining them with multivariate data analysis techniques we can visualize\ncredible regions in ecological ordination plots. The characteristics of the\nproposed model are illustrated through a simulation study and applications in\ntwo microbiome datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 03:05:10 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 23:09:08 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Ren", "Boyu", ""], ["Bacallado", "Sergio", ""], ["Favaro", "Stefano", ""], ["Holmes", "Susan", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "1601.05199", "submitter": "Leopoldo Catania", "authors": "Mauro Bernardi and Leopoldo Catania", "title": "Portfolio Optimisation Under Flexible Dynamic Dependence Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signals coming from multivariate higher order conditional moments as well as\nthe information contained in exogenous covariates, can be effectively exploited\nby rational investors to allocate their wealth among different risky investment\nopportunities. This paper proposes a new flexible dynamic copula model being\nable to explain and forecast the time-varying shape of large dimensional asset\nreturns distributions. Moreover, we let the univariate marginal distributions\nto be driven by an updating mechanism based on the scaled score of the\nconditional distribution. This framework allows us to introduce time-variation\nin up to the fourth moment of the conditional distribution. The time-varying\ndependence pattern is subsequently modelled as function of a latent Markov\nSwitching process, allowing also for the inclusion of exogenous covariates in\nthe dynamic updating equation. We empirically assess that the proposed model\nsubstantially improves the optimal portfolio allocation of rational investors\nmaximising their expected utility.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 08:42:28 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Bernardi", "Mauro", ""], ["Catania", "Leopoldo", ""]]}, {"id": "1601.05408", "submitter": "Mevin Hooten", "authors": "Mevin B. Hooten and Devin S. Johnson", "title": "Basis Function Models for Animal Movement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in satellite-based data collection techniques have served as a\ncatalyst for new statistical methodology to analyze these data. In wildlife\necological studies, satellite-based data and methodology have provided a wealth\nof information about animal space use and the investigation of individual-based\nanimal-environment relationships. With the technology for data collection\nimproving dramatically over time, we are left with massive archives of\nhistorical animal telemetry data of varying quality. While many contemporary\nstatistical approaches for inferring movement behavior are specified in\ndiscrete time, we develop a flexible continuous-time stochastic integral\nequation framework that is amenable to reduced-rank second-order covariance\nparameterizations. We demonstrate how the associated first-order basis\nfunctions can be constructed to mimic behavioral characteristics in realistic\ntrajectory processes using telemetry data from mule deer and mountain lion\nindividuals in western North America. Our approach is parallelizable and\nprovides inference for heterogeneous trajectories using nonstationary spatial\nmodeling techniques that are feasible for large telemetry data sets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 20:59:48 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 22:52:03 GMT"}, {"version": "v3", "created": "Thu, 6 Oct 2016 16:49:26 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Hooten", "Mevin B.", ""], ["Johnson", "Devin S.", ""]]}, {"id": "1601.05630", "submitter": "John Palowitch", "authors": "John Palowitch, Shankar Bhamidi, Andrew B. Nobel", "title": "Significance-based community detection in weighted networks", "comments": "Code and supplemental info available at\n  http://stats.johnpalowitch.com/ccme. V3 changes: based on lengthy referee\n  revision process, new theoretical sections added, + major organizational\n  changes. V2 changes: grant info added, 1 reference added, bibliography\n  section moved to end, condensed bib line spacing, corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is the process of grouping strongly connected nodes in a\nnetwork. Many community detection methods for un-weighted networks have a\ntheoretical basis in a null model. Communities discovered by these methods\ntherefore have interpretations in terms of statistical signficance. In this\npaper, we introduce a null for weighted networks called the continuous\nconfiguration model. We use the model both as a tool for community detection\nand for simulating weighted networks with null nodes. First, we propose a\ncommunity extraction algorithm for weighted networks which incorporates\niterative hypothesis testing under the null. We prove a central limit theorem\nfor edge-weight sums and asymptotic consistency of the algorithm under a\nweighted stochastic block model. We then incorporate the algorithm in a\ncommunity detection method called CCME. To benchmark the method, we provide a\nsimulation framework incorporating the null to plant \"background\" nodes in\nweighted networks with communities. We show that the empirical performance of\nCCME on these simulations is competitive with existing methods, particularly\nwhen overlapping communities and background nodes are present. To further\nvalidate the method, we present two real-world networks with potential\nbackground nodes and analyze them with CCME, yielding results that reveal\nmacro-features of the corresponding systems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 13:44:03 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2016 17:51:11 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 19:51:12 GMT"}, {"version": "v4", "created": "Mon, 23 Oct 2017 07:39:11 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Palowitch", "John", ""], ["Bhamidi", "Shankar", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1601.05633", "submitter": "Hyungsuk Tak", "authors": "Hyungsuk Tak, Xiao-Li Meng, David A. van Dyk", "title": "A Repelling-Attracting Metropolis Algorithm for Multimodality", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2017.1415911", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the Metropolis algorithm is simple to implement, it often has\ndifficulties exploring multimodal distributions. We propose the\nrepelling-attracting Metropolis (RAM) algorithm that maintains the\nsimple-to-implement nature of the Metropolis algorithm, but is more likely to\njump between modes. The RAM algorithm is a Metropolis-Hastings algorithm with a\nproposal that consists of a downhill move in density that aims to make local\nmodes repelling, followed by an uphill move in density that aims to make local\nmodes attracting. The downhill move is achieved via a reciprocal Metropolis\nratio so that the algorithm prefers downward movement. The uphill move does the\nopposite using the standard Metropolis ratio which prefers upward movement.\nThis down-up movement in density increases the probability of a proposed move\nto a different mode. Because the acceptance probability of the proposal\ninvolves a ratio of intractable integrals, we introduce an auxiliary variable\nwhich creates a term in the acceptance probability that cancels with the\nintractable ratio. Using several examples, we demonstrate the potential for the\nRAM algorithm to explore a multimodal distribution more efficiently than a\nMetropolis algorithm and with less tuning than is commonly required by\ntempering-based methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 13:54:17 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 22:13:29 GMT"}, {"version": "v3", "created": "Tue, 13 Sep 2016 13:26:00 GMT"}, {"version": "v4", "created": "Fri, 16 Jun 2017 16:37:32 GMT"}, {"version": "v5", "created": "Fri, 20 Oct 2017 16:02:37 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Tak", "Hyungsuk", ""], ["Meng", "Xiao-Li", ""], ["van Dyk", "David A.", ""]]}, {"id": "1601.05820", "submitter": "Eric Tchetgen Tchetgen", "authors": "James Robins, Lingling Li, Eric Tchetgen Tchetgen, Aad van der Vaart", "title": "Technical Report: Higher Order Influence Functions and Minimax\n  Estimation of Nonlinear Functionals", "comments": "arXiv admin note: substantial text overlap with arXiv:0805.3040", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robins et al, 2008, published a theory of higher order influence functions\nfor inference in semi- and non-parametric models. This paper is a comprehensive\nmanuscript from which Robins et al, was drawn. The current paper includes many\nresults and proofs that were not included in Robins et al due to space\nlimitation. Particular results contained in the present paper that were not\nreported in Robins et al include the following. Given a set of functionals and\ntheir corresponding higher order influence functions, we show how to derive the\nhigher order influence function of their product. We apply this result to\nobtain higher order influence functions and associated estimators for the mean\nof a response Y subject to monotone missingness under missing at random. These\nresults also apply to estimating the causal effect of a time dependent\ntreatment on an outcome Y in the presence of time-varying confounding. Finally,\nwe include an appendix that contains proofs for all theorems that were stated\nwithout proof in Robins et al, 2008. The initial part of the paper is closely\nrelated to Robins et al, the latter parts differ.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 03:09:42 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Robins", "James", ""], ["Li", "Lingling", ""], ["Tchetgen", "Eric Tchetgen", ""], ["van der Vaart", "Aad", ""]]}, {"id": "1601.05886", "submitter": "Zhendong Huang", "authors": "Zhendong Huang, Davide Ferrari and Guoqi Qian", "title": "Parsimonious and powerful composite likelihood testing for group\n  difference and genotype-phenotype association", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing the association between a phenotype and many genetic variants from\ncase-control data is essential in genome-wide association study (GWAS). This is\na challenging task as many such variants are correlated or non-informative.\nSimilarities exist in testing the population difference between two groups of\nhigh dimensional data with intractable full likelihood function. Testing may be\ntackled by a maximum composite likelihood (MCL) not entailing the full\nlikelihood, but current MCL tests are subject to power loss for involving\nnon-informative or redundant sub-likelihoods. In this paper, we develop a\nforward search and test method for simultaneous powerful group difference\ntesting and informative sub-likelihoods composition. Our method constructs a\nsequence of Wald-type test statistics by including only informative\nsub-likelihoods progressively so as to improve the test power under local\nsparsity alternatives. Numerical studies show that it achieves considerable\nimprovement over the available tests as the modeling complexity grows. Our\nmethod is further validated by testing the motivating GWAS data on breast\ncancer with interesting results obtained.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 06:26:10 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Huang", "Zhendong", ""], ["Ferrari", "Davide", ""], ["Qian", "Guoqi", ""]]}, {"id": "1601.05887", "submitter": "Pritam Ranjan", "authors": "Derek Bingham, Pritam Ranjan, William Welch", "title": "Design of Computer Experiments for Optimization, Estimation of Function\n  Contours, and Related Objectives", "comments": "14 pages, 3 figures. in Chapter 7 - Statistics in Action: A Canadian\n  Outlook (ISBN 9781482236231 - CAT# K23109), Edited by Jerald F . Lawless\n  Chapman and Hall/CRC, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computer code or simulator is a mathematical representation of a physical\nsystem, for example a set of differential equations. Running the code with\ngiven values of the vector of inputs, x, leads to an output y(x) or several\nsuch outputs. For instance, one application we use for illustration simulates\nthe average tidal power, y, generated as a function of the turbine location, x\n= (x1, x2), in the Bay of Fundy, Nova Scotia, Canada (Ranjan et al. 2011).\nPerforming scientific or engineering experiments via such a computer code is\noften more time and cost effective than running a physical experiment.\n  Choosing new runs sequentially for optimization, moving y to a target, etc.\nhas been formalized using the concept of expected improvement (Jones et al.\n1998). The next experimental run is made where the expected improvement in the\nfunction of interest is largest. This expectation is with respect to the\npredictive distribution of y from a statistical model relating y to x. By\nconsidering a set of possible inputs x for the new run, we can choose that\nwhich gives the largest expectation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 06:26:19 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Bingham", "Derek", ""], ["Ranjan", "Pritam", ""], ["Welch", "William", ""]]}, {"id": "1601.05890", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao", "title": "Covariate Balancing Propensity Score by Tailored Loss Functions", "comments": "28 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, propensity scores are commonly estimated by maxi-\nmum likelihood but may fail to balance high-dimensional pre-treatment\ncovariates even after specification search. We introduce a general framework\nthat unifies and generalizes several recent proposals to improve covariate\nbalance when designing an observational study. In- stead of the likelihood\nfunction, we propose to optimize special loss functions---covariate balancing\nscoring rules (CBSR)---to estimate the propensity score. A CBSR is uniquely\ndetermined by the link function in the GLM and the estimand (a weighted average\ntreatment effect). We show CBSR does not lose asymptotic efficiency to the\nBernoulli likelihood in estimating the weighted average treatment effect\ncompared, but CBSR is much more robust in finite sample. Borrowing tools\ndeveloped in statistical learning, we propose practical strategies to balance\ncovariate functions in rich function classes. This is useful to estimate the\nmaximum bias of the inverse probability weighting (IPW) estimators and\nconstruct honest confidence interval in finite sample. Lastly, we provide\nseveral numerical examples to demonstrate the trade-off of bias and variance in\nthe IPW-type estimators and the trade-off in balancing different function\nclasses of the covariates.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 06:33:41 GMT"}, {"version": "v2", "created": "Sun, 17 Apr 2016 23:03:26 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 19:37:13 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Zhao", "Qingyuan", ""]]}, {"id": "1601.05914", "submitter": "Bertrand Iooss", "authors": "Lo\\\"ic Le Gratiet, Bertrand Iooss (GdR MASCOT-NUM), G\\'eraud Blatman\n  (EDF R\\&D), Thomas Browne (UPD5), Sara Cordeiro, Benjamin Goursaud (EDF R\\&D)", "title": "Model Assisted Probability of Detection curves: New statistical tools\n  and progressive methodology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Probability Of Detection (POD) curve is a standard tool in several\nindustries to evaluate the performance of Non Destructive Testing (NDT)\nprocedures for the detection of harmful defects for the inspected structure.\nDue to new capabilities of NDT process numerical simulation , Model Assisted\nProbability of Detection (MAPOD) approaches have also been recently developed.\nIn this paper, a generic and progressive MAPOD methodology is proposed. Limits\nand assumptions of the classical methods are enlightened, while new\nmetamodel-based methods are proposed. They allow to access to relevant\ninformation based on sensitivity analysis of MAPOD inputs. Applications are\nperformed on Eddy Current Non Destructive Examination numerical data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 09:12:03 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Gratiet", "Lo\u00efc Le", "", "GdR MASCOT-NUM"], ["Iooss", "Bertrand", "", "GdR MASCOT-NUM"], ["Blatman", "G\u00e9raud", "", "EDF R\\&D"], ["Browne", "Thomas", "", "UPD5"], ["Cordeiro", "Sara", "", "EDF R\\&D"], ["Goursaud", "Benjamin", "", "EDF R\\&D"]]}, {"id": "1601.05940", "submitter": "Anthony Chronopoulos", "authors": "Antonios Bassias and Anthony Chronopoulos", "title": "Statistical Performance Analysis of the MUSIC Algorithm in Angular\n  Sectors", "comments": null, "journal-ref": "Journal of Signal Processing, Vol.15, No.1, pp. 37-46, 2011", "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with the problem of the statistical performance analysis\nof the MUSIC ( Multiple Signal Classification ) algorithm which is an eigen\ndecomposition based method for the estimation of the angles of arrival of\nsignals received by an array of sensors. In past work the performance of the\nMUSIC algorithm was studied ( via an asymptotic statistical analysis of the\nnull spectrum of the algorithm ) for the case of two plane waves of equal power\nin noise. In this article, a new theoretical formula is derived for the signal\nto noise ratio resolution threshold of two uncorrelated, narrow band plane\nwaves with equal powers in angular sectors received by an array of sensors. The\naccuracy of the formula is assessed using examples which compute the\ntheoretical signal to noise ratio resolution threshold and compare it with the\nthreshold obtained from simulations.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 10:15:48 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Bassias", "Antonios", ""], ["Chronopoulos", "Anthony", ""]]}, {"id": "1601.06288", "submitter": "Zijian Guo", "authors": "Zijian Guo, Jing Cheng, Scott A. Lorch and Dylan S. Small", "title": "Using an Instrumental Variable to Test for Unmeasured Confounding", "comments": null, "journal-ref": "Statistics in Medicine 2014, Volume 33, Issue 20, Pages 3528-3546", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important concern in an observational study is whether or not there is\nunmeasured confounding, i.e., unmeasured ways in which the treatment and\ncontrol groups differ before treatment that affect the outcome. We develop a\ntest of whether there is unmeasured confounding when an instrumental variable\n(IV) is available. An IV is a variable that is independent of the unmeasured\nconfounding and encourages a subject to take one treatment level vs. another,\nwhile having no effect on the outcome beyond its encouragement of a certain\ntreatment level. We show what types of unmeasured confounding can be tested for\nwith an IV and develop a test for this type of unmeasured confounding that has\ncorrect type I error rate. We show that the widely used Durbin-Wu-Hausman (DWH)\ntest can have inflated type I error rates when there is treatment effect\nheterogeneity. Additionally, we show that our test provides more insight into\nthe nature of the unmeasured confounding than the DWH test. We apply our test\nto an observational study of the effect of a premature infant being delivered\nin a high-level neonatal intensive care unit (one with mechanical assisted\nventilation and high volume) vs. a lower level unit, using the excess travel\ntime a mother lives from the nearest high-level unit to the nearest lower-level\nunit as an IV.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 16:54:11 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Guo", "Zijian", ""], ["Cheng", "Jing", ""], ["Lorch", "Scott A.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1601.06294", "submitter": "Zijian Guo", "authors": "Zijian Guo, Dylan S. Small, Stuart A. Gansky and Jing Cheng", "title": "Mediation Analysis for Count and Zero-Inflated Count Data without\n  Sequential Ignorability and Its Application in Dental Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis seeks to understand the mechanism by which a treatment\naffects an outcome. Count or zero-inflated count outcome are common in many\nstudies in which mediation analysis is of interest. For example, in dental\nstudies, outcomes such as decayed, missing and filled teeth are typically zero\ninflated. Existing mediation analysis approaches for count data assume\nsequential ignorability of the mediator. This is often not plausible because\nthe mediator is not randomized so that there are unmeasured confounders\nassociated with the mediator and the outcome. In this paper, we develop causal\nmethods based on instrumental variable (IV) approaches for mediation analysis\nfor count data possibly with a lot of zeros that do not require the assumption\nof sequential ignorability. We first define the direct and indirect effect\nratios for those data, and then propose estimating equations and use empirical\nlikelihood to estimate the direct and indirect effects consistently. A\nsensitivity analysis is proposed for violations of the IV exclusion restriction\nassumption. Simulation studies demonstrate that our method works well for\ndifferent types of outcomes under different settings. Our method is applied to\na randomized dental caries prevention trial and a study of the effect of a\nmassive flood in Bangladesh on children's diarrhea.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 17:19:19 GMT"}, {"version": "v2", "created": "Sat, 9 Jul 2016 11:34:17 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Guo", "Zijian", ""], ["Small", "Dylan S.", ""], ["Gansky", "Stuart A.", ""], ["Cheng", "Jing", ""]]}, {"id": "1601.06432", "submitter": "Zhong Guan", "authors": "Zhong Guan", "title": "Accelerated Nonparametric Maximum Likelihood Density Deconvolution Using\n  Bernstein Polynomial", "comments": "An error in the proof of Theorem 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new maximum likelihood method for deconvoluting a continuous density with a\npositive lower bound on a known compact support in additive measurement error\nmodels with known error distribution using the approximate Bernstein type\npolynomial model, a finite mixture of specific beta distributions, is proposed.\nThe change-point detection method is used to choose an optimal model degree.\nBased on a contaminated sample of size $n$, under an assumption which is\nsatisfied, among others, by the generalized normal error distribution, the\noptimal rate of convergence of the mean integrated squared error is proved to\nbe $k^{-1}\\mathcal{O}(n^{-1+1/k}\\log^3 n)$ if the underlying unknown density\nhas continuous $2k$th derivative with $k>1$. Simulation shows that small sample\nperformance of our estimator is better than the deconvolution kernel density\nestimator. The proposed method is illustrated by a real data application.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2016 21:19:06 GMT"}, {"version": "v2", "created": "Sat, 3 Dec 2016 15:50:27 GMT"}, {"version": "v3", "created": "Fri, 26 Jan 2018 23:24:24 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Guan", "Zhong", ""]]}, {"id": "1601.06459", "submitter": "Joel Atkins", "authors": "Joel Atkins and David B. Zax", "title": "Nested Orthogonal Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal Arrays allow us to test various levels of each factor and balance\nthe different factors so that we can estimate interactions as well as first\norder effects. There is a trade-off between how well we can sample different\nlevels of each factor and how many interactions we are able to estimate. This\npaper describes one method to mitigate this trade-off. This method will allow\nus, with n observations, to sample n levels of each factor and minimize the\ncorrelation between the estimates of first order terms and their interactions.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 00:55:09 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Atkins", "Joel", ""], ["Zax", "David B.", ""]]}, {"id": "1601.06533", "submitter": "Christian R\\\"over", "authors": "Tim Friede, Christian R\\\"over, Simon Wandel and Beat Neuenschwander", "title": "Meta-analysis of few small studies in orphan diseases", "comments": "23 pages, 6 figures, 3 tables", "journal-ref": "Research Synthesis Methods, 8(1):79-91, 2017", "doi": "10.1002/jrsm.1217", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analyses in orphan diseases and small populations generally face\nparticular problems including small numbers of studies, small study sizes, and\nheterogeneity of results. However, the heterogeneity is difficult to estimate\nif only very few studies are included. Motivated by a systematic review in\nimmunosuppression following liver transplantation in children we investigate\nthe properties of a range of commonly used frequentist and Bayesian procedures\nin extensive simulation studies. Furthermore, the consequences for interval\nestimation of the common treatment effect in random effects meta-analysis are\nassessed. The Bayesian credibility intervals using weakly informative priors\nfor the between-trial heterogeneity exhibited coverage probabilities in excess\nof the nominal level for a range of scenarios considered. However, they tended\nto be shorter than those obtained by the Knapp-Hartung method, which were also\nconservative. In contrast, methods based on normal quantiles exhibited\ncoverages well below the nominal levels in many scenarios. With very few\nstudies, the performance of the Bayesian credibility intervals is of course\nsensitive to the specification of the prior for the between trial\nheterogeneity. In conclusion, the use of weakly informative priors as\nexemplified by half-normal priors (with scale 0.5 or 1.0) for log odds ratios\nis recommended for applications in rare diseases.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 09:53:13 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Friede", "Tim", ""], ["R\u00f6ver", "Christian", ""], ["Wandel", "Simon", ""], ["Neuenschwander", "Beat", ""]]}, {"id": "1601.06630", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle", "title": "Bayesian Estimation of Bipartite Matchings for Record Linkage", "comments": "This is a preprint of an article accepted for publication in the\n  Journal of the American Statistical Association. The final version contains\n  more materials and is organized differently", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bipartite record linkage task consists of merging two disparate datafiles\ncontaining information on two overlapping sets of entities. This is non-trivial\nin the absence of unique identifiers and it is important for a wide variety of\napplications given that it needs to be solved whenever we have to combine\ninformation from different sources. Most statistical techniques currently used\nfor record linkage are derived from a seminal paper by Fellegi and Sunter\n(1969). These techniques usually assume independence in the matching statuses\nof record pairs to derive estimation procedures and optimal point estimators.\nWe argue that this independence assumption is unreasonable and instead target a\nbipartite matching between the two datafiles as our parameter of interest.\nBayesian implementations allow us to quantify uncertainty on the matching\ndecisions and derive a variety of point estimators using different loss\nfunctions. We propose partial Bayes estimates that allow uncertain parts of the\nbipartite matching to be left unresolved. We evaluate our approach to record\nlinkage using a variety of challenging scenarios and show that it outperforms\nthe traditional methodology. We illustrate the advantages of our methods\nmerging two datafiles on casualties from the civil war of El Salvador.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 14:58:41 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Sadinle", "Mauricio", ""]]}, {"id": "1601.06722", "submitter": "Florian Heinrichs", "authors": "Holger Dette, Kirsten Schorning, Maria Konstantinou", "title": "Optimal designs for comparing regression models with correlated\n  observations", "comments": "Keywords and Phrases: linear regression, correlated observations,\n  comparing regression curves, confidence band, optimal design AMS Subject\n  classification: 62K05", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of efficient statistical inference for comparing two\nregression curves estimated from two samples of dependent measurements. Based\non a representation of the best pair of linear unbiased estimators in\ncontinuous time models as a stochastic integral, an efficient pair of linear\nunbiased estimators with corresponding optimal designs for finite sample size\nis constructed. This pair minimises the width of the confidence band for the\ndifference between the estimated curves. We thus extend results readily\navailable in the literature to the case of correlated observations and provide\nan easily implementable and efficient solution. The advantages of using such\npairs of estimators with corresponding optimal designs for the comparison of\nregression models are illustrated via numerical examples.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 19:00:14 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 15:37:07 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Dette", "Holger", ""], ["Schorning", "Kirsten", ""], ["Konstantinou", "Maria", ""]]}, {"id": "1601.06743", "submitter": "Cheng Zheng", "authors": "Cheng Zheng, David C. Atkins, Melissa A. Lewis, Xiao-Hua Zhou", "title": "On estimating causal controlled direct and mediator effects for count\n  outcomes without assuming sequential ignorability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis is an important statistical method in social and\nmedical studies, as it can provide insights about why an intervention works and\ninform the development of future interventions. Currently, most causal\nmediation methods focus on mediation effects defined on a mean scale. However,\nin health-risk studies, such as alcohol or risky sex, outcomes are typically\ncount data and heavily skewed. Thus, mediation effects in these setting would\nbe natural on a rate ratio scale, such as in Poisson and negative binomial\nregression methods. Existing methods also mainly rely on the assumption of no\nunmeasured confounding between mediator and outcome. To allow for potential\nconfounders between the mediator and outcome, we define the direct and mediator\neffects on a new scale and propose a multiplicative structural mean model for\nmediation analysis with count outcomes. The estimator is compared with both\nPoisson and negative binomial regression methods assuming sequential\nignorability using a simulation study and a real world example about an\nalcohol-related intervention study. Mediation analyses using the new methods\nconfirm the study hypothesis that the intervention decreases drinking by\ndecreasing individual's normative perceptions of alcohol use.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 19:51:02 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Zheng", "Cheng", ""], ["Atkins", "David C.", ""], ["Lewis", "Melissa A.", ""], ["Zhou", "Xiao-Hua", ""]]}, {"id": "1601.06749", "submitter": "Deirel Paz-Linares", "authors": "Deirel Paz-Linares, Mayrim Vega-Hern\\'andez, Pedro A. Rojas-L\\'opez,\n  Pedro A. Vald\\'es-Sosa and Eduardo Mart\\'inez-Montes", "title": "Empirical bayes formulation of the elastic net and mixed-norm models:\n  application to the eeg inverse problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of EEG generating sources constitutes an Inverse Problem (IP)\nin Neuroscience. This is an ill-posed problem, due to the non-uniqueness of the\nsolution, and many kinds of prior information have been used to constrain it. A\ncombination of smoothness (L2 norm-based) and sparseness (L1 norm-based)\nconstraints is a flexible approach that have been pursued by important examples\nsuch as the Elastic Net (ENET) and mixed-norm (MXN) models. The former is used\nto find solutions with a small number of smooth non-zero patches, while the\nlatter imposes sparseness and smoothness simultaneously along different\ndimensions of the spatio-temporal matrix solutions. Both models have been\naddressed within the penalized regression approach, where the regularization\nparameters are selected heuristically, leading usually to non-optimal\nsolutions. The existing Bayesian formulation of ENET allows hyperparameter\nlearning, but using computationally intensive Monte Carlo/Expectation\nMaximization methods. In this work we attempt to solve the EEG IP using a\nBayesian framework for models based on mixtures of L1/L2 norms penalization\nfunctions (Laplace/Normal priors) such as ENET and MXN. We propose a Sparse\nBayesian Learning algorithm based on combining the Empirical Bayes and the\niterative coordinate descent procedures to estimate both the parameters and\nhyperparameters. Using simple but realistic simulations we found that our\nmethods are able to recover complicated source setups more accurately and with\na more robust variable selection than the ENET and LASSO solutions using\nclassical algorithms. We also solve the EEG IP using data coming from a visual\nattention experiment, finding more interpretable neurophysiological patterns\nwith our methods, as compared with other known methods such as LORETA, ENET and\nLASSO FUSION using the classical regularization approach.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 20:14:05 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 01:28:00 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Paz-Linares", "Deirel", ""], ["Vega-Hern\u00e1ndez", "Mayrim", ""], ["Rojas-L\u00f3pez", "Pedro A.", ""], ["Vald\u00e9s-Sosa", "Pedro A.", ""], ["Mart\u00ednez-Montes", "Eduardo", ""]]}, {"id": "1601.06858", "submitter": "Karthyek Rajhaa Annaswamy Murthy", "authors": "Jose Blanchet, Fei He, and Karthyek R. A. Murthy", "title": "On distributionally robust extreme value analysis", "comments": null, "journal-ref": null, "doi": "10.1007/s10687-019-00371-1", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributional robustness in the context of Extreme Value Theory\n(EVT). We provide a data-driven method for estimating extreme quantiles in a\nmanner that is robust against incorrect model assumptions underlying the\napplication of the standard Extremal Types Theorem. Typical studies in\ndistributional robustness involve computing worst case estimates over a model\nuncertainty region expressed in terms of the Kullback-Leibler discrepancy. We\ngo beyond standard distributional robustness in that we investigate different\nforms of discrepancies, and prove rigorous results which are helpful for\nunderstanding the role of a putative model uncertainty region in the context of\nextreme quantile estimation. Finally, we illustrate our data-driven method in\nvarious settings, including examples showing how standard EVT can significantly\nunderestimate quantiles of interest.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 01:04:03 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 03:46:10 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Blanchet", "Jose", ""], ["He", "Fei", ""], ["Murthy", "Karthyek R. A.", ""]]}, {"id": "1601.06868", "submitter": "Jingxian Wu", "authors": "Jingxian Wu and Jing Yang", "title": "Quickest Change Detection with Mismatched Post-Change Models", "comments": "Abbreviated version submitted to IEEE ISIT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the quickest change detection with mismatched\npost-change models. A change point is the time instant at which the\ndistribution of a random process changes. The objective of quickest change\ndetection is to minimize the detection delay of an unknown change point under\ncertain performance constraints, such as average run length (ARL) to false\nalarm or probability of false alarm (PFA). Most existing change detection\nprocedures assume perfect knowledge of the random process distributions before\nand after the change point. However, in many practical applications such as\nanomaly detection, the post-change distribution is often unknown and needs to\nbe estimated with a limited number of samples. In this paper, we study the case\nthat there is a mismatch between the true post-change distribution and the one\nused during detection. We analytically identify the impacts of mismatched\npost-change models on two classical detection procedures, the cumulative sum\n(CUSUM) procedure and the Shiryaev-Roberts (SR) procedure. The impacts of\nmismatched models are characterized in terms of various finite or asymptotic\nperformance bounds on ARL, PFA, and average detection delay (ADD). It is shown\nthat post-change model mismatch results in an increase in ADD, and the rate of\nperformance degradation depends on the difference between two Kullback-Leibler\n(KL) divergences, one is between the priori- and post-change distributions, and\nthe other one is between the true and mismatched post-change distributions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 02:29:49 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Wu", "Jingxian", ""], ["Yang", "Jing", ""]]}, {"id": "1601.06911", "submitter": "Irene Epifanio", "authors": "Irene Epifanio", "title": "Functional archetype and archetypoid analysis", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, Volume 104, December\n  2016, Pages 24-34", "doi": "10.1016/j.csda.2016.06.007", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archetype and archetypoid analysis can be extended to functional data. Each\nfunction is represented as a mixture of actual observations (functional\narchetypoids) or functional archetypes, which are a mixture of observations in\nthe data set. Well-known Canadian temperature data are used to illustrate the\nanalysis developed. Computational methods are proposed for performing these\nanalyses, based on the coefficients of a basis. Unlike a previous attempt to\ncompute functional archetypes, which was only valid for an orthogonal basis,\nthe proposed methodology can be used for any basis. It is computationally less\ndemanding than the simple approach of discretizing the functions. Multivariate\nfunctional archetype and archetypoid analysis are also introduced and applied\nin an interesting problem about the study of human development around the world\nover the last 50 years. These tools can contribute to the understanding of a\nfunctional data set, as in the multivariate case.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 07:34:41 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 06:15:21 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Epifanio", "Irene", ""]]}, {"id": "1601.07127", "submitter": "Karla DiazOrdaz", "authors": "Karla DiazOrdaz, Angelo Franchini and Richard Grieve", "title": "Instrumental variable approaches for estimating complier average causal\n  effects on bivariate outcomes in randomised trials with non-compliance", "comments": "40 pages, 2 figures, includes R and Stata code to fit the models", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Randomised Controlled Trials (RCT) with treatment non-compliance,\ninstrumental variable approaches are used to estimate complier average causal\neffects. We extend these approaches to cost-effectiveness analyses, where\nmethods need to recognise the correlation between cost and health outcomes.\n  We propose a Bayesian full likelihood (BFL) approach, which jointly models\nthe effects of random assignment on treatment received and the outcomes, and a\nthree-stage least squares (3sls) method, which acknowledges the correlation\nbetween the endpoints, and the endogeneity of the treatment received.\n  This investigation is motivated by the REFLUX study, which exemplifies the\nsetting where compliance differs between the RCT and routine practice. A\nsimulation is used to compare the methods performance.\n  We find that failure to model the correlation between the outcomes and\ntreatment received correctly can result in poor CI coverage and biased\nestimates. By contrast, BFL and 3sls methods provide unbiased estimates with\ngood coverage.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 18:36:57 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 10:24:09 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["DiazOrdaz", "Karla", ""], ["Franchini", "Angelo", ""], ["Grieve", "Richard", ""]]}, {"id": "1601.07235", "submitter": "Brendan Rocks Mr.", "authors": "Brendan Rocks", "title": "Interval Estimation for the 'Net Promoter Score'", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Net Promoter Score (NPS) is a novel summary statistic used by thousands\nof companies as a key performance indicator of customer loyalty. While adoption\nof the statistic has grown rapidly over the last decade, there has been little\npublished on its statistical properties. Common interval estimation techniques\nare adapted for use with the NPS, and performance assessed on the largest\navailable database of companies' Net Promoter Scores. Variations on the\nAdjusted Wald, and an iterative Score test are found to have superior\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 00:55:40 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Rocks", "Brendan", ""]]}, {"id": "1601.07251", "submitter": "Peter Rousseeuw", "authors": "Peter J. Rousseeuw and Wannes Van den Bossche", "title": "Detecting deviating data cells", "comments": "To appear in Technometrics", "journal-ref": "Technometrics, 60, 135-145, 2018", "doi": "10.1080/00401706.2017.1340909", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multivariate dataset consists of $n$ cases in $d$ dimensions, and is often\nstored in an $n$ by $d$ data matrix. It is well-known that real data may\ncontain outliers. Depending on the situation, outliers may be (a) undesirable\nerrors which can adversely affect the data analysis, or (b) valuable nuggets of\nunexpected information. In statistics and data analysis the word outlier\nusually refers to a row of the data matrix, and the methods to detect such\noutliers only work when at least half the rows are clean. But often many rows\nhave a few contaminated cell values, which may not be visible by looking at\neach variable (column) separately. We propose the first method to detect\ndeviating data cells in a multivariate sample which takes the correlations\nbetween the variables into account. It has no restriction on the number of\nclean rows, and can deal with high dimensions. Other advantages are that it\nprovides estimates of the `expected' values of the outlying cells, while\nimputing missing values at the same time. We illustrate the method on several\nreal data sets, where it uncovers more structure than found by purely\ncolumnwise methods or purely rowwise methods. The proposed method can help to\ndiagnose why a certain row is outlying, e.g. in process control. It may also\nserve as an initial step for estimating multivariate location and scatter\nmatrices.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 03:12:45 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 15:23:26 GMT"}, {"version": "v3", "created": "Tue, 29 Nov 2016 22:12:43 GMT"}, {"version": "v4", "created": "Sat, 14 Oct 2017 10:07:43 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Rousseeuw", "Peter J.", ""], ["Bossche", "Wannes Van den", ""]]}, {"id": "1601.07344", "submitter": "Bruno Santos", "authors": "Bruno Santos and Heleno Bolfarine", "title": "On Bayesian quantile regression and outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we discuss the progress of Bayesian quantile regression models\nsince their first proposal and we discuss the importance of all parameters\ninvolved in the inference process. Using a representation of the asymmetric\nLaplace distribution as a mixture of a normal and an exponential distribution,\nwe discuss the relevance of the presence of a scale parameter to control for\nthe variance in the model. Besides that we consider the posterior distribution\nof the latent variable present in the mixture representation to showcase\noutlying observations given the Bayesian quantile regression fits, where we\ncompare the posterior distribution for each latent variable with the others. We\nillustrate these results with simulation studies and also with data about Gini\nindexes in Brazilian states from years with census information.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 12:41:56 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Santos", "Bruno", ""], ["Bolfarine", "Heleno", ""]]}, {"id": "1601.07463", "submitter": "Kenichiro McAlinn", "authors": "Kenichiro McAlinn and Mike West", "title": "Dynamic Bayesian Predictive Synthesis in Time Series Forecasting", "comments": null, "journal-ref": null, "doi": "10.1016/j.jeconom.2018.11.010", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss model and forecast combination in time series forecasting. A\nfoundational Bayesian perspective based on agent opinion analysis theory\ndefines a new framework for density forecast combination, and encompasses\nseveral existing forecast pooling methods. We develop a novel class of dynamic\nlatent factor models for time series forecast synthesis; simulation-based\ncomputation enables implementation. These models can dynamically adapt to\ntime-varying biases, miscalibration and inter-dependencies among multiple\nmodels or forecasters. A macroeconomic forecasting study highlights the dynamic\nrelationships among synthesized forecast densities, as well as the potential\nfor improved forecast accuracy at multiple horizons.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 17:44:34 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 15:18:10 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2016 20:13:43 GMT"}, {"version": "v4", "created": "Sat, 30 Sep 2017 20:09:29 GMT"}, {"version": "v5", "created": "Sun, 5 Nov 2017 21:04:58 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["McAlinn", "Kenichiro", ""], ["West", "Mike", ""]]}, {"id": "1601.07496", "submitter": "Simeng Qu", "authors": "Simeng Qu, Jane-Ling Wang and Xiao Wang", "title": "Optimal Estimation for the Functional Cox Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional covariates are common in many medical, biodemographic, and\nneuroimaging studies. The aim of this paper is to study functional Cox models\nwith right-censored data in the presence of both functional and scalar\ncovariates. We study the asymptotic properties of the maximum partial\nlikelihood estimator and establish the asymptotic normality and efficiency of\nthe estimator of the finite-dimensional estimator. Under the framework of\nreproducing kernel Hilbert space, the estimator of the coefficient function for\na functional covariate achieves the minimax optimal rate of convergence under a\nweighted $L_2$-risk. This optimal rate is determined jointly by the censoring\nscheme, the reproducing kernel and the covariance kernel of the functional\ncovariates. Implementation of the estimation approach and the selection of the\nsmoothing parameter are discussed in detail. The finite sample performance is\nillustrated by simulated examples and a real application.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 18:54:38 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Qu", "Simeng", ""], ["Wang", "Jane-Ling", ""], ["Wang", "Xiao", ""]]}, {"id": "1601.07700", "submitter": "Kai Gong", "authors": "Shi-Long Luo, Kai Gong, Li Kang", "title": "Identifying Influential Spreaders of Epidemics on Community Networks", "comments": "10 pages, 7 figures, 1 table", "journal-ref": null, "doi": "10.21078/JSSI-2018-366-10", "report-no": null, "categories": "physics.soc-ph cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient strategy for the identification of influential spreaders that\ncould be used to control epidemics within populations would be of considerable\nimportance. Generally, populations are characterized by its community\nstructures and by the heterogeneous distributions of weak ties among nodes\nbridging over communities. A strategy for community networks capable of\nidentifying influential spreaders that accelerate the spread of disease is here\nproposed. In this strategy, influential spreaders serve as target nodes. This\nis based on the idea that, in k-shell decomposition, weak ties and strong ties\nare processed separately. The strategy was used on empirical networks\nconstructed from online social networks, and results indicated that this\nstrategy is more accurate than other strategies. Its effectiveness stems from\nthe patterns of connectivity among neighbors, and it successfully identified\nthe important nodes. In addition, the performance of the strategy remained\nrobust even when there were errors in the structure of the network.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 09:28:19 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Luo", "Shi-Long", ""], ["Gong", "Kai", ""], ["Kang", "Li", ""]]}, {"id": "1601.07739", "submitter": "Elisa Perrone", "authors": "Elisa Perrone, Andreas Rappold, Werner G. M\\\"uller", "title": "Optimal discrimination design for copula models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimum experimental design theory has recently been extended for parameter\nestimation in copula models. However, the choice of the correct dependence\nstructure still requires wider analyses. In this work the issue of copula\nselection is treated by using discrimination design techniques. The new\nproposed approach consists in the use of $D_s$-optimality following an\nextension of corresponding equivalence theory. We also present some examples\nand highlight the strength of such a criterion as a way to discriminate between\nvarious classes of dependences.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 12:36:37 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Perrone", "Elisa", ""], ["Rappold", "Andreas", ""], ["M\u00fcller", "Werner G.", ""]]}, {"id": "1601.07780", "submitter": "Dominik Liebl", "authors": "Dominik Liebl", "title": "Inference for Sparse and Dense Functional Data with Covariate\n  Adjustments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference for the mean and covariance functions of covariate\nadjusted functional data using Local Linear Kernel (LLK) estimators. By means\nof a double asymptotic, we differentiate between sparse and dense covariate\nadjusted functional data - depending on the relative order of m (the\ndiscretization points per function) and n (the number of functions). Our\nsimulation results demonstrate that the existing asymptotic normality results\ncan lead to severely misleading inferences in finite samples. We explain this\nphenomenon based on our theoretical results and propose finite-sample\ncorrections which provide practically useful approximations for inference in\nsparse and dense data scenarios. The relevance of our theoretical results is\nshowcased using a real-data application.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 14:51:24 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 13:15:52 GMT"}, {"version": "v3", "created": "Thu, 7 Sep 2017 10:12:55 GMT"}, {"version": "v4", "created": "Tue, 27 Feb 2018 18:50:24 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Liebl", "Dominik", ""]]}, {"id": "1601.07872", "submitter": "Daren Wang", "authors": "Mattia Ciollaro, Christopher R. Genovese, Daren Wang", "title": "Nonparametric Clustering of Functional Data Using Pseudo-Densities", "comments": null, "journal-ref": "Electron. J. Statist., Volume 10, Number 2 (2016), 2922-2972", "doi": "10.1214/16-EJS1198", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonparametric clustering of smooth random curves on the basis of the\nL2 gradient flow associated to a pseudo-density functional and we show that the\nclustering is well-defined both at the population and at the sample level. We\nprovide an algorithm to mark significant local modes, which are associated to\ninformative sample clusters, and we derive its consistency properties. Our\ntheory is developed under weak assumptions, which essentially reduce to the\nintegrability of the random curves, and does not require to project the random\ncurves on a finite-dimensional subspace. However, if the underlying probability\ndistribution is supported on a finite-dimensional subspace, we show that the\npseudo-density and the expectation of a kernel density estimator induce the\nsame gradient flow, and therefore the same clustering. Although our theory is\ndeveloped for smooth curves that belong to an infinite-dimensional functional\nspace, we also provide consistent procedures that can be used with real data\n(discretized and noisy observations).\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 19:27:27 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Ciollaro", "Mattia", ""], ["Genovese", "Christopher R.", ""], ["Wang", "Daren", ""]]}, {"id": "1601.08002", "submitter": "Katsuyuki Hagiawra", "authors": "Katsuyuki Hagiwara", "title": "Adaptive scaling for soft-thresholding estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soft-thresholding is a sparse modeling method that is typically applied to\nwavelet denoising in statistical signal processing and analysis. It has a\nsingle parameter that controls a threshold level on wavelet coefficients and,\nsimultaneously, amount of shrinkage for coefficients of un-removed components.\nThis parametrization is possible to cause excess shrinkage, thus, estimation\nbias at a sparse representation; i.e. there is a dilemma between sparsity and\nprediction accuracy. To relax this problem, we considered to introduce positive\nscaling on soft-thresholding estimator, by which threshold level and amount of\nshrinkage are independently controlled. Especially, in this paper, we proposed\ncomponent-wise and data-dependent scaling in a setting of non-parametric\northogonal regression problem including discrete wavelet transform. We call our\nscaling method adaptive scaling. We here employed soft-thresholding method\nbased on LARS(least angle regression), by which the model selection problem\nreduces to the determination of the number of un-removed components. We derived\na risk under LARS-based soft-thresholding with the proposed adaptive scaling\nand established a model selection criterion as an unbiased estimate of the\nrisk. We also analyzed some properties of the risk curve and found that the\nmodel selection criterion is possible to select a model with low risk and high\nsparsity compared to a naive soft-thresholding method. This theoretical\nspeculation was verified by a simple numerical experiment and an application to\nwavelet denoising.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 08:47:26 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Hagiwara", "Katsuyuki", ""]]}, {"id": "1601.08057", "submitter": "Samuel Livingstone", "authors": "Samuel Livingstone, Michael Betancourt, Simon Byrne and Mark Girolami", "title": "On the Geometric Ergodicity of Hamiltonian Monte Carlo", "comments": "29 pages + supplement (included in arXival as Appendix), 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish general conditions under which Markov chains produced by the\nHamiltonian Monte Carlo method will and will not be geometrically ergodic. We\nconsider implementations with both position-independent and position-dependent\nintegration times. In the former case we find that the conditions for geometric\nergodicity are essentially a gradient of the log-density which asymptotically\npoints towards the centre of the space and grows no faster than linearly. In an\nidealised scenario in which the integration time is allowed to change in\ndifferent regions of the space, we show that geometric ergodicity can be\nrecovered for a much broader class of tail behaviours, leading to some\nguidelines for the choice of this free parameter in practice.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 11:13:46 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 13:13:20 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 11:08:32 GMT"}, {"version": "v4", "created": "Fri, 16 Nov 2018 11:32:00 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Livingstone", "Samuel", ""], ["Betancourt", "Michael", ""], ["Byrne", "Simon", ""], ["Girolami", "Mark", ""]]}, {"id": "1601.08065", "submitter": "Gabriela Ciuperca", "authors": "Gabriela Ciuperca", "title": "Adaptive group LASSO selection in quantile models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers a linear model with grouped explanatory variables. If the\nmodel errors are not with zero mean and bounded variance or if model contains\noutliers, then the least squares framework is not appropriate. Thus, the\nquantile regression is an interesting alternative. In order to automatically\nselect the relevant variable groups, we propose and study here the adaptive\ngroup LASSO quantile estimator. We establish the sparsity and asymptotic\nnormality of the proposed estimator in two cases: fixed number and divergent\nnumber of variable groups. Numerical study by Monte Carlo simulations confirms\nthe theoretical results and illustrates the performance of the proposed\nestimator.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 11:51:13 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 07:56:45 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Ciuperca", "Gabriela", ""]]}, {"id": "1601.08088", "submitter": "David Woods", "authors": "David C. Woods, James M. McGree and Susan M. Lewis", "title": "Model selection via Bayesian information capacity designs for\n  generalised linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first investigation is made of designs for screening experiments where\nthe response variable is approximated by a generalised linear model. A Bayesian\ninformation capacity criterion is defined for the selection of designs that are\nrobust to the form of the linear predictor. For binomial data and logistic\nregression, the effectiveness of these designs for screening is assessed\nthrough simulation studies using all-subsets regression and model selection via\nmaximum penalised likelihood and a generalised information criterion. For\nPoisson data and log-linear regression, similar assessments are made using\nmaximum likelihood and the Akaike information criterion for minimally-supported\ndesigns that are constructed analytically. The results show that effective\nscreening, that is, high power with moderate type I error rate and false\ndiscovery rate, can be achieved through suitable choices for the number of\ndesign support points and experiment size. Logistic regression is shown to\npresent a more challenging problem than log-linear regression. Some areas for\nfuture work are also indicated.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 13:06:38 GMT"}, {"version": "v2", "created": "Sun, 14 Aug 2016 10:37:12 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 16:37:56 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Woods", "David C.", ""], ["McGree", "James M.", ""], ["Lewis", "Susan M.", ""]]}, {"id": "1601.08118", "submitter": "Konstantinos Spiliopoulos", "authors": "Luc Rey-Bellet and Konstantinos Spiliopoulos", "title": "Improving the convergence of reversible samplers", "comments": "Final version will appear in the Journal of Statistical Physics", "journal-ref": null, "doi": "10.1007/s10955-016-1565-1", "report-no": null, "categories": "math.PR math-ph math.MP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Monte-Carlo methods the Markov processes used to sample a given target\ndistribution usually satisfy detailed balance, i.e. they are time-reversible.\nHowever, relatively recent results have demonstrated that appropriate\nreversible and irreversible perturbations can accelerate convergence to\nequilibrium. In this paper we present some general design principles which\napply to general Markov processes. Working with the generator of Markov\nprocesses, we prove that for some of the most commonly used performance\ncriteria, i.e., spectral gap, asymptotic variance and large deviation\nfunctionals, sampling is improved for appropriate reversible and irreversible\nperturbations of some initially given reversible sampler. Moreover we provide\nspecific constructions for such reversible and irreversible perturbations for\nvarious commonly used Markov processes, such as Markov chains and diffusions.\nIn the case of diffusions, we make the discussion more specific using the large\ndeviations rate function as a measure of performance.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 14:18:38 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 16:32:53 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Rey-Bellet", "Luc", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1601.08133", "submitter": "Peter Rousseeuw", "authors": "Mia Hubert, Jakob Raymaekers, Peter J. Rousseeuw, Pieter Segaert", "title": "Finding Outliers in Surface Data and Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface, image and video data can be considered as functional data with a\nbivariate domain. To detect outlying surfaces or images, a new method is\nproposed based on the mean and the variability of the degree of outlyingness at\neach grid point. A rule is constructed to flag the outliers in the resulting\nfunctional outlier map. Heatmaps of their outlyingness indicate the regions\nwhich are most deviating from the regular surfaces. The method is applied to\nfluorescence excitation-emission spectra after fitting a PARAFAC model, to MRI\nimage data which are augmented with their gradients, and to video surveillance\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 14:43:54 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Hubert", "Mia", ""], ["Raymaekers", "Jakob", ""], ["Rousseeuw", "Peter J.", ""], ["Segaert", "Pieter", ""]]}, {"id": "1601.08169", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J Kir\\'aly, Harald Oberhauser", "title": "Kernels for sequentially ordered data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for kernel learning with sequential data of any\nkind, such as time series, sequences of graphs, or strings. Our approach is\nbased on signature features which can be seen as an ordered variant of sample\n(cross-)moments; it allows to obtain a \"sequentialized\" version of any static\nkernel. The sequential kernels are efficiently computable for discrete\nsequences and are shown to approximate a continuous moment form in a sampling\nsense.\n  A number of known kernels for sequences arise as \"sequentializations\" of\nsuitable static kernels: string kernels may be obtained as a special case, and\nalignment kernels are closely related up to a modification that resolves their\nopen non-definiteness issue. Our experiments indicate that our signature-based\nsequential kernel framework may be a promising approach to learning with\nsequential data, such as time series, that allows to avoid extensive manual\npre-processing.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 16:06:36 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Kir\u00e1ly", "Franz J", ""], ["Oberhauser", "Harald", ""]]}, {"id": "1601.08197", "submitter": "Mar Rodr\\'iguez-Girondo", "authors": "Mar Rodr\\'iguez-Girondo, Perttu Salo, Tomasz Burzykowski, Markus\n  Perola, Jeanine Houwing-Duistermaat and Bart Mertens", "title": "Sequential double cross-validation for assessment of added predictive\n  ability in high-dimensional omic applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enriching existing predictive models with new biomolecular markers is an\nimportant task in the new multi-omic era. Clinical studies increasingly include\nnew sets of omic measurements which may prove their added value in terms of\npredictive performance. We introduce a two-step approach for the assessment of\nthe added predictive ability of omic predictors, based on sequential double\ncross-validation and regularized regression models. We propose several\nperformance indices to summarize the two-stage prediction procedure and a\npermutation test to formally assess the added predictive value of a second omic\nset of predictors over a primary omic source. The performance of the test is\ninvestigated through simulations. We illustrate the new method through the\nsystematic assessment and comparison of the performance of transcriptomics and\nmetabolomics sources in the prediction of body mass index (BMI) using\nlongitudinal data from the Dietary, Lifestyle, and Genetic determinants of\nObesity and Metabolic syndrome (DILGOM) study, a population-based cohort from\nFinland.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 17:13:04 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 13:38:02 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2016 06:54:54 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Rodr\u00edguez-Girondo", "Mar", ""], ["Salo", "Perttu", ""], ["Burzykowski", "Tomasz", ""], ["Perola", "Markus", ""], ["Houwing-Duistermaat", "Jeanine", ""], ["Mertens", "Bart", ""]]}, {"id": "1601.08244", "submitter": "Greg P. Kochanski", "authors": "Burton Rosner and Greg Kochanski", "title": "Categorical Judgment with a Variable Decision Rule", "comments": "Contains source code as an attachment to this PDF, in files\n  2015-12-02_speechresearch.tgz and Rosner_Kochanski_2016.tgz", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new Thurstonian rating scale model uses a variable decision rule (VDR) that\nincorporates three previously formulated, distinct decision rules. The model\nincludes probabilities for choosing each rule, along with Gaussian\nrepresentation and criterion densities. Numerical optimisation techniques were\nvalidated through demonstrating that the model fits simulated data tightly. For\nsimulations with 400 trials per stimulus (tps), useful information emerged\nabout the generating parameters. However, larger experiments (e.g. 4000 tps)\nproved desirable for better recovery of generating parameters and to support\ntrustworthy choices between competing models by the Akaike Information\nCriterion. In reanalyses of experiments by others, the VDR model explained most\nof the data better than did classical signal detection theory models.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 16:19:16 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Rosner", "Burton", ""], ["Kochanski", "Greg", ""]]}]