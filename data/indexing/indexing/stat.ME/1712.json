[{"id": "1712.00038", "submitter": "David Hirshberg", "authors": "David A. Hirshberg and Stefan Wager", "title": "Augmented Minimax Linear Estimation", "comments": "67 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical estimands can expressed as continuous linear functionals of\na conditional expectation function. This includes the average treatment effect\nunder unconfoundedness and generalizations for continuous-valued and\npersonalized treatments. In this paper, we discuss a general approach to\nestimating such quantities: we begin with a simple plug-in estimator based on\nan estimate of the conditional expectation function, and then correct the\nplug-in estimator by subtracting a minimax linear estimate of its error. We\nshow that our method is semiparametrically efficient under weak conditions and\nobserve promising performance on both real and simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 19:21:47 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 01:24:24 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 18:20:37 GMT"}, {"version": "v4", "created": "Mon, 29 Oct 2018 15:52:20 GMT"}, {"version": "v5", "created": "Sat, 15 Jun 2019 03:43:57 GMT"}, {"version": "v6", "created": "Thu, 19 Nov 2020 23:10:34 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Hirshberg", "David A.", ""], ["Wager", "Stefan", ""]]}, {"id": "1712.00229", "submitter": "Michael Grayling", "authors": "Michael Grayling, James Wason, Adrian Mander", "title": "Efficient determination of optimised multi-arm multi-stage experimental\n  designs with control of generalised error-rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primarily motivated by the drug development process, several publications\nhave now presented methodology for the design of multi-arm multi-stage\nexperiments with normally distributed outcome variables of known variance.\nHere, we extend these past considerations to allow the design of what we refer\nto as an abcd multi-arm multi-stage experiment. We provide a proof of how\nstrong control of the a-generalised type-I familywise error-rate can be\nensured. We then describe how to attain the power to reject at least b out of c\nfalse hypotheses, which is related to controlling the b-generalised type-II\nfamilywise error-rate. Following this, we detail how a design can be optimised\nfor a scenario in which rejection of any d null hypotheses brings about\ntermination of the experiment. We achieve this by proposing a highly\ncomputationally efficient approach for evaluating the performance of a\ncandidate design. Finally, using a real clinical trial as a motivating example,\nwe explore the effect of the design's control parameters on the statistical\noperating characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 08:23:22 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Grayling", "Michael", ""], ["Wason", "James", ""], ["Mander", "Adrian", ""]]}, {"id": "1712.00292", "submitter": "Minna Genb\\\"ack", "authors": "Minna Genb\\\"ack and Xavier de Luna", "title": "Causal inference taking into account unobserved confounding", "comments": "Biometrics 2018", "journal-ref": null, "doi": "10.1111/biom.13001", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference with observational data can be performed under an assumption\nof no unobserved confounders (unconfoundedness assumption). There is, however,\nseldom clear subject-matter or empirical evidence for such an assumption. We\ntherefore develop uncertainty intervals for average causal effects based on\noutcome regression estimators and doubly robust estimators, which provide\ninference taking into account both sampling variability and uncertainty due to\nunobserved confounders. In contrast with sampling variation, uncertainty due\nunobserved confounding does not decrease with increasing sample size. The\nintervals introduced are obtained by deriving the bias of the estimators due to\nunobserved confounders. We are thus also able to contrast the size of the bias\ndue to violation of the unconfoundedness assumption, with bias due to\nmisspecification of the models used to explain potential outcomes. This is\nillustrated through numerical experiments where bias due to moderate unobserved\nconfounding dominates misspecification bias for typical situations in terms of\nsample size and modeling assumptions. We also study the empirical coverage of\nthe uncertainty intervals introduced and apply the results to a study of the\neffect of regular food intake on health. An R-package implementing the\ninference proposed is available.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 12:36:07 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 09:47:15 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Genb\u00e4ck", "Minna", ""], ["de Luna", "Xavier", ""]]}, {"id": "1712.00484", "submitter": "Rob Tibshirani", "authors": "Robert Tibshirani and Jerome Friedman", "title": "A Pliable Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalization of the lasso that allows the model coefficients\nto vary as a function of a general set of modifying variables. These modifiers\nmight be variables such as gender, age or time. The paradigm is quite general,\nwith each lasso coefficient modified by a sparse linear function of the\nmodifying variables $Z$. The model is estimated in a hierarchical fashion to\ncontrol the degrees of freedom and avoid overfitting. The modifying variables\nmay be observed, observed only in the training set, or unobserved overall.\nThere are connections of our proposal to varying coefficient models and\nhigh-dimensional interaction models. We present a computationally efficient\nalgorithm for its optimization, with exact screening rules to facilitate\napplication to large numbers of predictors. The method is illustrated on a\nnumber of different simulated and real examples.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 20:33:36 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 05:43:30 GMT"}, {"version": "v3", "created": "Fri, 8 Dec 2017 12:53:18 GMT"}, {"version": "v4", "created": "Wed, 10 Jan 2018 06:06:48 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Tibshirani", "Robert", ""], ["Friedman", "Jerome", ""]]}, {"id": "1712.00561", "submitter": "Eric Kawaguchi", "authors": "Eric S. Kawaguchi, Marc A. Suchard, Zhenqiu Liu, Gang Li", "title": "Scalable Sparse Cox's Regression for Large-Scale Survival Data via\n  Broken Adaptive Ridge", "comments": null, "journal-ref": null, "doi": "10.1002/sim.8438", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a new scalable sparse Cox regression tool for sparse\nhigh-dimensional massive sample size (sHDMSS) survival data. The method is a\nlocal $L_0$-penalized Cox regression via repeatedly performing reweighted\n$L_2$-penalized Cox regression. We show that the resulting estimator enjoys the\nbest of $L_0$- and $L_2$-penalized Cox regressions while overcoming their\nlimitations. Specifically, the estimator is selection consistent, oracle for\nparameter estimation, and possesses a grouping property for highly correlated\ncovariates. Simulation results suggest that when the sample size is large, the\nproposed method with pre-specified tuning parameters has a comparable or better\nperformance than some popular penalized regression methods. More importantly,\nbecause the method naturally enables adaptation of efficient algorithms for\nmassive $L_2$-penalized optimization and does not require costly data driven\ntuning parameter selection, it has a significant computational advantage for\nsHDMSS data, offering an average of 5-fold speedup over its closest competitor\nin empirical studies.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 07:11:41 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 23:13:22 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kawaguchi", "Eric S.", ""], ["Suchard", "Marc A.", ""], ["Liu", "Zhenqiu", ""], ["Li", "Gang", ""]]}, {"id": "1712.00642", "submitter": "Xiao Wu", "authors": "Xiao Wu, Danielle Braun, Marianthi-Anna Kioumourtzoglou, Christine\n  Choirat, Qian Di and Francesca Dominici", "title": "Causal inference in the context of an error prone exposure: air\n  pollution and mortality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for estimating causal effects when the exposure is\nmeasured with error and confounding adjustment is performed via a generalized\npropensity score (GPS). Using validation data, we propose a regression\ncalibration (RC)-based adjustment for a continuous error-prone exposure\ncombined with GPS to adjust for confounding (RC-GPS). The outcome analysis is\nconducted after transforming the corrected continuous exposure into a\ncategorical exposure. We consider confounding adjustment in the context of GPS\nsubclassification, inverse probability treatment weighting (IPTW) and matching.\nIn simulations with varying degrees of exposure error and confounding bias,\nRC-GPS eliminates bias from exposure error and confounding compared to standard\napproaches that rely on the error-prone exposure. We applied RC-GPS to a rich\ndata platform to estimate the causal effect of long-term exposure to fine\nparticles ($PM_{2.5}$) on mortality in New England for the period from 2000 to\n2012. The main study consists of $2,202$ zip codes covered by $217,660$ 1km\n$\\times$ 1km grid cells with yearly mortality rates, yearly $PM_{2.5}$ averages\nestimated from a spatio-temporal model (error-prone exposure) and several\npotential confounders. The internal validation study includes a subset of 83\n1km $\\times$ 1km grid cells within 75 zip codes from the main study with\nerror-free yearly $PM_{2.5}$ exposures obtained from monitor stations. Under\nassumptions of non-interference and weak unconfoundedness, using matching we\nfound that exposure to moderate levels of $PM_{2.5}$ ($8 <$ $PM_{2.5}$ $\\leq\n10\\ {\\rm \\mu g/m^3}$) causes a $2.8\\%$ ($95\\%$ CI: $0.6\\%, 3.6\\%$) increase in\nall-cause mortality compared to low exposure ($PM_{2.5}$ $\\leq 8\\ {\\rm \\mu\ng/m^3}$).\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 17:25:32 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 21:26:51 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Wu", "Xiao", ""], ["Braun", "Danielle", ""], ["Kioumourtzoglou", "Marianthi-Anna", ""], ["Choirat", "Christine", ""], ["Di", "Qian", ""], ["Dominici", "Francesca", ""]]}, {"id": "1712.00685", "submitter": "Nicolas Bousquet", "authors": "Nicolas Bousquet and Merlin Keller", "title": "Bayesian prior elicitation and selection for extreme values", "comments": "Manuscript in preparation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major issue of extreme value analysis is the determination of the shape\nparameter $\\xi$ common to Generalized Extreme Value (GEV) and Generalized\nPareto (GP) distributions, which drives the tail behavior, and is of major\nimpact on the estimation of return levels and periods. Many practitioners make\nthe choice of a Bayesian framework to conduct this assessment for accounting of\nparametric uncertainties, which are typically high in such analyses\ncharacterized by a low number of observations. Nonetheless, such approaches can\nprovide large credibility domains for $\\xi$, including negative and positive\nvalues, which does not allow to conclude on the nature of the tail. Considering\nthe block maxima framework, a generic approach of the determination of the\nvalue and sign of $\\xi$ arises from model selection between the Fr\\'echet,\nGumbel and Weibull possible domains of attraction conditionally to\nobservations. Opposite to the common choice of the GEV as an appropriate model\nfor {\\it sampling} extreme values, this model selection must be conducted with\ngreat care. The elicitation of proper, informative and easy-to use priors is\nconducted based on the following principle: for all parameter dimensions they\nact as posteriors of noninformative priors and virtual samples. Statistics of\nthese virtual samples can be assessed from prior predictive information, and a\ncompatibility rule can be carried out to complete the calibration, even though\nthey are only semi-conjugated. Besides, the model selection is conducted using\na mixture encompassing framework, which allows to tackle the computation of\nBayes factors. Motivating by a real case-study involving the elicitation of\nexpert knowledge on meteorological magnitudes, the overall methodology is\nillustrated by toy examples too.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 00:17:23 GMT"}, {"version": "v2", "created": "Sun, 17 Jun 2018 07:54:15 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Bousquet", "Nicolas", ""], ["Keller", "Merlin", ""]]}, {"id": "1712.00771", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen, Kengo Kato", "title": "Randomized incomplete $U$-statistics in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies inference for the mean vector of a high-dimensional\n$U$-statistic. In the era of Big Data, the dimension $d$ of the $U$-statistic\nand the sample size $n$ of the observations tend to be both large, and the\ncomputation of the $U$-statistic is prohibitively demanding. Data-dependent\ninferential procedures such as the empirical bootstrap for $U$-statistics is\neven more computationally expensive. To overcome such computational bottleneck,\nincomplete $U$-statistics obtained by sampling fewer terms of the $U$-statistic\nare attractive alternatives. In this paper, we introduce randomized incomplete\n$U$-statistics with sparse weights whose computational cost can be made\nindependent of the order of the $U$-statistic. We derive non-asymptotic\nGaussian approximation error bounds for the randomized incomplete\n$U$-statistics in high dimensions, namely in cases where the dimension $d$ is\npossibly much larger than the sample size $n$, for both non-degenerate and\ndegenerate kernels. In addition, we propose generic bootstrap methods for the\nincomplete $U$-statistics that are computationally much less-demanding than\nexisting bootstrap methods, and establish finite sample validity of the\nproposed bootstrap methods. Our methods are illustrated on the application to\nnonparametric testing for the pairwise independence of a high-dimensional\nrandom vector under weaker assumptions than those appearing in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 14:01:42 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 16:34:38 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 06:26:12 GMT"}, {"version": "v4", "created": "Sun, 27 Jan 2019 20:11:03 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Chen", "Xiaohui", ""], ["Kato", "Kengo", ""]]}, {"id": "1712.01231", "submitter": "Mohamad Elmasri", "authors": "Mohamad Elmasri", "title": "Sub-clustering in decomposable graphs and size-varying junction trees", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel representation of decomposable graphs based on\nsemi-latent tree-dependent bipartite graphs. The novel representation has two\nmain benefits. First, it enables a form of sub-clustering within maximal\ncliques of the graph, adding informational richness to the general use of\ndecomposable graphs that could be harnessed in applications with behavioural\ntype of data. Second, it allows for a new node-driven Markov chain Monte Carlo\nsampler of decomposable graphs that can easily parallelize and scale. The\nproposed sampler also benefits from the computational efficiency of\njunction-tree-based samplers of decomposable graphs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 18:12:24 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Elmasri", "Mohamad", ""]]}, {"id": "1712.01435", "submitter": "Runjing Liu", "authors": "Ryan Giordano, Runjing Liu, Nelle Varoquaux, Michael I. Jordan, Tamara\n  Broderick", "title": "Measuring Cluster Stability for Bayesian Nonparametrics Using the Linear\n  Bootstrap", "comments": "9 pages, NIPS 2017 Advances in Approximate Bayesian Inference\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering procedures typically estimate which data points are clustered\ntogether, a quantity of primary importance in many analyses. Often used as a\npreliminary step for dimensionality reduction or to facilitate interpretation,\nfinding robust and stable clusters is often crucial for appropriate for\ndownstream analysis. In the present work, we consider Bayesian nonparametric\n(BNP) models, a particularly popular set of Bayesian models for clustering due\nto their flexibility. Because of its complexity, the Bayesian posterior often\ncannot be computed exactly, and approximations must be employed. Mean-field\nvariational Bayes forms a posterior approximation by solving an optimization\nproblem and is widely used due to its speed. An exact BNP posterior might vary\ndramatically when presented with different data. As such, stability and\nrobustness of the clustering should be assessed.\n  A popular mean to assess stability is to apply the bootstrap by resampling\nthe data, and rerun the clustering for each simulated data set. The time cost\nis thus often very expensive, especially for the sort of exploratory analysis\nwhere clustering is typically used. We propose to use a fast and automatic\napproximation to the full bootstrap called the \"linear bootstrap\", which can be\nseen by local data perturbation. In this work, we demonstrate how to apply this\nidea to a data analysis pipeline, consisting of an MFVB approximation to a BNP\nclustering posterior of time course gene expression data. We show that using\nauto-differentiation tools, the necessary calculations can be done\nautomatically, and that the linear bootstrap is a fast but approximate\nalternative to the bootstrap.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 01:13:48 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Giordano", "Ryan", ""], ["Liu", "Runjing", ""], ["Varoquaux", "Nelle", ""], ["Jordan", "Michael I.", ""], ["Broderick", "Tamara", ""]]}, {"id": "1712.01555", "submitter": "Matthias Eckardt", "authors": "Matthias Eckardt and Jorge Mateu", "title": "Second-order and local characteristics of network intensity functions", "comments": "submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has witnessed an increase of interest in the spatial analysis\nof structured point patterns over networks whose analysis is challenging\nbecause of geometrical complexities and unique methodological problems. In this\ncontext, it is essential to incorporate the network specificity into the\nanalysis as the locations of events are restricted to areas covered by line\nsegments. Relying on concepts originating from graph theory, we extend the\nnotions of first-order network intensity functions to second-order and local\nnetwork intensity functions. We consider two types of local indicators of\nnetwork association functions which can be understood as adaptations of the\nprimary ideas of local analysis on the plane. We develop the node-wise and\ncross-hierarchical type of local functions. A real dataset on urban\ndisturbances is also presented.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 10:12:39 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Eckardt", "Matthias", ""], ["Mateu", "Jorge", ""]]}, {"id": "1712.01620", "submitter": "Sophie Marque-Pucheu", "authors": "Sophie Marque-Pucheu and Guillaume Perrin and Josselin Garnier", "title": "Efficient sequential experimental design for surrogate modeling of\n  nested codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to computing power increase, the certification and the conception of\ncomplex systems relies more and more on simulation. To this end, predictive\ncodes are needed, which have generally to be evaluated in a huge number of\ninput points. When the computational cost of these codes is high, surrogate\nmodels are introduced to emulate the response of these codes. In this paper, we\nconsider the situation when the system response can be modeled by two nested\ncomputer codes. By two nested computer codes, we mean that some inputs of the\nsecond code are outputs of the first code. More precisely, the idea is to\npropose sequential designs to improve the accuracy of the nested code's\npredictor by exploiting the nested structure of the codes. In particular, a\nselection criterion is proposed to allow the modeler to choose the code to\ncall, depending on the expected learning rate and the computational cost of\neach code. The sequential designs are based on the minimization of the\nprediction variance, so adaptations of the Gaussian process formalism are\nproposed for this particular configuration in order to quickly evaluate the\nmean and the variance of the predictor. The proposed methods are then applied\nto examples.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 13:53:46 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Marque-Pucheu", "Sophie", ""], ["Perrin", "Guillaume", ""], ["Garnier", "Josselin", ""]]}, {"id": "1712.01634", "submitter": "Tuomas Rajala", "authors": "Tuomas Rajala and Claudia Redenbach and Aila S\\\"arkk\\\"a and Martina\n  Sormani", "title": "A review on anisotropy analysis of spatial point patterns", "comments": "Submitted to Spatial Statistics -journal's special issue of the\n  Spatial Statistics 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A spatial point pattern is called anisotropic if its spatial structure\ndepends on direction. Several methods for anisotropy analysis have been\nintroduced in the literature. In this paper, we give an overview of\nnonparametric methods for anisotropy analysis of (stationary) point patterns in\n$\\mathbf{R}^2$ and $\\mathbf{R}^3$. We discuss methods based on nearest\nneighbour and second order summary statistics as well as spectral and wavelet\nanalysis. All techniques are illustrated on both a clustered and a regular\nexample. Finally, we discuss methods for testing for isotropy as well as for\nestimating preferred directions in a point pattern.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 14:14:46 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 11:00:10 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Rajala", "Tuomas", ""], ["Redenbach", "Claudia", ""], ["S\u00e4rkk\u00e4", "Aila", ""], ["Sormani", "Martina", ""]]}, {"id": "1712.01793", "submitter": "Chris Oates", "authors": "Chris. J. Oates, Alessandro Barp, Mark Girolami", "title": "Posterior Integration on a Riemannian Manifold", "comments": "This paper was superseded by arXiv:1810.04946", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The geodesic Markov chain Monte Carlo method and its variants enable\ncomputation of integrals with respect to a posterior supported on a manifold.\nHowever, for regular integrals, the convergence rate of the ergodic average\nwill be sub-optimal. To fill this gap, this paper extends the efficient\nposterior integration method of Oates et al. (2017) to the case of a Riemannian\nmanifold. In contrast to the original Euclidean case, no non-trivial boundary\nconditions are needed for a closed manifold. The method is assessed through\nsimulation and deployed to compute posterior integrals for an Australian\nMesozoic paleomagnetic pole model, whose parameters are constrained to lie on\nthe manifold $M = \\mathbb{S}^2 \\times \\mathbb{R}_+$.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 18:15:39 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 18:13:43 GMT"}, {"version": "v3", "created": "Mon, 15 Jan 2018 10:13:25 GMT"}, {"version": "v4", "created": "Sun, 14 Oct 2018 11:26:27 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Oates", "Chris. J.", ""], ["Barp", "Alessandro", ""], ["Girolami", "Mark", ""]]}, {"id": "1712.01798", "submitter": "Jun Li", "authors": "Jun Li, Yumou Qiu and Lingjun Li", "title": "A Neighborhood-Assisted Hotelling's $T^2$ Test for High-Dimensional\n  Means", "comments": "41 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tests have been proposed to remedy the classical Hotelling's $T^2$ test\nin the \"large $p$, small $n$\" paradigm, but the existence of an optimal\nsum-of-squares type test has not been explored. This paper shows that under\ncertain conditions, the population Hotelling's $T^2$ test with the known\n$\\Sigma^{-1}$ attains the best power among all the $L_2$-norm based tests with\nthe data transformation by $\\Sigma^{\\eta}$ for $\\eta \\in (-\\infty, \\infty)$. To\nextend the result to the case of unknown $\\Sigma^{-1}$, we propose a\nNeighborhood-Assisted Hotelling's $T^2$ statistic obtained by replacing the\ninverse of sample covariance matrix in the classical Hotelling's $T^2$\nstatistic with a regularized covariance estimator. Utilizing a regression\nmodel, we establish its asymptotic normality under mild conditions. We show\nthat the proposed test is able to match the performance of the population\nHotelling's $T^2$ test under certain conditions, and thus possesses certain\noptimality. Moreover, it can adaptively attain the best power by empirically\nchoosing a neighborhood size to maximize its signal-to-noise ratio. Simulation\nexperiments and case studies are given to demonstrate the empirical performance\nof the proposed test.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 18:24:16 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 03:44:05 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Li", "Jun", ""], ["Qiu", "Yumou", ""], ["Li", "Lingjun", ""]]}, {"id": "1712.01824", "submitter": "Fernanda Paula", "authors": "Fernanda V. Paula, Abra\\~ao D. C. Nascimento and Get\\'ulio J. A.\n  Amaral", "title": "A new extended Cardioid model: an application to wind data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cardioid distribution is a relevant model for circular data. However,\nthis model is not suitable for scenarios were there is asymmetry or\nmultimodality. In order to overcome this gap, an extended Cardioid model is\nproposed, which is called Exponentiated Cardioid (EC) distribution. Besides,\nsome of its properties are derived, such as trigonometric moments, kurtosis and\nskewness. A discussion about the modality and and expressions for the quantiles\nthrough approximations of the studied model are also presented. To fit the EC\nmodel, two estimation methods are presented based on maximum likelihood and\nquantile least squares procedures. The performance of proposed estimators is\nevaluated in a Monte Carlo simulation study, adopting both bias and mean square\nerror as comparison criteria. Finally, the proposed model is applied to a\ndataset in the wind direction context. Results indicate that the EC\ndistribution may outperform Cardioid and the von Mises distributions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 18:56:52 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Paula", "Fernanda V.", ""], ["Nascimento", "Abra\u00e3o D. C.", ""], ["Amaral", "Get\u00falio J. A.", ""]]}, {"id": "1712.01992", "submitter": "Felipe Tagle", "authors": "Felipe Tagle, Stefano Castruccio and Marc G. Genton", "title": "A Multi-Resolution Spatial Model for Large Datasets Based on the Skew-t\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large, non-Gaussian spatial datasets pose a considerable modeling challenge\nas the dependence structure implied by the model needs to be captured at\ndifferent scales, while retaining feasible inference. Skew-normal and skew-t\ndistributions have only recently begun to appear in the spatial statistics\nliterature, without much consideration, however, for the ability to capture\ndependence at multiple resolutions, and simultaneously achieve feasible\ninference for increasingly large data sets. This article presents the first\nmulti-resolution spatial model inspired by the skew-t distribution, where a\nlarge-scale effect follows a multivariate normal distribution and the\nfine-scale effects follow a multivariate skew-normal distributions. The\nresulting marginal distribution for each region is skew-t, thereby allowing for\ngreater flexibility in capturing skewness and heavy tails characterizing many\nenvironmental datasets. Likelihood-based inference is performed using a Monte\nCarlo EM algorithm. The model is applied as a stochastic generator of daily\nwind speeds over Saudi Arabia.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 01:17:01 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Tagle", "Felipe", ""], ["Castruccio", "Stefano", ""], ["Genton", "Marc G.", ""]]}, {"id": "1712.02043", "submitter": "Kun Liang", "authors": "Peter MacDonald and Kun Liang and Arnold Janssen", "title": "Dynamic adaptive procedures that control the false discovery rate", "comments": "To appear in Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the multiple testing problem with independent tests, the classical linear\nstep-up procedure controls the false discovery rate (FDR) at level\n$\\pi_0\\alpha$, where $\\pi_0$ is the proportion of true null hypotheses and\n$\\alpha$ is the target FDR level. Adaptive procedures can improve power by\nincorporating estimates of $\\pi_0$, which typically rely on a tuning parameter.\nFixed adaptive procedures set their tuning parameters before seeing the data\nand can be shown to control the FDR in finite samples. We develop theoretical\nresults for dynamic adaptive procedures whose tuning parameters are determined\nby the data. We show that, if the tuning parameter is chosen according to a\nleft-to-right stopping time rule, the corresponding dynamic adaptive procedure\ncontrols the FDR in finite samples. Examples include the recently proposed\nright-boundary procedure and the widely used lowest-slope procedure, among\nothers. Simulation results show that the right-boundary procedure is more\npowerful than other dynamic adaptive procedures under independence and mild\ndependence conditions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 05:31:42 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 14:54:24 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 16:00:56 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["MacDonald", "Peter", ""], ["Liang", "Kun", ""], ["Janssen", "Arnold", ""]]}, {"id": "1712.02150", "submitter": "Abhishek Dey", "authors": "Abhishek Dey, Kushal Chakrabarti, Krishan Kumar Gola and Shaunak Sen", "title": "A Kalman Filter Approach for Biomolecular Systems with Noise Covariance\n  Updating", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important part of system modeling is determining parameter values,\nparticularly for biomolecular systems, where direct measurements of individual\nparameters are typically hard. While Extended Kalman Filters have been used for\nthis purpose, the choice of the process noise covariance is generally unclear.\nIn this chapter, we address this issue for biomolecular systems using a\ncombination of Monte Carlo simulations and experimental data, exploiting the\ndependence of the process noise covariance on the states and parameters, as\ngiven in the Langevin framework. We adapt a Hybrid Extended Kalman Filtering\ntechnique by updating the process noise covariance at each time step based on\nestimates. We compare the performance of this framework with different fixed\nvalues of process noise covariance in biomolecular system models, including an\noscillator model, as well as in experimentally measured data for a negative\ntranscriptional feedback circuit. We find that the Extended Kalman Filter with\nsuch process noise covariance update is closer to the optimality condition in\nthe sense that the innovation sequence becomes white and in achieving a balance\nbetween the mean square estimation error and parameter convergence time. The\nresults of this chapter may help in the use of Extended Kalman Filters for\nsystems where process noise covariance depends on states and/or parameters.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 12:09:42 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 10:12:15 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 15:34:11 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Dey", "Abhishek", ""], ["Chakrabarti", "Kushal", ""], ["Gola", "Krishan Kumar", ""], ["Sen", "Shaunak", ""]]}, {"id": "1712.02195", "submitter": "Ranjan Maitra", "authors": "Alejandro Murua and Ranjan Maitra", "title": "Fast spatial inference in the homogeneous Ising model", "comments": "18 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ising model is important in statistical modeling and inference in many\napplications, however its normalizing constant, mean number of active vertices\nand mean spin interaction are intractable. We provide accurate approximations\nthat make it possible to calculate these quantities numerically. Simulation\nstudies indicate good performance when compared to Markov Chain Monte Carlo\nmethods and at a tiny fraction of the time. The methodology is also used to\nperform Bayesian inference in a functional Magnetic Resonance Imaging\nactivation detection experiment.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 14:24:34 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 00:10:33 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Murua", "Alejandro", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1712.02214", "submitter": "Chaojie Wang", "authors": "Chaojie Wang, Linghao Shen, Han Li and Xiaodan Fan", "title": "Nonparametric Statistical Inference and Imputation for Incomplete\n  Categorical Data", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missingness in categorical data is a common problem in various real\napplications. Traditional approaches either utilize only the complete\nobservations or impute the missing data by some ad hoc methods rather than the\ntrue conditional distribution of the missing data, thus losing or distorting\nthe rich information in the partial observations. In this paper, we propose the\nDirichlet Process Mixture of Collapsed Product-Multinomials (DPMCPM) to model\nthe full data jointly and compute the model efficiently. By fitting an infinite\nmixture of product-multinomial distributions, DPMCPM is applicable for any\ncategorical data regardless of the true distribution, which may contain complex\nassociation among variables. Under the framework of latent class analysis, we\nshow that DPMCPM can model general missing mechanisms by creating an extra\ncategory to denote missingness, which implicitly integrates out the missing\npart with regard to their true conditional distribution. Through simulation\nstudies and a real application, we demonstrate that DPMCPM outperforms existing\napproaches on statistical inference and imputation for incomplete categorical\ndata of various missing mechanisms. DPMCPM is implemented as the R package\n\\texttt{MMDai}, which is available from the Comprehensive R Archive Network at\nhttps://cran.r-project.org/web/packages/MMDai/index.html.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 08:51:36 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 06:22:21 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Wang", "Chaojie", ""], ["Shen", "Linghao", ""], ["Li", "Han", ""], ["Fan", "Xiaodan", ""]]}, {"id": "1712.02226", "submitter": "Stefan Czesla", "authors": "S. Czesla, T. Molle, J. H. M. M. Schmitt", "title": "A posteriori noise estimation in variable data sets", "comments": "Accepted for publication in A&A", "journal-ref": null, "doi": "10.1051/0004-6361/201730618", "report-no": null, "categories": "stat.ME astro-ph.IM astro-ph.SR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most physical data sets contain a stochastic contribution produced by\nmeasurement noise or other random sources along with the signal. Usually,\nneither the signal nor the noise are accurately known prior to the measurement\nso that both have to be estimated a posteriori. We have studied a procedure to\nestimate the standard deviation of the stochastic contribution assuming\nnormality and independence, requiring a sufficiently well-sampled data set to\nyield reliable results. This procedure is based on estimating the standard\ndeviation in a sample of weighted sums of arbitrarily sampled data points and\nis identical to the so-called DER_SNR algorithm for specific parameter\nsettings. To demonstrate the applicability of our procedure, we present\napplications to synthetic data, high-resolution spectra, and a large sample of\nspace-based light curves and, finally, give guidelines to apply the procedure\nin situation not explicitly considered here to promote its adoption in data\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 18:08:32 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Czesla", "S.", ""], ["Molle", "T.", ""], ["Schmitt", "J. H. M. M.", ""]]}, {"id": "1712.02379", "submitter": "Todd Kuffner", "authors": "Liang Hong and Todd A. Kuffner and Ryan Martin", "title": "On overfitting and post-selection uncertainty assessments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a regression context, when the relevant subset of explanatory variables is\nuncertain, it is common to use a data-driven model selection procedure.\nClassical linear model theory, applied naively to the selected sub-model, may\nnot be valid because it ignores the selected sub-model's dependence on the\ndata. We provide an explanation of this phenomenon, in terms of overfitting,\nfor a class of model selection criteria.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 19:14:30 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Hong", "Liang", ""], ["Kuffner", "Todd A.", ""], ["Martin", "Ryan", ""]]}, {"id": "1712.02383", "submitter": "Srinjoy Das", "authors": "Srinjoy Das, Dimitris N. Politis", "title": "Predictive inference for locally stationary time series with an\n  application to climate data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Model-free Prediction Principle of Politis (2015) has been successfully\napplied to general regression problems, as well as problems involving\nstationary time series. However, with long time series, e.g. annual temperature\nmeasurements spanning over 100 years or daily financial returns spanning\nseveral years, it may be unrealistic to assume stationarity throughout the span\nof the dataset. In the paper at hand, we show how Model-free Prediction can be\napplied to handle time series that are only locally stationary, i.e., they can\nbe assumed to be as stationary only over short time-windows. Surprisingly there\nis little literature on point prediction for general locally stationary time\nseries even in model-based setups and there is no literature on the\nconstruction of prediction intervals of locally stationary time series. We\nattempt to fill this gap here as well. Both one-step-ahead point predictors and\nprediction intervals are constructed, and the performance of model-free is\ncompared to model-based prediction using models that incorporate a trend and/or\nheteroscedasticity. Both aspects of the paper, model-free and model-based, are\nnovel in the context of time-series that are locally (but not globally)\nstationary. We also demonstrate the application of our Model-based and\nModel-free prediction methods to speleothem climate data which exhibits local\nstationarity and show that our best model-free point prediction results\noutperform that obtained with the RAMPFIT algorithm previously used for\nanalysis of this data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 19:32:18 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 07:26:31 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Das", "Srinjoy", ""], ["Politis", "Dimitris N.", ""]]}, {"id": "1712.02412", "submitter": "Guo Yu", "authors": "Guo Yu and Jacob Bien", "title": "Estimating the error variance in a high-dimensional linear model", "comments": "Biometrika(2019)", "journal-ref": null, "doi": "10.1093/biomet/asz017", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lasso has been studied extensively as a tool for estimating the\ncoefficient vector in the high-dimensional linear model; however, considerably\nless is known about estimating the error variance in this context. In this\npaper, we propose the natural lasso estimator for the error variance, which\nmaximizes a penalized likelihood objective. A key aspect of the natural lasso\nis that the likelihood is expressed in terms of the natural parameterization of\nthe multiparameter exponential family of a Gaussian with unknown mean and\nvariance. The result is a remarkably simple estimator of the error variance\nwith provably good performance in terms of mean squared error. These\ntheoretical results do not require placing any assumptions on the design matrix\nor the true regression coefficients. We also propose a companion estimator,\ncalled the organic lasso, which theoretically does not require tuning of the\nregularization parameter. Both estimators do well empirically compared to\npreexisting methods, especially in settings where successful recovery of the\ntrue support of the coefficient vector is hard. Finally, we show that existing\nmethods can do well under fewer assumptions than previously known, thus\nproviding a fuller story about the problem of estimating the error variance in\nhigh-dimensional linear models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 21:16:35 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 18:06:46 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2019 09:15:05 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Yu", "Guo", ""], ["Bien", "Jacob", ""]]}, {"id": "1712.02445", "submitter": "Minerva Mukhopadhyay", "authors": "Minerva Mukhopadhyay and David B. Dunson", "title": "Targeted Random Projection for Prediction from High-Dimensional Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computationally-efficient prediction from high\ndimensional and highly correlated predictors in challenging settings where\naccurate variable selection is effectively impossible. Direct application of\npenalization or Bayesian methods implemented with Markov chain Monte Carlo can\nbe computationally daunting and unstable. Hence, some type of dimensionality\nreduction prior to statistical analysis is in order. Common solutions include\napplication of screening algorithms to reduce the regressors, or dimension\nreduction using projections of the design matrix. The former approach can be\nhighly sensitive to threshold choice in finite samples, while the later can\nhave poor performance in very high-dimensional settings. We propose a TArgeted\nRandom Projection (TARP) approach that combines positive aspects of both\nstrategies to boost performance. In particular, we propose to use information\nfrom independent screening to order the inclusion probabilities of the features\nin the projection matrix used for dimension reduction, leading to data-informed\nsparsity. We provide theoretical support for a Bayesian predictive algorithm\nbased on TARP, including both statistical and computational complexity\nguarantees. Examples for simulated and real data applications illustrate gains\nrelative to a variety of competitors.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 23:42:51 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Mukhopadhyay", "Minerva", ""], ["Dunson", "David B.", ""]]}, {"id": "1712.02469", "submitter": "Chunlin Wang", "authors": "Chunlin Wang, Paul Marriott and Pengfei Li", "title": "Asymptotic coverage probabilities of bootstrap percentile confidence\n  intervals for constrained parameters", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic behaviour of the commonly used bootstrap percentile confidence\ninterval is investigated when the parameters are subject to linear inequality\nconstraints. We concentrate on the important one- and two-sample problems with\ndata generated from general parametric distributions in the natural exponential\nfamily. The focus of this paper is on quantifying the coverage probabilities of\nthe parametric bootstrap percentile confidence intervals, in particular their\nlimiting behaviour near boundaries. We propose a local asymptotic framework to\nstudy this subtle coverage behaviour. Under this framework, we discover that\nwhen the true parameters are on, or close to, the restriction boundary, the\nasymptotic coverage probabilities can always exceed the nominal level in the\none-sample case; however, they can be, remarkably, both under and over the\nnominal level in the two-sample case. Using illustrative examples, we show that\nthe results provide theoretical justification and guidance on applying the\nbootstrap percentile method to constrained inference problems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 01:51:35 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Wang", "Chunlin", ""], ["Marriott", "Paul", ""], ["Li", "Pengfei", ""]]}, {"id": "1712.02476", "submitter": "Dilanka Shenal Dedduwakumara", "authors": "Dilanka S. Dedduwakumara, Luke A. Prendergast", "title": "Confidence Intervals for Quantiles from Histograms and Other Grouped\n  Data", "comments": "19 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval estimation of quantiles has been treated by many in the literature.\nHowever, to the best of our knowledge there has been no consideration for\ninterval estimation when the data are available in grouped format. Motivated by\nthis, we introduce several methods to obtain confidence intervals for quantiles\nwhen only grouped data is available. Our preferred method for interval\nestimation is to approximate the underlying density using the Generalized\nLambda Distribution (GLD) to both estimate the quantiles and variance of the\nquantile estimators. We compare the GLD method with some other methods that we\nalso introduce which are based on a frequency approximation approach and a\nlinear interpolation approximation of the density. Our methods are strongly\nsupported by simulations showing that excellent coverage can be achieved for a\nwide number of distributions. These distributions include highly-skewed\ndistributions such as the log-normal, Dagum and Singh-Maddala distributions. We\nalso apply our methods to real data and show that inference can be carried out\non published outcomes that have been summarized only by a histogram. Our\nmethods are therefore useful for a broad range of applications. We have also\ncreated a web application that can be used to conveniently calculate the\nestimators.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 02:42:52 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Dedduwakumara", "Dilanka S.", ""], ["Prendergast", "Luke A.", ""]]}, {"id": "1712.02497", "submitter": "Yanjun He", "authors": "Yanjun He and Peter D. Hoff", "title": "Multiplicative Coevolution Regression Models for Longitudinal Networks\n  and Nodal Attributes", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple and extendable coevolution model for the analysis of\nlongitudinal network and nodal attribute data. The model features parameters\nthat describe three phenomena: homophily, contagion and autocorrelation of the\nnetwork and nodal attribute process. Homophily here describes how changes to\nthe network may be associated with between-node similarities in terms of their\nnodal attributes. Contagion refers to how node-level attributes may change\ndepending on the network. The model we present is based upon a pair of\nintertwined autoregressive processes. We obtain least-squares parameter\nestimates for continuous-valued fully-observed network and attribute data. We\nalso provide methods for Bayesian inference in several other cases, including\nordinal network and attribute data, and models involving latent nodal\nattributes. These model extensions are applied to an analysis of international\nrelations data and to data from a study of teen delinquency and friendship\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 05:29:40 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["He", "Yanjun", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1712.02605", "submitter": "Dario Briscolini", "authors": "Dario Briscolini, Loredana Di Consiglio, Brunero Liseo, Andrea\n  Tancredi, Tiziana Tuoto", "title": "New Methods for Small Area Estimation with Linkage Uncertainty", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Official Statistics, interest for data integration has been increasingly\ngrowing, due to the need of extracting information from different sources.\nHowever, the effects of these procedures on the validity of the resulting\nstatistical analyses has been disregarded for a long time. In recent years, it\nhas been largely recognized that linkage is not an error-free procedure and\nlinkage errors, as false links and/or missed links, can invalidate the\nreliability of estimates in standard statistical models. In this paper we\nconsider the general problem of making inference using data that have been\nprobabilistically linked and we explore the effect of potential linkage errors\non the production of small area estimates. We describe the existing methods and\npropose and compare new approaches both from a classical and from a Bayesian\nperspective. We perform a simulation study to assess pros and cons of each\nproposed method; our simulation scheme aims at reproducing a realistic context\nboth for small area estimation and record linkage procedures.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 13:08:23 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Briscolini", "Dario", ""], ["Di Consiglio", "Loredana", ""], ["Liseo", "Brunero", ""], ["Tancredi", "Andrea", ""], ["Tuoto", "Tiziana", ""]]}, {"id": "1712.02675", "submitter": "Andreas Svensson", "authors": "Andreas Svensson, Dave Zachariah, Thomas B. Sch\\\"on", "title": "How consistent is my model with the data? Information-Theoretic Model\n  Check", "comments": "The title has been updated, but no other significant changes have\n  been made from the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of model class is fundamental in statistical learning and system\nidentification, no matter whether the class is derived from physical principles\nor is a generic black-box. We develop a method to evaluate the specified model\nclass by assessing its capability of reproducing data that is similar to the\nobserved data record. This model check is based on the information-theoretic\nproperties of models viewed as data generators and is applicable to e.g.\nsequential data and nonlinear dynamical models. The method can be understood as\na specific two-sided posterior predictive test. We apply the\ninformation-theoretic model check to both synthetic and real data and compare\nit with a classical whiteness test.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 15:40:17 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 08:55:47 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Svensson", "Andreas", ""], ["Zachariah", "Dave", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1712.02685", "submitter": "Natalie Neumeyer", "authors": "Natalie Neumeyer and Ingrid Van Keilegom", "title": "Bootstrap of residual processes in regression: to smooth or not to\n  smooth ?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a location model of the form $Y = m(X) +\n\\varepsilon$, where $m(\\cdot)$ is the unknown regression function, the error\n$\\varepsilon$ is independent of the $p$-dimensional covariate $X$ and\n$E(\\varepsilon)=0$. Given i.i.d. data $(X_1,Y_1),\\ldots,(X_n,Y_n)$ and given an\nestimator $\\hat m(\\cdot)$ of the function $m(\\cdot)$ (which can be parametric\nor nonparametric of nature), we estimate the distribution of the error term\n$\\varepsilon$ by the empirical distribution of the residuals $Y_i-\\hat m(X_i)$,\n$i=1,\\ldots,n$. To approximate the distribution of this estimator, Koul and\nLahiri (1994) and Neumeyer (2008, 2009) proposed bootstrap procedures, based on\nsmoothing the residuals either before or after drawing bootstrap samples. So\nfar it has been an open question whether a classical non-smooth residual\nbootstrap is asymptotically valid in this context. In this paper we solve this\nopen problem, and show that the non-smooth residual bootstrap is consistent. We\nillustrate this theoretical result by means of simulations, that show the\naccuracy of this bootstrap procedure for various models, testing procedures and\nsample sizes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 15:52:00 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Neumeyer", "Natalie", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "1712.02845", "submitter": "Grant Izmirlian", "authors": "Grant Izmirlian", "title": "The Shrinkage Variance Hotelling $T^2$ Test for Genomic Profiling\n  Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designed gene expression micro-array experiments, consisting of several\ntreatment levels with a number of replicates per level, are analyzed by\napplying simple tests for group differences at the per gene level. The gene\nlevel statistics are sorted and a criterion for selecting important genes which\ntakes into account multiplicity is applied. A caveat arises in that true\nsignals (genes truly over or under expressed) are \"competing\" with fairly large\ntype I error signals. False positives near the top of a sorted list can occur\nwhen genes having very small fold-change are compensated by small enough\nvariance to yield a large test statistic. One of the first attempts around this\ncaveat as the development of \"significance analysis of micro-arrays (SAM)\",\nwhich used a modified t-type statistic thresholded against its permutation\ndistribution. The key innovation of the modified t-statistic was the addition\nof a constant to the per gene standard errors in order to stabilize the\ncoefficient of variation of the resulting test statistic. Since then, several\nauthors have proposed the use of shrinkage variance estimators in conjunction\nwith t-type, and more generally, ANOVA type tests at the gene level. Our new\napproach proposes the use of a shrinkage variance Hotelling T-squared statistic\nin which the per gene sample covariance matrix is replaced by a shrinkage\nestimate borrowing strength from across all genes. It is demonstrated that the\nnew statistic retains the F-distribution under the null, with added degrees of\nfreedom in the denominator. Advantages of this class of tests are (i)\nflexibility in that a whole family of hypothesis tests is possible (ii) the\ngains of the above-mentioned earlier innovations are enjoyed more fully. This\npaper summarizes our results and presents a simulation study benchmarking the\nnew statistic against another recently proposed statistic.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 20:04:11 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Izmirlian", "Grant", ""]]}, {"id": "1712.03032", "submitter": "Leonhard Held", "authors": "Leonhard Held", "title": "p-Values for Credibility", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of credibility is a reverse-Bayes technique that has been proposed\nby Matthews (2001) to overcome some of the shortcomings of significance tests.\nA significant result is deemed credible if current knowledge about the effect\nsize is in conflict with any sceptical prior that would make the effect\nnon-significant. In this paper I formalize the approach and propose to use\nBayesian predictive tail probabilities to quantify the evidence for\ncredibility. This gives rise to a p-value for extrinsic credibility, taking\ninto account both the internal and the external evidence for an effect. The\nassessment of intrinsic credibility leads to a new threshold for ordinary\nsignificance that is remarkably close to the recently proposed 0.005 level.\nFinally, a p-value for intrinsic credibility is proposed that is a simple\nfunction of the ordinary p-value for significance and has a direct frequentist\ninterpretation in terms of the replication probability that a future study\nunder identical conditions will give an estimated effect in the same direction\nas the first study.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 11:33:32 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Held", "Leonhard", ""]]}, {"id": "1712.03040", "submitter": "Frederic Lavancier", "authors": "Jean-Fran\\c{c}ois Coeurjolly and Fr\\'ed\\'eric Lavancier", "title": "Approximation intensity for pairwise interaction Gibbs point processes\n  using determinantal point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intensity of a Gibbs point process is usually an intractable function of\nthe model parameters. For repulsive pairwise interaction point processes, this\nintensity can be expressed as the Laplace transform of some particular\nfunction. Baddeley and Nair (2002) developped the Poisson-saddlepoint\napproximation which consists, for basic models, in calculating this Laplace\ntransform with respect to a homogeneous Poisson point process. In this paper,\nwe develop an approximation which consists in calculating the same Laplace\ntransform with respect to a specific determinantal point process. This new\napproximation is efficiently implemented and turns out to be more accurate than\nthe Poisson-saddlepoint approximation, as demonstrated by some numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 12:12:10 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Lavancier", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1712.03058", "submitter": "Theresa Stocks", "authors": "Theresa Stocks", "title": "Iterated filtering methods for Markov process epidemic models", "comments": "This manuscript is a preprint of a chapter to appear in the Handbook\n  of Infectious Disease Data Analysis, Held, L., Hens, N., O'Neill, P.D. and\n  Wallinga, J. (Eds.). Chapman \\& Hall/CRC, 2018. Please use the book for\n  possible citations. Corrected typo in the references and modified second\n  example", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic epidemic models have proven valuable for public health decision\nmakers as they provide useful insights into the understanding and prevention of\ninfectious diseases. However, inference for these types of models can be\ndifficult because the disease spread is typically only partially observed e.g.\nin form of reported incidences in given time periods. This chapter discusses\nhow to perform likelihood-based inference for partially observed Markov\nepidemic models when it is relatively easy to generate samples from the Markov\ntransmission model while the likelihood function is intractable. The first part\nof the chapter reviews the theoretical background of inference for partially\nobserved Markov processes (POMP) via iterated filtering. In the second part of\nthe chapter the performance of the method and associated practical difficulties\nare illustrated on two examples. In the first example a simulated outbreak data\nset consisting of the number of newly reported cases aggregated by week is\nfitted to a POMP where the underlying disease transmission model is assumed to\nbe a simple Markovian SIR model. The second example illustrates possible model\nextensions such as seasonal forcing and over-dispersion in both, the\ntransmission and observation model, which can be used, e.g., when analysing\nroutinely collected rotavirus surveillance data. Both examples are implemented\nusing the R-package pomp (King et al., 2016) and the code is made available\nonline.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 13:44:34 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 09:08:34 GMT"}, {"version": "v3", "created": "Sun, 28 Oct 2018 10:18:55 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Stocks", "Theresa", ""]]}, {"id": "1712.03198", "submitter": "Tim Morris", "authors": "Tim P Morris, Ian R White, Michael J Crowther", "title": "Using simulation studies to evaluate statistical methods", "comments": "31 pages, 9 figures (2 in appendix), 8 tables (1 in appendix)", "journal-ref": null, "doi": "10.1002/sim.8086", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulation studies are computer experiments that involve creating data by\npseudorandom sampling. The key strength of simulation studies is the ability to\nunderstand the behaviour of statistical methods because some 'truth' (usually\nsome parameter/s of interest) is known from the process of generating the data.\nThis allows us to consider properties of methods, such as bias. While widely\nused, simulation studies are often poorly designed, analysed and reported. This\ntutorial outlines the rationale for using simulation studies and offers\nguidance for design, execution, analysis, reporting and presentation. In\nparticular, this tutorial provides: a structured approach for planning and\nreporting simulation studies, which involves defining aims, data-generating\nmechanisms, estimands, methods and performance measures ('ADEMP'); coherent\nterminology for simulation studies; guidance on coding simulation studies; a\ncritical discussion of key performance measures and their estimation; guidance\non structuring tabular and graphical presentation of results; and new graphical\npresentations. With a view to describing recent practice, we review 100\narticles taken from Volume 34 of Statistics in Medicine that included at least\none simulation study and identify areas for improvement.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 17:49:28 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 15:25:49 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 16:57:15 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Morris", "Tim P", ""], ["White", "Ian R", ""], ["Crowther", "Michael J", ""]]}, {"id": "1712.03310", "submitter": "Simon Mak", "authors": "Simon Mak, Yao Xie", "title": "Maximum entropy low-rank matrix recovery", "comments": "Fixing typos", "journal-ref": null, "doi": "10.1109/JSTSP.2018.2840481", "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper a novel, information-theoretic method, called\nMaxEnt, for efficient data acquisition for low-rank matrix recovery. This\nproposed method has important applications to a wide range of problems,\nincluding image processing and text document indexing. Fundamental to our\ndesign approach is the so-called maximum entropy principle, which states that\nthe measurement masks which maximize the entropy of observations, also maximize\nthe information gain on the unknown matrix $\\mathbf{X}$. Coupled with a\nlow-rank stochastic model for $\\mathbf{X}$, such a principle (i) reveals novel\nconnections between information-theoretic sampling and subspace packings, and\n(ii) yields efficient mask construction algorithms for matrix recovery, which\nsignificantly outperforms random measurements. We illustrate the effectiveness\nof MaxEnt in simulation experiments, and demonstrate its usefulness in two\nreal-world applications on image recovery and text document indexing.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 23:08:12 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 03:01:30 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 20:06:03 GMT"}, {"version": "v4", "created": "Wed, 25 Jul 2018 19:30:16 GMT"}, {"version": "v5", "created": "Mon, 10 Sep 2018 06:45:07 GMT"}, {"version": "v6", "created": "Wed, 21 Nov 2018 05:11:51 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Mak", "Simon", ""], ["Xie", "Yao", ""]]}, {"id": "1712.03358", "submitter": "Kayanan Manickavasagar", "authors": "Manickavasagar Kayanan and Pushpakanthie Wijekoon", "title": "Stochastic Restricted Biased Estimators in misspecified regression model\n  with incomplete prior information", "comments": "35 Pages, 6 Figures", "journal-ref": "Kayanan, M., & Wijekoon, P. (2018). Stochastic Restricted Biased\n  Estimators in Misspecified Regression Model with Incomplete Prior\n  Information. Journal of Probability and Statistics, 8.\n  doi:10.1155/2018/1452181", "doi": "10.1155/2018/1452181", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, the analysis of misspecification was extended to the\nrecently introduced stochastic restricted biased estimators when\nmulticollinearity exists among the explanatory variables. The Stochastic\nRestricted Ridge Estimator (SRRE), Stochastic Restricted Almost Unbiased Ridge\nEstimator (SRAURE), Stochastic Restricted Liu Estimator (SRLE), Stochastic\nRestricted Almost Unbiased Liu Estimator (SRAULE), Stochastic Restricted\nPrincipal Component Regression Estimator (SRPCR), Stochastic Restricted r-k\nclass estimator (SRrk) and Stochastic Restricted r-d class estimator (SRrd)\nwere examined in the misspecified regression model due to missing relevant\nexplanatory variables when incomplete prior information of the regression\ncoefficients is available. Further, the superiority conditions between\nestimators and their respective predictors were obtained in the mean square\nerror matrix (MSEM) sense. Finally, a numerical example and a Monte Carlo\nsimulation study were used to illustrate the theoretical findings.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 08:59:00 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 19:40:27 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Kayanan", "Manickavasagar", ""], ["Wijekoon", "Pushpakanthie", ""]]}, {"id": "1712.03448", "submitter": "Xinwei Ma", "authors": "Matias D. Cattaneo, Xinwei Ma, Yusufcan Masatlioglu, Elchin Suleymanov", "title": "A Random Attention Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.TH stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper illustrates how one can deduce preference from observed choices\nwhen attention is not only limited but also random. In contrast to earlier\napproaches, we introduce a Random Attention Model (RAM) where we abstain from\nany particular attention formation, and instead consider a large class of\nnonparametric random attention rules. Our model imposes one intuitive\ncondition, termed Monotonic Attention, which captures the idea that each\nconsideration set competes for the decision-maker's attention. We then develop\nrevealed preference theory within RAM and obtain precise testable implications\nfor observable choice probabilities. Based on these theoretical findings, we\npropose econometric methods for identification, estimation, and inference of\nthe decision maker's preferences. To illustrate the applicability of our\nresults and their concrete empirical content in specific settings, we also\ndevelop revealed preference theory and accompanying econometric methods under\nadditional nonparametric assumptions on the consideration set for binary choice\nproblems. Finally, we provide general purpose software implementation of our\nestimation and inference results, and showcase their performance using\nsimulations.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 22:50:39 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 15:35:57 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 21:32:22 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Ma", "Xinwei", ""], ["Masatlioglu", "Yusufcan", ""], ["Suleymanov", "Elchin", ""]]}, {"id": "1712.03512", "submitter": "Vladimir Bochkarev", "authors": "Inna A. Belashova and Vladimir V. Bochkarev", "title": "Comparative analysis of criteria for filtering time series of word usage\n  frequencies", "comments": "10 pages, 4 figures. This report was presented at ITISE 2017\n  (International work-conference on Time Series), September 18-20th 2017,\n  Granada, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method of nonlinear wavelet thresholding of time\nseries. The Ramachandran-Ranganathan runs test is used to assess the quality of\napproximation. To minimize the objective function, it is proposed to use\ngenetic algorithms - one of the stochastic optimization methods. The suggested\nmethod is tested both on the model series and on the word frequency series\nusing the Google Books Ngram data. It is shown that method of filtering which\nuses the runs criterion shows significantly better results compared with the\nstandard wavelet thresholding. The method can be used when quality of filtering\nis of primary importance but not the speed of calculations.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 12:04:19 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Belashova", "Inna A.", ""], ["Bochkarev", "Vladimir V.", ""]]}, {"id": "1712.03561", "submitter": "Anthony Christidis", "authors": "Anthony Christidis, Ruben Zamar, Laks V.S. Lakshmanan and Ezequiel\n  Smucler", "title": "Split Regularized Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for fitting linear regression models that splits the\nset of covariates into groups. The optimal split of the variables into groups\nand the regularized estimation of the regression coefficients are performed by\nminimizing an objective function that encourages sparsity within each group and\ndiversity among them. The estimated coefficients are then pooled together to\nform the final fit. Our procedure works on top of a given penalized linear\nregression estimator (e.g., Lasso, elastic net) by fitting it to possibly\noverlapping groups of features, encouraging diversity among these groups to\nreduce the correlation of the corresponding predictions. For the case of two\ngroups, elastic net penalty and orthogonal predictors, we give a closed form\nsolution for the regression coefficients in each group. We establish the\nconsistency of our method with the number of predictors possibly increasing\nwith the sample size. An extensive simulation study and real-data applications\nshow that in general the proposed method improves the prediction accuracy of\nthe base estimator used in the procedure. Possible extensions to GLMs and other\nmodels are discussed. The supplemental material for this article, available\nonline, contains the proofs of our theoretical results and the full results of\nour simulation study.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 17:08:33 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 23:56:33 GMT"}, {"version": "v3", "created": "Thu, 31 Jan 2019 03:02:19 GMT"}, {"version": "v4", "created": "Fri, 26 Apr 2019 04:04:53 GMT"}, {"version": "v5", "created": "Tue, 7 May 2019 08:21:45 GMT"}, {"version": "v6", "created": "Thu, 11 Jul 2019 04:53:52 GMT"}, {"version": "v7", "created": "Tue, 30 Jul 2019 06:00:18 GMT"}, {"version": "v8", "created": "Thu, 12 Dec 2019 06:26:22 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Christidis", "Anthony", ""], ["Zamar", "Ruben", ""], ["Lakshmanan", "Laks V. S.", ""], ["Smucler", "Ezequiel", ""]]}, {"id": "1712.03589", "submitter": "Simon Mak", "authors": "Simon Mak, C. F. Jeff Wu", "title": "Analysis-of-marginal-Tail-Means (ATM): a robust method for discrete\n  black-box optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method, called Analysis-of-marginal-Tail-Means (ATM), for\neffective robust optimization of discrete black-box problems. ATM has important\napplications to many real-world engineering problems (e.g., manufacturing\noptimization, product design, molecular engineering), where the objective to\noptimize is black-box and expensive, and the design space is inherently\ndiscrete. One weakness of existing methods is that they are not robust: these\nmethods perform well under certain assumptions, but yield poor results when\nsuch assumptions (which are difficult to verify in black-box problems) are\nviolated. ATM addresses this via the use of marginal tail means for\noptimization, which combines both rank-based and model-based methods. The\ntrade-off between rank- and model-based optimization is tuned by first\nidentifying important main effects and interactions, then finding a good\ncompromise which best exploits additive structure. By adaptively tuning this\ntrade-off from data, ATM provides improved robust optimization over existing\nmethods, particularly in problems with (i) a large number of factors, (ii)\nunordered factors, or (iii) experimental noise. We demonstrate the\neffectiveness of ATM in simulations and in two real-world engineering problems:\nthe first on robust parameter design of a circular piston, and the second on\nproduct family design of a thermistor network.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 20:49:26 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 03:04:35 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Mak", "Simon", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "1712.03646", "submitter": "Kenichiro McAlinn", "authors": "Kenichiro McAlinn", "title": "Dynamic Mixed Frequency Synthesis for Economic Nowcasting", "comments": "arXiv admin note: text overlap with arXiv:1601.07463", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel Bayesian framework for dynamic modeling of mixed frequency\ndata to nowcast quarterly U.S. GDP growth. The introduced framework utilizes\nfoundational Bayesian theory and treats data sampled at different frequencies\nas latent factors that are later synthesized, allowing flexible methodological\nspecifications based on interests and utility. Time-varying inter-dependencies\nbetween the mixed frequency data are learnt and effectively mapped onto easily\ninterpretable parameters. A macroeconomic study of nowcasting quarterly U.S.\nGDP growth using a number of monthly economic variables demonstrates\nimprovements in terms of nowcast performance and interpretability compared to\nthe standard in the literature. The study further shows that incorporating\ninformation during a quarter markedly improves the performance in terms of both\npoint and density nowcasts.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 05:01:50 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 17:09:16 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 23:48:56 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["McAlinn", "Kenichiro", ""]]}, {"id": "1712.03769", "submitter": "Johannes Lutzeyer", "authors": "J. F. Lutzeyer and A. T. Walden", "title": "Comparing Graph Spectra of Adjacency and Laplacian Matrices", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-36683-4_16", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, graph structures are represented by one of three different\nmatrices: the adjacency matrix, the unnormalised and the normalised graph\nLaplacian matrices. The spectral (eigenvalue) properties of these different\nmatrices are compared. For each pair, the comparison is made by applying an\naffine transformation to one of them, which enables comparison whilst\npreserving certain key properties such as normalised eigengaps. Bounds are\ngiven on the eigenvalue differences thus found, which depend on the minimum and\nmaximum degree of the graph. The monotonicity of the bounds and the structure\nof the graphs are related. The bounds on a real social network graph, and on\nthree model graphs, are illustrated and analysed. The methodology is extended\nto provide bounds on normalised eigengap differences which again turn out to be\nin terms of the graph's degree extremes. It is found that if the degree extreme\ndifference is large, different choices of representation matrix may give rise\nto disparate inference drawn from graph signal processing algorithms; smaller\ndegree extreme differences result in consistent inference, whatever the choice\nof representation matrix. The different inference drawn from signal processing\nalgorithms is visualised using the spectral clustering algorithm on the three\nrepresentation matrices corresponding to a model graph and a real social\nnetwork graph.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 13:47:48 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Lutzeyer", "J. F.", ""], ["Walden", "A. T.", ""]]}, {"id": "1712.03817", "submitter": "Yi-Hui Zhou", "authors": "Yi-Hui Zhou", "title": "Set-based differential covariance testing for high-throughput data", "comments": "arXiv admin note: substantial text overlap with arXiv:1609.00736", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting changes in covariance for a single pair of features\nhas been studied in some detail, but may be limited in importance or general\napplicability. In contrast, testing equality of covariance matrices of a {\\it\nset} of features may offer increased power and interpretability. Such\napproaches have received increasing attention in recent years, especially in\nthe context of high-dimensional testing. These approaches have been limited to\nthe two-sample problem and involve varying assumptions on the number of\nfeatures $p$ vs. the sample size $n$. In addition, there has been little\ndiscussion of the motivating principles underlying various choices of\nstatistic, and no general approaches to test association of covariances with a\ncontinuous outcome. We propose a uniform framework to test association of\ncovariance matrices with an experimental variable, whether discrete or\ncontinuous. We describe four different summary statistics, to ensure power and\nflexibility under various settings, including a new \"connectivity\" statistic\nthat is sensitive to changes in overall covariance magnitude. The approach is\nnot limited by the data dimensions, and is applicable to situations where $p >>\nn$. For several statistics we obtain asymptotic $p$-values under relatively\nmild conditions. For the two-sample special case, we show that the proposed\nstatistics are permutationally equivalent or similar to existing proposed\nstatistics. We demonstrate the power and utility of our approaches via\nsimulation and analysis of real data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 22:06:58 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Zhou", "Yi-Hui", ""]]}, {"id": "1712.03889", "submitter": "Peter McCullagh", "authors": "Peter McCullagh, Nicholas Polson", "title": "Statistical sparsity", "comments": "21 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main contribution of this paper is a mathematical definition of\nstatistical sparsity, which is expressed as a limiting property of a sequence\nof probability distributions. The limit is characterized by an exceedance\nmeasure~$H$ and a rate parameter~$\\rho > 0$, both of which are unrelated to\nsample size. The definition is sufficient to encompass all sparsity models that\nhave been suggested in the signal-detection literature. Sparsity implies that\n$\\rho$~is small, and a sparse approximation is asymptotic in the rate\nparameter, typically with error $o(\\rho)$ in the sparse limit $\\rho \\to 0$. To\nfirst order in sparsity, the sparse signal plus Gaussian noise convolution\ndepends on the signal distribution only through its rate parameter and\nexceedance measure. This is one of several asymptotic approximations implied by\nthe definition, each of which is most conveniently expressed in terms of the\nzeta-transformation of the exceedance measure. One implication is that two\nsparse families having the same exceedance measure are inferentially\nequivalent, and cannot be distinguished to first order. A converse implication\nfor methodological strategy is that it may be more fruitful to focus on the\nexceedance measure, ignoring aspects of the signal distribution that have\nnegligible effect on observables and on inferences. From this point of view,\nscale models and inverse-power measures seem particularly attractive.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 17:04:39 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 14:12:34 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["McCullagh", "Peter", ""], ["Polson", "Nicholas", ""]]}, {"id": "1712.04088", "submitter": "Manoel Santos Neto", "authors": "Danillo Xavier and Manoel Santos-Neto and Marcelo Bourguignon and Vera\n  Tomazella", "title": "Zero-Modified Poisson-Lindley distribution with applications in\n  zero-inflated and zero-deflated count data", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main object of this article is to present an extension of the\nzero-inflated Poisson-Lindley distribution, called of zero-modified\nPoisson-Lindley. The additional parameter $\\pi$ of the zero-modified\nPoisson-Lindley has a natural interpretation in terms of either\nzero-deflated/inflated proportion. Inference is dealt with by using the\nlikelihood approach. In particular the maximum likelihood estimators of the\ndistribution's parameter are compared in small and large samples. We also\nconsider an alternative bias-correction mechanism based on Efron's bootstrap\nresampling. The model is applied to real data sets and found to perform better\nthan other competing models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 01:18:15 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 11:03:12 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Xavier", "Danillo", ""], ["Santos-Neto", "Manoel", ""], ["Bourguignon", "Marcelo", ""], ["Tomazella", "Vera", ""]]}, {"id": "1712.04221", "submitter": "Hiroki Mori", "authors": "Hiroki Mori and Keisuke Kawano and Hiroki Yokoyama", "title": "Causal Patterns: Extraction of multiple causal relationships by Mixture\n  of Probabilistic Partial Canonical Correlation Analysis", "comments": "DSAA2017 - The 4th IEEE International Conference on Data Science and\n  Advanced Analytics", "journal-ref": "Proceedings of the 4th IEEE International Conference on Data\n  Science and Advanced Analytics, pp.744-754, 2017", "doi": "10.1109/DSAA.2017.60", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a mixture of probabilistic partial canonical\ncorrelation analysis (MPPCCA) that extracts the Causal Patterns from two\nmultivariate time series. Causal patterns refer to the signal patterns within\ninteractions of two elements having multiple types of mutually causal\nrelationships, rather than a mixture of simultaneous correlations or the\nabsence of presence of a causal relationship between the elements. In\nmultivariate statistics, partial canonical correlation analysis (PCCA)\nevaluates the correlation between two multivariates after subtracting the\neffect of the third multivariate. PCCA can calculate the Granger Causal- ity\nIndex (which tests whether a time-series can be predicted from an- other\ntime-series), but is not applicable to data containing multiple partial\ncanonical correlations. After introducing the MPPCCA, we propose an\nexpectation-maxmization (EM) algorithm that estimates the parameters and latent\nvariables of the MPPCCA. The MPPCCA is expected to ex- tract multiple partial\ncanonical correlations from data series without any supervised signals to split\nthe data as clusters. The method was then eval- uated in synthetic data\nexperiments. In the synthetic dataset, our method estimated the multiple\npartial canonical correlations more accurately than the existing method. To\ndetermine the types of patterns detectable by the method, experiments were also\nconducted on real datasets. The method estimated the communication patterns In\nmotion-capture data. The MP- PCCA is applicable to various type of signals such\nas brain signals, human communication and nonlinear complex multibody systems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 10:46:27 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Mori", "Hiroki", ""], ["Kawano", "Keisuke", ""], ["Yokoyama", "Hiroki", ""]]}, {"id": "1712.04243", "submitter": "Enkelejd Hashorva", "authors": "Krzysztof Debicki and Enkelejd Hashorva", "title": "Approximation of Supremum of Max-Stable Stationary Processes and\n  Pickands Constants", "comments": "Accepted in J. Theoretical Probability", "journal-ref": null, "doi": "10.1007/s10959-018-00876-8", "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X(t),t\\in \\mathbb{R}$ be a stochastically continuous stationary\nmax-stable process with Fr\\'{e}chet marginals $\\Phi_\\alpha, \\alpha>0$ and set\n$M_X(T)=\\sup_{t \\in [0,T]} X(t),T>0$. In the light of the seminal articles\n[1,2], it follows that $A_T=M_X(T)/T^{1/\\alpha}$ converges in distribution as\n$T\\to \\infty$ to $\\mathcal{H}_Z^{1/\\alpha} X(1)$, where $\\mathcal{H}_Z$ is the\nPickands constant corresponding to the spectral process $Z$ of $X$. In this\ncontribution we derive explicit formulas for $\\mathcal{H}_Z$ in terms of $Z$\nand show necessary and sufficient conditions for its positivity. From our\nanalysis it follows that $A_T^\\beta,T>0$ is uniformly integrable for any $\\beta\n\\in (0,\\alpha)$. Further, we discuss the dissipative Rosi\\'nski (or mixed\nmoving maxima) representation of $X$. Additionally, for Brown-Resnick $X$ we\nshow the validity of the celebrated Slepian inequality and obtain lower bounds\non the growth of supremum of Gaussian processes with stationary increments by\nexploiting the link between Pickands constants and Wills functional. Moreover,\nwe derive upper bounds for supremum of centered Gaussian processes given in\nterms of Wills functional, and discuss the relation between Pickands and\nPiterbarg constants.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 11:27:09 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 09:54:09 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 09:05:03 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Debicki", "Krzysztof", ""], ["Hashorva", "Enkelejd", ""]]}, {"id": "1712.04290", "submitter": "Anirvan Chakraborty Mr.", "authors": "Anirvan Chakraborty and Victor M. Panaretos", "title": "Regression with genuinely functional errors-in-covariates", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contamination of covariates by measurement error is a classical problem in\nmultivariate regression, where it is well known that failing to account for\nthis contamination can result in substantial bias in the parameter estimators.\nThe nature and degree of this effect on statistical inference is also\nunderstood to crucially depend on the specific distributional properties of the\nmeasurement error in question. When dealing with functional covariates,\nmeasurement error has thus far been modelled as additive white noise over the\nobservation grid. Such a setting implicitly assumes that the error arises\npurely at the discrete sampling stage, otherwise the model can only be viewed\nin a weak (stochastic differential equation) sense, white noise not being a\nsecond-order process. Departing from this simple distributional setting can\nhave serious consequences for inference, similar to the multivariate case, and\ncurrent methodology will break down. In this paper, we consider the case when\nthe additive measurement error is allowed to be a valid stochastic process. We\npropose a novel estimator of the slope parameter in a functional linear model,\nfor scalar as well as functional responses, in the presence of this general\nmeasurement error specification. The proposed estimator is inspired by the\nmultivariate regression calibration approach, but hinges on recent advances on\nmatrix completion methods for functional data in order to handle the nontrivial\n(and unknown) error covariance structure. The asymptotic properties of the\nproposed estimators are derived. We probe the performance of the proposed\nestimator of slope using simulations and observe that it substantially improves\nupon the spectral truncation estimator based on the erroneous observations,\ni.e., ignoring measurement error. We also investigate the behaviour of the\nestimators on a real dataset on hip and knee angle curves during a gait cycle.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 13:51:45 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Chakraborty", "Anirvan", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "1712.04487", "submitter": "Steve Huntsman", "authors": "Steve Huntsman", "title": "Topological mixture estimation", "comments": "10 pages, 10 figures, accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.AT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density functions that represent sample data are often multimodal, i.e. they\nexhibit more than one maximum. Typically this behavior is taken to indicate\nthat the underlying data deserves a more detailed representation as a mixture\nof densities with individually simpler structure. The usual specification of a\ncomponent density is quite restrictive, with log-concave the most general case\nconsidered in the literature, and Gaussian the overwhelmingly typical case. It\nis also necessary to determine the number of mixture components \\emph{a\npriori}, and much art is devoted to this. Here, we introduce \\emph{topological\nmixture estimation}, a completely nonparametric and computationally efficient\nsolution to the one-dimensional problem where mixture components need only be\nunimodal. We repeatedly perturb the unimodal decomposition of Baryshnikov and\nGhrist to produce a topologically and information-theoretically optimal\nunimodal mixture. We also detail a smoothing process that optimally exploits\ntopological persistence of the unimodal category in a natural way when working\ndirectly with sample data. Finally, we illustrate these techniques through\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 19:51:23 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 19:16:47 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Huntsman", "Steve", ""]]}, {"id": "1712.04594", "submitter": "Michal Koles\\'ar", "authors": "Timothy B. Armstrong and Michal Koles\\'ar", "title": "Finite-Sample Optimal Estimation and Inference on Average Treatment\n  Effects Under Unconfoundedness", "comments": "45 pages, plus supplemental materials (11 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation and inference on average treatment effects under\nunconfoundedness conditional on the realizations of the treatment variable and\ncovariates. Given nonparametric smoothness and/or shape restrictions on the\nconditional mean of the outcome variable, we derive estimators and confidence\nintervals (CIs) that are optimal in finite samples when the regression errors\nare normal with known variance. In contrast to conventional CIs, our CIs use a\nlarger critical value that explicitly takes into account the potential bias of\nthe estimator. When the error distribution is unknown, feasible versions of our\nCIs are valid asymptotically, even when $\\sqrt{n}$-inference is not possible\ndue to lack of overlap, or low smoothness of the conditional mean. We also\nderive the minimum smoothness conditions on the conditional mean that are\nnecessary for $\\sqrt{n}$-inference. When the conditional mean is restricted to\nbe Lipschitz with a large enough bound on the Lipschitz constant, the optimal\nestimator reduces to a matching estimator with the number of matches set to\none. We illustrate our methods in an application to the National Supported Work\nDemonstration.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 02:57:02 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 15:47:17 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 16:38:30 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2020 16:16:48 GMT"}, {"version": "v5", "created": "Mon, 18 Jan 2021 15:59:32 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Armstrong", "Timothy B.", ""], ["Koles\u00e1r", "Michal", ""]]}, {"id": "1712.04723", "submitter": "Jialiang Mao", "authors": "Jialiang Mao, Yuhan Chen, Li Ma", "title": "Bayesian graphical compositional regression for microbiome data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in microbiome studies is to test the existence of and give\ncharacterization to differences in the microbiome composition across groups of\nsamples. Important challenges of this problem include the large within-group\nheterogeneities among samples and the existence of potential confounding\nvariables that, when ignored, increase the chance of false discoveries and\nreduce the power for identifying true differences. We propose a probabilistic\nframework to overcome these issues by combining three ideas: (i) a phylogenetic\ntree-based decomposition of the cross-group comparison problem into a series of\nlocal tests, (ii) a graphical model that links the local tests to allow\ninformation sharing across taxa, and (iii) a Bayesian testing strategy that\nincorporates covariates and integrates out the within-group variation, avoiding\npotentially unstable point estimates. We derive an efficient inference\nalgorithm based on numerical integration and junction-tree message passing,\nconduct extensive simulation studies to investigate the performance of our\napproach, and compare it to state-of-the-art methods in a number of\nrepresentative settings. We then apply our method to the American Gut data to\nanalyze the association of dietary habits and human's gut microbiome\ncomposition in the presence of covariates, and illustrate the importance of\nincorporating covariates in microbiome cross-group comparison.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 12:06:21 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 19:08:17 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 20:46:00 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Mao", "Jialiang", ""], ["Chen", "Yuhan", ""], ["Ma", "Li", ""]]}, {"id": "1712.04775", "submitter": "Clementine Barreyre", "authors": "Cl\\'ementine Barreyre, B\\'eatrice Laurent (IMT), Jean-Michel Loubes\n  (IMT), Bertrand Cabon, Lo\\\"ic Boussouf", "title": "Multiple testing for outlier detection in functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel procedure for outlier detection in functional data, in a\nsemi-supervised framework. As the data is functional, we consider the\ncoefficients obtained after projecting the observations onto orthonormal bases\n(wavelet, PCA). A multiple testing procedure based on the two-sample test is\ndefined in order to highlight the levels of the coefficients on which the\noutliers appear as significantly different to the normal data. The selected\ncoefficients are then called features for the outlier detection, on which we\ncompute the Local Outlier Factor to highlight the outliers. This procedure to\nselect the features is applied on simulated data that mimic the behaviour of\nspace telemetries, and compared with existing dimension reduction techniques.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 14:07:55 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Barreyre", "Cl\u00e9mentine", "", "IMT"], ["Laurent", "B\u00e9atrice", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"], ["Cabon", "Bertrand", ""], ["Boussouf", "Lo\u00efc", ""]]}, {"id": "1712.04801", "submitter": "Hoang Vuong", "authors": "Quan-Hoang Vuong", "title": "Open data, open review and open dialogue in making social sciences\n  plausible", "comments": "5 pages", "journal-ref": "Vuong QH. (2017). Open data, open review and open dialogue in\n  making social sciences plausible. Nature: Scientific Data Updates, 12\n  December 2017", "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, protecting trust in social sciences also means engaging in open\ncommunity dialogue, which helps to safeguard robustness and improve efficiency\nof research methods. The combination of open data, open review and open\ndialogue may sound simple but implementation in the real world will not be\nstraightforward. However, in view of Begley and Ellis's (2012) statement that,\n\"the scientific process demands the highest standards of quality, ethics and\nrigour,\" they are worth implementing. More importantly, they are feasible to\nwork on and likely will help to restore plausibility to social sciences\nresearch. Therefore, I feel it likely that the triplet of open data, open\nreview and open dialogue will gradually emerge to become policy requirements\nregardless of the research funding source.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 14:47:26 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Vuong", "Quan-Hoang", ""]]}, {"id": "1712.04845", "submitter": "Daniell Toth Ph.D.", "authors": "Daniell Toth", "title": "A Permutation Test on Complex Sample Data", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation tests are a distribution free way of performing hypothesis tests.\nThese tests rely on the condition that the observed data are exchangeable among\nthe groups being tested under the null hypothesis. This assumption is easily\nsatisfied for data obtained from a simple random sample or a controlled study\nafter simple adjustments to the data, but there is no general method for\nadjusting survey data collected using a complex sample design to allow for\npermutation tests. In this article, we propose a general method for performing\na pseudo- permutation test that accounts for the complex sample design. The\nproposed method is not a true permutation test in that the new values do not\ncome from the set of observed values in general, but of an expanded set of\nvalues satisfying a random-effects model on the clustered residuals. Tests\nusing a simulated population comparing the performance of the proposed method\nto permutation tests that ignore the sample design demonstrate that it is\nnecessary to account for certain design features in order to obtain reasonable\np-value estimates.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 16:27:22 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Toth", "Daniell", ""]]}, {"id": "1712.05014", "submitter": "Zhigen Zhao", "authors": "Sanat K. Sarkar and Zhigen Zhao", "title": "Local False Discovery Rate Based Methods for Multiple Testing of One-Way\n  Classified Hypotheses", "comments": "26 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper continues the line of research initiated in\n\\cite{Liu:Sarkar:Zhao:2016} on developing a novel framework for multiple\ntesting of hypotheses grouped in a one-way classified form using\nhypothesis-specific local false discovery rates (Lfdr's). It is built on an\nextension of the standard two-class mixture model from single to multiple\ngroups, defining hypothesis-specific Lfdr as a function of the conditional Lfdr\nfor the hypothesis given that it is within a significant group and the Lfdr for\nthe group itself and involving a new parameter that measures grouping effect.\nThis definition captures the underlying group structure for the hypotheses\nbelonging to a group more effectively than the standard two-class mixture\nmodel. Two new Lfdr based methods, possessing meaningful optimalities, are\nproduced in their oracle forms. One, designed to control false discoveries\nacross the entire collection of hypotheses, is proposed as a powerful\nalternative to simply pooling all the hypotheses into a single group and using\ncommonly used Lfdr based method under the standard single-group two-class\nmixture model. The other is proposed as an Lfdr analog of the method of\n\\cite{Benjamini:Bogomolov:2014} for selective inference. It controls Lfdr based\nmeasure of false discoveries associated with selecting groups concurrently with\ncontrolling the average of within-group false discovery proportions across the\nselected groups. Simulation studies and real-data application show that our\nproposed methods are often more powerful than their relevant competitors.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 21:32:28 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 19:31:40 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Sarkar", "Sanat K.", ""], ["Zhao", "Zhigen", ""]]}, {"id": "1712.05072", "submitter": "Jun Li", "authors": "Jun Li", "title": "Nonparametric Adaptive CUSUM Chart for Detecting Arbitrary\n  Distributional Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric control charts that can detect arbitrary distributional changes\nare highly desirable due to their flexibility to adapt to different\ndistributional assumptions and distributional changes. However, most of such\ncontrol charts in the literature either involve some tuning parameter, which\nneeds to be pre-specified, or involve intensive computation. In this paper, we\npropose a new nonparametric adaptive CUSUM chart for detecting arbitrary\ndistributional changes. The proposed control chart does not depend on any\ntuning parameter and is efficient in computation. Its self-starting nature\nmakes the proposed control chart applicable to situations where no sufficiently\nlarge reference data are available. Our proposed control chart also has a\nbuilt-in post-signal diagnostics function that can identify what kind of\ndistributional changes have occurred after an alarm. Our simulation study and\nreal data analysis show that the proposed control chart performs well across a\nbroad range of settings, and compares favorably with existing nonparametric\ncontrol charts.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 02:11:14 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Li", "Jun", ""]]}, {"id": "1712.05074", "submitter": "Jun Li", "authors": "Jun Li", "title": "A Two-stage Online Monitoring Procedure for High-Dimensional Data\n  Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced computing and data acquisition technologies have made possible the\ncollection of high-dimensional data streams in many fields. Efficient online\nmonitoring tools which can correctly identify any abnormal data stream for such\ndata are highly sought after. However, most of the existing monitoring\nprocedures directly apply the false discover rate (FDR) controlling procedure\nto the data at each time point, and the FDR at each time point (the point-wise\nFDR) is either specified by users or determined by the in-control (IC) average\nrun length (ARL). If the point-wise FDR is specified by users, the resulting\nprocedure lacks control of the global FDR and keeps users in the dark in terms\nof the IC-ARL. If the point-wise FDR is determined by the IC-ARL, the resulting\nprocedure does not give users the flexibility to choose the number of false\nalarms (Type-I errors) they can tolerate when identifying abnormal data\nstreams, which often makes the procedure too conservative. To address those\nlimitations, we propose a two-stage monitoring procedure that can control both\nthe IC-ARL and Type-I errors at the levels specified by users. As a result, the\nproposed procedure allows users to choose not only how often they expect any\nfalse alarms when all data streams are IC, but also how many false alarms they\ncan tolerate when identifying abnormal data streams. With this extra\nflexibility, our proposed two-stage monitoring procedure is shown in the\nsimulation study and real data analysis to outperform the exiting methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 02:19:01 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Li", "Jun", ""]]}, {"id": "1712.05151", "submitter": "Peter Rousseeuw", "authors": "Jakob Raymaekers and Peter J. Rousseeuw", "title": "Fast robust correlation for high-dimensional data", "comments": null, "journal-ref": "Technometrics, vol. 63, 184-198 (2021)", "doi": "10.1080/00401706.2019.1677270", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The product moment covariance is a cornerstone of multivariate data analysis,\nfrom which one can derive correlations, principal components, Mahalanobis\ndistances and many other results. Unfortunately the product moment covariance\nand the corresponding Pearson correlation are very susceptible to outliers\n(anomalies) in the data. Several robust measures of covariance have been\ndeveloped, but few are suitable for the ultrahigh dimensional data that are\nbecoming more prevalent nowadays. For that one needs methods whose computation\nscales well with the dimension, are guaranteed to yield a positive semidefinite\ncovariance matrix, and are sufficiently robust to outliers as well as\nsufficiently accurate in the statistical sense of low variability. We construct\nsuch methods using data transformations. The resulting approach is simple, fast\nand widely applicable. We study its robustness by deriving influence functions\nand breakdown values, and computing the mean squared error on contaminated\ndata. Using these results we select a method that performs well overall. This\nalso allows us to construct a faster version of the DetectDeviatingCells method\n(Rousseeuw and Van den Bossche, 2018) to detect cellwise outliers, that can\ndeal with much higher dimensions. The approach is illustrated on genomic data\nwith 12,000 variables and color video data with 920,000 dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 10:23:47 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 13:41:29 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 05:21:53 GMT"}, {"version": "v4", "created": "Sun, 20 Oct 2019 21:49:54 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Raymaekers", "Jakob", ""], ["Rousseeuw", "Peter J.", ""]]}, {"id": "1712.05229", "submitter": "Federica Nicolussi", "authors": "Federica Nicolussi and Manuela Cazzaro", "title": "Context-specific independencies for ordinal variables in chain\n  regression models", "comments": "21 pages, 4 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we handle with categorical (ordinal) variables and we focus on\nthe (in)dependence relationship under the marginal, conditional and\ncontext-specific perspective. If the first two are well known, the last one\nconcerns independencies holding only in a subspace of the outcome space. We\ntake advantage from the Hierarchical Multinomial Marginal models and provide\nseveral original results about the representation of context-specific\nindependencies through these models. By considering the graphical aspect, we\ntake advantage from the chain graphical models. The resultant graphical model\nis a so-called \"stratified\" chain graphical model with labelled arcs. New\nMarkov properties are provided. Furthermore, we consider the graphical models\nunder the regression poit of view. Here we provide simplification of the\nregression parameters due to the context-specific independencies. Finally, an\napplication about the innovation degree of the Italian enterprises is provided.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 14:04:50 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 15:29:14 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Nicolussi", "Federica", ""], ["Cazzaro", "Manuela", ""]]}, {"id": "1712.05358", "submitter": "Lampros Bouranis", "authors": "Lampros Bouranis, Nial Friel, Florian Maire", "title": "Model comparison for Gibbs random fields using noisy reversible jump\n  Markov chain Monte Carlo", "comments": "Accepted for publication in Computational Statistics and Data\n  Analysis", "journal-ref": "Computational Statistics and Data Analysis 128 (2018) 221-241", "doi": "10.1016/j.csda.2018.07.005", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reversible jump Markov chain Monte Carlo (RJMCMC) method offers an\nacross-model simulation approach for Bayesian estimation and model comparison,\nby exploring the sampling space that consists of several models of possibly\nvarying dimensions. A naive implementation of RJMCMC to models like Gibbs\nrandom fields suffers from computational difficulties: the posterior\ndistribution for each model is termed doubly-intractable since computation of\nthe likelihood function is rarely available. Consequently, it is simply\nimpossible to simulate a transition of the Markov chain in the presence of\nlikelihood intractability. A variant of RJMCMC is presented, called noisy\nRJMCMC, where the underlying transition kernel is replaced with an\napproximation based on unbiased estimators. Based on previous theoretical\ndevelopments, convergence guarantees for the noisy RJMCMC algorithm are\nprovided. The experiments show that the noisy RJMCMC algorithm can be much more\nefficient than other exact methods, provided that an estimator with controlled\nMonte Carlo variance is used, a fact which is in agreement with the theoretical\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 17:37:52 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 14:20:51 GMT"}, {"version": "v3", "created": "Fri, 13 Jul 2018 22:51:33 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Bouranis", "Lampros", ""], ["Friel", "Nial", ""], ["Maire", "Florian", ""]]}, {"id": "1712.05445", "submitter": "Igor Cialenco", "authors": "Igor Cialenco", "title": "Statistical Inference for SPDEs: an overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to give an overview of the recent developments in the\narea of statistical inference for parabolic stochastic partial differential\nequations. Significant part of the paper is devoted to the spectral approach,\nwhich is the most studied sampling scheme under which the observations are done\nin the Fourier space over some finite time interval. We also discuss into\ndetails the practically important case of discrete sampling of the solution.\nOther relevant methodologies and some open problems are briefly discussed over\nthe course of the manuscript.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 20:39:26 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Cialenco", "Igor", ""]]}, {"id": "1712.05527", "submitter": "Gery Geenens", "authors": "Gery Geenens and Richard Dunn", "title": "A nonparametric copula approach to conditional Value-at-Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value-at-Risk and its conditional allegory, which takes into account the\navailable information about the economic environment, form the centrepiece of\nthe Basel framework for the evaluation of market risk in the banking sector. In\nthis paper, a new nonparametric framework for estimating this conditional\nValue-at-Risk is presented. A nonparametric approach is particularly pertinent\nas the traditionally used parametric distributions have been shown to be\ninsufficiently robust and flexible in most of the equity-return data sets\nobserved in practice. The method extracts the quantile of the conditional\ndistribution of interest, whose estimation is based on a novel estimator of the\ndensity of the copula describing the dynamic dependence observed in the series\nof returns. Real-world back-testing analyses demonstrate the potential of the\napproach, whose performance may be superior to its industry counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 04:27:25 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 03:20:40 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Geenens", "Gery", ""], ["Dunn", "Richard", ""]]}, {"id": "1712.05547", "submitter": "S\\\"oren Christensen", "authors": "Sebastian Jobj\\\"ornsson, S\\\"oren Christensen", "title": "Anscombe's Model for Sequential Clinical Trials Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Anscombe's classical model, the objective is to find the optimal\nsequential rule for learning about the difference between two alternative\ntreatments and subsequently selecting the superior one. The population for\nwhich the procedure is optimised has size $N$ and includes both the patients in\nthe trial and those which are treated with the chosen alternative after the\ntrial. We review earlier work on this problem and give a detailed treatment of\nthe problem itself. In particular, while previous work has mainly focused on\nthe case of conjugate normal priors for the incremental effect, we demonstrate\nhow to handle the problem for priors of a general form. We also discuss methods\nfor numerical solutions and the practical implications of the results for the\nregulation of clinical trials.\n  Two extensions of the model are proposed and analysed. The first breaks the\nsymmetry of the treatments, giving one the role of the current standard being\nadministered in parallel with the trial. We show how certain asymptotic results\ndue to Chernoff can be adapted to this asymmetric case. The other extension\nassumes that $N$ is a random variable instead of a known constant.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 06:02:59 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Jobj\u00f6rnsson", "Sebastian", ""], ["Christensen", "S\u00f6ren", ""]]}, {"id": "1712.05630", "submitter": "Richard Samworth", "authors": "Milana Gataric, Tengyao Wang and Richard J. Samworth", "title": "Sparse principal component analysis via axis-aligned random projections", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for sparse principal component analysis, based on\nthe aggregation of eigenvector information from carefully-selected axis-aligned\nrandom projections of the sample covariance matrix. Unlike most alternative\napproaches, our algorithm is non-iterative, so is not vulnerable to a bad\nchoice of initialisation. We provide theoretical guarantees under which our\nprincipal subspace estimator can attain the minimax optimal rate of convergence\nin polynomial time. In addition, our theory provides a more refined\nunderstanding of the statistical and computational trade-off in the problem of\nsparse principal component estimation, revealing a subtle interplay between the\neffective sample size and the number of random projections that are required to\nachieve the minimax optimal rate. Numerical studies provide further insight\ninto the procedure and confirm its highly competitive finite-sample\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 11:55:39 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 17:35:54 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 10:12:09 GMT"}, {"version": "v4", "created": "Mon, 6 May 2019 16:18:07 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Gataric", "Milana", ""], ["Wang", "Tengyao", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1712.05708", "submitter": "Daniell Toth Ph.D.", "authors": "Kelly S. McConville and Daniell Toth", "title": "Automated Selection of Post-Strata using a Model-Assisted Regression\n  Tree Estimator", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auxiliary information can increase the efficiency of survey estimators\nthrough an assisting model when the model captures some of the relationship\nbetween the auxiliary data and the study variables. Despite their superior\nproperties, model-assisted estimators are rarely used in anything but their\nsimplest form by statistical agencies to produce official statistics. This is\ndue to the fact that the more complicated models that have been used in\nmodel-assisted estimation are often ill suited to the available auxiliary data.\nUnder a model-assisted framework, we propose a regression tree estimator for a\nfinite population total. Regression tree models are adept at handling the type\nof auxiliary data usually available in the sampling frame and provide a model\nthat is easy to explain and justify. The estimator can be viewed as a\npost-stratification estimator where the post-strata are automatically selected\nby the recursive partitioning algorithm of the regression tree. We establish\nconsistency of the regression tree estimator and compare its performance to\nother survey estimators using the US Bureau of Labor Statistics Occupational\nEmployment Statistics Survey.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 15:21:34 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["McConville", "Kelly S.", ""], ["Toth", "Daniell", ""]]}, {"id": "1712.05717", "submitter": "Tim Sullivan", "authors": "H. C. Lie and T. J. Sullivan and A. L. Teckentrup", "title": "Random forward models and log-likelihoods in Bayesian inverse problems", "comments": "25 pages", "journal-ref": "ASA/SIAM Journal of Uncertainty Quantification (2018)", "doi": "10.1137/18M1166523", "report-no": null, "categories": "math.ST cs.NA math.NA math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the use of randomised forward models and log-likelihoods within\nthe Bayesian approach to inverse problems. Such random approximations to the\nexact forward model or log-likelihood arise naturally when a computationally\nexpensive model is approximated using a cheaper stochastic surrogate, as in\nGaussian process emulation (kriging), or in the field of probabilistic\nnumerical methods. We show that the Hellinger distance between the exact and\napproximate Bayesian posteriors is bounded by moments of the difference between\nthe true and approximate log-likelihoods. Example applications of these\nstability results are given for randomised misfit models in large data\napplications and the probabilistic solution of ordinary differential equations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 15:39:26 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 10:08:59 GMT"}, {"version": "v3", "created": "Wed, 31 Jan 2018 14:16:50 GMT"}, {"version": "v4", "created": "Wed, 27 Jun 2018 10:53:11 GMT"}, {"version": "v5", "created": "Fri, 28 Sep 2018 09:02:26 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Lie", "H. C.", ""], ["Sullivan", "T. J.", ""], ["Teckentrup", "A. L.", ""]]}, {"id": "1712.05786", "submitter": "Alex Gibberd Dr", "authors": "Alex J. Gibberd and Sandipan Roy", "title": "Multiple Changepoint Estimation in High-Dimensional Gaussian Graphical\n  Models", "comments": "39 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the consistency properties of a regularised estimator for the\nsimultaneous identification of both changepoints and graphical dependency\nstructure in multivariate time-series. Traditionally, estimation of Gaussian\nGraphical Models (GGM) is performed in an i.i.d setting. More recently, such\nmodels have been extended to allow for changes in the distribution, but only\nwhere changepoints are known a-priori. In this work, we study the Group-Fused\nGraphical Lasso (GFGL) which penalises partial-correlations with an L1 penalty\nwhile simultaneously inducing block-wise smoothness over time to detect\nmultiple changepoints. We present a proof of consistency for the estimator,\nboth in terms of changepoints, and the structure of the graphical models in\neach segment.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 18:43:39 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Gibberd", "Alex J.", ""], ["Roy", "Sandipan", ""]]}, {"id": "1712.05835", "submitter": "Alexander Luedtke", "authors": "Alex Luedtke and Jiacheng Wu", "title": "Efficient Principally Stratified Treatment Effect Estimation in\n  Crossover Studies with Absorbent Binary Endpoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose one wishes to estimate the effect of a binary treatment on a binary\nendpoint conditional on a post-randomization quantity in a counterfactual world\nin which all subjects received treatment. It is generally difficult to identify\nthis parameter without strong, untestable assumptions. It has been shown that\nidentifiability assumptions become much weaker under a crossover design in\nwhich subjects not receiving treatment are later given treatment. Under the\nassumption that the post-treatment biomarker observed in these crossover\nsubjects is the same as would have been observed had they received treatment at\nthe start of the study, one can identify the treatment effect with only mild\nadditional assumptions. This remains true if the endpoint is absorbent, i.e. an\nendpoint such as death or HIV infection such that the post-crossover treatment\nbiomarker is not meaningful if the endpoint has already occurred. In this work,\nwe review identifiability results for a parameter of the distribution of the\ndata observed under a crossover design with the principally stratified\ntreatment effect of interest. We describe situations in which these assumptions\nwould be falsifiable, and show that these assumptions are not otherwise\nfalsifiable. We then provide a targeted minimum loss-based estimator for the\nsetting that makes no assumptions on the distribution that generated the data.\nWhen the semiparametric efficiency bound is well defined, for which the primary\ncondition is that the biomarker is discrete-valued, this estimator is efficient\namong all regular and asymptotically linear estimators. We also present a\nversion of this estimator for situations in which the biomarker is continuous.\nImplications to closeout designs for vaccine trials are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 21:02:43 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2019 22:28:46 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Luedtke", "Alex", ""], ["Wu", "Jiacheng", ""]]}, {"id": "1712.05845", "submitter": "Nicole Barthel", "authors": "Nicole Barthel, Candida Geerdens, Claudia Czado and Paul Janssen", "title": "Dependence modeling for recurrent event times subject to right-censoring\n  with D-vine copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many time-to-event studies, the event of interest is recurrent. Here, the\ndata for each sample unit corresponds to a series of gap times between the\nsubsequent events. Given a limited follow-up period, the last gap time might be\nright-censored. In contrast to classical analysis, gap times and censoring\ntimes cannot be assumed independent, i.e. the sequential nature of the data\ninduces dependent censoring. Also, the recurrences typically vary between\nsample units leading to unbalanced data. To model the association pattern\nbetween gap times, so far only parametric margins combined with the restrictive\nclass of Archimedean copulas have been considered. Here, taking the specific\ndata features into account, we extend existing work in several directions: we\nallow for nonparametric margins and consider the flexible class of D-vine\ncopulas. A global and sequential (one- and two-stage) likelihood approach are\nsuggested. We discuss the computational efficiency of each estimation strategy.\nExtensive simulations show good finite sample performance of the proposed\nmethodology. It is used to analyze the association in recurrent asthma attacks\nin children. The analysis reveals that a D-vine copula detects relevant\ninsights, on how dependence changes in strength and type over time.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 21:29:34 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 19:43:08 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Barthel", "Nicole", ""], ["Geerdens", "Candida", ""], ["Czado", "Claudia", ""], ["Janssen", "Paul", ""]]}, {"id": "1712.05848", "submitter": "Jun Li", "authors": "Jun Li", "title": "Efficient Global Monitoring Statistics for High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global monitoring statistics play an important role for developing efficient\nmonitoring schemes for high-dimensional data streams. A number of global\nmonitoring statistics have been proposed in the literature. However, most of\nthem only work for certain types of abnormal scenarios under specific model\nassumptions. How to develop global monitoring statistics that are powerful for\nany abnormal scenarios under flexible model assumptions is a long-standing\nproblem in the statistical process monitoring field. To provide a potential\nsolution to this problem, we propose a novel class of global monitoring\nstatistics by making use of the quantile information in the underlying\ndistribution of the local monitoring statistic. Our proposed global monitoring\nstatistics are easy to calculate and can work under flexible model assumptions\nsince they can be built on any local monitoring statistic that is suitable for\nmonitoring a single data stream. Our simulation studies show that the proposed\nglobal monitoring statistics perform well across a broad range of settings, and\ncompare favorably with existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 21:36:22 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Li", "Jun", ""]]}, {"id": "1712.05907", "submitter": "Erin Conlon", "authors": "Zheng Wei and Erin M. Conlon", "title": "Parallel Markov Chain Monte Carlo for Bayesian Hierarchical Models with\n  Big Data, in Two Stages", "comments": "30 pages, 2 figures. New simulation example for logistic regression.\n  MCMC efficiency measure added. Details of convergence diagnostics added. One\n  additional table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the escalating growth of big data sets in recent years, new Bayesian\nMarkov chain Monte Carlo (MCMC) parallel computing methods have been developed.\nThese methods partition large data sets by observations into subsets. However,\nfor Bayesian nested hierarchical models, typically only a few parameters are\ncommon for the full data set, with most parameters being group-specific. Thus,\nparallel Bayesian MCMC methods that take into account the structure of the\nmodel and split the full data set by groups rather than by observations are a\nmore natural approach for analysis. Here, we adapt and extend a recently\nintroduced two-stage Bayesian hierarchical modeling approach, and we partition\ncomplete data sets by groups. In stage 1, the group-specific parameters are\nestimated independently in parallel. The stage 1 posteriors are used as\nproposal distributions in stage 2, where the target distribution is the full\nmodel. Using three-level and four-level models, we show in both simulation and\nreal data studies that results of our method agree closely with the full data\nanalysis, with greatly increased MCMC efficiency and greatly reduced\ncomputation times. The advantages of our method versus existing parallel MCMC\ncomputing methods are also described.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 06:14:18 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 22:07:54 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Wei", "Zheng", ""], ["Conlon", "Erin M.", ""]]}, {"id": "1712.06120", "submitter": "Sivaraman Balakrishnan", "authors": "Sivaraman Balakrishnan and Larry Wasserman", "title": "Hypothesis Testing for High-Dimensional Multinomials: A Selective Review", "comments": "19 pages, 6 figures. Written in memory of Stephen E. Fienberg", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis of discrete data has been the subject of extensive\nstatistical research dating back to the work of Pearson. In this survey we\nreview some recently developed methods for testing hypotheses about\nhigh-dimensional multinomials. Traditional tests like the $\\chi^2$ test and the\nlikelihood ratio test can have poor power in the high-dimensional setting. Much\nof the research in this area has focused on finding tests with asymptotically\nNormal limits and developing (stringent) conditions under which tests have\nNormal limits. We argue that this perspective suffers from a significant\ndeficiency: it can exclude many high-dimensional cases when - despite having\nnon Normal null distributions - carefully designed tests can have high power.\nFinally, we illustrate that taking a minimax perspective and considering\nrefinements of this perspective can lead naturally to powerful and practical\ntests.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 14:21:46 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "1712.06170", "submitter": "Fan Li", "authors": "Peng Ding and Fan Li", "title": "Causal Inference: A Missing Data Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring causal effects of treatments is a central goal in many disciplines.\nThe potential outcomes framework is a main statistical approach to causal\ninference, in which a causal effect is defined as a comparison of the potential\noutcomes of the same units under different treatment conditions. Because for\neach unit at most one of the potential outcomes is observed and the rest are\nmissing, causal inference is inherently a missing data problem. Indeed, there\nis a close analogy in the terminology and the inferential framework between\ncausal inference and missing data. Despite the intrinsic connection between the\ntwo subjects, statistical analyses of causal inference and missing data also\nhave marked differences in aims, settings and methods. This article provides a\nsystematic review of causal inference from the missing data perspective.\nFocusing on ignorable treatment assignment mechanisms, we discuss a wide range\nof causal inference methods that have analogues in missing data analysis, such\nas imputation, inverse probability weighting and doubly-robust methods. Under\neach of the three modes of inference--Frequentist, Bayesian, and Fisherian\nrandomization--we present the general structure of inference for both\nfinite-sample and super-population estimands, and illustrate via specific\nexamples. We identify open questions to motivate more research to bridge the\ntwo fields.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 20:02:20 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 17:59:57 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Ding", "Peng", ""], ["Li", "Fan", ""]]}, {"id": "1712.06201", "submitter": "Krzysztof Latuszynski", "authors": "Paul Fearnhead, Krzystof Latuszynski, Gareth O. Roberts, Giorgos\n  Sermaidis", "title": "Continious-time Importance Sampling: Monte Carlo Methods which Avoid\n  Time-discretisation Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a continuous-time sequential importance sampling\n(CIS) algorithm which eliminates time-discretisation errors and provides online\nunbiased estimation for continuous time Markov processes, in particular for\ndiffusions. Our work removes the strong conditions imposed by the EA and thus\nextends significantly the class of discretisation error-free MC methods for\ndiffusions. The reason that CIS can be applied more generally than EA is that\nit no longer works on the path space of the SDE. Instead it uses proposal\ndistributions for the transition density of the diffusion, and proposal\ndistributions that are absolutely continuous with respect to the true\ntransition density exist for general SDEs.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 22:55:50 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Fearnhead", "Paul", ""], ["Latuszynski", "Krzystof", ""], ["Roberts", "Gareth O.", ""], ["Sermaidis", "Giorgos", ""]]}, {"id": "1712.06230", "submitter": "Maryclare Griffin", "authors": "Maryclare Griffin and Peter D. Hoff", "title": "Testing Sparsity-Inducing Penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many penalized maximum likelihood estimators correspond to posterior mode\nestimators under specific prior distributions. Appropriateness of a particular\nclass of penalty functions can therefore be interpreted as the appropriateness\nof a prior for the parameters. For example, the appropriateness of a lasso\npenalty for regression coefficients depends on the extent to which the\nempirical distribution of the regression coefficients resembles a Laplace\ndistribution. We give a testing procedure of whether or not a Laplace prior is\nappropriate and accordingly, whether or not using a lasso penalized estimate is\nappropriate. This testing procedure is designed to have power against\nexponential power priors which correspond to $\\ell_q$ penalties. Via\nsimulations, we show that this testing procedure achieves the desired level and\nhas enough power to detect violations of the Laplace assumption when the\nnumbers of observations and unknown regression coefficients are large. We then\nintroduce an adaptive procedure that chooses a more appropriate prior and\ncorresponding penalty from the class of exponential power priors when the null\nhypothesis is rejected. We show that this can improve estimation of the\nregression coefficients both when they are drawn from an exponential power\ndistribution and when they are drawn from a spike-and-slab distribution.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 02:45:59 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 23:22:07 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Griffin", "Maryclare", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1712.06457", "submitter": "Andrea Saltelli", "authors": "Andrea Saltelli", "title": "Discussion Paper: Should statistics rescue mathematical modelling?", "comments": "Inserted a new Figure on the O'Neill conjecture and a few new\n  references e.g. to the discussion about retiring significance in statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics experiences a storm around the perceived misuse and possible abuse\nof its methods in the context of the so-called reproducibility crisis. The\nmethods and styles of quantification practiced in mathematical modelling rarely\nmake it to the headlines, though modelling practitioners writing in\ndisciplinary journals flag a host of problems in the field. Technical, cultural\nand ethical dimensions are simultaneously at play in the current predicaments\nof both statistics and mathematical modelling. Since mathematical modelling is\nnot a discipline like statistics, its shortcomings risk remaining untreated\nlonger. We suggest that the tools of statistics and its disciplinary\norganisation might offer a remedial contribution to mathematical modelling,\nstandardising methodologies and disseminating good practices. Statistics could\nprovide scientists and engineers from all disciplines with a point of anchorage\nfor sound modelling work. This is a vast and long-term undertaking. A step in\nthe proposed direction is offered here by focusing on the use of statistical\ntools for quality assurance of mathematical models. By way of illustration,\ntechniques for uncertainty quantification, sensitivity analysis and sensitivity\nauditing are suggested for incorporation in statistical syllabuses and\npractices.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 15:11:48 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 17:07:10 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 09:07:36 GMT"}, {"version": "v4", "created": "Sat, 17 Aug 2019 08:59:59 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Saltelli", "Andrea", ""]]}, {"id": "1712.06465", "submitter": "Asaf Weinstein", "authors": "Asaf Weinstein, Rina Barber, Emmanuel Candes", "title": "A Power and Prediction Analysis for Knockoffs with Lasso Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knockoffs is a new framework for controlling the false discovery rate (FDR)\nin multiple hypothesis testing problems involving complex statistical models.\nWhile there has been great emphasis on Type-I error control, Type-II errors\nhave been far less studied. In this paper we analyze the false negative rate\nor, equivalently, the power of a knockoff procedure associated with the Lasso\nsolution path under an i.i.d. Gaussian design, and find that knockoffs\nasymptotically achieve close to optimal power with respect to an omniscient\noracle. Furthermore, we demonstrate that for sparse signals, performing model\nselection via knockoff filtering achieves nearly ideal prediction errors as\ncompared to a Lasso oracle equipped with full knowledge of the distribution of\nthe unknown regression coefficients. The i.i.d. Gaussian design is adopted to\nleverage results concerning the empirical distribution of the Lasso estimates,\nwhich makes power calculation possible for both knockoff and oracle procedures.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 15:21:28 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Weinstein", "Asaf", ""], ["Barber", "Rina", ""], ["Candes", "Emmanuel", ""]]}, {"id": "1712.06532", "submitter": "Bj\\\"orn B\\\"ottcher", "authors": "Bj\\\"orn B\\\"ottcher", "title": "Dependence and dependence structures: estimation and visualization using\n  the unifying concept of distance multivariance", "comments": "restructured; several new results", "journal-ref": "Open Statistics, Vol. 1, No. 1 (2020)", "doi": "10.1515/stat-2020-0001", "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance multivariance is a multivariate dependence measure, which can detect\ndependencies between an arbitrary number of random vectors each of which can\nhave a distinct dimension. Here we discuss several new aspects, present a\nconcise overview and use it as the basis for several new results and concepts:\nIn particular, we show that distance multivariance unifies (and extends)\ndistance covariance and the Hilbert-Schmidt independence criterion HSIC,\nmoreover also the classical linear dependence measures: covariance, Pearson's\ncorrelation and the RV coefficient appear as limiting cases. Based on distance\nmultivariance several new measures are defined: a multicorrelation which\nsatisfies a natural set of multivariate dependence measure axioms and\n$m$-multivariance which is a dependence measure yielding tests for pairwise\nindependence and independence of higher order. These tests are computationally\nfeasible and under very mild moment conditions they are consistent against all\nalternatives. Moreover, a general visualization scheme for higher order\ndependencies is proposed, including consistent estimators (based on distance\nmultivariance) for the dependence structure.\n  Many illustrative examples are provided. All functions for the use of\ndistance multivariance in applications are published in the R-package\n'multivariance'.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 17:17:54 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 11:53:11 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 19:59:49 GMT"}, {"version": "v4", "created": "Tue, 6 Aug 2019 12:24:56 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["B\u00f6ttcher", "Bj\u00f6rn", ""]]}, {"id": "1712.06643", "submitter": "Arjun Sondhi", "authors": "Arjun Sondhi, Kenneth Martin Rice", "title": "Fast permutation tests and related methods, for association between rare\n  variants and binary outcomes", "comments": null, "journal-ref": "Sondhi A, Rice KM. Fast permutation tests and related methods, for\n  association between rare variants and binary outcomes. Ann Hum Genet.\n  2017;00:1-9", "doi": "10.1111/ahg.12229", "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large scale genetic association studies, a primary aim is to test for\nassociation between genetic variants and a disease outcome. The variants of\ninterest are often rare, and appear with low frequency among subjects. In this\nsituation, statistical tests based on standard asymptotic results do not\nadequately control the Type I error rate, especially if the case:control ratio\nis unbalanced. In this paper, we propose the use of permutation and approximate\nunconditional tests for testing association with rare variants. We use novel\nanalytical calculations to efficiently approximate the true Type I error rate\nunder common study designs, and in numerical studies show that the proposed\nclasses of tests significantly improve upon standard testing methods. We also\nillustrate our methods in data from a recent case-control study, for genetic\ncauses of a severe side-effect of a common drug treatment.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 19:41:22 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Sondhi", "Arjun", ""], ["Rice", "Kenneth Martin", ""]]}, {"id": "1712.06718", "submitter": "Ruitao Lin", "authors": "Haitao Pan, Ruitao Lin, and Ying Yuan", "title": "Statistical Properties of the Keyboard Design with Extension to\n  Drug-Combination Trials", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The keyboard design is a novel phase I dose-finding method that is simple and\nhas good operating characteristics. This paper studies theoretical properties\nof the keyboard design, including the optimality of its decision rules,\ncoherence in dose transition, and convergence to the target dose. Establishing\nthese theoretical properties explains the mechanism of the design and provides\nassurance to practitioners regarding the behavior of the keyboard design. We\nfurther extend the keyboard design to dual-agent dose-finding trials, which\ninherit the same statistical properties and simplicity as the single-agent\nkeyboard design. Extensive simulations are conducted to evaluate the\nperformance of the proposed keyboard drug-combination design using a novel,\nrandom two-dimensional dose--toxicity scenario generating algorithm. The\nsimulation results confirm the desirable and competitive operating\ncharacteristics of the keyboard design as established by the theoretical study.\nAn R Shiny application is developed to facilitate implementing the keyboard\ncombination design in practice.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 23:23:58 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Pan", "Haitao", ""], ["Lin", "Ruitao", ""], ["Yuan", "Ying", ""]]}, {"id": "1712.06743", "submitter": "Arkaprava Roy", "authors": "Arkaprava Roy, Subhashis Ghosal and Kingshuk Roy Choudhury", "title": "High-dimensional single-index Bayesian modeling of brain atrophy", "comments": null, "journal-ref": "Bayesian Analysis (2019)", "doi": "10.1214/19-BA1186", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model of brain atrophy as a function of high-dimensional genetic\ninformation and low dimensional covariates such as gender, age, APOE gene, and\ndisease status. A nonparametric single-index Bayesian model of high dimension\nis proposed to model the relationship with B-spline series prior on the unknown\nfunctions and Dirichlet process scale mixture of centered normal prior on the\ndistributions of the random effects. The posterior rate of contraction without\nthe random effect is established for a fixed number of regions and time points\nwith increasing sample size. We implement an efficient computation algorithm\nthrough a Hamiltonian Monte Carlo (HMC) algorithm. The performance of the\nproposed Bayesian method is compared with the corresponding least square\nestimator in the linear model with horseshoe prior, LASSO and SCAD penalization\non the high-dimensional covariates. The proposed Bayesian method is applied to\na dataset on volumes of brain regions recorded over multiple visits of 748\nindividuals using 620,901 SNPs and 6 other covariates for each individual, to\nidentify factors associated with brain atrophy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 01:33:34 GMT"}, {"version": "v2", "created": "Sat, 30 Dec 2017 23:36:37 GMT"}, {"version": "v3", "created": "Fri, 5 Jan 2018 01:03:08 GMT"}, {"version": "v4", "created": "Thu, 13 Sep 2018 20:42:57 GMT"}, {"version": "v5", "created": "Mon, 11 Feb 2019 18:58:07 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Roy", "Arkaprava", ""], ["Ghosal", "Subhashis", ""], ["Choudhury", "Kingshuk Roy", ""]]}, {"id": "1712.06748", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen, Xiaoou Li, Siliang Zhang", "title": "Joint Maximum Likelihood Estimation for High-dimensional Exploratory\n  Item Response Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint maximum likelihood (JML) estimation is one of the earliest approaches\nto fitting item response theory (IRT) models. This procedure treats both the\nitem and person parameters as unknown but fixed model parameters and estimates\nthem simultaneously by solving an optimization problem. However, the JML\nestimator is known to be asymptotically inconsistent for many IRT models, when\nthe sample size goes to infinity and the number of items keeps fixed.\nConsequently, in the psychometrics literature, this estimator is less preferred\nto the marginal maximum likelihood (MML) estimator. In this paper, we\nre-investigate the JML estimator for high-dimensional exploratory item factor\nanalysis, from both statistical and computational perspectives. In particular,\nwe establish a notion of statistical consistency for a constrained JML\nestimator, under an asymptotic setting that both the numbers of items and\npeople grow to infinity and that many responses may be missing. A parallel\ncomputing algorithm is proposed for this estimator that can scale to very large\ndatasets. Via simulation studies, we show that when the dimensionality is high,\nthe proposed estimator yields similar or even better results than those from\nthe MML estimator, but can be obtained computationally much more efficiently.\nAn illustrative real data example is provided based on the revised version of\nEysenck's Personality Questionaire (EPQ-R).\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 02:05:04 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 10:08:12 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Chen", "Yunxiao", ""], ["Li", "Xiaoou", ""], ["Zhang", "Siliang", ""]]}, {"id": "1712.06916", "submitter": "Elena Pesce", "authors": "Elena Pesce, Eva Riccomagno, Henry P. Wynn", "title": "Experimental Design Issues in Big Data. The Question of Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data can be collected in scientific studies via a controlled experiment or\npassive observation. Big data is often collected in a passive way, e.g. from\nsocial media. In studies of causation great efforts are made to guard against\nbias and hidden confounders or feedback which can destroy the identification of\ncausation by corrupting or omitting counterfactuals (controls). Various\nsolutions of these problems are discussed, including randomization.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 13:31:59 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 11:56:37 GMT"}, {"version": "v3", "created": "Thu, 11 Jan 2018 14:59:42 GMT"}, {"version": "v4", "created": "Tue, 20 Nov 2018 10:05:54 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Pesce", "Elena", ""], ["Riccomagno", "Eva", ""], ["Wynn", "Henry P.", ""]]}, {"id": "1712.06941", "submitter": "Johnny van Doorn", "authors": "Johnny van Doorn, Alexander Ly, Maarten Marsman, Eric-Jan Wagenmakers", "title": "Bayesian Rank-Based Hypothesis Testing for the Rank Sum Test, the Signed\n  Rank Test, and Spearman's $\\rho$", "comments": "33 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for rank-order problems is frustrated by the absence of an\nexplicit likelihood function. This hurdle can be overcome by assuming a latent\nnormal representation that is consistent with the ordinal information in the\ndata: the observed ranks are conceptualized as an impoverished reflection of an\nunderlying continuous scale, and inference concerns the parameters that govern\nthe latent representation. We apply this generic data-augmentation method to\nobtain Bayes factors for three popular rank-based tests: the rank sum test, the\nsigned rank test, and Spearman's $\\rho_s$.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 14:20:54 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 13:56:56 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 10:33:15 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["van Doorn", "Johnny", ""], ["Ly", "Alexander", ""], ["Marsman", "Maarten", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "1712.07110", "submitter": "Heather Mattie", "authors": "Heather Mattie and Jukka-Pekka Onnela", "title": "Generalizations of Edge Overlap to Weighted and Directed Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing availability of behavioral data from diverse digital\nsources, such as social media sites and cell phones, it is now possible to\nobtain detailed information about the structure, strength, and directionality\nof social interactions in varied settings. While most metrics of network\nstructure have traditionally been defined for unweighted and undirected\nnetworks only, the richness of current network data calls for extending these\nmetrics to weighted and directed networks. One fundamental metric in social\nnetworks is edge overlap, the proportion of friends shared by two connected\nindividuals. Here we extend definitions of edge overlap to weighted and\ndirected networks, and present closed-form expressions for the mean and\nvariance of each version for the Erdos-Renyi random graph and its weighted and\ndirected counterparts. We apply these results to social network data collected\nin rural villages in southern Karnataka, India. We use our analytical results\nto quantify the extent to which the average overlap of the empirical social\nnetwork deviates from that of corresponding random graphs and compare the\nvalues of overlap across networks. Our novel definitions allow the calculation\nof edge overlap for more complex networks and our derivations provide a\nstatistically rigorous way for comparing edge overlap across networks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 18:53:14 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 03:05:18 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Mattie", "Heather", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1712.07166", "submitter": "Sidney Resnick", "authors": "Phyllis Wan, Tiandong Wang, Richard A. Davis, Sidney I. Resnick", "title": "Are Extreme Value Estimation Methods Useful for Network Data?", "comments": "23 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preferential attachment is an appealing edge generating mechanism for\nmodeling social networks. It provides both an intuitive description of network\ngrowth and an explanation for the observed power laws in degree distributions.\nHowever, there are often limitations in fitting parametric network models to\ndata due to the complex nature of real-world networks. In this paper, we\nconsider a semi-parametric estimation approach by looking at only the nodes\nwith large in- or out-degrees of the network. This method examines the tail\nbehavior of both the marginal and joint degree distributions and is based on\nextreme value theory. We compare it with the existing parametric approaches and\ndemonstrate how it can provide more robust estimates of parameters associated\nwith the network when the data are corrupted or when the model is misspecified.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 19:33:37 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Wan", "Phyllis", ""], ["Wang", "Tiandong", ""], ["Davis", "Richard A.", ""], ["Resnick", "Sidney I.", ""]]}, {"id": "1712.07197", "submitter": "Mohamad Shakil Hasan", "authors": "Mohamad S. Hasan", "title": "Optimal P-value Weighting with Independent Information", "comments": "This is a PhD dissertation of Mohamad S. Hasan under the supervision\n  of Dr. Paul Schliekelman, Department of Statistics, University of Georgia", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large-scale multiple testing inherent to high throughput biological data\nnecessitates very high statistical stringency and thus true effects in data are\ndifficult to detect unless they have high effect sizes. One solution to this\nproblem is to use an independent information to prioritize the most promising\nfeatures of the data and thus increase the power to detect them. Weighted\np-values provide a general framework for doing this in a statistically rigorous\nfashion. However, calculating weights that incorporate the independent\ninformation and optimize statistical power remains a challenging problem\ndespite recent advances in this area. Existing methods tend to perform poorly\nin the common situation that true positive features are rare and of low effect\nsize. We introduce covariate based weighting methods for calculating optimal\nweights conditioned on the effect sizes of the tests. This approach uses the\nprobabilistic relationship between covariate and test effect size to calculate\nmore informative weights that are not diluted by null effects as is common with\ngroup-based methods. This relationship can be calculated theoretically for\nnormally distributed covariates or estimated empirically in other cases. We\nshowed via simulations and applications to data that this method outperforms\nexisting methods by a large margin in the rare/low effect size scenario and has\nat least comparable performance in all scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 20:48:12 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Hasan", "Mohamad S.", ""]]}, {"id": "1712.07216", "submitter": "Marco Geraci", "authors": "Marco Geraci", "title": "Mixed-effects models using the normal and the Laplace distributions: A\n  $\\mathbf{2 \\times 2}$ convolution scheme for applied research", "comments": "30 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical applications, the normal and the Laplace distributions are\noften contrasted: the former as a standard tool of analysis, the latter as its\nrobust counterpart. I discuss the convolutions of these two popular\ndistributions and their applications in research. I consider four models within\na simple $2\\times 2$ scheme which is of practical interest in the analysis of\nclustered (e.g., longitudinal) data. In my view, these models, some of which\nare less known than others by the majority of applied researchers, constitute a\n'family' of sensible alternatives when modelling issues arise. In three\nexamples, I revisit data published recently in the epidemiological and clinical\nliterature as well as a classic biological dataset.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 21:15:49 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Geraci", "Marco", ""]]}, {"id": "1712.07265", "submitter": "Eric Fu", "authors": "Eric Fu, Nancy Heckman", "title": "Model-based curve registration via stochastic approximation EM algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data often exhibit both amplitude and phase variation around a\ncommon base shape, with phase variation represented by a so called warping\nfunction. The process removing phase variation by curve alignment and inference\nof the warping functions is referred to as curve registration. When functional\ndata are observed with substantial noise, model-based methods can be employed\nfor simultaneous smoothing and curve registration. However, the nonlinearity of\nthe model often renders the inference computationally challenging. In this\npaper, we propose an alternative method for model-based curve registration\nwhich is computationally more stable and efficient than existing approaches in\nthe literature. We apply our method to the analysis of elephant seal dive\nprofiles and show that more intuitive groupings can be obtained by clustering\non phase variations via the predicted warping functions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 23:50:49 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 00:02:26 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Fu", "Eric", ""], ["Heckman", "Nancy", ""]]}, {"id": "1712.07270", "submitter": "Dustin Pluta", "authors": "Dustin Pluta, Hernando Ombao, Chuansheng Chen, Gui Xue, Robert Moyzis,\n  Zhaoxia Yu", "title": "Adaptive Mantel Test for AssociationTesting in Imaging Genetics Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mantel's test (MT) for association is conducted by testing the linear\nrelationship of similarity of all pairs of subjects between two observational\ndomains. Motivated by applications to neuroimaging and genetics data, and\nfollowing the succes of shrinkage and kernel methods for prediction with\nhigh-dimensional data, we here introduce the adaptive Mantel test as an\nextension of the MT. By utilizing kernels and penalized similarity measures,\nthe adaptive Mantel test is able to achieve higher statistical power relative\nto the classical MT in many settings. Furthermore, the adaptive Mantel test is\ndesigned to simultaneously test over multiple similarity measures such that the\ncorrect type I error rate under the null hypothesis is maintained without the\nneed to directly adjust the significance threshold for multiple testing. The\nperformance of the adaptive Mantel test is evaluated on simulated data, and is\nused to investigate associations between genetics markers related to\nAlzheimer's Disease and heatlhy brain physiology with data from a working\nmemory study of 350 college students from Beijing Normal University.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 00:06:22 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 03:45:17 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Pluta", "Dustin", ""], ["Ombao", "Hernando", ""], ["Chen", "Chuansheng", ""], ["Xue", "Gui", ""], ["Moyzis", "Robert", ""], ["Yu", "Zhaoxia", ""]]}, {"id": "1712.07319", "submitter": "Yuanjun Gao", "authors": "Yuanjun Gao, Jack Goetz, Rahul Mazumder, Matthew Connelly", "title": "Mining Events with Declassified Diplomatic Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 1973 the State Department has been using electronic records systems to\npreserve classified communications. Recently, approximately 1.9 million of\nthese records from 1973-77 have been made available by the U.S. National\nArchives. While some of these communication streams have periods witnessing an\nacceleration in the rate of transmission; others do not show any notable\npatterns in communication intensity. Given the sheer volume of these\ncommunications -- far greater than what had been available until now --\nscholars need automated statistical techniques to identify the communications\nthat warrant closer study. We develop a statistical framework that can\nsemi-automatically identify from a large corpus of documents a handful that\nhistorians would consider more interesting electronic records. Our approach\nbrings together related but distinct statistical concepts from nonparametric\nsignal estimation and statistical hypothesis testing -- which when put together\nhelp us identify and analyze various geometrical aspects of the communication\nstreams. Dominant periods of heightened and sustained activities aka bursts, as\nidentified through these methods, correspond well with historical events\nrecognized by standard reference works on the 1970s.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 04:39:28 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Gao", "Yuanjun", ""], ["Goetz", "Jack", ""], ["Mazumder", "Rahul", ""], ["Connelly", "Matthew", ""]]}, {"id": "1712.07325", "submitter": "Lingzhou Xue", "authors": "Kevin H. Lee, Lingzhou Xue and David R. Hunter", "title": "Model-Based Clustering of Time-Evolving Networks through Temporal\n  Exponential-Family Random Graph Models", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic networks are a general language for describing time-evolving complex\nsystems, and discrete time network models provide an emerging statistical\ntechnique for various applications. It is a fundamental research question to\ndetect the community structure in time-evolving networks. However, due to\nsignificant computational challenges and difficulties in modeling communities\nof time-evolving networks, there is little progress in the current literature\nto effectively find communities in time-evolving networks. In this work, we\npropose a novel model-based clustering framework for time-evolving networks\nbased on discrete time exponential-family random graph models. To choose the\nnumber of communities, we use conditional likelihood to construct an effective\nmodel selection criterion. Furthermore, we propose an efficient variational\nexpectation-maximization (EM) algorithm to find approximate maximum likelihood\nestimates of network parameters and mixing proportions. By using variational\nmethods and minorization-maximization (MM) techniques, our method has appealing\nscalability for large-scale time-evolving networks. The power of our method is\ndemonstrated in simulation studies and empirical applications to international\ntrade networks and the collaboration networks of a large American research\nuniversity.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 05:36:00 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Lee", "Kevin H.", ""], ["Xue", "Lingzhou", ""], ["Hunter", "David R.", ""]]}, {"id": "1712.07364", "submitter": "Martin Spindler", "authors": "Sven Klaassen and Jannis Kueck and Martin Spindler", "title": "Transformation Models in High-Dimensions", "comments": "63 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformation models are a very important tool for applied statisticians and\neconometricians. In many applications, the dependent variable is transformed so\nthat homogeneity or normal distribution of the error holds. In this paper, we\nanalyze transformation models in a high-dimensional setting, where the set of\npotential covariates is large. We propose an estimator for the transformation\nparameter and we show that it is asymptotically normally distributed using an\northogonalized moment condition where the nuisance functions depend on the\ntarget parameter. In a simulation study, we show that the proposed estimator\nworks well in small samples. A common practice in labor economics is to\ntransform wage with the log-function. In this study, we test if this\ntransformation holds in CPS data from the United States.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 08:58:41 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Klaassen", "Sven", ""], ["Kueck", "Jannis", ""], ["Spindler", "Martin", ""]]}, {"id": "1712.07422", "submitter": "Leon Brian Lucy", "authors": "Leon B. Lucy (Astrophysics Group, Imperial College London)", "title": "Bayesian model checking: A comparison of tests", "comments": "4 pages, 3 figures. Submitted to Astronomy & Astrophysics", "journal-ref": null, "doi": "10.1051/0004-6361/201732461", "report-no": null, "categories": "astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two procedures for checking Bayesian models are compared using a simple test\nproblem based on the local Hubble expansion. Over four orders of magnitude,\np-values derived from a global goodness-of-fit criterion for posterior\nprobability density functions (Lucy 2017) agree closely with posterior\npredictive p-values. The former can therefore serve as an effective proxy for\nthe difficult-to-calculate posterior predictive p-values.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 11:29:20 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Lucy", "Leon B.", "", "Astrophysics Group, Imperial College London"]]}, {"id": "1712.07437", "submitter": "Marcelo Hartmann", "authors": "Marcelo Hartmann and Jarno Vanhatalo", "title": "Laplace approximation and the natural gradient for Gaussian process\n  regression with the heteroscedastic Student-t model", "comments": null, "journal-ref": "Statistics and Computing 2018", "doi": "10.1007/s11222-018-9836-0)", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper considers the Laplace method to derive approximate inference for\nthe Gaussian process (GP) regression in the location and scale parameters of\nthe Student-t probabilistic model. This allows both mean and variance of the\ndata to vary as a function of covariates with the attractive feature that the\nStudent-t model has been widely used as a useful tool for robustifying data\nanalysis. The challenge in the approximate inference for the GP regression with\nthe Student-t probabilistic model, lies in the analytical intractability of the\nposterior distribution and the lack of concavity of the log-likelihood\nfunction. We present the natural gradient adaptation for the estimation process\nwhich primarily relies on the property that the Student-t model naturally has\northogonal parametrization with respect to the location and scale paramaters.\nDue to this particular property of the model, we also introduce an alternative\nLaplace approximation by using the Fisher information matrix in place of the\nHessian matrix of the negative log-likelihood function. According to\nexperiments this alternative approximation provides very similar posterior\napproximations and predictive performance when compared to the traditional\nLaplace approximation. We also compare both of these Laplace approximations\nwith the Monte Carlo Markov Chain (MCMC) method. Moreover, we compare our\nheteroscedastic Student-t model and the GP regression with the heteroscedastic\nGaussian model. We also discuss how our approach can improve the inference\nalgorithm in cases where the probabilistic model assumed for the data is not\nlog-concave.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 12:10:13 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 10:32:09 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Hartmann", "Marcelo", ""], ["Vanhatalo", "Jarno", ""]]}, {"id": "1712.07513", "submitter": "Dibyendu Bhaumik Mr.", "authors": "Dibyendu Bhaumik and Debasis Sengupta", "title": "Estimating historic movement of a climatological variable from a pair of\n  misaligned data sets", "comments": "41 pages, 5 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": "ASU/2017/17 Indian Statistical Institute, Kolkata, India", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider in this paper the problem of estimating the mean function from a\npair of paleoclimatic functional data sets, after one of them has been\nregistered with the other. We show theoretically that registering one data set\nwith respect to the other is the right way to formulate this problem, which is\nin contrast with estimation of the mean function in a \"neutral\" time scale that\nis preferred in the analysis of multiple sets of longitudinal growth data. Once\nthis registration is done, the Nadaraya-Watson estimator of the mean function\nmay be computed from the pooled data. We show that, if a consistent estimator\nof the time transformation is used for this registration, the above estimator\nof the mean function would be consistent under a few additional conditions. We\nstudy the potential change in asymptotic mean squared error of the estimator\nthat may be possible because of the contribution of the time-transformed data\nset. After demonstrating through simulation that the additional data can lead\nto improved estimation in spite of estimation error in registration, we\nestimate the mean function of three pairs of paleoclimatic data sets. The\nanalysis reveals some interesting aspects of the data sets and the estimation\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 15:05:21 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Bhaumik", "Dibyendu", ""], ["Sengupta", "Debasis", ""]]}, {"id": "1712.07519", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang and Weijie Su", "title": "Statistical Inference for the Population Landscape via Moment Adjusted\n  Stochastic Gradients", "comments": "Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology) 2019, to appear", "journal-ref": "Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology) 81 (2019) 431-456", "doi": "10.1111/rssb.12313", "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern statistical inference tasks often require iterative optimization\nmethods to compute the solution. Convergence analysis from an optimization\nviewpoint only informs us how well the solution is approximated numerically but\noverlooks the sampling nature of the data. In contrast, recognizing the\nrandomness in the data, statisticians are keen to provide uncertainty\nquantification, or confidence, for the solution obtained using iterative\noptimization methods. This paper makes progress along this direction by\nintroducing the moment-adjusted stochastic gradient descents, a new stochastic\noptimization method for statistical inference. We establish non-asymptotic\ntheory that characterizes the statistical distribution for certain iterative\nmethods with optimization guarantees. On the statistical front, the theory\nallows for model mis-specification, with very mild conditions on the data. For\noptimization, the theory is flexible for both convex and non-convex cases.\nRemarkably, the moment-adjusting idea motivated from \"error standardization\" in\nstatistics achieves a similar effect as acceleration in first-order\noptimization methods used to fit generalized linear models. We also demonstrate\nthis acceleration effect in the non-convex setting through numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 15:16:53 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 23:31:12 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Liang", "Tengyuan", ""], ["Su", "Weijie", ""]]}, {"id": "1712.07750", "submitter": "David Frazier", "authors": "David T. Frazier, Worapree Maneesoonthorn, Gael M. Martin, and Brendan\n  P.M. McCabe", "title": "Approximate Bayesian Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) has become increasingly prominent as a\nmethod for conducting parameter inference in a range of challenging statistical\nproblems, most notably those characterized by an intractable likelihood\nfunction. In this paper, we focus on the use of ABC not as a tool for\nparametric inference, but as a means of generating probabilistic forecasts; or\nfor conducting what we refer to as `approximate Bayesian forecasting'. The four\nkey issues explored are: i) the link between the theoretical behavior of the\nABC posterior and that of the ABC-based predictive; ii) the use of proper\nscoring rules to measure the (potential) loss of forecast accuracy when using\nan approximate rather than an exact predictive; iii) the performance of\napproximate Bayesian forecasting in state space models; and iv) the use of\nforecasting criteria to inform the selection of ABC summaries in empirical\nsettings. The primary finding of the paper is that ABC can provide a\ncomputationally efficient means of generating probabilistic forecasts that are\nnearly identical to those produced by the exact predictive, and in a fraction\nof the time required to produce predictions via an exact method. y identical to\nthose produced by the exact predictive, and in a fraction of the time required\nto produce predictions via an exact method.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 23:56:18 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 02:09:07 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Frazier", "David T.", ""], ["Maneesoonthorn", "Worapree", ""], ["Martin", "Gael M.", ""], ["McCabe", "Brendan P. M.", ""]]}, {"id": "1712.07800", "submitter": "Lingzhou Xue", "authors": "Amal Agarwal and Lingzhou Xue", "title": "Model-Based Clustering of Nonparametric Weighted Networks with\n  Application to Water Pollution Analysis", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water pollution is a major global environmental problem, and it poses a great\nenvironmental risk to public health and biological diversity. This work is\nmotivated by assessing the potential environmental threat of coal mining\nthrough increased sulfate concentrations in river networks, which do not belong\nto any simple parametric distribution. However, existing network models mainly\nfocus on binary or discrete networks and weighted networks with known\nparametric weight distributions. We propose a principled nonparametric weighted\nnetwork model based on exponential-family random graph models and local\nlikelihood estimation and study its model-based clustering with application to\nlarge-scale water pollution network analysis. We do not require any parametric\ndistribution assumption on network weights. The proposed method greatly extends\nthe methodology and applicability of statistical network models. Furthermore,\nit is scalable to large and complex networks in large-scale environmental\nstudies. The power of our proposed methods is demonstrated in simulation\nstudies and a real application to sulfate pollution network analysis in Ohio\nwatershed located in Pennsylvania, United States.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 05:36:10 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 03:31:53 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Agarwal", "Amal", ""], ["Xue", "Lingzhou", ""]]}, {"id": "1712.07801", "submitter": "Chao Gao", "authors": "Haoyang Liu and Chao Gao", "title": "Density Estimation with Contaminated Data: Minimax Rates and Theory of\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies density estimation under pointwise loss in the setting of\ncontamination model. The goal is to estimate $f(x_0)$ at some\n$x_0\\in\\mathbb{R}$ with i.i.d. observations, $$ X_1,\\dots,X_n\\sim\n(1-\\epsilon)f+\\epsilon g, $$ where $g$ stands for a contamination distribution.\nIn the context of multiple testing, this can be interpreted as estimating the\nnull density at a point. We carefully study the effect of contamination on\nestimation through the following model indices: contamination proportion\n$\\epsilon$, smoothness of target density $\\beta_0$, smoothness of contamination\ndensity $\\beta_1$, and level of contamination $m$ at the point to be estimated,\ni.e. $g(x_0)\\leq m$. It is shown that the minimax rate with respect to the\nsquared error loss is of order $$\n[n^{-\\frac{2\\beta_0}{2\\beta_0+1}}]\\vee[\\epsilon^2(1\\wedge\nm)^2]\\vee[n^{-\\frac{2\\beta_1}{2\\beta_1+1}}\\epsilon^{\\frac{2}{2\\beta_1+1}}], $$\nwhich characterizes the exact influence of contamination on the difficulty of\nthe problem. We then establish the minimal cost of adaptation to contamination\nproportion, to smoothness and to both of the numbers. It is shown that some\nsmall price needs to be paid for adaptation in any of the three cases.\nVariations of Lepski's method are considered to achieve optimal adaptation.\n  The problem is also studied when there is no smoothness assumption on the\ncontamination distribution. This setting that allows for an arbitrary\ncontamination distribution is recognized as Huber's $\\epsilon$-contamination\nmodel. The minimax rate is shown to be $$\n[n^{-\\frac{2\\beta_0}{2\\beta_0+1}}]\\vee [\\epsilon^{\\frac{2\\beta_0}{\\beta_0+1}}].\n$$ The adaptation theory is also different from the smooth contamination case.\nWhile adaptation to either contamination proportion or smoothness only costs a\nlogarithmic factor, adaptation to both numbers is proved to be impossible.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 05:56:19 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 01:54:01 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2018 03:22:00 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Liu", "Haoyang", ""], ["Gao", "Chao", ""]]}, {"id": "1712.07811", "submitter": "Takashi Kurokawa", "authors": "Takashi Kurokawa, Taihei Oki, Hiromichi Nagao", "title": "Multi-dimensional Graph Fourier Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many signals on Cartesian product graphs appear in the real world, such as\ndigital images, sensor observation time series, and movie ratings on Netflix.\nThese signals are \"multi-dimensional\" and have directional characteristics\nalong each factor graph. However, the existing graph Fourier transform does not\ndistinguish these directions, and assigns 1-D spectra to signals on product\ngraphs. Further, these spectra are often multi-valued at some frequencies. Our\nmain result is a multi-dimensional graph Fourier transform that solves such\nproblems associated with the conventional GFT. Using algebraic properties of\nCartesian products, the proposed transform rearranges 1-D spectra obtained by\nthe conventional GFT into the multi-dimensional frequency domain, of which each\ndimension represents a directional frequency along each factor graph. Thus, the\nmulti-dimensional graph Fourier transform enables directional frequency\nanalysis, in addition to frequency analysis with the conventional GFT.\nMoreover, this rearrangement resolves the multi-valuedness of spectra in some\ncases. The multi-dimensional graph Fourier transform is a foundation of novel\nfilterings and stationarities that utilize dimensional information of graph\nsignals, which are also discussed in this study. The proposed methods are\napplicable to a wide variety of data that can be regarded as signals on\nCartesian product graphs. This study also notes that multivariate graph signals\ncan be regarded as 2-D univariate graph signals. This correspondence provides\nnatural definitions of the multivariate graph Fourier transform and the\nmultivariate stationarity based on their 2-D univariate versions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 06:57:55 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Kurokawa", "Takashi", ""], ["Oki", "Taihei", ""], ["Nagao", "Hiromichi", ""]]}, {"id": "1712.08048", "submitter": "Topi Paananen", "authors": "Topi Paananen, Juho Piironen, Michael Riis Andersen, Aki Vehtari", "title": "Variable selection for Gaussian processes via sensitivity analysis of\n  the posterior predictive distribution", "comments": "Minor changes to text, additions to supplementary material", "journal-ref": "Proceedings of the 22nd International Conference on Artificial\n  Intelligence and Statistics (AISTATS), PMLR 89: 1743-1752, 2019", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection for Gaussian process models is often done using automatic\nrelevance determination, which uses the inverse length-scale parameter of each\ninput variable as a proxy for variable relevance. This implicitly determined\nrelevance has several drawbacks that prevent the selection of optimal input\nvariables in terms of predictive performance. To improve on this, we propose\ntwo novel variable selection methods for Gaussian process models that utilize\nthe predictions of a full model in the vicinity of the training points and\nthereby rank the variables based on their predictive relevance. Our empirical\nresults on synthetic and real world data sets demonstrate improved variable\nselection compared to automatic relevance determination in terms of variability\nand predictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 16:15:34 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 08:03:12 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 09:19:03 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Paananen", "Topi", ""], ["Piironen", "Juho", ""], ["Andersen", "Michael Riis", ""], ["Vehtari", "Aki", ""]]}, {"id": "1712.08211", "submitter": "Eric Tramel", "authors": "Baptiste Goujaud, Eric W. Tramel, Pierre Courtiol, Mikhail Zaslavskiy,\n  Gilles Wainrib", "title": "Robust Detection of Covariate-Treatment Interactions in Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of interactions between treatment effects and patient descriptors\nin clinical trials is critical for optimizing the drug development process. The\nincreasing volume of data accumulated in clinical trials provides a unique\nopportunity to discover new biomarkers and further the goal of personalized\nmedicine, but it also requires innovative robust biomarker detection methods\ncapable of detecting non-linear, and sometimes weak, signals. We propose a set\nof novel univariate statistical tests, based on the theory of random walks,\nwhich are able to capture non-linear and non-monotonic covariate-treatment\ninteractions. We also propose a novel combined test, which leverages the power\nof all of our proposed univariate tests into a single general-case tool. We\npresent results for both synthetic trials as well as real-world clinical\ntrials, where we compare our method with state-of-the-art techniques and\ndemonstrate the utility and robustness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 21:09:13 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Goujaud", "Baptiste", ""], ["Tramel", "Eric W.", ""], ["Courtiol", "Pierre", ""], ["Zaslavskiy", "Mikhail", ""], ["Wainrib", "Gilles", ""]]}, {"id": "1712.08243", "submitter": "St\\'ephane Ga\\\"iffas", "authors": "Maryan Morel, Emmanuel Bacry, St\\'ephane Ga\\\"iffas, Agathe Guilloux,\n  Fanny Leroy", "title": "ConvSCCS: convolutional self-controlled case series model for lagged\n  adverse event detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased availability of large databases of electronic health\nrecords (EHRs) comes the chance of enhancing health risks screening. Most\npost-marketing detections of adverse drug reaction (ADR) rely on physicians'\nspontaneous reports, leading to under reporting. To take up this challenge, we\ndevelop a scalable model to estimate the effect of multiple longitudinal\nfeatures (drug exposures) on a rare longitudinal outcome. Our procedure is\nbased on a conditional Poisson model also known as self-controlled case series\n(SCCS). We model the intensity of outcomes using a convolution between\nexposures and step functions, that are penalized using a combination of\ngroup-Lasso and total-variation. This approach does not require the\nspecification of precise risk periods, and allows to study in the same model\nseveral exposures at the same time. We illustrate the fact that this approach\nimproves the state-of-the-art for the estimation of the relative risks both on\nsimulations and on a cohort of diabetic patients, extracted from the large\nFrench national health insurance database (SNIIRAM), a SQL database built\naround medical reimbursements of more than 65 million people. This work has\nbeen done in the context of a research partnership between Ecole Polytechnique\nand CNAMTS (in charge of SNIIRAM).\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 23:06:22 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 23:27:15 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Morel", "Maryan", ""], ["Bacry", "Emmanuel", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Guilloux", "Agathe", ""], ["Leroy", "Fanny", ""]]}, {"id": "1712.08499", "submitter": "Adam Lane", "authors": "Adam Lane", "title": "Adaptive Designs for Optimal Observed Fisher Information", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": "10.1111/rssb.12378", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expected Fisher information can be found a priori and as a result its inverse\nis the primary variance approximation used in the design of experiments. This\nis in contrast to the common claim that the inverse of observed Fisher\ninformation is a better approximation to the variance of the maximum likelihood\nestimator. Observed Fisher information cannot be known a priori; however, if an\nexperiment is conducted sequentially (in a series of runs) the observed Fisher\ninformation from previous runs is available. In the current work two adaptive\ndesigns are proposed that use the observed Fisher information from previous\nruns in the design of the current run.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 15:16:37 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 14:25:22 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Lane", "Adam", ""]]}, {"id": "1712.08522", "submitter": "Allyson Seyb", "authors": "Jack Lothian, Anders Holmberg and Allyson Seyb", "title": "Linking Administrative Data: An Evolutionary Schema", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics New Zealand (Stats NZ) has committed unreservedly to an\nadministrative data first policy. Thus, all new methods used at Stats NZ are to\nbe viewed within this context and discussing strategies for using\nadministrative data is an integral part of every working day. As statistical\nmethodologists, the three authors were drawn into these discussions. Like most\nmethodologists, the authors see surveys and the publications of their results\nas a process where estimation is the key tool to achieve the final goal of an\naccurate statistical output. Randomness and sampling exists to support this\ngoal, and early on it was clear to us that the incoming it-is-what-it-is data\nsources were not randomly selected. These sources were obviously biased and\nthus would produce biased estimates. So, we set out to design a strategy to\ndeal with this issue. This led us to the concept of representativeness which is\nclosely related to statistical bias but has a wider context invoking both\nrandomness and judgement. The representativeness issue was the principal\nquestion that we set out to answer. The necessary components that we gathered\nfor our solution are summarized in the paper.\n  Keywords: Representativeness, Timeline Databases, Statistical Registers,\nEstimation\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 21:31:14 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Lothian", "Jack", ""], ["Holmberg", "Anders", ""], ["Seyb", "Allyson", ""]]}, {"id": "1712.08567", "submitter": "Estelle Kuhn", "authors": "Charlotte Baey and Paul-Henry Courn\\`ede and Estelle Kuhn", "title": "Likelihood ratio test for variance components in nonlinear mixed effects\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed effects models are widely used to describe heterogeneity in a\npopulation. A crucial issue when adjusting such a model to data consists in\nidentifying fixed and random effects. From a statistical point of view, it\nremains to test the nullity of the variances of a given subset of random\neffects. Some authors have proposed to use the likelihood ratio test and have\nestablished its asymptotic distribution in some particular cases. Nevertheless,\nto the best of our knowledge, no general variance components testing procedure\nhas been fully investigated yet. In this paper, we study the likelihood ratio\ntest properties to test that the variances of a general subset of the random\neffects are equal to zero in both linear and nonlinear mixed effects model,\nextending the existing results. We prove that the asymptotic distribution of\nthe test is a chi-bar-square distribution, that is to say a mixture of\nchi-square distributions, and we identify the corresponding weights. We\nhighlight in particular that the limiting distribution depends on the presence\nof correlations between the random effects but not on the linear or nonlinear\nstructure of the mixed effects model. We illustrate the finite sample size\nproperties of the test procedure through simulation studies and apply the test\nprocedure to two real datasets of dental growth and of coucal growth.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 16:44:21 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Baey", "Charlotte", ""], ["Courn\u00e8de", "Paul-Henry", ""], ["Kuhn", "Estelle", ""]]}, {"id": "1712.08586", "submitter": "Xinyu Kang", "authors": "Xinyu Kang, Apratim Ganguly, and Eric D. Kolaczyk", "title": "Dynamic Networks with Multi-scale Temporal Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel method for modeling non-stationary multivariate time\nseries, with time-varying conditional dependencies represented through dynamic\nnetworks. Our proposed approach combines traditional multi-scale modeling and\nnetwork based neighborhood selection, aiming at capturing temporally local\nstructure in the data while maintaining sparsity of the potential interactions.\nOur multi-scale framework is based on recursive dyadic partitioning, which\nrecursively partitions the temporal axis into finer intervals and allows us to\ndetect local network structural changes at varying temporal resolutions. The\ndynamic neighborhood selection is achieved through penalized likelihood\nestimation, where the penalty seeks to limit the number of neighbors used to\nmodel the data. We present theoretical and numerical results describing the\nperformance of our method, which is motivated and illustrated using task-based\nmagnetoencephalography (MEG) data in neuroscience.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 17:33:40 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Kang", "Xinyu", ""], ["Ganguly", "Apratim", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "1712.08588", "submitter": "Kathryn Laing", "authors": "Kathryn Laing, Peter Adam Thwaites and John Paul Gosling", "title": "Rank Pruning for Dominance Queries in CP-Nets", "comments": "58 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional preference networks (CP-nets) are a graphical representation of a\nperson's (conditional) preferences over a set of discrete variables. In this\npaper, we introduce a novel method of quantifying preference for any given\noutcome based on a CP-net representation of a user's preferences. We\ndemonstrate that these values are useful for reasoning about user preferences.\nIn particular, they allow us to order (any subset of) the possible outcomes in\naccordance with the user's preferences. Further, these values can be used to\nimprove the efficiency of outcome dominance testing. That is, given a pair of\noutcomes, we can determine which the user prefers more efficiently. Through\nexperimental results, we show that this method is more effective than existing\ntechniques for improving dominance testing efficiency. We show that the above\nresults also hold for CP-nets that express indifference between variable\nvalues.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 17:41:11 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 13:41:55 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Laing", "Kathryn", ""], ["Thwaites", "Peter Adam", ""], ["Gosling", "John Paul", ""]]}, {"id": "1712.08641", "submitter": "Anna Smith", "authors": "Anna L. Smith, Dena M. Asta and Catherine A. Calder", "title": "The Geometry of Continuous Latent Space Models for Network Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the class of continuous latent space (statistical) models for\nnetwork data, paying particular attention to the role of the geometry of the\nlatent space. In these models, the presence/absence of network dyadic ties are\nassumed to be conditionally independent given the dyads? unobserved positions\nin a latent space. In this way, these models provide a probabilistic framework\nfor embedding network nodes in a continuous space equipped with a geometry that\nfacilitates the description of dependence between random dyadic ties.\nSpecifically, these models naturally capture homophilous tendencies and triadic\nclustering, among other common properties of observed networks. In addition to\nreviewing the literature on continuous latent space models from a geometric\nperspective, we highlight the important role the geometry of the latent space\nplays on properties of networks arising from these models via intuition and\nsimulation. Finally, we discuss results from spectral graph theory that allow\nus to explore the role of the geometry of the latent space, independent of\nnetwork size. We conclude with conjectures about how these results might be\nused to infer the appropriate latent space geometry from observed networks.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 20:11:32 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 18:08:15 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Smith", "Anna L.", ""], ["Asta", "Dena M.", ""], ["Calder", "Catherine A.", ""]]}, {"id": "1712.08664", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "A Mixture of Matrix Variate Bilinear Factor Analyzers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years data has become increasingly higher dimensional, which has\nprompted an increased need for dimension reduction techniques. This is perhaps\nespecially true for clustering (unsupervised classification) as well as\nsemi-supervised and supervised classification. Although dimension reduction in\nthe area of clustering for multivariate data has been quite thoroughly\ndiscussed within the literature, there is relatively little work in the area of\nthree-way, or matrix variate, data. Herein, we develop a mixture of matrix\nvariate bilinear factor analyzers (MMVBFA) model for use in clustering\nhigh-dimensional matrix variate data. This work can be considered both the\nfirst matrix variate bilinear factor analysis model as well as the first MMVBFA\nmodel. Parameter estimation is discussed, and the MMVBFA model is illustrated\nusing simulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 21:26:09 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 16:45:03 GMT"}, {"version": "v3", "created": "Sat, 29 Sep 2018 23:41:55 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1712.08669", "submitter": "Mimoza Zografi", "authors": "Mimoza Zografi and Evdokia Xekalaki", "title": "Modeling Spatial Overdispersion with the Generalized Waring Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling spatial overdispersion requires point processes models with finite\ndimensional distributions that are overdisperse relative to the Poisson.\nFitting such models usually heavily relies on the properties of stationarity,\nergodicity, and orderliness. And, though processes based on negative binomial\nfinite dimensional distributions have been widely considered, they typically\nfail to simultaneously satisfy the three required properties for fitting.\nIndeed, it has been conjectured by Diggle and Milne that no negative binomial\nmodel can satisfy all three properties. In light of this, we change\nperspective, and construct a new process based on a different overdisperse\ncount model, the Generalized Waring Distribution. While comparably tractable\nand flexible to negative binomial processes, the Generalized Waring process is\nshown to possess all required properties, and additionally span the negative\nbinomial and Poisson processes as limiting cases. In this sense, the GW process\nprovides an approximate resolution to the conundrum highlighted by Diggle and\nMilne.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 22:00:27 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Zografi", "Mimoza", ""], ["Xekalaki", "Evdokia", ""]]}, {"id": "1712.08698", "submitter": "Hang Xu", "authors": "Hang Xu, Mayer Alvo and Philip L.H. Yu", "title": "Angle-Based Models for Ranking Data", "comments": "Accepted by Computational Statistics and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of general exponential ranking models is introduced which we\nlabel angle-based models for ranking data. A consensus score vector is assumed,\nwhich assigns scores to a set of items, where the scores reflect a consensus\nview of the relative preference of the items. The probability of observing a\nranking is modeled to be proportional to its cosine of the angle from the\nconsensus vector. Bayesian variational inference is employed to determine the\ncorresponding predictive density. It can be seen from simulation experiments\nthat the Bayesian variational inference approach not only has great\ncomputational advantage compared to the traditional MCMC, but also avoids the\nproblem of overfitting inherent when using maximum likelihood methods. The\nmodel also works when a large number of items are ranked which is usually an\nNP-hard problem to find the estimate of parameters for other classes of ranking\nmodels. Model extensions to incomplete rankings and mixture models are also\ndeveloped. Real data applications demonstrate that the model and extensions can\nhandle different tasks for the analysis of ranking data.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 01:55:42 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Xu", "Hang", ""], ["Alvo", "Mayer", ""], ["Yu", "Philip L. H.", ""]]}, {"id": "1712.08732", "submitter": "Linquan Ma", "authors": "Linquan Ma, Yunjian Yin, Lan Liu and Zhi Geng", "title": "On the Individual Surrogate Paradox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the primary outcome is difficult to collect, surrogate endpoint is\ntypically used as a substitute. It is possible that for every individual,\ntreatment has a positive effect on surrogate, and surrogate has a positive\neffect on primary outcome, but for some individuals, treatment has a negative\neffect on primary outcome. For example, a treatment may be substantially\neffective in preventing the stroke for everyone, and lowering the risk of\nstroke is universally beneficial for a longer survival time, however, the\ntreatment may still cause death for some individuals. We define such\nparadoxical phenomenon as individual surrogate paradox. The individual\nsurrogate paradox is preposed to capture the treatment effect heterogeneity,\nwhich is unable to be described by either the surrogate paradox based on\naverage causal effect (ACE) (Chen et al., 2007) or that based on distributional\ncausal effect (DCE) (Ju and Geng, 2010). We investigate existing surrogate\ncriteria in terms of whether the individual surrogate paradox could manifest.\nWe find that only the strong binary surrogate can avoid such paradox without\nadditional assumptions. Utilizing the sharp bounds, we propose novel criteria\nto exclude the individual surrogate paradox. Our methods are illustrated in an\napplication to determine the effect of the intensive glycemia on the risk of\ndevelopment or progression of diabetic retinopathy.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 08:06:29 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Ma", "Linquan", ""], ["Yin", "Yunjian", ""], ["Liu", "Lan", ""], ["Geng", "Zhi", ""]]}, {"id": "1712.08754", "submitter": "Hirofumi Ohta", "authors": "Hirofumi Ohta, Satoshi Hara", "title": "On Estimation of Conditional Modes Using Multiple Quantile Regressions", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an estimation method for the conditional mode when the\nconditioning variable is high-dimensional. In the proposed method, we first\nestimate the conditional density by solving quantile regressions multiple\ntimes. We then estimate the conditional mode by finding the maximum of the\nestimated conditional density. The proposed method has two advantages in that\nit is computationally stable because it has no initial parameter dependencies,\nand it is statistically efficient with a fast convergence rate. Synthetic and\nreal-world data experiments demonstrate the better performance of the proposed\nmethod compared to other existing ones.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 11:02:20 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Ohta", "Hirofumi", ""], ["Hara", "Satoshi", ""]]}, {"id": "1712.08781", "submitter": "Xuejun Ma X.J. Ma", "authors": "Xin Chen, Xuejun Ma and Wang Zhou", "title": "Distribution Regression", "comments": "25 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression is a fundamental and popular statistical method. There are\nvarious kinds of linear regression, such as mean regression and quantile\nregression. In this paper, we propose a new one called distribution regression,\nwhich allows broad-spectrum of the error distribution in the linear regression.\nOur method uses nonparametric technique to estimate regression parameters. Our\nstudies indicate that our method provides a better alternative than mean\nregression and quantile regression under many settings, particularly for\nasymmetrical heavy-tailed distribution or multimodal distribution of the error\nterm. Under some regular conditions, our estimator is $\\sqrt n$-consistent and\npossesses the asymptotically normal distribution. The proof of the asymptotic\nnormality of our estimator is very challenging because our nonparametric\nlikelihood function cannot be transformed into sum of independent and\nidentically distributed random variables. Furthermore, penalized likelihood\nestimator is proposed and enjoys the so-called oracle property with diverging\nnumber of parameters. Numerical studies also demonstrate the effectiveness and\nthe flexibility of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 14:51:53 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Chen", "Xin", ""], ["Ma", "Xuejun", ""], ["Zhou", "Wang", ""]]}, {"id": "1712.08786", "submitter": "Ranjan Maitra", "authors": "Anna D. Peterson and Arka P. Ghosh and Ranjan Maitra", "title": "Merging $K$-means with hierarchical clustering for identifying\n  general-shaped groups", "comments": "16 pages, 1 table, 9 figures; accepted for publication in Stat", "journal-ref": null, "doi": "10.1002/sta4.172", "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering partitions a dataset such that observations placed together in a\ngroup are similar but different from those in other groups. Hierarchical and\n$K$-means clustering are two approaches but have different strengths and\nweaknesses. For instance, hierarchical clustering identifies groups in a\ntree-like structure but suffers from computational complexity in large datasets\nwhile $K$-means clustering is efficient but designed to identify homogeneous\nspherically-shaped clusters. We present a hybrid non-parametric clustering\napproach that amalgamates the two methods to identify general-shaped clusters\nand that can be applied to larger datasets. Specifically, we first partition\nthe dataset into spherical groups using $K$-means. We next merge these groups\nusing hierarchical methods with a data-driven distance measure as a stopping\ncriterion. Our proposal has the potential to reveal groups with general shapes\nand structure in a dataset. We demonstrate good performance on several\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 15:07:00 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Peterson", "Anna D.", ""], ["Ghosh", "Arka P.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1712.08823", "submitter": "Pavel Mozgunov", "authors": "Pavel Mozgunov, Thomas Jaki and Xavier Paoletti", "title": "A Benchmark for Dose Finding Studies with Continuous Outcomes", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important tool to evaluate the performance of any design is an optimal\nbenchmark proposed by O'Quigley and others (2002, Biostatistics 3(1), 51-56)\nthat provides an upper bound on the performance of a design under a given\nscenario. The original benchmark can be applied to dose finding studies with a\nbinary endpoint only. However, there is a growing interest in dose finding\nstudies involving continuous outcomes, but no benchmark for such studies has\nbeen developed. We show that the original benchmark and its extension by Cheung\n(2014, Biometrics 70(2), 389-397), when looked at from a different perspective,\ncan be generalised to various settings with several discrete and continuous\noutcomes. We illustrate and compare the benchmark performance in the setting of\na Phase I clinical trial with continuous toxicity endpoint and in the setting\nof a Phase I/II clinical trial with continuous efficacy outcome. We show that\nthe proposed benchmark provides an accurate upper bound for model-based dose\nfinding methods and serves as a powerful tool for evaluating designs.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 19:14:21 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 10:24:32 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Mozgunov", "Pavel", ""], ["Jaki", "Thomas", ""], ["Paoletti", "Xavier", ""]]}, {"id": "1712.08837", "submitter": "Ze Jin", "authors": "Ze Jin, Benjamin B. Risk, David S. Matteson", "title": "Optimization and Testing in Linear Non-Gaussian Component Analysis", "comments": "33 pages, 3 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) decomposes multivariate data into\nmutually independent components (ICs). The ICA model is subject to a constraint\nthat at most one of these components is Gaussian, which is required for model\nidentifiability. Linear non-Gaussian component analysis (LNGCA) generalizes the\nICA model to a linear latent factor model with any number of both non-Gaussian\ncomponents (signals) and Gaussian components (noise), where observations are\nlinear combinations of independent components. Although the individual Gaussian\ncomponents are not identifiable, the Gaussian subspace is identifiable. We\nintroduce an estimator along with its optimization approach in which\nnon-Gaussian and Gaussian components are estimated simultaneously, maximizing\nthe discrepancy of each non-Gaussian component from Gaussianity while\nminimizing the discrepancy of each Gaussian component from Gaussianity. When\nthe number of non-Gaussian components is unknown, we develop a statistical test\nto determine it based on resampling and the discrepancy of estimated\ncomponents. Through a variety of simulation studies, we demonstrate the\nimprovements of our estimator over competing estimators, and we illustrate the\neffectiveness of the test to determine the number of non-Gaussian components.\nFurther, we apply our method to real data examples and demonstrate its\npractical value.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 20:37:26 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 20:23:30 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Risk", "Benjamin B.", ""], ["Matteson", "David S.", ""]]}, {"id": "1712.08887", "submitter": "Linda S. L. Tan", "authors": "Linda S. L. Tan", "title": "Efficient data augmentation techniques for state space models", "comments": "40 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the first part of the article, we propose a data augmentation scheme for\nimproving the rate of convergence of the EM algorithm in estimating Gaussian\nstate space models. The scheme considers a linear transformation of the latent\nstates in which two working parameters are introduced for rescaling and\nrecentering. We derive optimal values of the working parameters by minimizing\nthe fraction of missing information, and study their large sample properties\nand dependence on the persistence and signal-to-noise ratio. An alternating\nexpectation-conditional maximization (AECM) algorithm is designed to take\nadvantage of the proposed scheme, and shown to be a more attractive alternative\nto the centered parametrization (CP) or noncentered parametrization (NCP). In\nthe second part, we extend earlier results to Bayesian Markov chain Monte Carlo\n(MCMC) algorithms for non-Gaussian state space models, focusing on the\nstochastic volatility and stochastic conditional duration models. A\nblock-specific reparametrization (BSR) strategy for multi-block MCMC samplers\nis proposed which enables the EM data augmentation scheme to be applied to\nnon-Gaussian models via a mixture of normals approximation. Applications on\nsimulated data and benchmark real data sets indicate that the BSR strategy is\nable to yield improvements in simulation efficiency compared with the CP or\nNCP, and sometimes even over ASIS (which interweaves the CP and NCP).\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 08:22:02 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 21:10:34 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Tan", "Linda S. L.", ""]]}, {"id": "1712.08914", "submitter": "Ahmed Alaa", "authors": "Ahmed M. Alaa and Mihaela van der Schaar", "title": "Bayesian Nonparametric Causal Inference: Information Rates and Learning\n  Algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2018.2848230", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of estimating the causal effect of a treatment on\nindividual subjects from observational data, this is a central problem in\nvarious application domains, including healthcare, social sciences, and online\nadvertising. Within the Neyman Rubin potential outcomes model, we use the\nKullback Leibler (KL) divergence between the estimated and true distributions\nas a measure of accuracy of the estimate, and we define the information rate of\nthe Bayesian causal inference procedure as the (asymptotic equivalence class of\nthe) expected value of the KL divergence between the estimated and true\ndistributions as a function of the number of samples. Using Fano method, we\nestablish a fundamental limit on the information rate that can be achieved by\nany Bayesian estimator, and show that this fundamental limit is independent of\nthe selection bias in the observational data. We characterize the Bayesian\npriors on the potential (factual and counterfactual) outcomes that achieve the\noptimal information rate. As a consequence, we show that a particular class of\npriors that have been widely used in the causal inference literature cannot\nachieve the optimal information rate. On the other hand, a broader class of\npriors can achieve the optimal information rate. We go on to propose a prior\nadaptation procedure (which we call the information based empirical Bayes\nprocedure) that optimizes the Bayesian prior by maximizing an information\ntheoretic criterion on the recovered causal effects rather than maximizing the\nmarginal likelihood of the observed (factual) data. Building on our analysis,\nwe construct an information optimal Bayesian causal inference algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 12:36:23 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 23:16:54 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Alaa", "Ahmed M.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1712.08946", "submitter": "Ruobin Gong", "authors": "Ruobin Gong and Xiao-Li Meng", "title": "Judicious Judgment Meets Unsettling Updating: Dilation, Sure Loss, and\n  Simpson's Paradox", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical learning using imprecise probabilities is gaining more attention\nbecause it presents an alternative strategy for reducing irreplicable findings\nby freeing the user from the task of making up unwarranted high-resolution\nassumptions. However, model updating as a mathematical operation is inherently\nexact, hence updating imprecise models requires the user's judgment in choosing\namong competing updating rules. These rules often lead to incompatible\ninferences, and can exhibit unsettling phenomena like dilation, contraction and\nsure loss, which cannot occur with the Bayes rule and precise probabilities. We\nrevisit a number of famous \"paradoxes\", including the three prisoners/Monty\nHall problem, revealing that a logical fallacy arises from a set of marginally\nplausible yet jointly incommensurable assumptions when updating the underlying\nimprecise model. We establish an equivalence between Simpson's paradox and an\nimplicit adoption of a pair of aggregation rules that induce sure loss. We also\nexplore behavioral discrepancies between the generalized Bayes rule, Dempster's\nrule and the Geometric rule as alternative posterior updating rules for Choquet\ncapacities of order 2. We show that both the generalized Bayes rule and\nGeometric rule are incapable of updating without prior information regardless\nof how strong the information in our data is, and that Dempster's rule and the\nGeometric rule can mathematically contradict each other with respect to\ndilation and contraction. Our findings show that unsettling updates reflect a\ncollision between the rules' assumptions and the inexactness allowed by the\nmodel itself, highlighting the invaluable role of judicious judgment in\nhandling low-resolution information, and the care we must take when applying\nlearning rules to update imprecise probabilities.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 17:03:37 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Gong", "Ruobin", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "1712.08966", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen, Xiaoou Li, Siliang Zhang", "title": "Structured Latent Factor Analysis for Large-scale Data: Identifiability,\n  Estimability, and Their Implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent factor models are widely used to measure unobserved latent traits in\nsocial and behavioral sciences, including psychology, education, and marketing.\nWhen used in a confirmatory manner, design information is incorporated,\nyielding structured (confirmatory) latent factor models. Motivated by the\napplications of latent factor models to large-scale measurements which consist\nof many manifest variables (e.g. test items) and a large sample size, we study\nthe properties of structured latent factor models under an asymptotic setting\nwhere both the number of manifest variables and the sample size grow to\ninfinity. Specifically, under such an asymptotic regime, we provide a\ndefinition of the structural identifiability of the latent factors and\nestablish necessary and sufficient conditions on the measurement design that\nensure the structural identifiability under a general family of structured\nlatent factor models. In addition, we propose an estimator that can\nconsistently recover the latent factors under mild conditions. This estimator\ncan be efficiently computed through parallel computing. Our results shed lights\non the design of large-scale measurement and have important implications on\nmeasurement validity. The properties of the proposed estimator are verified\nthrough simulation studies.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 20:27:43 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 07:47:20 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Chen", "Yunxiao", ""], ["Li", "Xiaoou", ""], ["Zhang", "Siliang", ""]]}, {"id": "1712.09065", "submitter": "Gane Samb Lo", "authors": "Tchilabalo Atozou Kpanzou, Modou Ngom, Cherif Mamadou Moctar Traor\\'e,\n  Moumouni Diallo, Gane Samb Lo", "title": "Uniform Rates of Convergence of Some Representations of Extremes : a\n  first approach", "comments": "10", "journal-ref": null, "doi": "10.16929/jmfsp/2018.101", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniform convergence rates are provided for asymptotic representations of\nsample extremes. These bounds which are universal in the sense that they do not\ndepend on the extreme value index are meant to be extended to arbitrary samples\nextremes in coming papers.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 12:47:05 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 09:09:13 GMT"}, {"version": "v3", "created": "Sat, 29 Feb 2020 03:08:19 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Kpanzou", "Tchilabalo Atozou", ""], ["Ngom", "Modou", ""], ["Traor\u00e9", "Cherif Mamadou Moctar", ""], ["Diallo", "Moumouni", ""], ["Lo", "Gane Samb", ""]]}, {"id": "1712.09070", "submitter": "Gane Samb Lo", "authors": "Tchilabalo Abozou Kpanzou, Tertius de Wet, Gane Samb Lo", "title": "Measuring inequality: application of semi-parametric methods to real\n  life data", "comments": "10", "journal-ref": "African Journal of Applied Statistics, Vol (4)(1), 2017, pages\n  157-164", "doi": "10.16929/ajas/2017.157.207", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of methods have been introduced in order to measure the inequality\nin various situations such as income and expenditure. In order to curry out\nstatistical inference, one often needs to estimate the available measures of\ninequality. Many estimators are available in the literature, the most used ones\nbeing the non parametric estimators. kpanzou(2011) has developed\nsemi-parametric estimators for measures of inequality and showed that these are\nvery appropriate especially for heavy tailed distributions. In this paper we\napply such semi-parametric methods to a practical data set and show how they\ncompare to the non parametric estimators. A guidance is also given on the\nchoice of parametric distributions to fit in the tails of the data\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 14:12:11 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Kpanzou", "Tchilabalo Abozou", ""], ["de Wet", "Tertius", ""], ["Lo", "Gane Samb", ""]]}, {"id": "1712.09074", "submitter": "V. Roshan Joseph", "authors": "V. Roshan Joseph, Li Gu, Shan Ba, William R. Myers", "title": "Space-Filling Designs for Robustness Experiments", "comments": null, "journal-ref": null, "doi": "10.1080/00401706.2018.1451390", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To identify the robust settings of the control factors, it is very important\nto understand how they interact with the noise factors. In this article, we\npropose space-filling designs for computer experiments that are more capable of\naccurately estimating the control-by-noise interactions. Moreover, the existing\nspace-filling designs focus on uniformly distributing the points in the design\nspace, which are not suitable for noise factors because they usually follow\nnon-uniform distributions such as normal distribution. This would suggest\nplacing more points in the regions with high probability mass. However, noise\nfactors also tend to have a smooth relationship with the response and\ntherefore, placing more points towards the tails of the distribution is also\nuseful for accurately estimating the relationship. These two opposing effects\nmake the experimental design methodology a challenging problem. We propose\noptimal and computationally efficient solutions to this problem and demonstrate\ntheir advantages using simulated examples and a real industry example involving\na manufacturing packing line.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 14:32:14 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Joseph", "V. Roshan", ""], ["Gu", "Li", ""], ["Ba", "Shan", ""], ["Myers", "William R.", ""]]}, {"id": "1712.09089", "submitter": "Yinchu Zhu", "authors": "Victor Chernozhukov, Kaspar W\\\"uthrich, Yinchu Zhu", "title": "An Exact and Robust Conformal Inference Method for Counterfactual and\n  Synthetic Controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new inference procedures for counterfactual and synthetic\ncontrol methods for policy evaluation. We recast the causal inference problem\nas a counterfactual prediction and a structural breaks testing problem. This\nallows us to exploit insights from conformal prediction and structural breaks\ntesting to develop permutation inference procedures that accommodate modern\nhigh-dimensional estimators, are valid under weak and easy-to-verify\nconditions, and are provably robust against misspecification. Our methods work\nin conjunction with many different approaches for predicting counterfactual\nmean outcomes in the absence of the policy intervention. Examples include\nsynthetic controls, difference-in-differences, factor and matrix completion\nmodels, and (fused) time series panel data models. Our approach demonstrates an\nexcellent small-sample performance in simulations and is taken to a data\napplication where we re-evaluate the consequences of decriminalizing indoor\nprostitution. Open-source software for implementing our conformal inference\nmethods is available.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 15:29:10 GMT"}, {"version": "v10", "created": "Thu, 20 May 2021 16:29:17 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 07:08:30 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 01:11:31 GMT"}, {"version": "v4", "created": "Fri, 5 Oct 2018 06:35:12 GMT"}, {"version": "v5", "created": "Wed, 5 Dec 2018 18:13:21 GMT"}, {"version": "v6", "created": "Sun, 7 Jul 2019 16:48:07 GMT"}, {"version": "v7", "created": "Fri, 22 Nov 2019 00:16:51 GMT"}, {"version": "v8", "created": "Tue, 1 Sep 2020 12:52:53 GMT"}, {"version": "v9", "created": "Sun, 16 May 2021 15:12:14 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Chernozhukov", "Victor", ""], ["W\u00fcthrich", "Kaspar", ""], ["Zhu", "Yinchu", ""]]}, {"id": "1712.09150", "submitter": "Michael Smith", "authors": "Ruben Loaiza-Maya and Michael Stanley Smith", "title": "Variational Bayes Estimation of Discrete-Margined Copula Models with\n  Application to Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new variational Bayes estimator for high-dimensional copulas\nwith discrete, or a combination of discrete and continuous, margins. The method\nis based on a variational approximation to a tractable augmented posterior, and\nis faster than previous likelihood-based approaches. We use it to estimate\ndrawable vine copulas for univariate and multivariate Markov ordinal and mixed\ntime series. These have dimension $rT$, where $T$ is the number of observations\nand $r$ is the number of series, and are difficult to estimate using previous\nmethods. The vine pair-copulas are carefully selected to allow for\nheteroskedasticity, which is a feature of most ordinal time series data. When\ncombined with flexible margins, the resulting time series models also allow for\nother common features of ordinal data, such as zero inflation, multiple modes\nand under- or over-dispersion. Using six example series, we illustrate both the\nflexibility of the time series copula models, and the efficacy of the\nvariational Bayes estimator for copulas of up to 792 dimensions and 60\nparameters. This far exceeds the size and complexity of copula models for\ndiscrete data that can be estimated using previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 00:38:39 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 05:18:15 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Loaiza-Maya", "Ruben", ""], ["Smith", "Michael Stanley", ""]]}, {"id": "1712.09188", "submitter": "Benjamin All\\'evius", "authors": "Benjamin All\\'evius and Michael H\\\"ohle", "title": "An expectation-based space-time scan statistic for ZIP-distributed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An expectation-based scan statistic is proposed for the prospective\nmonitoring of spatio-temporal count data with an excess of zeros. The method,\nwhich is based on an outbreak model for the zero-inflated Poisson distribution,\nis shown to be superior to traditional scan statistics based on the Poisson\ndistribution in the presence of structural zeros. The spatial accuracy and\ndetection timeliness of the proposed scan statistic is investigated by means of\nsimulation, and an application on weekly cases of Campylobacteriosis in Germany\nillustrates how the scan statistic could be used to detect emerging disease\noutbreaks. An implementation of the method is provided in the open source R\npackage scanstatistics available on CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 06:49:13 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["All\u00e9vius", "Benjamin", ""], ["H\u00f6hle", "Michael", ""]]}, {"id": "1712.09222", "submitter": "Takuya Ishihara", "authors": "Takuya Ishihara", "title": "Identification and Estimation of Time-Varying Nonseparable Panel Data\n  Models without Stayers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the identification and estimation of nonseparable panel\ndata models. We show that the structural function is nonparametrically\nidentified when it is strictly increasing in a scalar unobservable variable,\nthe conditional distributions of unobservable variables do not change over\ntime, and the joint support of explanatory variables satisfies some weak\nassumptions. To identify the target parameters, existing studies assume that\nthe structural function does not change over time, and that there are\n\"stayers\", namely individuals with the same regressor values in two time\nperiods. Our approach, by contrast, allows the structural function to depend on\nthe time period in an arbitrary manner and does not require the existence of\nstayers. In estimation part of the paper, we consider parametric models and\ndevelop an estimator that implements our identification results. We then show\nthe consistency and asymptotic normality of our estimator. Monte Carlo studies\nindicate that our estimator performs well in finite samples. Finally, we extend\nour identification results to models with discrete outcomes, and show that the\nstructural function is partially identified.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 09:27:54 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 02:27:28 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Ishihara", "Takuya", ""]]}, {"id": "1712.09225", "submitter": "Clement Dombry", "authors": "Zhen Wai Olivier Ho and Clement Dombry", "title": "Simple models for multivariate regular variations and the H\\\"usler-Reiss\n  Pareto distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit multivariate extreme value theory modeling by emphasizing\nmultivariate regular variations and the multivariate Breiman Lemma. This allows\nus to recover in a simple framework the most popular multivariate extreme value\ndistributions, such as the logistic, negative logistic, Dirichlet, extremal-$t$\nand H\\\"usler-Reiss models. In a second part of the paper, we focus on the\nH\\\"usler-Reiss Pareto model and its surprising exponential family property.\nAfter a thorough study of this exponential family structure, we focus on\nmaximum likelihood estimation. We also consider the generalized H\\\"usler-Reiss\nPareto model with different tail indices and a likelihood ratio test for\ndiscriminating constant tail index versus varying tail indices.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 09:56:50 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Ho", "Zhen Wai Olivier", ""], ["Dombry", "Clement", ""]]}, {"id": "1712.09483", "submitter": "Zhao Ren", "authors": "Yu Liu and Zhao Ren", "title": "Minimax Estimation of Large Precision Matrices with Bandable Cholesky\n  Factor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Last decade witnesses significant methodological and theoretical advances in\nestimating large precision matrices. In particular, there are scientific\napplications such as longitudinal data, meteorology and spectroscopy in which\nthe ordering of the variables can be interpreted through a bandable structure\non the Cholesky factor of the precision matrix. However, the minimax theory has\nstill been largely unknown, as opposed to the well established minimax results\nover the corresponding bandable covariance matrices. In this paper, we focus on\ntwo commonly used types of parameter spaces, and develop the optimal rates of\nconvergence under both the operator norm and the Frobenius norm. A striking\nphenomenon is found: two types of parameter spaces are fundamentally different\nunder the operator norm but enjoy the same rate optimality under the Frobenius\nnorm, which is in sharp contrast to the equivalence of corresponding two types\nof bandable covariance matrices under both norms. This fundamental difference\nis established by carefully constructing the corresponding minimax lower\nbounds. Two new estimation procedures are developed: for the operator norm, our\noptimal procedure is based on a novel local cropping estimator targeting on all\nprinciple submatrices of the precision matrix while for the Frobenius norm, our\noptimal procedure relies on a delicate regression-based thresholding rule.\nLepski's method is considered to achieve optimal adaptation. We further\nestablish rate optimality in the nonparanormal model. Numerical studies are\ncarried out to confirm our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 03:26:25 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 06:35:21 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 03:47:33 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Liu", "Yu", ""], ["Ren", "Zhao", ""]]}, {"id": "1712.09562", "submitter": "Achmad Choiruddin", "authors": "Achmad Choiruddin, Jean-Fran\\c{c}ois Coeurjolly, and Fr\\'ed\\'erique\n  Letu\\'e", "title": "Spatial point processes intensity estimation with a diverging number of\n  covariates", "comments": "arXiv admin note: text overlap with arXiv:1703.02462", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection procedures for spatial point processes parametric intensity\nestimation have been recently developed since more and more applications\ninvolve a large number of covariates. In this paper, we investigate the setting\nwhere the number of covariates diverges as the domain of observation increases.\nIn particular, we consider estimating equations based on Campbell theorems\nderived from Poisson and logistic regression likelihoods regularized by a\ngeneral penalty function. We prove that, under some conditions, the\nconsistency, the sparsity, and the asymptotic normality are valid for such a\nsetting. We support the theoretical results by numerical ones obtained from\nsimulation experiments and an application to forestry datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 12:34:05 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Choiruddin", "Achmad", ""], ["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Letu\u00e9", "Fr\u00e9d\u00e9rique", ""]]}, {"id": "1712.09566", "submitter": "Virgilio Gomez-Rubio", "authors": "Virgilio Gomez-Rubio", "title": "Mixture model fitting using conditional models and modal Gibbs sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach to fitting mixture models based on\nestimating first the posterior distribution of the auxiliary variables that\nassign each observation to a group in the mixture. The posterior distributions\nof the remainder of the parameters in the mixture is obtained by averaging over\ntheir conditional posterior marginals on the auxiliary variables using Bayesian\nmodel averaging.\n  A new algorithm based on Gibbs sampling is used to approximate the posterior\ndistribution of the auxiliary variables without sampling any other parameter in\nthe model. In particular, the modes of the full conditionals of the parameters\nof the densities in the mixture are computed and these are plugged-in to the\nfull conditional of the auxiliary variables to draw samples. This\napproximation, that we have called 'modal' Gibbs sampling, reduces the\ncomputational burden in the Gibbs sampling algorithm and still provides very\ngood estimates of the posterior distribution of the auxiliary variables.\nConditional models on the auxiliary variables are fitted using the Integrated\nNested Laplace Approximation (INLA) to obtain the conditional posterior\ndistributions, including modes, of the distributional parameters in the\nmixtures.\n  This approach is general enough to consider mixture models with discrete or\ncontinuous outcomes from a wide range of distributions and latent models as\nconditional model fitting is done with INLA. This presents several other\nadvantages, such as fast fitting of the conditional models, not being\nrestricted to the use of conjugate priors on the model parameters and being\nless prone to label switching. Within this framework, computing the marginal\nlikelihood of the mixture model when the number of groups in the mixture is\nknown is easy and it can be used to tackle selection of the number of\ncomponents.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 12:45:53 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Gomez-Rubio", "Virgilio", ""]]}, {"id": "1712.09577", "submitter": "Simone Padoan PhD", "authors": "Enkelejd Hashorva, Simone A. Padoan, Stefano Rizzelli", "title": "Multivariate Extremes Over a Random Number of Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical multivariate extreme-value theory concerns the modeling of\nextremes in a multivariate random sample, suggesting the use of max-stable\ndistributions. In this work, the classical theory is extended to the case where\naggregated data, such as maxima of a random number of observations, are\nconsidered. We derive a limit theorem concerning the attractors for the\ndistributions of the aggregated data, which boil down to a new family of\nmax-stable distributions. We also connect the extremal dependence structure of\nclassical max-stable distributions and that of our new family of max-stable\ndistributions. By means of an inversion method, we derive a semiparametric\ncomposite-estimator for the extremal dependence of the unobservable data,\nstarting from a preliminary estimator of the extremal dependence of the\naggregated data. Furthermore, we develop the large-sample theory of the\ncomposite-estimator and illustrate its finite-sample performance via a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 13:23:38 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 11:32:11 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 14:13:24 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Hashorva", "Enkelejd", ""], ["Padoan", "Simone A.", ""], ["Rizzelli", "Stefano", ""]]}, {"id": "1712.09596", "submitter": "Ralph Brinks", "authors": "Ralph Brinks", "title": "New ways of estimating excess mortality of chronic diseases: Insights\n  from the illness-death model", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we have shown that the age-specific prevalence of a disease can be\nrelated to the transition rates in the illness-death model via a partial\ndifferential equation (PDE). In case of a chronic disease, we show that the PDE\ncan be used to estimate excess mortality from prevalence and incidence.\nApplicability of the new method is demonstrated in a simulation and claims data\nabout diabetes in German men.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 15:07:59 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Brinks", "Ralph", ""]]}, {"id": "1712.09694", "submitter": "Haolei Weng", "authors": "Haolei Weng and Yang Feng", "title": "On the estimation of correlation in a binary sequence model", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a binary sequence generated by thresholding a hidden continuous\nsequence. The hidden variables are assumed to have a compound symmetry\ncovariance structure with a single parameter characterizing the common\ncorrelation. We study the parameter estimation problem under such one-parameter\nmodels. We demonstrate that maximizing the likelihood function does not yield\nconsistent estimates for the correlation. We then formally prove the\nnonestimability of the parameter by deriving a non-vanishing minimax lower\nbound. This counter-intuitive phenomenon provides an interesting insight that\none-bit information of each latent variable is not sufficient to consistently\nrecover their common correlation. On the other hand, we further show that\ntrinary data generated from the hidden variables can consistently estimate the\ncorrelation with parametric convergence rate. Thus we reveal a phase transition\nphenomenon regarding the discretization of latent continuous variables while\npreserving the estimability of the correlation. Numerical experiments are\nperformed to validate the conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 22:19:19 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 19:27:54 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Weng", "Haolei", ""], ["Feng", "Yang", ""]]}, {"id": "1712.09767", "submitter": "Sanvesh Srivastava", "authors": "Rajarshi Guhaniyogi, Cheng Li, Terrance D. Savitsky, and Sanvesh\n  Srivastava", "title": "A Divide-and-Conquer Bayesian Approach to Large-Scale Kriging", "comments": "29 pages, including 4 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a three-step divide-and-conquer strategy within the Bayesian\nparadigm that delivers massive scalability for any spatial process model. We\npartition the data into a large number of subsets, apply a readily available\nBayesian spatial process model on every subset, in parallel, and optimally\ncombine the posterior distributions estimated across all the subsets into a\npseudo-posterior distribution that conditions on the entire data. The combined\npseudo posterior distribution replaces the full data posterior distribution for\npredicting the responses at arbitrary locations and for inference on the model\nparameters and spatial surface. Based on distributed Bayesian inference, our\napproach is called \"Distributed Kriging\" (DISK) and offers significant\nadvantages in massive data applications where the full data are stored across\nmultiple machines. We show theoretically that the Bayes $L_2$-risk of the DISK\nposterior distribution achieves the near optimal convergence rate in estimating\nthe true spatial surface with various types of covariance functions, and\nprovide upper bounds for the number of subsets as a function of the full sample\nsize. The model-free feature of DISK is demonstrated by scaling posterior\ncomputations in spatial process models with a stationary full-rank and a\nnonstationary low-rank Gaussian process (GP) prior. A variety of simulations\nand a geostatistical analysis of the Pacific Ocean sea surface temperature data\nvalidate our theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 06:04:31 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 09:12:59 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 07:38:12 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Guhaniyogi", "Rajarshi", ""], ["Li", "Cheng", ""], ["Savitsky", "Terrance D.", ""], ["Srivastava", "Sanvesh", ""]]}, {"id": "1712.09813", "submitter": "Mansoor Sheikh", "authors": "M Sheikh and A C C Coolen", "title": "Accurate Bayesian Data Classification without Hyperparameter\n  Cross-validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the standard Bayesian multivariate Gaussian generative data\nclassifier by considering a generalization of the conjugate, normal-Wishart\nprior distribution and by deriving the hyperparameters analytically via\nevidence maximization. The behaviour of the optimal hyperparameters is explored\nin the high-dimensional data regime. The classification accuracy of the\nresulting generalized model is competitive with state-of-the art Bayesian\ndiscriminant analysis methods, but without the usual computational burden of\ncross-validation.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 10:36:34 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Sheikh", "M", ""], ["Coolen", "A C C", ""]]}, {"id": "1712.09816", "submitter": "Marco Oesting", "authors": "Sebastian Engelke and Raphael de Fondeville and Marco Oesting", "title": "Extremal Behavior of Aggregated Data with an Application to Downscaling", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of spatially aggregated data from a stochastic process $X$\nmay exhibit a different tail behavior than its marginal distributions. For a\nlarge class of aggregating functionals $\\ell$ we introduce the $\\ell$-extremal\ncoefficient that quantifies this difference as a function of the extremal\nspatial dependence in $X$. We also obtain the joint extremal dependence for\nmultiple aggregation functionals applied to the same process. Explicit formulas\nfor the $\\ell$-extremal coefficients and multivariate dependence structures are\nderived in important special cases. The results provide a theoretical link\nbetween the extremal distribution of the aggregated data and the corresponding\nunderlying process, which we exploit to develop a method for statistical\ndownscaling. We apply our framework to downscale daily temperature maxima in\nthe south of France from a gridded data set and use our model to generate high\nresolution maps of the warmest day during the 2003 heatwave.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 10:40:00 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Engelke", "Sebastian", ""], ["de Fondeville", "Raphael", ""], ["Oesting", "Marco", ""]]}, {"id": "1712.09870", "submitter": "Thiago Do Rego Sousa", "authors": "Thiago do R\\^ego Sousa, Stephan Haug and Claudia Kl\\\"uppelberg", "title": "Indirect Inference for L\\'evy-driven continuous-time GARCH models", "comments": "39 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advocate the use of an Indirect Inference method to estimate the parameter\nof a COGARCH(1,1) process for equally spaced observations. This requires that\nthe true model can be simulated and a reasonable estimation method for an\napproximate auxiliary model. We follow previous approaches and use linear\nprojections leading to an auxiliary autoregressive model for the squared\nCOGARCH returns. The asymptotic theory of the Indirect Inference estimator\nrelies {on a uniform SLLN and asymptotic normality of the parameter estimates\nof the auxiliary model, which require continuity and differentiability of the\nCOGARCH process} with respect to its parameter and which we prove via\nKolmogorov's continuity criterion. This leads to consistent and asymptotically\nnormal Indirect Inference estimates under moment conditions on the driving\nL\\'evy process. A simulation study shows that the method yields a substantial\nfinite sample bias reduction compared to previous estimators.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 14:15:26 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 20:18:14 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Sousa", "Thiago do R\u00eago", ""], ["Haug", "Stephan", ""], ["Kl\u00fcppelberg", "Claudia", ""]]}, {"id": "1712.09957", "submitter": "Moreno Bevilacqua", "authors": "Moreno Bevilacqua and Tarik Faouzi", "title": "Estimation and prediction of Gaussian processes using generalized Cauchy\n  covariance model under fixed domain asymptotics", "comments": "arXiv admin note: text overlap with arXiv:1607.06921", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study estimation and prediction of Gaussian processes with covariance\nmodel belonging to the generalized Cauchy (GC) family, under fixed domain\nasymptotics. Gaussian processes with this kind of covariance function provide\nseparate characterization of fractal dimension and long range dependence, an\nappealing feature in many physical, biological or geological systems. The\nresults of the paper are classified into three parts. In the first part, we\ncharacterize the equivalence of two Gaussian measures with GC covariance\nfunction. Then we provide sufficient conditions for the equivalence of two\nGaussian measures with Mat{\\'e}rn (MT) and GC covariance functions and two\nGaussian measures with Generalized Wendland (GW) and GC covariance functions.\nIn the second part, we establish strong consistency and asymptotic distribution\nof the maximum likelihood estimator of the microergodic parameter associated to\nGC covariance model, under fixed domain asymptotics. The last part focuses on\noptimal prediction with GC model and specifically, we give conditions for\nasymptotic efficiency prediction and asymptotically correct estimation of mean\nsquare error using a misspecified GC, MT or GW model, under fixed domain\nasymptotics. Our findings are illustrated through a simulation study: the first\ncompares the finite sample behavior of the maximum likelihood estimation of the\nmicroergodic parameter of the GC model with the given asymptotic distribution.\nWe then compare the finite-sample behavior of the prediction and its associated\nmean square error when the true model is GC and the prediction is performed\nusing the true model and a misspecified GW model.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 18:07:32 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 17:17:48 GMT"}, {"version": "v3", "created": "Sat, 11 Aug 2018 15:18:51 GMT"}, {"version": "v4", "created": "Sat, 20 Jul 2019 08:39:42 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Bevilacqua", "Moreno", ""], ["Faouzi", "Tarik", ""]]}, {"id": "1712.09981", "submitter": "Marco Geraci", "authors": "Marco Geraci", "title": "Nonlinear quantile mixed models", "comments": "26 pages, 8 figures, 8 tables", "journal-ref": "Computational Statistics & Data Analysis, Volume 136, 2019, Pages\n  30-46", "doi": "10.1016/j.csda.2018.12.005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression applications, the presence of nonlinearity and correlation\namong observations offer computational challenges not only in traditional\nsettings such as least squares regression, but also (and especially) when the\nobjective function is non-smooth as in the case of quantile regression. In this\npaper, we develop methods for the modeling and estimation of nonlinear\nconditional quantile functions when data are clustered within two-level nested\ndesigns. This work represents an extension of the linear quantile mixed models\nof Geraci and Bottai (2014, Statistics and Computing). We develop a novel\nalgorithm which is a blend of a smoothing algorithm for quantile regression and\na second order Laplacian approximation for nonlinear mixed models. To assess\nthe proposed methods, we present a simulation study and two applications, one\nin pharmacokinetics and one related to growth curve modeling in agriculture.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 18:41:23 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 12:06:18 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Geraci", "Marco", ""]]}, {"id": "1712.09982", "submitter": "Miguel de Carvalho", "authors": "Miguel de Carvalho, Bradley J. Barney, Garritt L. Page", "title": "Affinity-based measures of medical diagnostic test accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new summary measures of diagnostic test accuracy which can be used\nas companions to existing diagnostic accuracy measures. Conceptually, our\nsummary measures are tantamount to the so-called Hellinger affinity and we show\nthat they can be regarded as measures of agreement constructed from similar\ngeometrical principles as Pearson correlation. A covariate-specific version of\nour summary index is developed, which can be used to assess the discrimination\nperformance of a diagnostic test, conditionally on the value of a predictor.\nNonparametric Bayes estimators for the proposed indexes are devised,\ntheoretical properties of the corresponding priors are derived, and the\nperformance of our methods is assessed through a simulation study. Data from a\nprostate cancer diagnosis study are used to illustrate our methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 18:41:40 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["de Carvalho", "Miguel", ""], ["Barney", "Bradley J.", ""], ["Page", "Garritt L.", ""]]}, {"id": "1712.10099", "submitter": "Yixuan Qiu", "authors": "Yixuan Qiu and Lingsong Zhang", "title": "Finite-sample bounds for the multivariate Behrens-Fisher distribution\n  with proportional covariances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Behrens-Fisher problem is a well-known hypothesis testing problem in\nstatistics concerning two-sample mean comparison. In this article, we confirm\none conjecture in Eaton and Olshen (1972), which provides stochastic bounds for\nthe multivariate Behrens-Fisher test statistic under the null hypothesis. We\nalso extend their results on the stochastic ordering of random quotients to the\narbitrary finite dimensional case. This work can also be seen as a\ngeneralization of Hsu (1938) that provided the bounds for the univariate\nBehrens-Fisher problem. The results obtained in this article can be used to\nderive a testing procedure for the multivariate Behrens-Fisher problem that\nstrongly controls the Type I error.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 02:34:45 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Qiu", "Yixuan", ""], ["Zhang", "Lingsong", ""]]}, {"id": "1712.10120", "submitter": "Robert Staudte", "authors": "Luke A. Prendergast and Robert G. Staudte", "title": "Decomposing the Quantile Ratio Index with applications to Australian\n  income and wealth data", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantile ratio index introduced by Prendergast and Staudte 2017 is a\nsimple and effective measure of relative inequality for income data that is\nresistant to outliers. It measures the average relative distance of a randomly\nchosen income from its symmetric quantile. Another useful property of this\nindex is investigated here: given a partition of the income distribution into a\nunion of sets of symmetric quantiles, one can find the conditional inequality\nfor each set as measured by the quantile ratio index and readily combine them\nin a weighted average to obtain the index for the entire population. When\napplied to data for various years, one can track how these contributions to\ninequality vary over time, as illustrated here for Australian Bureau of\nStatistics income and wealth data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 05:09:33 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Prendergast", "Luke A.", ""], ["Staudte", "Robert G.", ""]]}, {"id": "1712.10131", "submitter": "Alireza Doostan", "authors": "Paul Diaz, Alireza Doostan, and Jerrad Hampton", "title": "Sparse Polynomial Chaos Expansions via Compressed Sensing and D-optimal\n  Design", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2018.03.020", "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of uncertainty quantification, sparse polynomial chaos (PC)\nexpansions are commonly used by researchers for a variety of purposes, such as\nsurrogate modeling. Ideas from compressed sensing may be employed to exploit\nthis sparsity in order to reduce computational costs. A class of greedy\ncompressed sensing algorithms use least squares minimization to approximate PC\ncoefficients. This least squares problem lends itself to the theory of optimal\ndesign of experiments (ODE). Our work focuses on selecting an experimental\ndesign that improves the accuracy of sparse PC approximations for a fixed\ncomputational budget. We propose DSP, a novel sequential design, greedy\nalgorithm for sparse PC approximation. The algorithm sequentially augments an\nexperimental design according to a set of the basis polynomials deemed\nimportant by the magnitude of their coefficients, at each iteration. Our\nalgorithm incorporates topics from ODE to estimate the PC coefficients. A\nvariety of numerical simulations are performed on three physical models and\nmanufactured sparse PC expansions to provide a comparative study between our\nproposed algorithm and other non-adaptive methods. Further, we examine the\nimportance of sampling by comparing different strategies in terms of their\nability to generate a candidate pool from which an optimal experimental design\nis chosen. It is demonstrated that the most accurate PC coefficient\napproximations, with the least variability, are produced with our\ndesign-adaptive greedy algorithm and the use of a studied importance sampling\nstrategy. We provide theoretical and numerical results which show that using an\noptimal sampling strategy for the candidate pool is key, both in terms of\naccuracy in the approximation, but also in terms of constructing an optimal\ndesign.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 07:09:13 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Diaz", "Paul", ""], ["Doostan", "Alireza", ""], ["Hampton", "Jerrad", ""]]}]